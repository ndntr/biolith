{
  "updated_at": "2026-01-12T23:17:40.881Z",
  "clusters": [
    {
      "id": "cluster_18",
      "coverage": 3,
      "updated_at": "Mon, 12 Jan 2026 16:00:45 -0500",
      "title": "Elon Musk says Apple and Google's Gemini deal \"seems like an unreasonable concentration of power for Google given that [they] also have Android and Chrome\" (Joe Rossignol/MacRumors)",
      "neutral_headline": "Can Google save Apple AI? Gemini to power a new, personalized Siri",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/260112/p32#a260112p32",
          "published_at": "Mon, 12 Jan 2026 16:00:45 -0500",
          "title": "Elon Musk says Apple and Google's Gemini deal \"seems like an unreasonable concentration of power for Google given that [they] also have Android and Chrome\" (Joe Rossignol/MacRumors)",
          "standfirst": "Joe Rossignol / MacRumors: Elon Musk says Apple and Google's Gemini deal &ldquo;seems like an unreasonable concentration of power for Google given that [they] also have Android and Chrome&rdquo; &mdash; Elon Musk today expressed concern about Apple and Google partnering on a more personalized version of Siri powered by Google's generative AI platform Gemini.",
          "content": "Joe Rossignol / MacRumors: Elon Musk says Apple and Google's Gemini deal &ldquo;seems like an unreasonable concentration of power for Google given that [they] also have Android and Chrome&rdquo; &mdash; Elon Musk today expressed concern about Apple and Google partnering on a more personalized version of Siri powered by Google's generative AI platform Gemini.",
          "feed_position": 4,
          "image_url": "http://www.techmeme.com/260112/i32.jpg"
        },
        {
          "source": "ZDNet",
          "url": "https://www.zdnet.com/article/gemini-apple-intelligence-siri-upgrade/",
          "published_at": "Mon, 12 Jan 2026 20:53:36 GMT",
          "title": "Can Google save Apple AI? Gemini to power a new, personalized Siri",
          "standfirst": "A new deal between Apple and Google makes Gemini the cloud-based technology driving Apple Intelligence and Siri. Here's what that could look like.",
          "content": "A new deal between Apple and Google makes Gemini the cloud-based technology driving Apple Intelligence and Siri. Here's what that could look like.",
          "feed_position": 0
        },
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2026/01/12/googles-gemini-to-power-apples-ai-features-like-siri/",
          "published_at": "Mon, 12 Jan 2026 17:12:41 +0000",
          "title": "Google’s Gemini to power Apple’s AI features like Siri",
          "standfirst": "Apple and Google have embarked on a non-exclusive, multi-year partnership that will involve Apple using Gemini models and Google cloud technology for future foundational models.",
          "content": "Apple and Google have embarked on a non-exclusive, multi-year partnership that will involve Apple using Gemini models and Google cloud technology for future foundational models.",
          "feed_position": 9
        },
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/260112/p25#a260112p25",
          "published_at": "Mon, 12 Jan 2026 12:10:01 -0500",
          "title": "Google says Gemini will help power not only a more personalized version of Siri, but also future Apple Intelligence features (Joe Rossignol/MacRumors)",
          "standfirst": "Joe Rossignol / MacRumors: Google says Gemini will help power not only a more personalized version of Siri, but also future Apple Intelligence features &mdash; Google today announced that its AI platform Gemini will help power not only a more personalized version of Siri, but a range of future Apple Intelligence features.",
          "content": "Joe Rossignol / MacRumors: Google says Gemini will help power not only a more personalized version of Siri, but also future Apple Intelligence features &mdash; Google today announced that its AI platform Gemini will help power not only a more personalized version of Siri, but a range of future Apple Intelligence features.",
          "feed_position": 11,
          "image_url": "http://www.techmeme.com/260112/i25.jpg"
        }
      ],
      "featured_image": "http://www.techmeme.com/260112/i32.jpg",
      "popularity_score": 3017.7178108333333
    },
    {
      "id": "cluster_0",
      "coverage": 2,
      "updated_at": "Mon, 12 Jan 2026 18:05:00 -0500",
      "title": "Personal finance app Betterment says an individual accessed certain systems to send fake crypto scam notification and believes the person accessed user info (Emily Mason/Bloomberg)",
      "neutral_headline": "Personal finance app Betterment says an individual accessed certain systems to send fake crypto scam notification and...",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/260112/p36#a260112p36",
          "published_at": "Mon, 12 Jan 2026 18:05:00 -0500",
          "title": "Personal finance app Betterment says an individual accessed certain systems to send fake crypto scam notification and believes the person accessed user info (Emily Mason/Bloomberg)",
          "standfirst": "Emily Mason / Bloomberg: Personal finance app Betterment says an individual accessed certain systems to send fake crypto scam notification and believes the person accessed user info &mdash; Personal finance platform Betterment fell victim to an online attack that allowed an unauthorized individual to send some customers a &hellip;",
          "content": "Emily Mason / Bloomberg: Personal finance app Betterment says an individual accessed certain systems to send fake crypto scam notification and believes the person accessed user info &mdash; Personal finance platform Betterment fell victim to an online attack that allowed an unauthorized individual to send some customers a &hellip;",
          "feed_position": 0,
          "image_url": "http://www.techmeme.com/260112/i36.jpg"
        },
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2026/01/12/fintech-firm-betterment-confirms-data-breach-after-hackers-send-fake-crypto-scam-notification-to-users/",
          "published_at": "Mon, 12 Jan 2026 18:14:10 +0000",
          "title": "Fintech firm Betterment confirms data breach after hackers send fake crypto scam notification to users",
          "standfirst": "Hackers gained access to some Betterment customers’ personal information through a social engineering attack, then targeted some of them with a crypto-related phishing message.",
          "content": "Hackers gained access to some Betterment customers’ personal information through a social engineering attack, then targeted some of them with a crypto-related phishing message.",
          "feed_position": 7
        }
      ],
      "featured_image": "http://www.techmeme.com/260112/i36.jpg",
      "popularity_score": 2019.7886441666667
    },
    {
      "id": "cluster_8",
      "coverage": 2,
      "updated_at": "2026-01-12T17:14:58-05:00",
      "title": "Meta plans to lay off hundreds of metaverse employees this week",
      "neutral_headline": "Meta plans to lay off hundreds of metaverse employees this week",
      "items": [
        {
          "source": "The Verge",
          "url": "https://www.theverge.com/news/860984/meta-reality-labs-layoffs-metaverse",
          "published_at": "2026-01-12T17:14:58-05:00",
          "title": "Meta plans to lay off hundreds of metaverse employees this week",
          "standfirst": "Meta's Reality Labs team is expected to lose around 10 percent of its staff, with layoffs concentrated on the division's metaverse employees, as reported by The New York Times. The layoffs are apparently a side effect of Meta's AI ambitions, which are pulling focus away from its virtual reality division. According to the Times, Meta's [&#8230;]",
          "content": "Meta's Reality Labs team is expected to lose around 10 percent of its staff, with layoffs concentrated on the division's metaverse employees, as reported by The New York Times. The layoffs are apparently a side effect of Meta's AI ambitions, which are pulling focus away from its virtual reality division. According to the Times, Meta's chief technology officer, Andrew Bosworth, called a meeting for Wednesday that he \"urged staff to attend in person,\" saying it will be the \"most important\" meeting of the year. Bosworth oversees the Reality Labs division, which employs about 15,000 people. Unfortunately, layoffs to Meta's VR team may not come … Read the full story at The Verge.",
          "feed_position": 2
        },
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/260112/p31#a260112p31",
          "published_at": "Mon, 12 Jan 2026 15:42:14 -0500",
          "title": "Sources: Meta plans to lay off ~10% of its 15,000-person Reality Labs division, disproportionately affecting those working on VR headsets and Horizon Worlds (New York Times)",
          "standfirst": "New York Times: Sources: Meta plans to lay off ~10% of its 15,000-person Reality Labs division, disproportionately affecting those working on VR headsets and Horizon Worlds &mdash; The layoffs are set to be announced this week and would affect Meta's work on the metaverse, as the company spends heavily on building artificial intelligence.",
          "content": "New York Times: Sources: Meta plans to lay off ~10% of its 15,000-person Reality Labs division, disproportionately affecting those working on VR headsets and Horizon Worlds &mdash; The layoffs are set to be announced this week and would affect Meta's work on the metaverse, as the company spends heavily on building artificial intelligence.",
          "feed_position": 5,
          "image_url": "http://www.techmeme.com/260112/i31.jpg"
        }
      ],
      "featured_image": "http://www.techmeme.com/260112/i31.jpg",
      "popularity_score": 2018.9547552777778
    },
    {
      "id": "cluster_13",
      "coverage": 2,
      "updated_at": "Mon, 12 Jan 2026 21:47:32 +0000",
      "title": "Google removes some AI health summaries after investigation finds “dangerous” flaws",
      "neutral_headline": "Google removes some AI health summaries after investigation finds “dangerous” flaws",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/ai/2026/01/google-removes-some-ai-health-summaries-after-investigation-finds-dangerous-flaws/",
          "published_at": "Mon, 12 Jan 2026 21:47:32 +0000",
          "title": "Google removes some AI health summaries after investigation finds “dangerous” flaws",
          "standfirst": "AI Overviews provided false liver test information experts called alarming.",
          "content": "On Sunday, Google removed some of its AI Overviews health summaries after a Guardian investigation found people were being put at risk by false and misleading information. The removals came after the newspaper found that Google's generative AI feature delivered inaccurate health information at the top of search results, potentially leading seriously ill patients to mistakenly conclude they are in good health. Google disabled specific queries, such as \"what is the normal range for liver blood tests,\" after experts contacted by The Guardian flagged the results as dangerous. The report also highlighted a critical error regarding pancreatic cancer: The AI suggested patients avoid high-fat foods, a recommendation that contradicts standard medical guidance to maintain weight and could jeopardize patient health. Despite these findings, Google only deactivated the summaries for the liver test queries, leaving other potentially harmful answers accessible. The investigation revealed that searching for liver test norms generated raw data tables (listing specific enzymes like ALT, AST, and alkaline phosphatase) that lacked essential context. The AI feature also failed to adjust these figures for patient demographics such as age, sex, and ethnicity. Experts warned that because the AI model's definition of \"normal\" often differed from actual medical standards, patients with serious liver conditions might mistakenly believe they are healthy and skip necessary follow-up care.Read full article Comments",
          "feed_position": 2,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2024/05/GettyImages-1488311999-1152x648.jpg"
        },
        {
          "source": "Guardian Tech",
          "url": "https://www.theguardian.com/technology/2026/jan/11/google-ai-overviews-health-guardian-investigation",
          "published_at": "Sun, 11 Jan 2026 07:00:19 GMT",
          "title": "‘Dangerous and alarming’: Google removes some of its AI summaries after users’ health put at risk",
          "standfirst": "Exclusive: Guardian investigation finds AI Overviews provided inaccurate and false information when queried over blood testsGoogle has removed some of its artificial intelligence health summaries after a Guardian investigation found people were being put at risk of harm by false and misleading information.The company has said its AI Overviews, which use generative AI to provide snapshots of essential information about a topic or question, are “helpful” and “reliable”. Continue reading...",
          "content": "Exclusive: Guardian investigation finds AI Overviews provided inaccurate and false information when queried over blood testsGoogle has removed some of its artificial intelligence health summaries after a Guardian investigation found people were being put at risk of harm by false and misleading information.The company has said its AI Overviews, which use generative AI to provide snapshots of essential information about a topic or question, are “helpful” and “reliable”. Continue reading...",
          "feed_position": 2
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2024/05/GettyImages-1488311999-1152x648.jpg",
      "popularity_score": 2018.4975330555556
    },
    {
      "id": "cluster_15",
      "coverage": 2,
      "updated_at": "Mon, 12 Jan 2026 21:44:43 +0000",
      "title": "Mark Zuckerberg says Meta is launching its own AI infrastructure initiative",
      "neutral_headline": "Mark Zuckerberg says Meta is launching its own AI infrastructure initiative",
      "items": [
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2026/01/12/mark-zuckerberg-says-meta-is-launching-its-own-ai-infrastructure-initiative/",
          "published_at": "Mon, 12 Jan 2026 21:44:43 +0000",
          "title": "Mark Zuckerberg says Meta is launching its own AI infrastructure initiative",
          "standfirst": "Meta is ramping up its efforts to build out its AI capacity — Zuckerberg said the company intended to drastically expand its energy footprint in the coming years.",
          "content": "Meta is ramping up its efforts to build out its AI capacity — Zuckerberg said the company intended to drastically expand its energy footprint in the coming years.",
          "feed_position": 1
        },
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/260112/p28#a260112p28",
          "published_at": "Mon, 12 Jan 2026 13:20:04 -0500",
          "title": "Mark Zuckerberg says Meta is establishing a new \"top-level\" initiative called Meta Compute to build \"tens of gigawatts\" of AI infrastructure during this decade (Sara Fischer/Axios)",
          "standfirst": "Sara Fischer / Axios: Mark Zuckerberg says Meta is establishing a new &ldquo;top-level&rdquo; initiative called Meta Compute to build &ldquo;tens of gigawatts&rdquo; of AI infrastructure during this decade &mdash; Meta CEO Mark Zuckerberg on Monday said his company is establishing a new &ldquo;top-level&rdquo; &hellip;",
          "content": "Sara Fischer / Axios: Mark Zuckerberg says Meta is establishing a new &ldquo;top-level&rdquo; initiative called Meta Compute to build &ldquo;tens of gigawatts&rdquo; of AI infrastructure during this decade &mdash; Meta CEO Mark Zuckerberg on Monday said his company is establishing a new &ldquo;top-level&rdquo; &hellip;",
          "feed_position": 8,
          "image_url": "http://www.techmeme.com/260112/i28.jpg"
        }
      ],
      "featured_image": "http://www.techmeme.com/260112/i28.jpg",
      "popularity_score": 2018.450588611111
    },
    {
      "id": "cluster_23",
      "coverage": 2,
      "updated_at": "Mon, 12 Jan 2026 20:25:47 +0000",
      "title": "Dell revives its XPS laptops after a boneheaded rebranding",
      "neutral_headline": "Dell revives its XPS laptops after a boneheaded rebranding",
      "items": [
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/computing/laptops/dell-revives-its-xps-laptops-after-a-boneheaded-rebranding-001028029.html",
          "published_at": "Mon, 12 Jan 2026 20:25:47 +0000",
          "title": "Dell revives its XPS laptops after a boneheaded rebranding",
          "standfirst": "Last year, Dell killed off all of its PC brands, including the iconic XPS lineup, and replaced them with a simplified naming scheme. It was a move meant to make it easier for people to discern between the company's many brands, but in reality, it just just made the company's lineup even more confusing. We called it an unforced error at the time, but after seeing how much Dell's PC market share fell over 2025, it's fair to say that rebranding was an absolute marketing disaster. So, with its tail between its legs, Dell has returned to CES some welcome news for its fans: XPS lives! And the company plans to double-down on the brand in ways it never did before. Today, Dell revealed the new XPS 14 and 16 notebooks, which feature a more practical design than the previous models. There's a new function row with traditional keys, instead of the odd capacitive buttons that disappeared in sunlight. And while the company is sticking with its \"invisible\" trackpad, which sits flush alongside the wrist rest, there's now a light border around the edges that lets you feel exactly where the trackpad begins and ends.So, in short, Dell seems to have solved most of our recent complaints about the XPS lineup. To signify its commitment to the brand, it's also emblazoning the XPS logo on all of these new machines, replacing the previous Dell name. That’s something I could never imagine a less humbled Dell doing. The redesign also gave Dell room to shave off some weight and thickness from both machines. The XPS 14 weighs around three pounds now, a half-pound lighter than the previous generation, while the XPS 16 weighs 3.6 pounds, a whole pound lighter than before. The new cases make both machines look a lot more like Microsoft’s extra-subtle Surface Laptop, but that’s not necessarily a bad thing. Both systems are powered by Intel’s new Panther Lake Core Ultra Series 3 chips, and they also offer tandem OLED display options.Dell also briefly teased the return of a new XPS 13 later this year, which is set to be the company’s thinnest and lightest notebook ever. Dell says it’ll be cheaper than the XPS has been in the past.The new XPS 14 and 16 will be available on January 6, starting at $2,050 and $2,200, respectively. A Dell representative tells us these aren’t entry-level configurations, instead we can expect to see cheaper prices with lower specs in February.Update 1/6/26, 12:30p: Pricing updated to reflecrt new numbers from Dell. Originally, we were told they would start at $1,650 and $1,850.Update 1/12, 3:00p: Added a mention of lower entry-level configurations coming eventually.This article originally appeared on Engadget at https://www.engadget.com/computing/laptops/dell-revives-its-xps-laptops-after-a-boneheaded-rebranding-001028029.html?src=rss",
          "content": "Last year, Dell killed off all of its PC brands, including the iconic XPS lineup, and replaced them with a simplified naming scheme. It was a move meant to make it easier for people to discern between the company's many brands, but in reality, it just just made the company's lineup even more confusing. We called it an unforced error at the time, but after seeing how much Dell's PC market share fell over 2025, it's fair to say that rebranding was an absolute marketing disaster. So, with its tail between its legs, Dell has returned to CES some welcome news for its fans: XPS lives! And the company plans to double-down on the brand in ways it never did before. Today, Dell revealed the new XPS 14 and 16 notebooks, which feature a more practical design than the previous models. There's a new function row with traditional keys, instead of the odd capacitive buttons that disappeared in sunlight. And while the company is sticking with its \"invisible\" trackpad, which sits flush alongside the wrist rest, there's now a light border around the edges that lets you feel exactly where the trackpad begins and ends.So, in short, Dell seems to have solved most of our recent complaints about the XPS lineup. To signify its commitment to the brand, it's also emblazoning the XPS logo on all of these new machines, replacing the previous Dell name. That’s something I could never imagine a less humbled Dell doing. The redesign also gave Dell room to shave off some weight and thickness from both machines. The XPS 14 weighs around three pounds now, a half-pound lighter than the previous generation, while the XPS 16 weighs 3.6 pounds, a whole pound lighter than before. The new cases make both machines look a lot more like Microsoft’s extra-subtle Surface Laptop, but that’s not necessarily a bad thing. Both systems are powered by Intel’s new Panther Lake Core Ultra Series 3 chips, and they also offer tandem OLED display options.Dell also briefly teased the return of a new XPS 13 later this year, which is set to be the company’s thinnest and lightest notebook ever. Dell says it’ll be cheaper than the XPS has been in the past.The new XPS 14 and 16 will be available on January 6, starting at $2,050 and $2,200, respectively. A Dell representative tells us these aren’t entry-level configurations, instead we can expect to see cheaper prices with lower specs in February.Update 1/6/26, 12:30p: Pricing updated to reflecrt new numbers from Dell. Originally, we were told they would start at $1,650 and $1,850.Update 1/12, 3:00p: Added a mention of lower entry-level configurations coming eventually.This article originally appeared on Engadget at https://www.engadget.com/computing/laptops/dell-revives-its-xps-laptops-after-a-boneheaded-rebranding-001028029.html?src=rss",
          "feed_position": 2
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/computing/ces-2026-proved-the-pc-industry-is-hosed-this-year-174500314.html",
          "published_at": "Mon, 12 Jan 2026 20:25:38 +0000",
          "title": "CES 2026 proved the PC industry is hosed this year",
          "standfirst": "Dell's XPS 14 currently costs over $2,000. An AMD executive predicts that PC builders will likely make piecemeal upgrades this year, instead of building entirely new systems. And new AI supercomputers from NVIDIA and AMD are gobbling up the RAM market. At CES 2026, it was hard not to notice the dire year ahead for the computing industry, one that will likely lead to higher prices and more limited availability for consumer goods across the board.Really, though, the show just confirmed what was apparent since RAM prices skyrocketed over the last few months, driven by demand from AI datacenters. As Samsung's marketing leader, Wonjin Lee, told Bloomberg at CES: \"There's going to be issues around semiconductor supplies, and it's going to affect everyone. Prices are going up even as we speak.\"At first, it appeared that Dell's new XPS 14 and XPS 16 were among the earliest systems hit by these demands. Last year's models started at $1,699 and $1,899, respectively, and we were initially told the new models would actually come in cheaper at $1,650 and $1,850. At the moment, the XPS 14 starts at $2,050, while the XPS 16 is $2,200. A Dell representative tells us these aren’t entry-level configurations, instead we can expect to see cheaper systems below $2,000 in February. While those prices haven’t been finalized, the reps say it should be similar to the earlier figures we were given.It’s also worth noting that it didn't take much to configure the earlier models upwards of $2,000. It’s just unfortunate that Dell doesn’t have cheaper configurations available for the launch if its new systems, especially since they look so compelling. Meanwhile, Apple still hasn't budged its $1,599 MacBook Pro 14-inch pricing. At least Dell still comes in cheaper than the $2,499 MacBook Pro 16-inch.On the desktop front, AMD's David McAfee, Corporate Vice President and GM of Client Channel Business, noted that the longevity of the company's AM4 and AM5 platforms might be a boon for gamers, since they can upgrade their CPUs without buying new RAM kits and motherboards. That allows for a pathway to better performance without paying out the nose for over-priced RAM.\"I think that will be potentially a trend that we see in 2026 with more component upgrades, as opposed to full system swap outs and, and altogether rebuilds,\" he said in a group interview with Engadget and other outlets. \"Some of the most popular CPUs that are still running in gamers’ platforms are parts like the 2600 back to the Pinnacle Ridge days, or 3000 series... Stepping even from there into a little bit more modern 5,000 series processors in an AM4 socket and motherboard, there's a pretty big boost there.\"McAfee added that around 30 to 40 percent of AMD's business still revolves around the AM4 platform, even without the specter of a wild memory market.\"There's no product that has memory in it that's immune to some of these forces around DRAM pricing and, and what it's doing to the market,\" he said, when asked about potential GPU price increases. \"I think the, the truth is the volatility that we've seen over the past two months or so has really been unprecedented.\" Looking ahead, he said he expects prices to settle within the first three to six months of the year, but he didn't discuss his reasoning further. As an aside, he also noted that AMD's X3D chips, which feature 3D V-cache, actually don't see much of a hit from slower RAM. Their high amounts of onboard L2 and L3 cache make up for less ideal memory transfer speeds, McAfee said.That McAfee commented at all about the state of RAM is noteworthy. Every PC maker I’ve asked, including Dell and Acer, refused to comment on the volatile state of the memory industry ahead of CES. Perhaps they were hoping things would calm down before they had to price their new systems. Ultimately, they’re beholden to an increasingly limited supply of RAM. And where is all that memory going? At CES, NVIDIA announced its new Vera Rubin AI supercomputer, which supports up to 54TB of RAM across 36 Vera CPUs and 20.7TB of memory across 72 GPUs. AMD, as well, announced its new Helios AI rack, which supports up to 31TB of memory across 72 AMD Instinct MI455X GPUs. Given the endless appetite for computing to power AI model building and inferencing, there’s likely going to be a significant demand for these beastly systems.Put simply: Our global supply of memory is being sacrificed to appease the AI industry. That’s good news for the likes of OpenAI, Microsoft and NVIDIA, but bad news for anyone who cares about PCs and the consumer products we use every day. Get ready for a year of price hikes. Update 1/12, 3:00p: Added a mention of lower entry-level configurations coming eventually.This article originally appeared on Engadget at https://www.engadget.com/computing/ces-2026-proved-the-pc-industry-is-hosed-this-year-174500314.html?src=rss",
          "content": "Dell's XPS 14 currently costs over $2,000. An AMD executive predicts that PC builders will likely make piecemeal upgrades this year, instead of building entirely new systems. And new AI supercomputers from NVIDIA and AMD are gobbling up the RAM market. At CES 2026, it was hard not to notice the dire year ahead for the computing industry, one that will likely lead to higher prices and more limited availability for consumer goods across the board.Really, though, the show just confirmed what was apparent since RAM prices skyrocketed over the last few months, driven by demand from AI datacenters. As Samsung's marketing leader, Wonjin Lee, told Bloomberg at CES: \"There's going to be issues around semiconductor supplies, and it's going to affect everyone. Prices are going up even as we speak.\"At first, it appeared that Dell's new XPS 14 and XPS 16 were among the earliest systems hit by these demands. Last year's models started at $1,699 and $1,899, respectively, and we were initially told the new models would actually come in cheaper at $1,650 and $1,850. At the moment, the XPS 14 starts at $2,050, while the XPS 16 is $2,200. A Dell representative tells us these aren’t entry-level configurations, instead we can expect to see cheaper systems below $2,000 in February. While those prices haven’t been finalized, the reps say it should be similar to the earlier figures we were given.It’s also worth noting that it didn't take much to configure the earlier models upwards of $2,000. It’s just unfortunate that Dell doesn’t have cheaper configurations available for the launch if its new systems, especially since they look so compelling. Meanwhile, Apple still hasn't budged its $1,599 MacBook Pro 14-inch pricing. At least Dell still comes in cheaper than the $2,499 MacBook Pro 16-inch.On the desktop front, AMD's David McAfee, Corporate Vice President and GM of Client Channel Business, noted that the longevity of the company's AM4 and AM5 platforms might be a boon for gamers, since they can upgrade their CPUs without buying new RAM kits and motherboards. That allows for a pathway to better performance without paying out the nose for over-priced RAM.\"I think that will be potentially a trend that we see in 2026 with more component upgrades, as opposed to full system swap outs and, and altogether rebuilds,\" he said in a group interview with Engadget and other outlets. \"Some of the most popular CPUs that are still running in gamers’ platforms are parts like the 2600 back to the Pinnacle Ridge days, or 3000 series... Stepping even from there into a little bit more modern 5,000 series processors in an AM4 socket and motherboard, there's a pretty big boost there.\"McAfee added that around 30 to 40 percent of AMD's business still revolves around the AM4 platform, even without the specter of a wild memory market.\"There's no product that has memory in it that's immune to some of these forces around DRAM pricing and, and what it's doing to the market,\" he said, when asked about potential GPU price increases. \"I think the, the truth is the volatility that we've seen over the past two months or so has really been unprecedented.\" Looking ahead, he said he expects prices to settle within the first three to six months of the year, but he didn't discuss his reasoning further. As an aside, he also noted that AMD's X3D chips, which feature 3D V-cache, actually don't see much of a hit from slower RAM. Their high amounts of onboard L2 and L3 cache make up for less ideal memory transfer speeds, McAfee said.That McAfee commented at all about the state of RAM is noteworthy. Every PC maker I’ve asked, including Dell and Acer, refused to comment on the volatile state of the memory industry ahead of CES. Perhaps they were hoping things would calm down before they had to price their new systems. Ultimately, they’re beholden to an increasingly limited supply of RAM. And where is all that memory going? At CES, NVIDIA announced its new Vera Rubin AI supercomputer, which supports up to 54TB of RAM across 36 Vera CPUs and 20.7TB of memory across 72 GPUs. AMD, as well, announced its new Helios AI rack, which supports up to 31TB of memory across 72 AMD Instinct MI455X GPUs. Given the endless appetite for computing to power AI model building and inferencing, there’s likely going to be a significant demand for these beastly systems.Put simply: Our global supply of memory is being sacrificed to appease the AI industry. That’s good news for the likes of OpenAI, Microsoft and NVIDIA, but bad news for anyone who cares about PCs and the consumer products we use every day. Get ready for a year of price hikes. Update 1/12, 3:00p: Added a mention of lower entry-level configurations coming eventually.This article originally appeared on Engadget at https://www.engadget.com/computing/ces-2026-proved-the-pc-industry-is-hosed-this-year-174500314.html?src=rss",
          "feed_position": 3
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/orchestration/why-your-llm-bill-is-exploding-and-how-semantic-caching-can-cut-it-by-73",
          "published_at": "Mon, 12 Jan 2026 19:00:00 GMT",
          "title": "Why your LLM bill is exploding — and how semantic caching can cut it by 73%",
          "standfirst": "Our LLM API bill was growing 30% month-over-month. Traffic was increasing, but not that fast. When I analyzed our query logs, I found the real problem: Users ask the same questions in different ways.\"What&#x27;s your return policy?,\" \"How do I return something?\", and \"Can I get a refund?\" were all hitting our LLM separately, generating nearly identical responses, each incurring full API costs.Exact-match caching, the obvious first solution, captured only 18% of these redundant calls. The same semantic question, phrased differently, bypassed the cache entirely.So, I implemented semantic caching based on what queries mean, not how they&#x27;re worded. After implementing it, our cache hit rate increased to 67%, reducing LLM API costs by 73%. But getting there requires solving problems that naive implementations miss.Why exact-match caching falls shortTraditional caching uses query text as the cache key. This works when queries are identical:# Exact-match cachingcache_key = hash(query_text)if cache_key in cache: return cache[cache_key]But users don&#x27;t phrase questions identically. My analysis of 100,000 production queries found:Only 18% were exact duplicates of previous queries47% were semantically similar to previous queries (same intent, different wording)35% were genuinely novel queriesThat 47% represented massive cost savings we were missing. Each semantically-similar query triggered a full LLM call, generating a response nearly identical to one we&#x27;d already computed.Semantic caching architectureSemantic caching replaces text-based keys with embedding-based similarity lookup:class SemanticCache: def __init__(self, embedding_model, similarity_threshold=0.92): self.embedding_model = embedding_model self.threshold = similarity_threshold self.vector_store = VectorStore() # FAISS, Pinecone, etc. self.response_store = ResponseStore() # Redis, DynamoDB, etc. def get(self, query: str) -> Optional[str]: \"\"\"Return cached response if semantically similar query exists.\"\"\" query_embedding = self.embedding_model.encode(query) # Find most similar cached query matches = self.vector_store.search(query_embedding, top_k=1) if matches and matches[0].similarity >= self.threshold: cache_id = matches[0].id return self.response_store.get(cache_id) return None def set(self, query: str, response: str): \"\"\"Cache query-response pair.\"\"\" query_embedding = self.embedding_model.encode(query) cache_id = generate_id() self.vector_store.add(cache_id, query_embedding) self.response_store.set(cache_id, { &#x27;query&#x27;: query, &#x27;response&#x27;: response, &#x27;timestamp&#x27;: datetime.utcnow() })The key insight: Instead of hashing query text, I embed queries into vector space and find cached queries within a similarity threshold.The threshold problemThe similarity threshold is the critical parameter. Set it too high, and you miss valid cache hits. Set it too low, and you return wrong responses.Our initial threshold of 0.85 seemed reasonable; 85% similar should be \"the same question,\" right?Wrong. At 0.85, we got cache hits like:Query: \"How do I cancel my subscription?\"Cached: \"How do I cancel my order?\"Similarity: 0.87These are different questions with different answers. Returning the cached response would be incorrect.I discovered that optimal thresholds vary by query type:Query typeOptimal thresholdRationaleFAQ-style questions0.94High precision needed; wrong answers damage trustProduct searches0.88More tolerance for near-matchesSupport queries0.92Balance between coverage and accuracyTransactional queries0.97Very low tolerance for errorsI implemented query-type-specific thresholds:class AdaptiveSemanticCache: def __init__(self): self.thresholds = { &#x27;faq&#x27;: 0.94, &#x27;search&#x27;: 0.88, &#x27;support&#x27;: 0.92, &#x27;transactional&#x27;: 0.97, &#x27;default&#x27;: 0.92 } self.query_classifier = QueryClassifier() def get_threshold(self, query: str) -> float: query_type = self.query_classifier.classify(query) return self.thresholds.get(query_type, self.thresholds[&#x27;default&#x27;]) def get(self, query: str) -> Optional[str]: threshold = self.get_threshold(query) query_embedding = self.embedding_model.encode(query) matches = self.vector_store.search(query_embedding, top_k=1) if matches and matches[0].similarity >= threshold: return self.response_store.get(matches[0].id) return NoneThreshold tuning methodologyI couldn&#x27;t tune thresholds blindly. I needed ground truth on which query pairs were actually \"the same.\"Our methodology:Step 1: Sample query pairs. I sampled 5,000 query pairs at various similarity levels (0.80-0.99).Step 2: Human labeling. Annotators labeled each pair as \"same intent\" or \"different intent.\" I used three annotators per pair and took a majority vote.Step 3: Compute precision/recall curves. For each threshold, we computed:Precision: Of cache hits, what fraction had the same intent?Recall: Of same-intent pairs, what fraction did we cache-hit?def compute_precision_recall(pairs, labels, threshold): \"\"\"Compute precision and recall at given similarity threshold.\"\"\" predictions = [1 if pair.similarity >= threshold else 0 for pair in pairs] true_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 1) false_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 0) false_negatives = sum(1 for p, l in zip(predictions, labels) if p == 0 and l == 1) precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0 recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0 return precision, recallStep 4: Select threshold based on cost of errors. For FAQ queries where wrong answers damage trust, I optimized for precision (0.94 threshold gave 98% precision). For search queries where missing a cache hit just costs money, I optimized for recall (0.88 threshold).Latency overheadSemantic caching adds latency: You must embed the query and search the vector store before knowing whether to call the LLM.Our measurements:OperationLatency (p50)Latency (p99)Query embedding12ms28msVector search8ms19msTotal cache lookup20ms47msLLM API call850ms2400msThe 20ms overhead is negligible compared to the 850ms LLM call we avoid on cache hits. Even at p99, the 47ms overhead is acceptable.However, cache misses now take 20ms longer than before (embedding + search + LLM call). At our 67% hit rate, the math works out favorably:Before: 100% of queries × 850ms = 850ms averageAfter: (33% × 870ms) + (67% × 20ms) = 287ms + 13ms = 300ms averageNet latency improvement of 65% alongside the cost reduction.Cache invalidationCached responses go stale. Product information changes, policies update and yesterday&#x27;s correct answer becomes today&#x27;s wrong answer.I implemented three invalidation strategies:Time-based TTLSimple expiration based on content type:TTL_BY_CONTENT_TYPE = { &#x27;pricing&#x27;: timedelta(hours=4), # Changes frequently &#x27;policy&#x27;: timedelta(days=7), # Changes rarely &#x27;product_info&#x27;: timedelta(days=1), # Daily refresh &#x27;general_faq&#x27;: timedelta(days=14), # Very stable}Event-based invalidationWhen underlying data changes, invalidate related cache entries:class CacheInvalidator: def on_content_update(self, content_id: str, content_type: str): \"\"\"Invalidate cache entries related to updated content.\"\"\" # Find cached queries that referenced this content affected_queries = self.find_queries_referencing(content_id) for query_id in affected_queries: self.cache.invalidate(query_id) self.log_invalidation(content_id, len(affected_queries))Staleness detectionFor responses that might become stale without explicit events, I implemented periodic freshness checks:def check_freshness(self, cached_response: dict) -> bool: \"\"\"Verify cached response is still valid.\"\"\" # Re-run the query against current data fresh_response = self.generate_response(cached_response[&#x27;query&#x27;]) # Compare semantic similarity of responses cached_embedding = self.embed(cached_response[&#x27;response&#x27;]) fresh_embedding = self.embed(fresh_response) similarity = cosine_similarity(cached_embedding, fresh_embedding) # If responses diverged significantly, invalidate if similarity < 0.90: self.cache.invalidate(cached_response[&#x27;id&#x27;]) return False return TrueWe run freshness checks on a sample of cached entries daily, catching staleness that TTL and event-based invalidation miss.Production resultsAfter three months in production:MetricBeforeAfterChangeCache hit rate18%67%+272%LLM API costs$47K/month$12.7K/month-73%Average latency850ms300ms-65%False-positive rateN/A0.8%—Customer complaints (wrong answers)Baseline+0.3%Minimal increaseThe 0.8% false-positive rate (queries where we returned a cached response that was semantically incorrect) was within acceptable bounds. These cases occurred primarily at the boundaries of our threshold, where similarity was just above the cutoff but intent differed slightly.Pitfalls to avoidDon&#x27;t use a single global threshold. Different query types have different tolerance for errors. Tune thresholds per category.Don&#x27;t skip the embedding step on cache hits. You might be tempted to skip embedding overhead when returning cached responses, but you need the embedding for cache key generation. The overhead is unavoidable.Don&#x27;t forget invalidation. Semantic caching without invalidation strategy leads to stale responses that erode user trust. Build invalidation from day one.Don&#x27;t cache everything. Some queries shouldn&#x27;t be cached: Personalized responses, time-sensitive information, transactional confirmations. Build exclusion rules.def should_cache(self, query: str, response: str) -> bool: \"\"\"Determine if response should be cached.\"\" # Don&#x27;t cache personalized responses if self.contains_personal_info(response): return False # Don&#x27;t cache time-sensitive information if self.is_time_sensitive(query): return False # Don&#x27;t cache transactional confirmations if self.is_transactional(query): return False return TrueKey takeawaysSemantic caching is a practical pattern for LLM cost control that captures redundancy exact-match caching misses. The key challenges are threshold tuning (use query-type-specific thresholds based on precision/recall analysis) and cache invalidation (combine TTL, event-based and staleness detection).At 73% cost reduction, this was our highest-ROI optimization for production LLM systems. The implementation complexity is moderate, but the threshold tuning requires careful attention to avoid quality degradation.Sreenivasa Reddy Hulebeedu Reddy is a lead software engineer.",
          "content": "Our LLM API bill was growing 30% month-over-month. Traffic was increasing, but not that fast. When I analyzed our query logs, I found the real problem: Users ask the same questions in different ways.\"What&#x27;s your return policy?,\" \"How do I return something?\", and \"Can I get a refund?\" were all hitting our LLM separately, generating nearly identical responses, each incurring full API costs.Exact-match caching, the obvious first solution, captured only 18% of these redundant calls. The same semantic question, phrased differently, bypassed the cache entirely.So, I implemented semantic caching based on what queries mean, not how they&#x27;re worded. After implementing it, our cache hit rate increased to 67%, reducing LLM API costs by 73%. But getting there requires solving problems that naive implementations miss.Why exact-match caching falls shortTraditional caching uses query text as the cache key. This works when queries are identical:# Exact-match cachingcache_key = hash(query_text)if cache_key in cache: return cache[cache_key]But users don&#x27;t phrase questions identically. My analysis of 100,000 production queries found:Only 18% were exact duplicates of previous queries47% were semantically similar to previous queries (same intent, different wording)35% were genuinely novel queriesThat 47% represented massive cost savings we were missing. Each semantically-similar query triggered a full LLM call, generating a response nearly identical to one we&#x27;d already computed.Semantic caching architectureSemantic caching replaces text-based keys with embedding-based similarity lookup:class SemanticCache: def __init__(self, embedding_model, similarity_threshold=0.92): self.embedding_model = embedding_model self.threshold = similarity_threshold self.vector_store = VectorStore() # FAISS, Pinecone, etc. self.response_store = ResponseStore() # Redis, DynamoDB, etc. def get(self, query: str) -> Optional[str]: \"\"\"Return cached response if semantically similar query exists.\"\"\" query_embedding = self.embedding_model.encode(query) # Find most similar cached query matches = self.vector_store.search(query_embedding, top_k=1) if matches and matches[0].similarity >= self.threshold: cache_id = matches[0].id return self.response_store.get(cache_id) return None def set(self, query: str, response: str): \"\"\"Cache query-response pair.\"\"\" query_embedding = self.embedding_model.encode(query) cache_id = generate_id() self.vector_store.add(cache_id, query_embedding) self.response_store.set(cache_id, { &#x27;query&#x27;: query, &#x27;response&#x27;: response, &#x27;timestamp&#x27;: datetime.utcnow() })The key insight: Instead of hashing query text, I embed queries into vector space and find cached queries within a similarity threshold.The threshold problemThe similarity threshold is the critical parameter. Set it too high, and you miss valid cache hits. Set it too low, and you return wrong responses.Our initial threshold of 0.85 seemed reasonable; 85% similar should be \"the same question,\" right?Wrong. At 0.85, we got cache hits like:Query: \"How do I cancel my subscription?\"Cached: \"How do I cancel my order?\"Similarity: 0.87These are different questions with different answers. Returning the cached response would be incorrect.I discovered that optimal thresholds vary by query type:Query typeOptimal thresholdRationaleFAQ-style questions0.94High precision needed; wrong answers damage trustProduct searches0.88More tolerance for near-matchesSupport queries0.92Balance between coverage and accuracyTransactional queries0.97Very low tolerance for errorsI implemented query-type-specific thresholds:class AdaptiveSemanticCache: def __init__(self): self.thresholds = { &#x27;faq&#x27;: 0.94, &#x27;search&#x27;: 0.88, &#x27;support&#x27;: 0.92, &#x27;transactional&#x27;: 0.97, &#x27;default&#x27;: 0.92 } self.query_classifier = QueryClassifier() def get_threshold(self, query: str) -> float: query_type = self.query_classifier.classify(query) return self.thresholds.get(query_type, self.thresholds[&#x27;default&#x27;]) def get(self, query: str) -> Optional[str]: threshold = self.get_threshold(query) query_embedding = self.embedding_model.encode(query) matches = self.vector_store.search(query_embedding, top_k=1) if matches and matches[0].similarity >= threshold: return self.response_store.get(matches[0].id) return NoneThreshold tuning methodologyI couldn&#x27;t tune thresholds blindly. I needed ground truth on which query pairs were actually \"the same.\"Our methodology:Step 1: Sample query pairs. I sampled 5,000 query pairs at various similarity levels (0.80-0.99).Step 2: Human labeling. Annotators labeled each pair as \"same intent\" or \"different intent.\" I used three annotators per pair and took a majority vote.Step 3: Compute precision/recall curves. For each threshold, we computed:Precision: Of cache hits, what fraction had the same intent?Recall: Of same-intent pairs, what fraction did we cache-hit?def compute_precision_recall(pairs, labels, threshold): \"\"\"Compute precision and recall at given similarity threshold.\"\"\" predictions = [1 if pair.similarity >= threshold else 0 for pair in pairs] true_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 1) false_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 0) false_negatives = sum(1 for p, l in zip(predictions, labels) if p == 0 and l == 1) precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0 recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0 return precision, recallStep 4: Select threshold based on cost of errors. For FAQ queries where wrong answers damage trust, I optimized for precision (0.94 threshold gave 98% precision). For search queries where missing a cache hit just costs money, I optimized for recall (0.88 threshold).Latency overheadSemantic caching adds latency: You must embed the query and search the vector store before knowing whether to call the LLM.Our measurements:OperationLatency (p50)Latency (p99)Query embedding12ms28msVector search8ms19msTotal cache lookup20ms47msLLM API call850ms2400msThe 20ms overhead is negligible compared to the 850ms LLM call we avoid on cache hits. Even at p99, the 47ms overhead is acceptable.However, cache misses now take 20ms longer than before (embedding + search + LLM call). At our 67% hit rate, the math works out favorably:Before: 100% of queries × 850ms = 850ms averageAfter: (33% × 870ms) + (67% × 20ms) = 287ms + 13ms = 300ms averageNet latency improvement of 65% alongside the cost reduction.Cache invalidationCached responses go stale. Product information changes, policies update and yesterday&#x27;s correct answer becomes today&#x27;s wrong answer.I implemented three invalidation strategies:Time-based TTLSimple expiration based on content type:TTL_BY_CONTENT_TYPE = { &#x27;pricing&#x27;: timedelta(hours=4), # Changes frequently &#x27;policy&#x27;: timedelta(days=7), # Changes rarely &#x27;product_info&#x27;: timedelta(days=1), # Daily refresh &#x27;general_faq&#x27;: timedelta(days=14), # Very stable}Event-based invalidationWhen underlying data changes, invalidate related cache entries:class CacheInvalidator: def on_content_update(self, content_id: str, content_type: str): \"\"\"Invalidate cache entries related to updated content.\"\"\" # Find cached queries that referenced this content affected_queries = self.find_queries_referencing(content_id) for query_id in affected_queries: self.cache.invalidate(query_id) self.log_invalidation(content_id, len(affected_queries))Staleness detectionFor responses that might become stale without explicit events, I implemented periodic freshness checks:def check_freshness(self, cached_response: dict) -> bool: \"\"\"Verify cached response is still valid.\"\"\" # Re-run the query against current data fresh_response = self.generate_response(cached_response[&#x27;query&#x27;]) # Compare semantic similarity of responses cached_embedding = self.embed(cached_response[&#x27;response&#x27;]) fresh_embedding = self.embed(fresh_response) similarity = cosine_similarity(cached_embedding, fresh_embedding) # If responses diverged significantly, invalidate if similarity < 0.90: self.cache.invalidate(cached_response[&#x27;id&#x27;]) return False return TrueWe run freshness checks on a sample of cached entries daily, catching staleness that TTL and event-based invalidation miss.Production resultsAfter three months in production:MetricBeforeAfterChangeCache hit rate18%67%+272%LLM API costs$47K/month$12.7K/month-73%Average latency850ms300ms-65%False-positive rateN/A0.8%—Customer complaints (wrong answers)Baseline+0.3%Minimal increaseThe 0.8% false-positive rate (queries where we returned a cached response that was semantically incorrect) was within acceptable bounds. These cases occurred primarily at the boundaries of our threshold, where similarity was just above the cutoff but intent differed slightly.Pitfalls to avoidDon&#x27;t use a single global threshold. Different query types have different tolerance for errors. Tune thresholds per category.Don&#x27;t skip the embedding step on cache hits. You might be tempted to skip embedding overhead when returning cached responses, but you need the embedding for cache key generation. The overhead is unavoidable.Don&#x27;t forget invalidation. Semantic caching without invalidation strategy leads to stale responses that erode user trust. Build invalidation from day one.Don&#x27;t cache everything. Some queries shouldn&#x27;t be cached: Personalized responses, time-sensitive information, transactional confirmations. Build exclusion rules.def should_cache(self, query: str, response: str) -> bool: \"\"\"Determine if response should be cached.\"\" # Don&#x27;t cache personalized responses if self.contains_personal_info(response): return False # Don&#x27;t cache time-sensitive information if self.is_time_sensitive(query): return False # Don&#x27;t cache transactional confirmations if self.is_transactional(query): return False return TrueKey takeawaysSemantic caching is a practical pattern for LLM cost control that captures redundancy exact-match caching misses. The key challenges are threshold tuning (use query-type-specific thresholds based on precision/recall analysis) and cache invalidation (combine TTL, event-based and staleness detection).At 73% cost reduction, this was our highest-ROI optimization for production LLM systems. The implementation complexity is moderate, but the threshold tuning requires careful attention to avoid quality degradation.Sreenivasa Reddy Hulebeedu Reddy is a lead software engineer.",
          "feed_position": 0,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/7iyQoeSwdOqqpfcE0PFWgF/48db7d0305019eee107028d9f018d2ac/Semantic_caching.png?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/security/nvidia-rubin-rack-scale-encryption-enterprise-ai-security",
          "published_at": "Mon, 12 Jan 2026 16:00:00 GMT",
          "title": "Nvidia Rubin's rack-scale encryption signals a turning point for enterprise AI security",
          "standfirst": "Nvidia&#x27;s Vera Rubin NVL72, announced at CES 2026, encrypts every bus across 72 GPUs, 36 CPUs, and the entire NVLink fabric. It&#x27;s the first rack-scale platform to deliver confidential computing across CPU, GPU, and NVLink domains.For security leaders, this fundamentally shifts the conversation. Rather than attempting to secure complex hybrid cloud configurations through contractual trust with cloud providers, they can verify them cryptographically. That’s a critical distinction that matters when nation-state adversaries have proven they are capable of launching targeted cyberattacks at machine speed.The brutal economics of unprotected AIEpoch AI research shows frontier training costs have grown at 2.4x annually since 2016, which means billion-dollar training runs could be a reality within a few short years. Yet the infrastructure protecting these investments remains fundamentally insecure in most deployments. Security budgets created to protect frontier training models aren&#x27;t keeping up with the exceptionally fast pace of model training. The result is that more models are under threat as existing approaches can&#x27;t scale and keep up with adversaries&#x27; tradecraft.IBM&#x27;s 2025 Cost of Data Breach Report found that 13% of organizations experienced breaches of AI models or applications. Among those breached, 97% lacked proper AI access controls. Shadow AI incidents cost $4.63 million on average, or $670,000 more than standard breaches, with one in five breaches now involving unsanctioned tools that disproportionately expose customer PII (65%) and intellectual property (40%).Think about what this means for organizations spending $50 million or $500 million on a training run. Their model weights sit in multi-tenant environments where cloud providers can inspect the data. Hardware-level encryption that proves the environment hasn&#x27;t been tampered with changes that financial equation entirely.The GTG-1002 wake-up callIn November 2025, Anthropic disclosed something unprecedented: A Chinese state-sponsored group designated GTG-1002 had manipulated Claude Code to conduct what the company described as the first documented case of a large-scale cyberattack executed without substantial human intervention.State-sponsored adversaries turned it into an autonomous intrusion agent that discovered vulnerabilities, crafted exploits, harvested credentials, moved laterally through networks, and categorized stolen data by intelligence value. Human operators stepped in only at critical junctures. According to Anthropic&#x27;s analysis, the AI executed around 80 to 90% of all tactical work independently.The implications extend beyond this single incident. Attack surfaces that once required teams of experienced attackers can now be probed at machine speed by opponents with access to foundation models.Comparing the performance of Blackwell vs. RubinSpecificationBlackwell GB300 NVL72Rubin NVL72Inference compute (FP4)1.44 exaFLOPS3.6 exaFLOPSNVFP4 per GPU (inference)20 PFLOPS50 PFLOPSPer-GPU NVLink bandwidth1.8 TB/s3.6 TB/sRack NVLink bandwidth130 TB/s260 TB/sHBM bandwidth per GPU~8 TB/s~22 TB/sIndustry momentum and AMD&#x27;s alternativeNvidia isn&#x27;t operating in isolation. Research from the Confidential Computing Consortium and IDC, released in December, found that 75% of organizations are adopting confidential computing, with 18% already in production and 57% piloting deployments.\"Confidential Computing has grown from a niche concept into a vital strategy for data security and trusted AI innovation,\" said Nelly Porter, governing board chair of the Confidential Computing Consortium. Real barriers remain: attestation validation challenges affect 84% of respondents, and a skills gap hampers 75%.AMD&#x27;s Helios rack takes a different approach. Built on Meta&#x27;s Open Rack Wide specification, announced at OCP Global Summit in October 2025, it delivers approximately 2.9 exaflops of FP4 compute with 31 TB of HBM4 memory and 1.4 PB/s aggregate bandwidth. Where Nvidia designs confidential computing into every component, AMD prioritizes open standards through the Ultra Accelerator Link and Ultra Ethernet consortia. The competition between Nvidia and AMD is giving security leaders more of a choice than they otherwise would have had. Comparing the tradeoffs of Nvidia&#x27;s integrated approach versus AMD&#x27;s open-standards flexibility for their specific infrastructures and business-specific threat models is key.What security leaders are doing nowHardware-level confidentiality doesn&#x27;t replace zero-trust principles; it gives them teeth. What Nvidia and AMD are building lets security leaders verify trust cryptographically rather than assume it contractually. That&#x27;s a meaningful shift for anyone running sensitive workloads on shared infrastructure. And if the attestation claims hold up in production, this approach could let enterprises extend zero-trust enforcement across thousands of nodes without the policy sprawl and agent overhead that software-only implementations require.Before deployment: Verify attestation to confirm environments haven&#x27;t been tampered with. Cryptographic proof of compliance should be a prerequisite for signing contracts, not an afterthought or worse, a nice-to-have. If your cloud provider can&#x27;t demonstrate attestation capabilities, that&#x27;s a question worth raising in your next QBR.During operation: Maintain separate enclaves for training and inference, and include security teams in the model pipeline from the very start. IBM&#x27;s research showed 63% of breached organizations had no AI governance policy. You can&#x27;t bolt security on after development; that translates into an onramp for mediocre security design-ins and lengthy red teaming that catches bugs that needed to be engineered out of a model or app early.Across the organization: Run joint exercises between security and data science teams to surface vulnerabilities before attackers find them. Shadow AI accounted for 20% of breaches and exposed customer PII and IP at higher rates than other breach types.Bottom line The GTG-1002 campaign demonstrated that adversaries can now automate large-scale intrusions with minimal human oversight at scale. Nearly every organization that experienced an AI-related breach lacked proper access controls.Nvidia&#x27;s Vera Rubin NVL72 transforms racks from potential liabilities into cryptographically attested assets by encrypting every bus. AMD&#x27;s Helios offers an open-standards alternative. Hardware confidentiality alone won&#x27;t stop a determined adversary, but combined with strong governance and realistic threat exercises, rack-scale encryption gives security leaders the foundation they need to protect investments measured in hundreds of millions of dollars.The question facing CISOs isn&#x27;t whether attested infrastructure is worth it. It&#x27;s whether organizations building high-value AI models can afford to operate without it.",
          "content": "Nvidia&#x27;s Vera Rubin NVL72, announced at CES 2026, encrypts every bus across 72 GPUs, 36 CPUs, and the entire NVLink fabric. It&#x27;s the first rack-scale platform to deliver confidential computing across CPU, GPU, and NVLink domains.For security leaders, this fundamentally shifts the conversation. Rather than attempting to secure complex hybrid cloud configurations through contractual trust with cloud providers, they can verify them cryptographically. That’s a critical distinction that matters when nation-state adversaries have proven they are capable of launching targeted cyberattacks at machine speed.The brutal economics of unprotected AIEpoch AI research shows frontier training costs have grown at 2.4x annually since 2016, which means billion-dollar training runs could be a reality within a few short years. Yet the infrastructure protecting these investments remains fundamentally insecure in most deployments. Security budgets created to protect frontier training models aren&#x27;t keeping up with the exceptionally fast pace of model training. The result is that more models are under threat as existing approaches can&#x27;t scale and keep up with adversaries&#x27; tradecraft.IBM&#x27;s 2025 Cost of Data Breach Report found that 13% of organizations experienced breaches of AI models or applications. Among those breached, 97% lacked proper AI access controls. Shadow AI incidents cost $4.63 million on average, or $670,000 more than standard breaches, with one in five breaches now involving unsanctioned tools that disproportionately expose customer PII (65%) and intellectual property (40%).Think about what this means for organizations spending $50 million or $500 million on a training run. Their model weights sit in multi-tenant environments where cloud providers can inspect the data. Hardware-level encryption that proves the environment hasn&#x27;t been tampered with changes that financial equation entirely.The GTG-1002 wake-up callIn November 2025, Anthropic disclosed something unprecedented: A Chinese state-sponsored group designated GTG-1002 had manipulated Claude Code to conduct what the company described as the first documented case of a large-scale cyberattack executed without substantial human intervention.State-sponsored adversaries turned it into an autonomous intrusion agent that discovered vulnerabilities, crafted exploits, harvested credentials, moved laterally through networks, and categorized stolen data by intelligence value. Human operators stepped in only at critical junctures. According to Anthropic&#x27;s analysis, the AI executed around 80 to 90% of all tactical work independently.The implications extend beyond this single incident. Attack surfaces that once required teams of experienced attackers can now be probed at machine speed by opponents with access to foundation models.Comparing the performance of Blackwell vs. RubinSpecificationBlackwell GB300 NVL72Rubin NVL72Inference compute (FP4)1.44 exaFLOPS3.6 exaFLOPSNVFP4 per GPU (inference)20 PFLOPS50 PFLOPSPer-GPU NVLink bandwidth1.8 TB/s3.6 TB/sRack NVLink bandwidth130 TB/s260 TB/sHBM bandwidth per GPU~8 TB/s~22 TB/sIndustry momentum and AMD&#x27;s alternativeNvidia isn&#x27;t operating in isolation. Research from the Confidential Computing Consortium and IDC, released in December, found that 75% of organizations are adopting confidential computing, with 18% already in production and 57% piloting deployments.\"Confidential Computing has grown from a niche concept into a vital strategy for data security and trusted AI innovation,\" said Nelly Porter, governing board chair of the Confidential Computing Consortium. Real barriers remain: attestation validation challenges affect 84% of respondents, and a skills gap hampers 75%.AMD&#x27;s Helios rack takes a different approach. Built on Meta&#x27;s Open Rack Wide specification, announced at OCP Global Summit in October 2025, it delivers approximately 2.9 exaflops of FP4 compute with 31 TB of HBM4 memory and 1.4 PB/s aggregate bandwidth. Where Nvidia designs confidential computing into every component, AMD prioritizes open standards through the Ultra Accelerator Link and Ultra Ethernet consortia. The competition between Nvidia and AMD is giving security leaders more of a choice than they otherwise would have had. Comparing the tradeoffs of Nvidia&#x27;s integrated approach versus AMD&#x27;s open-standards flexibility for their specific infrastructures and business-specific threat models is key.What security leaders are doing nowHardware-level confidentiality doesn&#x27;t replace zero-trust principles; it gives them teeth. What Nvidia and AMD are building lets security leaders verify trust cryptographically rather than assume it contractually. That&#x27;s a meaningful shift for anyone running sensitive workloads on shared infrastructure. And if the attestation claims hold up in production, this approach could let enterprises extend zero-trust enforcement across thousands of nodes without the policy sprawl and agent overhead that software-only implementations require.Before deployment: Verify attestation to confirm environments haven&#x27;t been tampered with. Cryptographic proof of compliance should be a prerequisite for signing contracts, not an afterthought or worse, a nice-to-have. If your cloud provider can&#x27;t demonstrate attestation capabilities, that&#x27;s a question worth raising in your next QBR.During operation: Maintain separate enclaves for training and inference, and include security teams in the model pipeline from the very start. IBM&#x27;s research showed 63% of breached organizations had no AI governance policy. You can&#x27;t bolt security on after development; that translates into an onramp for mediocre security design-ins and lengthy red teaming that catches bugs that needed to be engineered out of a model or app early.Across the organization: Run joint exercises between security and data science teams to surface vulnerabilities before attackers find them. Shadow AI accounted for 20% of breaches and exposed customer PII and IP at higher rates than other breach types.Bottom line The GTG-1002 campaign demonstrated that adversaries can now automate large-scale intrusions with minimal human oversight at scale. Nearly every organization that experienced an AI-related breach lacked proper access controls.Nvidia&#x27;s Vera Rubin NVL72 transforms racks from potential liabilities into cryptographically attested assets by encrypting every bus. AMD&#x27;s Helios offers an open-standards alternative. Hardware confidentiality alone won&#x27;t stop a determined adversary, but combined with strong governance and realistic threat exercises, rack-scale encryption gives security leaders the foundation they need to protect investments measured in hundreds of millions of dollars.The question facing CISOs isn&#x27;t whether attested infrastructure is worth it. It&#x27;s whether organizations building high-value AI models can afford to operate without it.",
          "feed_position": 1,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/2YSkElI5OHmat9celGziVv/a23ebd445f5a38cd5062055b9dd3b15e/jenson_at_ces.jpg?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/computing/accessories/best-laptop-power-bank-120040388.html",
          "published_at": "Mon, 12 Jan 2026 10:01:26 +0000",
          "title": "The best laptop power banks for 2026",
          "standfirst": "Laptop power banks come in handy if you often travel or work away from your desk. These larger batteries have enough capacity to refill your computer to around 75 percent, giving you many more productive hours. Of course, they can also charge smaller devices like your phone, camera or tablet a few times over. Nearly all portable laptop chargers fall below the 100-watt-hour mark so you can bring them in your carry-on when you fly. Additional features, like built-in cables and digital displays make these battery banks easier to use, too. We tested over a dozen options and put our thoughts below, so you can find the best laptop power bank for your next trip away from an outlet. Table of contents Best laptop power banks for 2026 What to look for in a laptop power bank How we test portable laptop chargers Other laptop power banks we tested Laptop power bank FAQs Recent updates Best laptop power banks for 2026 What to look for in a laptop power bank Flying with a laptop power bank Most portable batteries top out at around 27,000mAh so you can fly with them. The TSA currently limits the capacity carry-on batteries to 100Wh, which works out to around 27,500mAh for 3.6 volt lithium-ion batteries. Note that you’re not allowed to pack any batteries in your checked luggage, regardless of capacity. The TSA rules are intended to limit fire danger — and some airlines are implementing further restrictions due to recent on-board incidents. In March 2025, a Hong Kong flight was grounded after a battery pack caught fire in an overhead bin. A similar situation happened that same year in July on a domestic Delta flight, and again in August on a transatlantic KLM flight. As a result, some airlines, including Emirates, Southwest and others have announced further restrictions on flying with battery packs. Rules include limiting the number of allowed portable chargers and requiring flyers to keep power banks in clear view when using them to recharge a device. If the battery pack isn’t actively in use, however, most rules allow them to stay in your carry-on bag in the overhead bin. Before flying, it’s wise to check your airline’s policies. Capacity If you just need to keep a smartphone from dying before you can make it home, just about any of the best power banks will do. But if you need to revive multiple devices or the substantial battery of a laptop, you’ll want something with a high milliamp-hour​​ (mAh) capacity. A power bank capable of delivering enough power to a laptop will have a capacity between 20,000 and 27,000 mAh. If you want something even bigger than a laptop power bank, and don’t need to fly with it, you’ll likely want to look into portable power stations. These can be the size of a car battery or larger and can potentially fuel an entire weekend away. Another thing to keep in mind is that the capacity listed in a power bank's specs is not what will be delivered to your devices. As I mentioned, the capacity of these banks is around 25,000mAh. Even the huge battery on a 16-inch MacBook Pro or a Dell XPS 16 has a mAh rating of around 5,000 - 6,000mAh, so you might think you’d get five full charges but in reality, you only get about a single 70-percent charge. The voltage is different (typically 3.7V for the power bank and 11.4V for a laptop) which makes the watt-hours, or the amount of energy each battery can hold, different (working out to 92Wh for the battery and 72Wh for the built-in laptop batteries). On top of that, in order to feed a charge from a power bank to a laptop, a voltage conversion takes place and that dissipates a decent amount of energy. Without turning this into a physics lesson, this all means that a power bank with a 25,000mAh (or 92Wh) capacity will typically fill a 5,000mAh (or 72Wh) laptop battery to about 75 percent. In my tests, I averaged about a 60-percent efficiency rate between a power bank’s listed capacity and the actual charge delivered. Ports Every large power bank I’ve tested has at least three USB ports, with a mix of USB-C and USB-A, which should cover nearly any portable device you need to recharge — earbuds, phones, tablets, laptops, you name it. In addition to the different plug formats, some ports supply power at different wattages. For example, one built-in USB-C port might be rated for 60 watts, while the one next to it is rated for 100 watts. So if you’ve got a device that’s capable of 70W fast charging, such as the new MacBook Air, you’d want to opt for the 100W port to get the best charging speeds possible. Note that devices with a smaller wattage draw won’t be negatively affected by connecting to ports with high ratings. For example, a Galaxy S24 Ultra, capable of 45W super fast charging, is perfectly compatible with the 100W port. A device will only draw what it can take, regardless of what a port can supply. Just remember that the port, device and charging cable need to be at or above the desired wattage rating to achieve maximum charging rates. Some of these larger batteries also have AC ports. It might seem like a natural fit to plug in your laptop’s power adapter for a recharge. But really, the AC port should only be for devices that can’t use USB — such as a lamp or a printer. Plugging a power adapter into the AC port only wastes energy through conversion. First, the battery converts its DC power to supply the port with AC power, then the power adapter converts that AC power back to DC so your laptop can take it in. And as you’ll remember from physics class, each time energy is converted, some is lost to heat and other dissipations. Better to cut out the middleman and just send that DC power straight from the battery to the device. Also, you can use more than one port at a time with these devices; just remember that the speed of whatever you’re charging will likely go down, and of course, the battery is going to drain proportionally to what you’re refilling. Wireless charging Since I first started testing portable power banks a few years ago, wireless charging capabilities have noticeably improved. The first few I tried were painfully slow and not worth recommending. Now the wireless pads built into power banks are impressively fast — particularly, in my experience, when charging Samsung Galaxy phones (though the lack of a stabilizing magnetic connection like Apple’s MagSafe means they only work when rested flat on a pad). Most wireless charging connections can be used while other ports are also being employed, making them convenient for some mobile battlestation setups. Of course, wireless charging is always less efficient than wired, and recharging from an external battery is less efficient in general. If you want to waste as little energy as possible, you’re better off sticking to wired connections. Design All power banks are designed to be portable, but there’s a big difference between a pocket-friendly 5,000mAh battery and one of these laptop-compatible bruisers. Most of the latter weigh between a pound and a half to two pounds, which is a considerable addition to a backpack. Many of the options listed here have a display to tell you how much charge remains in the battery, which is helpful when you’re trying to judiciously meet out charges to your devices. If a bank has a wireless connection, the pad is usually on the flat top and any available AC connection is usually at one end. Both may require you to engage those charging methods. Don’t be like me and grumble loudly that you got a bum unit without pressing (and sometimes double pressing) all the buttons first. How we test portable laptop chargers For the past three years, I’ve been testing and using dozens of portable batteries for our other battery guide. Some of those batteries include the higher-capacity power banks you see here. I also got a hold of a few extra banks just for this guide to make sure we covered what’s available. I went for brands I’m already familiar with, as well as battery packs from well-received manufacturers I hadn’t tried before (like UGREEN and Lion Energy). I only considered banks with at least a 20,000mAh capacity and mostly stuck with those that rated 25,000mAh and higher. Here’s everything we tested: Zendure Supertank Pro Mophie Powerstation Pro XL Mophie Powerstation Pro AC Lion Energy Eclipse Mag Lion Energy Trek Baseus Blade Laptop Anker Prime 27,650mAh Goal Zero Sherpa 100 AC Anker Retractable Cable Laptop Bank HyperJuice 245W Anker Prime Power Bank (26K, 300W) UGreen Power Bank 25,000mAh 145W I tested each power bank with an Apple phone (iPhone 15 or 16), an Android phone (Galaxy S23 Ultra), a tablet (M1 iPad Air) and a laptop (16-inch MacBook Pro with the M1 Pro chip). Even though these banks can charge multiple devices at once, I refilled one at a time, to make side-by-side comparisons more straightforward. I drained the batteries of the phones and tablets to between zero and five percent and then didn’t use any device as it refilled. For the MacBook, I let it run down to 10 percent before plugging in the power bank. That's when most laptops give display a “connect to power” warning, as draining any battery to empty will compromise the battery life. I then used it as one might in a mobile office, with a Bluetooth keyboard and mouse, while connected to Wi-Fi and a VPN. For each test, I noted how long a completely charged battery took to get a device back to full and how much of the battery’s capacity was used up in one charge. I also noted things like portability, apparent durability, helpful features and overall design. For reference, here are the battery capacities of the devices I used: iPhone 15: 3,349mAh Galaxy S23 Ultra: 4,855mAh iPad Air (5th gen): 7,729mAh 16-inch M1 Pro MacBook Pro: 27,027mAh Other laptop power banks we tested HyperJuice 245W Hyper’s HyperJuice 245W brick looks great and has a hefty 27,000mAh capacity. The four USB-C ports can combine to output 245W of power and it got my MacBook Pro from nearly dead to 75 percent before depleting itself. When testing it with a Samsung Galaxy S23 Ultra, the handset got back up to a full charge in just over an hour. The screen tells you what each port is doing as well as displaying the amount of charge remaining in the pack itself. But the lack of port variety makes it feel less versatile than other picks on this list — the price is higher than our other options, too. Laptop power bank FAQs How do laptop power banks differ from phone power banks? The main difference is size. Phone power banks tend to have a capacity ranging from 5,000mAh to 20,000mAh and laptop powerbanks are typically rated between 20,000mAh and 27,000mAh. There’s no official definition, however. Laptop batteries are simply larger and need a bigger supply of power to give them a meaningful charge. How do you fast charge a power bank? You can charge a power bank exactly as fast as the power bank’s internal mechanisms will allow. Most batteries are limited in how quickly they can accept and deliver a charge to avoid dangerously overheating. But to make sure you’re charging a bank as quickly as possible, make sure the wall adapter and the USB-C cable you are using have a high wattage rating — using a 5W power brick and a 10W cable will take a lot longer to refill your bank than a 65W wall charger and a 100W cord. What size power bank do I need for a laptop? Look for a power bank with a rating of at least 20,000mAh. Slightly smaller batteries may work, but they won’t deliver a significant charge laptops. How many mAh to charge a laptop? A milliamp hour (mAh) is how much a battery can hold, and most portable batteries list their capacity using mAh. If you get a battery rated at 20,000mAh or above, it should be able to charge your laptop. Using mAh to discuss laptop batteries can be confusing. Due to differing voltages, you can’t directly compare the mAh ratings of a power bank battery to a laptop battery. Using watt-hours is a better gauge, as that calculation takes voltage into account. Recent updates November 2025: Updated our overall top pick to the Anker Laptop Power bank. Added a premium power bank pick. This article originally appeared on Engadget at https://www.engadget.com/computing/accessories/best-laptop-power-bank-120040388.html?src=rss",
          "content": "Laptop power banks come in handy if you often travel or work away from your desk. These larger batteries have enough capacity to refill your computer to around 75 percent, giving you many more productive hours. Of course, they can also charge smaller devices like your phone, camera or tablet a few times over. Nearly all portable laptop chargers fall below the 100-watt-hour mark so you can bring them in your carry-on when you fly. Additional features, like built-in cables and digital displays make these battery banks easier to use, too. We tested over a dozen options and put our thoughts below, so you can find the best laptop power bank for your next trip away from an outlet. Table of contents Best laptop power banks for 2026 What to look for in a laptop power bank How we test portable laptop chargers Other laptop power banks we tested Laptop power bank FAQs Recent updates Best laptop power banks for 2026 What to look for in a laptop power bank Flying with a laptop power bank Most portable batteries top out at around 27,000mAh so you can fly with them. The TSA currently limits the capacity carry-on batteries to 100Wh, which works out to around 27,500mAh for 3.6 volt lithium-ion batteries. Note that you’re not allowed to pack any batteries in your checked luggage, regardless of capacity. The TSA rules are intended to limit fire danger — and some airlines are implementing further restrictions due to recent on-board incidents. In March 2025, a Hong Kong flight was grounded after a battery pack caught fire in an overhead bin. A similar situation happened that same year in July on a domestic Delta flight, and again in August on a transatlantic KLM flight. As a result, some airlines, including Emirates, Southwest and others have announced further restrictions on flying with battery packs. Rules include limiting the number of allowed portable chargers and requiring flyers to keep power banks in clear view when using them to recharge a device. If the battery pack isn’t actively in use, however, most rules allow them to stay in your carry-on bag in the overhead bin. Before flying, it’s wise to check your airline’s policies. Capacity If you just need to keep a smartphone from dying before you can make it home, just about any of the best power banks will do. But if you need to revive multiple devices or the substantial battery of a laptop, you’ll want something with a high milliamp-hour​​ (mAh) capacity. A power bank capable of delivering enough power to a laptop will have a capacity between 20,000 and 27,000 mAh. If you want something even bigger than a laptop power bank, and don’t need to fly with it, you’ll likely want to look into portable power stations. These can be the size of a car battery or larger and can potentially fuel an entire weekend away. Another thing to keep in mind is that the capacity listed in a power bank's specs is not what will be delivered to your devices. As I mentioned, the capacity of these banks is around 25,000mAh. Even the huge battery on a 16-inch MacBook Pro or a Dell XPS 16 has a mAh rating of around 5,000 - 6,000mAh, so you might think you’d get five full charges but in reality, you only get about a single 70-percent charge. The voltage is different (typically 3.7V for the power bank and 11.4V for a laptop) which makes the watt-hours, or the amount of energy each battery can hold, different (working out to 92Wh for the battery and 72Wh for the built-in laptop batteries). On top of that, in order to feed a charge from a power bank to a laptop, a voltage conversion takes place and that dissipates a decent amount of energy. Without turning this into a physics lesson, this all means that a power bank with a 25,000mAh (or 92Wh) capacity will typically fill a 5,000mAh (or 72Wh) laptop battery to about 75 percent. In my tests, I averaged about a 60-percent efficiency rate between a power bank’s listed capacity and the actual charge delivered. Ports Every large power bank I’ve tested has at least three USB ports, with a mix of USB-C and USB-A, which should cover nearly any portable device you need to recharge — earbuds, phones, tablets, laptops, you name it. In addition to the different plug formats, some ports supply power at different wattages. For example, one built-in USB-C port might be rated for 60 watts, while the one next to it is rated for 100 watts. So if you’ve got a device that’s capable of 70W fast charging, such as the new MacBook Air, you’d want to opt for the 100W port to get the best charging speeds possible. Note that devices with a smaller wattage draw won’t be negatively affected by connecting to ports with high ratings. For example, a Galaxy S24 Ultra, capable of 45W super fast charging, is perfectly compatible with the 100W port. A device will only draw what it can take, regardless of what a port can supply. Just remember that the port, device and charging cable need to be at or above the desired wattage rating to achieve maximum charging rates. Some of these larger batteries also have AC ports. It might seem like a natural fit to plug in your laptop’s power adapter for a recharge. But really, the AC port should only be for devices that can’t use USB — such as a lamp or a printer. Plugging a power adapter into the AC port only wastes energy through conversion. First, the battery converts its DC power to supply the port with AC power, then the power adapter converts that AC power back to DC so your laptop can take it in. And as you’ll remember from physics class, each time energy is converted, some is lost to heat and other dissipations. Better to cut out the middleman and just send that DC power straight from the battery to the device. Also, you can use more than one port at a time with these devices; just remember that the speed of whatever you’re charging will likely go down, and of course, the battery is going to drain proportionally to what you’re refilling. Wireless charging Since I first started testing portable power banks a few years ago, wireless charging capabilities have noticeably improved. The first few I tried were painfully slow and not worth recommending. Now the wireless pads built into power banks are impressively fast — particularly, in my experience, when charging Samsung Galaxy phones (though the lack of a stabilizing magnetic connection like Apple’s MagSafe means they only work when rested flat on a pad). Most wireless charging connections can be used while other ports are also being employed, making them convenient for some mobile battlestation setups. Of course, wireless charging is always less efficient than wired, and recharging from an external battery is less efficient in general. If you want to waste as little energy as possible, you’re better off sticking to wired connections. Design All power banks are designed to be portable, but there’s a big difference between a pocket-friendly 5,000mAh battery and one of these laptop-compatible bruisers. Most of the latter weigh between a pound and a half to two pounds, which is a considerable addition to a backpack. Many of the options listed here have a display to tell you how much charge remains in the battery, which is helpful when you’re trying to judiciously meet out charges to your devices. If a bank has a wireless connection, the pad is usually on the flat top and any available AC connection is usually at one end. Both may require you to engage those charging methods. Don’t be like me and grumble loudly that you got a bum unit without pressing (and sometimes double pressing) all the buttons first. How we test portable laptop chargers For the past three years, I’ve been testing and using dozens of portable batteries for our other battery guide. Some of those batteries include the higher-capacity power banks you see here. I also got a hold of a few extra banks just for this guide to make sure we covered what’s available. I went for brands I’m already familiar with, as well as battery packs from well-received manufacturers I hadn’t tried before (like UGREEN and Lion Energy). I only considered banks with at least a 20,000mAh capacity and mostly stuck with those that rated 25,000mAh and higher. Here’s everything we tested: Zendure Supertank Pro Mophie Powerstation Pro XL Mophie Powerstation Pro AC Lion Energy Eclipse Mag Lion Energy Trek Baseus Blade Laptop Anker Prime 27,650mAh Goal Zero Sherpa 100 AC Anker Retractable Cable Laptop Bank HyperJuice 245W Anker Prime Power Bank (26K, 300W) UGreen Power Bank 25,000mAh 145W I tested each power bank with an Apple phone (iPhone 15 or 16), an Android phone (Galaxy S23 Ultra), a tablet (M1 iPad Air) and a laptop (16-inch MacBook Pro with the M1 Pro chip). Even though these banks can charge multiple devices at once, I refilled one at a time, to make side-by-side comparisons more straightforward. I drained the batteries of the phones and tablets to between zero and five percent and then didn’t use any device as it refilled. For the MacBook, I let it run down to 10 percent before plugging in the power bank. That's when most laptops give display a “connect to power” warning, as draining any battery to empty will compromise the battery life. I then used it as one might in a mobile office, with a Bluetooth keyboard and mouse, while connected to Wi-Fi and a VPN. For each test, I noted how long a completely charged battery took to get a device back to full and how much of the battery’s capacity was used up in one charge. I also noted things like portability, apparent durability, helpful features and overall design. For reference, here are the battery capacities of the devices I used: iPhone 15: 3,349mAh Galaxy S23 Ultra: 4,855mAh iPad Air (5th gen): 7,729mAh 16-inch M1 Pro MacBook Pro: 27,027mAh Other laptop power banks we tested HyperJuice 245W Hyper’s HyperJuice 245W brick looks great and has a hefty 27,000mAh capacity. The four USB-C ports can combine to output 245W of power and it got my MacBook Pro from nearly dead to 75 percent before depleting itself. When testing it with a Samsung Galaxy S23 Ultra, the handset got back up to a full charge in just over an hour. The screen tells you what each port is doing as well as displaying the amount of charge remaining in the pack itself. But the lack of port variety makes it feel less versatile than other picks on this list — the price is higher than our other options, too. Laptop power bank FAQs How do laptop power banks differ from phone power banks? The main difference is size. Phone power banks tend to have a capacity ranging from 5,000mAh to 20,000mAh and laptop powerbanks are typically rated between 20,000mAh and 27,000mAh. There’s no official definition, however. Laptop batteries are simply larger and need a bigger supply of power to give them a meaningful charge. How do you fast charge a power bank? You can charge a power bank exactly as fast as the power bank’s internal mechanisms will allow. Most batteries are limited in how quickly they can accept and deliver a charge to avoid dangerously overheating. But to make sure you’re charging a bank as quickly as possible, make sure the wall adapter and the USB-C cable you are using have a high wattage rating — using a 5W power brick and a 10W cable will take a lot longer to refill your bank than a 65W wall charger and a 100W cord. What size power bank do I need for a laptop? Look for a power bank with a rating of at least 20,000mAh. Slightly smaller batteries may work, but they won’t deliver a significant charge laptops. How many mAh to charge a laptop? A milliamp hour (mAh) is how much a battery can hold, and most portable batteries list their capacity using mAh. If you get a battery rated at 20,000mAh or above, it should be able to charge your laptop. Using mAh to discuss laptop batteries can be confusing. Due to differing voltages, you can’t directly compare the mAh ratings of a power bank battery to a laptop battery. Using watt-hours is a better gauge, as that calculation takes voltage into account. Recent updates November 2025: Updated our overall top pick to the Anker Laptop Power bank. Added a premium power bank pick. This article originally appeared on Engadget at https://www.engadget.com/computing/accessories/best-laptop-power-bank-120040388.html?src=rss",
          "feed_position": 20
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/infrastructure/how-doordash-scaled-without-a-costly-erp-overhaul",
          "published_at": "Mon, 12 Jan 2026 05:00:00 GMT",
          "title": "How DoorDash scaled without a costly ERP overhaul",
          "standfirst": "Presented by NetSuiteMost companies racing from startup to an industry leader face a choice: limp along with scrappy early systems or endure a costly platform migration.DoorDash did neither. The local-commerce giant scaled from its 2013 founding through IPO and global expansion — acquiring the Helsiniki-based technology company Wolt in 2022 and UK-based Deliveroo in 2025 — while keeping its original Oracle NetSuite business system. Today, it serves over 50 million consumers in more than 40 countries.*Chief Accounting Officer Gordon Lee says the secret is building a scalable ecosystem that allows teams to use tools that work best for them.Choosing flexibility over uniformityWhen DoorDash selected NetSuite as its corporate financial control center, it wasn&#x27;t looking for a system to enforce uniformity. It sought a scalable platform that could connect all its systems, from ERP, CRM, HR, sourcing, and more. \"Our philosophy has been to create a platform that allows our customers and business partners to use whatever tools work best for them,\" Lee says. \"When we&#x27;re managing growth, the majority of the conversation is about managing expectations — what people expect when you grow from A to B.\"The migration questionTwo years after its founding, DoorDash surpassed one million deliveries and expanded into Canada. As the company scaled, Lee faced growing pressure from vendors insisting that rapid growth required a new enterprise platform.He ran the numbers. The move to another platform could cost millions and consume months of his team&#x27;s focus.Instead, DoorDash stayed with NetSuite, which continued to scale alongside the company’s growth. Built on Oracle Cloud Infrastructure, NetSuite delivers the performance and reliability of an enterprise platform without the cost or disruption of migration. Lee concluded: \"Why do I bother to move? I already have the scalability I need from NetSuite.\"Today, DoorDash’s NetSuite backend provides enterprise-grade security while its familiar front end provides the team flexibility, creating a stable, modern foundation for sustained, high-velocity growth.Expanding the menu without the technical indigestionThat flexibility soon proved invaluable. The ability to add new applications quickly — without long, costly integrations — became a major advantage during hypergrowth.For example, as DoorDash expanded from restaurant delivery into grocery, convenience, and retail, Lee turned to NetSuite’s inventory modules to handle the distinct demands of those new categories.“The flexibility to have and not have, and turn the switch on and off, is easy because it’s all integrated,” he explains.Today, DoorDash’s technology stack spans multiple systems — all integrating seamlessly with NetSuite as the financial hub. “They do it, and you’re done,” Lee says.Embedding expertise to scale smarter, not biggerFor Lee, true partnerships turn vendors into part of the team — and that’s exactly how he describes NetSuite Advanced Customer Support (ACS).\"They are here with us every week. They know all my schematics, they know all my data infrastructure, they know all my database structure within NetSuite. Essentially, they are an extension of my team,\" Lee explains.Close collaboration benefits both parties. DoorDash keeps NetSuite attuned to the realities of hypergrowth and gets instant feedback on technology capability and scalability. In turn, NetSuite stays close to a marquee customer. Interaction is ongoing — and frank, according to Lee. “We work directly with NetSuite ACS and often ask, &#x27;Can NetSuite do this?’ If they can prove it can, we stay with NetSuite.\"Another benefit is the ability to extend DoorDash&#x27;s expertise without expanding headcount. \"If someone says to me, &#x27;Gordon, you&#x27;re just an accountant. How do you know about systems? I say, I don&#x27;t. I have a network guy with us, an expert.’ That&#x27;s the kind of partner I want to surround myself with, so that I can grow beyond what I am.”By embedding expertise within our partnerships, DoorDash scales with precision and control. Lee says the model applies to other companies preparing for IPOs or global expansion. He adds that sustainable growth depends as much on shared understanding as on technology itself. Too often, finance and IT “look at the same requirement but see completely different things,” Lee says, describing what he calls the “blue versus purple” problem. “The accountant doesn’t understand the configuration of the system,” he explains. “The IT guy doesn’t understand what the accountant was trying to tell them.”NetSuite bridges that gap. With a unified data model and built-in best practices across finance, operations, and more, it keeps teams aligned and information consistent. That close collaboration, Lee notes, is what keeps rollouts smooth, data clean, and growth sustainable at any stage.AI strategy: Trust only internal data, get data ducks in a rowLee plans to test the NetSuite AI Connector Service — which supports Model Context Protocol (MCP) and lets customers connect their own AI to NetSuite — to see how faster access to accurate data can drive growth.By implementing an internal instance, Lee is less worried about disruptive errors from LLMs trained on public data sources. \"Think about a generative AI chatbot. When you ask a question, it can reflect many perspectives,” he explains. On the other hand, a chatbot trained on private enterprise systems benefits from “a clean data infrastructure.”Lee is taking a methodical approach: first get data pristine, then train AI on domain-specific terminology, and finally see how internal AI can both find the right information and automate downstream accounting processes to save resources and accelerate growth.Betting long-term on its original financial coreFrom early growth to major acquisitions that helped expand its footprint across the globe, DoorDash has relied on NetSuite as a consistent foundation for innovation and scale.Lee credits NetSuite’s flexible architecture and close partnership with helping enable DoorDash as it continued to scale and cement itself as a leader in local commerce globally.His mantra is simple: “Focus on growth instead of churning through vendors.”* Based on the combined numbers for DoorDash, Wolt, and Deliveroo, measured as of September 2025.Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.",
          "content": "Presented by NetSuiteMost companies racing from startup to an industry leader face a choice: limp along with scrappy early systems or endure a costly platform migration.DoorDash did neither. The local-commerce giant scaled from its 2013 founding through IPO and global expansion — acquiring the Helsiniki-based technology company Wolt in 2022 and UK-based Deliveroo in 2025 — while keeping its original Oracle NetSuite business system. Today, it serves over 50 million consumers in more than 40 countries.*Chief Accounting Officer Gordon Lee says the secret is building a scalable ecosystem that allows teams to use tools that work best for them.Choosing flexibility over uniformityWhen DoorDash selected NetSuite as its corporate financial control center, it wasn&#x27;t looking for a system to enforce uniformity. It sought a scalable platform that could connect all its systems, from ERP, CRM, HR, sourcing, and more. \"Our philosophy has been to create a platform that allows our customers and business partners to use whatever tools work best for them,\" Lee says. \"When we&#x27;re managing growth, the majority of the conversation is about managing expectations — what people expect when you grow from A to B.\"The migration questionTwo years after its founding, DoorDash surpassed one million deliveries and expanded into Canada. As the company scaled, Lee faced growing pressure from vendors insisting that rapid growth required a new enterprise platform.He ran the numbers. The move to another platform could cost millions and consume months of his team&#x27;s focus.Instead, DoorDash stayed with NetSuite, which continued to scale alongside the company’s growth. Built on Oracle Cloud Infrastructure, NetSuite delivers the performance and reliability of an enterprise platform without the cost or disruption of migration. Lee concluded: \"Why do I bother to move? I already have the scalability I need from NetSuite.\"Today, DoorDash’s NetSuite backend provides enterprise-grade security while its familiar front end provides the team flexibility, creating a stable, modern foundation for sustained, high-velocity growth.Expanding the menu without the technical indigestionThat flexibility soon proved invaluable. The ability to add new applications quickly — without long, costly integrations — became a major advantage during hypergrowth.For example, as DoorDash expanded from restaurant delivery into grocery, convenience, and retail, Lee turned to NetSuite’s inventory modules to handle the distinct demands of those new categories.“The flexibility to have and not have, and turn the switch on and off, is easy because it’s all integrated,” he explains.Today, DoorDash’s technology stack spans multiple systems — all integrating seamlessly with NetSuite as the financial hub. “They do it, and you’re done,” Lee says.Embedding expertise to scale smarter, not biggerFor Lee, true partnerships turn vendors into part of the team — and that’s exactly how he describes NetSuite Advanced Customer Support (ACS).\"They are here with us every week. They know all my schematics, they know all my data infrastructure, they know all my database structure within NetSuite. Essentially, they are an extension of my team,\" Lee explains.Close collaboration benefits both parties. DoorDash keeps NetSuite attuned to the realities of hypergrowth and gets instant feedback on technology capability and scalability. In turn, NetSuite stays close to a marquee customer. Interaction is ongoing — and frank, according to Lee. “We work directly with NetSuite ACS and often ask, &#x27;Can NetSuite do this?’ If they can prove it can, we stay with NetSuite.\"Another benefit is the ability to extend DoorDash&#x27;s expertise without expanding headcount. \"If someone says to me, &#x27;Gordon, you&#x27;re just an accountant. How do you know about systems? I say, I don&#x27;t. I have a network guy with us, an expert.’ That&#x27;s the kind of partner I want to surround myself with, so that I can grow beyond what I am.”By embedding expertise within our partnerships, DoorDash scales with precision and control. Lee says the model applies to other companies preparing for IPOs or global expansion. He adds that sustainable growth depends as much on shared understanding as on technology itself. Too often, finance and IT “look at the same requirement but see completely different things,” Lee says, describing what he calls the “blue versus purple” problem. “The accountant doesn’t understand the configuration of the system,” he explains. “The IT guy doesn’t understand what the accountant was trying to tell them.”NetSuite bridges that gap. With a unified data model and built-in best practices across finance, operations, and more, it keeps teams aligned and information consistent. That close collaboration, Lee notes, is what keeps rollouts smooth, data clean, and growth sustainable at any stage.AI strategy: Trust only internal data, get data ducks in a rowLee plans to test the NetSuite AI Connector Service — which supports Model Context Protocol (MCP) and lets customers connect their own AI to NetSuite — to see how faster access to accurate data can drive growth.By implementing an internal instance, Lee is less worried about disruptive errors from LLMs trained on public data sources. \"Think about a generative AI chatbot. When you ask a question, it can reflect many perspectives,” he explains. On the other hand, a chatbot trained on private enterprise systems benefits from “a clean data infrastructure.”Lee is taking a methodical approach: first get data pristine, then train AI on domain-specific terminology, and finally see how internal AI can both find the right information and automate downstream accounting processes to save resources and accelerate growth.Betting long-term on its original financial coreFrom early growth to major acquisitions that helped expand its footprint across the globe, DoorDash has relied on NetSuite as a consistent foundation for innovation and scale.Lee credits NetSuite’s flexible architecture and close partnership with helping enable DoorDash as it continued to scale and cement itself as a leader in local commerce globally.His mantra is simple: “Focus on growth instead of churning through vendors.”* Based on the combined numbers for DoorDash, Wolt, and Deliveroo, measured as of September 2025.Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.",
          "feed_position": 2,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/6ZnGG4zYxTpmnj2txjQwJD/9b3b125e58398fe9ab2b4865aade5e62/AdobeStock_792967914_Editorial_Use_Only.jpeg?w=300&q=30"
        }
      ],
      "featured_image": "https://images.ctfassets.net/jdtwqhzvc2n1/7iyQoeSwdOqqpfcE0PFWgF/48db7d0305019eee107028d9f018d2ac/Semantic_caching.png?w=300&q=30",
      "popularity_score": 2017.1350330555556
    },
    {
      "id": "cluster_28",
      "coverage": 2,
      "updated_at": "Mon, 12 Jan 2026 15:00:48 -0500",
      "title": "Anthropic launches Cowork for Claude, built on Claude Code to automate complex tasks with minimal prompting, as a research preview for Claude Max subscribers (Webb Wright/ZDNET)",
      "neutral_headline": "Claude Cowork automates complex tasks for you now - at your own risk",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/260112/p30#a260112p30",
          "published_at": "Mon, 12 Jan 2026 15:00:48 -0500",
          "title": "Anthropic launches Cowork for Claude, built on Claude Code to automate complex tasks with minimal prompting, as a research preview for Claude Max subscribers (Webb Wright/ZDNET)",
          "standfirst": "Webb Wright / ZDNET: Anthropic launches Cowork for Claude, built on Claude Code to automate complex tasks with minimal prompting, as a research preview for Claude Max subscribers &mdash; ZDNET's key takeaways &mdash; Anthropic is launching Cowork for Claude as a research preview. &mdash; It's built upon Claude Code and can automate complex tasks.",
          "content": "Webb Wright / ZDNET: Anthropic launches Cowork for Claude, built on Claude Code to automate complex tasks with minimal prompting, as a research preview for Claude Max subscribers &mdash; ZDNET's key takeaways &mdash; Anthropic is launching Cowork for Claude as a research preview. &mdash; It's built upon Claude Code and can automate complex tasks.",
          "feed_position": 6,
          "image_url": "http://www.techmeme.com/260112/i30.jpg"
        },
        {
          "source": "ZDNet",
          "url": "https://www.zdnet.com/article/anthropic-cowork-for-claude-complex-actions-security-risks/",
          "published_at": "Mon, 12 Jan 2026 19:30:40 GMT",
          "title": "Claude Cowork automates complex tasks for you now - at your own risk",
          "standfirst": "Available first to Claude Max subscribers, the research preview empowers Anthropic's chatbot to handle complex tasks.",
          "content": "Available first to Claude Max subscribers, the research preview empowers Anthropic's chatbot to handle complex tasks.",
          "feed_position": 3
        }
      ],
      "featured_image": "http://www.techmeme.com/260112/i30.jpg",
      "popularity_score": 2016.7186441666668
    },
    {
      "id": "cluster_1",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 23:04:33 +0000",
      "title": "You can now reserve a hotel room on the Moon for $250,000",
      "neutral_headline": "You can now reserve a hotel room on the Moon for $250,000",
      "items": [
        {
          "source": "Ars Technica Main",
          "url": "https://arstechnica.com/space/2026/01/you-can-now-reserve-a-hotel-room-on-the-moon-for-250000/",
          "published_at": "Mon, 12 Jan 2026 23:04:33 +0000",
          "title": "You can now reserve a hotel room on the Moon for $250,000",
          "standfirst": "\"We can't keep everyone living on that first ship that sailed to North America.\"",
          "content": "A company called GRU Space publicly announced its intent to construct a series of increasingly sophisticated habitats on the Moon, culminating in a hotel inspired by the Palace of the Fine Arts in San Francisco. On Monday, the company invited those interested in a berth to plunk down a deposit between $250,000 and $1 million, qualifying them for a spot on one of its early lunar surface missions in as little as six years from now. It sounds crazy, doesn't it? After all, GRU Space had, as of late December when I spoke to founder Skyler Chan, a single full-time employee aside from himself. And Chan, in fact, only recently graduated from the University of California, Berkeley.Read full article Comments",
          "feed_position": 0,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/lunar-hotel-1-1152x648-1768258910.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/lunar-hotel-1-1152x648-1768258910.jpg",
      "popularity_score": 352.78114416666665
    },
    {
      "id": "cluster_4",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 22:27:50 +0000",
      "title": "Hobby GitHub repo shows Linus Torvalds vibe codes (sometimes)",
      "neutral_headline": "Hobby GitHub repo shows Linus Torvalds vibe codes (sometimes)",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/ai/2026/01/hobby-github-repo-shows-linus-torvalds-vibe-codes-sometimes/",
          "published_at": "Mon, 12 Jan 2026 22:27:50 +0000",
          "title": "Hobby GitHub repo shows Linus Torvalds vibe codes (sometimes)",
          "standfirst": "\"But then I cut out the middle man—me.\"",
          "content": "Linux and Git creator Linus Torvalds' latest hobby project contains code that was \"basically written by vibe coding,\" but you shouldn't read that to mean that Torvalds is embracing that approach for anything and everything. Torvalds sometimes works on a small hobby project over holiday breaks. Last year, he made guitar pedals. This year, he did some work on AudioNoise, which he calls \"another silly guitar-pedal-related repo.\" It creates random digital audio effects. Torvalds revealed that he had used an AI coding tool in the README for the repo:Read full article Comments",
          "feed_position": 0,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2015/08/LinuxCon_Europe_Linus_Torvalds_05-1152x648-1768254932.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2015/08/LinuxCon_Europe_Linus_Torvalds_05-1152x648-1768254932.jpg",
      "popularity_score": 352.16919972222223
    },
    {
      "id": "cluster_2",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 22:49:31 +0000",
      "title": "Paramount sues WBD over Netflix deal. WBD says Paramount’s price is still inadequate.",
      "neutral_headline": "Paramount sues WBD over Netflix deal. WBD says Paramount’s price is still inadequate.",
      "items": [
        {
          "source": "Ars Technica Main",
          "url": "https://arstechnica.com/gadgets/2026/01/paramount-sues-wbd-over-netflix-deal-wbd-says-paramounts-price-is-still-inadequate/",
          "published_at": "Mon, 12 Jan 2026 22:49:31 +0000",
          "title": "Paramount sues WBD over Netflix deal. WBD says Paramount’s price is still inadequate.",
          "standfirst": "WBD calls Paramount's lawsuit \"meritless\" and its offer deficient.",
          "content": "Paramount Skydance escalated its hostile takeover bid of Warner Bros. Discovery (WBD) today by filing a lawsuit in Delaware Chancery Court against WBD, declaring its intention to fight Netflix’s acquisition. In December, WBD agreed to sell its streaming and movie businesses to Netflix for $82.7 billion. The deal would see WBD’s Global Networks division, comprised of WBD's legacy cable networks, spun out into a separate company called Discovery Global. But in December, Paramount submitted a hostile takeover bid and amended its bid for WBD. Subsequently, the company has aggressively tried to convince WBD’s shareholders that its $108.4 billion offer for all of WBD is superior to the Netflix deal. Today, Paramount CEO David Ellison wrote a letter to WBD shareholders informing them of Paramount’s lawsuit. The lawsuit requests the court to force WBD to disclose “how it valued the Global Networks stub equity, how it valued the overall Netflix transaction, how the purchase price reduction for debt works in the Netflix transaction, or even what the basis is for its ‘risk adjustment’” of Paramount’s $30 per share all-cash offer. Netflix’s offer equates to $27.72 per share, including $23.25 in cash and shares of Netflix common stock. Paramount hopes the information will encourage more WBD shareholders to tender their shares under Paramount's offer by the January 21 deadline.Read full article Comments",
          "feed_position": 1,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2215193098-1152x648-1768255617.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2215193098-1152x648-1768255617.jpg",
      "popularity_score": 342.5305886111111
    },
    {
      "id": "cluster_10",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 22:07:23 +0000",
      "title": "Verizon to stop automatic unlocking of phones as FCC ends 60-day unlock rule",
      "neutral_headline": "Verizon to stop automatic unlocking of phones as FCC ends 60-day unlock rule",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2026/01/fcc-lets-verizon-lock-phones-for-longer-making-it-harder-to-switch-carriers/",
          "published_at": "Mon, 12 Jan 2026 22:07:23 +0000",
          "title": "Verizon to stop automatic unlocking of phones as FCC ends 60-day unlock rule",
          "standfirst": "FCC waives rule that forced Verizon to unlock phones 60 days after activation.",
          "content": "The Federal Communications Commission is letting Verizon lock phones to its network for longer periods, eliminating a requirement to unlock handsets 60 days after they are activated on its network. The change will make it harder for people to switch from Verizon to other carriers. The FCC today granted Verizon's petition for a waiver of the 60-day unlocking requirement. While the waiver is in effect, Verizon only has to comply with the CTIA trade group's voluntary unlocking policy. The CTIA policy calls for unlocking prepaid mobile devices one year after activation, while devices on postpaid plans can be unlocked after a contract, device financing plan, or early termination fee is paid. Unlocking a phone allows it to be used on another carrier's network. While Verizon was previously required to unlock phones automatically after 60 days, the CTIA code says carriers only have to unlock phones \"upon request\" from consumers. The FCC said the Verizon waiver will remain in effect until the agency \"decides on an appropriate industry-wide approach for the unlocking of handsets.\"Read full article Comments",
          "feed_position": 1,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/verizon-jerks-locked-phone-1152x648-1765486982.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/verizon-jerks-locked-phone-1152x648-1765486982.jpg",
      "popularity_score": 341.82836638888887
    },
    {
      "id": "cluster_16",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 21:27:35 +0000",
      "title": "Judge: Trump violated Fifth Amendment by ending energy grants in only blue states",
      "neutral_headline": "Judge: Trump violated Fifth Amendment by ending energy grants in only blue states",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2026/01/judge-trump-violated-fifth-amendment-by-ending-energy-grants-in-only-blue-states/",
          "published_at": "Mon, 12 Jan 2026 21:27:35 +0000",
          "title": "Judge: Trump violated Fifth Amendment by ending energy grants in only blue states",
          "standfirst": "Donald Trump’s social media post triggers rare Fifth Amendment ruling.",
          "content": "The Trump administration violated the Fifth Amendment when canceling billions of dollars in environmental grants for projects in \"blue states\" that didn't vote for him in the last election, a judge ruled Monday. Trump's blatant discrimination came on the same day as the government shut down last fall. In total, 315 grants were terminated in October, ending support for 223 projects worth approximately $7.5 billion, the Department of Energy confirmed. All the awardees, except for one, were based in states where Donald Trump lost the majority vote to Kamala Harris in 2024. Only seven awardees sued, defending projects that helped states with \"electric vehicle development, updating building energy codes, and addressing methane emissions.\" They accused Trump officials of clearly discriminating against Democratic voters by pointing to their social media posts boasting about punishing blue states.Read full article Comments",
          "feed_position": 3,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2254848310-1024x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2254848310-1024x648.jpg",
      "popularity_score": 321.16503305555557
    },
    {
      "id": "cluster_27",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 20:00:54 +0000",
      "title": "Switching water sources improved hygiene of Pompeii’s public baths",
      "neutral_headline": "Switching water sources improved hygiene of Pompeii’s public baths",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2026/01/switching-water-sources-improved-hygiene-of-pompeiis-public-baths/",
          "published_at": "Mon, 12 Jan 2026 20:00:54 +0000",
          "title": "Switching water sources improved hygiene of Pompeii’s public baths",
          "standfirst": "Scientists analyzed carbonate deposits from baths, aqueduct to learn more about city's changing water supply.",
          "content": "The eruption of Mount Vesuvius in 79 CE released thermal energy roughly equivalent to 100,000 times the atomic bombs dropped on Hiroshima and Nagasaki at the end of World War II, spewing molten rock, pumice, and hot ash over Pompeii. Pompeii's public baths, aqueduct, and water towers were among the preserved structures frozen in time. A new paper published in the Proceedings of the National Academy of Sciences analyzed calcium carbonate deposits from those structures to learn more about the city's water supply and how it changed over time. Pompeii was founded in the sixth century BCE. Prior research revealed that, early on, the city relied on rainwater stored in cisterns and wells for its water supply. The public baths used weight-lifting machinery to lift water up well shafts that were as deep as 40 meters. As the city developed, so did the complexity of its water supply system, most notably with the construction of an aqueduct between 27 BCE and 14 CE. The authors of this latest paper were interested in the calcium carbonate deposits left by water in well shafts as well as the baths and aqueduct. The different layers have \"different chemical and isotope composition, calcite crystal size, and shape,\" which in turn could reveal information about seasonal changes in temperature, as well as changes over time in the chemical composition of the water. Analyzing those properties would enable them to \"reconstruct the history of such systems—particularly public baths—revealing aspects of their maintenance and the adaptations made during their period of use,\" the authors wrote.Read full article Comments",
          "feed_position": 4,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/bath1-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/bath1-1152x648.jpg",
      "popularity_score": 309.7203108333333
    },
    {
      "id": "cluster_31",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 19:36:17 +0000",
      "title": "Apps like Grok are explicitly banned under Google’s rules—why is it still in the Play Store?",
      "neutral_headline": "Apps like Grok are explicitly banned under Google’s rules—why is it...",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/google/2026/01/apps-like-grok-are-explicitly-banned-under-googles-rules-why-is-it-still-in-the-play-store/",
          "published_at": "Mon, 12 Jan 2026 19:36:17 +0000",
          "title": "Apps like Grok are explicitly banned under Google’s rules—why is it still in the Play Store?",
          "standfirst": "Google describes apps exactly like Grok and says they are banned from Google Play.",
          "content": "Elon Musk's xAI recently weakened content guard rails for image generation in the Grok AI bot. This led to a new spate of non-consensual sexual imagery on X, much of it aimed at silencing women on the platform. This, along with the creation of sexualized images of children in the more compliant Grok, has led regulators to begin investigating xAI. In the meantime, Google has rules in place for exactly this eventuality—it's just not enforcing them. It really could not be more clear from Google's publicly available policies that Grok should have been banned yesterday. And yet, it remains in the Play Store. Not only that—it enjoys a T for Teen rating, one notch below the M-rated X app. Apple also still offers the Grok app on its platform, but its rules actually leave more wiggle room. App content restrictions at Apple and Google have evolved in very different ways. From the start, Apple has been prone to removing apps on a whim, so developers have come to expect that Apple's guidelines may not mention every possible eventuality. As Google has shifted from a laissez-faire attitude to more hard-nosed control of the Play Store, it has progressively piled on clarifications in the content policy. As a result, Google's rules are spelled out in no uncertain terms, and Grok runs afoul of them.Read full article Comments",
          "feed_position": 6,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/07/GettyImages-2225386195-1024x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/07/GettyImages-2225386195-1024x648.jpg",
      "popularity_score": 301.31003305555555
    },
    {
      "id": "cluster_30",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 19:56:01 +0000",
      "title": "Supreme Court takes case that could strip FCC of authority to issue fines",
      "neutral_headline": "Supreme Court takes case that could strip FCC of authority to issue fines",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2026/01/supreme-court-takes-case-that-could-strip-fcc-of-authority-to-issue-fines/",
          "published_at": "Mon, 12 Jan 2026 19:56:01 +0000",
          "title": "Supreme Court takes case that could strip FCC of authority to issue fines",
          "standfirst": "AT&#038;T and Verizon claim right to a jury trial was violated by FCC fines.",
          "content": "The Supreme Court will hear a case that could invalidate the Federal Communications Commission's authority to issue fines against companies regulated by the FCC. AT&T, Verizon, and T-Mobile challenged the FCC's ability to punish them after the commission fined the carriers for selling customer location data without their users’ consent. AT&T convinced the US Court of Appeals for the 5th Circuit to overturn its fine, while Verizon lost in the 2nd Circuit and T-Mobile lost in the District of Columbia Circuit. Verizon petitioned the Supreme Court to reverse its loss, while the FCC and Justice Department petitioned the court to overturn AT&T's victory in the 5th Circuit. The Supreme Court granted both petitions to hear the challenges and consolidated the cases in a list of orders released Friday. Oral arguments will be held.Read full article Comments",
          "feed_position": 5,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2023/09/getty-supreme-court-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2023/09/getty-supreme-court-1152x648.jpg",
      "popularity_score": 299.63892194444446
    },
    {
      "id": "cluster_52",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 17:57:32 +0000",
      "title": "Apple chooses Google’s Gemini over OpenAI’s ChatGPT to power next-gen Siri",
      "neutral_headline": "Apple chooses Google’s Gemini over OpenAI’s ChatGPT to power next-gen Siri",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/apple/2026/01/apple-says-its-new-ai-powered-siri-will-use-googles-gemini-language-models/",
          "published_at": "Mon, 12 Jan 2026 17:57:32 +0000",
          "title": "Apple chooses Google’s Gemini over OpenAI’s ChatGPT to power next-gen Siri",
          "standfirst": "Apple goes with Google's tech despite using OpenAI's ChatGPT elsewhere in iOS.",
          "content": "The \"more intelligent\" version of Siri that Apple plans to release later this year will be backed by Google's Gemini language models, the company announced today. CNBC reports that the deal is part of a \"multi-year partnership\" between Apple and Google that will allow Apple to use Google's AI models in its own software. \"After careful evaluation, we determined that Google’s technology provides the most capable foundation for Apple Foundation Models and we’re excited about the innovative new experiences it will unlock for our users,” reads an Apple statement given to CNBC. Today's announcement confirms reporting by Bloomberg's Mark Gurman late last year that Apple and Google were nearing a deal. Apple didn't disclose terms, but Gurman said that Apple would be paying Google \"about $1 billion a year\" for access to its AI models \"following an extensive evaluation period.\"Read full article Comments",
          "feed_position": 8,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/apple_google_hero_3-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/apple_google_hero_3-1152x648.jpg",
      "popularity_score": 294.66419972222224
    },
    {
      "id": "cluster_36",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 19:25:17 +0000",
      "title": "NASA launches new mission to get the most out of the James Webb Space Telescope",
      "neutral_headline": "NASA launches new mission to get the most out of the James Webb Space Telescope",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2026/01/nasas-newest-telescope-will-play-an-outsize-role-in-finding-earth-2-0/",
          "published_at": "Mon, 12 Jan 2026 19:25:17 +0000",
          "title": "NASA launches new mission to get the most out of the James Webb Space Telescope",
          "standfirst": "\"It was not recognized how serious a problem that is until... about 2017 or 2018.\"",
          "content": "Among other things, the James Webb Space Telescope is designed to get us closer to finding habitable worlds around faraway stars. From its perch a million miles from Earth, Webb's huge gold-coated mirror collects more light than any other telescope put into space. The Webb telescope, launched in 2021 at a cost of more than $10 billion, has the sensitivity to peer into distant planetary systems and detect the telltale chemical fingerprints of molecules critical to or indicative of potential life, like water vapor, carbon dioxide, and methane. Webb can do this while also observing the oldest observable galaxies in the Universe and studying planets, moons, and smaller objects within our own Solar System. Naturally, astronomers want to get the most out of their big-budget observatory. That's where NASA's Pandora mission comes in.Read full article Comments",
          "feed_position": 7,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/Pandora_integrated_blue_BCT-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/Pandora_integrated_blue_BCT-1152x648.jpg",
      "popularity_score": 287.1266997222222
    },
    {
      "id": "cluster_58",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 17:08:56 +0000",
      "title": "Is this the beginning of the end for GameStop?",
      "neutral_headline": "Is this the beginning of the end for GameStop",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gaming/2026/01/is-this-the-beginning-of-the-end-for-gamestop/",
          "published_at": "Mon, 12 Jan 2026 17:08:56 +0000",
          "title": "Is this the beginning of the end for GameStop?",
          "standfirst": "The sudden closure of hundreds of storefronts isn't exactly a great sign...",
          "content": "Six and a half years ago—after a failed corporate sale attempt, massive financial losses, and the departure/layoff of many key staff—I wrote about what seemed at the time like the \"imminent demise\" of GameStop. Now, after five years of meme stock mania that helped prop up the company's finances a bit, I'll admit the video game and Funko Pop retailer has lasted much longer as a relevant entity than I anticipated. GameStop's surprisingly extended run may be coming to an end, though, with Polygon reporting late last week that GameStop has abruptly shut down 400 stores across the US, with even more closures expected before the end of the month. That comes on top of 590 US stores that were shuttered in fiscal 2024 (which ended in January 2025) and stated plans to close hundreds of remaining international stores across Canada, Australia, and Europe in the coming months, per SEC filings. GameStop still had just over 3,200 stores worldwide as of February 1, 2025, so even hundreds of new and planned store closures don't literally mean the immediate end of the company as a going concern. But when you consider that there were still nearly 6,000 GameStop locations worldwide as of 2019—nearly 4,000 of which were in the US—the long-term trend is clear.Read full article Comments",
          "feed_position": 9,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2019/08/GettyImages-1135950796-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2019/08/GettyImages-1135950796-1152x648.jpg",
      "popularity_score": 253.85419972222223
    },
    {
      "id": "cluster_75",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 15:21:38 +0000",
      "title": "NASA topples towers used to test Saturn rockets, space shuttle",
      "neutral_headline": "NASA topples towers used to test Saturn rockets, space shuttle",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2026/01/nasa-topples-towers-used-to-test-saturn-rockets-space-shuttle/",
          "published_at": "Mon, 12 Jan 2026 15:21:38 +0000",
          "title": "NASA topples towers used to test Saturn rockets, space shuttle",
          "standfirst": "The Propulsion and Structural Test Facility and Dynamic Test Facility are no more.",
          "content": "Two historic NASA test facilities used in the development of the Saturn V and space shuttle launch vehicles have been demolished after towering over the Marshall Space Flight Center in Alabama since the start of the Space Age. The Propulsion and Structural Test Facility, which was erected in 1957—the same year the first artificial satellite entered Earth orbit—and the Dynamic Test Facility, which has stood since 1964, were brought down by a coordinated series of implosions on Saturday, January 10. Located in Marshall's East Test Area on the US Army's Redstone Arsenal in Huntsville, the two structures were no longer in use and, according to NASA, had a backlog of $25 million in needed repairs. \"This work reflects smart stewardship of taxpayer resources,\" Jared Isaacman, NASA administrator, said in a statement. \"Clearing outdated infrastructure allows NASA to safely modernize, streamline operations and fully leverage the infrastructure investments signed into law by President Trump to keep Marshall positioned at the forefront of aerospace innovation.\"Read full article Comments",
          "feed_position": 13,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/news-010826c-lg-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/news-010826c-lg-1152x648.jpg",
      "popularity_score": 158.06586638888888
    },
    {
      "id": "cluster_66",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 15:59:15 +0000",
      "title": "The Chevrolet Bolt is back... but for how long?",
      "neutral_headline": "The Chevrolet Bolt is back... but for how long",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/cars/2026/01/the-chevrolet-bolt-is-back-but-for-how-long/",
          "published_at": "Mon, 12 Jan 2026 15:59:15 +0000",
          "title": "The Chevrolet Bolt is back... but for how long?",
          "standfirst": "The new LFP battery pack has 262 miles of range and fast-charges at 150 kW.",
          "content": "The new Chevrolet Equinox EV is a solid entry into the compact crossover market, and with a (just) sub-$35,000 starting price, it also counts as affordable by the standards of 2026. But if you think that's too rich for your blood, or that the Equinox is still too large for your needs, take heart—the Chevrolet Bolt is back in dealerships now as well. The Bolt was GM's first modern electric vehicle, following on from the hand-built, pre-lithium ion EV1 and the compliance car that was the Spark EV. We're big fans of the Bolt here at Ars Technica. It offered well more than 200 miles of range in a mass-produced EV at a reasonable price well before Tesla's Model 3 started clogging up our roads, it got more efficient over time, and it managed to be fun to drive in the process. General Motors (which owns Chevrolet) probably feels less well-disposed toward the Bolt. It lost thousands of dollars on each car it sold, even before the entire fleet had to be recalled for a costly battery replacement. The issue was due to improperly folded tabs on some cells that could cause a battery fire, giving GM (and its battery partner LG) plenty of bad press in the process. That recall alone cost $1.8 billion.Read full article Comments",
          "feed_position": 11,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/Chevrolet-Bolt-2027-DriverRear-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/Chevrolet-Bolt-2027-DriverRear-1152x648.jpg",
      "popularity_score": 148.69281083333334
    },
    {
      "id": "cluster_64",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 16:32:21 +0000",
      "title": "UK probes X over Grok CSAM scandal; Elon Musk cries censorship",
      "neutral_headline": "UK probes X over Grok CSAM scandal; Elon Musk cries censorship",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2026/01/uk-investigating-x-after-grok-undressed-thousands-of-women-and-children/",
          "published_at": "Mon, 12 Jan 2026 16:32:21 +0000",
          "title": "UK probes X over Grok CSAM scandal; Elon Musk cries censorship",
          "standfirst": "Grok tests if UK can penalize platforms for sexualized deepfakes generated by AI.",
          "content": "Elon Musk's X is currently under investigation in the United Kingdom after failing to stop the platform's chatbot, Grok, from generating thousands of sexualized images of women and children. On Monday, UK media regulator Ofcom confirmed that X may have violated the UK's Online Safety Act, which requires platforms to block illegal content. The proliferation of \"undressed images of people\" by X users may amount to intimate image abuse, pornography, and child sexual abuse material (CSAM), the regulator said. And X may also have neglected its duty to stop kids from seeing porn. \"Reports of Grok being used to create and share illegal non-consensual intimate images and child sexual abuse material on X have been deeply concerning,\" an Ofcom spokesperson said. \"Platforms must protect people in the UK from content that’s illegal in the UK, and we won’t hesitate to investigate where we suspect companies are failing in their duties, especially where there’s a risk of harm to children.\"Read full article Comments",
          "feed_position": 10,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2246892016-1024x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2246892016-1024x648.jpg",
      "popularity_score": 146.2444775
    },
    {
      "id": "cluster_67",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 15:55:20 +0000",
      "title": "New research shows how shunning ultraprocessed foods helps with aging",
      "neutral_headline": "New research shows how shunning ultraprocessed foods helps with aging",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/health/2026/01/avoiding-ultraprocessed-foods-supports-healthier-aging/",
          "published_at": "Mon, 12 Jan 2026 15:55:20 +0000",
          "title": "New research shows how shunning ultraprocessed foods helps with aging",
          "standfirst": "Studies have linked ultraprocessed foods to poor health outcomes.",
          "content": "Older adults can dramatically reduce the amount of ultraprocessed foods they eat while keeping a familiar, balanced diet—and this shift leads to improvements across several key markers related to how the body regulates appetite and metabolism. That’s the main finding of a new study my colleagues and I published in the journal Clinical Nutrition. Ultraprocessed foods are made using industrial techniques and ingredients that aren’t typically used in home cooking. They often contain additives such as emulsifiers, flavorings, colors, and preservatives. Common examples include packaged snacks, ready-to-eat meals, and some processed meats. Studies have linked diets high in ultraprocessed foods to poorer health outcomes. My team and I enrolled Americans ages 65 and older in our study, many of whom were overweight or had metabolic risk factors such as insulin resistance or high cholesterol. Participants followed two diets low in ultraprocessed foods for eight weeks each. One included lean red meat (pork); the other was vegetarian with milk and eggs. For two weeks in between, participants returned to their usual diets.Read full article Comments",
          "feed_position": 12,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/ultraprocessed-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/ultraprocessed-1152x648.jpg",
      "popularity_score": 145.62753305555555
    },
    {
      "id": "cluster_86",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 11:30:06 +0000",
      "title": "The most fascinating monitors at CES 2026",
      "neutral_headline": "The most fascinating monitors at CES 2026",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2026/01/the-most-fascinating-monitors-at-ces-2026/",
          "published_at": "Mon, 12 Jan 2026 11:30:06 +0000",
          "title": "The most fascinating monitors at CES 2026",
          "standfirst": "Big sizes, big resolution, and big ideas.",
          "content": "CES 2026 took place in Las Vegas last week, and as usual, we're looking at the most interesting monitors from the show. Not every display is a monitor in the strictest sense, but they all provide a display for computers and have a unique twist that make them worth exploring. Dell’s massive UltraSharp Dell's biggest UltraSharp has a 21:9 aspect ratio. Credit: Dell It was a pretty safe bet that Dell would announce new UltraSharp monitors at CES. The displays are a solid recommendation for reliable USB-C monitors, including for Mac users and people needing something polished for professional or creative work. In recent years, UltraSharp monitors have also boasted more modern features, including integrated web cameras and IPS Black tech. This year, the strategy was clear: Bigger is better.Read full article Comments",
          "feed_position": 14,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/TCXAIO.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/TCXAIO.jpg",
      "popularity_score": 138.2069775
    },
    {
      "id": "cluster_98",
      "coverage": 1,
      "updated_at": "Sun, 11 Jan 2026 20:35:33 +0000",
      "title": "That time Will Smith helped discover new species of anaconda",
      "neutral_headline": "That time Will Smith helped discover new species of anaconda",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2026/01/that-time-will-smith-helped-discover-new-species-of-anaconda/",
          "published_at": "Sun, 11 Jan 2026 20:35:33 +0000",
          "title": "That time Will Smith helped discover new species of anaconda",
          "standfirst": "Footage of the 2024 discovery appears in NatGeo's new documentary series Pole to Pole with Will Smith.",
          "content": "In 2024, scientists announced the discovery of a new species of giant anaconda in South America. A National Geographic camera crew was on hand for the 2022 expedition that documented the new species—and so was actor Will Smith, since they were filming for NatGeo's new documentary series, Pole to Pole with Will Smith. Now we can all share in Smith's Amazon experience, courtesy of the three-minute clip above. Along with venom expert Bryan Fry, we follow Smith's journey by boat with a team of indigenous Waorani guides, scouring the river banks for anacondas. And they find one: a female green anaconda about 16 to 17 feet long, \"pure muscle.\" The Waorani secure the giant snake—anacondas aren't venomous but they do bite—so that Fry (with Smith's understandably reluctant help) can collect a scale sample for further analysis. Fry says that this will enable him to determine the accumulation of pollutants in the water. That and other collected samples also enabled scientists to conduct the genetic analysis that resulted in the declaration of a new species: the northern green anaconda (Eunectes akayama, which roughly translates to \"the great snake\"). It is genetically distinct from the southern green anaconda (Eunectes murinus); the two species likely diverged some 10 million years ago. The northern green anaconda's turf includes Venezuela, Colombia, Suriname, French Guyana, and the northern part of Brazil.Read full article Comments",
          "feed_position": 15,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/anaconda3-1152x648-1768069746.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/anaconda3-1152x648-1768069746.jpg",
      "popularity_score": 133
    },
    {
      "id": "cluster_105",
      "coverage": 1,
      "updated_at": "Sun, 11 Jan 2026 12:00:50 +0000",
      "title": "The oceans just keep getting hotter",
      "neutral_headline": "The oceans just keep getting hotter",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2026/01/the-oceans-just-keep-getting-hotter/",
          "published_at": "Sun, 11 Jan 2026 12:00:50 +0000",
          "title": "The oceans just keep getting hotter",
          "standfirst": "For the eighth year in a row, the world’s oceans absorbed a record-breaking amount of heat in 2025.",
          "content": "Since 2018, a group of researchers from around the world has crunched the numbers on how much heat the world’s oceans are absorbing each year. In 2025, their measurements broke records once again, making this the eighth year in a row that the world’s oceans have absorbed more heat than in the years before. The study, which was published Friday in the journal Advances in Atmospheric Science, found that the world’s oceans absorbed an additional 23 zettajoules’ worth of heat in 2025, the most in any year since modern measurements began in the 1960s. That’s significantly higher than the 16 additional zettajoules they absorbed in 2024. The research comes from a team of more than 50 scientists across the United States, Europe, and China. A joule is a common way to measure energy. A single joule is a relatively small unit of measurement—it’s about enough to power a tiny lightbulb for a second, or slightly heat a gram of water. But a zettajoule is one sextillion joules; numerically, the 23 zettajoules the oceans absorbed this year can be written out as 23,000,000,000,000,000,000,000.Read full article Comments",
          "feed_position": 16,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2023/04/sun-over-ocean-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2023/04/sun-over-ocean-1152x648.jpg",
      "popularity_score": 130
    }
  ]
}