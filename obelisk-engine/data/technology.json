{
  "updated_at": "2025-12-31T11:19:16.798Z",
  "clusters": [
    {
      "id": "cluster_4",
      "coverage": 2,
      "updated_at": "Wed, 31 Dec 2025 10:01:26 +0000",
      "title": "The best cameras for 2026",
      "neutral_headline": "The best cameras for 2026",
      "items": [
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/cameras/best-cameras-151524327.html",
          "published_at": "Wed, 31 Dec 2025 10:01:26 +0000",
          "title": "The best cameras for 2026",
          "standfirst": "Choosing a camera is not just about resolution or price. It is about finding something that matches how you like to shoot. Some photographers want a compact mirrorless camera that travels easily. Others want an action cam that can handle rough hikes, or a full-frame system that delivers the best possible image quality. With so many options today, there is a camera built for almost every creative style.We tested the top models across categories to help you decide which one fits your needs. Whether you are filming your first vlog, shooting portraits or capturing fast-moving action, these are the best cameras to help you grow your skills. Best cameras for 2026 Best mirrorless cameras Mirrorless is the largest camera category in terms of models available, so it’s the best way to go if you’re looking for something with the most advanced features. Canon and Nikon recently announced they’re discontinuing development of new DSLRs, simply because most of the advantages of that category are gone, as I detailed in a video. The biggest selling feature of a mirrorless camera is the ability to change lenses depending on the type of shooting you want to do. Best action camera or gimbal camera The most important features to look for in an action cam are image quality, stabilization and battery life. GoPro has easily been beating all rivals recently in all those areas, but DJI has taken a lot of its business with the Osmo Pocket 3 gimbal camera. Best compact camera This category has fewer cameras than it did even a few years ago and many models are older, as manufacturers focus instead on mirrorless models. However, I’m still a big believer in compact cameras. They’re a noticeable step up from smartphones quality-wise, and a lot of people will take a compact traveling or to events when they’d never bother with the hassle of a DSLR or mirrorless camera. Compacts largely have type 1-inch sensors, but a few offer larger options, particularly Fujifilm’s XF-100V. Another popular model, Sony’s XV-1, is primarily aimed at content creators looking to step up. In any case, desirable qualities include image quality, a fast lens, relatively long zoom, flip-out display, good battery life, a high quality EVF, decent video and good pocketability. What to consider before choosing a camera Though smartphones get better for video and photos every year, full cameras still have an edge in many ways. The larger sensors in mirrorless cameras let more light in, and you have a wide choice of lenses with far superior optics. Dedicated cameras are also faster for shooting things like sports or wildlife, offer superior video for content creators and create more professional results. Sensor size There are a few key things to consider to get the most out of a camera. The first is sensor size: in general, the larger the sensor, the better (and usually more expensive) the camera. Full frame is the largest sensor size for mainstream cameras, and it’s available on models like the new Panasonic S9, the Nikon Z III and Canon EOS R5 II. At a size equivalent to 35mm film (36 x 24mm), it offers the best performance in terms of image quality, low-light capability and depth of field. But it’s also very expensive and finicky. While bokeh looks incredible at an aperture of f/1.4, the depth of field is so razor thin that your subject's eyebrow might be in focus but not their eye. This can also make shooting video difficult. The next size category is APS-C (around 23.5 x 15.6mm for most models and 22.2 x 14.8mm for Canon), offered on Fujifilm's X Series lineup, the Canon R10, the Sony ZV-E10 II and the Nikon Z50. It's cheaper than full frame, both for the camera body and lenses, but still brings most of the advantages like decent bokeh, high ISOs for low-light shooting and relatively high resolution. With a sensor size the same as movie cameras, it's ideal for shooting video, and it’s easier to hold focus than with full-frame cameras. Micro Four Thirds (17.3 x 13mm), a format shared by Panasonic and Olympus, is the next step down in sensor size. It offers less bokeh and light-gathering capability than APS-C and full frame, but allows for smaller and lighter cameras and lenses. For video, you can still get reasonably tight depth of field with good prime lenses, but focus is easier to control. The other common sensor size is Type 1 (1 inch), which is actually smaller than one inch at 12.7 x 9.5mm. That's used mostly by compact models like Sony’s ZV-1 vlogging camera. Finally, action cameras like the GoPro Hero 11 and DJI’s Osmo 3 have even smaller sensors (1/1.9 and 1/1.7 inches, respectively). Autofocus For photographers, another key factor is autofocus (AF) speed and accuracy. Most modern mirrorless cameras have hybrid phase-detect AF systems that allow for rapid focus and fast burst speeds. The majority also offer AI features like eye-detect AF for people and animals, which locks in on the subject’s eyes, face or body to keep them in focus. However, some models are faster and more reactive than others. Displays The electronic viewfinder (EVF) and rear display are also crucial. The best models have the sharpest and brightest EVFs that help you judge a shot before taking it. For things like street photography, it’s best to have as bright and sharp a rear display as possible, so it’s easy to see your subject and check focus in all manner of lighting conditions. You may also want a screen that flips out rather than just tilting, too. Lenses DSLRs and mirrorless cameras let you change lenses, but you're stuck with what's built into a compact camera. While that's great for portability, a single lens means you're going to sacrifice something along the way. The Fujifilm X100V, for instance, has a fast but fixed 35mm-equivalent f/2.0 lens and no zoom. The Sony RX100 V has a 24-70mm zoom, but it's slower at the telephoto end (f/2.8) and less sharp than a fixed focal (prime) lens. When choosing a lens for a mirrorless camera, you’ll need to consider the focal or zoom length, along with the minimum aperture. Smaller numbers like f/1.4 for a prime lens or f/2.8 for a zoom are best, as they let you work in darker environments and maximize background blur to isolate your subject. However, those lenses are more complex and thus more expensive. Video recording When it comes to video, there are other factors to consider. Some cameras combine or skip over pixels (line skipping or pixel binning) for video recording, which is not ideal because it can reduce sharpness. Better cameras tend to read out the entire sensor and then “downsample” to improve video sharpness (camera manufacturers don’t often say if video is pixel binned, but will say if it’s downsampled). Another important factor is sensor speed, as slower sensors tend to have more rolling shutter that can create a “jello” effect that skews video. In addition, how’s the battery life? How do you like the handling and feel? How long can you shoot before the camera heats up or stops? Does it support 10-bit HDR video? Is there a microphone and/or a headphone jack? (If you record a lot of interviews, it's preferable to have both.) How's the video autofocus? All of these things play a part in your decision.This article originally appeared on Engadget at https://www.engadget.com/cameras/best-cameras-151524327.html?src=rss",
          "content": "Choosing a camera is not just about resolution or price. It is about finding something that matches how you like to shoot. Some photographers want a compact mirrorless camera that travels easily. Others want an action cam that can handle rough hikes, or a full-frame system that delivers the best possible image quality. With so many options today, there is a camera built for almost every creative style.We tested the top models across categories to help you decide which one fits your needs. Whether you are filming your first vlog, shooting portraits or capturing fast-moving action, these are the best cameras to help you grow your skills. Best cameras for 2026 Best mirrorless cameras Mirrorless is the largest camera category in terms of models available, so it’s the best way to go if you’re looking for something with the most advanced features. Canon and Nikon recently announced they’re discontinuing development of new DSLRs, simply because most of the advantages of that category are gone, as I detailed in a video. The biggest selling feature of a mirrorless camera is the ability to change lenses depending on the type of shooting you want to do. Best action camera or gimbal camera The most important features to look for in an action cam are image quality, stabilization and battery life. GoPro has easily been beating all rivals recently in all those areas, but DJI has taken a lot of its business with the Osmo Pocket 3 gimbal camera. Best compact camera This category has fewer cameras than it did even a few years ago and many models are older, as manufacturers focus instead on mirrorless models. However, I’m still a big believer in compact cameras. They’re a noticeable step up from smartphones quality-wise, and a lot of people will take a compact traveling or to events when they’d never bother with the hassle of a DSLR or mirrorless camera. Compacts largely have type 1-inch sensors, but a few offer larger options, particularly Fujifilm’s XF-100V. Another popular model, Sony’s XV-1, is primarily aimed at content creators looking to step up. In any case, desirable qualities include image quality, a fast lens, relatively long zoom, flip-out display, good battery life, a high quality EVF, decent video and good pocketability. What to consider before choosing a camera Though smartphones get better for video and photos every year, full cameras still have an edge in many ways. The larger sensors in mirrorless cameras let more light in, and you have a wide choice of lenses with far superior optics. Dedicated cameras are also faster for shooting things like sports or wildlife, offer superior video for content creators and create more professional results. Sensor size There are a few key things to consider to get the most out of a camera. The first is sensor size: in general, the larger the sensor, the better (and usually more expensive) the camera. Full frame is the largest sensor size for mainstream cameras, and it’s available on models like the new Panasonic S9, the Nikon Z III and Canon EOS R5 II. At a size equivalent to 35mm film (36 x 24mm), it offers the best performance in terms of image quality, low-light capability and depth of field. But it’s also very expensive and finicky. While bokeh looks incredible at an aperture of f/1.4, the depth of field is so razor thin that your subject's eyebrow might be in focus but not their eye. This can also make shooting video difficult. The next size category is APS-C (around 23.5 x 15.6mm for most models and 22.2 x 14.8mm for Canon), offered on Fujifilm's X Series lineup, the Canon R10, the Sony ZV-E10 II and the Nikon Z50. It's cheaper than full frame, both for the camera body and lenses, but still brings most of the advantages like decent bokeh, high ISOs for low-light shooting and relatively high resolution. With a sensor size the same as movie cameras, it's ideal for shooting video, and it’s easier to hold focus than with full-frame cameras. Micro Four Thirds (17.3 x 13mm), a format shared by Panasonic and Olympus, is the next step down in sensor size. It offers less bokeh and light-gathering capability than APS-C and full frame, but allows for smaller and lighter cameras and lenses. For video, you can still get reasonably tight depth of field with good prime lenses, but focus is easier to control. The other common sensor size is Type 1 (1 inch), which is actually smaller than one inch at 12.7 x 9.5mm. That's used mostly by compact models like Sony’s ZV-1 vlogging camera. Finally, action cameras like the GoPro Hero 11 and DJI’s Osmo 3 have even smaller sensors (1/1.9 and 1/1.7 inches, respectively). Autofocus For photographers, another key factor is autofocus (AF) speed and accuracy. Most modern mirrorless cameras have hybrid phase-detect AF systems that allow for rapid focus and fast burst speeds. The majority also offer AI features like eye-detect AF for people and animals, which locks in on the subject’s eyes, face or body to keep them in focus. However, some models are faster and more reactive than others. Displays The electronic viewfinder (EVF) and rear display are also crucial. The best models have the sharpest and brightest EVFs that help you judge a shot before taking it. For things like street photography, it’s best to have as bright and sharp a rear display as possible, so it’s easy to see your subject and check focus in all manner of lighting conditions. You may also want a screen that flips out rather than just tilting, too. Lenses DSLRs and mirrorless cameras let you change lenses, but you're stuck with what's built into a compact camera. While that's great for portability, a single lens means you're going to sacrifice something along the way. The Fujifilm X100V, for instance, has a fast but fixed 35mm-equivalent f/2.0 lens and no zoom. The Sony RX100 V has a 24-70mm zoom, but it's slower at the telephoto end (f/2.8) and less sharp than a fixed focal (prime) lens. When choosing a lens for a mirrorless camera, you’ll need to consider the focal or zoom length, along with the minimum aperture. Smaller numbers like f/1.4 for a prime lens or f/2.8 for a zoom are best, as they let you work in darker environments and maximize background blur to isolate your subject. However, those lenses are more complex and thus more expensive. Video recording When it comes to video, there are other factors to consider. Some cameras combine or skip over pixels (line skipping or pixel binning) for video recording, which is not ideal because it can reduce sharpness. Better cameras tend to read out the entire sensor and then “downsample” to improve video sharpness (camera manufacturers don’t often say if video is pixel binned, but will say if it’s downsampled). Another important factor is sensor speed, as slower sensors tend to have more rolling shutter that can create a “jello” effect that skews video. In addition, how’s the battery life? How do you like the handling and feel? How long can you shoot before the camera heats up or stops? Does it support 10-bit HDR video? Is there a microphone and/or a headphone jack? (If you record a lot of interviews, it's preferable to have both.) How's the video autofocus? All of these things play a part in your decision.This article originally appeared on Engadget at https://www.engadget.com/cameras/best-cameras-151524327.html?src=rss",
          "feed_position": 0
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/orchestration/why-meta-bought-manus-and-what-it-means-for-your-enterprise-ai-agent",
          "published_at": "Tue, 30 Dec 2025 17:00:00 GMT",
          "title": "Why Meta bought Manus — and what it signals for your enterprise AI agent strategy",
          "standfirst": "Facebook and Instagram parent company Meta’s agreement to acquire Manus for more than $2 billion — announced last night by both companies and reported in The Wall Street Journal — marks one of the clearest signals yet that large tech platforms are no longer just competing on model quality, but on who controls the execution layer of AI-powered work.Manus, a Singapore-based startup founded by Chinese entrepreneurs that debuted earlier this year, has built a general-purpose AI agent designed to autonomously carry out multi-step tasks such as research, analysis, coding, planning, and content generation. The company will continue operating from Singapore and selling its subscription product while its team and technology are integrated into Meta’s broader AI organization. Manus co-founder and CEO Xiao Hong, who goes by “Red,” is expected to report to Meta COO Javier Olivan.The deal arrives as Meta accelerates its AI investments to compete with Google, Microsoft, and OpenAI — and as the industry’s focus shifts from conversational demos to systems that can reliably produce artifacts, complete workflows, and operate with minimal supervision.Manus as an execution layer, not a chat interfaceManus has consistently positioned itself less as an assistant and more as an execution engine. Rather than answering isolated prompts, its agent is designed to plan tasks, invoke tools, iterate on intermediate outputs, and deliver finished work.It gained 2 million users on its waitlist alone after unveiling itself in spring 2025. At that time, Manus outperformed OpenAI&#x27;s Deep Research agent (powered then by the o3 model) and other state-of-the-art systems on the GAIA benchmark, which evaluates how well AI agents complete real-world, multi-step tasks, by more than 10% in some cases.And in the acquisition announcement last night, Manus said its system has processed more than 147 trillion tokens and created over 80 million virtual computers, metrics that suggest sustained, production-level usage rather than limited experimentation. Meta, meanwhile, said Manus can independently execute complex tasks such as market research, coding, and data analysis, and confirmed it will continue operating and selling the Manus service while integrating it into Meta AI and other products.For enterprises, this distinction matters. Many early “agent” systems fail not because the underlying models can’t reason, but because execution breaks down: tools fail silently, intermediate steps drift, or long-running tasks can’t be resumed or audited. Manus’s core value proposition is that it manages those failure modes.What Manus users were actually doing with the agentEvidence of that execution-first positioning shows up clearly in Manus’s own community. In the official Manus Discord server, a “Use Case Channel” post shared by a community member named Yesly on March 6, 2025 catalogued real examples of how users were already deploying the agent.Those use cases went far beyond casual prompting. They included:Generating long-form research reports, such as a detailed analysis of climate change impacts on Earth and human society over the next centuryProducing data-driven visual artifacts, including an NBA scoring efficiency four-quadrant chart based on player statisticsConducting product and market research, such as comparing every MacBook model across Apple’s historyPlanning and synthesizing complex, multi-country travel itineraries, complete with budget estimates, accommodations, and a generated travel handbookTackling technical and academic tasks, including summarizing high-temperature superconductivity research, proposing PhD research directions, and outlining simulation-based approaches to room-temperature superconductorsDrafting structured proposals, such as designs for a solar-powered, self-sufficient home with defined geographic coordinates and engineering constraintsEach example was shared as a replayable Manus session, reinforcing that the system wasn’t just generating text, but orchestrating multi-step work to produce finished outputs.This pattern matters because it shows Manus operating in the messy middle ground where enterprise AI often stalls: tasks that are too complex for a single prompt, but too open-ended for rigid automation.Manus&#x27;s recent updatesThe pace at which Manus shipped updates was also impressive, which likely added to its momentum with users and as a ripe acquisition target for Meta.In October, the company released Manus 1.5, an update aimed squarely at where early agent systems tended to break down: long, brittle tasks that lost context or stalled halfway through. Manus re-architected its core agent engine and saw immediate gains. The company said average task completion times dropped from roughly 15 minutes earlier in the year to under four minutes, nearly a fourfold speedup. The system dynamically allocated more reasoning time and compute to harder problems instead of treating every task the same. Manus also expanded the agent’s context windows, enabling it to track longer conversations and more intricate workflows without dropping key details. Together, those changes reduced outright task failures and improved output quality for research-heavy, analytical, and multi-step jobs that previously required frequent human intervention.In December, Manus built on that foundation with version 1.6, extending those execution gains into more autonomous, creative, and platform-spanning work. The release introduced a higher-performance agent tuned to complete more tasks successfully in a single pass, along with new support for mobile application development, not just web-based projects. Users could describe a mobile app and have the agent handle the end-to-end build process, expanding Manus’s reach beyond the browser. At the same time, the agent carried creative objectives across an entire production arc — from research and ideation to drafting, visual creation, revision, and final delivery — within one continuous session.That included generating and editing images through a visual interface, assembling presentations and reports, and building full-stack web applications the agent could launch, test, and fix on its own. Taken together, the updates reinforced Manus’s positioning not as a prompt-driven assistant, but as an execution system designed to stay with a job, adapt when things broke, and reliably deliver finished work across analytical, creative, web, and mobile workflows.Application-layer traction over proprietary modelsNotably, Manus does not train its own frontier model. Reporting on the deal says it relies on third-party AI models from providers including Anthropic and Alibaba, focusing its differentiation on orchestration, reliability, and execution.That hasn’t prevented commercial traction. Yuchen Jin, co-founder and chief technology officer (CTO) of AI cloud GPU-as-a-service provider Hyperbolic Labs, highlighted this dynamic in a public post discussing the acquisition. Jin noted that Manus by its own admission reached roughly $100 million in annual recurring revenue just eight months after launch, despite having no proprietary large language model (LLM) of its own, relying on the aforementioned providers.“People keep assuming a small update from OpenAI or Google will wipe out a lot of AI startups,” Jin wrote. “But in reality, the AI application layer should be where most of the opportunity is.”A similar interpretation came from Dev Shah, lead developer relations at Resemble AI, who argued that Meta didn’t acquire a model company so much as an “environment company” and that “intelligence cannot exist in isolation.\"His point? Agentic capability emerges from how models are coupled with tools, memory, and execution environments — a new concept he described as “Situated Agency.” From that perspective, Manus’s achievement was not training a proprietary foundation model, but engineering an execution layer that allows models like Claude to browse the web, write and run code, manipulate files, and complete multi-step workflows autonomously. Shah suggested this may align more closely with Meta’s long-term strategy: rather than winning the race for state-of-the-art models, Meta could focus on owning the agentic infrastructure — the orchestration, context engineering, and interfaces — and swap in whichever model performs best over time. If that thesis holds, the Manus acquisition signals a shift toward treating foundation models as interchangeable inputs, while the execution environment becomes the primary source of durable value.These perspectives help explain Meta’s move. Rather than buying another model team, it is acquiring a system that has already proven it can package existing models into a product users will pay for — and keep using.What this means for your enterprise AI strategyFor enterprise technical decision-makers, the Manus acquisition is less a vendor endorsement and more a strategic signal.First, it reinforces that orchestration layers — systems that manage planning, tools, retries, memory, and monitoring — are becoming as important as the models themselves. Enterprises building internal AI capabilities may want to invest more heavily in agent infrastructure that sits above models and can survive rapid shifts in the underlying model ecosystem.In that sense, building an internal agent layer is not speculative or redundant. It is exactly the class of software that large platforms now view as strategically valuable — whether as acquisition targets or as internal accelerators.A video recorded ahead of this announcement by VentureBeat founder and CEO Matt Marshall and Red Dragon co-founder Witteveen delves deeper into this subject. Watch it free below or on YouTube.Second, the deal does not automatically mean enterprises should rush to standardize on Manus itself. Meta’s history with enterprise products gives reason for caution. Tools like Workplace by Facebook gained early adoption but ultimately failed to become deeply embedded enterprise platforms, in part due to shifting internal priorities and inconsistent long-term investment.That history suggests a measured approach. Enterprises evaluating Manus today may want to treat it as a pilot or adjunct tool, not a foundational dependency, until Meta’s integration strategy becomes clearer. Key questions include whether Manus remains product-led rather than ad- or data-driven, how governance and compliance evolve under Meta, and whether the roadmap continues to prioritize execution reliability over surface-level integration.Finally, the acquisition sharpens a broader choice facing enterprises: whether to wait for vendors to define the agent layer, or to build and control it themselves. Manus’s trajectory suggests that the real leverage in AI increasingly lives not in who owns the smartest model, but in who owns the systems that turn reasoning into completed work.In that light, Meta’s acquisition is less about Manus alone — and more about where the next durable layer of the AI stack is taking shape.Why this deal matters beyond MetaFrom the perspective of some of us here at VentureBeat, the Manus acquisition is best read as confirmation of where value is consolidating in the AI stack (and Meta’s enterprise AI agent ambitions, though the latter is far less assured.)The defining signal is not that Manus built novel models, but that it demonstrated how quickly well-designed agents can be turned into revenue-generating products by focusing on execution, speed, and concrete outcomes. That shift — from debating what frontier models can do to measuring what agents actually deliver — increasingly frames how AI progress is evaluated.The deal also sharpens an important distinction for enterprise readers: this is not primarily about adopting a Meta-backed product, but about recognizing that agent orchestration has become strategically material. Manus succeeded by targeting tractable, real-world tasks and shipping agents that worked end to end, even if those use cases skewed more consumer-oriented. The broader implication is that enterprises can apply the same approach in their own domains, building agent systems where they already possess data, expertise, and operational leverage.At the same time, we&#x27;re cautious about reading this as a direct enterprise buying signal. Meta’s history suggests that long-term enterprise trust is difficult to earn without sustained focus and specialized go-to-market muscle. Where the acquisition may make more immediate sense is on the consumer and small-business side of Meta’s own ecosystem, particularly within products already designed to manage commerce, content, and customer interaction at scale. Manus’s agentic capabilities map cleanly onto surfaces like Meta Business Suite, where small businesses already juggle content calendars, inboxes, ads, analytics, and monetization tools across Facebook and Instagram. An execution-oriented agent could plausibly automate or coordinate many of those tasks end to end, from drafting and scheduling posts to responding to messages, optimizing ads, or assembling performance reports.Manus&#x27;s \"Design View\" feature, which launched publicly just a week prior to the Meta acquisition announcement and allows users to generate new imagery with editable discrete components using natural language, would seem to be tailor-made for a social network ad creation experience:Beyond creators and small businesses, those agents could plausibly extend to everyday users navigating Instagram or Facebook for shopping, discovery, or personal expression. An execution-oriented agent could assist regular users with tasks such as browsing and comparing products, managing purchases, assembling wish lists, or coordinating returns, while also helping them create and edit posts, reels, or stories for friends and family — not as professional content, but as casual, social, and entertainment-driven output.That framing aligns closely with Meta’s historical strengths. The company has been most successful when AI capabilities are tightly integrated into high-frequency consumer workflows rather than positioned as standalone enterprise software. A Manus-powered agent that helps users do things — shop, create, plan, or manage interactions inside Meta’s apps — would fit naturally into Instagram and Facebook’s evolution toward more agentic experiences. In that scenario, Manus functions less as an enterprise brand and more as an invisible execution layer, powering AI assistants that operate natively within Meta’s consumer ecosystem, where scale, engagement, and commerce already converge.As a result, the acquisition’s clearest relevance is not whether enterprises should standardize on Manus, but that investments in internal agent frameworks, orchestration layers, and governance now appear increasingly well-justified — because that is precisely the layer large platforms are now willing to pay for.",
          "content": "Facebook and Instagram parent company Meta’s agreement to acquire Manus for more than $2 billion — announced last night by both companies and reported in The Wall Street Journal — marks one of the clearest signals yet that large tech platforms are no longer just competing on model quality, but on who controls the execution layer of AI-powered work.Manus, a Singapore-based startup founded by Chinese entrepreneurs that debuted earlier this year, has built a general-purpose AI agent designed to autonomously carry out multi-step tasks such as research, analysis, coding, planning, and content generation. The company will continue operating from Singapore and selling its subscription product while its team and technology are integrated into Meta’s broader AI organization. Manus co-founder and CEO Xiao Hong, who goes by “Red,” is expected to report to Meta COO Javier Olivan.The deal arrives as Meta accelerates its AI investments to compete with Google, Microsoft, and OpenAI — and as the industry’s focus shifts from conversational demos to systems that can reliably produce artifacts, complete workflows, and operate with minimal supervision.Manus as an execution layer, not a chat interfaceManus has consistently positioned itself less as an assistant and more as an execution engine. Rather than answering isolated prompts, its agent is designed to plan tasks, invoke tools, iterate on intermediate outputs, and deliver finished work.It gained 2 million users on its waitlist alone after unveiling itself in spring 2025. At that time, Manus outperformed OpenAI&#x27;s Deep Research agent (powered then by the o3 model) and other state-of-the-art systems on the GAIA benchmark, which evaluates how well AI agents complete real-world, multi-step tasks, by more than 10% in some cases.And in the acquisition announcement last night, Manus said its system has processed more than 147 trillion tokens and created over 80 million virtual computers, metrics that suggest sustained, production-level usage rather than limited experimentation. Meta, meanwhile, said Manus can independently execute complex tasks such as market research, coding, and data analysis, and confirmed it will continue operating and selling the Manus service while integrating it into Meta AI and other products.For enterprises, this distinction matters. Many early “agent” systems fail not because the underlying models can’t reason, but because execution breaks down: tools fail silently, intermediate steps drift, or long-running tasks can’t be resumed or audited. Manus’s core value proposition is that it manages those failure modes.What Manus users were actually doing with the agentEvidence of that execution-first positioning shows up clearly in Manus’s own community. In the official Manus Discord server, a “Use Case Channel” post shared by a community member named Yesly on March 6, 2025 catalogued real examples of how users were already deploying the agent.Those use cases went far beyond casual prompting. They included:Generating long-form research reports, such as a detailed analysis of climate change impacts on Earth and human society over the next centuryProducing data-driven visual artifacts, including an NBA scoring efficiency four-quadrant chart based on player statisticsConducting product and market research, such as comparing every MacBook model across Apple’s historyPlanning and synthesizing complex, multi-country travel itineraries, complete with budget estimates, accommodations, and a generated travel handbookTackling technical and academic tasks, including summarizing high-temperature superconductivity research, proposing PhD research directions, and outlining simulation-based approaches to room-temperature superconductorsDrafting structured proposals, such as designs for a solar-powered, self-sufficient home with defined geographic coordinates and engineering constraintsEach example was shared as a replayable Manus session, reinforcing that the system wasn’t just generating text, but orchestrating multi-step work to produce finished outputs.This pattern matters because it shows Manus operating in the messy middle ground where enterprise AI often stalls: tasks that are too complex for a single prompt, but too open-ended for rigid automation.Manus&#x27;s recent updatesThe pace at which Manus shipped updates was also impressive, which likely added to its momentum with users and as a ripe acquisition target for Meta.In October, the company released Manus 1.5, an update aimed squarely at where early agent systems tended to break down: long, brittle tasks that lost context or stalled halfway through. Manus re-architected its core agent engine and saw immediate gains. The company said average task completion times dropped from roughly 15 minutes earlier in the year to under four minutes, nearly a fourfold speedup. The system dynamically allocated more reasoning time and compute to harder problems instead of treating every task the same. Manus also expanded the agent’s context windows, enabling it to track longer conversations and more intricate workflows without dropping key details. Together, those changes reduced outright task failures and improved output quality for research-heavy, analytical, and multi-step jobs that previously required frequent human intervention.In December, Manus built on that foundation with version 1.6, extending those execution gains into more autonomous, creative, and platform-spanning work. The release introduced a higher-performance agent tuned to complete more tasks successfully in a single pass, along with new support for mobile application development, not just web-based projects. Users could describe a mobile app and have the agent handle the end-to-end build process, expanding Manus’s reach beyond the browser. At the same time, the agent carried creative objectives across an entire production arc — from research and ideation to drafting, visual creation, revision, and final delivery — within one continuous session.That included generating and editing images through a visual interface, assembling presentations and reports, and building full-stack web applications the agent could launch, test, and fix on its own. Taken together, the updates reinforced Manus’s positioning not as a prompt-driven assistant, but as an execution system designed to stay with a job, adapt when things broke, and reliably deliver finished work across analytical, creative, web, and mobile workflows.Application-layer traction over proprietary modelsNotably, Manus does not train its own frontier model. Reporting on the deal says it relies on third-party AI models from providers including Anthropic and Alibaba, focusing its differentiation on orchestration, reliability, and execution.That hasn’t prevented commercial traction. Yuchen Jin, co-founder and chief technology officer (CTO) of AI cloud GPU-as-a-service provider Hyperbolic Labs, highlighted this dynamic in a public post discussing the acquisition. Jin noted that Manus by its own admission reached roughly $100 million in annual recurring revenue just eight months after launch, despite having no proprietary large language model (LLM) of its own, relying on the aforementioned providers.“People keep assuming a small update from OpenAI or Google will wipe out a lot of AI startups,” Jin wrote. “But in reality, the AI application layer should be where most of the opportunity is.”A similar interpretation came from Dev Shah, lead developer relations at Resemble AI, who argued that Meta didn’t acquire a model company so much as an “environment company” and that “intelligence cannot exist in isolation.\"His point? Agentic capability emerges from how models are coupled with tools, memory, and execution environments — a new concept he described as “Situated Agency.” From that perspective, Manus’s achievement was not training a proprietary foundation model, but engineering an execution layer that allows models like Claude to browse the web, write and run code, manipulate files, and complete multi-step workflows autonomously. Shah suggested this may align more closely with Meta’s long-term strategy: rather than winning the race for state-of-the-art models, Meta could focus on owning the agentic infrastructure — the orchestration, context engineering, and interfaces — and swap in whichever model performs best over time. If that thesis holds, the Manus acquisition signals a shift toward treating foundation models as interchangeable inputs, while the execution environment becomes the primary source of durable value.These perspectives help explain Meta’s move. Rather than buying another model team, it is acquiring a system that has already proven it can package existing models into a product users will pay for — and keep using.What this means for your enterprise AI strategyFor enterprise technical decision-makers, the Manus acquisition is less a vendor endorsement and more a strategic signal.First, it reinforces that orchestration layers — systems that manage planning, tools, retries, memory, and monitoring — are becoming as important as the models themselves. Enterprises building internal AI capabilities may want to invest more heavily in agent infrastructure that sits above models and can survive rapid shifts in the underlying model ecosystem.In that sense, building an internal agent layer is not speculative or redundant. It is exactly the class of software that large platforms now view as strategically valuable — whether as acquisition targets or as internal accelerators.A video recorded ahead of this announcement by VentureBeat founder and CEO Matt Marshall and Red Dragon co-founder Witteveen delves deeper into this subject. Watch it free below or on YouTube.Second, the deal does not automatically mean enterprises should rush to standardize on Manus itself. Meta’s history with enterprise products gives reason for caution. Tools like Workplace by Facebook gained early adoption but ultimately failed to become deeply embedded enterprise platforms, in part due to shifting internal priorities and inconsistent long-term investment.That history suggests a measured approach. Enterprises evaluating Manus today may want to treat it as a pilot or adjunct tool, not a foundational dependency, until Meta’s integration strategy becomes clearer. Key questions include whether Manus remains product-led rather than ad- or data-driven, how governance and compliance evolve under Meta, and whether the roadmap continues to prioritize execution reliability over surface-level integration.Finally, the acquisition sharpens a broader choice facing enterprises: whether to wait for vendors to define the agent layer, or to build and control it themselves. Manus’s trajectory suggests that the real leverage in AI increasingly lives not in who owns the smartest model, but in who owns the systems that turn reasoning into completed work.In that light, Meta’s acquisition is less about Manus alone — and more about where the next durable layer of the AI stack is taking shape.Why this deal matters beyond MetaFrom the perspective of some of us here at VentureBeat, the Manus acquisition is best read as confirmation of where value is consolidating in the AI stack (and Meta’s enterprise AI agent ambitions, though the latter is far less assured.)The defining signal is not that Manus built novel models, but that it demonstrated how quickly well-designed agents can be turned into revenue-generating products by focusing on execution, speed, and concrete outcomes. That shift — from debating what frontier models can do to measuring what agents actually deliver — increasingly frames how AI progress is evaluated.The deal also sharpens an important distinction for enterprise readers: this is not primarily about adopting a Meta-backed product, but about recognizing that agent orchestration has become strategically material. Manus succeeded by targeting tractable, real-world tasks and shipping agents that worked end to end, even if those use cases skewed more consumer-oriented. The broader implication is that enterprises can apply the same approach in their own domains, building agent systems where they already possess data, expertise, and operational leverage.At the same time, we&#x27;re cautious about reading this as a direct enterprise buying signal. Meta’s history suggests that long-term enterprise trust is difficult to earn without sustained focus and specialized go-to-market muscle. Where the acquisition may make more immediate sense is on the consumer and small-business side of Meta’s own ecosystem, particularly within products already designed to manage commerce, content, and customer interaction at scale. Manus’s agentic capabilities map cleanly onto surfaces like Meta Business Suite, where small businesses already juggle content calendars, inboxes, ads, analytics, and monetization tools across Facebook and Instagram. An execution-oriented agent could plausibly automate or coordinate many of those tasks end to end, from drafting and scheduling posts to responding to messages, optimizing ads, or assembling performance reports.Manus&#x27;s \"Design View\" feature, which launched publicly just a week prior to the Meta acquisition announcement and allows users to generate new imagery with editable discrete components using natural language, would seem to be tailor-made for a social network ad creation experience:Beyond creators and small businesses, those agents could plausibly extend to everyday users navigating Instagram or Facebook for shopping, discovery, or personal expression. An execution-oriented agent could assist regular users with tasks such as browsing and comparing products, managing purchases, assembling wish lists, or coordinating returns, while also helping them create and edit posts, reels, or stories for friends and family — not as professional content, but as casual, social, and entertainment-driven output.That framing aligns closely with Meta’s historical strengths. The company has been most successful when AI capabilities are tightly integrated into high-frequency consumer workflows rather than positioned as standalone enterprise software. A Manus-powered agent that helps users do things — shop, create, plan, or manage interactions inside Meta’s apps — would fit naturally into Instagram and Facebook’s evolution toward more agentic experiences. In that scenario, Manus functions less as an enterprise brand and more as an invisible execution layer, powering AI assistants that operate natively within Meta’s consumer ecosystem, where scale, engagement, and commerce already converge.As a result, the acquisition’s clearest relevance is not whether enterprises should standardize on Manus, but that investments in internal agent frameworks, orchestration layers, and governance now appear increasingly well-justified — because that is precisely the layer large platforms are now willing to pay for.",
          "feed_position": 0,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/7nVD4zIrY5Lmnpu1c0lGBG/c788567a8bc13386445b652f6bcbdde6/IQnl4wLLz4s3ciS8747gk.png?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/security/machine-identities-outnumber-humans-82-to-1-legacy-iam-cant-keep-up",
          "published_at": "Tue, 30 Dec 2025 14:00:00 GMT",
          "title": "Legacy IAM was built for humans — and AI agents now outnumber them 82 to 1",
          "standfirst": "Active Directory, LDAP, and early PAM were built for humans. AI agents and machines were the exception. Today, they outnumber people 82 to 1, and that human-first identity model is breaking down at machine speed.AI agents are the fastest-growing and least-governed class of these machine identities — and they don’t just authenticate, they act. ServiceNow spent roughly $11.6 billion on security acquisitions in 2025 alone — a signal that identity, not models, is becoming the control plane for enterprise AI risk.CyberArk&#x27;s 2025 research confirms what security teams and AI builders have long suspected: Machine identities now outnumber humans by a wide margin. Microsoft Copilot Studio users created over 1 million AI agents in a single quarter, up 130% from the previous period. Gartner predicts that by 2028, 25% of enterprise breaches will trace back to AI agent abuse.Why legacy architectures fail at machine scaleBuilders don’t create shadow agents or over-permissioned service accounts out of negligence. They do it because cloud IAM is slow, security reviews don’t map cleanly to agent workflows, and production pressure rewards speed over precision. Static credentials become the path of least resistance — until they become the breach vector.Gartner analysts explain the core problem in a report published in May: \"Traditional IAM approaches, designed for human users, fall short of addressing the unique requirements of machines, such as devices and workloads.\"Their research identifies why retrofitting fails: \"Retrofitting human IAM approaches to fit machine IAM use cases leads to fragmented and ineffective management of machine identities, running afoul of regulatory mandates and exposing the organization to unnecessary risks.\"The governance gap is stark. CyberArk&#x27;s 2025 Identity Security Landscape survey of 2,600 security decision-makers reveals a dangerous disconnect: Though machine identities now outnumber humans 82 to 1, 88% of organizations still define only human identities as \"privileged users.\" The result is that machine identities actually have higher rates of sensitive access than humans.That 42% figure represents millions of API keys, service accounts, and automated processes with access to crown jewels, all governed by policies designed for employees who clock in and out.The visibility gap compounds the problem. A Gartner survey of 335 IAM leaders found that IAM teams are only responsible for 44% of an organization&#x27;s machine identities, meaning the majority operate outside security&#x27;s visibility. Without a cohesive machine IAM strategy, Gartner warns, \"organizations risk compromising the security and integrity of their IT infrastructure.\"The Gartner Leaders&#x27; Guide explains why legacy service accounts create systemic risk: They persist after the workloads they support disappear, leaving orphaned credentials with no clear owner or lifecycle.In several enterprise breaches investigated in 2024, attackers didn’t compromise models or endpoints. They reused long-lived API keys tied to abandoned automation workflows — keys no one realized were still active because the agent that created them no longer existed.Elia Zaitsev, CrowdStrike&#x27;s CTO, explained why attackers have shifted away from endpoints and toward identity in a recent VentureBeat interview: \"Cloud, identity and remote management tools and legitimate credentials are where the adversary has been moving because it&#x27;s too hard to operate unconstrained on the endpoint. Why try to bypass and deal with a sophisticated platform like CrowdStrike on the endpoint when you could log in as an admin user?\"Why agentic AI breaks identity assumptionsThe emergence of AI agents requiring their own credentials introduces a category of machine identity that legacy systems never anticipated or were designed for. Gartner&#x27;s researchers specifically call out agentic AI as a critical use case: \"AI agents require credentials to interact with other systems. In some instances, they use delegated human credentials, while in others, they operate with their own credentials. These credentials must be meticulously scoped to adhere to the principle of least privilege.\"The researchers also cite the Model Context Protocol (MCP) as an example of this challenge, the same protocol security researchers have flagged for its lack of built-in authentication. MCP isn’t just missing authentication — it collapses traditional identity boundaries by allowing agents to traverse data and tools without a stable, auditable identity surface.The governance problem compounds when organizations deploy multiple GenAI tools simultaneously. Security teams need visibility into which AI integrations have action capabilities, including the ability to execute tasks, not just generate text, and whether those capabilities have been scoped appropriately. Platforms that unify identity, endpoint, and cloud telemetry are emerging as the only viable way to detect agent abuse in real time. Fragmented point tools simply can’t keep up with machine-speed lateral movement.Machine-to-machine interactions already operate at a scale and speed human governance models were never designed to handle.Getting ahead of dynamic service identity shifts Gartner&#x27;s research points to dynamic service identities as the path forward. They’re defined as being ephemeral, tightly scoped, policy-driven credentials that drastically reduce the attack surface. Because of this, Gartner is advising that security leaders \"move to a dynamic service identity model, rather than defaulting to a legacy service account model. Dynamic service identities do not require separate accounts to be created, thus reducing management overhead and the attack surface.\"The ultimate objective is achieving just-in-time access and zero standing privileges. Platforms that unify identity, endpoint, and cloud telemetry are increasingly the only viable way to detect and contain agent abuse across the full identity attack chain.Practical steps security and AI builders can take today The organizations getting agentic identity right are treating it as a collaboration problem between security teams and AI builders. Based on Gartner&#x27;s Leaders&#x27; Guide, OpenID Foundation guidance, and vendor best practices, these priorities are emerging for enterprises deploying AI agents.Conduct a comprehensive discovery and audit of every account and credential first. It’s a good idea to get a baseline in place first to see how many accounts and credentials are in use across all machines in IT. CISOs and security leaders tell VentureBeat that this often turns up between six and ten times more identities than the security team had known about before the audit. One hotel chain found that it had been tracking only a tenth of its machine identities before the audit.Build and tightly manage agent inventory before production. Being on top of this makes sure AI builders know what they&#x27;re deploying and security teams know what they need to track. When there is too much of a gap between those functions, it&#x27;s easier for shadow agents to get created, evading governance in the process. A shared registry should track ownership, permissions, data access, and API connections for every agentic identity before agents reach production environments.Go all in on dynamic service identities and excel at them. Transition from static service accounts to cloud-native alternatives like AWS IAM roles, Azure managed identities, or Kubernetes service accounts. These identities are ephemeral and need to be tightly scoped, managed and policy-driven. The goal is to excel at compliance while providing AI builders the identities they need to get apps built.Implement just-in-time credentials over static secrets. Integrating just-in-time credential provisioning, automatic secret rotation, and least-privilege defaults into CI/CD pipelines and agent frameworks is critical. These are all foundational elements of zero trust that need to be core to devops pipelines. Take the advice of seasoned security leaders defending AI builders, who often tell VentureBeat to pass along the advice of never trusting perimeter security with any AI devops workflows or CI/CD processes. Go big on zero trust and identity security when it comes to protecting AI builders’ workflows.Establish auditable delegation chains. When agents spawn sub-agents or invoke external APIs, authorization chains become hard to track. Make sure humans are accountable for all services, which include AI agents. Enterprises need behavioral baselines and real-time drift detection to maintain accountability.Deploy continuous monitoring. In keeping with the precepts of zero trust, continuously monitor every use of machine credentials with the deliberate goal of excelling at observability. This includes auditing as it helps detect anomalous activities such as unauthorized privilege escalation and lateral movement. Evaluate posture management. Assess potential exploitation pathways, the extent of possible damage (blast radius), and any shadow admin access. This involves removing unnecessary or outdated access and identifying misconfigurations that attackers could exploit. Start enforcing agent lifecycle management. Every agent needs human oversight, whether as part of a group of agents or in the context of an agent-based workflow. When AI builders move to new projects, their agents should trigger the same offboarding workflows as departing employees. Orphaned agents with standing privileges can become breach vectors.Prioritize unified platforms over point solutions. Fragmented tools create fragmented visibility. Platforms that unify identity, endpoint, and cloud security give AI builders self-service visibility while giving security teams cross-domain detection.Expect to see the gap widen in 2026The gap between what AI builders deploy and what security teams can govern keeps widening. Every major technology transition has, unfortunately, also led to another generation of security breaches often forcing its own unique industry-wide reckoning. Just as hybrid cloud misconfigurations, shadow AI, and API sprawl continue to challenge security leaders and the AI builders they support, 2026 will see the gap widen between what can be contained when it comes to machine identity attacks and what needs to improve to stop determined adversaries.The 82-to-1 ratio isn&#x27;t static. It&#x27;s accelerating. Organizations that continue relying on human-first IAM architectures aren&#x27;t just accepting technical debt; they&#x27;re building security models that grow weaker with every new agent deployed.Agentic AI doesn’t break security because it’s intelligent — it breaks security because it multiplies identity faster than governance can follow. Turning what for many organizations is one of their most glaring security weaknesses into a strength starts by realizing that perimeter-based, legacy identity security is no match for the intensity, speed, and scale of machine-on-machine attacks that are the new normal and will proliferate in 2026.",
          "content": "Active Directory, LDAP, and early PAM were built for humans. AI agents and machines were the exception. Today, they outnumber people 82 to 1, and that human-first identity model is breaking down at machine speed.AI agents are the fastest-growing and least-governed class of these machine identities — and they don’t just authenticate, they act. ServiceNow spent roughly $11.6 billion on security acquisitions in 2025 alone — a signal that identity, not models, is becoming the control plane for enterprise AI risk.CyberArk&#x27;s 2025 research confirms what security teams and AI builders have long suspected: Machine identities now outnumber humans by a wide margin. Microsoft Copilot Studio users created over 1 million AI agents in a single quarter, up 130% from the previous period. Gartner predicts that by 2028, 25% of enterprise breaches will trace back to AI agent abuse.Why legacy architectures fail at machine scaleBuilders don’t create shadow agents or over-permissioned service accounts out of negligence. They do it because cloud IAM is slow, security reviews don’t map cleanly to agent workflows, and production pressure rewards speed over precision. Static credentials become the path of least resistance — until they become the breach vector.Gartner analysts explain the core problem in a report published in May: \"Traditional IAM approaches, designed for human users, fall short of addressing the unique requirements of machines, such as devices and workloads.\"Their research identifies why retrofitting fails: \"Retrofitting human IAM approaches to fit machine IAM use cases leads to fragmented and ineffective management of machine identities, running afoul of regulatory mandates and exposing the organization to unnecessary risks.\"The governance gap is stark. CyberArk&#x27;s 2025 Identity Security Landscape survey of 2,600 security decision-makers reveals a dangerous disconnect: Though machine identities now outnumber humans 82 to 1, 88% of organizations still define only human identities as \"privileged users.\" The result is that machine identities actually have higher rates of sensitive access than humans.That 42% figure represents millions of API keys, service accounts, and automated processes with access to crown jewels, all governed by policies designed for employees who clock in and out.The visibility gap compounds the problem. A Gartner survey of 335 IAM leaders found that IAM teams are only responsible for 44% of an organization&#x27;s machine identities, meaning the majority operate outside security&#x27;s visibility. Without a cohesive machine IAM strategy, Gartner warns, \"organizations risk compromising the security and integrity of their IT infrastructure.\"The Gartner Leaders&#x27; Guide explains why legacy service accounts create systemic risk: They persist after the workloads they support disappear, leaving orphaned credentials with no clear owner or lifecycle.In several enterprise breaches investigated in 2024, attackers didn’t compromise models or endpoints. They reused long-lived API keys tied to abandoned automation workflows — keys no one realized were still active because the agent that created them no longer existed.Elia Zaitsev, CrowdStrike&#x27;s CTO, explained why attackers have shifted away from endpoints and toward identity in a recent VentureBeat interview: \"Cloud, identity and remote management tools and legitimate credentials are where the adversary has been moving because it&#x27;s too hard to operate unconstrained on the endpoint. Why try to bypass and deal with a sophisticated platform like CrowdStrike on the endpoint when you could log in as an admin user?\"Why agentic AI breaks identity assumptionsThe emergence of AI agents requiring their own credentials introduces a category of machine identity that legacy systems never anticipated or were designed for. Gartner&#x27;s researchers specifically call out agentic AI as a critical use case: \"AI agents require credentials to interact with other systems. In some instances, they use delegated human credentials, while in others, they operate with their own credentials. These credentials must be meticulously scoped to adhere to the principle of least privilege.\"The researchers also cite the Model Context Protocol (MCP) as an example of this challenge, the same protocol security researchers have flagged for its lack of built-in authentication. MCP isn’t just missing authentication — it collapses traditional identity boundaries by allowing agents to traverse data and tools without a stable, auditable identity surface.The governance problem compounds when organizations deploy multiple GenAI tools simultaneously. Security teams need visibility into which AI integrations have action capabilities, including the ability to execute tasks, not just generate text, and whether those capabilities have been scoped appropriately. Platforms that unify identity, endpoint, and cloud telemetry are emerging as the only viable way to detect agent abuse in real time. Fragmented point tools simply can’t keep up with machine-speed lateral movement.Machine-to-machine interactions already operate at a scale and speed human governance models were never designed to handle.Getting ahead of dynamic service identity shifts Gartner&#x27;s research points to dynamic service identities as the path forward. They’re defined as being ephemeral, tightly scoped, policy-driven credentials that drastically reduce the attack surface. Because of this, Gartner is advising that security leaders \"move to a dynamic service identity model, rather than defaulting to a legacy service account model. Dynamic service identities do not require separate accounts to be created, thus reducing management overhead and the attack surface.\"The ultimate objective is achieving just-in-time access and zero standing privileges. Platforms that unify identity, endpoint, and cloud telemetry are increasingly the only viable way to detect and contain agent abuse across the full identity attack chain.Practical steps security and AI builders can take today The organizations getting agentic identity right are treating it as a collaboration problem between security teams and AI builders. Based on Gartner&#x27;s Leaders&#x27; Guide, OpenID Foundation guidance, and vendor best practices, these priorities are emerging for enterprises deploying AI agents.Conduct a comprehensive discovery and audit of every account and credential first. It’s a good idea to get a baseline in place first to see how many accounts and credentials are in use across all machines in IT. CISOs and security leaders tell VentureBeat that this often turns up between six and ten times more identities than the security team had known about before the audit. One hotel chain found that it had been tracking only a tenth of its machine identities before the audit.Build and tightly manage agent inventory before production. Being on top of this makes sure AI builders know what they&#x27;re deploying and security teams know what they need to track. When there is too much of a gap between those functions, it&#x27;s easier for shadow agents to get created, evading governance in the process. A shared registry should track ownership, permissions, data access, and API connections for every agentic identity before agents reach production environments.Go all in on dynamic service identities and excel at them. Transition from static service accounts to cloud-native alternatives like AWS IAM roles, Azure managed identities, or Kubernetes service accounts. These identities are ephemeral and need to be tightly scoped, managed and policy-driven. The goal is to excel at compliance while providing AI builders the identities they need to get apps built.Implement just-in-time credentials over static secrets. Integrating just-in-time credential provisioning, automatic secret rotation, and least-privilege defaults into CI/CD pipelines and agent frameworks is critical. These are all foundational elements of zero trust that need to be core to devops pipelines. Take the advice of seasoned security leaders defending AI builders, who often tell VentureBeat to pass along the advice of never trusting perimeter security with any AI devops workflows or CI/CD processes. Go big on zero trust and identity security when it comes to protecting AI builders’ workflows.Establish auditable delegation chains. When agents spawn sub-agents or invoke external APIs, authorization chains become hard to track. Make sure humans are accountable for all services, which include AI agents. Enterprises need behavioral baselines and real-time drift detection to maintain accountability.Deploy continuous monitoring. In keeping with the precepts of zero trust, continuously monitor every use of machine credentials with the deliberate goal of excelling at observability. This includes auditing as it helps detect anomalous activities such as unauthorized privilege escalation and lateral movement. Evaluate posture management. Assess potential exploitation pathways, the extent of possible damage (blast radius), and any shadow admin access. This involves removing unnecessary or outdated access and identifying misconfigurations that attackers could exploit. Start enforcing agent lifecycle management. Every agent needs human oversight, whether as part of a group of agents or in the context of an agent-based workflow. When AI builders move to new projects, their agents should trigger the same offboarding workflows as departing employees. Orphaned agents with standing privileges can become breach vectors.Prioritize unified platforms over point solutions. Fragmented tools create fragmented visibility. Platforms that unify identity, endpoint, and cloud security give AI builders self-service visibility while giving security teams cross-domain detection.Expect to see the gap widen in 2026The gap between what AI builders deploy and what security teams can govern keeps widening. Every major technology transition has, unfortunately, also led to another generation of security breaches often forcing its own unique industry-wide reckoning. Just as hybrid cloud misconfigurations, shadow AI, and API sprawl continue to challenge security leaders and the AI builders they support, 2026 will see the gap widen between what can be contained when it comes to machine identity attacks and what needs to improve to stop determined adversaries.The 82-to-1 ratio isn&#x27;t static. It&#x27;s accelerating. Organizations that continue relying on human-first IAM architectures aren&#x27;t just accepting technical debt; they&#x27;re building security models that grow weaker with every new agent deployed.Agentic AI doesn’t break security because it’s intelligent — it breaks security because it multiplies identity faster than governance can follow. Turning what for many organizations is one of their most glaring security weaknesses into a strength starts by realizing that perimeter-based, legacy identity security is no match for the intensity, speed, and scale of machine-on-machine attacks that are the new normal and will proliferate in 2026.",
          "feed_position": 1,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/7vyBAayF7Y0KIG0vH8a7HW/fbd69ce8561899e446950b1e6ad38fc0/hero_for_identity_article.jpg?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/general/the-morning-after-engadget-newsletter-121511445.html",
          "published_at": "Tue, 30 Dec 2025 12:15:11 +0000",
          "title": "The Morning After: What to expect at CES 2026",
          "standfirst": "CES 2026 is right around the corner, and the pre-show hype cycle/ early reveals suggest, yes, there’s going to be an awful lot of AI-powered insert-product-category-here alongside, thankfully, some major announcements from the likes of Intel, Sony and NVIDIA. Intel is finally unveiling its Panther Lake (Core Ultra Series 3) chips. The first chips built on Intel’s 2nm process could offer a 50 percent performance boost, which is sorely needed amid intense competition. NVIDIA’s Jensen Huang is taking the stage for a keynote expected to feature a lot of AI hype, while AMD’s Lisa Su will likely counter with new Ryzen 9000-series chips and the latest on AI upscaling tech. LG Over the years, CES has consistently been the show for TV innovation and heady next-gen displays. This year, we’ll be talking a lot about Micro RGB. LG is introducing a new Micro RGB Evo panel with over 1,000 dimming zones, while Samsung plans to launch a full range of Micro RGB TVs from 55 inches to 115 inches. In 2025, Sony introduced a new RGB LED panel that uses individual Mini LED backlights in red, green and blue to produce even brighter, more accurate colors. The company has trademarked “True RGB,” which could become what Sony calls its spin on RGB displays. We’ll be covering all the keynotes, press conferences and big reveals in person. And figuring which of the 100s of AI-branded devices and gadgets are worth reporting on. I’m also taking bets on the most niche celebrity appearance/endorsement at CES 2026. We’ve seen 50 Cent, Big Bird, Martha Stewart, Archbishop Desmond Tutu, Guillermo del Toro, Justin Bieber and will.i.am (multiple times), so who will join this pantheon of stars? — Mat Smith The other big stories this morning CD Projekt co-founder has acquired GOG, the company’s game storefront 2025 was the year Xbox died NASA finally has a leader, but its future is no more certain Samsung’s two new speakers will deliver crisp audio while blending into your decor The Music Studio 5 and 7 will be on display at CES 2026. Samsung It isn’t just TVs with Samsung. The company has already teased a pair of new understated speakers. Likely inspired by the Samsung Frame, the new Wi-Fi speakers, called the Music Studio 5 and 7, blend into your living room. The Music Studio 5 has a four-inch woofer and dual tweeters, with a built-in waveguide to deliver better sound. The Music Studio 7 comes with a 3.1.1-channel spatial audio with top-, front-, left- and right-firing speakers. No prices yet. Expect to hear more at CES itself or once the speakers arrive in stores. And as the press image above suggests, we can't wait to sit stoically in front of one, with a glass of water (?). Continue reading. Xiaomi’s 17 Ultra Leica Edition smartphone has a manual zoom ring And a 1-inch sensor, 200MP telephoto camera and 3,500 nit display. Xiaomi Xiaomi’s latest smartphone is once again a spec beast. It features a 1-inch sensor 50MP f/1.67 main camera and 1/1.4-inch 200MP periscope telephoto camera. And it also has an interesting new mechanical feature: a manual zoom ring. This surrounds the rear camera unit. Both the regular Xiaomi 17 Ultra and Leica edition come with a Snapdragon 8 Elite Gen 5 SoC with up to 16GB of LPDDR5X RAM and 1TB of UFS 4.1 storage, along with a 6.9-inch 120Hz AMOLED display that can hit up to 3,500 nits of peak brightness. But the camera features are the standout elements. The 17 Ultra by Leica adds some very, well, Leica touches: a two-tone finish, red dot status symbol on the front, textured edges and film simulations, like Leica’s Monopan 50 black and white. Xiaomi says the zoom ring “[eliminates] the need for tedious screen taps... and can detect displacements as small as 0.03mm.” It can also be reprogrammed for manual focus. Xiaomi’s 17 Ultra by Leica and the regular 17 Ultra start at CNY 7,999 ($1,140) and CNY 6,999 ($995), on par with the latest high-end Pixel 10s and Galaxy S25s. Continue reading. You may soon be able to change your Gmail address A Google support page in Hindi says the feature is ‘gradually rolling out to all users.’ A Google support page in Hindi indicates the ability to change your Gmail address might be coming. The feature would allow you to replace your current @gmail.com address with another. Your old address would remain active as an alias on the account, and all your data would stick around, unaffected. The support page (translated) says “the ability to change your Google Account email address is gradually rolling out to all users.” Continue reading.This article originally appeared on Engadget at https://www.engadget.com/general/the-morning-after-engadget-newsletter-121511445.html?src=rss",
          "content": "CES 2026 is right around the corner, and the pre-show hype cycle/ early reveals suggest, yes, there’s going to be an awful lot of AI-powered insert-product-category-here alongside, thankfully, some major announcements from the likes of Intel, Sony and NVIDIA. Intel is finally unveiling its Panther Lake (Core Ultra Series 3) chips. The first chips built on Intel’s 2nm process could offer a 50 percent performance boost, which is sorely needed amid intense competition. NVIDIA’s Jensen Huang is taking the stage for a keynote expected to feature a lot of AI hype, while AMD’s Lisa Su will likely counter with new Ryzen 9000-series chips and the latest on AI upscaling tech. LG Over the years, CES has consistently been the show for TV innovation and heady next-gen displays. This year, we’ll be talking a lot about Micro RGB. LG is introducing a new Micro RGB Evo panel with over 1,000 dimming zones, while Samsung plans to launch a full range of Micro RGB TVs from 55 inches to 115 inches. In 2025, Sony introduced a new RGB LED panel that uses individual Mini LED backlights in red, green and blue to produce even brighter, more accurate colors. The company has trademarked “True RGB,” which could become what Sony calls its spin on RGB displays. We’ll be covering all the keynotes, press conferences and big reveals in person. And figuring which of the 100s of AI-branded devices and gadgets are worth reporting on. I’m also taking bets on the most niche celebrity appearance/endorsement at CES 2026. We’ve seen 50 Cent, Big Bird, Martha Stewart, Archbishop Desmond Tutu, Guillermo del Toro, Justin Bieber and will.i.am (multiple times), so who will join this pantheon of stars? — Mat Smith The other big stories this morning CD Projekt co-founder has acquired GOG, the company’s game storefront 2025 was the year Xbox died NASA finally has a leader, but its future is no more certain Samsung’s two new speakers will deliver crisp audio while blending into your decor The Music Studio 5 and 7 will be on display at CES 2026. Samsung It isn’t just TVs with Samsung. The company has already teased a pair of new understated speakers. Likely inspired by the Samsung Frame, the new Wi-Fi speakers, called the Music Studio 5 and 7, blend into your living room. The Music Studio 5 has a four-inch woofer and dual tweeters, with a built-in waveguide to deliver better sound. The Music Studio 7 comes with a 3.1.1-channel spatial audio with top-, front-, left- and right-firing speakers. No prices yet. Expect to hear more at CES itself or once the speakers arrive in stores. And as the press image above suggests, we can't wait to sit stoically in front of one, with a glass of water (?). Continue reading. Xiaomi’s 17 Ultra Leica Edition smartphone has a manual zoom ring And a 1-inch sensor, 200MP telephoto camera and 3,500 nit display. Xiaomi Xiaomi’s latest smartphone is once again a spec beast. It features a 1-inch sensor 50MP f/1.67 main camera and 1/1.4-inch 200MP periscope telephoto camera. And it also has an interesting new mechanical feature: a manual zoom ring. This surrounds the rear camera unit. Both the regular Xiaomi 17 Ultra and Leica edition come with a Snapdragon 8 Elite Gen 5 SoC with up to 16GB of LPDDR5X RAM and 1TB of UFS 4.1 storage, along with a 6.9-inch 120Hz AMOLED display that can hit up to 3,500 nits of peak brightness. But the camera features are the standout elements. The 17 Ultra by Leica adds some very, well, Leica touches: a two-tone finish, red dot status symbol on the front, textured edges and film simulations, like Leica’s Monopan 50 black and white. Xiaomi says the zoom ring “[eliminates] the need for tedious screen taps... and can detect displacements as small as 0.03mm.” It can also be reprogrammed for manual focus. Xiaomi’s 17 Ultra by Leica and the regular 17 Ultra start at CNY 7,999 ($1,140) and CNY 6,999 ($995), on par with the latest high-end Pixel 10s and Galaxy S25s. Continue reading. You may soon be able to change your Gmail address A Google support page in Hindi says the feature is ‘gradually rolling out to all users.’ A Google support page in Hindi indicates the ability to change your Gmail address might be coming. The feature would allow you to replace your current @gmail.com address with another. Your old address would remain active as an alias on the account, and all your data would stick around, unaffected. The support page (translated) says “the ability to change your Google Account email address is gradually rolling out to all users.” Continue reading.This article originally appeared on Engadget at https://www.engadget.com/general/the-morning-after-engadget-newsletter-121511445.html?src=rss",
          "feed_position": 11,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2025-12/a56cb510-e573-11f0-b7fd-8ea2e3a10112"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/infrastructure/why-ai-adoption-fails-without-it-led-workflow-integration",
          "published_at": "Tue, 30 Dec 2025 08:00:00 GMT",
          "title": "Why AI adoption fails without IT-led workflow integration",
          "standfirst": "At 77-year-old promotional products company Gold Bond Inc., CIO Matt Price knew generative AI adoption wouldn’t come from rolling out a chatbot. Employees needed AI embedded into the work they already hated doing: messy ERP intake, document processing, and call follow-ups.Instead of pitching benchmarks, Price built a small group of “super-users” to surface Gold Bond–specific examples and train the rest of the org. They then wired Gemini and other models into high-friction workflows, backed by sandbox testing, guardrails, and human review for anything public-facing.The payoff showed up as behavior change, not hype: Daily AI usage rose from 20% to 71%, and 43% of employees reported saving up to two hours a day. “I wanted to bring everybody on the journey,” Price told VentureBeat. “After we reset some expectations, people started leaning towards it. Our adoption has taken off.”ERP streamlining, product visualizations Gold Bond, Inc. — not to be mistaken with the skin care company — is one of the largest suppliers in the $20.5 billion promotional products industry, producing custom swag and corporate gifts for 8,500 active customers.Orders, quotes, and sample requests arrive via the website, email, fax, and more — in every format imaginable. “So it gets very messy,” Price said.AI proved a natural fit. Previously, employees manually keyed order details into the ERP. Now, Google Cloud ingests incoming documents and normalizes them, while Gemini and OpenAI extract and structure the fields before pushing a completed purchase order into the system, Price said.From there, Gold Bond expanded into a pragmatic multi-model approach: Gemini inside Workspace, ChatGPT for backend automation, Claude for QA/reasoning checks, and smaller models for edge experiments.\"We’re pretty agnostic on utilizing AI technology,” Price said. Gold Bond is largely set up as a Google shop, with implementation and change management led by Google premier partner Promevo. Early wins included phone call summaries, email drafting, and contract review. A more advanced use case is AI-assisted “virtual mockups” of branded products; teams use Recraft to iterate on sample visuals before sending previews to customers, Price said.Employees also use AI to generate Google Sheets formulas (including Excel-style XLOOKUP logic), while NotebookLM helps build an internal knowledge base for procedures and training.Other ways Gold Bond uses AI internally: Presentations: Work that took four hours now takes about 30 minutes, Price said. Code auditing: Developers run NetSuite scripts, then use two models to review them before moving to testing.Research: Tracking importer trends and tactics in response to tariffs.AI also compresses early-stage planning. “We go back and forth with AI and come up with a high level project that we can then build out for execution,” Price explained. “We get to concepts a lot quicker. We have a lot fewer meetings, which is great.”To quantify impact, Price’s team runs Kaizen events — short workshops that document baseline workflows and compare them with AI- and automation-assisted versions.To validate multi-LLM workflows, Gold Bond tests changes in a sandbox environment and runs QA scenarios before rollout. “Our technical team, along with the subject matter experts, sign off prior to shipping the changes or integrating to production,” Price said.Change management is a mustAdoption wasn’t automatic — at a legacy company, change management was the work. “It&#x27;s just apprehension a little bit, it&#x27;s something different,” Price said. Most users start with Gemini because it’s built into Workspace, then move to ChatGPT, Claude, or Mistral when they need different capabilities — or a second opinion.Price relies on a “small cool group” of about eight early adopters to test bleeding-edge tools; once they land a use case, they train the rest of the team.“You can&#x27;t just look at something like a new piece of software,\" noted Promevo CTO John Pettit. \"You really have to change people&#x27;s thoughts and behaviors around it.”But even as Price&#x27;s team is promoting widespread use, blind trust is not an option, he emphasized. Gold Bond added policies, DLP controls, and identity layers to reduce shadow AI use. It also uses LibreChat to centralize access to approved tools, enforce paid/approved usage, and block certain models when needed.Human-in-the-loop is mandatory: Public-facing content goes through approval, and outputs must be verified. “You have to set the right temperature of trust, but verify,” he said. Even with strong prompts, outputs still require verification. “You get the data back, you can&#x27;t just blatantly take it and use it.”For instance, he’ll ask for sources and reasoning — “Give me all the work cited, where you are grabbing this data from” — and treats that verification step as part of the workflow, he said.Price also cautioned against overreach. “Agentic solutions can only go so far — there still need to be humans in the loop,” he said. “Some people have bigger visions than what the tech is capable of.”His advice for other enterprises: Don’t overwhelm yourself with the hype. Start simple. Start basic. “Provide detailed prompting, test it, play around with it.”",
          "content": "At 77-year-old promotional products company Gold Bond Inc., CIO Matt Price knew generative AI adoption wouldn’t come from rolling out a chatbot. Employees needed AI embedded into the work they already hated doing: messy ERP intake, document processing, and call follow-ups.Instead of pitching benchmarks, Price built a small group of “super-users” to surface Gold Bond–specific examples and train the rest of the org. They then wired Gemini and other models into high-friction workflows, backed by sandbox testing, guardrails, and human review for anything public-facing.The payoff showed up as behavior change, not hype: Daily AI usage rose from 20% to 71%, and 43% of employees reported saving up to two hours a day. “I wanted to bring everybody on the journey,” Price told VentureBeat. “After we reset some expectations, people started leaning towards it. Our adoption has taken off.”ERP streamlining, product visualizations Gold Bond, Inc. — not to be mistaken with the skin care company — is one of the largest suppliers in the $20.5 billion promotional products industry, producing custom swag and corporate gifts for 8,500 active customers.Orders, quotes, and sample requests arrive via the website, email, fax, and more — in every format imaginable. “So it gets very messy,” Price said.AI proved a natural fit. Previously, employees manually keyed order details into the ERP. Now, Google Cloud ingests incoming documents and normalizes them, while Gemini and OpenAI extract and structure the fields before pushing a completed purchase order into the system, Price said.From there, Gold Bond expanded into a pragmatic multi-model approach: Gemini inside Workspace, ChatGPT for backend automation, Claude for QA/reasoning checks, and smaller models for edge experiments.\"We’re pretty agnostic on utilizing AI technology,” Price said. Gold Bond is largely set up as a Google shop, with implementation and change management led by Google premier partner Promevo. Early wins included phone call summaries, email drafting, and contract review. A more advanced use case is AI-assisted “virtual mockups” of branded products; teams use Recraft to iterate on sample visuals before sending previews to customers, Price said.Employees also use AI to generate Google Sheets formulas (including Excel-style XLOOKUP logic), while NotebookLM helps build an internal knowledge base for procedures and training.Other ways Gold Bond uses AI internally: Presentations: Work that took four hours now takes about 30 minutes, Price said. Code auditing: Developers run NetSuite scripts, then use two models to review them before moving to testing.Research: Tracking importer trends and tactics in response to tariffs.AI also compresses early-stage planning. “We go back and forth with AI and come up with a high level project that we can then build out for execution,” Price explained. “We get to concepts a lot quicker. We have a lot fewer meetings, which is great.”To quantify impact, Price’s team runs Kaizen events — short workshops that document baseline workflows and compare them with AI- and automation-assisted versions.To validate multi-LLM workflows, Gold Bond tests changes in a sandbox environment and runs QA scenarios before rollout. “Our technical team, along with the subject matter experts, sign off prior to shipping the changes or integrating to production,” Price said.Change management is a mustAdoption wasn’t automatic — at a legacy company, change management was the work. “It&#x27;s just apprehension a little bit, it&#x27;s something different,” Price said. Most users start with Gemini because it’s built into Workspace, then move to ChatGPT, Claude, or Mistral when they need different capabilities — or a second opinion.Price relies on a “small cool group” of about eight early adopters to test bleeding-edge tools; once they land a use case, they train the rest of the team.“You can&#x27;t just look at something like a new piece of software,\" noted Promevo CTO John Pettit. \"You really have to change people&#x27;s thoughts and behaviors around it.”But even as Price&#x27;s team is promoting widespread use, blind trust is not an option, he emphasized. Gold Bond added policies, DLP controls, and identity layers to reduce shadow AI use. It also uses LibreChat to centralize access to approved tools, enforce paid/approved usage, and block certain models when needed.Human-in-the-loop is mandatory: Public-facing content goes through approval, and outputs must be verified. “You have to set the right temperature of trust, but verify,” he said. Even with strong prompts, outputs still require verification. “You get the data back, you can&#x27;t just blatantly take it and use it.”For instance, he’ll ask for sources and reasoning — “Give me all the work cited, where you are grabbing this data from” — and treats that verification step as part of the workflow, he said.Price also cautioned against overreach. “Agentic solutions can only go so far — there still need to be humans in the loop,” he said. “Some people have bigger visions than what the tech is capable of.”His advice for other enterprises: Don’t overwhelm yourself with the hype. Start simple. Start basic. “Provide detailed prompting, test it, play around with it.”",
          "feed_position": 2,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/gHyCdU2l3DYUaXg0ct3lb/e2f02e0f859b29b566632ed017dda6e6/Gold_Bond.png?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/technology/new-years-ai-surprise-fal-releases-its-own-version-of-flux-2-image-generator",
          "published_at": "Mon, 29 Dec 2025 20:35:00 GMT",
          "title": "New Year's AI surprise: Fal releases its own version of Flux 2 image generator that's 10x cheaper and 6x more efficient",
          "standfirst": "Hot on the heels of its new $140 million Series D fundraising round, the multi-modal enterprise AI media creation platform fal.ai, known simply as \"fal\" or \"Fal\" is back with a year-end surprise: a faster, more efficient, and cheaper version of the Flux.2 [dev] open source image model from Black Forest Labs.Fal&#x27;s new model FLUX.2 [dev] Turbo is a distilled, ultra-fast image generation model that’s already outperforming many of its larger rivals on public benchmarks, and is available now on Hugging Face, though very importantly: under a custom Black Forest non-commercial license. It’s not a full-stack image model in the traditional sense, but rather a LoRA adapter—a lightweight performance enhancer that attaches to the original FLUX.2 base model and unlocks high-quality images in a fraction of the time.It’s also open-weight. And for technical teams evaluating cost, speed, and deployment control in an increasingly API-gated ecosystem, it&#x27;s a compelling example of how taking open source models and optimizing them can achieve improvements in specific attributes — in this case, speed, cost, and efficiency. fal’s platform bet: AI media infrastructure, not just modelsfal is a platform for real-time generative media—a centralized hub where developers, startups, and enterprise teams can access a wide selection of open and proprietary models for generating images, video, audio, and 3D content. It counts more than 2 million developers among its customers, according to a recent press release.The platform runs on usage-based pricing, billed per token or per asset, and exposes these models through simple, high-performance APIs designed to eliminate DevOps overhead.In 2025, fal quietly became one of the fastest-growing backend providers for AI-generated content, serving billions of assets each month and attracting investment from Sequoia, NVIDIA’s NVentures, Kleiner Perkins, and a16z. Its users range from solo builders creating filters and web tools, to enterprise labs developing hyper-personalized media pipelines for retail, entertainment, and internal design use.FLUX.2 [dev] Turbo is the latest addition to this toolbox—and one of the most developer-friendly image models available in the open-weight space.What FLUX.2 Turbo does differentlyFLUX.2 Turbo is a distilled version of the original FLUX.2 [dev] model, which was released by German AI startup Black Forest Labs (formed by ex-Stability AI engineers) last month to provide a best-in-class, open source image generation alternative to the likes of Google&#x27;s Nano Banana Pro (Gemini 3 Image) and OpenAI&#x27;s GPT Image 1.5 (which launched afterwards, but still stands as a competitor today). While FLUX.2 required 50 inference steps to generate high-fidelity outputs, Turbo does it in just 8 steps, enabled by a customized DMD2 distillation technique.Despite its speedup, Turbo doesn’t sacrifice quality. In benchmark tests on independent AI testing firm Artificial Analysis, the model now holds the top ELO score (human judged pairwise comparisons of AI outputs of rival models, in this case, image outputs) among open-weight models (1,166), outperforming offerings from Alibaba and others. On the Yupp benchmark, which factors in latency, price, and user ratings, Turbo generates 1024x1024 images in 6.6 seconds at just $0.008 per image, the lowest cost of any model on the leaderboard.To put it in context:Turbo is 1.1x to 1.4x faster than most open-weight rivalsIt’s 6x more efficient than its own full-weight base modelIt matches or beats API-only alternatives in quality, while being 3–10x cheaperTurbo is compatible with Hugging Face’s diffusers library, integrates via fal’s commercial API, and supports both text-to-image and image editing. It works on consumer GPUs and slots easily into internal pipelines—ideal for rapid iteration or lightweight deployment.It supports text-to-image and image editing, works on consumer GPUs, and can be inserted into almost any pipeline where visual asset generation is required.Not for production — unless you use fal&#x27;s APIDespite its accessibility, Turbo is not licensed for commercial or production use without explicit permission. The model is governed by the FLUX [dev] Non-Commercial License v2.0, a license crafted by Black Forest Labs that allows personal, academic, and internal evaluation use — but prohibits commercial deployment or revenue-generating applications without a separate agreement.The license permits:Research, experimentation, and non-production useDistribution of derivatives for non-commercial useCommercial use of outputs (generated images), so long as they aren’t used to train or fine-tune other competitive modelsIt prohibits:Use in production applications or servicesCommercial use without a paid licenseUse in surveillance, biometric systems, or military projectsThus, if a business wants to use FLUX.2 [dev] Turbo to generate images for commercial purposes — including marketing, product visuals, or customer-facing applications — they should use it through fal’s commercial API or website. So why release the model weights on Hugging Face at all?This type of open (but non-commercial) release serves several purposes:Transparency and trust: Developers can inspect how the model works and verify its performance.Community testing and feedback: Open use enables experimentation, benchmarking, and improvements by the broader AI community.Adoption funnel: Enterprises can test the model internally—then upgrade to a paid API or license when they’re ready to deploy at scale.For researchers, educators, and technical teams testing viability, this is a green light. But for production use—especially in customer-facing or monetized systems—companies must acquire a commercial license, typically through fal’s platform.Why this matters—and what’s nextThe release of FLUX.2 Turbo signals more than a single model drop. It reinforces fal’s strategic position: delivering a mix of openness and scalability in a field where most performance gains are locked behind API keys and proprietary endpoints.For teams tasked with balancing innovation and control—whether building design assistants, deploying creative automation, or orchestrating multi-model backends—Turbo represents a viable new baseline. It’s fast, cost-efficient, open-weight, and modular. And it’s released by a company that’s just raised nine figures to scale this infrastructure worldwide.In a landscape where foundational models often come with foundational lock-in, Turbo is something different: fast enough for production, open enough for trust, and built to move.",
          "content": "Hot on the heels of its new $140 million Series D fundraising round, the multi-modal enterprise AI media creation platform fal.ai, known simply as \"fal\" or \"Fal\" is back with a year-end surprise: a faster, more efficient, and cheaper version of the Flux.2 [dev] open source image model from Black Forest Labs.Fal&#x27;s new model FLUX.2 [dev] Turbo is a distilled, ultra-fast image generation model that’s already outperforming many of its larger rivals on public benchmarks, and is available now on Hugging Face, though very importantly: under a custom Black Forest non-commercial license. It’s not a full-stack image model in the traditional sense, but rather a LoRA adapter—a lightweight performance enhancer that attaches to the original FLUX.2 base model and unlocks high-quality images in a fraction of the time.It’s also open-weight. And for technical teams evaluating cost, speed, and deployment control in an increasingly API-gated ecosystem, it&#x27;s a compelling example of how taking open source models and optimizing them can achieve improvements in specific attributes — in this case, speed, cost, and efficiency. fal’s platform bet: AI media infrastructure, not just modelsfal is a platform for real-time generative media—a centralized hub where developers, startups, and enterprise teams can access a wide selection of open and proprietary models for generating images, video, audio, and 3D content. It counts more than 2 million developers among its customers, according to a recent press release.The platform runs on usage-based pricing, billed per token or per asset, and exposes these models through simple, high-performance APIs designed to eliminate DevOps overhead.In 2025, fal quietly became one of the fastest-growing backend providers for AI-generated content, serving billions of assets each month and attracting investment from Sequoia, NVIDIA’s NVentures, Kleiner Perkins, and a16z. Its users range from solo builders creating filters and web tools, to enterprise labs developing hyper-personalized media pipelines for retail, entertainment, and internal design use.FLUX.2 [dev] Turbo is the latest addition to this toolbox—and one of the most developer-friendly image models available in the open-weight space.What FLUX.2 Turbo does differentlyFLUX.2 Turbo is a distilled version of the original FLUX.2 [dev] model, which was released by German AI startup Black Forest Labs (formed by ex-Stability AI engineers) last month to provide a best-in-class, open source image generation alternative to the likes of Google&#x27;s Nano Banana Pro (Gemini 3 Image) and OpenAI&#x27;s GPT Image 1.5 (which launched afterwards, but still stands as a competitor today). While FLUX.2 required 50 inference steps to generate high-fidelity outputs, Turbo does it in just 8 steps, enabled by a customized DMD2 distillation technique.Despite its speedup, Turbo doesn’t sacrifice quality. In benchmark tests on independent AI testing firm Artificial Analysis, the model now holds the top ELO score (human judged pairwise comparisons of AI outputs of rival models, in this case, image outputs) among open-weight models (1,166), outperforming offerings from Alibaba and others. On the Yupp benchmark, which factors in latency, price, and user ratings, Turbo generates 1024x1024 images in 6.6 seconds at just $0.008 per image, the lowest cost of any model on the leaderboard.To put it in context:Turbo is 1.1x to 1.4x faster than most open-weight rivalsIt’s 6x more efficient than its own full-weight base modelIt matches or beats API-only alternatives in quality, while being 3–10x cheaperTurbo is compatible with Hugging Face’s diffusers library, integrates via fal’s commercial API, and supports both text-to-image and image editing. It works on consumer GPUs and slots easily into internal pipelines—ideal for rapid iteration or lightweight deployment.It supports text-to-image and image editing, works on consumer GPUs, and can be inserted into almost any pipeline where visual asset generation is required.Not for production — unless you use fal&#x27;s APIDespite its accessibility, Turbo is not licensed for commercial or production use without explicit permission. The model is governed by the FLUX [dev] Non-Commercial License v2.0, a license crafted by Black Forest Labs that allows personal, academic, and internal evaluation use — but prohibits commercial deployment or revenue-generating applications without a separate agreement.The license permits:Research, experimentation, and non-production useDistribution of derivatives for non-commercial useCommercial use of outputs (generated images), so long as they aren’t used to train or fine-tune other competitive modelsIt prohibits:Use in production applications or servicesCommercial use without a paid licenseUse in surveillance, biometric systems, or military projectsThus, if a business wants to use FLUX.2 [dev] Turbo to generate images for commercial purposes — including marketing, product visuals, or customer-facing applications — they should use it through fal’s commercial API or website. So why release the model weights on Hugging Face at all?This type of open (but non-commercial) release serves several purposes:Transparency and trust: Developers can inspect how the model works and verify its performance.Community testing and feedback: Open use enables experimentation, benchmarking, and improvements by the broader AI community.Adoption funnel: Enterprises can test the model internally—then upgrade to a paid API or license when they’re ready to deploy at scale.For researchers, educators, and technical teams testing viability, this is a green light. But for production use—especially in customer-facing or monetized systems—companies must acquire a commercial license, typically through fal’s platform.Why this matters—and what’s nextThe release of FLUX.2 Turbo signals more than a single model drop. It reinforces fal’s strategic position: delivering a mix of openness and scalability in a field where most performance gains are locked behind API keys and proprietary endpoints.For teams tasked with balancing innovation and control—whether building design assistants, deploying creative automation, or orchestrating multi-model backends—Turbo represents a viable new baseline. It’s fast, cost-efficient, open-weight, and modular. And it’s released by a company that’s just raised nine figures to scale this infrastructure worldwide.In a landscape where foundational models often come with foundational lock-in, Turbo is something different: fast enough for production, open enough for trust, and built to move.",
          "feed_position": 3,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/2n0BrHwAx3SBC1C5qoM3RJ/7a727d97c2cffadbe2a989eef735460b/4VKjOe6d0YwLLUuUM1b7D.jpg?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/science/space/nasa-finally-has-a-leader-but-its-future-is-no-more-certain-201109072.html",
          "published_at": "Mon, 29 Dec 2025 20:11:09 +0000",
          "title": "NASA finally has a leader, but its future is no more certain",
          "standfirst": "After a rudderless year and an exodus of around 4,000 employees due to Trump administration cuts, NASA got what may be its first piece of good news recently. On December 17, the Senate confirmed billionaire Jared Isaacman as the agency's new administrator. He now holds the power to rehabilitate a battered engine of scientific research, or steer it towards even more disruption. Considering the caliber of President Trump's other appointees, Isaacman is probably the best candidate for the job. Outside of being a successful entrepreneur, he has flown fighter jets and been to space twice as part of the Inspiration4 and Polaris Dawn private missions. One of those flights saw him complete the first commercial space walk, and travel farther from Earth than any human since the end of the Apollo program. \"Perfect is the enemy of the good. Isaacman checks a lot of boxes,\" says Keith Cowing, a former NASA employee and the founder of NASA Watch, a blog dedicated to the agency. \"He's passed every requirement to fly in a spacecraft that American astronauts at NASA are required to pass. He also went out of his way to have a diverse crew, and shove as much science as he could in those missions.\" And yet if you're a NASA employee or just someone who cares about the agency's work, there are still plenty of reasons to be concerned for its future. When Trump first nominated Isaacman in the spring, the billionaire wrote a 62-page document detailing his vision for NASA. In November, Politico obtained a copy of that plan, titled Project Athena.To some insiders, Project Athena painted a picture of someone who, at least at the time when it was written, fundamentally misunderstood how NASA works and how scientific discovery is funded in the US and elsewhere. It also suggests Isaacman may be more open to Trump's NASA agenda than would appear at first glance. When asked about the plan by Politico, one former NASA official characterized it as \"bizarre and careless.\" Another called it “presumptuous,\" given many of the proposed changes to the agency's structure would require Congressional approval. In one section, Isaacman recommended taking “NASA out of the taxpayer funded climate science business and [leaving] it for academia to determine.” In another section, he promised to evaluate the “relevance and ongoing necessity” of every agency center, particularly NASA's iconic Jet Propulsion Laboratory, saying the facility and others must increase the “output and time to science KPI.” A lot has changed since Isaacman first wrote that document. It came before the workforce cuts, before the future of Goddard Space Flight Center became uncertain and before Trump surprised everyone by renominating Isaacman. But during his Senate testimony earlier this month, the billionaire said “I do stand behind everything in the document, even though it was written seven months ago. I think it was all directionally correct.” He did appear to distance himself from some viewpoints expressed in or inferred by Project Athena, however. Isaacman stated that “anything suggesting that I am anti-science or want to outsource that responsibility is simply untrue.” He also came out against the administration's plan to cut NASA's science budget nearly in half, claiming the proposals would not lead to \"an optimal outcome.\"One thing is clear, Isaacman is not your typical bureaucrat. \"One of the pitfalls of some prior NASA administrators has been that they've shown too much reverence for the internal processes and bureaucratic structure of the agency to the detriment of decision-making and performance,\" said Casey Drier, chief of space policy at The Planetary Society, a nonprofit that advocates for the exploration and study of space. \"Isaacman has positioned himself as the opposite of that. Clearly, that's something that could lead to a lot of political and congressional challenges if taken too far.\" Even if Isaacman doesn't follow through on any of the proposals made in Project Athena, there's only so much a NASA administrator — even one sympathetic to civil servants working under them — can do. \"Once a budget request goes out publicly, everyone in the administration has to defend it. Anything he does will have to be internal and private,\" Drier explains. \"He never explicitly criticized the administration during his hearing. He's also coming relatively late in the budget process.\" A lot of NASA's future will depend on the White House Office of Management and Budget (OMB), which is responsible for implementing the president's agenda across the executive branch. As a direct result of guidance the OMB issued over the summer, NASA awarded 25 percent fewer new grants in 2025 than it did on average between 2020 and 2024. \"The OMB has added layers of requirements that scientists now have to go through to spend the money they've already been allocated. The administration has worked against its own stated goals of efficiency,\" Drier said. \"Isaacman can't solve that himself. He can't tell the OMB what to do. That's going to be a serious challenge.\" Looming over everything is the fact NASA still does not have a full-year budget for 2026. Congress has until January 30 to fund NASA and the rest of the federal government before the short-term funding bill it passed on November 12 runs out. \"On paper, the official policy of the administration is still to terminate a third of NASA's scientific capability,\" Drier points out.There are reasons to be cautiously optimistic. Publicly, both the House and Senate have come out against Trump's funding cuts. And some science missions that were slated to be cancelled, such as OSIRIS-APEX, have been approved for another full year of operations. What NASA needs now is someone who will, as Drier puts it, \"vigorously advocate\" for the agency in whatever way they can. It remains to be seen if that's Jared Isaacman.This article originally appeared on Engadget at https://www.engadget.com/science/space/nasa-finally-has-a-leader-but-its-future-is-no-more-certain-201109072.html?src=rss",
          "content": "After a rudderless year and an exodus of around 4,000 employees due to Trump administration cuts, NASA got what may be its first piece of good news recently. On December 17, the Senate confirmed billionaire Jared Isaacman as the agency's new administrator. He now holds the power to rehabilitate a battered engine of scientific research, or steer it towards even more disruption. Considering the caliber of President Trump's other appointees, Isaacman is probably the best candidate for the job. Outside of being a successful entrepreneur, he has flown fighter jets and been to space twice as part of the Inspiration4 and Polaris Dawn private missions. One of those flights saw him complete the first commercial space walk, and travel farther from Earth than any human since the end of the Apollo program. \"Perfect is the enemy of the good. Isaacman checks a lot of boxes,\" says Keith Cowing, a former NASA employee and the founder of NASA Watch, a blog dedicated to the agency. \"He's passed every requirement to fly in a spacecraft that American astronauts at NASA are required to pass. He also went out of his way to have a diverse crew, and shove as much science as he could in those missions.\" And yet if you're a NASA employee or just someone who cares about the agency's work, there are still plenty of reasons to be concerned for its future. When Trump first nominated Isaacman in the spring, the billionaire wrote a 62-page document detailing his vision for NASA. In November, Politico obtained a copy of that plan, titled Project Athena.To some insiders, Project Athena painted a picture of someone who, at least at the time when it was written, fundamentally misunderstood how NASA works and how scientific discovery is funded in the US and elsewhere. It also suggests Isaacman may be more open to Trump's NASA agenda than would appear at first glance. When asked about the plan by Politico, one former NASA official characterized it as \"bizarre and careless.\" Another called it “presumptuous,\" given many of the proposed changes to the agency's structure would require Congressional approval. In one section, Isaacman recommended taking “NASA out of the taxpayer funded climate science business and [leaving] it for academia to determine.” In another section, he promised to evaluate the “relevance and ongoing necessity” of every agency center, particularly NASA's iconic Jet Propulsion Laboratory, saying the facility and others must increase the “output and time to science KPI.” A lot has changed since Isaacman first wrote that document. It came before the workforce cuts, before the future of Goddard Space Flight Center became uncertain and before Trump surprised everyone by renominating Isaacman. But during his Senate testimony earlier this month, the billionaire said “I do stand behind everything in the document, even though it was written seven months ago. I think it was all directionally correct.” He did appear to distance himself from some viewpoints expressed in or inferred by Project Athena, however. Isaacman stated that “anything suggesting that I am anti-science or want to outsource that responsibility is simply untrue.” He also came out against the administration's plan to cut NASA's science budget nearly in half, claiming the proposals would not lead to \"an optimal outcome.\"One thing is clear, Isaacman is not your typical bureaucrat. \"One of the pitfalls of some prior NASA administrators has been that they've shown too much reverence for the internal processes and bureaucratic structure of the agency to the detriment of decision-making and performance,\" said Casey Drier, chief of space policy at The Planetary Society, a nonprofit that advocates for the exploration and study of space. \"Isaacman has positioned himself as the opposite of that. Clearly, that's something that could lead to a lot of political and congressional challenges if taken too far.\" Even if Isaacman doesn't follow through on any of the proposals made in Project Athena, there's only so much a NASA administrator — even one sympathetic to civil servants working under them — can do. \"Once a budget request goes out publicly, everyone in the administration has to defend it. Anything he does will have to be internal and private,\" Drier explains. \"He never explicitly criticized the administration during his hearing. He's also coming relatively late in the budget process.\" A lot of NASA's future will depend on the White House Office of Management and Budget (OMB), which is responsible for implementing the president's agenda across the executive branch. As a direct result of guidance the OMB issued over the summer, NASA awarded 25 percent fewer new grants in 2025 than it did on average between 2020 and 2024. \"The OMB has added layers of requirements that scientists now have to go through to spend the money they've already been allocated. The administration has worked against its own stated goals of efficiency,\" Drier said. \"Isaacman can't solve that himself. He can't tell the OMB what to do. That's going to be a serious challenge.\" Looming over everything is the fact NASA still does not have a full-year budget for 2026. Congress has until January 30 to fund NASA and the rest of the federal government before the short-term funding bill it passed on November 12 runs out. \"On paper, the official policy of the administration is still to terminate a third of NASA's scientific capability,\" Drier points out.There are reasons to be cautiously optimistic. Publicly, both the House and Senate have come out against Trump's funding cuts. And some science missions that were slated to be cancelled, such as OSIRIS-APEX, have been approved for another full year of operations. What NASA needs now is someone who will, as Drier puts it, \"vigorously advocate\" for the agency in whatever way they can. It remains to be seen if that's Jared Isaacman.This article originally appeared on Engadget at https://www.engadget.com/science/space/nasa-finally-has-a-leader-but-its-future-is-no-more-certain-201109072.html?src=rss",
          "feed_position": 15
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/big-tech/ces-2026-what-to-expect-from-techs-biggest-conference-in-january-120000369.html",
          "published_at": "Mon, 29 Dec 2025 16:03:00 +0000",
          "title": "CES 2026: What to expect from tech’s biggest conference in January",
          "standfirst": "CES is the January trade show where the tech industry kicks off the year with a bevy of new and notable announcements — and it’s less than a week away. The CES 2026 show floor is officially open from January 6 through 9, but the fun kicks off with events on Sunday January 4 and a host of press conferences on Monday. As always, product demos, announcements and networking will be happening at the Las Vegas Convention Center and other hotels all over the city. As usual, Engadget will be covering the event in-person and remotely, bringing you news and hands-ons straight from the show floor.More specific details and pre-announcements are already trickling out as CES approaches, and thanks to the CTA’s schedule we also do know what companies will be hosting press conferences. We’re also using our experience and expertise to predict what tech trends could rear their heads at the show.The CES 2026 schedulePress conferences and show floor booths are the bread and butter of CES. The Consumer Technology Association has already published a searchable directory of who will have a presence at the show, along with a schedule of every official panel and presentation.On Sunday, January 4, Samsung will kick-off CES with \"The First Look,\" a presentation hosted by TM Roh, the CEO of Samsung's DX Division, on the company's \"vision for the DX (Device eXperience) Division in 2026, along with new AI-driven customer experiences.\" Ahead of that, though, Samsung has already outlined a variety of more specifics. That'll be followed by multiple press conferences throughout Monday, January 5. LG is hosting its \"Innovation in Tune with You\" presentation to share \"its vision for elevating daily life through Affectionate Intelligence\" at the start of the day, Intel is launching its new Core Ultra Series 3 processors in the afternoon, Sony Honda Mobility is holding a press conference offering yet more details on its first car and AMD CEO Lisa Su will cover AMD's upcoming chip announcements at a keynote address that closes out the day.On the week of December 15, the CTA added a keynote by NVIDIA CEO Jensen Huang to its schedule. The event will take place on January 5 at 1PM PT (4PM ET) and, according to the website, will last about 90 minutes. Based on the description on the listing, the presentation will “showcase the latest NVIDIA solutions driving innovation and productivity across industries.”Finally, on Tuesday, January 6, Lenovo CEO Yuanqing Yang will host Lenovo's Tech World Conference at the Las Vegas Sphere, using the large and decidedly curved screen to share the company's \"commitment to delivering smarter AI for all by constantly redefining how technology can engage, inspire, and empower.\" It’s worth noting that Lenovo is the parent company of Motorola, which still makes phones and foldables that feature AI tools, so it’s possible those devices feature in the presentation as well. Samsung and LG vie for pre-show publicityAs they typically do, some companies have already gotten a head start on the CES news by publicly sharing their announcements in the weeks leading up to January. LG, for example, has said it will debut its first Micro RGB television at CES. While details are scarce, the company’s press release for the LG Micro RGB evo did confirm it has received certifications by Intertek for 100 percent color gamut coverage in DCI-P3 an Adobe RGB, and that it has more than a thousand dimming zones for brightness control. And if PC gaming displays are more your speed, LG will have that covered, too, with a new line of 5K-capable gaming monitors on deck with built-in AI upscaling.But LG’s not just showing off displays. The Korean multinational will also introduce a Dolby-powered modular home audio system and flex its automation muscles with a humanoid home automation robot named CLOiD. Of course, Samsung refuses to be outdone by its hometown rival, and has also released a pre-CES press release document dump. Samsung will be launching its own lineup of Micro RGB TVs at CES, for starters. The company already introduced its first Micro RGB TV at CES 2025, which was a 115-inch model available for a cool $30,000. Next year, Samsung is expanding the range with 55-, 65-, 75-, 85-, 100- and 115-inch models that use the next evolution of the company’s Micro RGB technology. Samsung is also countering LG’s 5K monitors with a 6K model that aims to deliver glasses-free 3D (another long-time CES staple). It’ll be one of several new displays in the company’s Odyssey gaming line.And on the audio front, Samsung has teased several new soundbars and speakers, including Sonos-style Wi-Fi streaming models call the Music Studio 5 and Studio 7.Outside of the formal introduction of new products and initiatives, reading the tea leaves of what was announced last year and what companies are reportedly working on, we can make some educated guesses at what we could see at CES 2026.New chips from AMD, Intel and QualcommCES is frequently the start of a cascade of new chip announcements for a given year, and one of the first places new silicon appears in real consumer products. AMD will likely use its keynote to introduce new versions of its Ryzen chips, including the recently spotted Ryzen 7 9850X3D, which is expected to offer better single-threaded performance, and the Ryzen 9000G series, which could be built with AMD's Zen 5 architecture. The company might also use its CES stage to go over its new FSR Redstone AI upscaling tech.Intel has already publicly announced that it'll launch its Panther Lake chips at CES 2026. The officially titled Intel Core Ultra Series 3 chips fit into Intel's overall \"AI PC\" push, but are specifically meant for premium laptops. Based on a preview from October 2025, Intel says the first chip made with its 2-nanometer 18A process will offer 50 percent more processing performance than previous generations and for the chip's Arc GPU, a 50 percent performance bump from last generation.Qualcomm is also rumored to be targeting laptops at the show, building on the work it's done moving its Snapdragon chips out of phones and tablets and into other types of computers. The company's Snapdragon X2 Elite and X2 Elite Premium chips should start appearing in laptops at CES 2026, offering a look at the improved speed and AI performance the company promised in 2025.Brighter, \"truer\" screensAs noted above, Samsung and LG appear to be going all-in on Micro RGB display tech for TVs. Expect that to be a huge buzzword at CES, with Hisense and Sony debuting new models, too.Sony announced a collection of new Bravia TVs in April 2025, replacing the company's flagship, filling in its midrange options and adding a new budget model to the mix. The star of this updated Bravia lineup is the Bravia 9, which features a QD-OLED panel, but Sony appears to be prepping entirely new display tech for 2026. In March 2025, Sony introduced a new RGB LED panel that uses individual Mini LED backlights colored in red, green and blue to produce even brighter, more accurate colors. In contrast to a QD-OLED, which filters a layer of blue organic light emitting diodes through quantum dots that change color, Sony's \"General RGB LED Backlight Technology\" can get as bright as a Mini LED panel without needing an extra filter layer or worrying about OLED's problems with burn-in. The company has already trademarked the name \"True RGB,\" which could end up being what Sony calls this new flavor of display if it decides to show them off at CES. It seems entirely likely, because CES is nothing if not a TV show — it’s a sure bet that we’ll see new TVs from the likes of LG and Samsung in addition to Sony. If the company doesn't introduce new display tech for its TVs, it does have a new 240Hz PlayStation monitor coming in 2026 that it could show off at CES instead.Sony isn't the only company hyped on bright screens. Samsung is reportedly pushing an updated version of the HDR10 and HDR10+ standards that could be ready to demo at CES 2026. The new HDR10+ Advanced standard would be Samsung's answer to Dolby Vision 2, which includes support for things bi-directional tone mapping and intelligent features that automatically adapt sports and gaming content. Samsung's take will reportedly offer improved brightness, genre-based tone mapping and intelligent motion smoothing options, among other improvements.And maybe your future TV won’t need a power cord, either: Displace will be showing off a mounting option that includes a 15,000mAh battery to juice up whatever giant TV screen you choose to attach.Ballie Watch 2026The ball-shaped yellow robot lovingly known as \"Ballie\" has been announced twice, first in 2020 and then again in 2024 with a projector in tow. Samsung said Ballie would go on sale in 2025 at CES last year and then shared in April 2025 that Ballie would ship this summer with Google's Gemini onboard. But it's nearly 2026, and Ballie is nowhere to be seen. It's possible Samsung could make a third attempt at announcing its robot at CES 2026, but whether or not it does, robotics will still be a big part of the show.Robot vacuums and mops were a major highlight of CES 2025, and it's safe to expect notable improvements from the new models that are announced at CES 2026. Not every company will adopt the retractable arm of the Roborock Saros Z70, but robot vacuums with legs for rising over small ledges like the Dreame X50 seem like they could become the norm. Roborock could also show off its new Roborock Qrevo Curv 2 Flow, the first of its robot vacuums to feature a retractable roller mop.Beyond just traversing spaces more efficiently, improving robots' navigation could also be a major concern at the show. Prominent members of the AI industry are turning their attention from large language models to world models, which aim to give AI a deep understanding of physical space. Those world models could be the key to making robots — like LG’s aforementioned CLOiD — competent at navigating homes and workplaces, and will likely be a significant talking point at CES 2026.We’ll be updating this article throughout the month as more rumors surface and new products are confirmed — stay tuned for future updates!Update, December 11 2025, 11:03AM ET: This story has been updated to include detail on Lenovo being Motorola’s parent company and how the latter might have a part in the Tuesday presentation.Update, December 16 2025, 1:33PM ET: This story has been updated to include the NVIDIA press conference, which was added to the CTA schedule within the last two days.Update, December 23 2025, 7:28AM ET: This story has been updated to include LG and Samsung’s Micro RGB TV announcements, which were made public in the past seven days. The intro was also tweaked to reflect how soon CES is at this point.Update, December 29 2025, 11:03AM ET: This story has been updated to include additional details on pre-announcements from Samsung, LG and Displace. This article originally appeared on Engadget at https://www.engadget.com/big-tech/ces-2026-what-to-expect-from-techs-biggest-conference-in-january-120000369.html?src=rss",
          "content": "CES is the January trade show where the tech industry kicks off the year with a bevy of new and notable announcements — and it’s less than a week away. The CES 2026 show floor is officially open from January 6 through 9, but the fun kicks off with events on Sunday January 4 and a host of press conferences on Monday. As always, product demos, announcements and networking will be happening at the Las Vegas Convention Center and other hotels all over the city. As usual, Engadget will be covering the event in-person and remotely, bringing you news and hands-ons straight from the show floor.More specific details and pre-announcements are already trickling out as CES approaches, and thanks to the CTA’s schedule we also do know what companies will be hosting press conferences. We’re also using our experience and expertise to predict what tech trends could rear their heads at the show.The CES 2026 schedulePress conferences and show floor booths are the bread and butter of CES. The Consumer Technology Association has already published a searchable directory of who will have a presence at the show, along with a schedule of every official panel and presentation.On Sunday, January 4, Samsung will kick-off CES with \"The First Look,\" a presentation hosted by TM Roh, the CEO of Samsung's DX Division, on the company's \"vision for the DX (Device eXperience) Division in 2026, along with new AI-driven customer experiences.\" Ahead of that, though, Samsung has already outlined a variety of more specifics. That'll be followed by multiple press conferences throughout Monday, January 5. LG is hosting its \"Innovation in Tune with You\" presentation to share \"its vision for elevating daily life through Affectionate Intelligence\" at the start of the day, Intel is launching its new Core Ultra Series 3 processors in the afternoon, Sony Honda Mobility is holding a press conference offering yet more details on its first car and AMD CEO Lisa Su will cover AMD's upcoming chip announcements at a keynote address that closes out the day.On the week of December 15, the CTA added a keynote by NVIDIA CEO Jensen Huang to its schedule. The event will take place on January 5 at 1PM PT (4PM ET) and, according to the website, will last about 90 minutes. Based on the description on the listing, the presentation will “showcase the latest NVIDIA solutions driving innovation and productivity across industries.”Finally, on Tuesday, January 6, Lenovo CEO Yuanqing Yang will host Lenovo's Tech World Conference at the Las Vegas Sphere, using the large and decidedly curved screen to share the company's \"commitment to delivering smarter AI for all by constantly redefining how technology can engage, inspire, and empower.\" It’s worth noting that Lenovo is the parent company of Motorola, which still makes phones and foldables that feature AI tools, so it’s possible those devices feature in the presentation as well. Samsung and LG vie for pre-show publicityAs they typically do, some companies have already gotten a head start on the CES news by publicly sharing their announcements in the weeks leading up to January. LG, for example, has said it will debut its first Micro RGB television at CES. While details are scarce, the company’s press release for the LG Micro RGB evo did confirm it has received certifications by Intertek for 100 percent color gamut coverage in DCI-P3 an Adobe RGB, and that it has more than a thousand dimming zones for brightness control. And if PC gaming displays are more your speed, LG will have that covered, too, with a new line of 5K-capable gaming monitors on deck with built-in AI upscaling.But LG’s not just showing off displays. The Korean multinational will also introduce a Dolby-powered modular home audio system and flex its automation muscles with a humanoid home automation robot named CLOiD. Of course, Samsung refuses to be outdone by its hometown rival, and has also released a pre-CES press release document dump. Samsung will be launching its own lineup of Micro RGB TVs at CES, for starters. The company already introduced its first Micro RGB TV at CES 2025, which was a 115-inch model available for a cool $30,000. Next year, Samsung is expanding the range with 55-, 65-, 75-, 85-, 100- and 115-inch models that use the next evolution of the company’s Micro RGB technology. Samsung is also countering LG’s 5K monitors with a 6K model that aims to deliver glasses-free 3D (another long-time CES staple). It’ll be one of several new displays in the company’s Odyssey gaming line.And on the audio front, Samsung has teased several new soundbars and speakers, including Sonos-style Wi-Fi streaming models call the Music Studio 5 and Studio 7.Outside of the formal introduction of new products and initiatives, reading the tea leaves of what was announced last year and what companies are reportedly working on, we can make some educated guesses at what we could see at CES 2026.New chips from AMD, Intel and QualcommCES is frequently the start of a cascade of new chip announcements for a given year, and one of the first places new silicon appears in real consumer products. AMD will likely use its keynote to introduce new versions of its Ryzen chips, including the recently spotted Ryzen 7 9850X3D, which is expected to offer better single-threaded performance, and the Ryzen 9000G series, which could be built with AMD's Zen 5 architecture. The company might also use its CES stage to go over its new FSR Redstone AI upscaling tech.Intel has already publicly announced that it'll launch its Panther Lake chips at CES 2026. The officially titled Intel Core Ultra Series 3 chips fit into Intel's overall \"AI PC\" push, but are specifically meant for premium laptops. Based on a preview from October 2025, Intel says the first chip made with its 2-nanometer 18A process will offer 50 percent more processing performance than previous generations and for the chip's Arc GPU, a 50 percent performance bump from last generation.Qualcomm is also rumored to be targeting laptops at the show, building on the work it's done moving its Snapdragon chips out of phones and tablets and into other types of computers. The company's Snapdragon X2 Elite and X2 Elite Premium chips should start appearing in laptops at CES 2026, offering a look at the improved speed and AI performance the company promised in 2025.Brighter, \"truer\" screensAs noted above, Samsung and LG appear to be going all-in on Micro RGB display tech for TVs. Expect that to be a huge buzzword at CES, with Hisense and Sony debuting new models, too.Sony announced a collection of new Bravia TVs in April 2025, replacing the company's flagship, filling in its midrange options and adding a new budget model to the mix. The star of this updated Bravia lineup is the Bravia 9, which features a QD-OLED panel, but Sony appears to be prepping entirely new display tech for 2026. In March 2025, Sony introduced a new RGB LED panel that uses individual Mini LED backlights colored in red, green and blue to produce even brighter, more accurate colors. In contrast to a QD-OLED, which filters a layer of blue organic light emitting diodes through quantum dots that change color, Sony's \"General RGB LED Backlight Technology\" can get as bright as a Mini LED panel without needing an extra filter layer or worrying about OLED's problems with burn-in. The company has already trademarked the name \"True RGB,\" which could end up being what Sony calls this new flavor of display if it decides to show them off at CES. It seems entirely likely, because CES is nothing if not a TV show — it’s a sure bet that we’ll see new TVs from the likes of LG and Samsung in addition to Sony. If the company doesn't introduce new display tech for its TVs, it does have a new 240Hz PlayStation monitor coming in 2026 that it could show off at CES instead.Sony isn't the only company hyped on bright screens. Samsung is reportedly pushing an updated version of the HDR10 and HDR10+ standards that could be ready to demo at CES 2026. The new HDR10+ Advanced standard would be Samsung's answer to Dolby Vision 2, which includes support for things bi-directional tone mapping and intelligent features that automatically adapt sports and gaming content. Samsung's take will reportedly offer improved brightness, genre-based tone mapping and intelligent motion smoothing options, among other improvements.And maybe your future TV won’t need a power cord, either: Displace will be showing off a mounting option that includes a 15,000mAh battery to juice up whatever giant TV screen you choose to attach.Ballie Watch 2026The ball-shaped yellow robot lovingly known as \"Ballie\" has been announced twice, first in 2020 and then again in 2024 with a projector in tow. Samsung said Ballie would go on sale in 2025 at CES last year and then shared in April 2025 that Ballie would ship this summer with Google's Gemini onboard. But it's nearly 2026, and Ballie is nowhere to be seen. It's possible Samsung could make a third attempt at announcing its robot at CES 2026, but whether or not it does, robotics will still be a big part of the show.Robot vacuums and mops were a major highlight of CES 2025, and it's safe to expect notable improvements from the new models that are announced at CES 2026. Not every company will adopt the retractable arm of the Roborock Saros Z70, but robot vacuums with legs for rising over small ledges like the Dreame X50 seem like they could become the norm. Roborock could also show off its new Roborock Qrevo Curv 2 Flow, the first of its robot vacuums to feature a retractable roller mop.Beyond just traversing spaces more efficiently, improving robots' navigation could also be a major concern at the show. Prominent members of the AI industry are turning their attention from large language models to world models, which aim to give AI a deep understanding of physical space. Those world models could be the key to making robots — like LG’s aforementioned CLOiD — competent at navigating homes and workplaces, and will likely be a significant talking point at CES 2026.We’ll be updating this article throughout the month as more rumors surface and new products are confirmed — stay tuned for future updates!Update, December 11 2025, 11:03AM ET: This story has been updated to include detail on Lenovo being Motorola’s parent company and how the latter might have a part in the Tuesday presentation.Update, December 16 2025, 1:33PM ET: This story has been updated to include the NVIDIA press conference, which was added to the CTA schedule within the last two days.Update, December 23 2025, 7:28AM ET: This story has been updated to include LG and Samsung’s Micro RGB TV announcements, which were made public in the past seven days. The intro was also tweaked to reflect how soon CES is at this point.Update, December 29 2025, 11:03AM ET: This story has been updated to include additional details on pre-announcements from Samsung, LG and Displace. This article originally appeared on Engadget at https://www.engadget.com/big-tech/ces-2026-what-to-expect-from-techs-biggest-conference-in-january-120000369.html?src=rss",
          "feed_position": 18
        }
      ],
      "featured_image": "https://images.ctfassets.net/jdtwqhzvc2n1/7nVD4zIrY5Lmnpu1c0lGBG/c788567a8bc13386445b652f6bcbdde6/IQnl4wLLz4s3ciS8747gk.png?w=300&q=30",
      "popularity_score": 2018.702556111111
    },
    {
      "id": "cluster_30",
      "coverage": 1,
      "updated_at": "Tue, 30 Dec 2025 21:30:04 +0000",
      "title": "The science of how (and when) we decide to speak out—or self-censor",
      "neutral_headline": "The science of how (and when) we decide to speak out—or self-censor",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2025/12/the-science-of-how-and-when-we-decide-to-speak-out-or-self-censor/",
          "published_at": "Tue, 30 Dec 2025 21:30:04 +0000",
          "title": "The science of how (and when) we decide to speak out—or self-censor",
          "standfirst": "The study's main takeaway: \"Be bold. It is the thing that slows down authoritarian creep.\"",
          "content": "Freedom of speech is a foundational principle of healthy democracies and hence a primary target for aspiring authoritarians, who typically try to squash dissent. There is a point where the threat from authorities is sufficiently severe that a population will self-censor rather than risk punishment. Social media has complicated matters, blurring traditional boundaries between public and private speech, while new technologies such as facial recognition and moderation algorithms give authoritarians powerful new tools. Researchers explored the nuanced dynamics of how people balance their desire to speak out vs their fear of punishment in a paper published in the Proceedings of the National Academy of Sciences. The authors had previously worked together on a model of political polarization, a project that wrapped up right around the time the social media space was experiencing significant changes in the ways different platforms were handling moderation. Some adopted a decidedly hands-off approach with little to no moderation. Weibo, on the other hand, began releasing the IP addresses of people who posted objectionable commentary, essentially making them targets.Read full article Comments",
          "feed_position": 0,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2014/05/censorship_2-1152x648-1767113011.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2014/05/censorship_2-1152x648-1767113011.jpg",
      "popularity_score": 339.17977833333333
    },
    {
      "id": "cluster_32",
      "coverage": 1,
      "updated_at": "Tue, 30 Dec 2025 20:45:34 +0000",
      "title": "Lawsuit over Trump rejecting medical research grants is settled",
      "neutral_headline": "Lawsuit over Trump rejecting medical research grants is settled",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2025/12/feds-researchers-settle-suit-over-grants-blocked-by-now-illegal-order/",
          "published_at": "Tue, 30 Dec 2025 20:45:34 +0000",
          "title": "Lawsuit over Trump rejecting medical research grants is settled",
          "standfirst": "Settlement forces NIH to review grants previously rejected on ideological grounds.",
          "content": "On Monday, the ACLU announced that it and other organizations representing medical researchers had reached a settlement in their suit against the federal government over grant applications that had been rejected under a policy that has since been voided by the court. The agreement, which still has to be approved by the judge overseeing the case, would see the National Institutes of Health restart reviews of grants that had been blocked on ideological grounds. It doesn't guarantee those grants will ultimately be funded, but it does mean they will go through the standard peer review process. The grants had previously been rejected without review because their content was ideologically opposed by the Trump administration. That policy has since been declared arbitrary and capricious, and thus in violation of the Administrative Procedure Act, a decision that was upheld by the Supreme Court. How'd we get here? Immediately after taking office, the Trump Administration identified a number of categories of research, some of them extremely vague, that it would not be supporting: climate change, DEI, pandemic preparedness, gender ideology, and more. Shortly thereafter, federal agencies started cancelling grants that they deemed to contain elements of these disfavored topics, and blocking consideration of grant applications for the same reasons. As a result, grants were cancelled that funded everything from research into antiviral drugs to the incidence of prostate cancer in African Americans.Read full article Comments",
          "feed_position": 1,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2224898877-1024x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2224898877-1024x648.jpg",
      "popularity_score": 328.43811166666666
    },
    {
      "id": "cluster_33",
      "coverage": 1,
      "updated_at": "Tue, 30 Dec 2025 20:30:01 +0000",
      "title": "DOGE did not find $2T in fraud, but that doesn’t matter, Musk allies say",
      "neutral_headline": "DOGE did not find $2T in fraud, but that doesn’t matter, Musk allies say",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/12/doge-did-not-find-2t-in-fraud-but-that-doesnt-matter-musk-allies-say/",
          "published_at": "Tue, 30 Dec 2025 20:30:01 +0000",
          "title": "DOGE did not find $2T in fraud, but that doesn’t matter, Musk allies say",
          "standfirst": "Musk allies spin DOGE as having a \"higher purpose\" beyond federal budget cuts.",
          "content": "Determining how \"successful\" Elon Musk's Department of Government Efficiency (DOGE) truly was depends on who you ask, but it's increasingly hard to claim that DOGE made any sizable dent in federal spending, which was its primary goal. Just two weeks ago, Musk himself notably downplayed DOGE as only being \"a little bit successful\" on a podcast, marking one of the first times that Musk admitted DOGE didn't live up to its promise. Then, more recently, on Monday, Musk revived evidence-free claims he made while campaigning for Donald Trump, insisting that government fraud remained vast and unchecked, seemingly despite DOGE's efforts. On X, he estimated that \"my lower bound guess for how much fraud there is nationally is [about 20 percent] of the Federal budget, which would mean $1.5 trillion per year. Probably much higher.\" Musk loudly left DOGE in May after clashing with Trump, complaining that a Trump budget bill threatened to undermine DOGE's work. These days, Musk does not appear confident that DOGE was worth the trouble of wading into government. Although he said on the December podcast that he considered DOGE to be his \"best side quest\" ever, the billionaire confirmed that if given the chance to go back in time, he probably would not have helmed the agency as a special government employee.Read full article Comments",
          "feed_position": 2,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2217857584-1024x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2217857584-1024x648.jpg",
      "popularity_score": 318.178945
    },
    {
      "id": "cluster_39",
      "coverage": 1,
      "updated_at": "Tue, 30 Dec 2025 19:00:00 +0000",
      "title": "Stranger Things series finale trailer is here",
      "neutral_headline": "Stranger Things series finale trailer is here",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/culture/2025/12/stranger-things-series-finale-trailer-is-here/",
          "published_at": "Tue, 30 Dec 2025 19:00:00 +0000",
          "title": "Stranger Things series finale trailer is here",
          "standfirst": "Netflix's finale will also have a two-day theatrical release to more than 600 locations.",
          "content": "Stranger Things fans are hyped for the premiere of the hotly anticipated series finale on New Year's Eve: they'll either be glued to their TVs or heading out to watch it in a bona fide theater. Netflix has dropped one last trailer for the finale—not that it really needs to do anything more to boost anticipation. (Some spoilers for Vols. 1 and 2 below but no major Vol. 2 reveals.) As previously reported, in Vol. 1, we found Hawkins under military occupation and Vecna targeting a new group of young children in his human form under the pseudonym “Mr. Whatsit” (a nod to A Wrinkle in Time). He kidnapped Holly Wheeler and took her to the Upside Down, where she found an ally in Max, still in a coma, but with her consciousness hiding in one of Vecna’s old memories. Dustin was struggling to process his grief over losing Eddie Munson in S4, causing a rift with Steve. The rest of the gang was devoted to stockpiling supplies and helping Eleven and Hopper track down Vecna in the Upside Down. They found Kali/Eight, Eleven’s psychic “sister” instead, being held captive in a military laboratory.Read full article Comments",
          "feed_position": 4,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/finale3-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/finale3-1152x648.jpg",
      "popularity_score": 308.6786672222222
    },
    {
      "id": "cluster_34",
      "coverage": 1,
      "updated_at": "Tue, 30 Dec 2025 20:00:43 +0000",
      "title": "NJ’s answer to flooding: it has bought out and demolished 1,200 properties",
      "neutral_headline": "NJ’s answer to flooding: it has bought out and demolished 1,200 properties",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/12/as-floods-become-more-severe-a-new-jersey-program-provides-a-model/",
          "published_at": "Tue, 30 Dec 2025 20:00:43 +0000",
          "title": "NJ’s answer to flooding: it has bought out and demolished 1,200 properties",
          "standfirst": "The state deals with flooding and sea level rise by buying homes in flood prone areas.",
          "content": "MANVILLE, N.J.—Richard Onderko said he will never forget the terrifying Saturday morning back in 1971 when the water rose so swiftly at his childhood home here that he and his brother had to be rescued by boat as the torrential rain from the remnants of Hurricane Doria swept through the neighborhood. It wasn’t the first time—or the last—that the town endured horrific downpours. In fact, the working-class town of 11,000, about 25 miles southwest of Newark, has long been known for getting swamped by tropical storms, nor’easters or even just a wicked rain. It was so bad, Onderko recalled, that the constant threat of flooding had strained his parents’ marriage, with his mom wanting to sell and his dad intent on staying. Eventually, his parents moved to Florida, selling the two-story house on North Second Avenue in 1995. But the new homeowner didn’t do so well either when storms hit, and in 2015, the property was sold one final time: to a state-run program that buys and demolishes houses in flood zones and permanently restores the property to open space.Read full article Comments",
          "feed_position": 3,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-113500412-1152x648-1767123440.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-113500412-1152x648-1767123440.jpg",
      "popularity_score": 307.69061166666665
    },
    {
      "id": "cluster_43",
      "coverage": 1,
      "updated_at": "Tue, 30 Dec 2025 18:45:54 +0000",
      "title": "Condé Nast user database reportedly breached, Ars unaffected",
      "neutral_headline": "Condé Nast user database reportedly breached, Ars unaffected",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/information-technology/2025/12/conde-nast-user-database-reportedly-breached-ars-unaffected/",
          "published_at": "Tue, 30 Dec 2025 18:45:54 +0000",
          "title": "Condé Nast user database reportedly breached, Ars unaffected",
          "standfirst": "A serious data breach has occurred, but Ars users have nothing to worry about.",
          "content": "Earlier this month, a hacker named Lovely claimed to have breached a Condé Nast user database and released a list of more than 2.3 million user records from our sister publication WIRED. The released materials contain demographic information (name, email, address, phone, etc.) but no passwords. The hacker also says that they will release an additional 40 million records for other Condé Nast properties, including our other sister publications Vogue, The New Yorker, Vanity Fair, and more. Of critical note to our readers, Ars Technica was not affected as we run on our own bespoke tech stack. The hacker said that they had urged Condé Nast to patch vulnerabilities to no avail. “Condé Nast does not care about the security of their users data,” the hacker wrote. “It took us an entire month to convince them to fix the vulnerabilities on their websites. We will leak more of their users’ data (40+ million) over the next few weeks. Enjoy!”Read full article Comments",
          "feed_position": 5,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/conde-hack-20205-1152x648-1767119864.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/conde-hack-20205-1152x648-1767119864.jpg",
      "popularity_score": 286.44366722222225
    },
    {
      "id": "cluster_65",
      "coverage": 1,
      "updated_at": "Tue, 30 Dec 2025 15:00:56 +0000",
      "title": "Looking for friends, lobsters may stumble into an ecological trap",
      "neutral_headline": "Looking for friends, lobsters may stumble into an ecological trap",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2025/12/looking-for-friends-lobsters-may-stumble-into-an-ecological-trap/",
          "published_at": "Tue, 30 Dec 2025 15:00:56 +0000",
          "title": "Looking for friends, lobsters may stumble into an ecological trap",
          "standfirst": "Gathering for mutual defense puts young spiny lobsters at risk of predators.",
          "content": "Lobsters are generally notable for their large claws, which can serve as a deterrent to any predators. But there's a whole family of spiny lobsters that lack these claws. They tend to ward off predators by forming large groups that collectively can present a lot of pointy bits towards anything attempting to eat them. In fact, studies found that the lobsters can sense the presence of other species-members using molecules emitted into the water, and use that to find peers to congregate with. A new study, however, finds that this same signal may lure young lobsters to their doom, causing them to try to congregate with older lobsters that are too big to be eaten by nearby predators. The smaller lobsters thus fall victim to a phenomenon called an \"ecological trap,\" which has rarely been seen to occur without human intervention. Lobsters vs. groupers The study was performed in the waters off Florida, where the seafloor is dotted by what are called \"solution holes.\" These features are the product of lower sea levels such as those that occur during periods of expanded glaciers and ice caps. During these times, much of the area off Florida was above sea level, and water dissolved the limestone rocks unevenly. This created an irregular array of small shallow pits and crevices, many of which have been reshaped by sea life since the area was submerged again.Read full article Comments",
          "feed_position": 6,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2251921243-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2251921243-1152x648.jpg",
      "popularity_score": 273
    },
    {
      "id": "cluster_72",
      "coverage": 1,
      "updated_at": "Tue, 30 Dec 2025 14:00:27 +0000",
      "title": "The top 5 most horrifying and fascinating medical cases of 2025",
      "neutral_headline": "The top 5 most horrifying and fascinating medical cases of 2025",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/health/2025/12/the-top-5-most-horrifying-and-fascinating-medical-cases-of-2025/",
          "published_at": "Tue, 30 Dec 2025 14:00:27 +0000",
          "title": "The top 5 most horrifying and fascinating medical cases of 2025",
          "standfirst": "Florida man makes two appearances on the list.",
          "content": "There were a lot of horrifying things in the news this year—a lot. But some of it was horrifying in a good way. Extraordinary medical cases—even the grisly and disturbing ones—offer a reprieve from the onslaught of current events and the stresses of our daily lives. With those remarkable reports, we can marvel at the workings, foibles, and resilience of the human body. They can remind us of the shared indignities from our existence in these mortal meatsacks. We can clear our minds of worry by learning about something we never even knew we should worry about—or by counting our blessings for avoiding so far. And sometimes, the reports are just grotesquely fascinating. Every year, there's a new lineup of such curious clinical conditions. There are always some unfortunate souls to mark medical firsts or present ultra-rare cases. There is also an endless stream of humans making poor life choices—and arriving at an emergency department with the results. This year was no different.Read full article Comments",
          "feed_position": 7,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/moles-many-medical-marvels-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/moles-many-medical-marvels-1152x648.jpg",
      "popularity_score": 263
    },
    {
      "id": "cluster_76",
      "coverage": 1,
      "updated_at": "Tue, 30 Dec 2025 13:30:40 +0000",
      "title": "The 10 best vehicles Ars Technica drove in 2025",
      "neutral_headline": "The 10 best vehicles Ars Technica drove in 2025",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/cars/2025/12/the-10-best-vehicles-ars-technica-drove-in-2025/",
          "published_at": "Tue, 30 Dec 2025 13:30:40 +0000",
          "title": "The 10 best vehicles Ars Technica drove in 2025",
          "standfirst": "Of all the cars we've driven and reviewed this year, these are our picks.",
          "content": "2025 has been a tumultuous year for the car world. After years of EV optimism, revanchists are pushing back against things like clean energy and fuel economy. Automakers have responded, postponing or canceling new electric vehicles in favor of gasoline-burning ones. It hasn't been all bad, though. Despite the changing winds, EV infrastructure continues to be built out and, anecdotally at least, feels far more reliable. We got to witness a pretty epic Formula 1 season right to the wire, in addition to some great sports car and Formula E racing. And we drove a whole bunch of cars, some of which stood out from the pack. Here are the 10 best things we sat behind the wheel of in 2025. 10th: Lotus Emira V6 A Lotus Emira doesn't need to be painted this bright color to remind you that driving can be a pleasure. Credit: Peter Nelson Let's be frank: The supposed resurgence of Lotus hasn't exactly gone to plan. When Geely bought the British Automaker in 2017, many of us hoped that the Chinese company would do for Lotus what it did for Volvo, only in Hethel instead of Gothenburg. Even before tariffs and other protectionist measures undermined the wisdom of building new Lotuses in China, the fact that most of these new cars were big, heavy EVs had already made them a hard sell. But a more traditional Lotus exists and is still built in Norfolk, England: the Lotus Emira.Read full article Comments",
          "feed_position": 8,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/ars-best-cars-2025-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/ars-best-cars-2025-1152x648.jpg",
      "popularity_score": 250
    },
    {
      "id": "cluster_91",
      "coverage": 1,
      "updated_at": "Mon, 29 Dec 2025 21:30:29 +0000",
      "title": "US can’t deport hate speech researcher for protected speech, lawsuit says",
      "neutral_headline": "US can’t deport hate speech researcher for protected speech, lawsuit says",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/12/us-cant-deport-hate-speech-researcher-for-protected-speech-lawsuit-says/",
          "published_at": "Mon, 29 Dec 2025 21:30:29 +0000",
          "title": "US can’t deport hate speech researcher for protected speech, lawsuit says",
          "standfirst": "On Monday, US officials must explain what steps they took to enforce shocking visa bans.",
          "content": "Imran Ahmed's biggest thorn in his side used to be Elon Musk, who made the hate speech researcher one of his earliest legal foes during his Twitter takeover. Now, it's the Trump administration, which planned to deport Ahmed, a legal permanent resident, just before Christmas. It would then ban him from returning to the United States, where he lives with his wife and young child, both US citizens. After suing US officials to block any attempted arrest or deportation, Ahmed was quickly granted a temporary restraining order on Christmas Day. Ahmed had successfully argued that he risked irreparable harm without the order, alleging that Trump officials continue \"to abuse the immigration system to punish and punitively detain noncitizens for protected speech and silence viewpoints with which it disagrees\" and confirming that his speech had been chilled.Read full article Comments",
          "feed_position": 9,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-1235140398.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-1235140398.jpg",
      "popularity_score": 243
    },
    {
      "id": "cluster_102",
      "coverage": 1,
      "updated_at": "Mon, 29 Dec 2025 16:30:45 +0000",
      "title": "China drafts world’s strictest rules to end AI-encouraged suicide, violence",
      "neutral_headline": "China drafts world’s strictest rules to end AI-encouraged suicide, violence",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/12/china-drafts-worlds-strictest-rules-to-end-ai-encouraged-suicide-violence/",
          "published_at": "Mon, 29 Dec 2025 16:30:45 +0000",
          "title": "China drafts world’s strictest rules to end AI-encouraged suicide, violence",
          "standfirst": "China wants a human to intervene and notify guardians if suicide is ever mentioned.",
          "content": "China drafted landmark rules to stop AI chatbots from emotionally manipulating users, including what could become the strictest policy worldwide intended to prevent AI-supported suicides, self-harm, and violence. China's Cyberspace Administration proposed the rules on Saturday. If finalized, they would apply to any AI products or services publicly available in China that use text, images, audio, video, or \"other means\" to simulate engaging human conversation. Winston Ma, adjunct professor at NYU School of Law, told CNBC that the \"planned rules would mark the world’s first attempt to regulate AI with human or anthropomorphic characteristics\" at a time when companion bot usage is rising globally. Growing awareness of problems In 2025, researchers flagged major harms of AI companions, including promotion of self-harm, violence, and terrorism. Beyond that, chatbots shared harmful misinformation, made unwanted sexual advances, encouraged substance abuse, and verbally abused users. Some psychiatrists are increasingly ready to link psychosis to chatbot use, the Wall Street Journal reported this weekend, while the most popular chatbot in the world, ChatGPT, has triggered lawsuits over outputs linked to child suicide and murder-suicide.Read full article Comments",
          "feed_position": 12,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2175706470-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2175706470-1152x648.jpg",
      "popularity_score": 153
    },
    {
      "id": "cluster_94",
      "coverage": 1,
      "updated_at": "Mon, 29 Dec 2025 19:30:47 +0000",
      "title": "Leonardo’s wood charring method predates Japanese practice",
      "neutral_headline": "Leonardo’s wood charring method predates Japanese practice",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2025/12/did-one-line-in-a-leonardo-codex-anticipate-yakisugi/",
          "published_at": "Mon, 29 Dec 2025 19:30:47 +0000",
          "title": "Leonardo’s wood charring method predates Japanese practice",
          "standfirst": "Yakisugi, a Japanese technique of burning wood surfaces, creates a protective carbonized layer",
          "content": "Yakisugi is a Japanese architectural technique for charring the surface of wood. It has become quite popular in bioarchitecture because the carbonized layer protects the wood from water, fire, insects, and fungi, thereby prolonging the lifespan of the wood. Yakisugi techniques were first codified in written form in the 17th and 18th centuries. But it seems Italian Renaissance polymath Leonardo da Vinci wrote about the protective benefits of charring wood surfaces more than 100 years earlier, according to a paper published in Zenodo, an open repository for EU funded research. Check the notes As previously reported, Leonardo produced more than 13,000 pages in his notebooks (later gathered into codices), less than a third of which have survived. The notebooks contain all manner of inventions that foreshadow future technologies: flying machines, bicycles, cranes, missiles, machine guns, an “unsinkable” double-hulled ship, dredges for clearing harbors and canals, and floating footwear akin to snowshoes to enable a person to walk on water. Leonardo foresaw the possibility of constructing a telescope in his Codex Atlanticus (1490)—he wrote of “making glasses to see the moon enlarged” a century before the instrument’s invention. In 2003, Alessandro Vezzosi, director of Italy’s Museo Ideale, came across some recipes for mysterious mixtures while flipping through Leonardo’s notes. Vezzosi experimented with the recipes, resulting in a mixture that would harden into a material eerily akin to Bakelite, a synthetic plastic widely used in the early 1900s. So Leonardo may well have invented the first manmade plastic.Read full article Comments",
          "feed_position": 10,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/leonardo1CROP-1152x648-1767021942.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/leonardo1CROP-1152x648-1767021942.jpg",
      "popularity_score": 133
    },
    {
      "id": "cluster_95",
      "coverage": 1,
      "updated_at": "Mon, 29 Dec 2025 19:00:42 +0000",
      "title": "Researchers make “neuromorphic” artificial skin for robots",
      "neutral_headline": "Researchers make “neuromorphic” artificial skin for robots",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2025/12/researchers-make-neuromorphic-artificial-skin-for-robots/",
          "published_at": "Mon, 29 Dec 2025 19:00:42 +0000",
          "title": "Researchers make “neuromorphic” artificial skin for robots",
          "standfirst": "Information from sensors is transmitted using neural-style activity spikes.",
          "content": "The nervous system does an astonishing job of tracking sensory information, and does so using signals that would drive many computer scientists insane: a noisy stream of activity spikes that may be transmitted to hundreds of additional neurons, where they are integrated with similar spike trains coming from still other neurons. Now, researchers have used spiking circuitry to build an artificial robotic skin, adopting some of the principles of how signals from our sensory neurons are transmitted and integrated. While the system relies on a few decidedly not-neural features, it has the advantage that we have chips that can run neural networks using spiking signals, which would allow this system to integrate smoothly with some energy-efficient hardware to run AI-based control software. Location via spikes The nervous system in our skin is remarkably complex. It has specialized sensors for different sensations: heat, cold, pressure, pain, and more. In most areas of the body, these feed into the spinal column, where some preliminary processing takes place, allowing reflex reactions to be triggered without even involving the brain. But signals do make their way along specialized neurons into the brain, allowing further processing and (potentially) conscious awareness.Read full article Comments",
          "feed_position": 11,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-1179602916-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-1179602916-1152x648.jpg",
      "popularity_score": 133
    },
    {
      "id": "cluster_104",
      "coverage": 1,
      "updated_at": "Mon, 29 Dec 2025 15:30:52 +0000",
      "title": "A quirky guide to myths and lore based in actual science",
      "neutral_headline": "A quirky guide to myths and lore based in actual science",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2025/12/a-quirky-guide-to-myths-and-lore-based-in-actual-science/",
          "published_at": "Mon, 29 Dec 2025 15:30:52 +0000",
          "title": "A quirky guide to myths and lore based in actual science",
          "standfirst": "Folklorist/historian Adrienne Mayor on her new book Mythopedia: A Brief Compendium of Natural History Lore",
          "content": "Earthquakes, volcanic eruption, eclipses, meteor showers, and many other natural phenomena have always been part of life on Earth. In ancient cultures that predated science, such events were often memorialized in myths and legends. There is a growing body of research that strives to connect those ancient stories with the real natural events that inspired them. Folklorist and historian Adrienne Mayor has put together a fascinating short compendium of such insights with Mythopedia: A Brief Compendium of Natural History Lore, from dry quicksand and rains of frogs to burning lakes, paleoburrows, and Scandinavian \"endless winters.\" Mayor's work has long straddled multiple disciplines, but one of her specialities is best described as geomythology, a term coined in 1968 by Indiana University geologist Dorothy Vitaliano, who was interested in classical legends about Atlantis and other civilizations that were lost due to natural disasters. Her interest resulted in Vitaliano's 1973 book Legends of the Earth: Their Geologic Origins. Mayor herself became interested in the field when she came across Greek and Roman descriptions of fossils, and that interest expanded over the years to incorporate other examples of \"folk science\" in cultures around the world. Her books include The Poison King: The Life and Legend of Mithradates, Rome's Deadliest Enemy (2009), as well as Greek Fire, Poison Arrows, & the Scorpion Bombs (2022), exploring the origins of biological and chemical warfare. Her 2018 book, Gods and Robots: Myths, Machines, and Ancient Dreams of Technology, explored ancient myths and folklore about creating automation, artificial life, and AI, connecting them to the robots and other ingenious mechanical devices actually designed and built during that era.Read full article Comments",
          "feed_position": 13,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/myth1-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/myth1-1152x648.jpg",
      "popularity_score": 133
    },
    {
      "id": "cluster_106",
      "coverage": 1,
      "updated_at": "Mon, 29 Dec 2025 15:10:16 +0000",
      "title": "GPS is vulnerable to jamming—here’s how we might fix it",
      "neutral_headline": "GPS is vulnerable to jamming—here’s how we might fix it",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/information-technology/2025/12/gps-is-vulnerable-to-jamming-heres-how-we-might-fix-it/",
          "published_at": "Mon, 29 Dec 2025 15:10:16 +0000",
          "title": "GPS is vulnerable to jamming—here’s how we might fix it",
          "standfirst": "GPS jamming has gotten cheap and easy, but there are potential solutions.",
          "content": "In September 2025, a Widerøe Airlines flight was trying to land in Vardø, Norway, which sits in the country’s far eastern arm, some 40 miles from the Russian coast. The cloud deck was low, and so was visibility. In such gray situations, pilots use GPS technology to help them land on a runway and not the side of a mountain. But on this day, GPS systems weren’t working correctly, the airwaves jammed with signals that prevented airplanes from accessing navigation information. The Widerøe flight had taken off during one of Russia’s frequent wargames, in which the country’s military simulates conflict as a preparation exercise. This one involved an imaginary war with a country. It was nicknamed Zapad-2025—translating to “West-2025”—and was happening just across the fjord from Vardø. According to European officials, GPS interference was frequent in the runup to the exercise. Russian forces, they suspected, were using GPS-signal-smashing technology, a tactic used in non-pretend conflict, too. (Russia has denied some allegations of GPS interference in the past.) Without that guidance from space, and with the cloudy weather, the Widerøe plane had to abort its landing and continue down the coast away from Russia, to Båtsfjord, a fishing village.Read full article Comments",
          "feed_position": 14,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-1316617936-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-1316617936-1152x648.jpg",
      "popularity_score": 133
    },
    {
      "id": "cluster_112",
      "coverage": 1,
      "updated_at": "Mon, 29 Dec 2025 13:00:01 +0000",
      "title": "Remembering what Windows 10 did right—and how it made modern Windows more annoying",
      "neutral_headline": "Remembering what Windows 10 did right—and how it made modern Windows more annoying",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2025/12/remembering-the-best-and-worst-about-windows-10-on-the-year-it-technically-died/",
          "published_at": "Mon, 29 Dec 2025 13:00:01 +0000",
          "title": "Remembering what Windows 10 did right—and how it made modern Windows more annoying",
          "standfirst": "Remembering Windows 10's rollout can help diagnose what ails Windows 11.",
          "content": "If you've been following our coverage for the last few years, you'll already know that 2025 is the year that Windows 10 died. Technically. \"Died,\" because Microsoft's formal end-of-support date came and went on October 14, as the company had been saying for years. \"Technically,\" because it's trivial for home users to get another free year of security updates with a few minutes of effort, and schools and businesses can get an additional two years of updates on top of that, and because load-bearing system apps like Edge and Windows Defender will keep getting updates through at least 2028 regardless. But 2025 was undoubtedly a tipping point for the so-called \"last version of Windows.\" StatCounter data says Windows 11 has overtaken Windows 10 as the most-used version of Windows both in the US (February 2025) and worldwide (July 2025). Its market share slid from just over 44 percent to just under 31 percent in the Steam Hardware Survey. And now that Microsoft's support for the OS has formally ended, games, apps, and drivers are already beginning the gradual process of ending or scaling back official Windows 10 support.Read full article Comments",
          "feed_position": 15,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2021/10/win10-wallpaper-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2021/10/win10-wallpaper-1152x648.jpg",
      "popularity_score": 133
    },
    {
      "id": "cluster_114",
      "coverage": 1,
      "updated_at": "Mon, 29 Dec 2025 12:45:50 +0000",
      "title": "I switched to eSIM in 2025, and I am full of regret",
      "neutral_headline": "I switched to eSIM in 2025, and I am full of regret",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2025/12/i-switched-to-esim-in-2025-and-i-am-full-of-regret/",
          "published_at": "Mon, 29 Dec 2025 12:45:50 +0000",
          "title": "I switched to eSIM in 2025, and I am full of regret",
          "standfirst": "Swapping SIM cards used to be easy, and then came eSIM.",
          "content": "SIM cards, the small slips of plastic that have held your mobile subscriber information since time immemorial, are on the verge of extinction. In an effort to save space for other components, device makers are finally dropping the SIM slot, and Google is the latest to move to embedded SIMs with the Pixel 10 series. After long avoiding eSIM, I had no choice but to take the plunge when the time came to review Google's new phones. And boy, do I regret it. The journey to eSIM SIM cards have existed in some form since the '90s. Back then, they were credit card-sized chunks of plastic that occupied a lot of space inside the clunky phones of the era. They slimmed down over time, going through the miniSIM, microSIM, and finally nanoSIM eras. A modern nanoSIM is about the size of your pinky nail, but space is at a premium inside smartphones. Enter, eSIM. The eSIM standard was introduced in 2016, slowly gaining support as a secondary option in smartphones. Rather than holding your phone number on a removable card, an eSIM is a programmable, non-removable component soldered to the circuit board. This allows you to store multiple SIMs and swap between them in software, and no one can swipe your SIM card from the phone. They also take up half as much space compared to a removable card, which is why OEMs have begun dropping the physical slot.Read full article Comments",
          "feed_position": 16,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/nanoSIM-card-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/nanoSIM-card-1152x648.jpg",
      "popularity_score": 133
    },
    {
      "id": "cluster_116",
      "coverage": 1,
      "updated_at": "Mon, 29 Dec 2025 12:00:29 +0000",
      "title": "Big Tech basically took Trump’s unpredictable trade war lying down",
      "neutral_headline": "Big Tech basically took Trump’s unpredictable trade war lying down",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/12/big-tech-basically-took-trumps-unpredictable-trade-war-lying-down/",
          "published_at": "Mon, 29 Dec 2025 12:00:29 +0000",
          "title": "Big Tech basically took Trump’s unpredictable trade war lying down",
          "standfirst": "From Apple gifting a gold statue to the US taking a stake in Intel.",
          "content": "As the first year of Donald Trump's chaotic trade war winds down, the tech industry is stuck scratching its head, with no practical way to anticipate what twists and turns to expect in 2026. Tech companies may have already grown numb to Trump's unpredictable moves. Back in February, Trump warned Americans to expect \"a little pain\" after he issued executive orders imposing 10–25 percent tariffs on imports from America’s biggest trading partners, including Canada, China, and Mexico. Immediately, industry associations sounded the alarm, warning that the costs of consumer tech could increase significantly. By April, Trump had ordered tariffs on all US trade partners to correct claimed trade deficits, using odd math that critics suspected came from a chatbot. (Those tariffs bizarrely targeted uninhabited islands that exported nothing and were populated by penguins.) Costs of tariffs only got higher as the year wore on. But the tech industry has done very little to push back against them. Instead, some of the biggest companies made their own surprising moves after Trump's trade war put them in deeply uncomfortable positions.Read full article Comments",
          "feed_position": 17,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/trump-tariffs-triton-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/trump-tariffs-triton-1152x648.jpg",
      "popularity_score": 133
    }
  ]
}