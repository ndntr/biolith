{
  "updated_at": "2025-11-12T19:16:54.132Z",
  "clusters": [
    {
      "id": "cluster_3",
      "coverage": 2,
      "updated_at": "Wed, 12 Nov 2025 19:04:23 +0000",
      "title": "Valve announces new Steam Machine and Steam Controller",
      "neutral_headline": "Valve announces new Steam Machine and Steam Controller",
      "items": [
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/gaming/pc/valve-announces-new-steam-machine-and-steam-controller-182836847.html",
          "published_at": "Wed, 12 Nov 2025 19:04:23 +0000",
          "title": "Valve announces new Steam Machine and Steam Controller",
          "standfirst": "Valve is taking another run at offering a console-style experience in your living room. The company has announced a new Steam Machine and Steam Controller that let you play PC games on your TV in the same way the Steam Deck lets you play them on the go.The Steam Machine works like a console, but is technically a compact PC. The boxy device features a customizable front plate and LED light strip, with USB-A and a microSD card slot available up front, and DisplayPort 1.4, HDMI 2.0 and ethernet ports on the back. Inside, the Steam Machine is powered by what Valve describes as a “semi-custom AMD Zen 4” CPU and a “semi-Custom AMD RDNA3 ” GPU with “16GB DDR5 + 8GB GDDR6 VRAM” and either 512GB or 2TB of SSD storage.Valve says the Steam Machine has “roughly six times the horsepower” of the Steam Deck, and is capable of supporting 4K gaming at 60 FPS with FSR. Interestingly, Valve is also pitching the device as a way to stream more demanding games to your Steam Deck, the Steam Frame VR headset the company also announced today or any device running Steam Link.Someone holding the new Steam Controller, with trackpads visible.ValveWhile you could use the Steam Machine with a traditional Bluetooth controller, Valve has created its own solution. The new Steam Controller puts all of the various control methods of the Steam Deck into a wireless controller. That includes sticks, face buttons, grip buttons, triggers and bumpers, but also trackpads for mouse controls and gyro controls, too. The Steam Controller works over both Bluetooth or a wired connection, and Valve is also including a charging dongle that doubles as a wireless transmitter for the fastest possible connection.Like the original Steam Controller, your input method can be individually customized for each game, and profiles can be shared. Valve also says the new controller will work with any device that runs Steam, including the Steam Deck, Steam Machine and Steam Frame.Missing from Valve’s announcement is any kind of official price. Early hands-ons with both the Steam Machine and Steam Controller suggest Valve wants the devices to be competitively priced with equivalent PCs and game controllers. Given the extra power and features, though, it seems like they might not be as much of a deal as the $400 Steam Deck was at launch.Developing…This article originally appeared on Engadget at https://www.engadget.com/gaming/pc/valve-announces-new-steam-machine-and-steam-controller-182836847.html?src=rss",
          "content": "Valve is taking another run at offering a console-style experience in your living room. The company has announced a new Steam Machine and Steam Controller that let you play PC games on your TV in the same way the Steam Deck lets you play them on the go.The Steam Machine works like a console, but is technically a compact PC. The boxy device features a customizable front plate and LED light strip, with USB-A and a microSD card slot available up front, and DisplayPort 1.4, HDMI 2.0 and ethernet ports on the back. Inside, the Steam Machine is powered by what Valve describes as a “semi-custom AMD Zen 4” CPU and a “semi-Custom AMD RDNA3 ” GPU with “16GB DDR5 + 8GB GDDR6 VRAM” and either 512GB or 2TB of SSD storage.Valve says the Steam Machine has “roughly six times the horsepower” of the Steam Deck, and is capable of supporting 4K gaming at 60 FPS with FSR. Interestingly, Valve is also pitching the device as a way to stream more demanding games to your Steam Deck, the Steam Frame VR headset the company also announced today or any device running Steam Link.Someone holding the new Steam Controller, with trackpads visible.ValveWhile you could use the Steam Machine with a traditional Bluetooth controller, Valve has created its own solution. The new Steam Controller puts all of the various control methods of the Steam Deck into a wireless controller. That includes sticks, face buttons, grip buttons, triggers and bumpers, but also trackpads for mouse controls and gyro controls, too. The Steam Controller works over both Bluetooth or a wired connection, and Valve is also including a charging dongle that doubles as a wireless transmitter for the fastest possible connection.Like the original Steam Controller, your input method can be individually customized for each game, and profiles can be shared. Valve also says the new controller will work with any device that runs Steam, including the Steam Deck, Steam Machine and Steam Frame.Missing from Valve’s announcement is any kind of official price. Early hands-ons with both the Steam Machine and Steam Controller suggest Valve wants the devices to be competitively priced with equivalent PCs and game controllers. Given the extra power and features, though, it seems like they might not be as much of a deal as the $400 Steam Deck was at launch.Developing…This article originally appeared on Engadget at https://www.engadget.com/gaming/pc/valve-announces-new-steam-machine-and-steam-controller-182836847.html?src=rss",
          "feed_position": 1,
          "image_url": "https://d29szjachogqwa.cloudfront.net/images/user-uploaded/Valve-Steam-Controller.jpg"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/the-best-vpn-deals-88-percent-discounts-on-protonvpn-surfshark-expressvpn-nordvpn-and-more-120056913.html",
          "published_at": "Wed, 12 Nov 2025 18:59:24 +0000",
          "title": "The best VPN deals: 88 percent discounts on ProtonVPN, Surfshark, ExpressVPN, NordVPN and more",
          "standfirst": "A virtual private network (VPN) is useful in several ways — a good one can stream foreign TV shows and events, protect your information from cybercrime and thwart those online trackers that show you creepily invasive ads. Although we strongly recommend using a VPN, a bit of comparison shopping goes a long way in this market. The pricing you see on VPN websites is often not an accurate portrayal of what you'll actually pay. Even so, there are some great bargains on the table. VPN providers want to boost their subscriber numbers, so they give out steep discounts to customers who sign up for a year or more at once. This is a win for you as well — while you pay out more upfront, if you divide the cost by the months of service, it's significantly cheaper over time. Most of the deals we highlight below follow that pattern, so make sure you're comfortable with a longer commitment before you take the plunge. If you've been thinking about subscribing to a VPN service, read on for the best VPN deals we could find right now. Best VPN deals ExpressVPN Basic — $97.72 for a two-year subscription with four months free (73 percent off): This is one of the best VPNs, especially for new users, who will find its apps and website headache-free on all platforms. In tests for my ExpressVPN review, it dropped my download speeds by less than 7 percent and successfully changed my virtual location 14 out of 15 times. In short, it's an all-around excellent service that only suffers from being a little overpriced — which is why I'm so excited whenever I find it offering a decent deal. This deal, which gets you 28 months of ExpressVPN service, represents a 73 percent savings. ExpressVPN Advanced — $125.72 for a two-year subscription with four months free (67 percent off): ExpressVPN recently split its pricing into multiple tiers, but they all still come with similar discounts for going long. In addition to top-tier VPN service, advanced users get two additional simultaneous connections (for a total of 12), the ExpressVPN Keys password manager, advanced ad and tracker blocking, ID protection features and a 50 percent discount on an AirCove router. NordVPN Basic — $80.73 for a two-year subscription with three months free (74 percent off): NordVPN gets the most important parts of a VPN right. It's fast, it doesn't leak any of your data and it's great at changing your virtual location. I noted in my NordVPN review that it always connects quickly and includes a support page that makes it easy to get live help. Although I'm sad to see it shutting down Meshnet, NordVPN still includes a lot of cool features, like servers that instantly connect you to Tor. This early Black Friday deal gives you 74 percent off the two-year plan, which also comes with three extra months. NordVPN Plus — $105.03 for a two-year subscription with three months free (74 percent off): In another early Black Friday discount, NordVPN has also taken 74 percent off its Plus subscription. For only a little more, you get a powerful ad and tracker blocker that can also catch malware downloads, plus access to the NordPass password manager. A Plus plan also adds a data breach scanner that checks the dark web for your sensitive information. Surfshark Starter — $53.73 for a two-year subscription with three months free (87 percent off): This is the \"basic\" level of Surfshark, but it includes the entire VPN; everything on Surfshark One is an extra perk. With this subscription, you'll get some of the most envelope-pushing features in the VPN world right now. Surfshark can rotate your IP constantly to help you evade detection — it even lets you choose your own entry and exit nodes for a double-hop connection. That all comes with a near-invisible impact on download speeds. With this year-round deal, you can save 87 percent on 27 months of Surfshark. Surfshark One — $59.13 for a two-year subscription with three months free (88 percent off): A VPN is great, but it's not enough to protect your data all on its own. Surfshark One adds several apps that boost your security beyond just VPN service, including Surfshark Antivirus (scans devices and downloads for malware) and Surfshark Alert (alerts you whenever your sensitive information shows up in a data breach), plus Surfshark Search and Alternative ID from the tier below. This extra-low deal gives you 88 percent off all those features. If you bump up to Surfshark One+, you'll also get data removal through Incogni, but the price jumps enough that it's not quite worthwhile in my eyes. CyberGhost — $56.94 for a two-year subscription with two months free (83 percent off): CyberGhost has some of the best automation you'll see on any VPN. With its Smart Rules system, you can determine how its apps respond to different types of Wi-Fi networks, with exceptions for specific networks you know by name. Typically, you can set it to auto-connect, disconnect or send you a message asking what to do. CyberGhost's other best feature is its streaming servers — I've found both better video quality and more consistent unblocking when I use them on streaming sites. Currently, you can get 26 months of CyberGhost for 83 percent off the usual price. hide.me — $59.95 for a two-year subscription with five months free (79 percent off): Hide.me is an excellent free VPN — in fact, it's my favorite on the market, even with EventVPN and the free version of Proton VPN as competition. If you do want to upgrade to its paid plan, though, the two-year subscription offers great savings. Hide.me works well as a no-frills beginner VPN, with apps and a server network it should frankly be charging more for. Private Internet Access — $79.20 for a three-year subscription with four months free (83 percent off): It's a bit hard to find (the link at the start of this paragraph includes the coupon), but Private Internet Access (PIA) is giving out the best available price right now on a VPN I'd recommend using. With this deal, you can get 39 months of PIA for a little bit over $2 per month — an 83 percent discount on its monthly price. Despite being so cheap, PIA has plenty of features, coming with its own DNS servers, a built-in ad blocker and automation powers to rival CyberGhost. However, internet speeds can fluctuate while you're connected. What makes a good VPN deal Practically every VPN heavily discounts its long-term subscriptions year-round, with even sharper discounts around occasions like Black Friday/Cyber Monday. The only noteworthy exception is Mullvad, the Costco hot dog of VPNs (that's a compliment, to be clear). When there's constantly a huge discount going on, it can be hard to tell when you're actually getting a good deal. The best way to squeeze out more savings is to look for seasonal deals, student discounts or exclusive sales like Proton VPN's coupon for Engadget readers. One trick VPNs often use is to add extra months onto an introductory deal, pushing the average monthly price even lower. When it comes time to renew, you usually can't get these extra months again. You often can't even renew for the same basic period of time — for example, you may only be able to renew a two-year subscription for one year. If you're planning to hold onto a VPN indefinitely, check the fine print to see how much it will cost per month after the first renewal, and ensure that fits into your budget. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/the-best-vpn-deals-88-percent-discounts-on-protonvpn-surfshark-expressvpn-nordvpn-and-more-120056913.html?src=rss",
          "content": "A virtual private network (VPN) is useful in several ways — a good one can stream foreign TV shows and events, protect your information from cybercrime and thwart those online trackers that show you creepily invasive ads. Although we strongly recommend using a VPN, a bit of comparison shopping goes a long way in this market. The pricing you see on VPN websites is often not an accurate portrayal of what you'll actually pay. Even so, there are some great bargains on the table. VPN providers want to boost their subscriber numbers, so they give out steep discounts to customers who sign up for a year or more at once. This is a win for you as well — while you pay out more upfront, if you divide the cost by the months of service, it's significantly cheaper over time. Most of the deals we highlight below follow that pattern, so make sure you're comfortable with a longer commitment before you take the plunge. If you've been thinking about subscribing to a VPN service, read on for the best VPN deals we could find right now. Best VPN deals ExpressVPN Basic — $97.72 for a two-year subscription with four months free (73 percent off): This is one of the best VPNs, especially for new users, who will find its apps and website headache-free on all platforms. In tests for my ExpressVPN review, it dropped my download speeds by less than 7 percent and successfully changed my virtual location 14 out of 15 times. In short, it's an all-around excellent service that only suffers from being a little overpriced — which is why I'm so excited whenever I find it offering a decent deal. This deal, which gets you 28 months of ExpressVPN service, represents a 73 percent savings. ExpressVPN Advanced — $125.72 for a two-year subscription with four months free (67 percent off): ExpressVPN recently split its pricing into multiple tiers, but they all still come with similar discounts for going long. In addition to top-tier VPN service, advanced users get two additional simultaneous connections (for a total of 12), the ExpressVPN Keys password manager, advanced ad and tracker blocking, ID protection features and a 50 percent discount on an AirCove router. NordVPN Basic — $80.73 for a two-year subscription with three months free (74 percent off): NordVPN gets the most important parts of a VPN right. It's fast, it doesn't leak any of your data and it's great at changing your virtual location. I noted in my NordVPN review that it always connects quickly and includes a support page that makes it easy to get live help. Although I'm sad to see it shutting down Meshnet, NordVPN still includes a lot of cool features, like servers that instantly connect you to Tor. This early Black Friday deal gives you 74 percent off the two-year plan, which also comes with three extra months. NordVPN Plus — $105.03 for a two-year subscription with three months free (74 percent off): In another early Black Friday discount, NordVPN has also taken 74 percent off its Plus subscription. For only a little more, you get a powerful ad and tracker blocker that can also catch malware downloads, plus access to the NordPass password manager. A Plus plan also adds a data breach scanner that checks the dark web for your sensitive information. Surfshark Starter — $53.73 for a two-year subscription with three months free (87 percent off): This is the \"basic\" level of Surfshark, but it includes the entire VPN; everything on Surfshark One is an extra perk. With this subscription, you'll get some of the most envelope-pushing features in the VPN world right now. Surfshark can rotate your IP constantly to help you evade detection — it even lets you choose your own entry and exit nodes for a double-hop connection. That all comes with a near-invisible impact on download speeds. With this year-round deal, you can save 87 percent on 27 months of Surfshark. Surfshark One — $59.13 for a two-year subscription with three months free (88 percent off): A VPN is great, but it's not enough to protect your data all on its own. Surfshark One adds several apps that boost your security beyond just VPN service, including Surfshark Antivirus (scans devices and downloads for malware) and Surfshark Alert (alerts you whenever your sensitive information shows up in a data breach), plus Surfshark Search and Alternative ID from the tier below. This extra-low deal gives you 88 percent off all those features. If you bump up to Surfshark One+, you'll also get data removal through Incogni, but the price jumps enough that it's not quite worthwhile in my eyes. CyberGhost — $56.94 for a two-year subscription with two months free (83 percent off): CyberGhost has some of the best automation you'll see on any VPN. With its Smart Rules system, you can determine how its apps respond to different types of Wi-Fi networks, with exceptions for specific networks you know by name. Typically, you can set it to auto-connect, disconnect or send you a message asking what to do. CyberGhost's other best feature is its streaming servers — I've found both better video quality and more consistent unblocking when I use them on streaming sites. Currently, you can get 26 months of CyberGhost for 83 percent off the usual price. hide.me — $59.95 for a two-year subscription with five months free (79 percent off): Hide.me is an excellent free VPN — in fact, it's my favorite on the market, even with EventVPN and the free version of Proton VPN as competition. If you do want to upgrade to its paid plan, though, the two-year subscription offers great savings. Hide.me works well as a no-frills beginner VPN, with apps and a server network it should frankly be charging more for. Private Internet Access — $79.20 for a three-year subscription with four months free (83 percent off): It's a bit hard to find (the link at the start of this paragraph includes the coupon), but Private Internet Access (PIA) is giving out the best available price right now on a VPN I'd recommend using. With this deal, you can get 39 months of PIA for a little bit over $2 per month — an 83 percent discount on its monthly price. Despite being so cheap, PIA has plenty of features, coming with its own DNS servers, a built-in ad blocker and automation powers to rival CyberGhost. However, internet speeds can fluctuate while you're connected. What makes a good VPN deal Practically every VPN heavily discounts its long-term subscriptions year-round, with even sharper discounts around occasions like Black Friday/Cyber Monday. The only noteworthy exception is Mullvad, the Costco hot dog of VPNs (that's a compliment, to be clear). When there's constantly a huge discount going on, it can be hard to tell when you're actually getting a good deal. The best way to squeeze out more savings is to look for seasonal deals, student discounts or exclusive sales like Proton VPN's coupon for Engadget readers. One trick VPNs often use is to add extra months onto an introductory deal, pushing the average monthly price even lower. When it comes time to renew, you usually can't get these extra months again. You often can't even renew for the same basic period of time — for example, you may only be able to renew a two-year subscription for one year. If you're planning to hold onto a VPN indefinitely, check the fine print to see how much it will cost per month after the first renewal, and ensure that fits into your budget. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/the-best-vpn-deals-88-percent-discounts-on-protonvpn-surfshark-expressvpn-nordvpn-and-more-120056913.html?src=rss",
          "feed_position": 2
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/ar-vr/valves-steam-frame-vr-headset-is-finally-official-and-its-coming-in-2026-181909387.html",
          "published_at": "Wed, 12 Nov 2025 18:59:15 +0000",
          "title": "Valve’s Steam Frame VR headset is finally official and it's coming in 2026",
          "standfirst": "Valve made a triumphant return to the hardware market with the Steam Deck and its OLED-toting counterpart, and now it’s having another crack at virtual reality with the Steam Frame. The Steam Frame is the long-rumored headset from Valve that had previously been codenamed \"Deckard.\" The company also announced a new Steam controller and PC called the Steam Machine. All three devices are coming in early 2026. Valve says the Steam Frame is a wireless, \"streaming-first\" headset and you can hop into your games as soon as you pop it on. It supports both VR and flatscreen games. The company made a plug-and-play 6GHz wireless adapter that you slot into your PC (or Steam Machine). It has a dual-radio setup to help minimize interference, with one radio dedicated to streaming audio and visuals to the headset, and the other for Wi-Fi. But you don't need a PC to play games on the Steam Frame. As with Meta Quest headsets, it can run games as a standalone device. The headset has a Snapdragon 8 Gen 3 chipset, 16GB of RAM and up to 1TB of built-in UFS storage. There's a microSD card slot too. The Steam Frame runs on a rechargeable 21.6 Wh Li-ion battery. There's one USB-C 2.0 port at the back that you'll use for both charging and data transfers. You can recharge the battery at a rate of up to 45W. It's unclear how long the Steam Frame's battery will run on a charge. The battery is positioned on the rear of the headstrap. So you won't necessarily need to have an external battery pack that's attached to the system by an annoying cable. It'll be possible to swap the standard headstrap (into which the audio drivers are integrated) for a different option, perhaps one with a larger battery. Even with the battery built into the headstrap, Valve says the Steam Frame weighs just under a pound at 440 grams. The core module — the front part — is 185 grams (6.5oz) and the headstrap weighs 245 grams (8.6 ounces). The Steam Frame has an optimization feature called Foveated Streaming. Valve says this uses low-latency eye-tracking (powered by two internal cameras) to optimize the detail in the image wherever your eyes are looking. The company claims it can offer a \"10x improvement in image quality and effective bandwidth.\" Foveated Streaming is said to work for every game in your Steam library. The headset has dual 2160 x 2160 LCD panels with refresh rates of up to 144Hz, a field of view of up to 110 degrees and an IPD target range of 60mm to 70mm. Valve added that \"thin and light custom pancake lenses provide edge-to-edge sharpness and a large eye box.\" The company says the maximum width for eye glasses is 140mm. As for audio, the Steam Frame has dual stereo speakers on each side with support for high-fidelity audio. Valve says the speakers on each side are \"oriented in opposite directions to cancel out vibrations,\" which can impact the tracking system. Speaking of which, the headset has four high-res monochrome cameras for controller and headset tracking — the Steam Frame uses inside-out tracking. Valve says there are infrared LEDs on the outside of the device that can help support tracking in dark environments. The Steam Frame is far from Valve's first VR headset. It released the Valve Index in 2019, and previously worked with HTC on its Vive headsets, which were initially consumer VR products before HTC shifted its focus to business and enterprise. While none of Valve’s previous PC-focused headsets had the mainstream impact of Meta’s Quest lineup or arguably even PlayStation VR (which by all accounts is still an active platform, not that Sony’s release calendar backs it up), the company is responsible for what is probably the medium’s greatest-ever game in Half-Life: Alyx. And with SteamOS on the Steam Deck being such a hit that other companies are practically begging Valve to let them put it in their own rival handhelds, it’s easy to imagine the Steam Frame becoming a serious rival to the Meta Quest. This story is developing, refresh for updates...This article originally appeared on Engadget at https://www.engadget.com/ar-vr/valves-steam-frame-vr-headset-is-finally-official-and-its-coming-in-2026-181909387.html?src=rss",
          "content": "Valve made a triumphant return to the hardware market with the Steam Deck and its OLED-toting counterpart, and now it’s having another crack at virtual reality with the Steam Frame. The Steam Frame is the long-rumored headset from Valve that had previously been codenamed \"Deckard.\" The company also announced a new Steam controller and PC called the Steam Machine. All three devices are coming in early 2026. Valve says the Steam Frame is a wireless, \"streaming-first\" headset and you can hop into your games as soon as you pop it on. It supports both VR and flatscreen games. The company made a plug-and-play 6GHz wireless adapter that you slot into your PC (or Steam Machine). It has a dual-radio setup to help minimize interference, with one radio dedicated to streaming audio and visuals to the headset, and the other for Wi-Fi. But you don't need a PC to play games on the Steam Frame. As with Meta Quest headsets, it can run games as a standalone device. The headset has a Snapdragon 8 Gen 3 chipset, 16GB of RAM and up to 1TB of built-in UFS storage. There's a microSD card slot too. The Steam Frame runs on a rechargeable 21.6 Wh Li-ion battery. There's one USB-C 2.0 port at the back that you'll use for both charging and data transfers. You can recharge the battery at a rate of up to 45W. It's unclear how long the Steam Frame's battery will run on a charge. The battery is positioned on the rear of the headstrap. So you won't necessarily need to have an external battery pack that's attached to the system by an annoying cable. It'll be possible to swap the standard headstrap (into which the audio drivers are integrated) for a different option, perhaps one with a larger battery. Even with the battery built into the headstrap, Valve says the Steam Frame weighs just under a pound at 440 grams. The core module — the front part — is 185 grams (6.5oz) and the headstrap weighs 245 grams (8.6 ounces). The Steam Frame has an optimization feature called Foveated Streaming. Valve says this uses low-latency eye-tracking (powered by two internal cameras) to optimize the detail in the image wherever your eyes are looking. The company claims it can offer a \"10x improvement in image quality and effective bandwidth.\" Foveated Streaming is said to work for every game in your Steam library. The headset has dual 2160 x 2160 LCD panels with refresh rates of up to 144Hz, a field of view of up to 110 degrees and an IPD target range of 60mm to 70mm. Valve added that \"thin and light custom pancake lenses provide edge-to-edge sharpness and a large eye box.\" The company says the maximum width for eye glasses is 140mm. As for audio, the Steam Frame has dual stereo speakers on each side with support for high-fidelity audio. Valve says the speakers on each side are \"oriented in opposite directions to cancel out vibrations,\" which can impact the tracking system. Speaking of which, the headset has four high-res monochrome cameras for controller and headset tracking — the Steam Frame uses inside-out tracking. Valve says there are infrared LEDs on the outside of the device that can help support tracking in dark environments. The Steam Frame is far from Valve's first VR headset. It released the Valve Index in 2019, and previously worked with HTC on its Vive headsets, which were initially consumer VR products before HTC shifted its focus to business and enterprise. While none of Valve’s previous PC-focused headsets had the mainstream impact of Meta’s Quest lineup or arguably even PlayStation VR (which by all accounts is still an active platform, not that Sony’s release calendar backs it up), the company is responsible for what is probably the medium’s greatest-ever game in Half-Life: Alyx. And with SteamOS on the Steam Deck being such a hit that other companies are practically begging Valve to let them put it in their own rival handhelds, it’s easy to imagine the Steam Frame becoming a serious rival to the Meta Quest. This story is developing, refresh for updates...This article originally appeared on Engadget at https://www.engadget.com/ar-vr/valves-steam-frame-vr-headset-is-finally-official-and-its-coming-in-2026-181909387.html?src=rss",
          "feed_position": 3
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/black-friday-deals-for-2025-are-here-early-we-found-the-best-tech-sales-from-apple-amazon-lego-anker-and-others-100052831.html",
          "published_at": "Wed, 12 Nov 2025 16:00:37 +0000",
          "title": "Black Friday deals for 2025 are here early: We found the best tech sales from Apple, Amazon, Lego, Anker and others",
          "standfirst": "November has turned into Black Friday and vice versa. What was once a one-day shopping sprint has turned into a month-long marathon, with retailers rolling out discounts week after week. Thanks to this, it can be easy to get deal fatigue after a while — but no one wants to miss out on a good discount, regardless of if you’re buying for yourself or someone else. We’re tracking all of the best Black Friday deals you can get right now so you don’t have to go searching for them.Engadget can help if you have tech on your shopping list this year. Here, we’ve curated the best Black Friday tech deals you can get right now, and we'll continue to update this post as we get closer to the big day at the end of November. Note that you probably have the best chance of snagging record-low prices when we get to about one week before Thanksgiving, but these deals available now are worth considering. Black Friday deals to shop now Apple AirTags (four pack) for $65 (34 percent off): iPhone users who frequently misplace things should invest in a few AirTags. Slip them into your wallet, bag, jacket and other belongings to keep track of their locations in the Find My app. Just make sure that, if you're going to attach one to your keys, you also pick up an AirTag holder to go along with it. Ninja Dual-Zone air fryer (10-quart) for $180 (22 percent off): If you cook for large crowds on Thanksgiving and other occasions, this is the air fryer to get. Not only is it a large, 10-quart capacity model, but it also has two separate cooking areas. You can crisp up potatoes on one side and brussel sprouts on the other with no issues. Use the Smart Finish feature to cook two separate foods in different ways and have them both be done at the same time, or Match Cook to copy the cooking method in both chambers. LEGO Star Wars Millennium Falcon A New Hope 25th Anniversary Collectable 75375 for $68 (20 percent off): This is a set that any Star Wars fan will love to build and then love to display once it's complete. The 921-piece set features a fully-detailed Millennium Falcone, buildable stand and nameplate. It's one of many Lego Black Friday deals you can get right now. Nintendo Switch 2 + Mario Kart World bundle for $499: Black Friday Nintendo sales were announced recently and, unsurprisingly, there won't be many true deals out there this year. There are no straight discounts on the Switch 2 console, so your best bet is to pick up a bundle that saves you some cash on a Switch 2 game. One of the best is the Mario Kart Wold bundle, but Pokémon fans should consider the Pokémon Legends: Z-A bundle, too. Apple Watch SE 3 for $200 ($50 off): The SE has been our top pick for the best Apple Watch for those on a budget, and the latest model only solidifies that further. It has the same chipset found in the latest flagship Apple Watches, fast-charging capabilities, an always-on display and most of the same activity-tracking features you'll find in more expensive model. Apple Mac Mini M4 for $499 ($100 off): Desktop users looking for an upgrade should consider the latest Mac Mini, which runs on the M4 chip and 16GB of RAM as standard in the base configuration. This version has a smaller design that takes up less space, front-facing USB-C ports and a headphone jack, plus Thunderbolt 5 support. Jisulife Life 7 handheld fan for $25 (14 percent off): This handy little fan is a must-have if you life in a warm climate or have a tropical vacation planned anytime soon. It can be used as a table or handheld fan and even be worn around the neck so you don't have to hold it at all. Its 5,000 mAh battery allows it to last hours on a single charge, and the small display in the middle of the fan's blades show its remaining battery level. Leebin 2025 electric spin scrubber for $40 (43 percent off, Prime exclusive): This weird little scrubber makes cleaning my bathroom and shower much less of a pain. Just choose the brush head you need for your job and the rotating head takes care of most of the hard work. I love the adjustable handle, which extends from 12 to 50 inches so you can get into hard-to-reach places without breaking a sweat. Monarch Money budgeting app (one year) for $50 (50 percent off with code MONARCHVIP): One of our favorite budgeting apps, Monarch Money gives you a lot of control over the organization of your funds. There's a helpful goals feature for when you're planning out big purchases or financial milestones you want to hit, and we found the month-in-review recap it provides to be more thorough than other budgeting apps we tried. There's even Zillow integration for folks looking to buy a home. SanDisk microSD Express card (256GB) for $60 (12 percent off): If you have a Switch 2, no regular microSD card will do if you want to expand the console's storage. You need a newer microSD Express card, and currently there are only a handful on the market. We did some testing to find the best microSD Express card for the Switch 2 and found that performance was, in general, very similar amongst all the readily available cards. We recommend getting whichever fits within your budget at the capacity you want. Google TV Streamer 4K for $75 ($25 off): Our top pick for the best streaming device right now, the latest version of Google's streamer supports 4K video and an excellent, easy-to-use interface that will feel familiar to anyone who's seen a set with the Google TV technology built in. It provides access to all of the major streaming services including Netflix, Disney+, HBO Max, YouTube and more, plus it has a handy on-screen pop up that lets you control all compatible smart home devices right from your TV. Also available at Walmart. Cosori 9-in-1 air fryer for $90 (25 percent off): I personally have this air fryer, one of our top picks, in my house and I've used it for over a year with no issues. I love that it makes good use of vertical space so it doesn't take up too much space on my counter, and its rounded-square shape allows me to cook more food than you'd think in one go in the basket. It crisps all kinds of foods up well and generally takes a lot of the guess work (and time) out of making a good meal. Dyson 360 Vis Nav robot vacuum for $400 ($600 off): This is one of the best robot vacuums you can get, period. It doesn't have a self-emptying base, but its superior suction power almost makes up for that. It's one of the strongest robot vacuums I've ever tested, and it has excellent obstacle avoidance. The latter means you will rarely, if ever, have to attend to it getting caught on the edge of a carpet or getting stuck under a piece of furniture. If a cordless stick vacuum is what you're looking for, don't forget to check out all of the other Dyson Black Friday deals. EcoFlow Black Friday deals — get up to 80 percent off: Portable power stations are an investment, but they can be crucial pieces of tech during emergencies. The top pick from our friends at Yahoo Tech has been heavily discounted in this early Black Friday sale. You can pick up the EcoFlow Delta Pro 3 for $1,400 off, down to $2,299, or the power station with an extra battery bundled in for $2,699 off, down to $3,599. Black Friday FAQs When is Black Friday 2025? Black Friday 2025 lands on November 28. Which stores have Black Friday deals? Many physical retail stores have Black Friday deals including Walmart, Target, Best Buy and others. Even more retailers have online Black Friday deals, including Amazon, GameStop, Costco and others. When do Black Friday sales start? Gone are the times when Black Friday sales were one-day-only affairs. Now, Black Friday deals are often available starting on Thanksgiving, or even earlier. Last year, we saw Black Friday deals online begin the week before Black Friday proper. When do Black Friday sales end? Black Friday and Cyber Monday have blended a lot over the past few years. Now, you can expect to see a good portion of Black Friday deals extend through the weekend and into Cyber Monday. It's not uncommon for Black Friday deals to expire at the end of Cyber Monday. Which retailers have the best Black Friday tech deals? The best Black Friday tech deals are typically available online at retailers like Amazon, Walmart, Best Buy and Target. It's also a good idea to check the store websites of the companies that make the products you want — for example, if you're looking for a Sonos speaker, check the Sonos website on Black Friday. Most of the time, you'll find the best Black Friday tech deals are matched at multiple retailers. Does Apple have Black Friday sales? No, you will usually not find Black Friday sales at Apple stores or on Apple's website. However, you can find Black Friday deals on Apple devices elsewhere; we recommend checking Amazon, Best Buy and other big retailers for discounts on iPads, Apple Watches and more on Black Friday. Does Amazon have Black Friday sales? Yes, Amazon has Black Friday sales. The online retailer's site will look similar to Prime Day on Black Friday, with discounts on all sorts of items from household essentials to fashion to tech.This article originally appeared on Engadget at https://www.engadget.com/deals/black-friday-deals-for-2025-are-here-early-we-found-the-best-tech-sales-from-apple-amazon-lego-anker-and-others-100052831.html?src=rss",
          "content": "November has turned into Black Friday and vice versa. What was once a one-day shopping sprint has turned into a month-long marathon, with retailers rolling out discounts week after week. Thanks to this, it can be easy to get deal fatigue after a while — but no one wants to miss out on a good discount, regardless of if you’re buying for yourself or someone else. We’re tracking all of the best Black Friday deals you can get right now so you don’t have to go searching for them.Engadget can help if you have tech on your shopping list this year. Here, we’ve curated the best Black Friday tech deals you can get right now, and we'll continue to update this post as we get closer to the big day at the end of November. Note that you probably have the best chance of snagging record-low prices when we get to about one week before Thanksgiving, but these deals available now are worth considering. Black Friday deals to shop now Apple AirTags (four pack) for $65 (34 percent off): iPhone users who frequently misplace things should invest in a few AirTags. Slip them into your wallet, bag, jacket and other belongings to keep track of their locations in the Find My app. Just make sure that, if you're going to attach one to your keys, you also pick up an AirTag holder to go along with it. Ninja Dual-Zone air fryer (10-quart) for $180 (22 percent off): If you cook for large crowds on Thanksgiving and other occasions, this is the air fryer to get. Not only is it a large, 10-quart capacity model, but it also has two separate cooking areas. You can crisp up potatoes on one side and brussel sprouts on the other with no issues. Use the Smart Finish feature to cook two separate foods in different ways and have them both be done at the same time, or Match Cook to copy the cooking method in both chambers. LEGO Star Wars Millennium Falcon A New Hope 25th Anniversary Collectable 75375 for $68 (20 percent off): This is a set that any Star Wars fan will love to build and then love to display once it's complete. The 921-piece set features a fully-detailed Millennium Falcone, buildable stand and nameplate. It's one of many Lego Black Friday deals you can get right now. Nintendo Switch 2 + Mario Kart World bundle for $499: Black Friday Nintendo sales were announced recently and, unsurprisingly, there won't be many true deals out there this year. There are no straight discounts on the Switch 2 console, so your best bet is to pick up a bundle that saves you some cash on a Switch 2 game. One of the best is the Mario Kart Wold bundle, but Pokémon fans should consider the Pokémon Legends: Z-A bundle, too. Apple Watch SE 3 for $200 ($50 off): The SE has been our top pick for the best Apple Watch for those on a budget, and the latest model only solidifies that further. It has the same chipset found in the latest flagship Apple Watches, fast-charging capabilities, an always-on display and most of the same activity-tracking features you'll find in more expensive model. Apple Mac Mini M4 for $499 ($100 off): Desktop users looking for an upgrade should consider the latest Mac Mini, which runs on the M4 chip and 16GB of RAM as standard in the base configuration. This version has a smaller design that takes up less space, front-facing USB-C ports and a headphone jack, plus Thunderbolt 5 support. Jisulife Life 7 handheld fan for $25 (14 percent off): This handy little fan is a must-have if you life in a warm climate or have a tropical vacation planned anytime soon. It can be used as a table or handheld fan and even be worn around the neck so you don't have to hold it at all. Its 5,000 mAh battery allows it to last hours on a single charge, and the small display in the middle of the fan's blades show its remaining battery level. Leebin 2025 electric spin scrubber for $40 (43 percent off, Prime exclusive): This weird little scrubber makes cleaning my bathroom and shower much less of a pain. Just choose the brush head you need for your job and the rotating head takes care of most of the hard work. I love the adjustable handle, which extends from 12 to 50 inches so you can get into hard-to-reach places without breaking a sweat. Monarch Money budgeting app (one year) for $50 (50 percent off with code MONARCHVIP): One of our favorite budgeting apps, Monarch Money gives you a lot of control over the organization of your funds. There's a helpful goals feature for when you're planning out big purchases or financial milestones you want to hit, and we found the month-in-review recap it provides to be more thorough than other budgeting apps we tried. There's even Zillow integration for folks looking to buy a home. SanDisk microSD Express card (256GB) for $60 (12 percent off): If you have a Switch 2, no regular microSD card will do if you want to expand the console's storage. You need a newer microSD Express card, and currently there are only a handful on the market. We did some testing to find the best microSD Express card for the Switch 2 and found that performance was, in general, very similar amongst all the readily available cards. We recommend getting whichever fits within your budget at the capacity you want. Google TV Streamer 4K for $75 ($25 off): Our top pick for the best streaming device right now, the latest version of Google's streamer supports 4K video and an excellent, easy-to-use interface that will feel familiar to anyone who's seen a set with the Google TV technology built in. It provides access to all of the major streaming services including Netflix, Disney+, HBO Max, YouTube and more, plus it has a handy on-screen pop up that lets you control all compatible smart home devices right from your TV. Also available at Walmart. Cosori 9-in-1 air fryer for $90 (25 percent off): I personally have this air fryer, one of our top picks, in my house and I've used it for over a year with no issues. I love that it makes good use of vertical space so it doesn't take up too much space on my counter, and its rounded-square shape allows me to cook more food than you'd think in one go in the basket. It crisps all kinds of foods up well and generally takes a lot of the guess work (and time) out of making a good meal. Dyson 360 Vis Nav robot vacuum for $400 ($600 off): This is one of the best robot vacuums you can get, period. It doesn't have a self-emptying base, but its superior suction power almost makes up for that. It's one of the strongest robot vacuums I've ever tested, and it has excellent obstacle avoidance. The latter means you will rarely, if ever, have to attend to it getting caught on the edge of a carpet or getting stuck under a piece of furniture. If a cordless stick vacuum is what you're looking for, don't forget to check out all of the other Dyson Black Friday deals. EcoFlow Black Friday deals — get up to 80 percent off: Portable power stations are an investment, but they can be crucial pieces of tech during emergencies. The top pick from our friends at Yahoo Tech has been heavily discounted in this early Black Friday sale. You can pick up the EcoFlow Delta Pro 3 for $1,400 off, down to $2,299, or the power station with an extra battery bundled in for $2,699 off, down to $3,599. Black Friday FAQs When is Black Friday 2025? Black Friday 2025 lands on November 28. Which stores have Black Friday deals? Many physical retail stores have Black Friday deals including Walmart, Target, Best Buy and others. Even more retailers have online Black Friday deals, including Amazon, GameStop, Costco and others. When do Black Friday sales start? Gone are the times when Black Friday sales were one-day-only affairs. Now, Black Friday deals are often available starting on Thanksgiving, or even earlier. Last year, we saw Black Friday deals online begin the week before Black Friday proper. When do Black Friday sales end? Black Friday and Cyber Monday have blended a lot over the past few years. Now, you can expect to see a good portion of Black Friday deals extend through the weekend and into Cyber Monday. It's not uncommon for Black Friday deals to expire at the end of Cyber Monday. Which retailers have the best Black Friday tech deals? The best Black Friday tech deals are typically available online at retailers like Amazon, Walmart, Best Buy and Target. It's also a good idea to check the store websites of the companies that make the products you want — for example, if you're looking for a Sonos speaker, check the Sonos website on Black Friday. Most of the time, you'll find the best Black Friday tech deals are matched at multiple retailers. Does Apple have Black Friday sales? No, you will usually not find Black Friday sales at Apple stores or on Apple's website. However, you can find Black Friday deals on Apple devices elsewhere; we recommend checking Amazon, Best Buy and other big retailers for discounts on iPads, Apple Watches and more on Black Friday. Does Amazon have Black Friday sales? Yes, Amazon has Black Friday sales. The online retailer's site will look similar to Prime Day on Black Friday, with discounts on all sorts of items from household essentials to fashion to tech.This article originally appeared on Engadget at https://www.engadget.com/deals/black-friday-deals-for-2025-are-here-early-we-found-the-best-tech-sales-from-apple-amazon-lego-anker-and-others-100052831.html?src=rss",
          "feed_position": 12
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/computing/laptops/framework-laptop-16-2025-upgrade-review-the-rtx-5070-is-the-star-160000464.html",
          "published_at": "Wed, 12 Nov 2025 16:00:00 +0000",
          "title": "Framework Laptop 16 (2025 upgrade) review: The RTX 5070 is the star",
          "standfirst": "Plenty of companies have promised to produce a gaming laptop that could be upgraded over time. If we’re honest, nobody has managed to properly deliver on that pledge until now, as Framework launches a meaningful CPU and GPU upgrade for the Laptop 16. Almost two years after the machine first went on sale, you can now swap out its discrete Radeon RX 7700S for NVIDIA’s GeForce RTX 5070. If the company deserves a standing ovation for that feat, then it gets an extra prize for bringing an NVIDIA GPU to AMD’s hinterland. Hardware Framework’s late-2025 upgrade for the laptop is arguably more important than every product it’s released since its very first. It’s the first chance for users (of any laptop, really) to swap out or add a discrete GPU to an existing machine. If you bought the first-generation model, you could have relied on the integrated graphics, or equipped it with a discrete Radeon RX 7700S. Now, you get the option to buy NVIDIA’s GeForce RTX 5070 with 8GB DDR7 RAM which you can add to the chassis yourself. The company has also repackaged the existing Radeon RX 7700S with the promise of less fan noise and better thermal performance than the previous model. The new GPU pulls some of the focus away from the new mainboards, which are equipped with a choice of AMD’s Ryzen AI 7 350 or Ryzen AI 9 HZ 370, both of which promise to deliver 45W TDP. As before, you can equip the board with up to 86GB RAM, one or two SSDs, and your pick of ports via the six expansion card slots housed in the chassis. If you’re buying the laptop new, you’ll get a raft of smaller upgrades, starting with a new 165Hz, 2,560 x 1,600 panel which supports NVIDIA G-Sync. Plus, a new top cover, improved keyboard, number pad, webcam, Wi-Fi 7 support and an upgraded 240W power adapter. Sadly, I can’t talk about these as I was testing the upgrade from the 2024 model which just included the new mainboard and GPU module. Framework did listen to gripes about that rear-slung USB-C port which previously didn’t support charging. It was an omission that severely vexed my colleague Devindra Hardawar in his review of the original machine. But now, if you splurge for the RTX 5070, you can now use the rear port in the way that most people would intend. (If you’re unfamiliar, the Laptop 16’s discrete GPUs are packaged in self-contained “Expansion Modules” that go into the back of the chassis. The Radeon version could only be used for accessories and/or connecting additional displays.) Rounding out the changes is Framework’s continual promise that it’s improved the cooling situation. The thermal paste has been switched out for Honeywell PTM, there’s a new, redesigned fan geometry and tweaked pipes for better airflow. And, look, I don’t want to ding Framework for failing to deliver on one promise when it’s kept so many others. But if you’ve followed the company for any length of time, you already know what I’m gonna say in the In-use section. The obligatory AMD port compromise Graphic showing which ports work with which cards with an AMD mainboard. Framework As is custom whenever discussing an AMD-toting Framework machine, you’ll need to memorize the diagram of which expansion card slots will work with which devices. We’re not going to ding Framework for an issue present in all AMD hardware, and the only reason it’s noticeable here is that you have the choice of which ports to use for what. You don’t have the sort of universal port flexibility that you might otherwise be expecting. Installation Laptop 16 is bigger and more complex than its smaller siblings, but that doesn’t mean it’s any harder to maintain. The company’s iFixit-style guides hold your hand so well that popping the mid plate off should feel as natural as breathing. And you get a real sense of how well the components are laid out when you’re asked to take them all apart and put them back together. The company says replacing the mainboard and graphics module should take you an hour, which is far too generous. It took me about 22 minutes to get everything swapped in and set up, to the point where I think installing the new drivers was more laborious than this. I can’t stress enough how much of a feat it is to have a modular, upgradeable gaming laptop that offers you the chance to leap a generation. Being able to pull out a two-year-old Radeon to swap in a fresh RTX is the stuff of dreams (for some people, at least). Imagine how long it’ll be possible to keep this machine going if this type of bi-annual upgrade cycle continues. This isn’t a particularly difficult process, making it easy enough for those folks who would otherwise blanch at the idea. In-use Image of the 2025 mainboard and expansion modules for the Framework Laptop 16. Daniel Cooper for Engadget Of course, strapping such a powerful chip and graphics [INAUDIBLE DUE TO FAN NOISE] lead to issues. As discrete components, both the mainboard and expansion module need their own self-contained cooling. That’s never going to be as efficient as a holistically designed laptop. When you’re not taxing the machine, it’s not an issue at all, it’s only when you use it for its intended purpose that it becomes a serious problem. If you want to play games with this thing, get headphones or put the subtitles on, and don’t even think about using this in public. Did… did you hear that? CAN YOU HEAR ME? I SAID… AS DISCRETE COMPONENTS… And that’s before we get to the heat that this thing kicks out. I’ve got my unit on a stand with about four inches of clearance from the desk. I put my hand underneath the chassis to feel how warm it was getting and it was enough to make me never want to put this on my lap, ever. It’s a shame the noise and heat is such a bear as it’s a machine with sufficient grunt to impress many a jaded enthusiast. I set Cyberpunk 2077 to the highest settings I could (Ray Tracing: Overdrive) on 1080p, and it was able to comfortably produce 140 fps. Setting it to the defaults (Ray Tracing: Low, but the resolution set to the display’s maximum) it was able to crank out 182 fps. You’ll find similarly-impressive performance if you use the Laptop 16 more for productivity than gaming. It compressed a 38GB 4K video file down to an 8GB HD mp4 in 28 minutes and 29 seconds. Using LM Studio, I was able to run Google’s Gemma 3 27B model with what I’d call fairly decent performance. Certainly, the chatbot wasn’t responding as quickly as Gemini would online, but it was hardly stuttering. I’d say that the performance here is more or less what you’d expect from the specs, with the one downside being that godawful fan noise. Pricing If you buy a new Laptop 16 pre-built from Framework, the Ryzen AI 7 configuration starts at $1,500, the AI 9 at $1,800. Add in the RTX 5070 and you can add another $699 to that price, which is the same cost as if you buy the GPU standalone as an upgrade. Or, if money’s tight, you could buy the new machine now and then add in the 5070 whenever you’d like — that’s the benefit of modularity. It should be obvious you can get laptops with these sorts of components for less if you look elsewhere. In the run-up to the holiday season, I’ve seen machines — such as HP’s Omen Max — offer a Ryzen AI 7 and an RTX 5070 Ti for under $2,000. But here you’re not just buying a laptop, you’re buying into Framework’s broader ethos. You’ll get the fastest machine it can sell you right now, plus the ability to cheaply swap out to the next big thing in a couple years’ time without the cost of buying a new machine. As I said back when reviewing the Ryzen AI 300 upgrades for the Laptop 13 earlier this year, Framework is well placed to take advantage of the world’s political situation. If the price of a whole new laptop skyrockets, then you can at least make a saving by just replacing what you need. Wrap-up I wonder if “Should you get one?” is the best question to ask and answer given the singular furrow Framework is ploughing. If you want a powerful laptop where every part can be replaced or upgraded, you don’t really have a serious alternative. Laptop 16’s natural target market is professionals and enthusiasts who value modularity and longevity over everything else. These new components give you enough power to play games, run AI models locally and whatever other demanding tasks you’ll throw at it. As for everyone else, it’s a question of how willing you are to accept the heat, the noise and the slightly agricultural aesthetics. After all, this machine isn’t the sort of gadget you’ll be looking to move on in a few years’ time, it’ll be one you’re committing to for a long while. This article originally appeared on Engadget at https://www.engadget.com/computing/laptops/framework-laptop-16-2025-upgrade-review-the-rtx-5070-is-the-star-160000464.html?src=rss",
          "content": "Plenty of companies have promised to produce a gaming laptop that could be upgraded over time. If we’re honest, nobody has managed to properly deliver on that pledge until now, as Framework launches a meaningful CPU and GPU upgrade for the Laptop 16. Almost two years after the machine first went on sale, you can now swap out its discrete Radeon RX 7700S for NVIDIA’s GeForce RTX 5070. If the company deserves a standing ovation for that feat, then it gets an extra prize for bringing an NVIDIA GPU to AMD’s hinterland. Hardware Framework’s late-2025 upgrade for the laptop is arguably more important than every product it’s released since its very first. It’s the first chance for users (of any laptop, really) to swap out or add a discrete GPU to an existing machine. If you bought the first-generation model, you could have relied on the integrated graphics, or equipped it with a discrete Radeon RX 7700S. Now, you get the option to buy NVIDIA’s GeForce RTX 5070 with 8GB DDR7 RAM which you can add to the chassis yourself. The company has also repackaged the existing Radeon RX 7700S with the promise of less fan noise and better thermal performance than the previous model. The new GPU pulls some of the focus away from the new mainboards, which are equipped with a choice of AMD’s Ryzen AI 7 350 or Ryzen AI 9 HZ 370, both of which promise to deliver 45W TDP. As before, you can equip the board with up to 86GB RAM, one or two SSDs, and your pick of ports via the six expansion card slots housed in the chassis. If you’re buying the laptop new, you’ll get a raft of smaller upgrades, starting with a new 165Hz, 2,560 x 1,600 panel which supports NVIDIA G-Sync. Plus, a new top cover, improved keyboard, number pad, webcam, Wi-Fi 7 support and an upgraded 240W power adapter. Sadly, I can’t talk about these as I was testing the upgrade from the 2024 model which just included the new mainboard and GPU module. Framework did listen to gripes about that rear-slung USB-C port which previously didn’t support charging. It was an omission that severely vexed my colleague Devindra Hardawar in his review of the original machine. But now, if you splurge for the RTX 5070, you can now use the rear port in the way that most people would intend. (If you’re unfamiliar, the Laptop 16’s discrete GPUs are packaged in self-contained “Expansion Modules” that go into the back of the chassis. The Radeon version could only be used for accessories and/or connecting additional displays.) Rounding out the changes is Framework’s continual promise that it’s improved the cooling situation. The thermal paste has been switched out for Honeywell PTM, there’s a new, redesigned fan geometry and tweaked pipes for better airflow. And, look, I don’t want to ding Framework for failing to deliver on one promise when it’s kept so many others. But if you’ve followed the company for any length of time, you already know what I’m gonna say in the In-use section. The obligatory AMD port compromise Graphic showing which ports work with which cards with an AMD mainboard. Framework As is custom whenever discussing an AMD-toting Framework machine, you’ll need to memorize the diagram of which expansion card slots will work with which devices. We’re not going to ding Framework for an issue present in all AMD hardware, and the only reason it’s noticeable here is that you have the choice of which ports to use for what. You don’t have the sort of universal port flexibility that you might otherwise be expecting. Installation Laptop 16 is bigger and more complex than its smaller siblings, but that doesn’t mean it’s any harder to maintain. The company’s iFixit-style guides hold your hand so well that popping the mid plate off should feel as natural as breathing. And you get a real sense of how well the components are laid out when you’re asked to take them all apart and put them back together. The company says replacing the mainboard and graphics module should take you an hour, which is far too generous. It took me about 22 minutes to get everything swapped in and set up, to the point where I think installing the new drivers was more laborious than this. I can’t stress enough how much of a feat it is to have a modular, upgradeable gaming laptop that offers you the chance to leap a generation. Being able to pull out a two-year-old Radeon to swap in a fresh RTX is the stuff of dreams (for some people, at least). Imagine how long it’ll be possible to keep this machine going if this type of bi-annual upgrade cycle continues. This isn’t a particularly difficult process, making it easy enough for those folks who would otherwise blanch at the idea. In-use Image of the 2025 mainboard and expansion modules for the Framework Laptop 16. Daniel Cooper for Engadget Of course, strapping such a powerful chip and graphics [INAUDIBLE DUE TO FAN NOISE] lead to issues. As discrete components, both the mainboard and expansion module need their own self-contained cooling. That’s never going to be as efficient as a holistically designed laptop. When you’re not taxing the machine, it’s not an issue at all, it’s only when you use it for its intended purpose that it becomes a serious problem. If you want to play games with this thing, get headphones or put the subtitles on, and don’t even think about using this in public. Did… did you hear that? CAN YOU HEAR ME? I SAID… AS DISCRETE COMPONENTS… And that’s before we get to the heat that this thing kicks out. I’ve got my unit on a stand with about four inches of clearance from the desk. I put my hand underneath the chassis to feel how warm it was getting and it was enough to make me never want to put this on my lap, ever. It’s a shame the noise and heat is such a bear as it’s a machine with sufficient grunt to impress many a jaded enthusiast. I set Cyberpunk 2077 to the highest settings I could (Ray Tracing: Overdrive) on 1080p, and it was able to comfortably produce 140 fps. Setting it to the defaults (Ray Tracing: Low, but the resolution set to the display’s maximum) it was able to crank out 182 fps. You’ll find similarly-impressive performance if you use the Laptop 16 more for productivity than gaming. It compressed a 38GB 4K video file down to an 8GB HD mp4 in 28 minutes and 29 seconds. Using LM Studio, I was able to run Google’s Gemma 3 27B model with what I’d call fairly decent performance. Certainly, the chatbot wasn’t responding as quickly as Gemini would online, but it was hardly stuttering. I’d say that the performance here is more or less what you’d expect from the specs, with the one downside being that godawful fan noise. Pricing If you buy a new Laptop 16 pre-built from Framework, the Ryzen AI 7 configuration starts at $1,500, the AI 9 at $1,800. Add in the RTX 5070 and you can add another $699 to that price, which is the same cost as if you buy the GPU standalone as an upgrade. Or, if money’s tight, you could buy the new machine now and then add in the 5070 whenever you’d like — that’s the benefit of modularity. It should be obvious you can get laptops with these sorts of components for less if you look elsewhere. In the run-up to the holiday season, I’ve seen machines — such as HP’s Omen Max — offer a Ryzen AI 7 and an RTX 5070 Ti for under $2,000. But here you’re not just buying a laptop, you’re buying into Framework’s broader ethos. You’ll get the fastest machine it can sell you right now, plus the ability to cheaply swap out to the next big thing in a couple years’ time without the cost of buying a new machine. As I said back when reviewing the Ryzen AI 300 upgrades for the Laptop 13 earlier this year, Framework is well placed to take advantage of the world’s political situation. If the price of a whole new laptop skyrockets, then you can at least make a saving by just replacing what you need. Wrap-up I wonder if “Should you get one?” is the best question to ask and answer given the singular furrow Framework is ploughing. If you want a powerful laptop where every part can be replaced or upgraded, you don’t really have a serious alternative. Laptop 16’s natural target market is professionals and enthusiasts who value modularity and longevity over everything else. These new components give you enough power to play games, run AI models locally and whatever other demanding tasks you’ll throw at it. As for everyone else, it’s a question of how willing you are to accept the heat, the noise and the slightly agricultural aesthetics. After all, this machine isn’t the sort of gadget you’ll be looking to move on in a few years’ time, it’ll be one you’re committing to for a long while. This article originally appeared on Engadget at https://www.engadget.com/computing/laptops/framework-laptop-16-2025-upgrade-review-the-rtx-5070-is-the-star-160000464.html?src=rss",
          "feed_position": 13,
          "image_url": "https://d29szjachogqwa.cloudfront.net/images/user-uploaded/FW16Ports.jpg"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/entertainment/streaming/the-youtube-tv-disney-blackout-continues-how-to-watch-wednesday-nba-games-and-prep-for-weekend-college-football-173330100.html",
          "published_at": "Wed, 12 Nov 2025 15:21:13 +0000",
          "title": "The YouTube TV Disney blackout continues: How to watch Wednesday NBA games and prep for weekend college football",
          "standfirst": "Engadget The Disney/YouTube TV saga, now in its second week, is still showing no sign of a resolution. While the blackout is painful for subscribers who have been left without access to over 20 channels, Disney is also feeling the pinch, with reports estimating they're losing $4.3 million per day during the dispute. The good news for YouTube subscribers is that the platform has finally started issuing $20 credits as consolation for their troubles, but will that be enough to keep their base from jumping ship and finding a new streaming service? As a reminder of how we got here, the Walt Disney Co. pulled its channels from YouTube TV on Oct. 30 after the two companies failed to reach new terms on their latest carriage agreement, and YouTube TV subscribers have gone without NFL, NBA and NCAA games on ABC and ESPN's suite of channels for two straight weekends. With no agreement in sight, YouTube TV subscribers will be left in the dark for tonight's NBA games, too. Tonight's basketball games between the Orlando Magic vs. New York Knicks and Los Angeles Lakers vs. Oklahoma City Thunder both air on ESPN, so if you want to catch either game (or watch The Golden Bachelor season finale on ABC!), you'll need to seek out alternative viewing methods. And unfortunately for YouTube TV's negotiating position, there are plenty of options. One of the cheapest ways to watch ESPN is with a Sling Day Pass — for just $5/day, you can tune into any and all ESPN programming with no other commitments. If you want a full switch from YouTube TV, there's Hulu + Live TV, DirecTV, or Fubo, where you can watch all the Disney-owned channels. (Remember, unlike a lot of cable plans, you can easily pause or cancel YouTube TV or any of these alternatives, so long as you have month-to-month subscriptions.) Below, we've outlined some of your best options to watch ESPN, the Disney Channel, ABC and more, all pulled from our list of best live TV streaming services to cut cable, as well as a comprehensive list of which channels have been affected, and the biggest sporting events of the week that won't be available to YouTube TV subscribers. What games are on ESPN/ABC this week? If you're wondering what games you might miss as a result of the YouTube TV/Disney blackout, here's a list of some upcoming sports you may not want to miss: NBA Wednesday, Nov. 12 7 p.m. | Orlando Magic vs. New York Knicks | ESPN 9:35 p.m. | Los Angeles Lakers vs. Oklahoma City Thunder | ESPN NCAA Football Thursday, Nov. 13 7:30 p.m. | Troy at Old Dominion | ESPN Friday, Nov. 14 5:30 p.m. | South Carolina State at North Carolina Central | ESPN27:30 p.m. | Clemson at No. 20 Louisville | ESPN Grab an ESPN bundle so you won't miss the NFL, NBA or any other games Get Hulu + Live TV at a great price Try Fubo free for a week and get $30 your first month Try DirecTV free for 5 days, and get $30 off your first month What about Sling \"day passes\"? You may have heard that Sling offers day, weekend and week passes to its streaming programming for as little as $5 per day. That is an option if you're looking for just some of the ESPN channels (the Sling Orange tier), but ABC isn't included. (If you're just looking to catch one of this week's big games, like Monday Night Football on ESPN, it's a great short-term solution.) If you want a longer-term solution, you can get both ESPN and ABC with Sling's Orange and Blue package ($30 a month to start, $61 thereafter), but you'll need to add on the Sports Extra package for ESPNU, which requires an additional charge. Get your local Disney/ABC programming for free Need your local ABC programming? Your station may have its own free local streaming news channel (many do), you can see if The Roku Channel carries your local station's news, or download your local news station app if it's a Nexstar channel. The other alternative — if you're within the broadcast radius of a local ABC affiliate — is to get an over-the-air antenna. You can plug in your ZIP code at antennaweb.org to see what channels are in your area. This off-brand unit has worked very well in our initial testing — it's under $30, and the channels are truly free. Which channels are no longer available on YouTube TV? Every channel that's owned by The Walt Disney Company is currently blacked out on YouTube TV. Those channels are: ABC ABC News Live ACC Network Disney Channel Disney Junior Disney XD ESPN ESPNews ESPN2 ESPNU Freeform FX FXM FXX Localish Nat Geo Nat Geo Wild SEC Network ESPN Deportes Baby TV Español Nat Geo Mundo Update Nov. 10 2025, 4:43PM ET: This story has been updated to include news on the $20 rebate for YouTube TV subscribers, as well as to update the list of upcoming football games for the week. Update Nov. 6 2025, 4:38PM ET: This story has been updated to include viewing info for weekend college football games, as well as the next Monday Night Football. Update Nov. 5 2025, 12:32PM ET: This story has been updated to include detailed info on tonight's ESPN NBA games. Update Nov. 3 2025, 6:36PM ET: This story has been updated to include YouTube TV's latest response to Disney's request to restore its channels for just 24 hours.This article originally appeared on Engadget at https://www.engadget.com/entertainment/streaming/the-youtube-tv-disney-blackout-continues-how-to-watch-wednesday-nba-games-and-prep-for-weekend-college-football-173330100.html?src=rss",
          "content": "Engadget The Disney/YouTube TV saga, now in its second week, is still showing no sign of a resolution. While the blackout is painful for subscribers who have been left without access to over 20 channels, Disney is also feeling the pinch, with reports estimating they're losing $4.3 million per day during the dispute. The good news for YouTube subscribers is that the platform has finally started issuing $20 credits as consolation for their troubles, but will that be enough to keep their base from jumping ship and finding a new streaming service? As a reminder of how we got here, the Walt Disney Co. pulled its channels from YouTube TV on Oct. 30 after the two companies failed to reach new terms on their latest carriage agreement, and YouTube TV subscribers have gone without NFL, NBA and NCAA games on ABC and ESPN's suite of channels for two straight weekends. With no agreement in sight, YouTube TV subscribers will be left in the dark for tonight's NBA games, too. Tonight's basketball games between the Orlando Magic vs. New York Knicks and Los Angeles Lakers vs. Oklahoma City Thunder both air on ESPN, so if you want to catch either game (or watch The Golden Bachelor season finale on ABC!), you'll need to seek out alternative viewing methods. And unfortunately for YouTube TV's negotiating position, there are plenty of options. One of the cheapest ways to watch ESPN is with a Sling Day Pass — for just $5/day, you can tune into any and all ESPN programming with no other commitments. If you want a full switch from YouTube TV, there's Hulu + Live TV, DirecTV, or Fubo, where you can watch all the Disney-owned channels. (Remember, unlike a lot of cable plans, you can easily pause or cancel YouTube TV or any of these alternatives, so long as you have month-to-month subscriptions.) Below, we've outlined some of your best options to watch ESPN, the Disney Channel, ABC and more, all pulled from our list of best live TV streaming services to cut cable, as well as a comprehensive list of which channels have been affected, and the biggest sporting events of the week that won't be available to YouTube TV subscribers. What games are on ESPN/ABC this week? If you're wondering what games you might miss as a result of the YouTube TV/Disney blackout, here's a list of some upcoming sports you may not want to miss: NBA Wednesday, Nov. 12 7 p.m. | Orlando Magic vs. New York Knicks | ESPN 9:35 p.m. | Los Angeles Lakers vs. Oklahoma City Thunder | ESPN NCAA Football Thursday, Nov. 13 7:30 p.m. | Troy at Old Dominion | ESPN Friday, Nov. 14 5:30 p.m. | South Carolina State at North Carolina Central | ESPN27:30 p.m. | Clemson at No. 20 Louisville | ESPN Grab an ESPN bundle so you won't miss the NFL, NBA or any other games Get Hulu + Live TV at a great price Try Fubo free for a week and get $30 your first month Try DirecTV free for 5 days, and get $30 off your first month What about Sling \"day passes\"? You may have heard that Sling offers day, weekend and week passes to its streaming programming for as little as $5 per day. That is an option if you're looking for just some of the ESPN channels (the Sling Orange tier), but ABC isn't included. (If you're just looking to catch one of this week's big games, like Monday Night Football on ESPN, it's a great short-term solution.) If you want a longer-term solution, you can get both ESPN and ABC with Sling's Orange and Blue package ($30 a month to start, $61 thereafter), but you'll need to add on the Sports Extra package for ESPNU, which requires an additional charge. Get your local Disney/ABC programming for free Need your local ABC programming? Your station may have its own free local streaming news channel (many do), you can see if The Roku Channel carries your local station's news, or download your local news station app if it's a Nexstar channel. The other alternative — if you're within the broadcast radius of a local ABC affiliate — is to get an over-the-air antenna. You can plug in your ZIP code at antennaweb.org to see what channels are in your area. This off-brand unit has worked very well in our initial testing — it's under $30, and the channels are truly free. Which channels are no longer available on YouTube TV? Every channel that's owned by The Walt Disney Company is currently blacked out on YouTube TV. Those channels are: ABC ABC News Live ACC Network Disney Channel Disney Junior Disney XD ESPN ESPNews ESPN2 ESPNU Freeform FX FXM FXX Localish Nat Geo Nat Geo Wild SEC Network ESPN Deportes Baby TV Español Nat Geo Mundo Update Nov. 10 2025, 4:43PM ET: This story has been updated to include news on the $20 rebate for YouTube TV subscribers, as well as to update the list of upcoming football games for the week. Update Nov. 6 2025, 4:38PM ET: This story has been updated to include viewing info for weekend college football games, as well as the next Monday Night Football. Update Nov. 5 2025, 12:32PM ET: This story has been updated to include detailed info on tonight's ESPN NBA games. Update Nov. 3 2025, 6:36PM ET: This story has been updated to include YouTube TV's latest response to Disney's request to restore its channels for just 24 hours.This article originally appeared on Engadget at https://www.engadget.com/entertainment/streaming/the-youtube-tv-disney-blackout-continues-how-to-watch-wednesday-nba-games-and-prep-for-weekend-college-football-173330100.html?src=rss",
          "feed_position": 14,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2025-11/0c8d0fa0-bb59-11f0-ab9e-898961c799a9"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/nintendo-announces-its-black-friday-and-cyber-monday-2025-sale-switch-2-bundles-switch-game-deals-accessories-and-more-155223057.html",
          "published_at": "Wed, 12 Nov 2025 15:15:35 +0000",
          "title": "Nintendo announces its Black Friday and Cyber Monday 2025 sale: Switch 2 bundles, Switch game deals, accessories and more",
          "standfirst": "Nintendo gear is always sought after during the holiday shopping season, but this year likely more so than others. The Nintendo Switch 2 is the console launch of 2025 and it will undoubtedly be at the top of many wish lists for both kids and adults alike. If you were hoping to save a bit on the console during the Black Friday shopping season, you may be disappointed. The Nintendo Black Friday sale was just announced, and unsurprisingly, there aren't a lot of true \"deals\" to be had. This is typical of Nintendo — legit Nintendo Black Friday deals are hard to come by — but there are ways to at least get the best value for your money if you're going to pick up a Switch 2 before the year is out. As has been the case for many years, the marquee Nintendo deals for the holidays come in the form of console bundles. When the Switch 2 launched earlier this year, it was available as just the console only for $449 or bundled with Mario Kart World for $499. Both options are still available now, but there's a new bundle to consider as well — the console with the new Pokémon Legends: Z-A game, which also costs $499. Considering the games by themselves cost $70 each, you do save a bit by picking up a console bundle. you can pick up the console and its bundles at most retailers including Amazon, Walmart, Best Buy and others. When it comes to deals on Nintendo Switch 2 games, the Nintendo eShop will have Cyber Deals starting on November 20, running through December 3. The shop will feature \"holiday offers on select games,\" so it appears we'll all just have to go to the online store on November 20 to see the games on offer. Starting on November 23, select retailers will have discounts on some physical Switch games including Princess Peach: Showtime!, The Legend of Zelda: Echoes of Wisdom, Luigi’s Mansion 3 and Kirby’s Return to Dream Land Deluxe. Those will each be $40, while other games like Super Mario Odyssey, Nintendo Switch Sports, Paper Mario: The Thousand-Year Door and Splatoon 3 will be $30. Even if you can't get huge discounts on Nintendo consoles or new games this year, that doesn't mean you can't find decent deals on other Nintendo gear. There are plenty of great ideas for gifts for the Nintendo fan in your life, and Engadget's Sam Rutherford got to see a bunch of them in person when he attended Nintendo's holiday showcase. From collectibles to clothing to plushies and holiday decor, there's really a ton to choose from — but you may want to pace yourself if you're also a Nintendo fan finding things that you want to pick up for yourself in the process of looking for good gifts. Here are just some of the best Nintendo gift ideas that you can look out for during Black Friday and Cyber Monday. This article originally appeared on Engadget at https://www.engadget.com/deals/nintendo-announces-its-black-friday-and-cyber-monday-2025-sale-switch-2-bundles-switch-game-deals-accessories-and-more-155223057.html?src=rss",
          "content": "Nintendo gear is always sought after during the holiday shopping season, but this year likely more so than others. The Nintendo Switch 2 is the console launch of 2025 and it will undoubtedly be at the top of many wish lists for both kids and adults alike. If you were hoping to save a bit on the console during the Black Friday shopping season, you may be disappointed. The Nintendo Black Friday sale was just announced, and unsurprisingly, there aren't a lot of true \"deals\" to be had. This is typical of Nintendo — legit Nintendo Black Friday deals are hard to come by — but there are ways to at least get the best value for your money if you're going to pick up a Switch 2 before the year is out. As has been the case for many years, the marquee Nintendo deals for the holidays come in the form of console bundles. When the Switch 2 launched earlier this year, it was available as just the console only for $449 or bundled with Mario Kart World for $499. Both options are still available now, but there's a new bundle to consider as well — the console with the new Pokémon Legends: Z-A game, which also costs $499. Considering the games by themselves cost $70 each, you do save a bit by picking up a console bundle. you can pick up the console and its bundles at most retailers including Amazon, Walmart, Best Buy and others. When it comes to deals on Nintendo Switch 2 games, the Nintendo eShop will have Cyber Deals starting on November 20, running through December 3. The shop will feature \"holiday offers on select games,\" so it appears we'll all just have to go to the online store on November 20 to see the games on offer. Starting on November 23, select retailers will have discounts on some physical Switch games including Princess Peach: Showtime!, The Legend of Zelda: Echoes of Wisdom, Luigi’s Mansion 3 and Kirby’s Return to Dream Land Deluxe. Those will each be $40, while other games like Super Mario Odyssey, Nintendo Switch Sports, Paper Mario: The Thousand-Year Door and Splatoon 3 will be $30. Even if you can't get huge discounts on Nintendo consoles or new games this year, that doesn't mean you can't find decent deals on other Nintendo gear. There are plenty of great ideas for gifts for the Nintendo fan in your life, and Engadget's Sam Rutherford got to see a bunch of them in person when he attended Nintendo's holiday showcase. From collectibles to clothing to plushies and holiday decor, there's really a ton to choose from — but you may want to pace yourself if you're also a Nintendo fan finding things that you want to pick up for yourself in the process of looking for good gifts. Here are just some of the best Nintendo gift ideas that you can look out for during Black Friday and Cyber Monday. This article originally appeared on Engadget at https://www.engadget.com/deals/nintendo-announces-its-black-friday-and-cyber-monday-2025-sale-switch-2-bundles-switch-game-deals-accessories-and-more-155223057.html?src=rss",
          "feed_position": 16
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/mobile/even-realities-g2-first-look-this-years-best-looking-new-smart-glasses-still-need-work-151500132.html",
          "published_at": "Wed, 12 Nov 2025 15:15:00 +0000",
          "title": "Even Realities G2 first look: This year's best-looking new smart glasses still need work",
          "standfirst": "A lot of people think the original Google Glass failed because of subpar tech. But the larger issue was that they were so ugly that people simply didn't want to wear them. And when it's a device that sits on your face, that's kind of important. Thankfully, that's a lesson Even Realities seemingly took to heart when it made the G1, which combined the stylishness of proper eyewear with built-in waveguide displays. Now the company is back with its second-gen smart glasses — the G2 — which offer even better optics, an updated UI and a companion ring (the R1) to their expanded list of features. While the design of the Even G2 hasn't changed a ton from its predecessor, that's not a bad thing as there wasn't much to fix. Compared to every other pair of smart glasses on sale today, these look the most like normal eyewear. Meanwhile, thanks to a titanium and magnesium alloy design and the company's very intentional decision not to include onboard cameras or speakers, the new model only weighs 36 grams, so they're also super comfortable. You even get two styles to choose from (panto or rectangular) along with a few different finishes (gray, brown and green), so chances are there's a combination that will work with your wardrobe. The Even G2 also features an IP67 rating for dust and water resistance, so you don't need to worry about wearing them in adverse conditions. The G2's battery life can last up to two days, while its case has juice for up to seven additional charges. Sam Rutherford for EngadgetThe G2’s optics are based on the company's new Even HAO 2.0 (Holistic Adaptive Optics) that use custom-designed dual waveguides to produce a green heads-up display. It’s not only brighter and sharper than before but also features a wider field of view. Now admittedly, that's not quite as impressive as the single RGB display in something like the Ray-Ban Meta Display. But considering that the Even G2 is way less chunky and sports significantly longer battery life (up to two days plus seven full recharges from its included case), that feels like a worthy trade-off to deliver increased usability.However, the biggest upgrade for the G2 is an expanded roster of smart features. In addition to showing notifications, turn-by-turn directions and the teleprompter functionality available on the previous model, Even has developed a much more robust UI that lets you read the news, track stock prices and create lists using your voice. There is now an onboard AI assistant and LLM that can translate speech on the fly or pick out key words during a conversation to give more background and context about unfamiliar terms. This means the glasses can handle a lot of tasks you'd normally need your phone for, which is great if you're walking around, or similarly indisposed, and would rather keep your hands free. In addition to basic health and fitness tracking, the R1 ring also has a tiny built-in touchpad (denoted by the four dots) to make it easier to navigate the G2's new UI and menus. Sam Rutherford for EngadgetWhile you can access most of these features using voice controls, the icing on the cake is the R1 companion ring that makes better use of the new UI. Not only does it track some basic health and fitness data (steps, heart rate, sleep, SpO2 and more), it also serves as a tiny touchpad so you can check notifications, revisit your notes and more without anyone nearby knowing what you're up to. All told, the G2 glasses and the R1 ring create a very stylish and discreet package that allows you to stay connected and keep your phone in your pocket. That said, you might want to wait before throwing money down on these. I've been testing the G2 and R1 over the past few days, and even though I really like the hardware, the company's software just isn't ready yet. It's important to note that I've been using a beta version of the Even Realties app, so encountering some bugs was not entirely unexpected. But even so, the touch controls on the ring feel imprecise and occasionally erratic. Many of the fitness metrics aren't being properly recorded and both devices have had a difficult time staying paired to the app. Even little things, like the auto brightness settings (which don't work at the moment) or ability to add more sources to the glasses' news feed, feel janky. The only outlet I've been able to successfully use is ABC. While the tech and features of the G2 are very interesting, I've run into a number of software issues while using a beta version of the app. Sam Rutherford for EngadgetGranted, some of the G2 and R1's issues that I've run into, like wonky touch input and the unfinished health tracking, are known problems that are currently being worked on by Even Realities. Still, this feels like a situation where the launch of these devices should have been delayed until the company could smooth out these hiccups.But if you are undeterred, the Even G2 Display Smart Glasses and Even R1 Smart Ring go on sale today for $599 and $249, respectively. For everyone else, I'm hoping to check back in on these devices after the company pushes out some software updates. I’m eager to see if they can eventually live up to their potential as an interesting alternative to bigger, chunkier and more intrusive smartglass alternatives like the Ray-Ban Meta Displays.This article originally appeared on Engadget at https://www.engadget.com/mobile/even-realities-g2-first-look-this-years-best-looking-new-smart-glasses-still-need-work-151500132.html?src=rss",
          "content": "A lot of people think the original Google Glass failed because of subpar tech. But the larger issue was that they were so ugly that people simply didn't want to wear them. And when it's a device that sits on your face, that's kind of important. Thankfully, that's a lesson Even Realities seemingly took to heart when it made the G1, which combined the stylishness of proper eyewear with built-in waveguide displays. Now the company is back with its second-gen smart glasses — the G2 — which offer even better optics, an updated UI and a companion ring (the R1) to their expanded list of features. While the design of the Even G2 hasn't changed a ton from its predecessor, that's not a bad thing as there wasn't much to fix. Compared to every other pair of smart glasses on sale today, these look the most like normal eyewear. Meanwhile, thanks to a titanium and magnesium alloy design and the company's very intentional decision not to include onboard cameras or speakers, the new model only weighs 36 grams, so they're also super comfortable. You even get two styles to choose from (panto or rectangular) along with a few different finishes (gray, brown and green), so chances are there's a combination that will work with your wardrobe. The Even G2 also features an IP67 rating for dust and water resistance, so you don't need to worry about wearing them in adverse conditions. The G2's battery life can last up to two days, while its case has juice for up to seven additional charges. Sam Rutherford for EngadgetThe G2’s optics are based on the company's new Even HAO 2.0 (Holistic Adaptive Optics) that use custom-designed dual waveguides to produce a green heads-up display. It’s not only brighter and sharper than before but also features a wider field of view. Now admittedly, that's not quite as impressive as the single RGB display in something like the Ray-Ban Meta Display. But considering that the Even G2 is way less chunky and sports significantly longer battery life (up to two days plus seven full recharges from its included case), that feels like a worthy trade-off to deliver increased usability.However, the biggest upgrade for the G2 is an expanded roster of smart features. In addition to showing notifications, turn-by-turn directions and the teleprompter functionality available on the previous model, Even has developed a much more robust UI that lets you read the news, track stock prices and create lists using your voice. There is now an onboard AI assistant and LLM that can translate speech on the fly or pick out key words during a conversation to give more background and context about unfamiliar terms. This means the glasses can handle a lot of tasks you'd normally need your phone for, which is great if you're walking around, or similarly indisposed, and would rather keep your hands free. In addition to basic health and fitness tracking, the R1 ring also has a tiny built-in touchpad (denoted by the four dots) to make it easier to navigate the G2's new UI and menus. Sam Rutherford for EngadgetWhile you can access most of these features using voice controls, the icing on the cake is the R1 companion ring that makes better use of the new UI. Not only does it track some basic health and fitness data (steps, heart rate, sleep, SpO2 and more), it also serves as a tiny touchpad so you can check notifications, revisit your notes and more without anyone nearby knowing what you're up to. All told, the G2 glasses and the R1 ring create a very stylish and discreet package that allows you to stay connected and keep your phone in your pocket. That said, you might want to wait before throwing money down on these. I've been testing the G2 and R1 over the past few days, and even though I really like the hardware, the company's software just isn't ready yet. It's important to note that I've been using a beta version of the Even Realties app, so encountering some bugs was not entirely unexpected. But even so, the touch controls on the ring feel imprecise and occasionally erratic. Many of the fitness metrics aren't being properly recorded and both devices have had a difficult time staying paired to the app. Even little things, like the auto brightness settings (which don't work at the moment) or ability to add more sources to the glasses' news feed, feel janky. The only outlet I've been able to successfully use is ABC. While the tech and features of the G2 are very interesting, I've run into a number of software issues while using a beta version of the app. Sam Rutherford for EngadgetGranted, some of the G2 and R1's issues that I've run into, like wonky touch input and the unfinished health tracking, are known problems that are currently being worked on by Even Realities. Still, this feels like a situation where the launch of these devices should have been delayed until the company could smooth out these hiccups.But if you are undeterred, the Even G2 Display Smart Glasses and Even R1 Smart Ring go on sale today for $599 and $249, respectively. For everyone else, I'm hoping to check back in on these devices after the company pushes out some software updates. I’m eager to see if they can eventually live up to their potential as an interesting alternative to bigger, chunkier and more intrusive smartglass alternatives like the Ray-Ban Meta Displays.This article originally appeared on Engadget at https://www.engadget.com/mobile/even-realities-g2-first-look-this-years-best-looking-new-smart-glasses-still-need-work-151500132.html?src=rss",
          "feed_position": 17,
          "image_url": "https://d29szjachogqwa.cloudfront.net/images/user-uploaded/Even-G2-edited.jpg"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/wearables/even-realities-g2-smart-glasses-can-be-controlled-with-a-smart-ring-151500125.html",
          "published_at": "Wed, 12 Nov 2025 15:15:00 +0000",
          "title": "Even Realities' G2 smart glasses can be controlled with a smart ring",
          "standfirst": "Even Realities is releasing the next version of its lightweight smart glasses, and this time it’s pairing them with an entirely new device that will act as both a controller and health tracker. The Even G2 Display Smart Glasses are a more refined version of the G1 smart glasses, and you'll be able to interact with them without having to touch the frames or uses voice commands thanks to the new Even R1 Smart Ring.The G2 features an updated version of the Even Realities' HAO optical engine (a combination of micro-LED projectors, waveguides and digitally surfaced lenses), that still displays text in bright green, but now with added depth and sharpness. For example, Even Realities says the new glasses can display pop-ups like AI prompts and notifications at a different depth from the glasses' normal interface, so you don't lose the context of whatever you're reading. The G2 also supports a wider range of prescription lenses (from -12 to +12 diopters), making them more accessible to people who already wear glasses.In comparison to the G1, Even Realities says the G2 has 54 percent slimmer temples and, in a first for the company, is IP67-rated for dust and water resistance. The G2 also gets two days of battery life in comparison to the G1's one and a half days, and its charging case can provide up to seven full charges.A ceramic and titanium R1 smart ring with a flat side.Even RealitiesWhile the G2 glasses can still be controlled by tapping a built-in touchpad or using voice commands, the R1 Smart Ring will let you interact with the smart glasses without moving your arm. The R1 is made from ceramic and medical-grade stainless steel, and features a flat touchpad surface for activating the G2's interface. Besides a touch sensor for navigation, the R1 also includes an optical heart rate sensor and accelerometer for tracking your heart rate and steps, which can be viewed on the G2. While less technically advanced than the Neural Band Meta included with the Meta Ray-Ban Display Glasses, Even Realities ring seems like it can make controlling its glasses similarly discrete, while being useful in its own right as a health tracker.Even Realities is also adding a new AI skill alongside its glasses and ring. The G2 supports the same translation, notifications and teleprompter features as the company's last model, but this time includes a feature called \"Conversate\" that attempts to offer AI-generated information during conversations. Even Realities says Conversate can provide explanations, context and follow-up questions during a conversation, and then generate a summary and key points once you're finished talking. The whole thing sounds a bit distracting, but might be something you have to demo to understand.A pair of Even G2 Display Smart Glasses in a charging case.Even RealitiesThat extra layer of complication seems inherent to the pitch for both the G2 glasses and the R1 ring. While Even Realities has made its smart glasses more convenient, and they're definitely not trying to be a phone replacement in the same way Meta's glasses are, they do seem like they'll have more of a learning curve than the last generation.The Even G2 Display Smart Glasses and Even R1 Smart Ring are available to order today, November 12, for $599 and $249, respectively. Even Realities says that anyone who purchases the G2 will be able to receive the R1 and other accessories for 50 percent off for a limited time.This article originally appeared on Engadget at https://www.engadget.com/wearables/even-realities-g2-smart-glasses-can-be-controlled-with-a-smart-ring-151500125.html?src=rss",
          "content": "Even Realities is releasing the next version of its lightweight smart glasses, and this time it’s pairing them with an entirely new device that will act as both a controller and health tracker. The Even G2 Display Smart Glasses are a more refined version of the G1 smart glasses, and you'll be able to interact with them without having to touch the frames or uses voice commands thanks to the new Even R1 Smart Ring.The G2 features an updated version of the Even Realities' HAO optical engine (a combination of micro-LED projectors, waveguides and digitally surfaced lenses), that still displays text in bright green, but now with added depth and sharpness. For example, Even Realities says the new glasses can display pop-ups like AI prompts and notifications at a different depth from the glasses' normal interface, so you don't lose the context of whatever you're reading. The G2 also supports a wider range of prescription lenses (from -12 to +12 diopters), making them more accessible to people who already wear glasses.In comparison to the G1, Even Realities says the G2 has 54 percent slimmer temples and, in a first for the company, is IP67-rated for dust and water resistance. The G2 also gets two days of battery life in comparison to the G1's one and a half days, and its charging case can provide up to seven full charges.A ceramic and titanium R1 smart ring with a flat side.Even RealitiesWhile the G2 glasses can still be controlled by tapping a built-in touchpad or using voice commands, the R1 Smart Ring will let you interact with the smart glasses without moving your arm. The R1 is made from ceramic and medical-grade stainless steel, and features a flat touchpad surface for activating the G2's interface. Besides a touch sensor for navigation, the R1 also includes an optical heart rate sensor and accelerometer for tracking your heart rate and steps, which can be viewed on the G2. While less technically advanced than the Neural Band Meta included with the Meta Ray-Ban Display Glasses, Even Realities ring seems like it can make controlling its glasses similarly discrete, while being useful in its own right as a health tracker.Even Realities is also adding a new AI skill alongside its glasses and ring. The G2 supports the same translation, notifications and teleprompter features as the company's last model, but this time includes a feature called \"Conversate\" that attempts to offer AI-generated information during conversations. Even Realities says Conversate can provide explanations, context and follow-up questions during a conversation, and then generate a summary and key points once you're finished talking. The whole thing sounds a bit distracting, but might be something you have to demo to understand.A pair of Even G2 Display Smart Glasses in a charging case.Even RealitiesThat extra layer of complication seems inherent to the pitch for both the G2 glasses and the R1 ring. While Even Realities has made its smart glasses more convenient, and they're definitely not trying to be a phone replacement in the same way Meta's glasses are, they do seem like they'll have more of a learning curve than the last generation.The Even G2 Display Smart Glasses and Even R1 Smart Ring are available to order today, November 12, for $599 and $249, respectively. Even Realities says that anyone who purchases the G2 will be able to receive the R1 and other accessories for 50 percent off for a limited time.This article originally appeared on Engadget at https://www.engadget.com/wearables/even-realities-g2-smart-glasses-can-be-controlled-with-a-smart-ring-151500125.html?src=rss",
          "feed_position": 18,
          "image_url": "https://d29szjachogqwa.cloudfront.net/images/user-uploaded/Even-R1-Smart-Ring.jpg"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/one-year-of-headspace-drops-to-only-35-with-this-black-friday-deal-163051565.html",
          "published_at": "Wed, 12 Nov 2025 15:00:36 +0000",
          "title": "One year of Headspace drops to only $35 with this Black Friday deal.",
          "standfirst": "Meditation app Headspace is bringing back one of its biggest annual deals this Black Friday. Through December 4, you’ll be able to get 50 percent off the regular annual subscription price, bringing a full year of guided meditations, sleep sounds and mindfulness tools down to $35 per year. If you’ve been looking to build a better daily routine, this discount makes it easier to start. Headspace has become one of the most recognizable names in digital mindfulness. The app blends practical meditation guidance with structured courses and calming soundscapes designed to make everyday stress easier to manage. Its programs cover everything from beginner-friendly introductions to mindfulness to focused content on topics like anxiety, productivity and sleep. Subscribers get access to hundreds of guided sessions led by the Headspace team, including short daily practices that can be completed in a few spare minutes, plus longer courses that help build consistency. The app’s Sleepcasts and soundscapes are unique, designed to create a steady nighttime routine that promotes better rest. For mornings, there are breathing exercises and motivational mini-sessions that can help set focus for the day ahead. Headspace also includes personalized progress tracking, mood check-ins and optional reminders that make it easier to stay consistent with your new mindfulness habits. For anyone new to meditation, the app’s clear structure is a major strength. You don’t have to know where to start, since it suggests sessions based on your goals or current mood. This annual deal is ideal for users who want to stick with mindfulness practice over time, or anyone interested in incorporating a new habit into their lives. Paying for the year upfront typically saves money compared with the monthly plan, and the discount brings that cost down even further. Whether you’re learning the basics of meditation or refining an existing routine, the full library provides enough variety to keep things engaging throughout the year. If you’re still comparing wellness apps, check out our guide to the best meditation apps to see how Headspace stacks up against other options. But for those ready to commit to a calmer routine, this annual offer is one of the simplest ways to start the habit at a lower cost.This article originally appeared on Engadget at https://www.engadget.com/deals/one-year-of-headspace-drops-to-only-35-with-this-black-friday-deal-163051565.html?src=rss",
          "content": "Meditation app Headspace is bringing back one of its biggest annual deals this Black Friday. Through December 4, you’ll be able to get 50 percent off the regular annual subscription price, bringing a full year of guided meditations, sleep sounds and mindfulness tools down to $35 per year. If you’ve been looking to build a better daily routine, this discount makes it easier to start. Headspace has become one of the most recognizable names in digital mindfulness. The app blends practical meditation guidance with structured courses and calming soundscapes designed to make everyday stress easier to manage. Its programs cover everything from beginner-friendly introductions to mindfulness to focused content on topics like anxiety, productivity and sleep. Subscribers get access to hundreds of guided sessions led by the Headspace team, including short daily practices that can be completed in a few spare minutes, plus longer courses that help build consistency. The app’s Sleepcasts and soundscapes are unique, designed to create a steady nighttime routine that promotes better rest. For mornings, there are breathing exercises and motivational mini-sessions that can help set focus for the day ahead. Headspace also includes personalized progress tracking, mood check-ins and optional reminders that make it easier to stay consistent with your new mindfulness habits. For anyone new to meditation, the app’s clear structure is a major strength. You don’t have to know where to start, since it suggests sessions based on your goals or current mood. This annual deal is ideal for users who want to stick with mindfulness practice over time, or anyone interested in incorporating a new habit into their lives. Paying for the year upfront typically saves money compared with the monthly plan, and the discount brings that cost down even further. Whether you’re learning the basics of meditation or refining an existing routine, the full library provides enough variety to keep things engaging throughout the year. If you’re still comparing wellness apps, check out our guide to the best meditation apps to see how Headspace stacks up against other options. But for those ready to commit to a calmer routine, this annual offer is one of the simplest ways to start the habit at a lower cost.This article originally appeared on Engadget at https://www.engadget.com/deals/one-year-of-headspace-drops-to-only-35-with-this-black-friday-deal-163051565.html?src=rss",
          "feed_position": 19
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/walmart-subscriptions-are-only-49-for-black-friday-and-it-includes-access-to-peacock-192739301.html",
          "published_at": "Wed, 12 Nov 2025 14:05:37 +0000",
          "title": "Walmart+ subscriptions are only $49 for Black Friday, and it includes access to Peacock",
          "standfirst": "If you've wanted to check out The Paper or any other new NBC show on Peacock, you can do so now while spending less thanks to this hack. Walmart, believe it or not, comes into play here: the retailer is offering Walmart+ subscriptions for half off right now, bringing the cost down to $49 for your first year. Thanks to a streaming benefit for subscribers, you can then sign up for Peacock at no extra cost. Walmart+ subscribers are able to choose between a Peacock Premium or a Paramount+ Essential subscription. Considering Peacock premium would run you $110 for the year on its own, signing up for Walmart+ while this discount is available gets you access to the streaming service for less than half the normal cost. Just about every major streaming service has raised its prices in the last year, including HBO Max, Disney+, Netflix, Apple TV and YouTube TV, so saving some money on one of them just might be worth the effort. Cord cutting is not nearly as affordable as it used to be, so finding a deal like this is pretty helpful. Walmart+ itself offers myriad additional benefits like early access to Black Friday deals, free shipping on orders over $35, discounts on gas, free online veterinary care and more. Earlier this year, Walmart+ subscribers got first dibs on the Nintendo Switch 2 at the retailer. You can also use that free shipping to take advantage of Walmart's drone delivery program in a handful of select cities.This article originally appeared on Engadget at https://www.engadget.com/deals/walmart-subscriptions-are-only-49-for-black-friday-and-it-includes-access-to-peacock-192739301.html?src=rss",
          "content": "If you've wanted to check out The Paper or any other new NBC show on Peacock, you can do so now while spending less thanks to this hack. Walmart, believe it or not, comes into play here: the retailer is offering Walmart+ subscriptions for half off right now, bringing the cost down to $49 for your first year. Thanks to a streaming benefit for subscribers, you can then sign up for Peacock at no extra cost. Walmart+ subscribers are able to choose between a Peacock Premium or a Paramount+ Essential subscription. Considering Peacock premium would run you $110 for the year on its own, signing up for Walmart+ while this discount is available gets you access to the streaming service for less than half the normal cost. Just about every major streaming service has raised its prices in the last year, including HBO Max, Disney+, Netflix, Apple TV and YouTube TV, so saving some money on one of them just might be worth the effort. Cord cutting is not nearly as affordable as it used to be, so finding a deal like this is pretty helpful. Walmart+ itself offers myriad additional benefits like early access to Black Friday deals, free shipping on orders over $35, discounts on gas, free online veterinary care and more. Earlier this year, Walmart+ subscribers got first dibs on the Nintendo Switch 2 at the retailer. You can also use that free shipping to take advantage of Walmart's drone delivery program in a handful of select cities.This article originally appeared on Engadget at https://www.engadget.com/deals/walmart-subscriptions-are-only-49-for-black-friday-and-it-includes-access-to-peacock-192739301.html?src=rss",
          "feed_position": 22
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/how-deductive-ai-saved-doordash-1-000-engineering-hours-by-automating",
          "published_at": "Wed, 12 Nov 2025 14:00:00 GMT",
          "title": "How Deductive AI saved DoorDash 1,000 engineering hours by automating software debugging",
          "standfirst": "As software systems grow more complex and AI tools generate code faster than ever, a fundamental problem is getting worse: Engineers are drowning in debugging work, spending up to half their time hunting down the causes of software failures instead of building new products. The challenge has become so acute that it&#x27;s creating a new category of tooling — AI agents that can diagnose production failures in minutes instead of hours.Deductive AI, a startup emerging from stealth mode Wednesday, believes it has found a solution by applying reinforcement learning — the same technology that powers game-playing AI systems — to the messy, high-stakes world of production software incidents. The company announced it has raised $7.5 million in seed funding led by CRV, with participation from Databricks Ventures, Thomvest Ventures, and PrimeSet, to commercialize what it calls \"AI SRE agents\" that can diagnose and help fix software failures at machine speed.The pitch resonates with a growing frustration inside engineering organizations: Modern observability tools can show that something broke, but they rarely explain why. When a production system fails at 3 a.m., engineers still face hours of manual detective work, cross-referencing logs, metrics, deployment histories, and code changes across dozens of interconnected services to identify the root cause.\"The complexities and inter-dependencies of modern infrastructure means that investigating the root cause of an outage or incident can feel like searching for a needle in a haystack, except the haystack is the size of a football field, it&#x27;s made of a million other needles, it&#x27;s constantly reshuffling itself, and is on fire — and every second you don&#x27;t find it equals lost revenue,\" said Sameer Agarwal, Deductive&#x27;s co-founder and chief technology officer, in an exclusive interview with VentureBeat.Deductive&#x27;s system builds what the company calls a \"knowledge graph\" that maps relationships across codebases, telemetry data, engineering discussions, and internal documentation. When an incident occurs, multiple AI agents work together to form hypotheses, test them against live system evidence, and converge on a root cause — mimicking the investigative workflow of experienced site reliability engineers, but completing the process in minutes rather than hours.The technology has already shown measurable impact at some of the world&#x27;s most demanding production environments. DoorDash&#x27;s advertising platform, which runs real-time auctions that must complete in under 100 milliseconds, has integrated Deductive into its incident response workflow. The company has set an ambitious 2026 goal of resolving production incidents within 10 minutes.\"Our Ads Platform operates at a pace where manual, slow-moving investigations are no longer viable. Every minute of downtime directly affects company revenue,\" said Shahrooz Ansari, Senior Director of Engineering at DoorDash, in an interview with VentureBeat. \"Deductive has become a critical extension of our team, rapidly synthesizing signals across dozens of services and surfacing the insights that matter—within minutes.\"DoorDash estimates that Deductive has root-caused approximately 100 production incidents over the past few months, translating to more than 1,000 hours of annual engineering productivity and a revenue impact \"in millions of dollars,\" according to Ansari. At location intelligence company Foursquare, Deductive reduced the time to diagnose Apache Spark job failures by 90% —t urning a process that previously took hours or days into one that completes in under 10 minutes — while generating over $275,000 in annual savings.Why AI-generated code is creating a debugging crisisThe timing of Deductive&#x27;s launch reflects a brewing tension in software development: AI coding assistants are enabling engineers to generate code faster than ever, but the resulting software is often harder to understand and maintain.\"Vibe coding,\" a term popularized by AI researcher Andrej Karpathy, refers to using natural-language prompts to generate code through AI assistants. While these tools accelerate development, they can introduce what Agarwal describes as \"redundancies, breaks in architectural boundaries, assumptions, or ignored design patterns\" that accumulate over time.\"Most AI-generated code still introduces redundancies, breaks architectural boundaries, makes assumptions, or ignores established design patterns,\" Agarwal told Venturebeat. \"In many ways, we now need AI to help clean up the mess that AI itself is creating.\"The claim that engineers spend roughly half their time on debugging isn&#x27;t hyperbole. The Association for Computing Machinery reports that developers spend 35% to 50% of their time validating and debugging software. More recently, Harness&#x27;s State of Software Delivery 2025 report found that 67% of developers are spending more time debugging AI-generated code.\"We&#x27;ve seen world-class engineers spending half of their time debugging instead of building,\" said Rakesh Kothari, Deductive&#x27;s co-founder and CEO. \"And as vibe coding generates new code at a rate we&#x27;ve never seen, this problem is only going to get worse.\"How Deductive&#x27;s AI agents actually investigate production failuresDeductive&#x27;s technical approach differs substantially from the AI features being added to existing observability platforms like Datadog or New Relic. Most of those systems use large language models to summarize data or identify correlations, but they lack what Agarwal calls \"code-aware reasoning\"—the ability to understand not just that something broke, but why the code behaves the way it does.\"Most enterprises use multiple observability tools across different teams and services, so no vendor has a single holistic view of how their systems behave, fail, and recover—nor are they able to pair that with an understanding of the code that defines system behavior,\" Agarwal explained. \"These are key ingredients to resolving software incidents and it is exactly the gap Deductive fills.\"The system connects to existing infrastructure using read-only API access to observability platforms, code repositories, incident management tools, and chat systems. It then continuously builds and updates its knowledge graph, mapping dependencies between services and tracking deployment histories.When an alert fires, Deductive launches what the company describes as a multi-agent investigation. Different agents specialize in different aspects of the problem: one might analyze recent code changes, another examines trace data, while a third correlates the timing of the incident with recent deployments. The agents share findings and iteratively refine their hypotheses.The critical difference from rule-based automation is Deductive&#x27;s use of reinforcement learning. The system learns from every incident which investigative steps led to correct diagnoses and which were dead ends. When engineers provide feedback, the system incorporates that signal into its learning model.\"Each time it observes an investigation, it learns which steps, data sources, and decisions led to the right outcome,\" Agarwal said. \"It learns how to think through problems, not just point them out.\"At DoorDash, a recent latency spike in an API initially appeared to be an isolated service issue. Deductive&#x27;s investigation revealed that the root cause was actually timeout errors from a downstream machine learning platform undergoing a deployment. The system connected these dots by analyzing log volumes, traces, and deployment metadata across multiple services.\"Without Deductive, our team would have had to manually correlate the latency spike across all logs, traces, and deployment histories,\" Ansari said. \"Deductive was able to explain not just what changed, but how and why it impacted production behavior.\"The company keeps humans in the loop—for nowWhile Deductive&#x27;s technology could theoretically push fixes directly to production systems, the company has deliberately chosen to keep humans in the loop—at least for now.\"While our system is capable of deeper automation and could push fixes to production, currently, we recommend precise fixes and mitigations that engineers can review, validate, and apply,\" Agarwal said. \"We believe maintaining a human in the loop is essential for trust, transparency and operational safety.\"However, he acknowledged that \"over time, we do think that deeper automation will come and how humans operate in the loop will evolve.\"Databricks and ThoughtSpot veterans bet on reasoning over observabilityThe founding team brings deep expertise from building some of Silicon Valley&#x27;s most successful data infrastructure platforms. Agarwal earned his Ph.D. at UC Berkeley, where he created BlinkDB, an influential system for approximate query processing. He was among the first engineers at Databricks, where he helped build Apache Spark. Kothari was an early engineer at ThoughtSpot, where he led teams focused on distributed query processing and large-scale system optimization.The investor syndicate reflects both the technical credibility and market opportunity. Beyond CRV&#x27;s Max Gazor, the round included participation from Ion Stoica, founder of Databricks and Anyscale; Ajeet Singh, founder of Nutanix and ThoughtSpot; and Ben Sigelman, founder of Lightstep.Rather than competing with platforms like Datadog or PagerDuty, Deductive positions itself as a complementary layer that sits on top of existing tools. The pricing model reflects this: Instead of charging based on data volume, Deductive charges based on the number of incidents investigated, plus a base platform fee.The company offers both cloud-hosted and self-hosted deployment options and emphasizes that it doesn&#x27;t store customer data on its servers or use it to train models for other customers — a critical assurance given the proprietary nature of both code and production system behavior.With fresh capital and early customer traction at companies like DoorDash, Foursquare, and Kumo AI, Deductive plans to expand its team and deepen the system&#x27;s reasoning capabilities from reactive incident analysis to proactive prevention. The near-term vision: helping teams predict problems before they occur.DoorDash&#x27;s Ansari offers a pragmatic endorsement of where the technology stands today: \"Investigations that were previously manual and time-consuming are now automated, allowing engineers to shift their energy toward prevention, business impact, and innovation.\"In an industry where every second of downtime translates to lost revenue, that shift from firefighting to building increasingly looks less like a luxury and more like table stakes.",
          "content": "As software systems grow more complex and AI tools generate code faster than ever, a fundamental problem is getting worse: Engineers are drowning in debugging work, spending up to half their time hunting down the causes of software failures instead of building new products. The challenge has become so acute that it&#x27;s creating a new category of tooling — AI agents that can diagnose production failures in minutes instead of hours.Deductive AI, a startup emerging from stealth mode Wednesday, believes it has found a solution by applying reinforcement learning — the same technology that powers game-playing AI systems — to the messy, high-stakes world of production software incidents. The company announced it has raised $7.5 million in seed funding led by CRV, with participation from Databricks Ventures, Thomvest Ventures, and PrimeSet, to commercialize what it calls \"AI SRE agents\" that can diagnose and help fix software failures at machine speed.The pitch resonates with a growing frustration inside engineering organizations: Modern observability tools can show that something broke, but they rarely explain why. When a production system fails at 3 a.m., engineers still face hours of manual detective work, cross-referencing logs, metrics, deployment histories, and code changes across dozens of interconnected services to identify the root cause.\"The complexities and inter-dependencies of modern infrastructure means that investigating the root cause of an outage or incident can feel like searching for a needle in a haystack, except the haystack is the size of a football field, it&#x27;s made of a million other needles, it&#x27;s constantly reshuffling itself, and is on fire — and every second you don&#x27;t find it equals lost revenue,\" said Sameer Agarwal, Deductive&#x27;s co-founder and chief technology officer, in an exclusive interview with VentureBeat.Deductive&#x27;s system builds what the company calls a \"knowledge graph\" that maps relationships across codebases, telemetry data, engineering discussions, and internal documentation. When an incident occurs, multiple AI agents work together to form hypotheses, test them against live system evidence, and converge on a root cause — mimicking the investigative workflow of experienced site reliability engineers, but completing the process in minutes rather than hours.The technology has already shown measurable impact at some of the world&#x27;s most demanding production environments. DoorDash&#x27;s advertising platform, which runs real-time auctions that must complete in under 100 milliseconds, has integrated Deductive into its incident response workflow. The company has set an ambitious 2026 goal of resolving production incidents within 10 minutes.\"Our Ads Platform operates at a pace where manual, slow-moving investigations are no longer viable. Every minute of downtime directly affects company revenue,\" said Shahrooz Ansari, Senior Director of Engineering at DoorDash, in an interview with VentureBeat. \"Deductive has become a critical extension of our team, rapidly synthesizing signals across dozens of services and surfacing the insights that matter—within minutes.\"DoorDash estimates that Deductive has root-caused approximately 100 production incidents over the past few months, translating to more than 1,000 hours of annual engineering productivity and a revenue impact \"in millions of dollars,\" according to Ansari. At location intelligence company Foursquare, Deductive reduced the time to diagnose Apache Spark job failures by 90% —t urning a process that previously took hours or days into one that completes in under 10 minutes — while generating over $275,000 in annual savings.Why AI-generated code is creating a debugging crisisThe timing of Deductive&#x27;s launch reflects a brewing tension in software development: AI coding assistants are enabling engineers to generate code faster than ever, but the resulting software is often harder to understand and maintain.\"Vibe coding,\" a term popularized by AI researcher Andrej Karpathy, refers to using natural-language prompts to generate code through AI assistants. While these tools accelerate development, they can introduce what Agarwal describes as \"redundancies, breaks in architectural boundaries, assumptions, or ignored design patterns\" that accumulate over time.\"Most AI-generated code still introduces redundancies, breaks architectural boundaries, makes assumptions, or ignores established design patterns,\" Agarwal told Venturebeat. \"In many ways, we now need AI to help clean up the mess that AI itself is creating.\"The claim that engineers spend roughly half their time on debugging isn&#x27;t hyperbole. The Association for Computing Machinery reports that developers spend 35% to 50% of their time validating and debugging software. More recently, Harness&#x27;s State of Software Delivery 2025 report found that 67% of developers are spending more time debugging AI-generated code.\"We&#x27;ve seen world-class engineers spending half of their time debugging instead of building,\" said Rakesh Kothari, Deductive&#x27;s co-founder and CEO. \"And as vibe coding generates new code at a rate we&#x27;ve never seen, this problem is only going to get worse.\"How Deductive&#x27;s AI agents actually investigate production failuresDeductive&#x27;s technical approach differs substantially from the AI features being added to existing observability platforms like Datadog or New Relic. Most of those systems use large language models to summarize data or identify correlations, but they lack what Agarwal calls \"code-aware reasoning\"—the ability to understand not just that something broke, but why the code behaves the way it does.\"Most enterprises use multiple observability tools across different teams and services, so no vendor has a single holistic view of how their systems behave, fail, and recover—nor are they able to pair that with an understanding of the code that defines system behavior,\" Agarwal explained. \"These are key ingredients to resolving software incidents and it is exactly the gap Deductive fills.\"The system connects to existing infrastructure using read-only API access to observability platforms, code repositories, incident management tools, and chat systems. It then continuously builds and updates its knowledge graph, mapping dependencies between services and tracking deployment histories.When an alert fires, Deductive launches what the company describes as a multi-agent investigation. Different agents specialize in different aspects of the problem: one might analyze recent code changes, another examines trace data, while a third correlates the timing of the incident with recent deployments. The agents share findings and iteratively refine their hypotheses.The critical difference from rule-based automation is Deductive&#x27;s use of reinforcement learning. The system learns from every incident which investigative steps led to correct diagnoses and which were dead ends. When engineers provide feedback, the system incorporates that signal into its learning model.\"Each time it observes an investigation, it learns which steps, data sources, and decisions led to the right outcome,\" Agarwal said. \"It learns how to think through problems, not just point them out.\"At DoorDash, a recent latency spike in an API initially appeared to be an isolated service issue. Deductive&#x27;s investigation revealed that the root cause was actually timeout errors from a downstream machine learning platform undergoing a deployment. The system connected these dots by analyzing log volumes, traces, and deployment metadata across multiple services.\"Without Deductive, our team would have had to manually correlate the latency spike across all logs, traces, and deployment histories,\" Ansari said. \"Deductive was able to explain not just what changed, but how and why it impacted production behavior.\"The company keeps humans in the loop—for nowWhile Deductive&#x27;s technology could theoretically push fixes directly to production systems, the company has deliberately chosen to keep humans in the loop—at least for now.\"While our system is capable of deeper automation and could push fixes to production, currently, we recommend precise fixes and mitigations that engineers can review, validate, and apply,\" Agarwal said. \"We believe maintaining a human in the loop is essential for trust, transparency and operational safety.\"However, he acknowledged that \"over time, we do think that deeper automation will come and how humans operate in the loop will evolve.\"Databricks and ThoughtSpot veterans bet on reasoning over observabilityThe founding team brings deep expertise from building some of Silicon Valley&#x27;s most successful data infrastructure platforms. Agarwal earned his Ph.D. at UC Berkeley, where he created BlinkDB, an influential system for approximate query processing. He was among the first engineers at Databricks, where he helped build Apache Spark. Kothari was an early engineer at ThoughtSpot, where he led teams focused on distributed query processing and large-scale system optimization.The investor syndicate reflects both the technical credibility and market opportunity. Beyond CRV&#x27;s Max Gazor, the round included participation from Ion Stoica, founder of Databricks and Anyscale; Ajeet Singh, founder of Nutanix and ThoughtSpot; and Ben Sigelman, founder of Lightstep.Rather than competing with platforms like Datadog or PagerDuty, Deductive positions itself as a complementary layer that sits on top of existing tools. The pricing model reflects this: Instead of charging based on data volume, Deductive charges based on the number of incidents investigated, plus a base platform fee.The company offers both cloud-hosted and self-hosted deployment options and emphasizes that it doesn&#x27;t store customer data on its servers or use it to train models for other customers — a critical assurance given the proprietary nature of both code and production system behavior.With fresh capital and early customer traction at companies like DoorDash, Foursquare, and Kumo AI, Deductive plans to expand its team and deepen the system&#x27;s reasoning capabilities from reactive incident analysis to proactive prevention. The near-term vision: helping teams predict problems before they occur.DoorDash&#x27;s Ansari offers a pragmatic endorsement of where the technology stands today: \"Investigations that were previously manual and time-consuming are now automated, allowing engineers to shift their energy toward prevention, business impact, and innovation.\"In an industry where every second of downtime translates to lost revenue, that shift from firefighting to building increasingly looks less like a luxury and more like table stakes.",
          "feed_position": 0,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/7mfhEiM01EDWrgDZYpDbte/23713914379b94e43303f9965ccc40ae/nuneybits_Vector_art_of_robot_holding_blueprint_193c9fc5-bbb5-46ea-9ff6-1a08bb03716e.webp?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/entertainment/music/new-streaming-app-coda-music-is-rolling-out-tools-for-labeling-and-blocking-ai-generated-tunes-140000530.html",
          "published_at": "Wed, 12 Nov 2025 14:00:00 +0000",
          "title": "New streaming app Coda Music is rolling out tools for labeling and blocking AI-generated tunes",
          "standfirst": "At this point, the streaming music landscape feels pretty well settled. Giants like Spotify, Amazon, Apple and YouTube duke it out at the top, while plenty of other players like Qobuz, Tidal, Deezer try their best to stand out from the pack. Somewhat surprisingly, though, a new player emerged in September. Coda Music used the recent backlash around Spotify co-founder Daniel Ek as a way to differentiate itself from the number one streamer, calling out Ek’s controversial funding of defense technology firm Helsing earlier in the year. (Spotify’s refusal to stop airing ICE recruitment ads certainly hasn’t helped the platform, either.)Today, the fledgling service is announcing a new feature that feels designed to answer another of the recent Spotify controversies: AI slop music flooding the platform. In response, Coda Music is launching AI identification tools with the purpose of finding and labeling songs that weren’t composed by actual humans.There are a few prongs to Coda’s approach. For starters, any artist added to Coda will be reviewed for AI origins, and their profile will be labeled “AI Artist” so that listeners know what they’re getting into. Coda is also letting users flag profiles of artists if they suspect the music is AI-generated; the company will then review them and label them if necessary. Finally, there’s a toggle in settings that just lets you turn off AI artists entirely. Obviously, how useful this setting is will depend on how good Coda gets at labeling AI-created music as such, but I can definitely see the appeal in just flipping that to “off” and avoiding as much slop as possible. Besides its stance on AI and the assurance that the company does not “invest in war,” there are a few other differentiators about Coda Music. The company says that it currently paying the “highest per-stream rate” in the industry — while at the same time, it acknowledges that no one is paying enough to artists. “The real problem isn’t how much is paid per stream, it’s that streaming alone doesn’t pay enough,” the company’s website says. “And minor improvements to a fundamentally flawed per stream model will not help.”To that end, the company also lets users pick an “independent or qualifying artist” who gets $1 of their monthly subscription fee. Sure, it’s only a dollar, but it’s the kind of thing that sweetens the pot at least a little bit for musicians. And Coda has good reason to want to make itself visible to users and artists alike. The last major differentiator for Coda is the company’s ambitions to turn its app into a social, music-sharing feed where you get recommendations from humans rather than algorithms. To that end, users can share anything from the app in their feed, and it also allows you to share external links and photos as well (go ahead and post your blurry images from that NIN concert!). The app’s home page prominently features fan-made playlists and recommended users to follow in addition to the usual suggestions based on what you’re listening to already. And there’s a social tab where you can see posts from people you follow; share songs, artists or albums; and see posts from artists you follow. That last part is key, as Coda wants artists interacting and sharing as well as just end users.It reminds me a little bit of the Fan Groups feature that Amazon Music just announced — and as with that feature, the problem facing Coda is getting people to start contributing to a new network rather than just posting things on whatever app they’re already using. Fortunately, music nerds love a community, so it’ll be interesting to see if this takes off at all. As for the new features for reporting and filtering out AI music, Coda says they’re available as of today in its iOS and Android apps. The company doesn’t have a web interface yet, but says it is coming soon. If ducking AI-generated tunes is something that catches your attention, Coda currently costs $11 a month, or $17 per month for a family plan with up to four listeners. This article originally appeared on Engadget at https://www.engadget.com/entertainment/music/new-streaming-app-coda-music-is-rolling-out-tools-for-labeling-and-blocking-ai-generated-tunes-140000530.html?src=rss",
          "content": "At this point, the streaming music landscape feels pretty well settled. Giants like Spotify, Amazon, Apple and YouTube duke it out at the top, while plenty of other players like Qobuz, Tidal, Deezer try their best to stand out from the pack. Somewhat surprisingly, though, a new player emerged in September. Coda Music used the recent backlash around Spotify co-founder Daniel Ek as a way to differentiate itself from the number one streamer, calling out Ek’s controversial funding of defense technology firm Helsing earlier in the year. (Spotify’s refusal to stop airing ICE recruitment ads certainly hasn’t helped the platform, either.)Today, the fledgling service is announcing a new feature that feels designed to answer another of the recent Spotify controversies: AI slop music flooding the platform. In response, Coda Music is launching AI identification tools with the purpose of finding and labeling songs that weren’t composed by actual humans.There are a few prongs to Coda’s approach. For starters, any artist added to Coda will be reviewed for AI origins, and their profile will be labeled “AI Artist” so that listeners know what they’re getting into. Coda is also letting users flag profiles of artists if they suspect the music is AI-generated; the company will then review them and label them if necessary. Finally, there’s a toggle in settings that just lets you turn off AI artists entirely. Obviously, how useful this setting is will depend on how good Coda gets at labeling AI-created music as such, but I can definitely see the appeal in just flipping that to “off” and avoiding as much slop as possible. Besides its stance on AI and the assurance that the company does not “invest in war,” there are a few other differentiators about Coda Music. The company says that it currently paying the “highest per-stream rate” in the industry — while at the same time, it acknowledges that no one is paying enough to artists. “The real problem isn’t how much is paid per stream, it’s that streaming alone doesn’t pay enough,” the company’s website says. “And minor improvements to a fundamentally flawed per stream model will not help.”To that end, the company also lets users pick an “independent or qualifying artist” who gets $1 of their monthly subscription fee. Sure, it’s only a dollar, but it’s the kind of thing that sweetens the pot at least a little bit for musicians. And Coda has good reason to want to make itself visible to users and artists alike. The last major differentiator for Coda is the company’s ambitions to turn its app into a social, music-sharing feed where you get recommendations from humans rather than algorithms. To that end, users can share anything from the app in their feed, and it also allows you to share external links and photos as well (go ahead and post your blurry images from that NIN concert!). The app’s home page prominently features fan-made playlists and recommended users to follow in addition to the usual suggestions based on what you’re listening to already. And there’s a social tab where you can see posts from people you follow; share songs, artists or albums; and see posts from artists you follow. That last part is key, as Coda wants artists interacting and sharing as well as just end users.It reminds me a little bit of the Fan Groups feature that Amazon Music just announced — and as with that feature, the problem facing Coda is getting people to start contributing to a new network rather than just posting things on whatever app they’re already using. Fortunately, music nerds love a community, so it’ll be interesting to see if this takes off at all. As for the new features for reporting and filtering out AI music, Coda says they’re available as of today in its iOS and Android apps. The company doesn’t have a web interface yet, but says it is coming soon. If ducking AI-generated tunes is something that catches your attention, Coda currently costs $11 a month, or $17 per month for a family plan with up to four listeners. This article originally appeared on Engadget at https://www.engadget.com/entertainment/music/new-streaming-app-coda-music-is-rolling-out-tools-for-labeling-and-blocking-ai-generated-tunes-140000530.html?src=rss",
          "feed_position": 23
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/grab-a-pair-of-airpods-4-earbuds-for-only-85-before-black-friday-162917254.html",
          "published_at": "Wed, 12 Nov 2025 13:45:37 +0000",
          "title": "Grab a pair of AirPods 4 earbuds for only $85 before Black Friday",
          "standfirst": "Black Friday Apple deals are already trickling in, and one of the best we're tracking is on the AirPods 4. They're down to $85 right now, which is a new record low. While these buds don't provide active noise cancellation, they're still a great option for those who want all of the conveniences of a pair of Apple earbuds in an affordable package. The Apple AirPods 4 are the best budget AirPods you can get in 2025, with Apple's H2 audio chip to support some of the more advanced audio features from more expensive models. They offer Voice Isolation, Personalized Spatial Audio with dynamic head tracking and more. If you get the model without active noise cancellation, you won't have features like Transparency Mode and Conversation Awareness, or Apple's hearing health tools. But, the entry-level model still offers great sound quality for the price. This model also features the redesigned shape, which makes for a more comfortable and secure fit so you don't have to worry about them falling out of your ears. A force sensor on the stem allows for basic touch controls, including play and pause, play next track, previous track and answer a call. You can also summon Siri by pressing and holding the stem. You can expect to get up to 5 hours of battery life on a charge with the non-ANC model, and up to 30 hours using the USB-C charging case. This article originally appeared on Engadget at https://www.engadget.com/deals/grab-a-pair-of-airpods-4-earbuds-for-only-85-before-black-friday-162917254.html?src=rss",
          "content": "Black Friday Apple deals are already trickling in, and one of the best we're tracking is on the AirPods 4. They're down to $85 right now, which is a new record low. While these buds don't provide active noise cancellation, they're still a great option for those who want all of the conveniences of a pair of Apple earbuds in an affordable package. The Apple AirPods 4 are the best budget AirPods you can get in 2025, with Apple's H2 audio chip to support some of the more advanced audio features from more expensive models. They offer Voice Isolation, Personalized Spatial Audio with dynamic head tracking and more. If you get the model without active noise cancellation, you won't have features like Transparency Mode and Conversation Awareness, or Apple's hearing health tools. But, the entry-level model still offers great sound quality for the price. This model also features the redesigned shape, which makes for a more comfortable and secure fit so you don't have to worry about them falling out of your ears. A force sensor on the stem allows for basic touch controls, including play and pause, play next track, previous track and answer a call. You can also summon Siri by pressing and holding the stem. You can expect to get up to 5 hours of battery life on a charge with the non-ANC model, and up to 30 hours using the USB-C charging case. This article originally appeared on Engadget at https://www.engadget.com/deals/grab-a-pair-of-airpods-4-earbuds-for-only-85-before-black-friday-162917254.html?src=rss",
          "feed_position": 24
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/pick-up-apples-mac-mini-m4-for-100-off-with-this-black-friday-deal-150749841.html",
          "published_at": "Wed, 12 Nov 2025 13:16:27 +0000",
          "title": "Pick up Apple's Mac mini M4 for $100 off with this Black Friday deal",
          "standfirst": "While there are lots of great Black Friday sales on cheaper devices, it's the big ticket items that really make a world of difference. Take Apple's 2024 Mac mini M4, which has dropped to $499 from $599 as part of early Black Friday deals. The 17 percent discount brings Apple's mini desktop computer with 16GB of RAM and 256GB of SSD to only $30 more than its all-time low. We gave the Mac mini M4 a 90 in our review, in part, because it packs an incredible amount of power into such a small design. It also has front facing USB-C and headphone ports, a first for the Mac mini lineup. Plus, it starts with 16GB of RAM, an upgrade from its predecessors. However, if you want more memory or storage, the other Mac Mini M4 models are also on sale. You can get 16GB of RAM and 512GB of SSD for $690, down from $799. Then there's the option for 24GB of RAM and 512GB of SSD at $890, down from $999. Plus, if you want to bundle in three years of AppleCare+, each model ends up being about $100 cheaper than normal. If you're looking to build a desktop setup from scratch, there's a small but notable discount on Apple's Magic Trackpad as well. It's down to $120, which is only seven percent off its usual price but it's the cheapest we've seen it. This article originally appeared on Engadget at https://www.engadget.com/deals/pick-up-apples-mac-mini-m4-for-100-off-with-this-black-friday-deal-150749841.html?src=rss",
          "content": "While there are lots of great Black Friday sales on cheaper devices, it's the big ticket items that really make a world of difference. Take Apple's 2024 Mac mini M4, which has dropped to $499 from $599 as part of early Black Friday deals. The 17 percent discount brings Apple's mini desktop computer with 16GB of RAM and 256GB of SSD to only $30 more than its all-time low. We gave the Mac mini M4 a 90 in our review, in part, because it packs an incredible amount of power into such a small design. It also has front facing USB-C and headphone ports, a first for the Mac mini lineup. Plus, it starts with 16GB of RAM, an upgrade from its predecessors. However, if you want more memory or storage, the other Mac Mini M4 models are also on sale. You can get 16GB of RAM and 512GB of SSD for $690, down from $799. Then there's the option for 24GB of RAM and 512GB of SSD at $890, down from $999. Plus, if you want to bundle in three years of AppleCare+, each model ends up being about $100 cheaper than normal. If you're looking to build a desktop setup from scratch, there's a small but notable discount on Apple's Magic Trackpad as well. It's down to $120, which is only seven percent off its usual price but it's the cheapest we've seen it. This article originally appeared on Engadget at https://www.engadget.com/deals/pick-up-apples-mac-mini-m4-for-100-off-with-this-black-friday-deal-150749841.html?src=rss",
          "feed_position": 26
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/our-favorite-streaming-device-drops-to-a-record-low-ahead-of-black-friday-173858910.html",
          "published_at": "Wed, 12 Nov 2025 12:59:04 +0000",
          "title": "Our favorite streaming device drops to a record low ahead of Black Friday",
          "standfirst": "If you'd rather not spend the money on a brand new TV this year, you can make an old set feel new again with a streaming device. Our favorite streaming device is on sale right now ahead of Black Friday: you can grab the Google TV Streamer 4K for only $75, which is the lowest price we've seen so far. The Google TV Streamer is our top pick for an all-in-one streaming device. It has a faster processor than Google's previous streaming devices (22 percent faster, according to the company), so you can switch between apps and different media without lagging. It also has more storage and memory, at 32GB and 4GB, respectively. Google TV streamer has an intuitive interface and keeps all of your favorite content from different streaming apps organized in one menu. It also seamlessly integrates Google Home, allowing you to control your smart home devices from a slideout panel on the TV. The 4K streamer comes in a set-top wedge design, rather than the dongle of Chromecasts past, but you'll have to pick up an HDMI cable separately if you don't already have one you can use. It comes with a small remote that you can ping by pressing a button on the streamer for when you inevitably misplace it. In her review of the device, Engadget's Amy Skorheim called the Google TV streamer \"a full-featured, competent device with an interface that’s better than most at pulling together all the disparate threads of a streaming experience.\" One of its only downsides is the relatively high cost at $100, so don't let this deal go to waste. In addition to the streaming device, Google has a bunch of other tech on sale for Black Friday. The entry-level Nest thermostat is on sale for $90 right now, and the Nest Wi-Fi Pro 6E router has dropped to $120 for a single-pack; that's 40 percent off. This article originally appeared on Engadget at https://www.engadget.com/deals/our-favorite-streaming-device-drops-to-a-record-low-ahead-of-black-friday-173858910.html?src=rss",
          "content": "If you'd rather not spend the money on a brand new TV this year, you can make an old set feel new again with a streaming device. Our favorite streaming device is on sale right now ahead of Black Friday: you can grab the Google TV Streamer 4K for only $75, which is the lowest price we've seen so far. The Google TV Streamer is our top pick for an all-in-one streaming device. It has a faster processor than Google's previous streaming devices (22 percent faster, according to the company), so you can switch between apps and different media without lagging. It also has more storage and memory, at 32GB and 4GB, respectively. Google TV streamer has an intuitive interface and keeps all of your favorite content from different streaming apps organized in one menu. It also seamlessly integrates Google Home, allowing you to control your smart home devices from a slideout panel on the TV. The 4K streamer comes in a set-top wedge design, rather than the dongle of Chromecasts past, but you'll have to pick up an HDMI cable separately if you don't already have one you can use. It comes with a small remote that you can ping by pressing a button on the streamer for when you inevitably misplace it. In her review of the device, Engadget's Amy Skorheim called the Google TV streamer \"a full-featured, competent device with an interface that’s better than most at pulling together all the disparate threads of a streaming experience.\" One of its only downsides is the relatively high cost at $100, so don't let this deal go to waste. In addition to the streaming device, Google has a bunch of other tech on sale for Black Friday. The entry-level Nest thermostat is on sale for $90 right now, and the Nest Wi-Fi Pro 6E router has dropped to $120 for a single-pack; that's 40 percent off. This article originally appeared on Engadget at https://www.engadget.com/deals/our-favorite-streaming-device-drops-to-a-record-low-ahead-of-black-friday-173858910.html?src=rss",
          "feed_position": 29
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/openai-reboots-chatgpt-experience-with-gpt-5-1-after-mixed-reviews-of-gpt-5",
          "published_at": "Wed, 12 Nov 2025 05:00:00 GMT",
          "title": "OpenAI reboots ChatGPT experience with GPT-5.1 after mixed reviews of GPT-5",
          "standfirst": "ChatGPT is about to become faster and more conversational as OpenAI upgrades its flagship model GPT-5 to GPT-5.1.OpenAI announced two updates to the GPT-5 series: GPT-5.1 Instant and GPT-5.1 Thinking. Both models are now accessible on ChatGPT. GPT-5.1 Instant, essentially the default and most-used model, is now “warmer, more intelligent, and better at following your instructions.” Meanwhile, GPT-5.1 Thinking is an advanced reasoning model that responds faster for simple tasks and yet more persistently on complex ones.“We heard clearly from users that great AI should not only be smart, but also enjoyable to talk to,” OpenAI said on the blog post. “GPT-5.1 improves meaningfully on both intelligence and communication style.” The company added that both models offer a way for users to “shape ChatGPT’s tone,” allowing people to control how the chat platform responds depending on the conversation they are having. Both models were rolled out to ChatGPT users, including Pro, Plus, Go, and Business users, and then to the free tier. Those on the Enterprise and Edu plans will get a seven-day early-access toggle for the models before GPT-5.1 becomes the default model. OpenAI said the models can also be accessible through the API, both with adapted reasoning. OpenAI has noted that it will soon update GPT-5 Pro to version 5.1. Instant and Thinking models The upgraded models, OpenAI said, the 5.1 tag reflects improvements to the base model and considers these as part of the GPT-5 family, trained on the same stack and data as its reasoning models. The biggest difference between 5.1 and 5 is its more natural and conversational tone, said OpenAI CEO for Applications Fidji Simo in a post. “Based on early testing, it often surprises people with its playfulness while remaining clear and useful,” OpenAI said in its blog post. Instant can use adaptive reasoning to help it decide when it needs to think about its answers first, especially for more complicated questions. OpenAI noted that it improved instruction following, so that while the model continues to respond quickly, it will actually be answering the user’s query. Recent model releases, such as Baidu’s ERNIE-4.5-VL-28B-A3B-Thinking, have been outperforming GPT-5 in benchmarks like instruction following. GPT-5.1 Thinking can figure out on its own how much reasoning power it should devote to a prompt. It adapts to the type and complexity of a query, so it will take longer to answer a fuller, complex question than a simple summary request. OpenAI said evaluations showed that GPT-5.1 Thinking spends less time and therefore uses fewer tokens on simple tasks, compared with GPT-5, outperforming the base model in terms of speed of response. One thing enterprises should note is that GPT-5.1 Thinking answers “with less jargon and fewer undefined terms.” OpenAI said removing jargony responses makes Thinking more approachable when it comes to explaining technical concepts.More personalizationAnother big update to ChatGPT is increased personalization. This allows users to toggle between a friendly and authoritative chat platform experience in their conversations. ChatGPT already allows users to choose preset options for the tone of the models, but the new update expands these options “to better reflect the most common ways people use ChatGPT.”These options include: Default, Friendly (formerly Listener), Efficient (previously Robot), Professional, Candid and Quirky. Two other personalities, Cynical and Nerdy, remain unchanged. “We think many people will find that GPT-5.1 does a better job of bringing IQ and EQ together, but one default clearly can’t meet everyone’s needs. That’s why we’re also making it easier to customize ChatGPT with a range of presets to choose from: professional, friendly, candid, quirky, efficient, cynical and nerdy. The model has the same capabilities whether you select the default or one of these options, but the style of its responses will differ — more formal or familiar, more playful or direct, more or less jargon or slang, and so on. Of course, eight personalities still don&#x27;t cover the full range of human diversity, but we know from our research that many people prefer simple, guided control over too many settings or open-ended options,” Simo said. People can also adjust how much ChatGPT uses emojis. It also offers granular controls for responses, with OpenAI experimenting with the ability to make the models more concise, warm or scannable.Saving a rolloutOpenAI’s GPT-5 rollout was…less than perfect. While company executives, including CEO Sam Altman, touted the new model’s capabilities, a decision to initially sunset older and beloved models on ChatGPT caused dissatisfaction among customers. Worse yet, many early adopters found that GPT-5 didn’t perform better than those older options in domains such as math, science, and writing. This led Altman to walk back some of his statements around model removal, and blamed performance issues on GPT-5’s router. The router, which automatically directs queries to the models best suited to it, is not going away as GPT-5.1 Auto will route prompts to the model type that can answer it best. OpenAI is careful to note that GPT-5 models Instant, Thinking, and Pro are still available in ChatGPT’s model dropdown, although paid subscribers only have three months to compare these older versions with the 5.1 update. The sunset period for GPT-5, however, will not impact models like GPT-4o.“Going forward, when we introduce new ChatGPT models, our approach is to give people ample space to evaluate what’s changed and share feedback, allowing us to continue innovating our frontier models while transitioning smoothly,” the company said. “Sunset periods will be communicated clearly and with plenty of advance notice.”",
          "content": "ChatGPT is about to become faster and more conversational as OpenAI upgrades its flagship model GPT-5 to GPT-5.1.OpenAI announced two updates to the GPT-5 series: GPT-5.1 Instant and GPT-5.1 Thinking. Both models are now accessible on ChatGPT. GPT-5.1 Instant, essentially the default and most-used model, is now “warmer, more intelligent, and better at following your instructions.” Meanwhile, GPT-5.1 Thinking is an advanced reasoning model that responds faster for simple tasks and yet more persistently on complex ones.“We heard clearly from users that great AI should not only be smart, but also enjoyable to talk to,” OpenAI said on the blog post. “GPT-5.1 improves meaningfully on both intelligence and communication style.” The company added that both models offer a way for users to “shape ChatGPT’s tone,” allowing people to control how the chat platform responds depending on the conversation they are having. Both models were rolled out to ChatGPT users, including Pro, Plus, Go, and Business users, and then to the free tier. Those on the Enterprise and Edu plans will get a seven-day early-access toggle for the models before GPT-5.1 becomes the default model. OpenAI said the models can also be accessible through the API, both with adapted reasoning. OpenAI has noted that it will soon update GPT-5 Pro to version 5.1. Instant and Thinking models The upgraded models, OpenAI said, the 5.1 tag reflects improvements to the base model and considers these as part of the GPT-5 family, trained on the same stack and data as its reasoning models. The biggest difference between 5.1 and 5 is its more natural and conversational tone, said OpenAI CEO for Applications Fidji Simo in a post. “Based on early testing, it often surprises people with its playfulness while remaining clear and useful,” OpenAI said in its blog post. Instant can use adaptive reasoning to help it decide when it needs to think about its answers first, especially for more complicated questions. OpenAI noted that it improved instruction following, so that while the model continues to respond quickly, it will actually be answering the user’s query. Recent model releases, such as Baidu’s ERNIE-4.5-VL-28B-A3B-Thinking, have been outperforming GPT-5 in benchmarks like instruction following. GPT-5.1 Thinking can figure out on its own how much reasoning power it should devote to a prompt. It adapts to the type and complexity of a query, so it will take longer to answer a fuller, complex question than a simple summary request. OpenAI said evaluations showed that GPT-5.1 Thinking spends less time and therefore uses fewer tokens on simple tasks, compared with GPT-5, outperforming the base model in terms of speed of response. One thing enterprises should note is that GPT-5.1 Thinking answers “with less jargon and fewer undefined terms.” OpenAI said removing jargony responses makes Thinking more approachable when it comes to explaining technical concepts.More personalizationAnother big update to ChatGPT is increased personalization. This allows users to toggle between a friendly and authoritative chat platform experience in their conversations. ChatGPT already allows users to choose preset options for the tone of the models, but the new update expands these options “to better reflect the most common ways people use ChatGPT.”These options include: Default, Friendly (formerly Listener), Efficient (previously Robot), Professional, Candid and Quirky. Two other personalities, Cynical and Nerdy, remain unchanged. “We think many people will find that GPT-5.1 does a better job of bringing IQ and EQ together, but one default clearly can’t meet everyone’s needs. That’s why we’re also making it easier to customize ChatGPT with a range of presets to choose from: professional, friendly, candid, quirky, efficient, cynical and nerdy. The model has the same capabilities whether you select the default or one of these options, but the style of its responses will differ — more formal or familiar, more playful or direct, more or less jargon or slang, and so on. Of course, eight personalities still don&#x27;t cover the full range of human diversity, but we know from our research that many people prefer simple, guided control over too many settings or open-ended options,” Simo said. People can also adjust how much ChatGPT uses emojis. It also offers granular controls for responses, with OpenAI experimenting with the ability to make the models more concise, warm or scannable.Saving a rolloutOpenAI’s GPT-5 rollout was…less than perfect. While company executives, including CEO Sam Altman, touted the new model’s capabilities, a decision to initially sunset older and beloved models on ChatGPT caused dissatisfaction among customers. Worse yet, many early adopters found that GPT-5 didn’t perform better than those older options in domains such as math, science, and writing. This led Altman to walk back some of his statements around model removal, and blamed performance issues on GPT-5’s router. The router, which automatically directs queries to the models best suited to it, is not going away as GPT-5.1 Auto will route prompts to the model type that can answer it best. OpenAI is careful to note that GPT-5 models Instant, Thinking, and Pro are still available in ChatGPT’s model dropdown, although paid subscribers only have three months to compare these older versions with the 5.1 update. The sunset period for GPT-5, however, will not impact models like GPT-4o.“Going forward, when we introduce new ChatGPT models, our approach is to give people ample space to evaluate what’s changed and share feedback, allowing us to continue innovating our frontier models while transitioning smoothly,” the company said. “Sunset periods will be communicated clearly and with plenty of advance notice.”",
          "feed_position": 1,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/3CpLfvpU0TfycKYEA3DBLw/f7dc8b8d4db1e3eef3d0b619d662879e/crimedy7_illustration_of_a_conversation_abstract_--ar_169_--v_5a880096-9873-4985-85ae-e8c247d831fc_0.png?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/baidu-just-dropped-an-open-source-multimodal-ai-that-it-claims-beats-gpt-5",
          "published_at": "Wed, 12 Nov 2025 00:00:00 GMT",
          "title": "Baidu just dropped an open-source multimodal AI that it claims beats GPT-5 and Gemini",
          "standfirst": "Baidu Inc., China&#x27;s largest search engine company, released a new artificial intelligence model on Monday that its developers claim outperforms competitors from Google and OpenAI on several vision-related benchmarks despite using a fraction of the computing resources typically required for such systems.The model, dubbed ERNIE-4.5-VL-28B-A3B-Thinking, is the latest salvo in an escalating competition among technology companies to build AI systems that can understand and reason about images, videos, and documents alongside traditional text — capabilities increasingly critical for enterprise applications ranging from automated document processing to industrial quality control.What sets Baidu&#x27;s release apart is its efficiency: the model activates just 3 billion parameters during operation while maintaining 28 billion total parameters through a sophisticated routing architecture. According to documentation released with the model, this design allows it to match or exceed the performance of much larger competing systems on tasks involving document understanding, chart analysis, and visual reasoning while consuming significantly less computational power and memory.\"Built upon the powerful ERNIE-4.5-VL-28B-A3B architecture, the newly upgraded ERNIE-4.5-VL-28B-A3B-Thinking achieves a remarkable leap forward in multimodal reasoning capabilities,\" Baidu wrote in the model&#x27;s technical documentation on Hugging Face, the AI model repository where the system was released.The company said the model underwent \"an extensive mid-training phase\" that incorporated \"a vast and highly diverse corpus of premium visual-language reasoning data,\" dramatically boosting its ability to align visual and textual information semantically.How the model mimics human visual problem-solving through dynamic image analysisPerhaps the model&#x27;s most distinctive feature is what Baidu calls \"Thinking with Images\" — a capability that allows the AI to dynamically zoom in and out of images to examine fine-grained details, mimicking how humans approach visual problem-solving tasks.\"The model thinks like a human, capable of freely zooming in and out of images to grasp every detail and uncover all information,\" according to the model card. When paired with tools like image search, Baidu claims this feature \"dramatically elevates the model&#x27;s ability to process fine-grained details and handle long-tail visual knowledge.\"This approach marks a departure from traditional vision-language models, which typically process images at a fixed resolution. By allowing dynamic image examination, the system can theoretically handle scenarios requiring both broad context and granular detail—such as analyzing complex technical diagrams or detecting subtle defects in manufacturing quality control.The model also supports what Baidu describes as enhanced \"visual grounding\" capabilities with \"more precise grounding and flexible instruction execution, easily triggering grounding functions in complex industrial scenarios,\" suggesting potential applications in robotics, warehouse automation, and other settings where AI systems must identify and locate specific objects in visual scenes.Baidu&#x27;s performance claims draw scrutiny as independent testing remains pendingBaidu&#x27;s assertion that the model outperforms Google&#x27;s Gemini 2.5 Pro and OpenAI&#x27;s GPT-5-High on various document and chart understanding benchmarks has drawn attention across social media, though independent verification of these claims remains pending.The company released the model under the permissive Apache 2.0 license, allowing unrestricted commercial use—a strategic decision that contrasts with the more restrictive licensing approaches of some competitors and could accelerate enterprise adoption.\"Apache 2.0 is smart,\" wrote one X user responding to Baidu&#x27;s announcement, highlighting the competitive advantage of open licensing in the enterprise market.According to Baidu&#x27;s documentation, the model demonstrates six core capabilities beyond traditional text processing. In visual reasoning, the system can perform what Baidu describes as \"multi-step reasoning, chart analysis, and causal reasoning capabilities in complex visual tasks,\" aided by what the company characterizes as \"large-scale reinforcement learning.\" For STEM problem solving, Baidu claims that \"leveraging its powerful visual abilities, the model achieves a leap in performance on STEM tasks like solving problems from photos.\" The visual grounding capability allows the model to identify and locate objects within images with what Baidu characterizes as industrial-grade precision. Through tool integration, the system can invoke external functions including image search capabilities to access information beyond its training data.For video understanding, Baidu claims the model possesses \"outstanding temporal awareness and event localization abilities, accurately identifying content changes across different time segments in a video.\" Finally, the thinking with images feature enables the dynamic zoom functionality that distinguishes this model from competitors.Inside the mixture-of-experts architecture that powers efficient multimodal processingUnder the hood, ERNIE-4.5-VL-28B-A3B-Thinking employs a Mixture-of-Experts (MoE) architecture — a design pattern that has become increasingly popular for building efficient large-scale AI systems. Rather than activating all 28 billion parameters for every task, the model uses a routing mechanism to selectively activate only the 3 billion parameters most relevant to each specific input.This approach offers substantial practical advantages for enterprise deployments. According to Baidu&#x27;s documentation, the model can run on a single 80GB GPU — hardware readily available in many corporate data centers — making it significantly more accessible than competing systems that may require multiple high-end accelerators.The technical documentation reveals that Baidu employed several advanced training techniques to achieve the model&#x27;s capabilities. The company used \"cutting-edge multimodal reinforcement learning techniques on verifiable tasks, integrating GSPO and IcePop strategies to stabilize MoE training combined with dynamic difficulty sampling for exceptional learning efficiency.\"Baidu also notes that in response to \"strong community demand,\" the company \"significantly strengthened the model&#x27;s grounding performance with improved instruction-following capabilities.\"The new model fits into Baidu&#x27;s ambitious multimodal AI ecosystemThe new release is one component of Baidu&#x27;s broader ERNIE 4.5 model family, which the company unveiled in June 2025. That family comprises 10 distinct variants, including Mixture-of-Experts models ranging from the flagship ERNIE-4.5-VL-424B-A47B with 424 billion total parameters down to a compact 0.3 billion parameter dense model.According to Baidu&#x27;s technical report on the ERNIE 4.5 family, the models incorporate \"a novel heterogeneous modality structure, which supports parameter sharing across modalities while also allowing dedicated parameters for each individual modality.\"This architectural choice addresses a longstanding challenge in multimodal AI development: training systems on both visual and textual data without one modality degrading the performance of the other. Baidu claims this design \"has the advantage to enhance multimodal understanding without compromising, and even improving, performance on text-related tasks.\"The company reported achieving 47% Model FLOPs Utilization (MFU) — a measure of training efficiency — during pre-training of its largest ERNIE 4.5 language model, using the PaddlePaddle deep learning framework developed in-house.Comprehensive developer tools aim to simplify enterprise deployment and integrationFor organizations looking to deploy the model, Baidu has released a comprehensive suite of development tools through ERNIEKit, what the company describes as an \"industrial-grade training and compression development toolkit.\"The model offers full compatibility with popular open-source frameworks including Hugging Face Transformers, vLLM (a high-performance inference engine), and Baidu&#x27;s own FastDeploy toolkit. This multi-platform support could prove critical for enterprise adoption, allowing organizations to integrate the model into existing AI infrastructure without wholesale platform changes.Sample code released by Baidu shows a relatively straightforward implementation path. Using the Transformers library, developers can load and run the model with approximately 30 lines of Python code, according to the documentation on Hugging Face.For production deployments requiring higher throughput, Baidu provides vLLM integration with specialized support for the model&#x27;s \"reasoning-parser\" and \"tool-call-parser\" capabilities — features that enable the dynamic image examination and external tool integration that distinguish this model from earlier systems.The company also offers FastDeploy, a proprietary inference toolkit that Baidu claims delivers \"production-ready, easy-to-use multi-hardware deployment solutions\" with support for various quantization schemes that can reduce memory requirements and increase inference speed.Why this release matters for the enterprise AI market at a critical inflection pointThe release comes at a pivotal moment in the enterprise AI market. As organizations move beyond experimental chatbot deployments toward production systems that process documents, analyze visual data, and automate complex workflows, demand for capable and cost-effective vision-language models has intensified.Several enterprise use cases appear particularly well-suited to the model&#x27;s capabilities. Document processing — extracting information from invoices, contracts, and forms — represents a massive market where accurate chart and table understanding directly translates to cost savings through automation. Manufacturing quality control, where AI systems must detect visual defects, could benefit from the model&#x27;s grounding capabilities. Customer service applications that handle images from users could leverage the multi-step visual reasoning.The model&#x27;s efficiency profile may prove especially attractive to mid-market organizations and startups that lack the computing budgets of large technology companies. By fitting on a single 80GB GPU — hardware costing roughly $10,000 to $30,000 depending on the specific model — the system becomes economically viable for a much broader range of organizations than models requiring multi-GPU setups costing hundreds of thousands of dollars.\"With all these new models, where&#x27;s the best place to actually build and scale? Access to compute is everything,\" wrote one X user in response to Baidu&#x27;s announcement, highlighting the persistent infrastructure challenges facing organizations attempting to deploy advanced AI systems.The Apache 2.0 licensing further lowers barriers to adoption. Unlike models released under more restrictive licenses that may limit commercial use or require revenue sharing, organizations can deploy ERNIE-4.5-VL-28B-A3B-Thinking in production applications without ongoing licensing fees or usage restrictions.Competition intensifies as Chinese tech giant takes aim at Google and OpenAIBaidu&#x27;s release intensifies competition in the vision-language model space, where Google, OpenAI, Anthropic, and Chinese companies including Alibaba and ByteDance have all released capable systems in recent months.The company&#x27;s performance claims — if validated by independent testing — would represent a significant achievement. Google&#x27;s Gemini 2.5 Pro and OpenAI&#x27;s GPT-5-High are substantially larger models backed by the deep resources of two of the world&#x27;s most valuable technology companies. That a more compact, openly available model could match or exceed their performance on specific tasks would suggest the field is advancing more rapidly than some analysts anticipated.\"Impressive that ERNIE is outperforming Gemini 2.5 Pro,\" wrote one social media commenter, expressing surprise at the claimed results.However, some observers counseled caution about benchmark comparisons. \"It&#x27;s fascinating to see how multimodal models are evolving, especially with features like &#x27;Thinking with Images,&#x27;\" wrote one X user. \"That said, I&#x27;m curious if ERNIE-4.5&#x27;s edge over competitors like Gemini-2.5-Pro and GPT-5-High primarily lies in specific use cases like document and chart\" understanding rather than general-purpose vision tasks.Industry analysts note that benchmark performance often fails to capture real-world behavior across the diverse scenarios enterprises encounter. A model that excels at document understanding may struggle with creative visual tasks or real-time video analysis. Organizations evaluating these systems typically conduct extensive internal testing on representative workloads before committing to production deployments.Technical limitations and infrastructure requirements that enterprises must considerDespite its capabilities, the model faces several technical challenges common to large vision-language systems. The minimum requirement of 80GB of GPU memory, while more accessible than some competitors, still represents a significant infrastructure investment. Organizations without existing GPU infrastructure would need to procure specialized hardware or rely on cloud computing services, introducing ongoing operational costs.The model&#x27;s context window — the amount of text and visual information it can process simultaneously — is listed as 128K tokens in Baidu&#x27;s documentation. While substantial, this may prove limiting for some document processing scenarios involving very long technical manuals or extensive video content.Questions also remain about the model&#x27;s behavior on adversarial inputs, out-of-distribution data, and edge cases. Baidu&#x27;s documentation does not provide detailed information about safety testing, bias mitigation, or failure modes — considerations increasingly important for enterprise deployments where errors could have financial or safety implications.What technical decision-makers need to evaluate beyond the benchmark numbersFor technical decision-makers evaluating the model, several implementation factors warrant consideration beyond raw performance metrics.The model&#x27;s MoE architecture, while efficient during inference, adds complexity to deployment and optimization. Organizations must ensure their infrastructure can properly route inputs to the appropriate expert subnetworks — a capability not universally supported across all deployment platforms.The \"Thinking with Images\" feature, while innovative, requires integration with image manipulation tools to achieve its full potential. Baidu&#x27;s documentation suggests this capability works best \"when paired with tools like image zooming and image search,\" implying that organizations may need to build additional infrastructure to fully leverage this functionality.The model&#x27;s video understanding capabilities, while highlighted in marketing materials, come with practical constraints. Processing video requires substantially more computational resources than static images, and the documentation does not specify maximum video length or optimal frame rates.Organizations considering deployment should also evaluate Baidu&#x27;s ongoing commitment to the model. Open-source AI models require continuing maintenance, security updates, and potential retraining as data distributions shift over time. While the Apache 2.0 license ensures the model remains available, future improvements and support depend on Baidu&#x27;s strategic priorities.Developer community responds with enthusiasm tempered by practical requestsEarly response from the AI research and development community has been cautiously optimistic. Developers have requested versions of the model in additional formats including GGUF (a quantization format popular for local deployment) and MNN (a mobile neural network framework), suggesting interest in running the system on resource-constrained devices.\"Release MNN and GGUF so I can run it on my phone,\" wrote one developer, highlighting demand for mobile deployment options.Other developers praised Baidu&#x27;s technical choices while requesting additional resources. \"Fantastic model! Did you use discoveries from PaddleOCR?\" asked one user, referencing Baidu&#x27;s open-source optical character recognition toolkit.The model&#x27;s lengthy name—ERNIE-4.5-VL-28B-A3B-Thinking—drew lighthearted commentary. \"ERNIE-4.5-VL-28B-A3B-Thinking might be the longest model name in history,\" joked one observer. \"But hey, if you&#x27;re outperforming Gemini-2.5-Pro with only 3B active params, you&#x27;ve earned the right to a dramatic name!\"Baidu plans to showcase the ERNIE lineup during its Baidu World 2025 conference on November 13, where the company is expected to provide additional details about the model&#x27;s development, performance validation, and future roadmap.The release marks a strategic move by Baidu to establish itself as a major player in the global AI infrastructure market. While Chinese AI companies have historically focused primarily on domestic markets, the open-source release under a permissive license signals ambitions to compete internationally with Western AI giants.For enterprises, the release adds another capable option to a rapidly expanding menu of AI models. Organizations no longer face a binary choice between building proprietary systems or licensing closed-source models from a handful of vendors. The proliferation of capable open-source alternatives like ERNIE-4.5-VL-28B-A3B-Thinking is reshaping the economics of AI deployment and accelerating adoption across industries.Whether the model delivers on its performance promises in real-world deployments remains to be seen. But for organizations seeking powerful, cost-effective tools for visual understanding and reasoning, one thing is certain. As one developer succinctly summarized: \"Open source plus commercial use equals chef&#x27;s kiss. Baidu not playing around.\"",
          "content": "Baidu Inc., China&#x27;s largest search engine company, released a new artificial intelligence model on Monday that its developers claim outperforms competitors from Google and OpenAI on several vision-related benchmarks despite using a fraction of the computing resources typically required for such systems.The model, dubbed ERNIE-4.5-VL-28B-A3B-Thinking, is the latest salvo in an escalating competition among technology companies to build AI systems that can understand and reason about images, videos, and documents alongside traditional text — capabilities increasingly critical for enterprise applications ranging from automated document processing to industrial quality control.What sets Baidu&#x27;s release apart is its efficiency: the model activates just 3 billion parameters during operation while maintaining 28 billion total parameters through a sophisticated routing architecture. According to documentation released with the model, this design allows it to match or exceed the performance of much larger competing systems on tasks involving document understanding, chart analysis, and visual reasoning while consuming significantly less computational power and memory.\"Built upon the powerful ERNIE-4.5-VL-28B-A3B architecture, the newly upgraded ERNIE-4.5-VL-28B-A3B-Thinking achieves a remarkable leap forward in multimodal reasoning capabilities,\" Baidu wrote in the model&#x27;s technical documentation on Hugging Face, the AI model repository where the system was released.The company said the model underwent \"an extensive mid-training phase\" that incorporated \"a vast and highly diverse corpus of premium visual-language reasoning data,\" dramatically boosting its ability to align visual and textual information semantically.How the model mimics human visual problem-solving through dynamic image analysisPerhaps the model&#x27;s most distinctive feature is what Baidu calls \"Thinking with Images\" — a capability that allows the AI to dynamically zoom in and out of images to examine fine-grained details, mimicking how humans approach visual problem-solving tasks.\"The model thinks like a human, capable of freely zooming in and out of images to grasp every detail and uncover all information,\" according to the model card. When paired with tools like image search, Baidu claims this feature \"dramatically elevates the model&#x27;s ability to process fine-grained details and handle long-tail visual knowledge.\"This approach marks a departure from traditional vision-language models, which typically process images at a fixed resolution. By allowing dynamic image examination, the system can theoretically handle scenarios requiring both broad context and granular detail—such as analyzing complex technical diagrams or detecting subtle defects in manufacturing quality control.The model also supports what Baidu describes as enhanced \"visual grounding\" capabilities with \"more precise grounding and flexible instruction execution, easily triggering grounding functions in complex industrial scenarios,\" suggesting potential applications in robotics, warehouse automation, and other settings where AI systems must identify and locate specific objects in visual scenes.Baidu&#x27;s performance claims draw scrutiny as independent testing remains pendingBaidu&#x27;s assertion that the model outperforms Google&#x27;s Gemini 2.5 Pro and OpenAI&#x27;s GPT-5-High on various document and chart understanding benchmarks has drawn attention across social media, though independent verification of these claims remains pending.The company released the model under the permissive Apache 2.0 license, allowing unrestricted commercial use—a strategic decision that contrasts with the more restrictive licensing approaches of some competitors and could accelerate enterprise adoption.\"Apache 2.0 is smart,\" wrote one X user responding to Baidu&#x27;s announcement, highlighting the competitive advantage of open licensing in the enterprise market.According to Baidu&#x27;s documentation, the model demonstrates six core capabilities beyond traditional text processing. In visual reasoning, the system can perform what Baidu describes as \"multi-step reasoning, chart analysis, and causal reasoning capabilities in complex visual tasks,\" aided by what the company characterizes as \"large-scale reinforcement learning.\" For STEM problem solving, Baidu claims that \"leveraging its powerful visual abilities, the model achieves a leap in performance on STEM tasks like solving problems from photos.\" The visual grounding capability allows the model to identify and locate objects within images with what Baidu characterizes as industrial-grade precision. Through tool integration, the system can invoke external functions including image search capabilities to access information beyond its training data.For video understanding, Baidu claims the model possesses \"outstanding temporal awareness and event localization abilities, accurately identifying content changes across different time segments in a video.\" Finally, the thinking with images feature enables the dynamic zoom functionality that distinguishes this model from competitors.Inside the mixture-of-experts architecture that powers efficient multimodal processingUnder the hood, ERNIE-4.5-VL-28B-A3B-Thinking employs a Mixture-of-Experts (MoE) architecture — a design pattern that has become increasingly popular for building efficient large-scale AI systems. Rather than activating all 28 billion parameters for every task, the model uses a routing mechanism to selectively activate only the 3 billion parameters most relevant to each specific input.This approach offers substantial practical advantages for enterprise deployments. According to Baidu&#x27;s documentation, the model can run on a single 80GB GPU — hardware readily available in many corporate data centers — making it significantly more accessible than competing systems that may require multiple high-end accelerators.The technical documentation reveals that Baidu employed several advanced training techniques to achieve the model&#x27;s capabilities. The company used \"cutting-edge multimodal reinforcement learning techniques on verifiable tasks, integrating GSPO and IcePop strategies to stabilize MoE training combined with dynamic difficulty sampling for exceptional learning efficiency.\"Baidu also notes that in response to \"strong community demand,\" the company \"significantly strengthened the model&#x27;s grounding performance with improved instruction-following capabilities.\"The new model fits into Baidu&#x27;s ambitious multimodal AI ecosystemThe new release is one component of Baidu&#x27;s broader ERNIE 4.5 model family, which the company unveiled in June 2025. That family comprises 10 distinct variants, including Mixture-of-Experts models ranging from the flagship ERNIE-4.5-VL-424B-A47B with 424 billion total parameters down to a compact 0.3 billion parameter dense model.According to Baidu&#x27;s technical report on the ERNIE 4.5 family, the models incorporate \"a novel heterogeneous modality structure, which supports parameter sharing across modalities while also allowing dedicated parameters for each individual modality.\"This architectural choice addresses a longstanding challenge in multimodal AI development: training systems on both visual and textual data without one modality degrading the performance of the other. Baidu claims this design \"has the advantage to enhance multimodal understanding without compromising, and even improving, performance on text-related tasks.\"The company reported achieving 47% Model FLOPs Utilization (MFU) — a measure of training efficiency — during pre-training of its largest ERNIE 4.5 language model, using the PaddlePaddle deep learning framework developed in-house.Comprehensive developer tools aim to simplify enterprise deployment and integrationFor organizations looking to deploy the model, Baidu has released a comprehensive suite of development tools through ERNIEKit, what the company describes as an \"industrial-grade training and compression development toolkit.\"The model offers full compatibility with popular open-source frameworks including Hugging Face Transformers, vLLM (a high-performance inference engine), and Baidu&#x27;s own FastDeploy toolkit. This multi-platform support could prove critical for enterprise adoption, allowing organizations to integrate the model into existing AI infrastructure without wholesale platform changes.Sample code released by Baidu shows a relatively straightforward implementation path. Using the Transformers library, developers can load and run the model with approximately 30 lines of Python code, according to the documentation on Hugging Face.For production deployments requiring higher throughput, Baidu provides vLLM integration with specialized support for the model&#x27;s \"reasoning-parser\" and \"tool-call-parser\" capabilities — features that enable the dynamic image examination and external tool integration that distinguish this model from earlier systems.The company also offers FastDeploy, a proprietary inference toolkit that Baidu claims delivers \"production-ready, easy-to-use multi-hardware deployment solutions\" with support for various quantization schemes that can reduce memory requirements and increase inference speed.Why this release matters for the enterprise AI market at a critical inflection pointThe release comes at a pivotal moment in the enterprise AI market. As organizations move beyond experimental chatbot deployments toward production systems that process documents, analyze visual data, and automate complex workflows, demand for capable and cost-effective vision-language models has intensified.Several enterprise use cases appear particularly well-suited to the model&#x27;s capabilities. Document processing — extracting information from invoices, contracts, and forms — represents a massive market where accurate chart and table understanding directly translates to cost savings through automation. Manufacturing quality control, where AI systems must detect visual defects, could benefit from the model&#x27;s grounding capabilities. Customer service applications that handle images from users could leverage the multi-step visual reasoning.The model&#x27;s efficiency profile may prove especially attractive to mid-market organizations and startups that lack the computing budgets of large technology companies. By fitting on a single 80GB GPU — hardware costing roughly $10,000 to $30,000 depending on the specific model — the system becomes economically viable for a much broader range of organizations than models requiring multi-GPU setups costing hundreds of thousands of dollars.\"With all these new models, where&#x27;s the best place to actually build and scale? Access to compute is everything,\" wrote one X user in response to Baidu&#x27;s announcement, highlighting the persistent infrastructure challenges facing organizations attempting to deploy advanced AI systems.The Apache 2.0 licensing further lowers barriers to adoption. Unlike models released under more restrictive licenses that may limit commercial use or require revenue sharing, organizations can deploy ERNIE-4.5-VL-28B-A3B-Thinking in production applications without ongoing licensing fees or usage restrictions.Competition intensifies as Chinese tech giant takes aim at Google and OpenAIBaidu&#x27;s release intensifies competition in the vision-language model space, where Google, OpenAI, Anthropic, and Chinese companies including Alibaba and ByteDance have all released capable systems in recent months.The company&#x27;s performance claims — if validated by independent testing — would represent a significant achievement. Google&#x27;s Gemini 2.5 Pro and OpenAI&#x27;s GPT-5-High are substantially larger models backed by the deep resources of two of the world&#x27;s most valuable technology companies. That a more compact, openly available model could match or exceed their performance on specific tasks would suggest the field is advancing more rapidly than some analysts anticipated.\"Impressive that ERNIE is outperforming Gemini 2.5 Pro,\" wrote one social media commenter, expressing surprise at the claimed results.However, some observers counseled caution about benchmark comparisons. \"It&#x27;s fascinating to see how multimodal models are evolving, especially with features like &#x27;Thinking with Images,&#x27;\" wrote one X user. \"That said, I&#x27;m curious if ERNIE-4.5&#x27;s edge over competitors like Gemini-2.5-Pro and GPT-5-High primarily lies in specific use cases like document and chart\" understanding rather than general-purpose vision tasks.Industry analysts note that benchmark performance often fails to capture real-world behavior across the diverse scenarios enterprises encounter. A model that excels at document understanding may struggle with creative visual tasks or real-time video analysis. Organizations evaluating these systems typically conduct extensive internal testing on representative workloads before committing to production deployments.Technical limitations and infrastructure requirements that enterprises must considerDespite its capabilities, the model faces several technical challenges common to large vision-language systems. The minimum requirement of 80GB of GPU memory, while more accessible than some competitors, still represents a significant infrastructure investment. Organizations without existing GPU infrastructure would need to procure specialized hardware or rely on cloud computing services, introducing ongoing operational costs.The model&#x27;s context window — the amount of text and visual information it can process simultaneously — is listed as 128K tokens in Baidu&#x27;s documentation. While substantial, this may prove limiting for some document processing scenarios involving very long technical manuals or extensive video content.Questions also remain about the model&#x27;s behavior on adversarial inputs, out-of-distribution data, and edge cases. Baidu&#x27;s documentation does not provide detailed information about safety testing, bias mitigation, or failure modes — considerations increasingly important for enterprise deployments where errors could have financial or safety implications.What technical decision-makers need to evaluate beyond the benchmark numbersFor technical decision-makers evaluating the model, several implementation factors warrant consideration beyond raw performance metrics.The model&#x27;s MoE architecture, while efficient during inference, adds complexity to deployment and optimization. Organizations must ensure their infrastructure can properly route inputs to the appropriate expert subnetworks — a capability not universally supported across all deployment platforms.The \"Thinking with Images\" feature, while innovative, requires integration with image manipulation tools to achieve its full potential. Baidu&#x27;s documentation suggests this capability works best \"when paired with tools like image zooming and image search,\" implying that organizations may need to build additional infrastructure to fully leverage this functionality.The model&#x27;s video understanding capabilities, while highlighted in marketing materials, come with practical constraints. Processing video requires substantially more computational resources than static images, and the documentation does not specify maximum video length or optimal frame rates.Organizations considering deployment should also evaluate Baidu&#x27;s ongoing commitment to the model. Open-source AI models require continuing maintenance, security updates, and potential retraining as data distributions shift over time. While the Apache 2.0 license ensures the model remains available, future improvements and support depend on Baidu&#x27;s strategic priorities.Developer community responds with enthusiasm tempered by practical requestsEarly response from the AI research and development community has been cautiously optimistic. Developers have requested versions of the model in additional formats including GGUF (a quantization format popular for local deployment) and MNN (a mobile neural network framework), suggesting interest in running the system on resource-constrained devices.\"Release MNN and GGUF so I can run it on my phone,\" wrote one developer, highlighting demand for mobile deployment options.Other developers praised Baidu&#x27;s technical choices while requesting additional resources. \"Fantastic model! Did you use discoveries from PaddleOCR?\" asked one user, referencing Baidu&#x27;s open-source optical character recognition toolkit.The model&#x27;s lengthy name—ERNIE-4.5-VL-28B-A3B-Thinking—drew lighthearted commentary. \"ERNIE-4.5-VL-28B-A3B-Thinking might be the longest model name in history,\" joked one observer. \"But hey, if you&#x27;re outperforming Gemini-2.5-Pro with only 3B active params, you&#x27;ve earned the right to a dramatic name!\"Baidu plans to showcase the ERNIE lineup during its Baidu World 2025 conference on November 13, where the company is expected to provide additional details about the model&#x27;s development, performance validation, and future roadmap.The release marks a strategic move by Baidu to establish itself as a major player in the global AI infrastructure market. While Chinese AI companies have historically focused primarily on domestic markets, the open-source release under a permissive license signals ambitions to compete internationally with Western AI giants.For enterprises, the release adds another capable option to a rapidly expanding menu of AI models. Organizations no longer face a binary choice between building proprietary systems or licensing closed-source models from a handful of vendors. The proliferation of capable open-source alternatives like ERNIE-4.5-VL-28B-A3B-Thinking is reshaping the economics of AI deployment and accelerating adoption across industries.Whether the model delivers on its performance promises in real-world deployments remains to be seen. But for organizations seeking powerful, cost-effective tools for visual understanding and reasoning, one thing is certain. As one developer succinctly summarized: \"Open source plus commercial use equals chef&#x27;s kiss. Baidu not playing around.\"",
          "feed_position": 2,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/6PAfmxLhH2Yv7BpN8vczIt/f33baa764e279c49d01c5a10da8eef61/nuneybits_Vector_art_of_a_GPU_made_out_of_computer_code_and_the_59f97a50-f492-452f-bd5d-1d6e6e904c4a.webp?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/metas-spice-framework-lets-ai-systems-teach-themselves-to-reason",
          "published_at": "Tue, 11 Nov 2025 22:21:00 GMT",
          "title": "Meta’s SPICE framework lets AI systems teach themselves to reason",
          "standfirst": "Researchers at Meta FAIR and the National University of Singapore have developed a new reinforcement learning framework for self-improving AI systems. Called Self-Play In Corpus Environments (SPICE), the framework pits two AI agents against each other, creating its own challenges and gradually improving without human supervision.While currently a proof-of-concept, this self-play mechanism could provide a basis for future AI systems that can dynamically adapt to their environments, making them more robust against the unpredictability of real-world applications.The challenge of self-improving AIThe goal of self-improving AI is to create systems that can enhance their capabilities by interacting with their environment. A common approach is reinforcement learning with verifiable rewards (RLVR), where models are rewarded for providing the correct answers to problems. This is often limited by its reliance on human-curated problem sets and domain-specific reward engineering, which makes it difficult to scale.Self-play, where a model improves by competing against itself, is another promising paradigm. But existing self-play methods for language models are often limited by two critical factors. Factual errors in generated questions and answers compound, leading to a feedback loop of hallucinations. When the problem generator and solver have information symmetry (i.e., share the same knowledge base) they fail to generate genuinely new challenges and fall into repetitive patterns. As the researchers note in their paper, “These systematic empirical failures indicate that self-improvement requires interaction with an external source providing diverse, verifiable feedback, rather than closed-loop pure introspection.”How SPICE worksSPICE is a self-play framework where a single model acts in two distinct roles. A \"Challenger\" constructs a curriculum of challenging problems from a large corpus of documents. A \"Reasoner\" then attempts to solve these problems without access to the source documents. This setup breaks the information symmetry that limits other self-play methods, as the Reasoner does not have access to the documents and knowledge that the Challenger uses to generate the problems.Grounding the tasks in a vast and diverse corpus of documents prevents hallucination by anchoring questions and answers in real-world content. This is important because for AI systems to reliably self-improve, they need external grounding sources. Therefore, LLM agents should learn from interactions with humans and the real world, not just their own outputs, to avoid compounding errors.The adversarial dynamic between the two roles creates an automatic curriculum. The Challenger is rewarded for generating problems that are both diverse and at the frontier of the Reasoner&#x27;s capability (not too easy and also not impossible). The Reasoner is rewarded for answering correctly. This symbiotic interaction pushes both agents to continuously discover and overcome new challenges. Because the system uses raw documents instead of pre-defined question-answer pairs, it can generate diverse task formats, such as multiple-choice and free-form questions. This flexibility allows SPICE to be applied to any domain, breaking the bottleneck that has confined previous methods to narrow fields like math and code. It also reduces dependence on expensive human-curated datasets for specialized domains like legal or medical analysis.SPICE in actionThe researchers evaluated SPICE on several base models, including Qwen3-4B-Base and OctoThinker-3B-Hybrid-Base. They compared its performance against baselines such as the base model with no training, a Reasoner model trained with a fixed \"Strong Challenger\" (Qwen3-32B-Instruct), and pure self-play methods like R-Zero and Absolute Zero. The evaluation covered a wide range of mathematical and general reasoning benchmarks.Across all models, SPICE consistently outperformed the baselines, delivering significant improvements in both mathematical and general reasoning tasks. The results show that the reasoning capabilities developed through corpus-grounded self-play transfer broadly across different models, thanks to the diverse external knowledge corpus they used.A key finding is that the adversarial dynamic creates an effective automatic curriculum. As training progresses, the Challenger learns to generate increasingly difficult problems. In one experiment, the Reasoner&#x27;s pass rate on a fixed set of problems increased from 55% to 85% over time, showing its improved capabilities. Meanwhile, later versions of the Challenger were able to generate questions that dropped the pass rate of an early-stage Reasoner from 55% to 35%, confirming that both roles co-evolve successfully.The researchers conclude that this approach presents a paradigm shift in self-improving reasoning methods from “closed-loop self-play that often stagnates due to hallucination drift, to open-ended improvement through interaction with the vast, verifiable knowledge embedded in web document corpora.”Currently, the corpus used for SPICE represents human experience captured in text. The ultimate goal is for self-improving systems to generate questions based on interactions with reality, including the physical world, the internet, and human interactions across multiple modalities like video, audio, and sensor data.",
          "content": "Researchers at Meta FAIR and the National University of Singapore have developed a new reinforcement learning framework for self-improving AI systems. Called Self-Play In Corpus Environments (SPICE), the framework pits two AI agents against each other, creating its own challenges and gradually improving without human supervision.While currently a proof-of-concept, this self-play mechanism could provide a basis for future AI systems that can dynamically adapt to their environments, making them more robust against the unpredictability of real-world applications.The challenge of self-improving AIThe goal of self-improving AI is to create systems that can enhance their capabilities by interacting with their environment. A common approach is reinforcement learning with verifiable rewards (RLVR), where models are rewarded for providing the correct answers to problems. This is often limited by its reliance on human-curated problem sets and domain-specific reward engineering, which makes it difficult to scale.Self-play, where a model improves by competing against itself, is another promising paradigm. But existing self-play methods for language models are often limited by two critical factors. Factual errors in generated questions and answers compound, leading to a feedback loop of hallucinations. When the problem generator and solver have information symmetry (i.e., share the same knowledge base) they fail to generate genuinely new challenges and fall into repetitive patterns. As the researchers note in their paper, “These systematic empirical failures indicate that self-improvement requires interaction with an external source providing diverse, verifiable feedback, rather than closed-loop pure introspection.”How SPICE worksSPICE is a self-play framework where a single model acts in two distinct roles. A \"Challenger\" constructs a curriculum of challenging problems from a large corpus of documents. A \"Reasoner\" then attempts to solve these problems without access to the source documents. This setup breaks the information symmetry that limits other self-play methods, as the Reasoner does not have access to the documents and knowledge that the Challenger uses to generate the problems.Grounding the tasks in a vast and diverse corpus of documents prevents hallucination by anchoring questions and answers in real-world content. This is important because for AI systems to reliably self-improve, they need external grounding sources. Therefore, LLM agents should learn from interactions with humans and the real world, not just their own outputs, to avoid compounding errors.The adversarial dynamic between the two roles creates an automatic curriculum. The Challenger is rewarded for generating problems that are both diverse and at the frontier of the Reasoner&#x27;s capability (not too easy and also not impossible). The Reasoner is rewarded for answering correctly. This symbiotic interaction pushes both agents to continuously discover and overcome new challenges. Because the system uses raw documents instead of pre-defined question-answer pairs, it can generate diverse task formats, such as multiple-choice and free-form questions. This flexibility allows SPICE to be applied to any domain, breaking the bottleneck that has confined previous methods to narrow fields like math and code. It also reduces dependence on expensive human-curated datasets for specialized domains like legal or medical analysis.SPICE in actionThe researchers evaluated SPICE on several base models, including Qwen3-4B-Base and OctoThinker-3B-Hybrid-Base. They compared its performance against baselines such as the base model with no training, a Reasoner model trained with a fixed \"Strong Challenger\" (Qwen3-32B-Instruct), and pure self-play methods like R-Zero and Absolute Zero. The evaluation covered a wide range of mathematical and general reasoning benchmarks.Across all models, SPICE consistently outperformed the baselines, delivering significant improvements in both mathematical and general reasoning tasks. The results show that the reasoning capabilities developed through corpus-grounded self-play transfer broadly across different models, thanks to the diverse external knowledge corpus they used.A key finding is that the adversarial dynamic creates an effective automatic curriculum. As training progresses, the Challenger learns to generate increasingly difficult problems. In one experiment, the Reasoner&#x27;s pass rate on a fixed set of problems increased from 55% to 85% over time, showing its improved capabilities. Meanwhile, later versions of the Challenger were able to generate questions that dropped the pass rate of an early-stage Reasoner from 55% to 35%, confirming that both roles co-evolve successfully.The researchers conclude that this approach presents a paradigm shift in self-improving reasoning methods from “closed-loop self-play that often stagnates due to hallucination drift, to open-ended improvement through interaction with the vast, verifiable knowledge embedded in web document corpora.”Currently, the corpus used for SPICE represents human experience captured in text. The ultimate goal is for self-improving systems to generate questions based on interactions with reality, including the physical world, the internet, and human interactions across multiple modalities like video, audio, and sensor data.",
          "feed_position": 3,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/UAYtncQWQ2a2g1rzWOCHR/19d3e60c8a6eef26b99a5a492491bbec/Adversarial_AI_training.jpg?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/only-9-of-developers-think-ai-code-can-be-used-without-human-oversight",
          "published_at": "Tue, 11 Nov 2025 19:43:00 GMT",
          "title": "Only 9% of developers think AI code can be used without human oversight, BairesDev survey reveals",
          "standfirst": "Senior software developers are preparing for a major shift in how they work as artificial intelligence becomes central to their workflows, according to BairesDev’s latest Dev Barometer report published today. VentureBeat was given an exclusive early look and the findings below come directly from that report. The quarterly global survey, which polled 501 developers and 19 project managers across 92 software initiatives, finds that nearly two-thirds (65%) of senior developers expect their roles to be redefined by AI in 2026. The data highlights a transformation underway in software development: fewer routine coding tasks, more emphasis on design and strategy, and a rising need for AI fluency.From Coders to StrategistsAmong those anticipating change, 74% say they expect to shift from hands-on coding to designing solutions. Another 61% plan to integrate AI-generated code into their workflows, and half foresee spending more time on system strategy and architecture.“It’s not about lines of code anymore,” said Justice Erolin, Chief Technology Officer at BairesDev, in a recent interview with VentureBeat conducted over video call. “It’s about the quality and type of code, and the kind of work developers are doing.”Erolin said the company is watching developers evolve from individual contributors into system thinkers.“AI is great at code scaffolding and generating unit tests, saving developers around eight hours a week,” he explained. “That time can now be used for solution architecture and strategy work—areas where AI still falls short.”The survey’s data reflects this shift. Developers are moving toward higher-value tasks while automation takes over much of the repetitive coding that once occupied junior engineers.Erolin noted that BairesDev’s internal data mirrors these findings. “We’re seeing a shift where senior engineers with AI tools are outperforming, and even replacing, the traditional senior-plus-junior team setup,” he said.Realism About AI’s LimitsDespite widespread enthusiasm, developers remain cautious about AI’s reliability.Over half (56%) describe AI-generated code as “somewhat reliable,” saying it still requires validation for accuracy and security. Only 9% trust it enough to use without human oversight.Erolin agreed with that sentiment. “AI doesn’t replace human oversight,” he said. “Even as tools improve, developers still need to understand how individual components fit into the bigger system.” He added that the biggest constraint in large language models today is “their context window”—the limited ability to retain and reason across entire systems. “Engineers need to think holistically about architecture, not just individual lines of code,” he said.The CTO described 2025 as a turning point for how engineers use AI tools like GitHub Copilot, Cursor, Claude, and OpenAI’s models. “We’re tracking what tools and models our engineers use,” he said. “But the bigger story is how those tools impact learning, productivity, and oversight.”That tempered optimism aligns with BairesDev’s previous Dev Barometer findings, which reported that 92% of developers were already using AI-assisted coding by Q3 2025, saving an average of 7.3 hours per week.A Year of UpskillingIn 2025, AI integration already brought tangible professional benefits. 74% of developers said the technology strengthened their technical skills, 50% reported better work-life balance, and 37% said AI tools expanded their career opportunities.Erolin said the company is seeing AI emerge as “a top use case for upskilling.” Developers use it to “learn new technologies faster and fill knowledge gaps,” he noted. “When developers understand how AI works and its limitations, they can use it to enhance—not replace—their critical thinking. They prompt better and learn more efficiently.”Still, he warned of a potential long-term risk in the industry’s current trajectory. “If junior engineers are being replaced or not hired, we’ll face a shortage of qualified senior engineers in ten years as current ones retire,” Erolin said.The Dev Barometer findings echo that concern. Developers expect leaner teams, but many also worry that fewer entry-level opportunities could lead to long-term talent pipeline issues.Leaner Teams, New PrioritiesDevelopers expect 2026 to bring smaller, more specialized teams. 58% say automation will reduce entry-level tasks, while 63% expect new career paths to emerge as AI redefines team structures. 59% anticipate that AI will create entirely new specialized roles.According to BairesDev’s data, developers currently divide their time between writing code (48%), debugging (42%), and documentation (35%). Only 19% report focusing primarily on creative problem-solving and innovation—a share that’s expected to grow as AI removes lower-level coding tasks.The report also highlights where developers see the fastest-growing areas for 2026: AI/ML (67%), data analytics (46%), and cybersecurity (45%). In parallel, 63% of project managers said developers will need more training in AI, cloud, and security.Erolin described the next generation of developers as “T-shaped engineers”—people with broad system knowledge and deep expertise in one or more areas. “The most important developer moving forward will be the T-shaped engineer,” he said. “Broad in understanding, deep in skill.”AI as an Industry StandardThe Q4 Dev Barometer frames AI not as an experiment but as a foundation for how teams will operate in 2026. Developers are moving beyond using AI as a coding shortcut and instead incorporating it into architecture, validation, and design decisions.Erolin emphasized that BairesDev is already adapting its internal teams to this new reality. “Our engineers are full-time with us, and we staff them out where they’re needed,” he said. “Some clients need help for six months to a year; others outsource their entire dev team to us.”He said BairesDev provides “about 5,000 software engineers from Latin America, offering clients timezone-aligned, culturally aligned, and highly fluent English-speaking talent.”As developers integrate AI deeper into their daily work, Erolin believes the competitive advantage will belong to those who understand both the technology’s capabilities and its constraints. “When developers learn to collaborate with AI instead of compete against it, that’s when the real productivity and creativity gains happen,” he said.Background: Who BairesDev IsFounded in Buenos Aires in 2009 by Nacho De Marco and Paul Azorin, BairesDev began with a mission to connect what it describes as the “top 1%” of Latin American developers with global companies seeking high-quality software solutions. The company grew from those early roots into a major nearshore software development and staffing provider, offering everything from individual developer placements to full end-to-end project outsourcing.Today, BairesDev claims to have delivered more than 1,200 projects across 130+ industries, serving hundreds of clients ranging from startups to Fortune 500 firms such as Google, Adobe, and Rolls-Royce. It operates with a remote-first model and a workforce of over 4,000 professionals across more than 40 countries, aligning its teams to North American time zones.The company emphasizes three core advantages: access to elite technical talent across 100+ technologies, rapid scalability for project needs, and nearshore proximity for real-time collaboration. It reports client relationships averaging over three years and a satisfaction rate around 91%.BairesDev’s unique position—bridging Latin American talent with global enterprise clients—gives it an unusually data-rich perspective on how AI is transforming software development at scale.The TakeawayThe Dev Barometer’s Q4 2025 results suggest 2026 will mark a turning point for software engineering. Developers are becoming system architects rather than pure coders, AI literacy is becoming a baseline requirement, and traditional entry-level roles may give way to new, specialized positions.As AI becomes embedded in every stage of development—from design to testing—developers who can combine technical fluency with strategic thinking are set to lead the next era of software creation.",
          "content": "Senior software developers are preparing for a major shift in how they work as artificial intelligence becomes central to their workflows, according to BairesDev’s latest Dev Barometer report published today. VentureBeat was given an exclusive early look and the findings below come directly from that report. The quarterly global survey, which polled 501 developers and 19 project managers across 92 software initiatives, finds that nearly two-thirds (65%) of senior developers expect their roles to be redefined by AI in 2026. The data highlights a transformation underway in software development: fewer routine coding tasks, more emphasis on design and strategy, and a rising need for AI fluency.From Coders to StrategistsAmong those anticipating change, 74% say they expect to shift from hands-on coding to designing solutions. Another 61% plan to integrate AI-generated code into their workflows, and half foresee spending more time on system strategy and architecture.“It’s not about lines of code anymore,” said Justice Erolin, Chief Technology Officer at BairesDev, in a recent interview with VentureBeat conducted over video call. “It’s about the quality and type of code, and the kind of work developers are doing.”Erolin said the company is watching developers evolve from individual contributors into system thinkers.“AI is great at code scaffolding and generating unit tests, saving developers around eight hours a week,” he explained. “That time can now be used for solution architecture and strategy work—areas where AI still falls short.”The survey’s data reflects this shift. Developers are moving toward higher-value tasks while automation takes over much of the repetitive coding that once occupied junior engineers.Erolin noted that BairesDev’s internal data mirrors these findings. “We’re seeing a shift where senior engineers with AI tools are outperforming, and even replacing, the traditional senior-plus-junior team setup,” he said.Realism About AI’s LimitsDespite widespread enthusiasm, developers remain cautious about AI’s reliability.Over half (56%) describe AI-generated code as “somewhat reliable,” saying it still requires validation for accuracy and security. Only 9% trust it enough to use without human oversight.Erolin agreed with that sentiment. “AI doesn’t replace human oversight,” he said. “Even as tools improve, developers still need to understand how individual components fit into the bigger system.” He added that the biggest constraint in large language models today is “their context window”—the limited ability to retain and reason across entire systems. “Engineers need to think holistically about architecture, not just individual lines of code,” he said.The CTO described 2025 as a turning point for how engineers use AI tools like GitHub Copilot, Cursor, Claude, and OpenAI’s models. “We’re tracking what tools and models our engineers use,” he said. “But the bigger story is how those tools impact learning, productivity, and oversight.”That tempered optimism aligns with BairesDev’s previous Dev Barometer findings, which reported that 92% of developers were already using AI-assisted coding by Q3 2025, saving an average of 7.3 hours per week.A Year of UpskillingIn 2025, AI integration already brought tangible professional benefits. 74% of developers said the technology strengthened their technical skills, 50% reported better work-life balance, and 37% said AI tools expanded their career opportunities.Erolin said the company is seeing AI emerge as “a top use case for upskilling.” Developers use it to “learn new technologies faster and fill knowledge gaps,” he noted. “When developers understand how AI works and its limitations, they can use it to enhance—not replace—their critical thinking. They prompt better and learn more efficiently.”Still, he warned of a potential long-term risk in the industry’s current trajectory. “If junior engineers are being replaced or not hired, we’ll face a shortage of qualified senior engineers in ten years as current ones retire,” Erolin said.The Dev Barometer findings echo that concern. Developers expect leaner teams, but many also worry that fewer entry-level opportunities could lead to long-term talent pipeline issues.Leaner Teams, New PrioritiesDevelopers expect 2026 to bring smaller, more specialized teams. 58% say automation will reduce entry-level tasks, while 63% expect new career paths to emerge as AI redefines team structures. 59% anticipate that AI will create entirely new specialized roles.According to BairesDev’s data, developers currently divide their time between writing code (48%), debugging (42%), and documentation (35%). Only 19% report focusing primarily on creative problem-solving and innovation—a share that’s expected to grow as AI removes lower-level coding tasks.The report also highlights where developers see the fastest-growing areas for 2026: AI/ML (67%), data analytics (46%), and cybersecurity (45%). In parallel, 63% of project managers said developers will need more training in AI, cloud, and security.Erolin described the next generation of developers as “T-shaped engineers”—people with broad system knowledge and deep expertise in one or more areas. “The most important developer moving forward will be the T-shaped engineer,” he said. “Broad in understanding, deep in skill.”AI as an Industry StandardThe Q4 Dev Barometer frames AI not as an experiment but as a foundation for how teams will operate in 2026. Developers are moving beyond using AI as a coding shortcut and instead incorporating it into architecture, validation, and design decisions.Erolin emphasized that BairesDev is already adapting its internal teams to this new reality. “Our engineers are full-time with us, and we staff them out where they’re needed,” he said. “Some clients need help for six months to a year; others outsource their entire dev team to us.”He said BairesDev provides “about 5,000 software engineers from Latin America, offering clients timezone-aligned, culturally aligned, and highly fluent English-speaking talent.”As developers integrate AI deeper into their daily work, Erolin believes the competitive advantage will belong to those who understand both the technology’s capabilities and its constraints. “When developers learn to collaborate with AI instead of compete against it, that’s when the real productivity and creativity gains happen,” he said.Background: Who BairesDev IsFounded in Buenos Aires in 2009 by Nacho De Marco and Paul Azorin, BairesDev began with a mission to connect what it describes as the “top 1%” of Latin American developers with global companies seeking high-quality software solutions. The company grew from those early roots into a major nearshore software development and staffing provider, offering everything from individual developer placements to full end-to-end project outsourcing.Today, BairesDev claims to have delivered more than 1,200 projects across 130+ industries, serving hundreds of clients ranging from startups to Fortune 500 firms such as Google, Adobe, and Rolls-Royce. It operates with a remote-first model and a workforce of over 4,000 professionals across more than 40 countries, aligning its teams to North American time zones.The company emphasizes three core advantages: access to elite technical talent across 100+ technologies, rapid scalability for project needs, and nearshore proximity for real-time collaboration. It reports client relationships averaging over three years and a satisfaction rate around 91%.BairesDev’s unique position—bridging Latin American talent with global enterprise clients—gives it an unusually data-rich perspective on how AI is transforming software development at scale.The TakeawayThe Dev Barometer’s Q4 2025 results suggest 2026 will mark a turning point for software engineering. Developers are becoming system architects rather than pure coders, AI literacy is becoming a baseline requirement, and traditional entry-level roles may give way to new, specialized positions.As AI becomes embedded in every stage of development—from design to testing—developers who can combine technical fluency with strategic thinking are set to lead the next era of software creation.",
          "feed_position": 4,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/5CiQuierRq34dYmuxJUNHb/a86df483038c29162f381f3fc96559f7/cfr0z3n_aerial_view_extended_view_of_hundreds_of_software_dev_51f077e9-5eef-4c86-bf4e-2dabd7ab231b_1.png?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/mobile/smartphones/remix-in-google-messages-brings-ai-photo-editing-to-even-more-phones-190000445.html",
          "published_at": "Tue, 11 Nov 2025 19:00:00 +0000",
          "title": "Remix in Google Messages brings AI photo editing to even more phones",
          "standfirst": "Google's November 2025 Pixel Drop is available today, and it includes some new features that should benefit more than just the Pixel phones and tablets Google typically targets. A new messaging feature called Remix in Google Messages expands access to Gemini-powered photo editing, and Google's improving its Scam Detection and Pixel VIP features, too.Remix in Google Messages is essentially Google's Nano Banana photo editing tool, but available directly in Google Messages. The feature uses the same image model as Gemini and Google Photos, and lets Messages users tweak photos directly in a chat. Importantly, the edited photos are viewable by anyone in the chat, even if they're not on Android. The feature is available in English in the US, UK, Australia, Canada, India, Ireland and New Zealand with RCS enabled. Google says remixed images can also be sent over MMS.In the Google Photos app, those photo editing skills will now also be even more personalized. Google says eligible Android users with Ask Photos and Face Groups enabled, can refer to people in their photos by name while they edit. The Photos app can use past photos of your labelled friends to make tweaks like adding a smile or opening someone's eyes without having to be provided a previous reference.Power Saving Mode in action.GoogleFor anyone who owns one of the latest Pixel 10 phones, the Pixel Drop includes a new Power Saving Mode in Google Maps that blacks out the screen and only shows essential information and directions. Google claims the feature and extend battery life for up to four hours. The company hasn’t announced any plans, but the feature seems like it could be an equally good fit on Android Auto.Scam Detection is Android's built-in feature for identifying scam calls and warning you with a notification. As part of the Pixel Drop, Scam Detection will now also work with messages, warning you in your notifications on Pixel 6 devices and up if you could be dealing with fraud. As part of the update, Scam Detection is also now available in the UK, Ireland, India, Australia and Canada. The update also includes support for Notification Summaries on the Pixel 9 and up, which summarize frequent group chat notifications as a recap in your notification shade. If you've marked anyone as a Pixel VIP (a feature added back in June), Android will now also prioritize their messages so you don't miss them.Alongside those more practical features, Google is also introducing a new seasonal Wicked: For Good theme pack on Pixel 6 and newer devices. The theme pack is accessible via a new Theme Packs app that was released earlier in November. While it uses existing options like your wallpaper and icon settings to set \"Glinda\" and \"Elphaba\" themes, the convenience of Theme Packs is the ability to change all those settings at once. It's unfortunate Google's introducing the tool with an ad, but it could prove useful down the line.This article originally appeared on Engadget at https://www.engadget.com/mobile/smartphones/remix-in-google-messages-brings-ai-photo-editing-to-even-more-phones-190000445.html?src=rss",
          "content": "Google's November 2025 Pixel Drop is available today, and it includes some new features that should benefit more than just the Pixel phones and tablets Google typically targets. A new messaging feature called Remix in Google Messages expands access to Gemini-powered photo editing, and Google's improving its Scam Detection and Pixel VIP features, too.Remix in Google Messages is essentially Google's Nano Banana photo editing tool, but available directly in Google Messages. The feature uses the same image model as Gemini and Google Photos, and lets Messages users tweak photos directly in a chat. Importantly, the edited photos are viewable by anyone in the chat, even if they're not on Android. The feature is available in English in the US, UK, Australia, Canada, India, Ireland and New Zealand with RCS enabled. Google says remixed images can also be sent over MMS.In the Google Photos app, those photo editing skills will now also be even more personalized. Google says eligible Android users with Ask Photos and Face Groups enabled, can refer to people in their photos by name while they edit. The Photos app can use past photos of your labelled friends to make tweaks like adding a smile or opening someone's eyes without having to be provided a previous reference.Power Saving Mode in action.GoogleFor anyone who owns one of the latest Pixel 10 phones, the Pixel Drop includes a new Power Saving Mode in Google Maps that blacks out the screen and only shows essential information and directions. Google claims the feature and extend battery life for up to four hours. The company hasn’t announced any plans, but the feature seems like it could be an equally good fit on Android Auto.Scam Detection is Android's built-in feature for identifying scam calls and warning you with a notification. As part of the Pixel Drop, Scam Detection will now also work with messages, warning you in your notifications on Pixel 6 devices and up if you could be dealing with fraud. As part of the update, Scam Detection is also now available in the UK, Ireland, India, Australia and Canada. The update also includes support for Notification Summaries on the Pixel 9 and up, which summarize frequent group chat notifications as a recap in your notification shade. If you've marked anyone as a Pixel VIP (a feature added back in June), Android will now also prioritize their messages so you don't miss them.Alongside those more practical features, Google is also introducing a new seasonal Wicked: For Good theme pack on Pixel 6 and newer devices. The theme pack is accessible via a new Theme Packs app that was released earlier in November. While it uses existing options like your wallpaper and icon settings to set \"Glinda\" and \"Elphaba\" themes, the convenience of Theme Packs is the ability to change all those settings at once. It's unfortunate Google's introducing the tool with an ad, but it could prove useful down the line.This article originally appeared on Engadget at https://www.engadget.com/mobile/smartphones/remix-in-google-messages-brings-ai-photo-editing-to-even-more-phones-190000445.html?src=rss",
          "feed_position": 40,
          "image_url": "https://d29szjachogqwa.cloudfront.net/images/user-uploaded/Google-Maps-November-Pixel-Drop.jpg"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/get-half-off-our-favorite-budgeting-app-for-black-friday-140025864.html",
          "published_at": "Tue, 11 Nov 2025 17:01:26 +0000",
          "title": "Black Friday deals include half off our favorite budgeting app",
          "standfirst": "Now's the time of year you might be reconsidering how you budget your finances, or establishing a plan if you don't have one already. While it's possible to do it all yourself, budgeting apps can automate some processes and make it easier to see where your money is going and patterns, both good and bad, that might be occurring. For Black Friday, you can get 50 percent off our favorite budgeting app, Quicken Simplifi. The Quicken Simplifi app is down to $3 monthly from $6 monthly, adding up to $36 for the year. Quicken Classic, the company's \"original desktop software\" for \"experienced investors\" is also half off at $6 monthly, down from $12 monthly. The sale starts today and is available until Wednesday, December 3. One of the many things that sets Quicken Simplifi apart from its competitors is its sleek, easy to use interface. The setup is pretty straightforward and it allows for your spouse or financial advisor to act as co-manager of the account. It also clearly shows figures like net worth, recent spending, upcoming recurring payments and more. Plus, there's an option to say if you're expecting a refund. Quicken Simplifi unfortunately doesn't offer a free trial so testing it out with a discount means less money invested if it's not for you. This article originally appeared on Engadget at https://www.engadget.com/deals/get-half-off-our-favorite-budgeting-app-for-black-friday-140025864.html?src=rss",
          "content": "Now's the time of year you might be reconsidering how you budget your finances, or establishing a plan if you don't have one already. While it's possible to do it all yourself, budgeting apps can automate some processes and make it easier to see where your money is going and patterns, both good and bad, that might be occurring. For Black Friday, you can get 50 percent off our favorite budgeting app, Quicken Simplifi. The Quicken Simplifi app is down to $3 monthly from $6 monthly, adding up to $36 for the year. Quicken Classic, the company's \"original desktop software\" for \"experienced investors\" is also half off at $6 monthly, down from $12 monthly. The sale starts today and is available until Wednesday, December 3. One of the many things that sets Quicken Simplifi apart from its competitors is its sleek, easy to use interface. The setup is pretty straightforward and it allows for your spouse or financial advisor to act as co-manager of the account. It also clearly shows figures like net worth, recent spending, upcoming recurring payments and more. Plus, there's an option to say if you're expecting a refund. Quicken Simplifi unfortunately doesn't offer a free trial so testing it out with a discount means less money invested if it's not for you. This article originally appeared on Engadget at https://www.engadget.com/deals/get-half-off-our-favorite-budgeting-app-for-black-friday-140025864.html?src=rss",
          "feed_position": 41
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/ai/google-photos-now-has-six-more-ai-powered-features-170000125.html",
          "published_at": "Tue, 11 Nov 2025 17:00:00 +0000",
          "title": "Google Photos now has six more AI-powered features",
          "standfirst": "Google Photos introduced a fresh batch of a half-dozen AI-powered features today. First is personalized photo editing. Using \"Help me edit,\" you can now make adjustments to portraits and group shots, such as removing sunglasses or fixing closed eyes. Google says that the AI uses other images stored in a user's face groups to make accurate changes to the people in your photo library. The \"Help me edit\" voice- or text-controlled photo editing tool is also starting to roll out to iOS users in the US.Next, the company is integrating its Nano Banana image editor into Google Photos. Users can make open-ended restyling requests in the \"Help me edit\" tool, such as making a picture look like a Renaissance painting or a mosaic. Nano Banana will also power a new Create with AI section, which will provide templates based on popular requests to jumpstart the AI editing process. This feature will roll out to the Create tab for Android users in the US and India beginning next week. Later on, Google will begin personalizing these templates to the particular hobbies and experiences captured in a person's photo library. Following a \"pause\" and restart in June, the Ask Photos tool is also expanding. The feature for AI-powered searches of the Google photo library will be available in more than 100 new markets and will support 17 new languages starting this week. Finally, Google Photos is getting a new Ask button aimed at delivering more details about a specific image. After tapping the button, a user can type questions about the content of the photo, find similar pictures in their library or begin describing desired edits. This feature is rolling out just in the US for now, but on both Android and iOS platforms.This article originally appeared on Engadget at https://www.engadget.com/ai/google-photos-now-has-six-more-ai-powered-features-170000125.html?src=rss",
          "content": "Google Photos introduced a fresh batch of a half-dozen AI-powered features today. First is personalized photo editing. Using \"Help me edit,\" you can now make adjustments to portraits and group shots, such as removing sunglasses or fixing closed eyes. Google says that the AI uses other images stored in a user's face groups to make accurate changes to the people in your photo library. The \"Help me edit\" voice- or text-controlled photo editing tool is also starting to roll out to iOS users in the US.Next, the company is integrating its Nano Banana image editor into Google Photos. Users can make open-ended restyling requests in the \"Help me edit\" tool, such as making a picture look like a Renaissance painting or a mosaic. Nano Banana will also power a new Create with AI section, which will provide templates based on popular requests to jumpstart the AI editing process. This feature will roll out to the Create tab for Android users in the US and India beginning next week. Later on, Google will begin personalizing these templates to the particular hobbies and experiences captured in a person's photo library. Following a \"pause\" and restart in June, the Ask Photos tool is also expanding. The feature for AI-powered searches of the Google photo library will be available in more than 100 new markets and will support 17 new languages starting this week. Finally, Google Photos is getting a new Ask button aimed at delivering more details about a specific image. After tapping the button, a user can type questions about the content of the photo, find similar pictures in their library or begin describing desired edits. This feature is rolling out just in the US for now, but on both Android and iOS platforms.This article originally appeared on Engadget at https://www.engadget.com/ai/google-photos-now-has-six-more-ai-powered-features-170000125.html?src=rss",
          "feed_position": 42
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/black-friday-apple-deals-include-the-apple-watch-se-3-on-sale-for-200-133057960.html",
          "published_at": "Tue, 11 Nov 2025 14:45:38 +0000",
          "title": "Black Friday Apple deals include the Apple Watch SE 3 on sale for $200",
          "standfirst": "Black Friday will be here before you know it, but you can already save on some sought-after tech. Case in point: the new Apple Watch SE 3. Apple's most affordable smartwatch is even cheaper right now, down to just $200. We consider this to be the best budget Apple Watch, and arguably the best smartwatch for folks who have never owned one before. The latest version runs on the same chipset found in the new flagship models, and it has most of the same fitness and workout tracking features you'll find in those more expensive devices as well. The SE 3 also now has an always-on display, making it easier to glance down throughout the day to check the time or see activity stats without moving your wrist, and fast-charging support makes it a more viable sleep tracker. Just plop it down on its charger for a bit at the end of the day and put it back on to monitor your sleep overnight. Also discounted is the high-end Apple Watch Ultra 3, which you can snag for $100 off. The sale model comes with 64GB of storage, a 49mm screen and GPS and cellular service. Notably, it's also only available with the one size, adjustable band and in two colors: a Black titanium case with Black Ocean band and a natural titanium Case with Anchor Blue Ocean band. The Apple Watch Ultra 3 came out in early September and is one of the first smartwatches to support satellite communications. This feature means you can call, send messages or share your location with emergency services through the watch — even if you don't have a connection. The new Ultra 3 also has a larger screen thanks to thinner bezels and a battery that can last for up to 42 hours. This article originally appeared on Engadget at https://www.engadget.com/deals/black-friday-apple-deals-include-the-apple-watch-se-3-on-sale-for-200-133057960.html?src=rss",
          "content": "Black Friday will be here before you know it, but you can already save on some sought-after tech. Case in point: the new Apple Watch SE 3. Apple's most affordable smartwatch is even cheaper right now, down to just $200. We consider this to be the best budget Apple Watch, and arguably the best smartwatch for folks who have never owned one before. The latest version runs on the same chipset found in the new flagship models, and it has most of the same fitness and workout tracking features you'll find in those more expensive devices as well. The SE 3 also now has an always-on display, making it easier to glance down throughout the day to check the time or see activity stats without moving your wrist, and fast-charging support makes it a more viable sleep tracker. Just plop it down on its charger for a bit at the end of the day and put it back on to monitor your sleep overnight. Also discounted is the high-end Apple Watch Ultra 3, which you can snag for $100 off. The sale model comes with 64GB of storage, a 49mm screen and GPS and cellular service. Notably, it's also only available with the one size, adjustable band and in two colors: a Black titanium case with Black Ocean band and a natural titanium Case with Anchor Blue Ocean band. The Apple Watch Ultra 3 came out in early September and is one of the first smartwatches to support satellite communications. This feature means you can call, send messages or share your location with emergency services through the watch — even if you don't have a connection. The new Ultra 3 also has a larger screen thanks to thinner bezels and a battery that can last for up to 42 hours. This article originally appeared on Engadget at https://www.engadget.com/deals/black-friday-apple-deals-include-the-apple-watch-se-3-on-sale-for-200-133057960.html?src=rss",
          "feed_position": 45
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/lego-black-friday-deals-star-wars-and-disney-sets-are-up-to-37-percent-off-with-these-early-sales-155007667.html",
          "published_at": "Tue, 11 Nov 2025 14:30:36 +0000",
          "title": "Lego Black Friday deals: Star Wars and Disney sets are up to 37 percent off with these early sales",
          "standfirst": "Lego sets are probably at the top of your kid's wish list, and maybe they're at the top of your personal list, too. With so many to choose from, you'll be able to find one that makes a great gift for anyone who you know loves these little building bricks. Black Friday Lego deals are what to look for this time of year, because you can typically save at least 20 percent on a good number of sets. Yes, that often includes the most popular ones from the Star Wars, Super Mario, Harry Potter and other collections. In general, we always recommend using a price tracker when determining if a Lego deal is in fact a good one. Below, we've collected the best Lego Black Friday deals we could find right now. You'll find Lego deals across the board this holiday season at retailers like Amazon and Walmart, but don't overlook Lego's own site. If you join the free Lego Insiders program, you'll build up points with each purchase that you can redeem in the future, get special discounts and sometimes get exclusive gifts when you buy. While not a deal, arguably the hottest Lego for Black Friday will be the brand new Star Trek USS Enterprise set, which was announced recently. It has a whopping 3,600 pieces and will be a must-have for any Star Trek fans. The set will be available starting November 28 for $400. Best Lego Black Friday deals LEGO Disney Frozen Advent Calendar 2025 43273 for $31 (32 percent off) Lego Harry Potter Advent Calendar 2025 76456 for $39 (13 percent off) LEGO Star Wars Brick-Built Star Wars Logo 75407 for $48 (20 percent off) LEGO Star Wars Grogu with Hover Pram Building Toy Set 75403 for $63 (37 percent off) LEGO Star Wars Millennium Falcon A New Hope 25th Anniversary Collectable Model 75375 for $68 (20 percent off) LEGO Star Wars R2-D2 Building Toy Set 75379 for $80 (20 percent off) LEGO Harry Potter Hogwarts Castle and Grounds 76419 for $136 (20 percent off) LEGO Creator 3 in 1 Magical Unicorn Toy 31140 for $7 (32 percent off) LEGO City Donut Truck Toy 60452 for $16 (20 percent off) LEGO Speed Champions 2 Fast 2 Furious Nissan Skyline GT-R (R34) Race Car 76917 for $18 (28 percent off) LEGO Botanicals Happy Plants Building Toys 10349 for $20 (13 percent off) LEGO Botanicals Mini Orchid Building Set 10343 for $24 (20 percent off) LEGO Art Hokusai The Great Wave Framed Japanese Wall Art Building Set 31208 for $85 (15 percent off) This article originally appeared on Engadget at https://www.engadget.com/deals/lego-black-friday-deals-star-wars-and-disney-sets-are-up-to-37-percent-off-with-these-early-sales-155007667.html?src=rss",
          "content": "Lego sets are probably at the top of your kid's wish list, and maybe they're at the top of your personal list, too. With so many to choose from, you'll be able to find one that makes a great gift for anyone who you know loves these little building bricks. Black Friday Lego deals are what to look for this time of year, because you can typically save at least 20 percent on a good number of sets. Yes, that often includes the most popular ones from the Star Wars, Super Mario, Harry Potter and other collections. In general, we always recommend using a price tracker when determining if a Lego deal is in fact a good one. Below, we've collected the best Lego Black Friday deals we could find right now. You'll find Lego deals across the board this holiday season at retailers like Amazon and Walmart, but don't overlook Lego's own site. If you join the free Lego Insiders program, you'll build up points with each purchase that you can redeem in the future, get special discounts and sometimes get exclusive gifts when you buy. While not a deal, arguably the hottest Lego for Black Friday will be the brand new Star Trek USS Enterprise set, which was announced recently. It has a whopping 3,600 pieces and will be a must-have for any Star Trek fans. The set will be available starting November 28 for $400. Best Lego Black Friday deals LEGO Disney Frozen Advent Calendar 2025 43273 for $31 (32 percent off) Lego Harry Potter Advent Calendar 2025 76456 for $39 (13 percent off) LEGO Star Wars Brick-Built Star Wars Logo 75407 for $48 (20 percent off) LEGO Star Wars Grogu with Hover Pram Building Toy Set 75403 for $63 (37 percent off) LEGO Star Wars Millennium Falcon A New Hope 25th Anniversary Collectable Model 75375 for $68 (20 percent off) LEGO Star Wars R2-D2 Building Toy Set 75379 for $80 (20 percent off) LEGO Harry Potter Hogwarts Castle and Grounds 76419 for $136 (20 percent off) LEGO Creator 3 in 1 Magical Unicorn Toy 31140 for $7 (32 percent off) LEGO City Donut Truck Toy 60452 for $16 (20 percent off) LEGO Speed Champions 2 Fast 2 Furious Nissan Skyline GT-R (R34) Race Car 76917 for $18 (28 percent off) LEGO Botanicals Happy Plants Building Toys 10349 for $20 (13 percent off) LEGO Botanicals Mini Orchid Building Set 10343 for $24 (20 percent off) LEGO Art Hokusai The Great Wave Framed Japanese Wall Art Building Set 31208 for $85 (15 percent off) This article originally appeared on Engadget at https://www.engadget.com/deals/lego-black-friday-deals-star-wars-and-disney-sets-are-up-to-37-percent-off-with-these-early-sales-155007667.html?src=rss",
          "feed_position": 46
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/get-three-months-of-audible-for-3-in-this-black-friday-deal-140010983.html",
          "published_at": "Tue, 11 Nov 2025 14:00:10 +0000",
          "title": "Get three months of Audible for $3 in this Black Friday deal",
          "standfirst": "Amazon is selling three months of Audible for just $3 in honor of Black Friday. The company typically reserves this sale for Prime Day, so it's nice to see the deal make an unexpected return. This breaks down to $1 per month for the first three months, which is a boon for audiobook fans. Just make sure to cancel before the 90 days are up, as the subscription will auto-renew at $15 per month. That's not the worst deal in the world, given the vast number of titles available on the platform, but still. Audible has a diverse catalog that goes beyond audiobooks. It also hosts podcasts and Audible Originals. Subscribers get to choose one audiobook each month to keep in their collection for free, including best-sellers or new releases. Users also get unlimited access to the Plus Catalog, which houses thousands of audiobooks. Finally, active members get discounts on many audiobooks when looking to purchase. Winter is coming and this is a good way to make sure you have plenty to listen to throughout the next three months. This deal does have a time limit. It expires on December 16.This article originally appeared on Engadget at https://www.engadget.com/deals/get-three-months-of-audible-for-3-in-this-black-friday-deal-140010983.html?src=rss",
          "content": "Amazon is selling three months of Audible for just $3 in honor of Black Friday. The company typically reserves this sale for Prime Day, so it's nice to see the deal make an unexpected return. This breaks down to $1 per month for the first three months, which is a boon for audiobook fans. Just make sure to cancel before the 90 days are up, as the subscription will auto-renew at $15 per month. That's not the worst deal in the world, given the vast number of titles available on the platform, but still. Audible has a diverse catalog that goes beyond audiobooks. It also hosts podcasts and Audible Originals. Subscribers get to choose one audiobook each month to keep in their collection for free, including best-sellers or new releases. Users also get unlimited access to the Plus Catalog, which houses thousands of audiobooks. Finally, active members get discounts on many audiobooks when looking to purchase. Winter is coming and this is a good way to make sure you have plenty to listen to throughout the next three months. This deal does have a time limit. It expires on December 16.This article originally appeared on Engadget at https://www.engadget.com/deals/get-three-months-of-audible-for-3-in-this-black-friday-deal-140010983.html?src=rss",
          "feed_position": 49
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/meta-returns-to-open-source-ai-with-omnilingual-asr-models-that-can",
          "published_at": "Mon, 10 Nov 2025 20:27:00 GMT",
          "title": "Meta returns to open source AI with Omnilingual ASR models that can transcribe 1,600+ languages natively",
          "standfirst": "Meta has just released a new multilingual automatic speech recognition (ASR) system supporting 1,600+ languages — dwarfing OpenAI’s open source Whisper model, which supports just 99. Is architecture also allows developers to extend that support to thousands more. Through a feature called zero-shot in-context learning, users can provide a few paired examples of audio and text in a new language at inference time, enabling the model to transcribe additional utterances in that language without any retraining.In practice, this expands potential coverage to more than 5,400 languages — roughly every spoken language with a known script.It’s a shift from static model capabilities to a flexible framework that communities can adapt themselves. So while the 1,600 languages reflect official training coverage, the broader figure represents Omnilingual ASR’s capacity to generalize on demand, making it the most extensible speech recognition system released to date.Best of all: it&#x27;s been open sourced under a plain Apache 2.0 license — not a restrictive, quasi open-source Llama license like the company&#x27;s prior releases, which limited use by larger enterprises unless they paid licensing fees — meaning researchers and developers are free to take and implement it right away, for free, without restrictions, even in commercial and enterprise-grade projects!Released on November 10 on Meta&#x27;s website, Github, along with a demo space on Hugging Face and technical paper, Meta’s Omnilingual ASR suite includes a family of speech recognition models, a 7-billion parameter multilingual audio representation model, and a massive speech corpus spanning over 350 previously underserved languages. All resources are freely available under open licenses, and the models support speech-to-text transcription out of the box.“By open sourcing these models and dataset, we aim to break down language barriers, expand digital access, and empower communities worldwide,” Meta posted on its @AIatMeta account on XDesigned for Speech-to-Text TranscriptionAt its core, Omnilingual ASR is a speech-to-text system. The models are trained to convert spoken language into written text, supporting applications like voice assistants, transcription tools, subtitles, oral archive digitization, and accessibility features for low-resource languages.Unlike earlier ASR models that required extensive labeled training data, Omnilingual ASR includes a zero-shot variant. This version can transcribe languages it has never seen before—using just a few paired examples of audio and corresponding text. This lowers the barrier for adding new or endangered languages dramatically, removing the need for large corpora or retraining.Model Family and Technical DesignThe Omnilingual ASR suite includes multiple model families trained on more than 4.3 million hours of audio from 1,600+ languages:wav2vec 2.0 models for self-supervised speech representation learning (300M–7B parameters)CTC-based ASR models for efficient supervised transcriptionLLM-ASR models combining a speech encoder with a Transformer-based text decoder for state-of-the-art transcriptionLLM-ZeroShot ASR model, enabling inference-time adaptation to unseen languagesAll models follow an encoder–decoder design: raw audio is converted into a language-agnostic representation, then decoded into written text.Why the Scale MattersWhile Whisper and similar models have advanced ASR capabilities for global languages, they fall short on the long tail of human linguistic diversity. Whisper supports 99 languages. Meta’s system:Directly supports 1,600+ languagesCan generalize to 5,400+ languages using in-context learningAchieves character error rates (CER) under 10% in 78% of supported languagesAmong those supported are more than 500 languages never previously covered by any ASR model, according to Meta’s research paper.This expansion opens new possibilities for communities whose languages are often excluded from digital toolsHere’s the revised and expanded background section, integrating the broader context of Meta’s 2025 AI strategy, leadership changes, and Llama 4’s reception, complete with in-text citations and links:Background: Meta’s AI Overhaul and a Rebound from Llama 4The release of Omnilingual ASR arrives at a pivotal moment in Meta’s AI strategy, following a year marked by organizational turbulence, leadership changes, and uneven product execution. Omnilingual ASR is the first major open-source model release since the rollout of Llama 4, Meta’s latest large language model, which debuted in April 2025 to mixed and ultimately poor reviews, with scant enterprise adoption compared to Chinese open source model competitors.The failure led Meta founder and CEO Mark Zuckerberg to appoint Alexandr Wang, co-founder and prior CEO of AI data supplier Scale AI, as Chief AI Officer, and embark on an extensive and costly hiring spree that shocked the AI and business communities with eye-watering pay packages for top AI researchers.In contrast, Omnilingual ASR represents a strategic and reputational reset. It returns Meta to a domain where the company has historically led — multilingual AI — and offers a truly extensible, community-oriented stack with minimal barriers to entry. The system’s support for 1,600+ languages and its extensibility to over 5,000 more via zero-shot in-context learning reassert Meta’s engineering credibility in language technology. Importantly, it does so through a free and permissively licensed release, under Apache 2.0, with transparent dataset sourcing and reproducible training protocols.This shift aligns with broader themes in Meta’s 2025 strategy. The company has refocused its narrative around a “personal superintelligence” vision, investing heavily in infrastructure (including a September release of custom AI accelerators and Arm-based inference stacks) source while downplaying the metaverse in favor of foundational AI capabilities. The return to public training data in Europe after a regulatory pause also underscores its intention to compete globally, despite privacy scrutiny source.Omnilingual ASR, then, is more than a model release — it’s a calculated move to reassert control of the narrative: from the fragmented rollout of Llama 4 to a high-utility, research-grounded contribution that aligns with Meta’s long-term AI platform strategy.Community-Centered Dataset CollectionTo achieve this scale, Meta partnered with researchers and community organizations in Africa, Asia, and elsewhere to create the Omnilingual ASR Corpus, a 3,350-hour dataset across 348 low-resource languages. Contributors were compensated local speakers, and recordings were gathered in collaboration with groups like:African Next Voices: A Gates Foundation–supported consortium including Maseno University (Kenya), University of Pretoria, and Data Science NigeriaMozilla Foundation’s Common Voice, supported through the Open Multilingual Speech FundLanfrica / NaijaVoices, which created data for 11 African languages including Igala, Serer, and UrhoboThe data collection focused on natural, unscripted speech. Prompts were designed to be culturally relevant and open-ended, such as “Is it better to have a few close friends or many casual acquaintances? Why?” Transcriptions used established writing systems, with quality assurance built into every step.Performance and Hardware ConsiderationsThe largest model in the suite, the omniASR_LLM_7B, requires ~17GB of GPU memory for inference, making it suitable for deployment on high-end hardware. Smaller models (300M–1B) can run on lower-power devices and deliver real-time transcription speeds.Performance benchmarks show strong results even in low-resource scenarios:CER <10% in 95% of high-resource and mid-resource languagesCER <10% in 36% of low-resource languagesRobustness in noisy conditions and unseen domains, especially with fine-tuningThe zero-shot system, omniASR_LLM_7B_ZS, can transcribe new languages with minimal setup. Users provide a few sample audio–text pairs, and the model generates transcriptions for new utterances in the same language.Open Access and Developer ToolingAll models and the dataset are licensed under permissive terms:Apache 2.0 for models and codeCC-BY 4.0 for the Omnilingual ASR Corpus on HuggingFaceInstallation is supported via PyPI and uv:pip install omnilingual-asrMeta also provides:A HuggingFace dataset integrationPre-built inference pipelinesLanguage-code conditioning for improved accuracyDevelopers can view the full list of supported languages using the API:from omnilingual_asr.models.wav2vec2_llama.lang_ids import supported_langsprint(len(supported_langs)) print(supported_langs)Broader ImplicationsOmnilingual ASR reframes language coverage in ASR from a fixed list to an extensible framework. It enables:Community-driven inclusion of underrepresented languagesDigital access for oral and endangered languagesResearch on speech tech in linguistically diverse contextsCrucially, Meta emphasizes ethical considerations throughout—advocating for open-source participation and collaboration with native-speaking communities.“No model can ever anticipate and include all of the world’s languages in advance,” the Omnilingual ASR paper states, “but Omnilingual ASR makes it possible for communities to extend recognition with their own data.”Access the ToolsAll resources are now available at:Code + Models: github.com/facebookresearch/omnilingual-asrDataset: huggingface.co/datasets/facebook/omnilingual-asr-corpusBlogpost: ai.meta.com/blog/omnilingual-asrWhat This Means for EnterprisesFor enterprise developers, especially those operating in multilingual or international markets, Omnilingual ASR significantly lowers the barrier to deploying speech-to-text systems across a broader range of customers and geographies. Instead of relying on commercial ASR APIs that support only a narrow set of high-resource languages, teams can now integrate an open-source pipeline that covers over 1,600 languages out of the box—with the option to extend it to thousands more via zero-shot learning.This flexibility is especially valuable for enterprises working in sectors like voice-based customer support, transcription services, accessibility, education, or civic technology, where local language coverage can be a competitive or regulatory necessity. Because the models are released under the permissive Apache 2.0 license, businesses can fine-tune, deploy, or integrate them into proprietary systems without restrictive terms.It also represents a shift in the ASR landscape—from centralized, cloud-gated offerings to community-extendable infrastructure. By making multilingual speech recognition more accessible, customizable, and cost-effective, Omnilingual ASR opens the door to a new generation of enterprise speech applications built around linguistic inclusion rather than linguistic limitation.",
          "content": "Meta has just released a new multilingual automatic speech recognition (ASR) system supporting 1,600+ languages — dwarfing OpenAI’s open source Whisper model, which supports just 99. Is architecture also allows developers to extend that support to thousands more. Through a feature called zero-shot in-context learning, users can provide a few paired examples of audio and text in a new language at inference time, enabling the model to transcribe additional utterances in that language without any retraining.In practice, this expands potential coverage to more than 5,400 languages — roughly every spoken language with a known script.It’s a shift from static model capabilities to a flexible framework that communities can adapt themselves. So while the 1,600 languages reflect official training coverage, the broader figure represents Omnilingual ASR’s capacity to generalize on demand, making it the most extensible speech recognition system released to date.Best of all: it&#x27;s been open sourced under a plain Apache 2.0 license — not a restrictive, quasi open-source Llama license like the company&#x27;s prior releases, which limited use by larger enterprises unless they paid licensing fees — meaning researchers and developers are free to take and implement it right away, for free, without restrictions, even in commercial and enterprise-grade projects!Released on November 10 on Meta&#x27;s website, Github, along with a demo space on Hugging Face and technical paper, Meta’s Omnilingual ASR suite includes a family of speech recognition models, a 7-billion parameter multilingual audio representation model, and a massive speech corpus spanning over 350 previously underserved languages. All resources are freely available under open licenses, and the models support speech-to-text transcription out of the box.“By open sourcing these models and dataset, we aim to break down language barriers, expand digital access, and empower communities worldwide,” Meta posted on its @AIatMeta account on XDesigned for Speech-to-Text TranscriptionAt its core, Omnilingual ASR is a speech-to-text system. The models are trained to convert spoken language into written text, supporting applications like voice assistants, transcription tools, subtitles, oral archive digitization, and accessibility features for low-resource languages.Unlike earlier ASR models that required extensive labeled training data, Omnilingual ASR includes a zero-shot variant. This version can transcribe languages it has never seen before—using just a few paired examples of audio and corresponding text. This lowers the barrier for adding new or endangered languages dramatically, removing the need for large corpora or retraining.Model Family and Technical DesignThe Omnilingual ASR suite includes multiple model families trained on more than 4.3 million hours of audio from 1,600+ languages:wav2vec 2.0 models for self-supervised speech representation learning (300M–7B parameters)CTC-based ASR models for efficient supervised transcriptionLLM-ASR models combining a speech encoder with a Transformer-based text decoder for state-of-the-art transcriptionLLM-ZeroShot ASR model, enabling inference-time adaptation to unseen languagesAll models follow an encoder–decoder design: raw audio is converted into a language-agnostic representation, then decoded into written text.Why the Scale MattersWhile Whisper and similar models have advanced ASR capabilities for global languages, they fall short on the long tail of human linguistic diversity. Whisper supports 99 languages. Meta’s system:Directly supports 1,600+ languagesCan generalize to 5,400+ languages using in-context learningAchieves character error rates (CER) under 10% in 78% of supported languagesAmong those supported are more than 500 languages never previously covered by any ASR model, according to Meta’s research paper.This expansion opens new possibilities for communities whose languages are often excluded from digital toolsHere’s the revised and expanded background section, integrating the broader context of Meta’s 2025 AI strategy, leadership changes, and Llama 4’s reception, complete with in-text citations and links:Background: Meta’s AI Overhaul and a Rebound from Llama 4The release of Omnilingual ASR arrives at a pivotal moment in Meta’s AI strategy, following a year marked by organizational turbulence, leadership changes, and uneven product execution. Omnilingual ASR is the first major open-source model release since the rollout of Llama 4, Meta’s latest large language model, which debuted in April 2025 to mixed and ultimately poor reviews, with scant enterprise adoption compared to Chinese open source model competitors.The failure led Meta founder and CEO Mark Zuckerberg to appoint Alexandr Wang, co-founder and prior CEO of AI data supplier Scale AI, as Chief AI Officer, and embark on an extensive and costly hiring spree that shocked the AI and business communities with eye-watering pay packages for top AI researchers.In contrast, Omnilingual ASR represents a strategic and reputational reset. It returns Meta to a domain where the company has historically led — multilingual AI — and offers a truly extensible, community-oriented stack with minimal barriers to entry. The system’s support for 1,600+ languages and its extensibility to over 5,000 more via zero-shot in-context learning reassert Meta’s engineering credibility in language technology. Importantly, it does so through a free and permissively licensed release, under Apache 2.0, with transparent dataset sourcing and reproducible training protocols.This shift aligns with broader themes in Meta’s 2025 strategy. The company has refocused its narrative around a “personal superintelligence” vision, investing heavily in infrastructure (including a September release of custom AI accelerators and Arm-based inference stacks) source while downplaying the metaverse in favor of foundational AI capabilities. The return to public training data in Europe after a regulatory pause also underscores its intention to compete globally, despite privacy scrutiny source.Omnilingual ASR, then, is more than a model release — it’s a calculated move to reassert control of the narrative: from the fragmented rollout of Llama 4 to a high-utility, research-grounded contribution that aligns with Meta’s long-term AI platform strategy.Community-Centered Dataset CollectionTo achieve this scale, Meta partnered with researchers and community organizations in Africa, Asia, and elsewhere to create the Omnilingual ASR Corpus, a 3,350-hour dataset across 348 low-resource languages. Contributors were compensated local speakers, and recordings were gathered in collaboration with groups like:African Next Voices: A Gates Foundation–supported consortium including Maseno University (Kenya), University of Pretoria, and Data Science NigeriaMozilla Foundation’s Common Voice, supported through the Open Multilingual Speech FundLanfrica / NaijaVoices, which created data for 11 African languages including Igala, Serer, and UrhoboThe data collection focused on natural, unscripted speech. Prompts were designed to be culturally relevant and open-ended, such as “Is it better to have a few close friends or many casual acquaintances? Why?” Transcriptions used established writing systems, with quality assurance built into every step.Performance and Hardware ConsiderationsThe largest model in the suite, the omniASR_LLM_7B, requires ~17GB of GPU memory for inference, making it suitable for deployment on high-end hardware. Smaller models (300M–1B) can run on lower-power devices and deliver real-time transcription speeds.Performance benchmarks show strong results even in low-resource scenarios:CER <10% in 95% of high-resource and mid-resource languagesCER <10% in 36% of low-resource languagesRobustness in noisy conditions and unseen domains, especially with fine-tuningThe zero-shot system, omniASR_LLM_7B_ZS, can transcribe new languages with minimal setup. Users provide a few sample audio–text pairs, and the model generates transcriptions for new utterances in the same language.Open Access and Developer ToolingAll models and the dataset are licensed under permissive terms:Apache 2.0 for models and codeCC-BY 4.0 for the Omnilingual ASR Corpus on HuggingFaceInstallation is supported via PyPI and uv:pip install omnilingual-asrMeta also provides:A HuggingFace dataset integrationPre-built inference pipelinesLanguage-code conditioning for improved accuracyDevelopers can view the full list of supported languages using the API:from omnilingual_asr.models.wav2vec2_llama.lang_ids import supported_langsprint(len(supported_langs)) print(supported_langs)Broader ImplicationsOmnilingual ASR reframes language coverage in ASR from a fixed list to an extensible framework. It enables:Community-driven inclusion of underrepresented languagesDigital access for oral and endangered languagesResearch on speech tech in linguistically diverse contextsCrucially, Meta emphasizes ethical considerations throughout—advocating for open-source participation and collaboration with native-speaking communities.“No model can ever anticipate and include all of the world’s languages in advance,” the Omnilingual ASR paper states, “but Omnilingual ASR makes it possible for communities to extend recognition with their own data.”Access the ToolsAll resources are now available at:Code + Models: github.com/facebookresearch/omnilingual-asrDataset: huggingface.co/datasets/facebook/omnilingual-asr-corpusBlogpost: ai.meta.com/blog/omnilingual-asrWhat This Means for EnterprisesFor enterprise developers, especially those operating in multilingual or international markets, Omnilingual ASR significantly lowers the barrier to deploying speech-to-text systems across a broader range of customers and geographies. Instead of relying on commercial ASR APIs that support only a narrow set of high-resource languages, teams can now integrate an open-source pipeline that covers over 1,600 languages out of the box—with the option to extend it to thousands more via zero-shot learning.This flexibility is especially valuable for enterprises working in sectors like voice-based customer support, transcription services, accessibility, education, or civic technology, where local language coverage can be a competitive or regulatory necessity. Because the models are released under the permissive Apache 2.0 license, businesses can fine-tune, deploy, or integrate them into proprietary systems without restrictive terms.It also represents a shift in the ASR landscape—from centralized, cloud-gated offerings to community-extendable infrastructure. By making multilingual speech recognition more accessible, customizable, and cost-effective, Omnilingual ASR opens the door to a new generation of enterprise speech applications built around linguistic inclusion rather than linguistic limitation.",
          "feed_position": 5,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/5WF8w75sB7wnAYEK5pYnPD/16059c6ac3f6b853b3b301bc66f950d3/cfr0z3n_graphic_novel_abstract_expressionist_outline_style_show_390da294-e50e-424d-ab36-20b02133d3d9.png?w=300&q=30"
        }
      ],
      "featured_image": "https://d29szjachogqwa.cloudfront.net/images/user-uploaded/Valve-Steam-Controller.jpg",
      "popularity_score": 2019.7913522222223
    },
    {
      "id": "cluster_21",
      "coverage": 2,
      "updated_at": "Wed, 12 Nov 2025 13:25:00 -0500",
      "title": "Valve plans to launch Steam Frame, a VR headset with a Snapdragon 8 Gen 3 chip for streaming games from a PC or playing Windows games locally, in 2026 (Jay Peters/The Verge)",
      "neutral_headline": "The Steam Frame is a surprising new twist on VR",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/251112/p33#a251112p33",
          "published_at": "Wed, 12 Nov 2025 13:25:00 -0500",
          "title": "Valve plans to launch Steam Frame, a VR headset with a Snapdragon 8 Gen 3 chip for streaming games from a PC or playing Windows games locally, in 2026 (Jay Peters/The Verge)",
          "standfirst": "Jay Peters / The Verge: Valve plans to launch Steam Frame, a VR headset with a Snapdragon 8 Gen 3 chip for streaming games from a PC or playing Windows games locally, in 2026 &mdash; Valve is about to launch a new virtual reality headset, and with it, a comprehensive new approach to what a VR device should be.",
          "content": "Jay Peters / The Verge: Valve plans to launch Steam Frame, a VR headset with a Snapdragon 8 Gen 3 chip for streaming games from a PC or playing Windows games locally, in 2026 &mdash; Valve is about to launch a new virtual reality headset, and with it, a comprehensive new approach to what a VR device should be.",
          "feed_position": 2,
          "image_url": "http://www.techmeme.com/251112/i33.jpg"
        },
        {
          "source": "The Verge",
          "url": "https://www.theverge.com/games/816118/valve-steam-frame-vr-headset-streaming-arm-steamos-hands-on",
          "published_at": "2025-11-12T13:00:00-05:00",
          "title": "The Steam Frame is a surprising new twist on VR",
          "standfirst": "Valve is about to launch a new virtual reality headset, and with it, a comprehensive new approach to what a VR device should be. Most VR headsets I've tried have ended up collecting dust after the novelty wore off, and I thought I had sworn off VR for good. But after trying Valve's new headset [&#8230;]",
          "content": "Here’s me wearing the Steam Frame, Valve’s new VR headset. Valve is about to launch a new virtual reality headset, and with it, a comprehensive new approach to what a VR device should be. Most VR headsets I've tried have ended up collecting dust after the novelty wore off, and I thought I had sworn off VR for good. But after trying Valve's new headset for myself at the company's headquarters, I was nearly ready to put down my credit card before I walked out the door. The new headset is called the Steam Frame, and it's trying to do several things at once. It's a standalone VR headset with a smartphone-caliber Arm chip inside that lets you play flat-screen Windows games locally off the onboard storag … Read the full story at The Verge.",
          "feed_position": 6
        }
      ],
      "featured_image": "http://www.techmeme.com/251112/i33.jpg",
      "popularity_score": 2019.1349633333334
    },
    {
      "id": "cluster_24",
      "coverage": 2,
      "updated_at": "Wed, 12 Nov 2025 13:20:02 -0500",
      "title": "Valve unveils a new Steam Machine, a cube-shaped gaming PC with SteamOS, a semi-custom AMD Zen 4 CPU and AMD RDNA 3 GPU, and 16GB of RAM, expected in early 2026 (Andrew E. Freedman/Tom's Hardware)",
      "neutral_headline": "Steam Deck minus the screen: Valve announces new Steam Machine, Controller hardware",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/251112/p32#a251112p32",
          "published_at": "Wed, 12 Nov 2025 13:20:02 -0500",
          "title": "Valve unveils a new Steam Machine, a cube-shaped gaming PC with SteamOS, a semi-custom AMD Zen 4 CPU and AMD RDNA 3 GPU, and 16GB of RAM, expected in early 2026 (Andrew E. Freedman/Tom's Hardware)",
          "standfirst": "Andrew E. Freedman / Tom's Hardware: Valve unveils a new Steam Machine, a cube-shaped gaming PC with SteamOS, a semi-custom AMD Zen 4 CPU and AMD RDNA 3 GPU, and 16GB of RAM, expected in early 2026 &mdash; Valve is promising 4K gaming at 60 FPS with FSR &mdash; Valve rattled the gaming industry in 2022 with the launch of the Steam Deck &hellip;",
          "content": "Andrew E. Freedman / Tom's Hardware: Valve unveils a new Steam Machine, a cube-shaped gaming PC with SteamOS, a semi-custom AMD Zen 4 CPU and AMD RDNA 3 GPU, and 16GB of RAM, expected in early 2026 &mdash; Valve is promising 4K gaming at 60 FPS with FSR &mdash; Valve rattled the gaming industry in 2022 with the launch of the Steam Deck &hellip;",
          "feed_position": 3,
          "image_url": "http://www.techmeme.com/251112/i32.jpg"
        },
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gaming/2025/11/steam-deck-minus-the-screen-valve-announces-new-steam-machine-controller-hardware/",
          "published_at": "Wed, 12 Nov 2025 18:00:37 +0000",
          "title": "Steam Deck minus the screen: Valve announces new Steam Machine, Controller hardware",
          "standfirst": "SteamOS-powered cube for your TV targets early 2026 launch, no pricing details.",
          "content": "Nearly four years after the Steam Deck changed the world of portable gaming, Valve is getting ready to release SteamOS-powered hardware designed for the living room TV, or even as a desktop PC gaming replacement. The simply named Steam Machine and Steam Controller, both planned to ship in early 2026, are “optimized for gaming on Steam and designed for players to get even more out of their Steam Library,” Valve said in a press release. A Steam Machine spec sheet shared by Valve lists a “semi-custom” six-core AMD Zen 4 CPU clocked at up to 4.8 Ghz alongside an AMD RDNA3 GPU with 28 compute units. The motherboard will include 16GB of DDR5 RAM and an additional 8GB of dedicated DDR6 VRAM for the GPU. The new hardware will come in two configurations with 512GB or 2TB of unspecified “SSD storage,” though Valve isn’t sharing pricing for either just yet. If you squint, you can make out a few ports on this unmarked black square. Credit: Valve A strip of LEDs adds a touch of color to the front face of the Steam Machine. I'm a fan of the big fan. Credit: Valve Those chips and numbers suggest the Steam Machine will have roughly the same horsepower as a mid-range desktop gaming PC from a few years back. But Valve says its “Machine”—which it ranks as “over 6x more powerful than the Steam Deck”—is powerful enough to support ray-tracing and/or 4K, 60 fps gaming using FSR upscaling.Read full article Comments",
          "feed_position": 3,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/SM_3Q-1152x648-1762899545.jpg"
        }
      ],
      "featured_image": "http://www.techmeme.com/251112/i32.jpg",
      "popularity_score": 2019.0521855555555
    },
    {
      "id": "cluster_35",
      "coverage": 2,
      "updated_at": "Wed, 12 Nov 2025 12:55:01 -0500",
      "title": "Apple launches Digital ID in Apple Wallet, letting iPhone and Apple Watch owners in the US carry a copy of their passport, which can be used at TSA checkpoints (Sarah Perez/TechCrunch)",
      "neutral_headline": "Apple launches Digital ID, a way to carry your passport on your...",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/251112/p30#a251112p30",
          "published_at": "Wed, 12 Nov 2025 12:55:01 -0500",
          "title": "Apple launches Digital ID in Apple Wallet, letting iPhone and Apple Watch owners in the US carry a copy of their passport, which can be used at TSA checkpoints (Sarah Perez/TechCrunch)",
          "standfirst": "Sarah Perez / TechCrunch: Apple launches Digital ID in Apple Wallet, letting iPhone and Apple Watch owners in the US carry a copy of their passport, which can be used at TSA checkpoints &mdash; iPhone and Apple Watch owners in the United States will now be able to carry a copy of their U.S. passport on their device &hellip;",
          "content": "Sarah Perez / TechCrunch: Apple launches Digital ID in Apple Wallet, letting iPhone and Apple Watch owners in the US carry a copy of their passport, which can be used at TSA checkpoints &mdash; iPhone and Apple Watch owners in the United States will now be able to carry a copy of their U.S. passport on their device &hellip;",
          "feed_position": 5,
          "image_url": "http://www.techmeme.com/251112/i30.jpg"
        },
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2025/11/12/apple-launches-digital-id-a-way-to-carry-your-passport-on-your-phone-for-use-at-tsa-checkpoints/",
          "published_at": "Wed, 12 Nov 2025 16:30:00 +0000",
          "title": "Apple launches Digital ID, a way to carry your passport on your phone for use at TSA checkpoints",
          "standfirst": "Apple's new Digital ID will let U.S. users carry their passport on their iPhone, and use it at TSA checkpoints.",
          "content": "Apple's new Digital ID will let U.S. users carry their passport on their iPhone, and use it at TSA checkpoints.",
          "feed_position": 7
        }
      ],
      "featured_image": "http://www.techmeme.com/251112/i30.jpg",
      "popularity_score": 2018.635241111111
    },
    {
      "id": "cluster_47",
      "coverage": 2,
      "updated_at": "Wed, 12 Nov 2025 12:20:07 -0500",
      "title": "WisdomAI, which offers AI-driven data analytics that can answer business questions, including from \"dirty\" data, raised a $50M Series A led by Kleiner Perkins (Julie Bort/TechCrunch)",
      "neutral_headline": "AI data startup WisdomAI has raised another $50M, led by Kleiner, Nvidia",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/251112/p27#a251112p27",
          "published_at": "Wed, 12 Nov 2025 12:20:07 -0500",
          "title": "WisdomAI, which offers AI-driven data analytics that can answer business questions, including from \"dirty\" data, raised a $50M Series A led by Kleiner Perkins (Julie Bort/TechCrunch)",
          "standfirst": "Julie Bort / TechCrunch: WisdomAI, which offers AI-driven data analytics that can answer business questions, including from &ldquo;dirty&rdquo; data, raised a $50M Series A led by Kleiner Perkins &mdash; WisdomAI, the new AI data analytics startup from Rubrik co-founder Soham Mazumdar, has landed a fresh $50 million Series &hellip;",
          "content": "Julie Bort / TechCrunch: WisdomAI, which offers AI-driven data analytics that can answer business questions, including from &ldquo;dirty&rdquo; data, raised a $50M Series A led by Kleiner Perkins &mdash; WisdomAI, the new AI data analytics startup from Rubrik co-founder Soham Mazumdar, has landed a fresh $50 million Series &hellip;",
          "feed_position": 8,
          "image_url": "http://www.techmeme.com/251112/i27.jpg"
        },
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2025/11/12/ai-data-startup-wisdomai-has-raised-another-50m-led-by-kleiner-nvidia/",
          "published_at": "Wed, 12 Nov 2025 16:00:00 +0000",
          "title": "AI data startup WisdomAI has raised another $50M, led by Kleiner, Nvidia",
          "standfirst": "WisdomAI is offering AI-driven data analytics that can answer business questions from structured, unstructured, and even “dirty” data, meaning data not cleaned of typos or errors.",
          "content": "WisdomAI is offering AI-driven data analytics that can answer business questions from structured, unstructured, and even “dirty” data, meaning data not cleaned of typos or errors.",
          "feed_position": 8
        }
      ],
      "featured_image": "http://www.techmeme.com/251112/i27.jpg",
      "popularity_score": 2018.0535744444444
    },
    {
      "id": "cluster_61",
      "coverage": 2,
      "updated_at": "Wed, 12 Nov 2025 11:25:01 -0500",
      "title": "Waymo becomes the first robotaxi provider to offer driverless rides on freeways, available 24/7 in San Francisco, Phoenix, and Los Angeles for users who opt in (Zoe Thomas/Bloomberg)",
      "neutral_headline": "Waymo robotaxis are now giving rides on freeways in LA, San Francisco, and Phoenix",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/251112/p26#a251112p26",
          "published_at": "Wed, 12 Nov 2025 11:25:01 -0500",
          "title": "Waymo becomes the first robotaxi provider to offer driverless rides on freeways, available 24/7 in San Francisco, Phoenix, and Los Angeles for users who opt in (Zoe Thomas/Bloomberg)",
          "standfirst": "Zoe Thomas / Bloomberg: Waymo becomes the first robotaxi provider to offer driverless rides on freeways, available 24/7 in San Francisco, Phoenix, and Los Angeles for users who opt in &mdash; Waymo will become the first robotaxi provider in the US to offer driverless rides on highways, a milestone that positions &hellip;",
          "content": "Zoe Thomas / Bloomberg: Waymo becomes the first robotaxi provider to offer driverless rides on freeways, available 24/7 in San Francisco, Phoenix, and Los Angeles for users who opt in &mdash; Waymo will become the first robotaxi provider in the US to offer driverless rides on highways, a milestone that positions &hellip;",
          "feed_position": 9,
          "image_url": "http://www.techmeme.com/251112/i26.jpg"
        },
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2025/11/12/waymo-robotaxis-are-now-giving-rides-on-freeways-in-these-3-cities/",
          "published_at": "Wed, 12 Nov 2025 16:00:00 +0000",
          "title": "Waymo robotaxis are now giving rides on freeways in LA, San Francisco, and Phoenix",
          "standfirst": "Waymo robotaxis will use freeways in Los Angeles, Phoenix, and San Francisco - an expansion that could reduce ride times by 50%.",
          "content": "Waymo robotaxis will use freeways in Los Angeles, Phoenix, and San Francisco - an expansion that could reduce ride times by 50%.",
          "feed_position": 9
        }
      ],
      "featured_image": "http://www.techmeme.com/251112/i26.jpg",
      "popularity_score": 2017.135241111111
    },
    {
      "id": "cluster_65",
      "coverage": 2,
      "updated_at": "Wed, 12 Nov 2025 15:52:47 +0000",
      "title": "Anthropic announces $50 billion data center plan",
      "neutral_headline": "Anthropic announces $50 billion data center plan",
      "items": [
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2025/11/12/anthropic-announces-50-billion-data-center-plan/",
          "published_at": "Wed, 12 Nov 2025 15:52:47 +0000",
          "title": "Anthropic announces $50 billion data center plan",
          "standfirst": "Anthropic has launched an ambitious new data center partnership with the U.K.-based Fluidstack, committing $50 billion to building facilities across the US.",
          "content": "Anthropic has launched an ambitious new data center partnership with the U.K.-based Fluidstack, committing $50 billion to building facilities across the US.",
          "feed_position": 10
        },
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/251112/p23#a251112p23",
          "published_at": "Wed, 12 Nov 2025 10:05:03 -0500",
          "title": "Anthropic plans to spend $50B on a US AI infrastructure buildout, starting with Texas and New York data centers in partnership with Fluidstack, opening in 2026 (MacKenzie Sigalos/CNBC)",
          "standfirst": "MacKenzie Sigalos / CNBC: Anthropic plans to spend $50B on a US AI infrastructure buildout, starting with Texas and New York data centers in partnership with Fluidstack, opening in 2026 &mdash; Anthropic announced plans Wednesday to spend $50 billion on U.S. artificial intelligence infrastructure buildout, starting with custom data centers in Texas and New York.",
          "content": "MacKenzie Sigalos / CNBC: Anthropic plans to spend $50B on a US AI infrastructure buildout, starting with Texas and New York data centers in partnership with Fluidstack, opening in 2026 &mdash; Anthropic announced plans Wednesday to spend $50 billion on U.S. artificial intelligence infrastructure buildout, starting with custom data centers in Texas and New York.",
          "feed_position": 12,
          "image_url": "http://www.techmeme.com/251112/i23.jpg"
        }
      ],
      "featured_image": "http://www.techmeme.com/251112/i23.jpg",
      "popularity_score": 2016.5980188888889
    },
    {
      "id": "cluster_77",
      "coverage": 2,
      "updated_at": "Wed, 12 Nov 2025 09:40:02 -0500",
      "title": "Fei-Fei Li's World Labs debuts Marble, its first world model, which lets users turn prompts, photos, and other media into editable 3D environments, after a beta (Rebecca Bellan/TechCrunch)",
      "neutral_headline": "Fei-Fei Li's World Labs debuts Marble, its first world model, which lets users turn prompts, photos, and other media...",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/251112/p21#a251112p21",
          "published_at": "Wed, 12 Nov 2025 09:40:02 -0500",
          "title": "Fei-Fei Li's World Labs debuts Marble, its first world model, which lets users turn prompts, photos, and other media into editable 3D environments, after a beta (Rebecca Bellan/TechCrunch)",
          "standfirst": "Rebecca Bellan / TechCrunch: Fei-Fei Li's World Labs debuts Marble, its first world model, which lets users turn prompts, photos, and other media into editable 3D environments, after a beta &mdash; World Labs, the startup founded by AI pioneer Fei-Fei Li, is launching its first commercial world model product.",
          "content": "Rebecca Bellan / TechCrunch: Fei-Fei Li's World Labs debuts Marble, its first world model, which lets users turn prompts, photos, and other media into editable 3D environments, after a beta &mdash; World Labs, the startup founded by AI pioneer Fei-Fei Li, is launching its first commercial world model product.",
          "feed_position": 14,
          "image_url": "http://www.techmeme.com/251112/i21.jpg"
        },
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2025/11/12/fei-fei-lis-world-labs-speeds-up-the-world-model-race-with-marble-its-first-commercial-product/",
          "published_at": "Wed, 12 Nov 2025 13:44:01 +0000",
          "title": "Fei-Fei Li&#8217;s World Labs speeds up the world model race with Marble, its first commercial product",
          "standfirst": "Marble is different from competitors like Odyssey, Decart, and Google's Genie because it creates persistent, downloadable 3D environments rather than generating worlds on-the-fly as you explore.",
          "content": "Marble is different from competitors like Odyssey, Decart, and Google's Genie because it creates persistent, downloadable 3D environments rather than generating worlds on-the-fly as you explore.",
          "feed_position": 13
        }
      ],
      "featured_image": "http://www.techmeme.com/251112/i21.jpg",
      "popularity_score": 2015.3855188888888
    },
    {
      "id": "cluster_15",
      "coverage": 1,
      "updated_at": "Wed, 12 Nov 2025 18:37:26 +0000",
      "title": "Nintendo drops official trailer for Super Mario Galaxy Movie",
      "neutral_headline": "Nintendo drops official trailer for Super Mario Galaxy Movie",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/culture/2025/11/nintendo-drops-official-trailer-for-super-mario-galaxy-movie/",
          "published_at": "Wed, 12 Nov 2025 18:37:26 +0000",
          "title": "Nintendo drops official trailer for Super Mario Galaxy Movie",
          "standfirst": "It's a sequel to 2023's Super Mario Bros. Movie, which racked up $1.36 billion at the box office.",
          "content": "The Super Mario Bros. Movie dominated the box office in 2023, racking up $1.36 billion and snagging several Oscar nominations for good measure. So naturally there’s a sequel, and Nintendo just dropped the official trailer for The Super Mario Galaxy Movie, due out next spring. (Spoilers for the 2023 film below.) The first attempt at a Super Mario movie adaptation in 1993 was notoriously a dismal failure, although it still has its ’90s-nostalgic fans. But 2023’s Super Mario Bros. Movie won over gaming fans who were skeptical about another adaption—including Ars Senior Gaming Editor Kyle Orland. “This film version captures all the fun and vibrancy of the Mario games, with enough references to familiar characters, items, and locations to make even a die-hard Mario fan’s head spin,” he wrote in his 2023 review, adding that, despite a few flaws, the film was “everything that a 10-year-old version of me could ever have dreamed a Mario movie could be.”Read full article Comments",
          "feed_position": 0,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/galaxy6-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/galaxy6-1152x648.jpg",
      "popularity_score": 367.34218555555555
    },
    {
      "id": "cluster_20",
      "coverage": 1,
      "updated_at": "Wed, 12 Nov 2025 18:27:27 +0000",
      "title": "OpenAI slams court order that lets NYT read 20 million complete user chats",
      "neutral_headline": "OpenAI slams court order that lets NYT read 20 million complete user chats",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/11/openai-fights-order-to-hand-over-20-million-private-chatgpt-conversations/",
          "published_at": "Wed, 12 Nov 2025 18:27:27 +0000",
          "title": "OpenAI slams court order that lets NYT read 20 million complete user chats",
          "standfirst": "OpenAI: NYT wants evidence of ChatGPT users trying to get around news paywall.",
          "content": "OpenAI wants a court to reverse a ruling forcing the ChatGPT maker to give 20 million user chats to The New York Times and other news plaintiffs that sued it over alleged copyright infringement. Although OpenAI previously offered 20 million user chats as a counter to the NYT’s demand for 120 million, the AI company says a court order requiring production of the chats is too broad. “The logs at issue here are complete conversations: each log in the 20 million sample represents a complete exchange of multiple prompt-output pairs between a user and ChatGPT,” OpenAI said today in a filing in US District Court for the Southern District of New York. “Disclosure of those logs is thus much more likely to expose private information [than individual prompt-output pairs], in the same way that eavesdropping on an entire conversation reveals more private information than a 5-second conversation fragment.” OpenAI’s filing said that “more than 99.99%” of the chats “have nothing to do with this case.” It asked the district court to “vacate the order and order News Plaintiffs to respond to OpenAI’s proposal for identifying relevant logs.” OpenAI could also seek review in a federal court of appeals.Read full article Comments",
          "feed_position": 1,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/chatgpt-app-icon-1152x648-1762971088.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/chatgpt-app-icon-1152x648-1762971088.jpg",
      "popularity_score": 357.17579666666666
    },
    {
      "id": "cluster_11",
      "coverage": 1,
      "updated_at": "Wed, 12 Nov 2025 18:59:38 +0000",
      "title": "Quantum computing tech keeps edging forward",
      "neutral_headline": "Quantum computing tech keeps edging forward",
      "items": [
        {
          "source": "Ars Technica Main",
          "url": "https://arstechnica.com/science/2025/11/quantum-roundup-lots-of-companies-announcing-new-tech/",
          "published_at": "Wed, 12 Nov 2025 18:59:38 +0000",
          "title": "Quantum computing tech keeps edging forward",
          "standfirst": "IBM follows through on its June promises, plus more trapped ion news.",
          "content": "The end of the year is usually a busy time in the quantum computing arena, as companies often try to announce that they’ve reached major milestones before the year wraps up. This year has been no exception. And while not all of these announcements involve interesting new architectures like the one we looked at recently, they’re a good way to mark progress in the field, and they often involve the sort of smaller, incremental steps needed to push the field forward. What follows is a quick look at a handful of announcements from the past few weeks that struck us as potentially interesting. IBM follows through IBM is one of the companies announcing a brand new architecture this year. That’s not at all a surprise, given that the company promised to do so back in June; this week sees the company confirming that it has built the two processors it said it would earlier in the year. These include one called Loon, which is focused on the architecture that IBM will use to host error-corrected logical qubits. Loon represents two major changes for the company: a shift to nearest-neighbor connections and the addition of long-distance connections.Read full article Comments",
          "feed_position": 0,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/IBM-Quantum-Loon-Wafer_2-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/IBM-Quantum-Loon-Wafer_2-1152x648.jpg",
      "popularity_score": 349.71218555555555
    },
    {
      "id": "cluster_29",
      "coverage": 1,
      "updated_at": "Wed, 12 Nov 2025 18:00:43 +0000",
      "title": "Valve rejoins the VR hardware wars with standalone Steam Frame",
      "neutral_headline": "Valve rejoins the VR hardware wars with standalone Steam Frame",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gaming/2025/11/valve-rejoins-the-vr-hardware-wars-with-standalone-steam-frame/",
          "published_at": "Wed, 12 Nov 2025 18:00:43 +0000",
          "title": "Valve rejoins the VR hardware wars with standalone Steam Frame",
          "standfirst": "SteamOS-powered headset sports semi-modular design, wireless \"low-latency\" PC streaming.",
          "content": "Six years ago, Valve made its second big virtual reality push, launching the Valve Index headset alongside VR blockbuster Half-Life Alyx. Since then, the company seems to have lost interest in virtual reality gaming, letting competitors like Meta release regular standalone hardware updates as the PC-tethered Index continued to age. Now, after years of rumors, Valve is finally ready to officially rejoin the VR hardware race. The Steam Frame, set to launch in early 2026, will run both VR and traditional Steam games locally through SteamOS or stream them wirelessly from a local PC. Powered by a Snapdragon 8 Gen 3 processor with 16 GB of RAM, the Steam Frame sports a 2160 x 2160 resolution display per eye at an “up to 110 degrees” field-of-view and up to 144 Hz. That’s all roughly in line with 2023’s Meta Quest 3, which runs on the slightly less performant Snapdragon XR2 Gen 2 processor. Valve’s new headset will be available in models sporting 256GB and 1TB or internal storage, both with the option for expansion via a microSD card slot. Pricing details have not yet been revealed publicly.Read full article Comments",
          "feed_position": 2,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/SF_headsetControllers_3Q-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/SF_headsetControllers_3Q-1152x648.jpg",
      "popularity_score": 331.7302411111111
    },
    {
      "id": "cluster_40",
      "coverage": 1,
      "updated_at": "Wed, 12 Nov 2025 17:35:36 +0000",
      "title": "Corals survived past climate changes by retreating to the deeps",
      "neutral_headline": "Corals survived past climate changes by retreating to the deeps",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2025/11/corals-survived-past-climate-changes-by-retreating-to-the-deeps/",
          "published_at": "Wed, 12 Nov 2025 17:35:36 +0000",
          "title": "Corals survived past climate changes by retreating to the deeps",
          "standfirst": "A recent die-off in Florida puts the spotlight on corals' survival strategies.",
          "content": "Scientists have found that the 2023 marine heat wave caused “functional extinction” of two Acropora reef-building coral species living in the Florida Reef, which stretches from the Dry Tortugas National Park to Miami. “At this point, we do not think there’s much of a chance for natural recovery—their numbers are so low that successful reproduction is incredibly unlikely,” said Ross Cunning, a coral biologist at the John G. Shedd Aquarium. This isn’t the first time corals have faced the borderline of extinction over the last 460 million years, and they have always managed to bounce back and recolonize habitats lost during severe climate changes. The problem is that we won’t live long enough to see them doing that again.Read full article Comments",
          "feed_position": 4,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-1708146111-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-1708146111-1152x648.jpg",
      "popularity_score": 319.31163
    },
    {
      "id": "cluster_49",
      "coverage": 1,
      "updated_at": "Wed, 12 Nov 2025 17:14:16 +0000",
      "title": "Meta’s star AI scientist Yann LeCun plans to leave for own startup",
      "neutral_headline": "Meta’s star AI scientist Yann LeCun plans to leave for own startup",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/ai/2025/11/metas-star-ai-scientist-yann-lecun-plans-to-leave-for-own-startup/",
          "published_at": "Wed, 12 Nov 2025 17:14:16 +0000",
          "title": "Meta’s star AI scientist Yann LeCun plans to leave for own startup",
          "standfirst": "AI pioneer reportedly frustrated with Meta's shift from research to rapid product releases.",
          "content": "Meta’s chief AI scientist and Turing Award winner Yann LeCun plans to leave the company to launch his own startup focused on a different type of AI called “world models,” the Financial Times reported. The French-US scientist has reportedly told associates he will depart in the coming months and is already in early talks to raise funds for the new venture. The departure comes as CEO Mark Zuckerberg radically overhauled Meta’s AI operations after deciding the company had fallen behind rivals such as OpenAI and Google. World models are hypothetical AI systems that some AI engineers expect to develop an internal “understanding” of the physical world by learning from video and spatial data rather than text alone. Unlike current large language models (such as the kind that power ChatGPT) that predict the next segment of data in a sequence, world models would ideally simulate cause-and-effect scenarios, understand physics, and enable machines to reason and plan more like animals do. LeCun has said this architecture could take a decade to fully develop. While some AI experts believe that Transformer-based AI models—such as large language models, video synthesis models, and interactive world synthesis models—have emergently modeled physics or absorbed the structural rules of the physical world from training data examples, the evidence so far generally points to sophisticated pattern-matching rather than a base understanding of how the physical world actually works.Read full article Comments",
          "feed_position": 5,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2024/11/GettyImages-1691376215-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2024/11/GettyImages-1691376215-1152x648.jpg",
      "popularity_score": 315.9560744444444
    },
    {
      "id": "cluster_57",
      "coverage": 1,
      "updated_at": "Wed, 12 Nov 2025 16:56:47 +0000",
      "title": "Good Luck, Have Fun, Don’t Die trailer ushers in AI apocalypse",
      "neutral_headline": "Good Luck, Have Fun, Don’t Die trailer ushers in AI apocalypse",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/culture/2025/11/sam-rockwells-a-time-traveler-in-good-luck-have-fun-dont-die-trailer/",
          "published_at": "Wed, 12 Nov 2025 16:56:47 +0000",
          "title": "Good Luck, Have Fun, Don’t Die trailer ushers in AI apocalypse",
          "standfirst": "\"I'm not gonna sugarcoat it. You're in for a really weird night.\"",
          "content": "Director Gore Verbinski has racked up an impressive filmography over the years, from The Ring and the first three installments of the Pirates of the Caribbean franchise to the 2011 Oscar-nominated animated western Rango. Granted, he’s had his share of failures (*cough* The Lone Ranger *cough*), but if this trailer is any indication, Verbinski has another winner on his hands with the absurdist sci-fi dark comedy Good Luck, Have Fun, Don’t Die. Sam Rockwell stars as the otherwise unnamed “Man from the Future,” who shows up at a Los Angeles diner looking like a homeless person but claiming to be a time traveler from an apocalyptic future. He’s there to recruit the locals into his war against a rogue AI, although the diner patrons are understandably dubious about his sanity. (“I come from a nightmare apocalypse,” he assures the crowd about his grubby appearance. “This is the height of f*@ing fashion!”) Somehow, he convinces a handful of Angelenos to join his crusade, and judging by the remaining footage, all kinds of chaos breaks out. In addition to the eminently watchable Rockwell, the cast includes Haley Lu Richardson as Ingrid, Michael Pena as Mark, Zazie Beetz as Janet, and Juno Temple as Susan. Dino Fetscher, Anna Acton, Asim Chaudhury, Daniel Barnett, and Domonique Maher also appear in as-yet-undisclosed roles. Matthew Robinson (The Invention of Lying, Love and Monsters) penned the script. This is Verbinski’s first indie film, and Tom Ortenberg, CEO of distributor Briarcliff Entertainment, praised it as “wildly original, endlessly entertaining, and unlike anything audiences have seen before.” Color us intrigued.Read full article Comments",
          "feed_position": 6,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/goodluck-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/goodluck-1152x648.jpg",
      "popularity_score": 305.66468555555554
    },
    {
      "id": "cluster_63",
      "coverage": 1,
      "updated_at": "Wed, 12 Nov 2025 16:00:39 +0000",
      "title": "Review: New Framework Laptop 16 takes a fresh stab at the upgradeable laptop GPU",
      "neutral_headline": "Review: New Framework Laptop 16 takes a fresh stab at the upgradeable laptop GPU",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2025/11/review-new-framework-laptop-16-takes-a-fresh-stab-at-the-upgradeable-laptop-gpu/",
          "published_at": "Wed, 12 Nov 2025 16:00:39 +0000",
          "title": "Review: New Framework Laptop 16 takes a fresh stab at the upgradeable laptop GPU",
          "standfirst": "New components make it more useful and powerful but no less odd.",
          "content": "The original Framework Laptop 16 was trying to crack a problem that laptop makers have wrestled with on and off for years: Can you deliver a reasonably powerful, portable workstation and gaming laptop that supports graphics card upgrades just like a desktop PC? Specs at a glance: Framework Laptop 16 (2025) OS Windows 11 25H2 CPU AMD Ryzen AI 7 350 (4 Zen 5 cores, 4 Zen 5c cores) RAM 32GB DDR5-5600 (upgradeable) GPU AMD Radeon 860M (integrated)/Nvidia GeForce RTX 5070 Mobile (dedicated) SSD 1TB Western Digital Black SN770 Battery 85 WHr Display 16-inch 2560×1600 165 Hz matte non-touchscreen Connectivity 6x recessed USB-C ports (2x USB 4, 4x USB 3.2) with customizable “Expansion Card” dongles Weight 4.63 pounds (2.1 kg) without GPU, 5.29 pounds (2.4 kg) with GPU Price as tested Roughly $2,649 for pre-built edition; $2,517 for DIY edition with no OS Even in these days of mostly incremental, not-too-exciting GPU upgrades, the graphics card in a gaming PC or graphics-centric workstation will still feel its age faster than your CPU will. And the chance to upgrade that one component for hundreds of dollars instead of spending thousands replacing the entire machine is an appealing proposition. Upgradeable, swappable GPUs would also make your laptop more flexible—you can pick and choose from various GPUs from multiple vendors based on what you want and need, whether that’s raw performance, power efficiency, Linux support, or CUDA capabilities.Read full article Comments",
          "feed_position": 7,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/IMG_1624-1152x648.jpeg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/IMG_1624-1152x648.jpeg",
      "popularity_score": 290.72913
    },
    {
      "id": "cluster_82",
      "coverage": 1,
      "updated_at": "Wed, 12 Nov 2025 13:22:11 +0000",
      "title": "Formula with “cleanest ingredients” recalled after 15 babies get botulism",
      "neutral_headline": "Formula with “cleanest ingredients” recalled after 15 babies get botulism",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/health/2025/11/formula-with-cleanest-ingredients-recalled-after-15-babies-get-botulism/",
          "published_at": "Wed, 12 Nov 2025 13:22:11 +0000",
          "title": "Formula with “cleanest ingredients” recalled after 15 babies get botulism",
          "standfirst": "Cases span 12 states. All affected babies have been hospitalized, but no deaths reported.",
          "content": "The maker of a specialty baby formula that touted having the “cleanest ingredients” and a “Purity Award” is recalling all of its products and lots amid an ongoing, multi-state outbreak of infant botulism. The outbreak was initially announced over the weekend by California and federal health officials. At that time, 13 cases of infant botulism had been flagged across 10 states. But on Tuesday, the outbreak expanded to 15 cases in 12 states. All 15 infants have been hospitalized, but no deaths have been reported. States reporting infant botulism linked to ByHeart formula. Credit: FDA The California Department of Public Health (CDPH) was the first to flag the outbreak. The department is the world’s sole source of the infant botulism treatment called BabyBIG, which is made of human-derived anti-botulism antibodies and is effective at easing symptoms and shortening recovery times. California health officials noted an unusual uptick in case reports and found they were linked to a specific formula: ByHeart Whole Nutrition Infant Formula. The department then did its own testing of some leftover formula, which was positive for the bacterium that causes botulism, Clostridium botulinum.Read full article Comments",
          "feed_position": 8,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/Screen-Shot-2025-11-11-at-7.10.57-PM-1084x648.png"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/Screen-Shot-2025-11-11-at-7.10.57-PM-1084x648.png",
      "popularity_score": 267.0880188888889
    },
    {
      "id": "cluster_94",
      "coverage": 1,
      "updated_at": "Wed, 12 Nov 2025 10:00:37 +0000",
      "title": "Google vows to stop scam E-Z Pass and USPS texts plaguing Americans",
      "neutral_headline": "Google vows to stop scam E-Z Pass and USPS texts plaguing Americans",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/11/google-vows-to-stop-scam-e-z-pass-and-usps-texts-plaguing-americans/",
          "published_at": "Wed, 12 Nov 2025 10:00:37 +0000",
          "title": "Google vows to stop scam E-Z Pass and USPS texts plaguing Americans",
          "standfirst": "\"Phishing for dummies\" kits make it easier to scam millions, Google alleged.",
          "content": "Google is suing to stop phishing attacks that target millions globally, including campaigns that fake toll notices, offer bogus e-commerce deals, and impersonate financial institutions. In a complaint filed Wednesday, the tech giant accused “a cybercriminal group in China” of selling “phishing for dummies” kits. The kits help unsavvy fraudsters easily “execute a large-scale phishing campaign,” tricking hordes of unsuspecting people into “disclosing sensitive information like passwords, credit card numbers, or banking information, often by impersonating well-known brands, government agencies, or even people the victim knows.” These branded “Lighthouse” kits offer two versions of software, depending on whether bad actors want to launch SMS and e-commerce scams. “Members may subscribe to weekly, monthly, seasonal, annual, or permanent licenses,” Google alleged. Kits include “hundreds of templates for fake websites, domain set-up tools for those fake websites, and other features designed to dupe victims into believing they are entering sensitive information on a legitimate website.”Read full article Comments",
          "feed_position": 9,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-950213928-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-950213928-1152x648.jpg",
      "popularity_score": 265.72857444444446
    },
    {
      "id": "cluster_124",
      "coverage": 1,
      "updated_at": "Tue, 11 Nov 2025 17:00:11 +0000",
      "title": "Google announces even more AI in Photos app, powered by Nano Banana",
      "neutral_headline": "Google announces even more AI in Photos app, powered by Nano Banana",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/google/2025/11/googles-nano-banana-ai-image-editing-is-finally-coming-to-google-photos/",
          "published_at": "Tue, 11 Nov 2025 17:00:11 +0000",
          "title": "Google announces even more AI in Photos app, powered by Nano Banana",
          "standfirst": "Google's Nano Banana is powering a raft of new features in the app.",
          "content": "We’re running out of ways to tell you that Google is releasing more generative AI features, but that’s what’s happening in Google Photos today. The Big G is finally making good on its promise to add its market-leading Nano Banana image-editing model to the app. The model powers a couple of features, and it’s not just for Google’s Android platform. Nano Banana edits are also coming to the iOS version of the app. Nano Banana started making waves when it appeared earlier this year as an unbranded demo. You simply feed the model an image and tell it what edits you want to see. Google said Nano Banana was destined for the Photos app back in October, but it’s only now beginning the rollout. The Photos app already had conversational editing in the “Help Me Edit” feature, but it was running an older non-fruit model that produced inferior results. Nano Banana editing will produce AI slop, yes, but it’s better slop. Nano Banana in Help me edit Nano Banana in Help me edit Google says the updated Help Me Edit feature has access to your private face groups, so you can use names in your instructions. For example, you could type “Remove Riley’s sunglasses,” and Nano Banana will identify Riley in the photo (assuming you have a person of that name saved) and make the edit without further instructions. You can also ask for more fantastical edits in Help Me Edit, changing the style of the image from top to bottom.Read full article Comments",
          "feed_position": 15,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/GP_Nov-AI-Feature_Blog-Post-Hero-1152x648.png"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/GP_Nov-AI-Feature_Blog-Post-Hero-1152x648.png",
      "popularity_score": 166
    },
    {
      "id": "cluster_110",
      "coverage": 1,
      "updated_at": "Tue, 11 Nov 2025 21:34:10 +0000",
      "title": "Google says new cloud-based “Private AI Compute” is just as secure as local processing",
      "neutral_headline": "Google says new cloud-based “Private AI Compute” is just...",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/google/2025/11/google-says-new-cloud-based-private-ai-compute-is-just-as-secure-as-local-processing/",
          "published_at": "Tue, 11 Nov 2025 21:34:10 +0000",
          "title": "Google says new cloud-based “Private AI Compute” is just as secure as local processing",
          "standfirst": "New system allows devices to connect directly to secure space in Google's AI servers.",
          "content": "Google’s current mission is to weave generative AI into as many products as it can, getting everyone accustomed to, and maybe even dependent on, working with confabulatory robots. That means it needs to feed the bots a lot of your data, and that’s getting easier with the company’s new Private AI Compute. Google claims its new secure cloud environment will power better AI experiences without sacrificing your privacy. The pitch sounds a lot like Apple’s Private Cloud Compute. Google’s Private AI Compute runs on “one seamless Google stack” powered by the company’s custom Tensor Processing Units (TPUs). These chips have integrated secure elements, and the new system allows devices to connect directly to the protected space via an encrypted link. Google’s TPUs rely on an AMD-based Trusted Execution Environment (TEE) that encrypts and isolates memory from the host. Theoretically, that means no one else—not even Google itself—can access your data. Google says independent analysis by NCC Group shows that Private AI Compute meets its strict privacy guidelines.Read full article Comments",
          "feed_position": 11,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/Google_Private_Inference-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/Google_Private_Inference-1152x648.jpg",
      "popularity_score": 160
    },
    {
      "id": "cluster_128",
      "coverage": 1,
      "updated_at": "Tue, 11 Nov 2025 15:54:11 +0000",
      "title": "You won’t believe the excuses lawyers have after getting busted for using AI",
      "neutral_headline": "You won’t believe the excuses lawyers have after getting busted for using AI",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/11/lawyers-keep-giving-weak-sauce-excuses-for-fake-ai-citations-in-court-docs/",
          "published_at": "Tue, 11 Nov 2025 15:54:11 +0000",
          "title": "You won’t believe the excuses lawyers have after getting busted for using AI",
          "standfirst": "I got hacked; I lost my login; it was a rough draft; toggling windows is hard.",
          "content": "Amid what one judge called an “epidemic” of fake AI-generated case citations bogging down courts, some common excuses are emerging from lawyers hoping to dodge the most severe sanctions for filings deemed misleading. Using a database compiled by French lawyer and AI researcher Damien Charlotin, Ars reviewed 23 cases where lawyers were sanctioned for AI hallucinations. In many, judges noted that the simplest path to avoid or diminish sanctions was to admit that AI was used as soon as it’s detected, act humble, self-report the error to relevant legal associations, and voluntarily take classes on AI and law. But not every lawyer takes the path of least resistance, Ars’ review found, with many instead offering excuses that no judge found credible. Some even lie about their AI use, judges concluded. Since 2023—when fake AI citations started being publicized—the most popular excuse has been that the lawyer didn’t know AI was used to draft a filing.Read full article Comments",
          "feed_position": 16,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/ai-shrugging-lawyer-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/ai-shrugging-lawyer-1152x648.jpg",
      "popularity_score": 154
    },
    {
      "id": "cluster_138",
      "coverage": 1,
      "updated_at": "Tue, 11 Nov 2025 12:30:51 +0000",
      "title": "ClickFix may be the biggest security threat your family has never heard of",
      "neutral_headline": "ClickFix may be the biggest security threat your family has never heard of",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/security/2025/11/clickfix-may-be-the-biggest-security-threat-your-family-has-never-heard-of/",
          "published_at": "Tue, 11 Nov 2025 12:30:51 +0000",
          "title": "ClickFix may be the biggest security threat your family has never heard of",
          "standfirst": "Relatively new technique can bypass many endpoint protections.",
          "content": "Over the past year, scammers have ramped up a new way to infect the computers of unsuspecting people. The increasingly common method, which many potential targets have yet to learn of, is quick, bypasses most endpoint protections, and works against both macOS and Windows users. ClickFix often starts with an email sent from a hotel that the target has a pending registration with and references the correct registration information. In other cases, ClickFix attacks begin with a WhatsApp message. In still other cases, the user receives the URL at the top of Google results for a search query. Once the mark accesses the malicious site referenced, it presents a CAPTCHA challenge or other pretext requiring user confirmation. The user receives an instruction to copy a string of text, open a terminal window, paste it in, and press Enter. One line is all it takes Once entered, the string of text causes the PC or Mac to surreptitiously visit a scammer-controlled server and download malware. Then, the machine automatically installs it—all with no indication to the target. With that, users are infected, usually with credential-stealing malware. Security firms say ClickFix campaigns have run rampant. The lack of awareness of the technique, combined with the links also coming from known addresses or in search results, and the ability to bypass some endpoint protections are all factors driving the growth.Read full article Comments",
          "feed_position": 18,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2020/10/malware-1000x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2020/10/malware-1000x648.jpg",
      "popularity_score": 149
    },
    {
      "id": "cluster_111",
      "coverage": 1,
      "updated_at": "Tue, 11 Nov 2025 21:20:32 +0000",
      "title": "Ryanair tries forcing app downloads by eliminating paper boarding passes",
      "neutral_headline": "Ryanair tries forcing app downloads by eliminating paper boarding passes",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2025/11/ryanair-tries-forcing-app-downloads-by-eliminating-paper-boarding-passes/",
          "published_at": "Tue, 11 Nov 2025 21:20:32 +0000",
          "title": "Ryanair tries forcing app downloads by eliminating paper boarding passes",
          "standfirst": "Ryanair CEO admits \"there’ll be some teething problems.\"",
          "content": "Ryanair is trying to force users to download its mobile app by eliminating paper boarding passes, starting on November 12. As announced in February and subsequently delayed from earlier start dates, Europe’s biggest airline is moving to digital-only boarding passes, meaning customers will no longer be able to print physical ones. In order to access their boarding passes, Ryanair flyers will have to download Ryanair’s app. “Almost 100 percent of passengers have smartphones, and we want to move everybody onto that smartphone technology,” Ryanair CEO Michael O’Leary said recently on The Independent’s daily travel podcast.Read full article Comments",
          "feed_position": 12,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-1200216952-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-1200216952-1152x648.jpg",
      "popularity_score": 148
    },
    {
      "id": "cluster_117",
      "coverage": 1,
      "updated_at": "Tue, 11 Nov 2025 19:21:30 +0000",
      "title": "Reddit mod jailed for sharing movie sex scenes in rare “moral rights” verdict",
      "neutral_headline": "Reddit mod jailed for sharing movie sex scenes in rare “moral rights” verdict",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/11/reddit-mod-jailed-for-sharing-movie-sex-scenes-in-rare-moral-rights-verdict/",
          "published_at": "Tue, 11 Nov 2025 19:21:30 +0000",
          "title": "Reddit mod jailed for sharing movie sex scenes in rare “moral rights” verdict",
          "standfirst": "Redditor confessed to violating actresses' \"moral rights\" in landmark ruling.",
          "content": "A Reddit moderator known as “KlammereFyr” was recently convicted by a Danish court after clipping and posting hundreds of nude scenes that actresses filmed for movies and TV shows but apparently never expected to be shared out of context. As TorrentFreak reported, dozens of actresses had complained about the mod’s sub-reddit, “SeDetForPlottet” (WatchItForthePlot), with some feeling “molested or abused.” Demanding Danish police put an end to the forum, the Rights Alliance—representing the Danish Actors’ Association, two broadcasters, and other rightsholders—pushed for a criminal probe.Read full article Comments",
          "feed_position": 13,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/reddit-s3xy-time-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/reddit-s3xy-time-1152x648.jpg",
      "popularity_score": 148
    },
    {
      "id": "cluster_107",
      "coverage": 1,
      "updated_at": "Tue, 11 Nov 2025 23:26:04 +0000",
      "title": "Original Mac calculator design came from letting Steve Jobs play with menus for 10 minutes",
      "neutral_headline": "Original Mac calculator design came from letting Steve Jobs play...",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2025/11/original-mac-calculators-design-came-from-letting-steve-jobs-play-with-sliders-for-ten-minutes/",
          "published_at": "Tue, 11 Nov 2025 23:26:04 +0000",
          "title": "Original Mac calculator design came from letting Steve Jobs play with menus for 10 minutes",
          "standfirst": "In 1982, a young Mac developer turned Jobs into a UI designer—and accidentally invented a new technique.",
          "content": "In February 1982, Apple employee #8 Chris Espinosa faced a problem that would feel familiar to anyone who has ever had a micromanaging boss: Steve Jobs wouldn’t stop critiquing his calculator design for the Mac. After days of revision cycles, the 21-year-old programmer found an elegant solution: He built what he called the “Steve Jobs Roll Your Own Calculator Construction Set” and let Jobs design it himself. This delightful true story comes from Andy Hertzfeld’s Folklore.org, a legendary tech history site that chronicles the development of the original Macintosh, which was released in January 1984. I ran across the story again recently and thought it was worth sharing as a fun anecdote in an age where influential software designs often come by committee. Design by menu Chris Espinosa started working for Apple at age 14 in 1976 as the company’s youngest employee. By 1981, while studying at UC Berkeley, Jobs convinced Espinosa to drop out and work on the Mac team full time.Read full article Comments",
          "feed_position": 10,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/calc_hero_1-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/calc_hero_1-1152x648.jpg",
      "popularity_score": 139.1527411111111
    },
    {
      "id": "cluster_122",
      "coverage": 1,
      "updated_at": "Tue, 11 Nov 2025 17:18:58 +0000",
      "title": "US states could lose $21 billion of broadband grants after Trump overhaul",
      "neutral_headline": "US states could lose $21 billion of broadband grants after Trump overhaul",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/11/us-states-could-lose-21-billion-of-broadband-grants-after-trump-overhaul/",
          "published_at": "Tue, 11 Nov 2025 17:18:58 +0000",
          "title": "US states could lose $21 billion of broadband grants after Trump overhaul",
          "standfirst": "Ernst bill would send broadband grant money to Treasury for deficit reduction.",
          "content": "A Senate Republican has drafted legislation that would effectively cut a $42 billion broadband deployment program in half. The bill would complement the Trump administration overhaul of the $42.45 billion Broadband Equity, Access, and Deployment (BEAD) program. The administration required states to rewrite their grant plans, reducing the overall projected spending and diverting some of the money from fiber projects to satellite. The result is that over $21 billion is projected to be left over after money is allocated to projects that expand broadband access. Current US law allows nondeployment funds to be used for other broadband-related purposes, like providing Wi-Fi and Internet-capable devices to US residents. But a draft bill by Sen. Joni Ernst (R-Iowa) would change the law to redirect all the remaining money to the US Treasury for deficit reduction.Read full article Comments",
          "feed_position": 14,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/shredded-money-1152x648-1762880813.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/shredded-money-1152x648-1762880813.jpg",
      "popularity_score": 133
    },
    {
      "id": "cluster_129",
      "coverage": 1,
      "updated_at": "Tue, 11 Nov 2025 15:15:01 +0000",
      "title": "Pirelli’s Cyber Tire might become highway agencies’ newest assistant",
      "neutral_headline": "Pirelli’s Cyber Tire might become highway agencies’ newest assistant",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/cars/2025/11/tires-that-talk-to-your-town-about-the-state-of-roads-are-on-the-way/",
          "published_at": "Tue, 11 Nov 2025 15:15:01 +0000",
          "title": "Pirelli’s Cyber Tire might become highway agencies’ newest assistant",
          "standfirst": "Pirelli's Cyber Tire can work for the greater good as well as the driver.",
          "content": "Pirelli’s sensor-embedded Cyber Tire is starting to find a whole new niche helping traffic agencies. When we first learned of the smart tire, it was making its debut fitted to McLaren’s then-new plug-in hybrid supercar. As an alternative to a tire pressure monitoring system fitted to the car’s wheels, the Cyber Tire wirelessly reports its temperature and pressure to its car via Bluetooth Low Energy, along with some specific information about the tire itself. Since then, Pirelli has continued to develop the technology. When it created Cyber Tires for the Pagani Utopia, it allowed a car to tailor its antilock braking and electronic stability control to the specific rubber fitted to the wheels. Right now, a car’s ABS or ESC will be tuned regardless of the tires it’s fitted to. But a high-performance summer tire acts quite differently from a winter tire, not just because of the composition of the rubber but also due to the tread pattern, depth, and stiffness, not to mention factors like sidewall stiffness. And the Utopia can take advantage of that fact.Read full article Comments",
          "feed_position": 17,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-875478034-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-875478034-1152x648.jpg",
      "popularity_score": 133
    },
    {
      "id": "cluster_143",
      "coverage": 1,
      "updated_at": "Tue, 11 Nov 2025 00:39:13 +0000",
      "title": "Neutron rocket’s debut slips into mid-2026 as company seeks success from the start",
      "neutral_headline": "Neutron rocket’s debut slips into mid-2026 as company seeks success from the start",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2025/11/neutron-rockets-debut-slips-into-mid-2026-as-company-seeks-success-from-the-start/",
          "published_at": "Tue, 11 Nov 2025 00:39:13 +0000",
          "title": "Neutron rocket’s debut slips into mid-2026 as company seeks success from the start",
          "standfirst": "\"Those who have failed to deliver are numerous.\"",
          "content": "During an earnings call on Monday, Rocket Lab chief executive Peter Beck announced that the company’s medium-lift launch vehicle, Neutron, would not launch this year. For anyone with the slightest understanding of the challenges involved in bringing a new rocket to the launch pad, as well as a calendar, the delay does not come as a surprise. Although Rocket Lab had been holding onto the possibility of launching Neutron this year publicly, it has been clear for months that a slip into 2026 was inevitable. According to Beck, speaking during a third-quarter 2025 earnings call, the new timeline has the company bringing Neutron to Launch Complex 2 at Wallops Flight Facility in Virginia during the first quarter of next year. The first launch is scheduled to occur “thereafter,” according to the company’s plans.Read full article Comments",
          "feed_position": 19,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/Neutron-Stack-Deploy-Artwork_med__ScaleHeightWzg1MF0-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/Neutron-Stack-Deploy-Artwork_med__ScaleHeightWzg1MF0-1152x648.jpg",
      "popularity_score": 133
    }
  ]
}