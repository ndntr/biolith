{
  "updated_at": "2025-11-06T19:16:04.941Z",
  "clusters": [
    {
      "id": "cluster_11",
      "coverage": 2,
      "updated_at": "Thu, 06 Nov 2025 18:27:00 GMT",
      "title": "Moonshot's Kimi K2 Thinking emerges as leading open source AI, outperforming GPT-5, Claude Sonnet 4.5 on key benchmarks",
      "neutral_headline": "Moonshot's Kimi K2 Thinking emerges as leading open source AI, outperforming GPT-5, Claude Sonnet 4.5 on key benchmarks",
      "items": [
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/moonshots-kimi-k2-thinking-emerges-as-leading-open-source-ai-outperforming",
          "published_at": "Thu, 06 Nov 2025 18:27:00 GMT",
          "title": "Moonshot's Kimi K2 Thinking emerges as leading open source AI, outperforming GPT-5, Claude Sonnet 4.5 on key benchmarks",
          "standfirst": "Even as concern and skepticism grows over U.S. AI startup OpenAI&#x27;s buildout strategy and high spending commitments, Chinese open source AI providers are escalating their competition and one has even caught up to OpenAI&#x27;s flagship, paid proprietary model GPT-5 in key third-party performance benchmarks with a new, free model. The Chinese AI startup Moonshot AI’s new Kimi K2 Thinking model, released today, has vaulted past both proprietary and open-weight competitors to claim the top position in reasoning, coding, and agentic-tool benchmarks. Despite being fully open-source, the model now outperforms OpenAI’s GPT-5, Anthropic’s Claude Sonnet 4.5 (Thinking mode), and xAI&#x27;s Grok-4 on several standard evaluations — an inflection point for the competitiveness of open AI systems.Developers can access the model via platform.moonshot.ai and kimi.com; weights and code are hosted on Hugging Face. The open release includes APIs for chat, reasoning, and multi-tool workflows.Users can try out Kimi K2 Thinking directly through its own ChatGPT-like website competitor and on a Hugging Face space as well. Modified Standard Open Source LicenseMoonshot AI has formally released Kimi K2 Thinking under a Modified MIT License on Hugging Face.The license grants full commercial and derivative rights — meaning individual researchers and developers working on behalf of enterprise clients can access it freely and use it in commercial applications — but adds one restriction:\"If the software or any derivative product serves over 100 million monthly active users or generates over $20 million USD per month in revenue, the deployer must prominently display &#x27;Kimi K2&#x27; on the product’s user interface.\"For most research and enterprise applications, this clause functions as a light-touch attribution requirement while preserving the freedoms of standard MIT licensing. It makes K2 Thinking one of the most permissively licensed frontier-class models currently available.A New Benchmark LeaderKimi K2 Thinking is a Mixture-of-Experts (MoE) model built around one trillion parameters, of which 32 billion activate per inference. It combines long-horizon reasoning with structured tool use, executing up to 200–300 sequential tool calls without human intervention.According to Moonshot’s published test results, K2 Thinking achieved:44.9 % on Humanity’s Last Exam (HLE), a state-of-the-art score;60.2 % on BrowseComp, an agentic web-search and reasoning test;71.3 % on SWE-Bench Verified and 83.1 % on LiveCodeBench v6, key coding evaluations;56.3 % on Seal-0, a benchmark for real-world information retrieval.Across these tasks, K2 Thinking consistently outperforms GPT-5’s corresponding scores and surpasses the previous open-weight leader MiniMax-M2—released just weeks earlier by Chinese rival MiniMax AI.Open Model Outperforms Proprietary SystemsGPT-5 and Claude Sonnet 4.5 Thinking remain the leading proprietary “thinking” models. Yet in the same benchmark suite, K2 Thinking’s agentic reasoning scores exceed both: for instance, on BrowseComp the open model’s 60.2 % decisively leads GPT-5’s 54.9 % and Claude 4.5’s 24.1 %.K2 Thinking also edges GPT-5 in GPQA Diamond (85.7 % vs 84.5 %) and matches it on mathematical reasoning tasks such as AIME 2025 and HMMT 2025. Only in certain heavy-mode configurations—where GPT-5 aggregates multiple trajectories—does the proprietary model regain parity.That Moonshot’s fully open-weight release can meet or exceed GPT-5’s scores marks a turning point. The gap between closed frontier systems and publicly available models has effectively collapsed for high-end reasoning and coding.Surpassing MiniMax-M2: The Previous Open-Source BenchmarkWhen VentureBeat profiled MiniMax-M2 just a week and a half ago, it was hailed as the “new king of open-source LLMs,” achieving top scores among open-weight systems:τ²-Bench 77.2BrowseComp 44.0FinSearchComp-global 65.5SWE-Bench Verified 69.4Those results placed MiniMax-M2 near GPT-5-level capability in agentic tool use. Yet Kimi K2 Thinking now eclipses them by wide margins.Its BrowseComp result of 60.2 % exceeds M2’s 44.0 %, and its SWE-Bench Verified 71.3 % edges out M2’s 69.4 %. Even on financial-reasoning tasks such as FinSearchComp-T3 (47.4 %), K2 Thinking performs comparably while maintaining superior general-purpose reasoning.Technically, both models adopt sparse Mixture-of-Experts architectures for compute efficiency, but Moonshot’s network activates more experts and deploys advanced quantization-aware training (INT4 QAT). This design doubles inference speed relative to standard precision without degrading accuracy—critical for long “thinking-token” sessions reaching 256 k context windows.Agentic Reasoning and Tool UseK2 Thinking’s defining capability lies in its explicit reasoning trace. The model outputs an auxiliary field, reasoning_content, revealing intermediate logic before each final response. This transparency preserves coherence across long multi-turn tasks and multi-step tool calls.A reference implementation published by Moonshot demonstrates how the model autonomously conducts a “daily news report” workflow: invoking date and web-search tools, analyzing retrieved content, and composing structured output—all while maintaining internal reasoning state.This end-to-end autonomy enables the model to plan, search, execute, and synthesize evidence across hundreds of steps, mirroring the emerging class of “agentic AI” systems that operate with minimal supervision.Efficiency and AccessDespite its trillion-parameter scale, K2 Thinking’s runtime cost remains modest. Moonshot lists usage at:$0.15 / 1 M tokens (cache hit)$0.60 / 1 M tokens (cache miss)$2.50 / 1 M tokens outputThese rates are competitive even against MiniMax-M2’s $0.30 input / $1.20 output pricing—and an order of magnitude below GPT-5 ($1.25 input / $10 output).Comparative Context: Open-Weight AccelerationThe rapid succession of M2 and K2 Thinking illustrates how quickly open-source research is catching frontier systems. MiniMax-M2 demonstrated that open models could approach GPT-5-class agentic capability at a fraction of the compute cost. Moonshot has now advanced that frontier further, pushing open weights beyond parity into outright leadership.Both models rely on sparse activation for efficiency, but K2 Thinking’s higher activation count (32 B vs 10 B active parameters) yields stronger reasoning fidelity across domains. Its test-time scaling—expanding “thinking tokens” and tool-calling turns—provides measurable performance gains without retraining, a feature not yet observed in MiniMax-M2.Technical OutlookMoonshot reports that K2 Thinking supports native INT4 inference and 256 k-token contexts with minimal performance degradation. Its architecture integrates quantization, parallel trajectory aggregation (“heavy mode”), and Mixture-of-Experts routing tuned for reasoning tasks.In practice, these optimizations allow K2 Thinking to sustain complex planning loops—code compile–test–fix, search–analyze–summarize—over hundreds of tool calls. This capability underpins its superior results on BrowseComp and SWE-Bench, where reasoning continuity is decisive.Enormous Implications for the AI EcosystemThe convergence of open and closed models at the high end signals a structural shift in the AI landscape. Enterprises that once relied exclusively on proprietary APIs can now deploy open alternatives matching GPT-5-level reasoning while retaining full control of weights, data, and compliance.Moonshot’s open publication strategy follows the precedent set by DeepSeek R1, Qwen3, GLM-4.6 and MiniMax-M2 but extends it to full agentic reasoning. For academic and enterprise developers, K2 Thinking provides both transparency and interoperability—the ability to inspect reasoning traces and fine-tune performance for domain-specific agents.The arrival of K2 Thinking signals that Moonshot — a young startup founded in 2023 with investment from some of China&#x27;s biggest apps and tech companies — is here to play in an intensifying competition, and comes amid growing scrutiny of the financial sustainability of AI’s largest players. Just a day ago, OpenAI CFO Sarah Friar sparked controversy after suggesting at WSJ Tech Live event that the U.S. government might eventually need to provide a “backstop” for the company’s more than $1.4 trillion in compute and data-center commitments — a comment widely interpreted as a call for taxpayer-backed loan guarantees.Although Friar later clarified that OpenAI was not seeking direct federal support, the episode reignited debate about the scale and concentration of AI capital spending. With OpenAI, Microsoft, Meta, and Google all racing to secure long-term chip supply, critics warn of an unsustainable investment bubble and “AI arms race” driven more by strategic fear than commercial returns — one that could \"blow up\" and take down the entire global economy with it if there is hesitation or market uncertainty, as so many trades and valuations have now been made in anticipation of continued hefty AI investment and massive returns. Against that backdrop, Moonshot AI’s and MiniMax’s open-weight releases put more pressure on U.S. proprietary AI firms and their backers to justify the size of the investments and paths to profitability. If an enterprise customer can just as easily get comparable or better performance from a free, open source Chinese AI model than they do with paid, proprietary AI solutions like OpenAI&#x27;s GPT-5, Anthropic&#x27;s Claude Sonnet 4.5, or Google&#x27;s Gemini 2.5 Pro — why would they continue paying to access the proprietary models? Already, Silicon Valley stalwarts like Airbnb have raised eyebrows for admitting to heavily using Chinese open source alternatives like Alibaba&#x27;s Qwen over OpenAI&#x27;s proprietary offerings. For investors and enterprises, these developments suggest that high-end AI capability is no longer synonymous with high-end capital expenditure. The most advanced reasoning systems may now come not from companies building gigascale data centers, but from research groups optimizing architectures and quantization for efficiency.In that sense, K2 Thinking’s benchmark dominance is not just a technical milestone—it’s a strategic one, arriving at a moment when the AI market’s biggest question has shifted from how powerful models can become to who can afford to sustain them.What It Means for Enterprises Going ForwardWithin weeks of MiniMax-M2’s ascent, Kimi K2 Thinking has overtaken it—along with GPT-5 and Claude 4.5—across nearly every reasoning and agentic benchmark. The model demonstrates that open-weight systems can now meet or surpass proprietary frontier models in both capability and efficiency.For the AI research community, K2 Thinking represents more than another open model: it is evidence that the frontier has become collaborative. The best-performing reasoning model available today is not a closed commercial product but an open-source system accessible to anyone.",
          "content": "Even as concern and skepticism grows over U.S. AI startup OpenAI&#x27;s buildout strategy and high spending commitments, Chinese open source AI providers are escalating their competition and one has even caught up to OpenAI&#x27;s flagship, paid proprietary model GPT-5 in key third-party performance benchmarks with a new, free model. The Chinese AI startup Moonshot AI’s new Kimi K2 Thinking model, released today, has vaulted past both proprietary and open-weight competitors to claim the top position in reasoning, coding, and agentic-tool benchmarks. Despite being fully open-source, the model now outperforms OpenAI’s GPT-5, Anthropic’s Claude Sonnet 4.5 (Thinking mode), and xAI&#x27;s Grok-4 on several standard evaluations — an inflection point for the competitiveness of open AI systems.Developers can access the model via platform.moonshot.ai and kimi.com; weights and code are hosted on Hugging Face. The open release includes APIs for chat, reasoning, and multi-tool workflows.Users can try out Kimi K2 Thinking directly through its own ChatGPT-like website competitor and on a Hugging Face space as well. Modified Standard Open Source LicenseMoonshot AI has formally released Kimi K2 Thinking under a Modified MIT License on Hugging Face.The license grants full commercial and derivative rights — meaning individual researchers and developers working on behalf of enterprise clients can access it freely and use it in commercial applications — but adds one restriction:\"If the software or any derivative product serves over 100 million monthly active users or generates over $20 million USD per month in revenue, the deployer must prominently display &#x27;Kimi K2&#x27; on the product’s user interface.\"For most research and enterprise applications, this clause functions as a light-touch attribution requirement while preserving the freedoms of standard MIT licensing. It makes K2 Thinking one of the most permissively licensed frontier-class models currently available.A New Benchmark LeaderKimi K2 Thinking is a Mixture-of-Experts (MoE) model built around one trillion parameters, of which 32 billion activate per inference. It combines long-horizon reasoning with structured tool use, executing up to 200–300 sequential tool calls without human intervention.According to Moonshot’s published test results, K2 Thinking achieved:44.9 % on Humanity’s Last Exam (HLE), a state-of-the-art score;60.2 % on BrowseComp, an agentic web-search and reasoning test;71.3 % on SWE-Bench Verified and 83.1 % on LiveCodeBench v6, key coding evaluations;56.3 % on Seal-0, a benchmark for real-world information retrieval.Across these tasks, K2 Thinking consistently outperforms GPT-5’s corresponding scores and surpasses the previous open-weight leader MiniMax-M2—released just weeks earlier by Chinese rival MiniMax AI.Open Model Outperforms Proprietary SystemsGPT-5 and Claude Sonnet 4.5 Thinking remain the leading proprietary “thinking” models. Yet in the same benchmark suite, K2 Thinking’s agentic reasoning scores exceed both: for instance, on BrowseComp the open model’s 60.2 % decisively leads GPT-5’s 54.9 % and Claude 4.5’s 24.1 %.K2 Thinking also edges GPT-5 in GPQA Diamond (85.7 % vs 84.5 %) and matches it on mathematical reasoning tasks such as AIME 2025 and HMMT 2025. Only in certain heavy-mode configurations—where GPT-5 aggregates multiple trajectories—does the proprietary model regain parity.That Moonshot’s fully open-weight release can meet or exceed GPT-5’s scores marks a turning point. The gap between closed frontier systems and publicly available models has effectively collapsed for high-end reasoning and coding.Surpassing MiniMax-M2: The Previous Open-Source BenchmarkWhen VentureBeat profiled MiniMax-M2 just a week and a half ago, it was hailed as the “new king of open-source LLMs,” achieving top scores among open-weight systems:τ²-Bench 77.2BrowseComp 44.0FinSearchComp-global 65.5SWE-Bench Verified 69.4Those results placed MiniMax-M2 near GPT-5-level capability in agentic tool use. Yet Kimi K2 Thinking now eclipses them by wide margins.Its BrowseComp result of 60.2 % exceeds M2’s 44.0 %, and its SWE-Bench Verified 71.3 % edges out M2’s 69.4 %. Even on financial-reasoning tasks such as FinSearchComp-T3 (47.4 %), K2 Thinking performs comparably while maintaining superior general-purpose reasoning.Technically, both models adopt sparse Mixture-of-Experts architectures for compute efficiency, but Moonshot’s network activates more experts and deploys advanced quantization-aware training (INT4 QAT). This design doubles inference speed relative to standard precision without degrading accuracy—critical for long “thinking-token” sessions reaching 256 k context windows.Agentic Reasoning and Tool UseK2 Thinking’s defining capability lies in its explicit reasoning trace. The model outputs an auxiliary field, reasoning_content, revealing intermediate logic before each final response. This transparency preserves coherence across long multi-turn tasks and multi-step tool calls.A reference implementation published by Moonshot demonstrates how the model autonomously conducts a “daily news report” workflow: invoking date and web-search tools, analyzing retrieved content, and composing structured output—all while maintaining internal reasoning state.This end-to-end autonomy enables the model to plan, search, execute, and synthesize evidence across hundreds of steps, mirroring the emerging class of “agentic AI” systems that operate with minimal supervision.Efficiency and AccessDespite its trillion-parameter scale, K2 Thinking’s runtime cost remains modest. Moonshot lists usage at:$0.15 / 1 M tokens (cache hit)$0.60 / 1 M tokens (cache miss)$2.50 / 1 M tokens outputThese rates are competitive even against MiniMax-M2’s $0.30 input / $1.20 output pricing—and an order of magnitude below GPT-5 ($1.25 input / $10 output).Comparative Context: Open-Weight AccelerationThe rapid succession of M2 and K2 Thinking illustrates how quickly open-source research is catching frontier systems. MiniMax-M2 demonstrated that open models could approach GPT-5-class agentic capability at a fraction of the compute cost. Moonshot has now advanced that frontier further, pushing open weights beyond parity into outright leadership.Both models rely on sparse activation for efficiency, but K2 Thinking’s higher activation count (32 B vs 10 B active parameters) yields stronger reasoning fidelity across domains. Its test-time scaling—expanding “thinking tokens” and tool-calling turns—provides measurable performance gains without retraining, a feature not yet observed in MiniMax-M2.Technical OutlookMoonshot reports that K2 Thinking supports native INT4 inference and 256 k-token contexts with minimal performance degradation. Its architecture integrates quantization, parallel trajectory aggregation (“heavy mode”), and Mixture-of-Experts routing tuned for reasoning tasks.In practice, these optimizations allow K2 Thinking to sustain complex planning loops—code compile–test–fix, search–analyze–summarize—over hundreds of tool calls. This capability underpins its superior results on BrowseComp and SWE-Bench, where reasoning continuity is decisive.Enormous Implications for the AI EcosystemThe convergence of open and closed models at the high end signals a structural shift in the AI landscape. Enterprises that once relied exclusively on proprietary APIs can now deploy open alternatives matching GPT-5-level reasoning while retaining full control of weights, data, and compliance.Moonshot’s open publication strategy follows the precedent set by DeepSeek R1, Qwen3, GLM-4.6 and MiniMax-M2 but extends it to full agentic reasoning. For academic and enterprise developers, K2 Thinking provides both transparency and interoperability—the ability to inspect reasoning traces and fine-tune performance for domain-specific agents.The arrival of K2 Thinking signals that Moonshot — a young startup founded in 2023 with investment from some of China&#x27;s biggest apps and tech companies — is here to play in an intensifying competition, and comes amid growing scrutiny of the financial sustainability of AI’s largest players. Just a day ago, OpenAI CFO Sarah Friar sparked controversy after suggesting at WSJ Tech Live event that the U.S. government might eventually need to provide a “backstop” for the company’s more than $1.4 trillion in compute and data-center commitments — a comment widely interpreted as a call for taxpayer-backed loan guarantees.Although Friar later clarified that OpenAI was not seeking direct federal support, the episode reignited debate about the scale and concentration of AI capital spending. With OpenAI, Microsoft, Meta, and Google all racing to secure long-term chip supply, critics warn of an unsustainable investment bubble and “AI arms race” driven more by strategic fear than commercial returns — one that could \"blow up\" and take down the entire global economy with it if there is hesitation or market uncertainty, as so many trades and valuations have now been made in anticipation of continued hefty AI investment and massive returns. Against that backdrop, Moonshot AI’s and MiniMax’s open-weight releases put more pressure on U.S. proprietary AI firms and their backers to justify the size of the investments and paths to profitability. If an enterprise customer can just as easily get comparable or better performance from a free, open source Chinese AI model than they do with paid, proprietary AI solutions like OpenAI&#x27;s GPT-5, Anthropic&#x27;s Claude Sonnet 4.5, or Google&#x27;s Gemini 2.5 Pro — why would they continue paying to access the proprietary models? Already, Silicon Valley stalwarts like Airbnb have raised eyebrows for admitting to heavily using Chinese open source alternatives like Alibaba&#x27;s Qwen over OpenAI&#x27;s proprietary offerings. For investors and enterprises, these developments suggest that high-end AI capability is no longer synonymous with high-end capital expenditure. The most advanced reasoning systems may now come not from companies building gigascale data centers, but from research groups optimizing architectures and quantization for efficiency.In that sense, K2 Thinking’s benchmark dominance is not just a technical milestone—it’s a strategic one, arriving at a moment when the AI market’s biggest question has shifted from how powerful models can become to who can afford to sustain them.What It Means for Enterprises Going ForwardWithin weeks of MiniMax-M2’s ascent, Kimi K2 Thinking has overtaken it—along with GPT-5 and Claude 4.5—across nearly every reasoning and agentic benchmark. The model demonstrates that open-weight systems can now meet or surpass proprietary frontier models in both capability and efficiency.For the AI research community, K2 Thinking represents more than another open model: it is evidence that the frontier has become collaborative. The best-performing reasoning model available today is not a closed commercial product but an open-source system accessible to anyone.",
          "feed_position": 0,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/1HGuj3BsedMoBgz8md53tP/b48cd315b64ffaf658892d572c61d188/cfr0z3n_fisheye_view_surreal_bold_deep_color_vibrant_high_contr_5795af91-597f-4686-8f07-ff43e6922e8c.png?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/headspace-black-friday-deal-get-50-percent-off-annual-subscriptions-150051737.html",
          "published_at": "Thu, 06 Nov 2025 16:30:51 +0000",
          "title": "Headspace Black Friday deal: Get 50 percent off annual subscriptions",
          "standfirst": "Headspace’s Black Friday deal is live, offering 50 percent off its annual subscription through December 4. That brings the cost of a full year down to $35, giving you access to guided meditations, courses and stress-management tools that can help you stay balanced heading into the new year. Headspace has become one of the most recognizable names in digital mindfulness. The app blends practical meditation guidance with structured courses and calming soundscapes designed to make everyday stress easier to manage. Its programs cover everything from beginner-friendly introductions to mindfulness to focused content on topics like anxiety, productivity and sleep. Subscribers get access to hundreds of guided sessions led by the Headspace team, including short daily practices that can be completed in a few spare minutes, plus longer courses that help build consistency. The app’s Sleepcasts and soundscapes are unique, designed to create a steady nighttime routine that promotes better rest. For mornings, there are breathing exercises and motivational mini-sessions that can help set focus for the day ahead. Headspace also includes personalized progress tracking, mood check-ins and optional reminders that make it easier to stay consistent with your new mindfulness habits. For anyone new to meditation, the app’s clear structure is a major strength. You don’t have to know where to start, since it suggests sessions based on your goals or current mood. This annual deal is ideal for users who want to stick with mindfulness practice over time, or anyone interested in incorporating a new habit into their lives. Paying for the year upfront typically saves money compared with the monthly plan, and the discount brings that cost down even further. Whether you’re learning the basics of meditation or refining an existing routine, the full library provides enough variety to keep things engaging throughout the year. If you’re still comparing wellness apps, check out our guide to the best meditation apps to see how Headspace stacks up against other options. But for those ready to commit to a calmer routine, this annual offer is one of the simplest ways to start the habit at a lower cost.This article originally appeared on Engadget at https://www.engadget.com/deals/headspace-black-friday-deal-get-50-percent-off-annual-subscriptions-150051737.html?src=rss",
          "content": "Headspace’s Black Friday deal is live, offering 50 percent off its annual subscription through December 4. That brings the cost of a full year down to $35, giving you access to guided meditations, courses and stress-management tools that can help you stay balanced heading into the new year. Headspace has become one of the most recognizable names in digital mindfulness. The app blends practical meditation guidance with structured courses and calming soundscapes designed to make everyday stress easier to manage. Its programs cover everything from beginner-friendly introductions to mindfulness to focused content on topics like anxiety, productivity and sleep. Subscribers get access to hundreds of guided sessions led by the Headspace team, including short daily practices that can be completed in a few spare minutes, plus longer courses that help build consistency. The app’s Sleepcasts and soundscapes are unique, designed to create a steady nighttime routine that promotes better rest. For mornings, there are breathing exercises and motivational mini-sessions that can help set focus for the day ahead. Headspace also includes personalized progress tracking, mood check-ins and optional reminders that make it easier to stay consistent with your new mindfulness habits. For anyone new to meditation, the app’s clear structure is a major strength. You don’t have to know where to start, since it suggests sessions based on your goals or current mood. This annual deal is ideal for users who want to stick with mindfulness practice over time, or anyone interested in incorporating a new habit into their lives. Paying for the year upfront typically saves money compared with the monthly plan, and the discount brings that cost down even further. Whether you’re learning the basics of meditation or refining an existing routine, the full library provides enough variety to keep things engaging throughout the year. If you’re still comparing wellness apps, check out our guide to the best meditation apps to see how Headspace stacks up against other options. But for those ready to commit to a calmer routine, this annual offer is one of the simplest ways to start the habit at a lower cost.This article originally appeared on Engadget at https://www.engadget.com/deals/headspace-black-friday-deal-get-50-percent-off-annual-subscriptions-150051737.html?src=rss",
          "feed_position": 11
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/black-friday-2025-the-best-early-tech-deals-on-apple-shark-lego-and-other-gear-ahead-of-the-biggest-sale-of-the-year-100052620.html",
          "published_at": "Thu, 06 Nov 2025 15:30:35 +0000",
          "title": "Black Friday 2025: The best early tech deals on Apple, Shark, Lego and other gear ahead of the biggest sale of the year",
          "standfirst": "November has turned into Black Friday and vice versa. What was once a one-day shopping sprint has turned into a month-long marathon, with retailers rolling out discounts week after week. Thanks to this, it can be easy to get deal fatigue after a while — but no one wants to miss out on a good discount, regardless of if you’re buying for yourself or someone else. We’re tracking all of the best Black Friday deals you can get right now so you don’t have to go searching for them.Engadget can help if you have tech on your shopping list this year. Here, we’ve curated the best Black Friday tech deals you can get right now, and we'll continue to update this post as we get closer to the big day at the end of November. Note that you probably have the best chance of snagging record-low prices when we get to about one week before Thanksgiving, but these deals available now are worth considering. Black Friday deals to shop now Apple iPad mini for $399 ($100 off): Apple's smallest tablet, the iPad mini is the best option for those who prefer their tablet be roughly the size of a paperback book. The latest model runs on the A17 Pro chipset and has an upgraded 128GB of storage in the base configuration. It also supports the Apple Pencil Pro. Apple Mac Mini M4 for $499 ($100 off): Desktop users looking for an upgrade should consider the latest Mac Mini, which runs on the M4 chip and 16GB of RAM as standard in the base configuration. This version has a smaller design that takes up less space, front-facing USB-C ports and a headphone jack, plus Thunderbolt 5 support. Google Pixel 9a for $499 (38 percent off): It doesn't get much better than the Pixel 9a if you're looking for a smartphone well under $1,000. It's our top pick for the best midrange smartphone on the market right now thanks to its simple yet excellent design, AMOLED display, strong battery life and solid camera array. Bose QuietComfort headphones for $199 (43 percent off): These noise-cancelling headphones have a comfortable (albeit a bit boring) design, an \"Aware\" mode that lets you hear more of your surroundings when you need to and up to 24 hours of battery life. Also available at Best Buy. Lego Disney advent calendar 2025 43253 for $40 (11 percent off): I probably don't need to tell you 'tis the season for advent calendars. Lego has a bunch, and this one in particular is on a good deal right now. Like most Lego advent calendars, this one includes a number of bricks and minifigs that, when put together, make a coherent, themed scene. EcoFlow Black Friday deals — get up to 80 percent off: Portable power stations are an investment, but they can be crucial pieces of tech during emergencies. The top pick from our friends at Yahoo Tech has been heavily discounted in this early Black Friday sale. You can pick up the EcoFlow Delta Pro 3 for $1,400 off, down to $2,299, or the power station with an extra battery bundled in for $2,699 off, down to $3,599. Apple Watch SE 3 for $200 ($50 off): The SE has been our top pick for the best Apple Watch for those on a budget, and the latest model only solidifies that further. It has the same chipset found in the latest flagship Apple Watches, fast-charging capabilities, an always-on display and most of the same activity-tracking features you'll find in more expensive model. Jisulife Life 7 handheld fan for $25 (14 percent off): This handy little fan is a must-have if you life in a warm climate or have a tropical vacation planned anytime soon. It can be used as a table or handheld fan and even be worn around the neck so you don't have to hold it at all. Its 5,000 mAh battery allows it to last hours on a single charge, and the small display in the middle of the fan's blades show its remaining battery level. Leebin 2025 electric spin scrubber for $40 (43 percent off, Prime exclusive): This weird little scrubber makes cleaning my bathroom and shower much less of a pain. Just choose the brush head you need for your job and the rotating head takes care of most of the hard work. I love the adjustable handle, which extends from 12 to 50 inches so you can get into hard-to-reach places without breaking a sweat. SanDisk microSD Express card (256GB) for $60 (12 percent off): If you have a Switch 2, no regular microSD card will do if you want to expand the console's storage. You need a newer microSD Express card, and currently there are only a handful on the market. We did some testing to find the best microSD Express card for the Switch 2 and found that performance was, in general, very similar amongst all the readily available cards. We recommend getting whichever fits within your budget at the capacity you want. Google TV Streamer 4K for $75 ($25 off): Our top pick for the best streaming device right now, the latest version of Google's streamer supports 4K video and an excellent, easy-to-use interface that will feel familiar to anyone who's seen a set with the Google TV technology built in. It provides access to all of the major streaming services including Netflix, Disney+, HBO Max, YouTube and more, plus it has a handy on-screen pop up that lets you control all compatible smart home devices right from your TV. Also available at Walmart. Cosori 9-in-1 air fryer for $90 (25 percent off): I personally have this air fryer, one of our top picks, in my house and I've used it for over a year with no issues. I love that it makes good use of vertical space so it doesn't take up too much space on my counter, and its rounded-square shape allows me to cook more food than you'd think in one go in the basket. It crisps all kinds of foods up well and generally takes a lot of the guess work (and time) out of making a good meal. Shark AI Ultra robot vacuum with 60-day self-emptying base for $300 (50 percent off): This is a version of one of our top picks for the best robot vacuums. We generally like Shark machines because they do a good job cleaning all types of flooring, produce accurate home maps and the companion app is pretty easy to use. This one in particularly comes with a self-emptying base that can hold up to 60 days worth of debris. Black Friday FAQs When is Black Friday 2025? Black Friday 2025 lands on November 28. Which stores have Black Friday deals? Many physical retail stores have Black Friday deals including Walmart, Target, Best Buy and others. Even more retailers have online Black Friday deals, including Amazon, GameStop, Costco and others. When do Black Friday sales start? Gone are the times when Black Friday sales were one-day-only affairs. Now, Black Friday deals are often available starting on Thanksgiving, or even earlier. Last year, we saw Black Friday deals online begin the week before Black Friday proper. When do Black Friday sales end? Black Friday and Cyber Monday have blended a lot over the past few years. Now, you can expect to see a good portion of Black Friday deals extend through the weekend and into Cyber Monday. It's not uncommon for Black Friday deals to expire at the end of Cyber Monday. Which retailers have the best Black Friday tech deals? The best Black Friday tech deals are typically available online at retailers like Amazon, Walmart, Best Buy and Target. It's also a good idea to check the store websites of the companies that make the products you want — for example, if you're looking for a Sonos speaker, check the Sonos website on Black Friday. Most of the time, you'll find the best Black Friday tech deals are matched at multiple retailers. Does Apple have Black Friday sales? No, you will usually not find Black Friday sales at Apple stores or on Apple's website. However, you can find Black Friday deals on Apple devices elsewhere; we recommend checking Amazon, Best Buy and other big retailers for discounts on iPads, Apple Watches and more on Black Friday. Does Amazon have Black Friday sales? Yes, Amazon has Black Friday sales. The online retailer's site will look similar to Prime Day on Black Friday, with discounts on all sorts of items from household essentials to fashion to tech.This article originally appeared on Engadget at https://www.engadget.com/deals/black-friday-2025-the-best-early-tech-deals-on-apple-shark-lego-and-other-gear-ahead-of-the-biggest-sale-of-the-year-100052620.html?src=rss",
          "content": "November has turned into Black Friday and vice versa. What was once a one-day shopping sprint has turned into a month-long marathon, with retailers rolling out discounts week after week. Thanks to this, it can be easy to get deal fatigue after a while — but no one wants to miss out on a good discount, regardless of if you’re buying for yourself or someone else. We’re tracking all of the best Black Friday deals you can get right now so you don’t have to go searching for them.Engadget can help if you have tech on your shopping list this year. Here, we’ve curated the best Black Friday tech deals you can get right now, and we'll continue to update this post as we get closer to the big day at the end of November. Note that you probably have the best chance of snagging record-low prices when we get to about one week before Thanksgiving, but these deals available now are worth considering. Black Friday deals to shop now Apple iPad mini for $399 ($100 off): Apple's smallest tablet, the iPad mini is the best option for those who prefer their tablet be roughly the size of a paperback book. The latest model runs on the A17 Pro chipset and has an upgraded 128GB of storage in the base configuration. It also supports the Apple Pencil Pro. Apple Mac Mini M4 for $499 ($100 off): Desktop users looking for an upgrade should consider the latest Mac Mini, which runs on the M4 chip and 16GB of RAM as standard in the base configuration. This version has a smaller design that takes up less space, front-facing USB-C ports and a headphone jack, plus Thunderbolt 5 support. Google Pixel 9a for $499 (38 percent off): It doesn't get much better than the Pixel 9a if you're looking for a smartphone well under $1,000. It's our top pick for the best midrange smartphone on the market right now thanks to its simple yet excellent design, AMOLED display, strong battery life and solid camera array. Bose QuietComfort headphones for $199 (43 percent off): These noise-cancelling headphones have a comfortable (albeit a bit boring) design, an \"Aware\" mode that lets you hear more of your surroundings when you need to and up to 24 hours of battery life. Also available at Best Buy. Lego Disney advent calendar 2025 43253 for $40 (11 percent off): I probably don't need to tell you 'tis the season for advent calendars. Lego has a bunch, and this one in particular is on a good deal right now. Like most Lego advent calendars, this one includes a number of bricks and minifigs that, when put together, make a coherent, themed scene. EcoFlow Black Friday deals — get up to 80 percent off: Portable power stations are an investment, but they can be crucial pieces of tech during emergencies. The top pick from our friends at Yahoo Tech has been heavily discounted in this early Black Friday sale. You can pick up the EcoFlow Delta Pro 3 for $1,400 off, down to $2,299, or the power station with an extra battery bundled in for $2,699 off, down to $3,599. Apple Watch SE 3 for $200 ($50 off): The SE has been our top pick for the best Apple Watch for those on a budget, and the latest model only solidifies that further. It has the same chipset found in the latest flagship Apple Watches, fast-charging capabilities, an always-on display and most of the same activity-tracking features you'll find in more expensive model. Jisulife Life 7 handheld fan for $25 (14 percent off): This handy little fan is a must-have if you life in a warm climate or have a tropical vacation planned anytime soon. It can be used as a table or handheld fan and even be worn around the neck so you don't have to hold it at all. Its 5,000 mAh battery allows it to last hours on a single charge, and the small display in the middle of the fan's blades show its remaining battery level. Leebin 2025 electric spin scrubber for $40 (43 percent off, Prime exclusive): This weird little scrubber makes cleaning my bathroom and shower much less of a pain. Just choose the brush head you need for your job and the rotating head takes care of most of the hard work. I love the adjustable handle, which extends from 12 to 50 inches so you can get into hard-to-reach places without breaking a sweat. SanDisk microSD Express card (256GB) for $60 (12 percent off): If you have a Switch 2, no regular microSD card will do if you want to expand the console's storage. You need a newer microSD Express card, and currently there are only a handful on the market. We did some testing to find the best microSD Express card for the Switch 2 and found that performance was, in general, very similar amongst all the readily available cards. We recommend getting whichever fits within your budget at the capacity you want. Google TV Streamer 4K for $75 ($25 off): Our top pick for the best streaming device right now, the latest version of Google's streamer supports 4K video and an excellent, easy-to-use interface that will feel familiar to anyone who's seen a set with the Google TV technology built in. It provides access to all of the major streaming services including Netflix, Disney+, HBO Max, YouTube and more, plus it has a handy on-screen pop up that lets you control all compatible smart home devices right from your TV. Also available at Walmart. Cosori 9-in-1 air fryer for $90 (25 percent off): I personally have this air fryer, one of our top picks, in my house and I've used it for over a year with no issues. I love that it makes good use of vertical space so it doesn't take up too much space on my counter, and its rounded-square shape allows me to cook more food than you'd think in one go in the basket. It crisps all kinds of foods up well and generally takes a lot of the guess work (and time) out of making a good meal. Shark AI Ultra robot vacuum with 60-day self-emptying base for $300 (50 percent off): This is a version of one of our top picks for the best robot vacuums. We generally like Shark machines because they do a good job cleaning all types of flooring, produce accurate home maps and the companion app is pretty easy to use. This one in particularly comes with a self-emptying base that can hold up to 60 days worth of debris. Black Friday FAQs When is Black Friday 2025? Black Friday 2025 lands on November 28. Which stores have Black Friday deals? Many physical retail stores have Black Friday deals including Walmart, Target, Best Buy and others. Even more retailers have online Black Friday deals, including Amazon, GameStop, Costco and others. When do Black Friday sales start? Gone are the times when Black Friday sales were one-day-only affairs. Now, Black Friday deals are often available starting on Thanksgiving, or even earlier. Last year, we saw Black Friday deals online begin the week before Black Friday proper. When do Black Friday sales end? Black Friday and Cyber Monday have blended a lot over the past few years. Now, you can expect to see a good portion of Black Friday deals extend through the weekend and into Cyber Monday. It's not uncommon for Black Friday deals to expire at the end of Cyber Monday. Which retailers have the best Black Friday tech deals? The best Black Friday tech deals are typically available online at retailers like Amazon, Walmart, Best Buy and Target. It's also a good idea to check the store websites of the companies that make the products you want — for example, if you're looking for a Sonos speaker, check the Sonos website on Black Friday. Most of the time, you'll find the best Black Friday tech deals are matched at multiple retailers. Does Apple have Black Friday sales? No, you will usually not find Black Friday sales at Apple stores or on Apple's website. However, you can find Black Friday deals on Apple devices elsewhere; we recommend checking Amazon, Best Buy and other big retailers for discounts on iPads, Apple Watches and more on Black Friday. Does Amazon have Black Friday sales? Yes, Amazon has Black Friday sales. The online retailer's site will look similar to Prime Day on Black Friday, with discounts on all sorts of items from household essentials to fashion to tech.This article originally appeared on Engadget at https://www.engadget.com/deals/black-friday-2025-the-best-early-tech-deals-on-apple-shark-lego-and-other-gear-ahead-of-the-biggest-sale-of-the-year-100052620.html?src=rss",
          "feed_position": 14
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/the-apple-macbook-air-m4-just-hit-a-new-all-time-low-of-749-ahead-of-black-friday-183808749.html",
          "published_at": "Thu, 06 Nov 2025 15:27:36 +0000",
          "title": "The Apple MacBook Air M4 just hit a new all-time-low of $749 ahead of Black Friday",
          "standfirst": "Now's a great time to pick up a new MacBook Air if you've been thinking about taking the plunge. Amazon has the M4-powered, 13-inch MacBook Air for a record-low price of $749 right now. The 25 percent discount applies to multiple colors, too. We ranked this as our favorite Apple laptop in our list of the best MacBook computers. Heck, it's even our very favorite laptop. Full stop. The performance is exceptionally snappy, thanks to the M4 chip. We appreciated the upgraded battery life, which now lasts for around 18 hours per charge. That's well beyond a full day of work. The design is lightweight, but sturdy. This has become a hallmark for modern MacBook Air computers. The screen is both gorgeous and roomy, even though it's technically just a 13-inch panel. There's support for the P3 wide color gamut and it can reach up to 500 nits of brightness. This is a near-perfect laptop, but there are a couple of nitpicks. There's no USB-C port on the right side, limiting how users can arrange accessories on a desk. Also, the screen is capped with a 60Hz refresh rate. Another potential complication is the looming specter of the M5 chip. The company has already released the MacBook Pro M5, so a new MacBook Air is likely coming in the nearish future. (Read: sometime in early 2026). If you need more screen space, you'll find a similar discount on the 15-inch MacBook Air on Amazon, too. Most color options are $250 off and down to $949 for the base model (you guessed it — another all-time low). This article originally appeared on Engadget at https://www.engadget.com/deals/the-apple-macbook-air-m4-just-hit-a-new-all-time-low-of-749-ahead-of-black-friday-183808749.html?src=rss",
          "content": "Now's a great time to pick up a new MacBook Air if you've been thinking about taking the plunge. Amazon has the M4-powered, 13-inch MacBook Air for a record-low price of $749 right now. The 25 percent discount applies to multiple colors, too. We ranked this as our favorite Apple laptop in our list of the best MacBook computers. Heck, it's even our very favorite laptop. Full stop. The performance is exceptionally snappy, thanks to the M4 chip. We appreciated the upgraded battery life, which now lasts for around 18 hours per charge. That's well beyond a full day of work. The design is lightweight, but sturdy. This has become a hallmark for modern MacBook Air computers. The screen is both gorgeous and roomy, even though it's technically just a 13-inch panel. There's support for the P3 wide color gamut and it can reach up to 500 nits of brightness. This is a near-perfect laptop, but there are a couple of nitpicks. There's no USB-C port on the right side, limiting how users can arrange accessories on a desk. Also, the screen is capped with a 60Hz refresh rate. Another potential complication is the looming specter of the M5 chip. The company has already released the MacBook Pro M5, so a new MacBook Air is likely coming in the nearish future. (Read: sometime in early 2026). If you need more screen space, you'll find a similar discount on the 15-inch MacBook Air on Amazon, too. Most color options are $250 off and down to $949 for the base model (you guessed it — another all-time low). This article originally appeared on Engadget at https://www.engadget.com/deals/the-apple-macbook-air-m4-just-hit-a-new-all-time-low-of-749-ahead-of-black-friday-183808749.html?src=rss",
          "feed_position": 16
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/wearables/the-ai-powered-stream-ring-is-designed-for-on-the-fly-voice-notes-143530840.html",
          "published_at": "Thu, 06 Nov 2025 14:35:30 +0000",
          "title": "The AI-powered Stream Ring is designed for on-the-fly voice notes",
          "standfirst": "Two former Meta employees are launching a new AI-powered smart ring. Stream Ring is the debut product from Sandbar, and it’s available to pre-order right now. Sandbar describes Stream as \"your extended self,\" which is to say that it’s a deliberately minimalist smart ring that you can use to take voice notes and interact with a chatbot directly using the built-in touch-activated microphone. When you create a voice note, the Stream Ring uses haptic feedback to confirm that it’s been recorded. You hold the sensor to speak and tap it if you want to interrupt and start over. It can automatically transcribe your voice interactions, whether you’re simply compiling a grocery list on the go, asking it to fetch some information from the web, or having a more back-and-forth conversation with the device. These notes will appear in the Stream app via Bluetooth, which will be iOS-only at launch. Sandbar thinks a ring is the best form for its wearable to take as it’s always available and accessible, whatever you’re doing, so you easily can log a thought as soon as it pops into your head. It wants people to think of the Stream Ring as a \"mouse for voice,\" and says the mic will always be able to pick up your voice clearly in a noisy room. Crucially, it isn’t always listening either, only activating when you hold the touchpad. As for the chatbot, it’s designed by default to somewhat mimic your own voice using AI, a feature Sandbar calls Inner Voice. You can update it if you think it sounds off, or if the whole concept creeps you out you can also switch to a non-personalized voice. Away from the core voice note functionality, the Stream Ring can also be used to control media playback using gestures, and Sandbar says it’s compatible with any headphones. Sandbar will offer a free plan that includes unlimited notes but limits AI interactions. For $10 per month you can upgrade to a Stream Pro subscription (you get three months for free with a new purchase), which enables unlimited chats and immediate access to any new features. Sandbar says your data is encrypted at rest and in transit and it will not sell your information to other companies. You can also delete data stored in the app at any time. Other smart rings have promised a seamless interactive experience built around AI, and the Stream Ring isn’t going down the health tracking route that many other products in this increasingly crowded space market themselves on. But if the transcription feature works as reliably as advertised and Sandbar’s AI proves to be a genuinely useful assistant, the Stream Ring could be a useful accessory that doesn’t get in the way when it isn’t needed. The Stream Ring is available in sizes 5-13 and is designed to be worn on your index finger. Sandbar promises \"all-day battery life\" but doesn’t go into specifics. It’s expected to start shipping next summer in the US and costs $249.This article originally appeared on Engadget at https://www.engadget.com/wearables/the-ai-powered-stream-ring-is-designed-for-on-the-fly-voice-notes-143530840.html?src=rss",
          "content": "Two former Meta employees are launching a new AI-powered smart ring. Stream Ring is the debut product from Sandbar, and it’s available to pre-order right now. Sandbar describes Stream as \"your extended self,\" which is to say that it’s a deliberately minimalist smart ring that you can use to take voice notes and interact with a chatbot directly using the built-in touch-activated microphone. When you create a voice note, the Stream Ring uses haptic feedback to confirm that it’s been recorded. You hold the sensor to speak and tap it if you want to interrupt and start over. It can automatically transcribe your voice interactions, whether you’re simply compiling a grocery list on the go, asking it to fetch some information from the web, or having a more back-and-forth conversation with the device. These notes will appear in the Stream app via Bluetooth, which will be iOS-only at launch. Sandbar thinks a ring is the best form for its wearable to take as it’s always available and accessible, whatever you’re doing, so you easily can log a thought as soon as it pops into your head. It wants people to think of the Stream Ring as a \"mouse for voice,\" and says the mic will always be able to pick up your voice clearly in a noisy room. Crucially, it isn’t always listening either, only activating when you hold the touchpad. As for the chatbot, it’s designed by default to somewhat mimic your own voice using AI, a feature Sandbar calls Inner Voice. You can update it if you think it sounds off, or if the whole concept creeps you out you can also switch to a non-personalized voice. Away from the core voice note functionality, the Stream Ring can also be used to control media playback using gestures, and Sandbar says it’s compatible with any headphones. Sandbar will offer a free plan that includes unlimited notes but limits AI interactions. For $10 per month you can upgrade to a Stream Pro subscription (you get three months for free with a new purchase), which enables unlimited chats and immediate access to any new features. Sandbar says your data is encrypted at rest and in transit and it will not sell your information to other companies. You can also delete data stored in the app at any time. Other smart rings have promised a seamless interactive experience built around AI, and the Stream Ring isn’t going down the health tracking route that many other products in this increasingly crowded space market themselves on. But if the transcription feature works as reliably as advertised and Sandbar’s AI proves to be a genuinely useful assistant, the Stream Ring could be a useful accessory that doesn’t get in the way when it isn’t needed. The Stream Ring is available in sizes 5-13 and is designed to be worn on your index finger. Sandbar promises \"all-day battery life\" but doesn’t go into specifics. It’s expected to start shipping next summer in the US and costs $249.This article originally appeared on Engadget at https://www.engadget.com/wearables/the-ai-powered-stream-ring-is-designed-for-on-the-fly-voice-notes-143530840.html?src=rss",
          "feed_position": 20
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/ecoflows-early-black-friday-deals-include-42-percent-off-portable-power-stations-130041544.html",
          "published_at": "Thu, 06 Nov 2025 14:31:25 +0000",
          "title": "EcoFlow's early Black Friday deals include 42 percent off portable power stations",
          "standfirst": "The EcoFlow Black Friday sale is in full swing, knocking thousands of dollars off portable power stations and their accessories. One of the best discounts at the moment is on the Delta Pro 3, which is 37 percent off and down to $2,299. That's the lowest we've seen it and, considering it typically costs $3,699, it's a great deal. Amazon's matching the sale price as well. The Delta Pro 3 topped Yahoo's list of the best portable power stations, and for very good reason. This thing is a beast. It boasts a 4,096Wh capacity, so it can power an average 500-watt refrigerator for over 24 hours. That's with continuous use. It can be stretched out to two or three days by only running the appliance during daylight hours. There's even a discounted bundle that includes an extra battery for $3,599. It includes four standard 120V AC outlets and a single 240V outlet. It could potentially be a temporary hub of a whole-home battery backup. There are numerous charging options here, including a standard AC outlet, solar panels and, interestingly, a cigarette lighter. The only potential downside here is the Delta Pro 3 really pushes the boundaries of what can be considered portable. It weighs 113 pounds, though it does have wheels and a telescoping handle. The Delta Pro 3 is just one of the products on sale right now. The Delta Pro Ultra, which is intended as a whole-home backup, is down to $3,999. This represents a savings of more than $2,000. Another Yahoo top pick, the Delta 2 Max, is $1,000 off and down to $899. This article originally appeared on Engadget at https://www.engadget.com/deals/ecoflows-early-black-friday-deals-include-42-percent-off-portable-power-stations-130041544.html?src=rss",
          "content": "The EcoFlow Black Friday sale is in full swing, knocking thousands of dollars off portable power stations and their accessories. One of the best discounts at the moment is on the Delta Pro 3, which is 37 percent off and down to $2,299. That's the lowest we've seen it and, considering it typically costs $3,699, it's a great deal. Amazon's matching the sale price as well. The Delta Pro 3 topped Yahoo's list of the best portable power stations, and for very good reason. This thing is a beast. It boasts a 4,096Wh capacity, so it can power an average 500-watt refrigerator for over 24 hours. That's with continuous use. It can be stretched out to two or three days by only running the appliance during daylight hours. There's even a discounted bundle that includes an extra battery for $3,599. It includes four standard 120V AC outlets and a single 240V outlet. It could potentially be a temporary hub of a whole-home battery backup. There are numerous charging options here, including a standard AC outlet, solar panels and, interestingly, a cigarette lighter. The only potential downside here is the Delta Pro 3 really pushes the boundaries of what can be considered portable. It weighs 113 pounds, though it does have wheels and a telescoping handle. The Delta Pro 3 is just one of the products on sale right now. The Delta Pro Ultra, which is intended as a whole-home backup, is down to $3,999. This represents a savings of more than $2,000. Another Yahoo top pick, the Delta 2 Max, is $1,000 off and down to $899. This article originally appeared on Engadget at https://www.engadget.com/deals/ecoflows-early-black-friday-deals-include-42-percent-off-portable-power-stations-130041544.html?src=rss",
          "feed_position": 21
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/apples-mac-mini-m4-is-100-off-in-this-early-black-friday-deal-150749744.html",
          "published_at": "Thu, 06 Nov 2025 14:05:38 +0000",
          "title": "Apple's Mac mini M4 is $100 off in this early Black Friday deal",
          "standfirst": "It's a good idea to look for Black Friday Apple deals this time of year if you're thinking about upgrading an aging laptop or picking up a pair of AirPods or an Apple Watch as a gift. One of the best discounts we're tracking at the moment is on the M4-powered Mac Mini desktop — it's $100 off right now, down from $599 to $499. That applies to the 16GB of RAM/256GB SSD model, and it's pretty close to its all-time-low price. We gave the Mac mini M4 a 90 in our review, in part, because it packs an incredible amount of power into such a small design. It also has front facing USB-C and headphone ports, a first for the Mac mini lineup. Plus, it starts with 16GB of RAM, an upgrade from its predecessors. However, if you want more memory or storage, the other Mac Mini M4 models are also on sale. You can get 16GB of RAM and 512GB of SSD for $690, down from $799. Then there's the option for 24GB of RAM and 512GB of SSD at $890, down from $999. Plus, if you want to bundle in three years of AppleCare+, each model ends up being about $100 cheaper than normal. If you're looking to build a desktop setup from scratch, there's a small but notable discount on Apple's Magic Trackpad as well. It's down to $120, which is only seven percent off its usual price but it's the cheapest we've seen it. This article originally appeared on Engadget at https://www.engadget.com/deals/apples-mac-mini-m4-is-100-off-in-this-early-black-friday-deal-150749744.html?src=rss",
          "content": "It's a good idea to look for Black Friday Apple deals this time of year if you're thinking about upgrading an aging laptop or picking up a pair of AirPods or an Apple Watch as a gift. One of the best discounts we're tracking at the moment is on the M4-powered Mac Mini desktop — it's $100 off right now, down from $599 to $499. That applies to the 16GB of RAM/256GB SSD model, and it's pretty close to its all-time-low price. We gave the Mac mini M4 a 90 in our review, in part, because it packs an incredible amount of power into such a small design. It also has front facing USB-C and headphone ports, a first for the Mac mini lineup. Plus, it starts with 16GB of RAM, an upgrade from its predecessors. However, if you want more memory or storage, the other Mac Mini M4 models are also on sale. You can get 16GB of RAM and 512GB of SSD for $690, down from $799. Then there's the option for 24GB of RAM and 512GB of SSD at $890, down from $999. Plus, if you want to bundle in three years of AppleCare+, each model ends up being about $100 cheaper than normal. If you're looking to build a desktop setup from scratch, there's a small but notable discount on Apple's Magic Trackpad as well. It's down to $120, which is only seven percent off its usual price but it's the cheapest we've seen it. This article originally appeared on Engadget at https://www.engadget.com/deals/apples-mac-mini-m4-is-100-off-in-this-early-black-friday-deal-150749744.html?src=rss",
          "feed_position": 23
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/one-of-our-favorite-budgeting-apps-is-half-off-ahead-of-black-friday-174011479.html",
          "published_at": "Thu, 06 Nov 2025 13:31:26 +0000",
          "title": "One of our favorite budgeting apps is half off ahead of Black Friday",
          "standfirst": "Monarch Money is one of our favorite budgeting apps and, fittingly enough, there's a way for newcomers to save money on a subscription right now. If you use the code MONARCHVIP at checkout, you can get an annual plan for 50 percent off. It typically costs $100, but you can get 12 months of access for $50 with this code. There are some key caveats here. The discount is only for new users, and it can't be combined with other offers. The code only works when you sign up through the web. You can't redeem it through the Monarch mobile app. We feel that Monarch has a steeper learning curve than some other budget trackers and that certain aspects of the app are slightly more complex than they probably need to be. But it offers a great deal of customization and granularity, which outweighs our misgivings. On the main dashboard, you'll see your net worth along with your latest transactions, spending versus the previous month, your income so far for the month and details about upcoming bills, your investments and goals you've set. There's also a link to a month-in-review page, which offers an in-depth overview of what's been happening with your money that month. You'll also be able to take a peek at how your net worth has changed over time. Monarch can connect to your bank and track Apple Card, Apple Cash and Savings accounts. It can pull in your transactions and balance history automatically and detect your recurring expenses and income. The app can even keep your car valuation up to date. While it might take a little work to set up Monarch (and you might have to tweak things here and there), it's a detailed budgeting app that can help you keep better track of your income, expenditure and net worth. If you're a former Mint user (RIP), Monarch Money is a great alternative if you haven't yet found a Mint replacement. But it's worth mentioning that our favorite Mint replacement service, Quicken Simplifi, also has a sale going on right now. It's offering 50 percent off when you sign up for an annual subscription, billed at $3 per month with the discount. That comes out to $36 for the first year. This article originally appeared on Engadget at https://www.engadget.com/deals/one-of-our-favorite-budgeting-apps-is-half-off-ahead-of-black-friday-174011479.html?src=rss",
          "content": "Monarch Money is one of our favorite budgeting apps and, fittingly enough, there's a way for newcomers to save money on a subscription right now. If you use the code MONARCHVIP at checkout, you can get an annual plan for 50 percent off. It typically costs $100, but you can get 12 months of access for $50 with this code. There are some key caveats here. The discount is only for new users, and it can't be combined with other offers. The code only works when you sign up through the web. You can't redeem it through the Monarch mobile app. We feel that Monarch has a steeper learning curve than some other budget trackers and that certain aspects of the app are slightly more complex than they probably need to be. But it offers a great deal of customization and granularity, which outweighs our misgivings. On the main dashboard, you'll see your net worth along with your latest transactions, spending versus the previous month, your income so far for the month and details about upcoming bills, your investments and goals you've set. There's also a link to a month-in-review page, which offers an in-depth overview of what's been happening with your money that month. You'll also be able to take a peek at how your net worth has changed over time. Monarch can connect to your bank and track Apple Card, Apple Cash and Savings accounts. It can pull in your transactions and balance history automatically and detect your recurring expenses and income. The app can even keep your car valuation up to date. While it might take a little work to set up Monarch (and you might have to tweak things here and there), it's a detailed budgeting app that can help you keep better track of your income, expenditure and net worth. If you're a former Mint user (RIP), Monarch Money is a great alternative if you haven't yet found a Mint replacement. But it's worth mentioning that our favorite Mint replacement service, Quicken Simplifi, also has a sale going on right now. It's offering 50 percent off when you sign up for an annual subscription, billed at $3 per month with the discount. That comes out to $36 for the first year. This article originally appeared on Engadget at https://www.engadget.com/deals/one-of-our-favorite-budgeting-apps-is-half-off-ahead-of-black-friday-174011479.html?src=rss",
          "feed_position": 26
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/cameras/the-gear-i-used-to-photograph-paul-mccartney-133033591.html",
          "published_at": "Thu, 06 Nov 2025 13:30:33 +0000",
          "title": "The gear I used to photograph Paul McCartney",
          "standfirst": "This story about Paul McCartney begins with one of his old bandmates. \"I'm not really Beatle George,\" the ever-philosophical George Harrison once said. \"For me, Beatle George was a suit or a shirt that I once wore. And the only problem is, for the rest of my life, people are going to look at that shirt and mistake it for me.\" On one hand, that’s, well, George being George. But his quote does speak to our need to mythologize the Beatles. It’s hard not to! The music is so exquisite, influential and timeless that we look for grand stories to tell about it. We want a stronger connection to it, so we pore over biographies, interviews and documentaries. We seek meaning and purpose in their story. Still, it must be surreal to be one of the four protagonists of that story. At some point, the narrative takes on a life of its own that may not reflect your experience. McCartney alluded to that in the 2013 song \"Early Days.\" \"Now everybody seems to have their own opinion on who did this and who did that,\" he sang. \"But as for me, I don't see how they can remember when they weren't where it was at.\" So, I’ll try not to mythologize the Beatles too much as I describe my experience photographing Sir Paul McCartney last month. I will, of course, fail spectacularly at that mission. The crowd ranged from seniors to teens in Sgt. Pepper costumes. Will Shanklin for Engadget Months before I watched him play for nearly three hours in front of 15,000 fans (at age 83!) at Albuquerque’s Isleta Amphitheater, I sent a press request to his team. A few days before the concert, I learned that my photography pass had been approved. Once it sank in, I screamed and giggled, not unlike the teenagers in Ed Sullivan's audience. (Don't judge those gals until you've been near a Beatle!) But there wasn’t much time to soak up the excitement. Without any real cameras on hand — my iPhone 17 Pro certainly wasn’t going to cut it — and only a few days to prepare, some quick decisions were in order. After enough internal debate to make my head spin off its axis, I settled on an oddball combination. For the body, I went with the Canon EOS R50, an ultra-compact mirrorless with a 24-megapixel APS-C sensor. Was it the best one available? Not at all. But instead of renting a $3,000 camera, I decided to buy something in my budget that I'll enjoy using for years. I'd already eyed it after handling a display model and reading Steve Dent's review. Plus, it created a fun challenge: How can a sub-$800 consumer-facing camera stand up to the unique demands of concert photography? The lens, on the other hand, is no place to mess around. So I rented the Canon EF 70-200mm f/2.8 L IS USM, a gargantuan, professional-grade telephoto one. (It's the precursor to this $2,399 one.) This choice was simple: It was by far the most concert-appropriate lens available to rent. It maintains sharpness and contrast across its long zoom range, its autofocus is fast and its f/2.8 aperture is crucial for the unique demands of stage lighting. Put the tiny camera and ginormous lens together (with this $38 adapter), and you get the odd couple you see below. To say this sucker was front-weighted would be an understatement. \"She's so heavy...\" Will Shanklin for Engadget Camera in hand (and Beatles hoodie equipped), I took my position in the tight press pen. The photography area was about 150 yards from the stage and didn’t allow for lateral movement, so ideas for creative compositions were set aside. My only option was to push that glass out to 200mm (or close to it) and fire away. Save those composition ideas for when it's time to crop. When photographing someone like Sir Paul, you ideally want an image that captures not only the man and the musician, but also that larger-than-life myth. It should be something grand that you’d want to hang on your wall. No pressure! Sir Paul's first number was the John Lennon-penned classic \"Help!\" Until this year's leg of the Got Back tour, McCartney hadn't played the song in full since 1990. We can only speculate about his reasons for pulling it out of his bag now. But I feel like the song's desperate pleas gain new poignancy in 2025. I can't count the times I've wanted to cry out to someone — anyone! — to \"Please, please help me\" after reading the news. We were huddled close enough together that I was glad I wore these $16 kneepads under my jeans. When the crowd in front of us settled down a bit, I kneeled to give my photographer cohorts more elbow room. My right knee bounced pleasantly onto the cozy leg pillow. Will Shanklin for Engadget With one song already down, the R50's burst mode was getting a workout. The stock Canon battery was still going strong, but I had these two third-party spares stashed in this camera bag to swap out if necessary. (I didn't end up needing them, despite snapping over 600 photos.) McCartney transitioned into his second number, \"Coming Up,\" the first track from 1980's McCartney II. That LP was ahead of its time, embracing synths, drum machines and other studio tricks before they became commonplace. Contemporary critics didn’t care much for it, but it later became a cult classic. That combination illustrates something about his solo career: always experimenting, sometimes misunderstood, but ultimately vindicated. Two songs were over in a flash. Macca addressed the crowd, and picture time was over. Off to leave my camera with security, and claim the faraway lawn seat I bought long before I knew I'd have press access. The rest of McCartney's set included a perfect balance of Beatles, Wings and solo numbers. (There was even an old Quarrymen song, \"In Spite of All the Danger.\") As you can see in the photos, he started on his trademark Höfner bass. But he moved on to piano, acoustic and electric guitars and ukulele. The latter was for his beautiful rendition of Harrison's \"Something.\" That number wasn’t the only point that moved me. The most notable was where he teamed with Lennon on \"I've Got a Feeling.\" Present-day McCartney singing with 1969 Lennon, who appeared on the giant screen above (via the restored rooftop concert footage in Get Back), was profound. \"I love that one because I get to sing with John again,\" he said. Will Shanklin for Engadget Sir Paul strikes me as someone who’s always looking forward. But the Got Back tour is a chance to look back. It lets us, the romanticizing fans, join him on the long and winding road from the Quarrymen to today. The entire production made me feel like a passenger on his journey. I could go on. But you don't need me to elevate Paul McCartney's musical legacy any more than you need me to explain Michael Jordan's basketball skills or Meryl Streep's acting chops. Listen to the music — and catch his tour if you can — and you'll feel it. As for the photos, my favorite is the one at the top of this article. (I also included a color version in the gallery below.) It’s the only one that (to me) captures the man, musician and myth as he plays his Höfner bass. Out of more than 600 rapidly-fired photos, one that feels just right is good enough for me. But even if they all sucked, who cares! Decades from now, I'll tell everyone at the old folks' home that, when I was young (and my heart was an open book), I snapped some pictures of Sir Paul McCartney. The story may grow more inflated by then, and maybe I’ll invent new details. But perhaps I can be forgiven for a bit of mythologizing. This article originally appeared on Engadget at https://www.engadget.com/cameras/the-gear-i-used-to-photograph-paul-mccartney-133033591.html?src=rss",
          "content": "This story about Paul McCartney begins with one of his old bandmates. \"I'm not really Beatle George,\" the ever-philosophical George Harrison once said. \"For me, Beatle George was a suit or a shirt that I once wore. And the only problem is, for the rest of my life, people are going to look at that shirt and mistake it for me.\" On one hand, that’s, well, George being George. But his quote does speak to our need to mythologize the Beatles. It’s hard not to! The music is so exquisite, influential and timeless that we look for grand stories to tell about it. We want a stronger connection to it, so we pore over biographies, interviews and documentaries. We seek meaning and purpose in their story. Still, it must be surreal to be one of the four protagonists of that story. At some point, the narrative takes on a life of its own that may not reflect your experience. McCartney alluded to that in the 2013 song \"Early Days.\" \"Now everybody seems to have their own opinion on who did this and who did that,\" he sang. \"But as for me, I don't see how they can remember when they weren't where it was at.\" So, I’ll try not to mythologize the Beatles too much as I describe my experience photographing Sir Paul McCartney last month. I will, of course, fail spectacularly at that mission. The crowd ranged from seniors to teens in Sgt. Pepper costumes. Will Shanklin for Engadget Months before I watched him play for nearly three hours in front of 15,000 fans (at age 83!) at Albuquerque’s Isleta Amphitheater, I sent a press request to his team. A few days before the concert, I learned that my photography pass had been approved. Once it sank in, I screamed and giggled, not unlike the teenagers in Ed Sullivan's audience. (Don't judge those gals until you've been near a Beatle!) But there wasn’t much time to soak up the excitement. Without any real cameras on hand — my iPhone 17 Pro certainly wasn’t going to cut it — and only a few days to prepare, some quick decisions were in order. After enough internal debate to make my head spin off its axis, I settled on an oddball combination. For the body, I went with the Canon EOS R50, an ultra-compact mirrorless with a 24-megapixel APS-C sensor. Was it the best one available? Not at all. But instead of renting a $3,000 camera, I decided to buy something in my budget that I'll enjoy using for years. I'd already eyed it after handling a display model and reading Steve Dent's review. Plus, it created a fun challenge: How can a sub-$800 consumer-facing camera stand up to the unique demands of concert photography? The lens, on the other hand, is no place to mess around. So I rented the Canon EF 70-200mm f/2.8 L IS USM, a gargantuan, professional-grade telephoto one. (It's the precursor to this $2,399 one.) This choice was simple: It was by far the most concert-appropriate lens available to rent. It maintains sharpness and contrast across its long zoom range, its autofocus is fast and its f/2.8 aperture is crucial for the unique demands of stage lighting. Put the tiny camera and ginormous lens together (with this $38 adapter), and you get the odd couple you see below. To say this sucker was front-weighted would be an understatement. \"She's so heavy...\" Will Shanklin for Engadget Camera in hand (and Beatles hoodie equipped), I took my position in the tight press pen. The photography area was about 150 yards from the stage and didn’t allow for lateral movement, so ideas for creative compositions were set aside. My only option was to push that glass out to 200mm (or close to it) and fire away. Save those composition ideas for when it's time to crop. When photographing someone like Sir Paul, you ideally want an image that captures not only the man and the musician, but also that larger-than-life myth. It should be something grand that you’d want to hang on your wall. No pressure! Sir Paul's first number was the John Lennon-penned classic \"Help!\" Until this year's leg of the Got Back tour, McCartney hadn't played the song in full since 1990. We can only speculate about his reasons for pulling it out of his bag now. But I feel like the song's desperate pleas gain new poignancy in 2025. I can't count the times I've wanted to cry out to someone — anyone! — to \"Please, please help me\" after reading the news. We were huddled close enough together that I was glad I wore these $16 kneepads under my jeans. When the crowd in front of us settled down a bit, I kneeled to give my photographer cohorts more elbow room. My right knee bounced pleasantly onto the cozy leg pillow. Will Shanklin for Engadget With one song already down, the R50's burst mode was getting a workout. The stock Canon battery was still going strong, but I had these two third-party spares stashed in this camera bag to swap out if necessary. (I didn't end up needing them, despite snapping over 600 photos.) McCartney transitioned into his second number, \"Coming Up,\" the first track from 1980's McCartney II. That LP was ahead of its time, embracing synths, drum machines and other studio tricks before they became commonplace. Contemporary critics didn’t care much for it, but it later became a cult classic. That combination illustrates something about his solo career: always experimenting, sometimes misunderstood, but ultimately vindicated. Two songs were over in a flash. Macca addressed the crowd, and picture time was over. Off to leave my camera with security, and claim the faraway lawn seat I bought long before I knew I'd have press access. The rest of McCartney's set included a perfect balance of Beatles, Wings and solo numbers. (There was even an old Quarrymen song, \"In Spite of All the Danger.\") As you can see in the photos, he started on his trademark Höfner bass. But he moved on to piano, acoustic and electric guitars and ukulele. The latter was for his beautiful rendition of Harrison's \"Something.\" That number wasn’t the only point that moved me. The most notable was where he teamed with Lennon on \"I've Got a Feeling.\" Present-day McCartney singing with 1969 Lennon, who appeared on the giant screen above (via the restored rooftop concert footage in Get Back), was profound. \"I love that one because I get to sing with John again,\" he said. Will Shanklin for Engadget Sir Paul strikes me as someone who’s always looking forward. But the Got Back tour is a chance to look back. It lets us, the romanticizing fans, join him on the long and winding road from the Quarrymen to today. The entire production made me feel like a passenger on his journey. I could go on. But you don't need me to elevate Paul McCartney's musical legacy any more than you need me to explain Michael Jordan's basketball skills or Meryl Streep's acting chops. Listen to the music — and catch his tour if you can — and you'll feel it. As for the photos, my favorite is the one at the top of this article. (I also included a color version in the gallery below.) It’s the only one that (to me) captures the man, musician and myth as he plays his Höfner bass. Out of more than 600 rapidly-fired photos, one that feels just right is good enough for me. But even if they all sucked, who cares! Decades from now, I'll tell everyone at the old folks' home that, when I was young (and my heart was an open book), I snapped some pictures of Sir Paul McCartney. The story may grow more inflated by then, and maybe I’ll invent new details. But perhaps I can be forgiven for a bit of mythologizing. This article originally appeared on Engadget at https://www.engadget.com/cameras/the-gear-i-used-to-photograph-paul-mccartney-133033591.html?src=rss",
          "feed_position": 27,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2025-11/25de9d70-ba7d-11f0-87da-aec6e2200205"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/the-google-tv-streamer-4k-drops-to-a-record-low-ahead-of-black-friday-173858907.html",
          "published_at": "Thu, 06 Nov 2025 13:05:09 +0000",
          "title": "The Google TV Streamer 4K drops to a record low ahead of Black Friday",
          "standfirst": "One of the best streaming devices you can get today is deeply discounted on Amazon ahead of Black Friday. The Google TV Streamer is going for just $75 right now — the lowest price we've seen it hit yet. The device normally costs $100. The Amazon deal applies to both color options, White and the soft gray Haze. The Google TV Streamer is our top pick for an all-in-one streaming device. It has a faster processor than Google's previous streaming devices (22 percent faster, according to the company), so you can switch between apps and different media without lagging. It also has more storage and memory, at 32GB and 4GB, respectively. Google TV streamer has an intuitive interface and keeps all of your favorite content from different streaming apps organized in one menu. It also seamlessly integrates Google Home, allowing you to control your smart home devices from a slideout panel on the TV. The 4K streamer comes in a set-top wedge design, rather than the dongle of Chromecasts past, but you'll have to pick up an HDMI cable separately if you don't already have one you can use. It comes with a small remote that you can ping by pressing a button on the streamer for when you inevitably misplace it. In her review of the device, Engadget's Amy Skorheim called the Google TV streamer \"a full-featured, competent device with an interface that’s better than most at pulling together all the disparate threads of a streaming experience.\" One of its only downsides is the relatively high cost at $100, so don't let this deal go to waste. In addition to the streaming device, Google has a bunch of other tech on sale for Black Friday. The entry-level Nest thermostat is on sale for $90 right now, and the Nest Wi-Fi Pro 6E router has dropped to $120 for a single-pack; that's 40 percent off. This article originally appeared on Engadget at https://www.engadget.com/deals/the-google-tv-streamer-4k-drops-to-a-record-low-ahead-of-black-friday-173858907.html?src=rss",
          "content": "One of the best streaming devices you can get today is deeply discounted on Amazon ahead of Black Friday. The Google TV Streamer is going for just $75 right now — the lowest price we've seen it hit yet. The device normally costs $100. The Amazon deal applies to both color options, White and the soft gray Haze. The Google TV Streamer is our top pick for an all-in-one streaming device. It has a faster processor than Google's previous streaming devices (22 percent faster, according to the company), so you can switch between apps and different media without lagging. It also has more storage and memory, at 32GB and 4GB, respectively. Google TV streamer has an intuitive interface and keeps all of your favorite content from different streaming apps organized in one menu. It also seamlessly integrates Google Home, allowing you to control your smart home devices from a slideout panel on the TV. The 4K streamer comes in a set-top wedge design, rather than the dongle of Chromecasts past, but you'll have to pick up an HDMI cable separately if you don't already have one you can use. It comes with a small remote that you can ping by pressing a button on the streamer for when you inevitably misplace it. In her review of the device, Engadget's Amy Skorheim called the Google TV streamer \"a full-featured, competent device with an interface that’s better than most at pulling together all the disparate threads of a streaming experience.\" One of its only downsides is the relatively high cost at $100, so don't let this deal go to waste. In addition to the streaming device, Google has a bunch of other tech on sale for Black Friday. The entry-level Nest thermostat is on sale for $90 right now, and the Nest Wi-Fi Pro 6E router has dropped to $120 for a single-pack; that's 40 percent off. This article originally appeared on Engadget at https://www.engadget.com/deals/the-google-tv-streamer-4k-drops-to-a-record-low-ahead-of-black-friday-173858907.html?src=rss",
          "feed_position": 28
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/google-debuts-ai-chips-with-4x-performance-boost-secures-anthropic-megadeal",
          "published_at": "Thu, 06 Nov 2025 13:00:00 GMT",
          "title": "Google debuts AI chips with 4X performance boost, secures Anthropic megadeal worth billions",
          "standfirst": "Google Cloud is introducing what it calls its most powerful artificial intelligence infrastructure to date, unveiling a seventh-generation Tensor Processing Unit and expanded Arm-based computing options designed to meet surging demand for AI model deployment — what the company characterizes as a fundamental industry shift from training models to serving them to billions of users.The announcement, made Thursday, centers on Ironwood, Google&#x27;s latest custom AI accelerator chip, which will become generally available in the coming weeks. In a striking validation of the technology, Anthropic, the AI safety company behind the Claude family of models, disclosed plans to access up to one million of these TPU chips — a commitment worth tens of billions of dollars and among the largest known AI infrastructure deals to date.The move underscores an intensifying competition among cloud providers to control the infrastructure layer powering artificial intelligence, even as questions mount about whether the industry can sustain its current pace of capital expenditure. Google&#x27;s approach — building custom silicon rather than relying solely on Nvidia&#x27;s dominant GPU chips — amounts to a long-term bet that vertical integration from chip design through software will deliver superior economics and performance.Why companies are racing to serve AI models, not just train themGoogle executives framed the announcements around what they call \"the age of inference\" — a transition point where companies shift resources from training frontier AI models to deploying them in production applications serving millions or billions of requests daily.\"Today&#x27;s frontier models, including Google&#x27;s Gemini, Veo, and Imagen and Anthropic&#x27;s Claude train and serve on Tensor Processing Units,\" said Amin Vahdat, vice president and general manager of AI and Infrastructure at Google Cloud. \"For many organizations, the focus is shifting from training these models to powering useful, responsive interactions with them.\"This transition has profound implications for infrastructure requirements. Where training workloads can often tolerate batch processing and longer completion times, inference — the process of actually running a trained model to generate responses — demands consistently low latency, high throughput, and unwavering reliability. A chatbot that takes 30 seconds to respond, or a coding assistant that frequently times out, becomes unusable regardless of the underlying model&#x27;s capabilities.Agentic workflows — where AI systems take autonomous actions rather than simply responding to prompts — create particularly complex infrastructure challenges, requiring tight coordination between specialized AI accelerators and general-purpose computing.Inside Ironwood&#x27;s architecture: 9,216 chips working as one supercomputerIronwood is more than incremental improvement over Google&#x27;s sixth-generation TPUs. According to technical specifications shared by the company, it delivers more than four times better performance for both training and inference workloads compared to its predecessor — gains that Google attributes to a system-level co-design approach rather than simply increasing transistor counts.The architecture&#x27;s most striking feature is its scale. A single Ironwood \"pod\" — a tightly integrated unit of TPU chips functioning as one supercomputer — can connect up to 9,216 individual chips through Google&#x27;s proprietary Inter-Chip Interconnect network operating at 9.6 terabits per second. To put that bandwidth in perspective, it&#x27;s roughly equivalent to downloading the entire Library of Congress in under two seconds.This massive interconnect fabric allows the 9,216 chips to share access to 1.77 petabytes of High Bandwidth Memory — memory fast enough to keep pace with the chips&#x27; processing speeds. That&#x27;s approximately 40,000 high-definition Blu-ray movies&#x27; worth of working memory, instantly accessible by thousands of processors simultaneously. \"For context, that means Ironwood Pods can deliver 118x more FP8 ExaFLOPS versus the next closest competitor,\" Google stated in technical documentation.The system employs Optical Circuit Switching technology that acts as a \"dynamic, reconfigurable fabric.\" When individual components fail or require maintenance — inevitable at this scale — the OCS technology automatically reroutes data traffic around the interruption within milliseconds, allowing workloads to continue running without user-visible disruption.This reliability focus reflects lessons learned from deploying five previous TPU generations. Google reported that its fleet-wide uptime for liquid-cooled systems has maintained approximately 99.999% availability since 2020 — equivalent to less than six minutes of downtime per year.Anthropic&#x27;s billion-dollar bet validates Google&#x27;s custom silicon strategyPerhaps the most significant external validation of Ironwood&#x27;s capabilities comes from Anthropic&#x27;s commitment to access up to one million TPU chips — a staggering figure in an industry where even clusters of 10,000 to 50,000 accelerators are considered massive.\"Anthropic and Google have a longstanding partnership and this latest expansion will help us continue to grow the compute we need to define the frontier of AI,\" said Krishna Rao, Anthropic&#x27;s chief financial officer, in the official partnership agreement. \"Our customers — from Fortune 500 companies to AI-native startups — depend on Claude for their most important work, and this expanded capacity ensures we can meet our exponentially growing demand.\"According to a separate statement, Anthropic will have access to \"well over a gigawatt of capacity coming online in 2026\" — enough electricity to power a small city. The company specifically cited TPUs&#x27; \"price-performance and efficiency\" as key factors in the decision, along with \"existing experience in training and serving its models with TPUs.\"Industry analysts estimate that a commitment to access one million TPU chips, with associated infrastructure, networking, power, and cooling, likely represents a multi-year contract worth tens of billions of dollars — among the largest known cloud infrastructure commitments in history.James Bradbury, Anthropic&#x27;s head of compute, elaborated on the inference focus: \"Ironwood&#x27;s improvements in both inference performance and training scalability will help us scale efficiently while maintaining the speed and reliability our customers expect.\"Google&#x27;s Axion processors target the computing workloads that make AI possibleAlongside Ironwood, Google introduced expanded options for its Axion processor family — custom Arm-based CPUs designed for general-purpose workloads that support AI applications but don&#x27;t require specialized accelerators.The N4A instance type, now entering preview, targets what Google describes as \"microservices, containerized applications, open-source databases, batch, data analytics, development environments, experimentation, data preparation and web serving jobs that make AI applications possible.\" The company claims N4A delivers up to 2X better price-performance than comparable current-generation x86-based virtual machines.Google is also previewing C4A metal, its first bare-metal Arm instance, which provides dedicated physical servers for specialized workloads such as Android development, automotive systems, and software with strict licensing requirements.The Axion strategy reflects a growing conviction that the future of computing infrastructure requires both specialized AI accelerators and highly efficient general-purpose processors. While a TPU handles the computationally intensive task of running an AI model, Axion-class processors manage data ingestion, preprocessing, application logic, API serving, and countless other tasks in a modern AI application stack.Early customer results suggest the approach delivers measurable economic benefits. Vimeo reported observing \"a 30% improvement in performance for our core transcoding workload compared to comparable x86 VMs\" in initial N4A tests. ZoomInfo measured \"a 60% improvement in price-performance\" for data processing pipelines running on Java services, according to Sergei Koren, the company&#x27;s chief infrastructure architect.Software tools turn raw silicon performance into developer productivityHardware performance means little if developers cannot easily harness it. Google emphasized that Ironwood and Axion are integrated into what it calls AI Hypercomputer — \"an integrated supercomputing system that brings together compute, networking, storage, and software to improve system-level performance and efficiency.\"According to an October 2025 IDC Business Value Snapshot study, AI Hypercomputer customers achieved on average 353% three-year return on investment, 28% lower IT costs, and 55% more efficient IT teams.Google disclosed several software enhancements designed to maximize Ironwood utilization. Google Kubernetes Engine now offers advanced maintenance and topology awareness for TPU clusters, enabling intelligent scheduling and highly resilient deployments. The company&#x27;s open-source MaxText framework now supports advanced training techniques including Supervised Fine-Tuning and Generative Reinforcement Policy Optimization.Perhaps most significant for production deployments, Google&#x27;s Inference Gateway intelligently load-balances requests across model servers to optimize critical metrics. According to Google, it can reduce time-to-first-token latency by 96% and serving costs by up to 30% through techniques like prefix-cache-aware routing.The Inference Gateway monitors key metrics including KV cache hits, GPU or TPU utilization, and request queue length, then routes incoming requests to the optimal replica. For conversational AI applications where multiple requests might share context, routing requests with shared prefixes to the same server instance can dramatically reduce redundant computation.The hidden challenge: powering and cooling one-megawatt server racksBehind these announcements lies a massive physical infrastructure challenge that Google addressed at the recent Open Compute Project EMEA Summit. The company disclosed that it&#x27;s implementing +/-400 volt direct current power delivery capable of supporting up to one megawatt per rack — a tenfold increase from typical deployments.\"The AI era requires even greater power delivery capabilities,\" explained Madhusudan Iyengar and Amber Huffman, Google principal engineers, in an April 2025 blog post. \"ML will require more than 500 kW per IT rack before 2030.\"Google is collaborating with Meta and Microsoft to standardize electrical and mechanical interfaces for high-voltage DC distribution. The company selected 400 VDC specifically to leverage the supply chain established by electric vehicles, \"for greater economies of scale, more efficient manufacturing, and improved quality and scale.\"On cooling, Google revealed it will contribute its fifth-generation cooling distribution unit design to the Open Compute Project. The company has deployed liquid cooling \"at GigaWatt scale across more than 2,000 TPU Pods in the past seven years\" with fleet-wide availability of approximately 99.999%.Water can transport approximately 4,000 times more heat per unit volume than air for a given temperature change — critical as individual AI accelerator chips increasingly dissipate 1,000 watts or more.Custom silicon gambit challenges Nvidia&#x27;s AI accelerator dominanceGoogle&#x27;s announcements come as the AI infrastructure market reaches an inflection point. While Nvidia maintains overwhelming dominance in AI accelerators — holding an estimated 80-95% market share — cloud providers are increasingly investing in custom silicon to differentiate their offerings and improve unit economics.Amazon Web Services pioneered this approach with Graviton Arm-based CPUs and Inferentia / Trainium AI chips. Microsoft has developed Cobalt processors and is reportedly working on AI accelerators. Google now offers the most comprehensive custom silicon portfolio among major cloud providers.The strategy faces inherent challenges. Custom chip development requires enormous upfront investment — often billions of dollars. The software ecosystem for specialized accelerators lags behind Nvidia&#x27;s CUDA platform, which benefits from 15+ years of developer tools. And rapid AI model architecture evolution creates risk that custom silicon optimized for today&#x27;s models becomes less relevant as new techniques emerge.Yet Google argues its approach delivers unique advantages. \"This is how we built the first TPU ten years ago, which in turn unlocked the invention of the Transformer eight years ago — the very architecture that powers most of modern AI,\" the company noted, referring to the seminal \"Attention Is All You Need\" paper from Google researchers in 2017.The argument is that tight integration — \"model research, software, and hardware development under one roof\" — enables optimizations impossible with off-the-shelf components.Beyond Anthropic, several other customers provided early feedback. Lightricks, which develops creative AI tools, reported that early Ironwood testing \"makes us highly enthusiastic\" about creating \"more nuanced, precise, and higher-fidelity image and video generation for our millions of global customers,\" said Yoav HaCohen, the company&#x27;s research director.Google&#x27;s announcements raise questions that will play out over coming quarters. Can the industry sustain current infrastructure spending, with major AI companies collectively committing hundreds of billions of dollars? Will custom silicon prove economically superior to Nvidia GPUs? How will model architectures evolve?For now, Google appears committed to a strategy that has defined the company for decades: building custom infrastructure to enable applications impossible on commodity hardware, then making that infrastructure available to customers who want similar capabilities without the capital investment.As the AI industry transitions from research labs to production deployments serving billions of users, that infrastructure layer — the silicon, software, networking, power, and cooling that make it all run — may prove as important as the models themselves.And if Anthropic&#x27;s willingness to commit to accessing up to one million chips is any indication, Google&#x27;s bet on custom silicon designed specifically for the age of inference may be paying off just as demand reaches its inflection point.",
          "content": "Google Cloud is introducing what it calls its most powerful artificial intelligence infrastructure to date, unveiling a seventh-generation Tensor Processing Unit and expanded Arm-based computing options designed to meet surging demand for AI model deployment — what the company characterizes as a fundamental industry shift from training models to serving them to billions of users.The announcement, made Thursday, centers on Ironwood, Google&#x27;s latest custom AI accelerator chip, which will become generally available in the coming weeks. In a striking validation of the technology, Anthropic, the AI safety company behind the Claude family of models, disclosed plans to access up to one million of these TPU chips — a commitment worth tens of billions of dollars and among the largest known AI infrastructure deals to date.The move underscores an intensifying competition among cloud providers to control the infrastructure layer powering artificial intelligence, even as questions mount about whether the industry can sustain its current pace of capital expenditure. Google&#x27;s approach — building custom silicon rather than relying solely on Nvidia&#x27;s dominant GPU chips — amounts to a long-term bet that vertical integration from chip design through software will deliver superior economics and performance.Why companies are racing to serve AI models, not just train themGoogle executives framed the announcements around what they call \"the age of inference\" — a transition point where companies shift resources from training frontier AI models to deploying them in production applications serving millions or billions of requests daily.\"Today&#x27;s frontier models, including Google&#x27;s Gemini, Veo, and Imagen and Anthropic&#x27;s Claude train and serve on Tensor Processing Units,\" said Amin Vahdat, vice president and general manager of AI and Infrastructure at Google Cloud. \"For many organizations, the focus is shifting from training these models to powering useful, responsive interactions with them.\"This transition has profound implications for infrastructure requirements. Where training workloads can often tolerate batch processing and longer completion times, inference — the process of actually running a trained model to generate responses — demands consistently low latency, high throughput, and unwavering reliability. A chatbot that takes 30 seconds to respond, or a coding assistant that frequently times out, becomes unusable regardless of the underlying model&#x27;s capabilities.Agentic workflows — where AI systems take autonomous actions rather than simply responding to prompts — create particularly complex infrastructure challenges, requiring tight coordination between specialized AI accelerators and general-purpose computing.Inside Ironwood&#x27;s architecture: 9,216 chips working as one supercomputerIronwood is more than incremental improvement over Google&#x27;s sixth-generation TPUs. According to technical specifications shared by the company, it delivers more than four times better performance for both training and inference workloads compared to its predecessor — gains that Google attributes to a system-level co-design approach rather than simply increasing transistor counts.The architecture&#x27;s most striking feature is its scale. A single Ironwood \"pod\" — a tightly integrated unit of TPU chips functioning as one supercomputer — can connect up to 9,216 individual chips through Google&#x27;s proprietary Inter-Chip Interconnect network operating at 9.6 terabits per second. To put that bandwidth in perspective, it&#x27;s roughly equivalent to downloading the entire Library of Congress in under two seconds.This massive interconnect fabric allows the 9,216 chips to share access to 1.77 petabytes of High Bandwidth Memory — memory fast enough to keep pace with the chips&#x27; processing speeds. That&#x27;s approximately 40,000 high-definition Blu-ray movies&#x27; worth of working memory, instantly accessible by thousands of processors simultaneously. \"For context, that means Ironwood Pods can deliver 118x more FP8 ExaFLOPS versus the next closest competitor,\" Google stated in technical documentation.The system employs Optical Circuit Switching technology that acts as a \"dynamic, reconfigurable fabric.\" When individual components fail or require maintenance — inevitable at this scale — the OCS technology automatically reroutes data traffic around the interruption within milliseconds, allowing workloads to continue running without user-visible disruption.This reliability focus reflects lessons learned from deploying five previous TPU generations. Google reported that its fleet-wide uptime for liquid-cooled systems has maintained approximately 99.999% availability since 2020 — equivalent to less than six minutes of downtime per year.Anthropic&#x27;s billion-dollar bet validates Google&#x27;s custom silicon strategyPerhaps the most significant external validation of Ironwood&#x27;s capabilities comes from Anthropic&#x27;s commitment to access up to one million TPU chips — a staggering figure in an industry where even clusters of 10,000 to 50,000 accelerators are considered massive.\"Anthropic and Google have a longstanding partnership and this latest expansion will help us continue to grow the compute we need to define the frontier of AI,\" said Krishna Rao, Anthropic&#x27;s chief financial officer, in the official partnership agreement. \"Our customers — from Fortune 500 companies to AI-native startups — depend on Claude for their most important work, and this expanded capacity ensures we can meet our exponentially growing demand.\"According to a separate statement, Anthropic will have access to \"well over a gigawatt of capacity coming online in 2026\" — enough electricity to power a small city. The company specifically cited TPUs&#x27; \"price-performance and efficiency\" as key factors in the decision, along with \"existing experience in training and serving its models with TPUs.\"Industry analysts estimate that a commitment to access one million TPU chips, with associated infrastructure, networking, power, and cooling, likely represents a multi-year contract worth tens of billions of dollars — among the largest known cloud infrastructure commitments in history.James Bradbury, Anthropic&#x27;s head of compute, elaborated on the inference focus: \"Ironwood&#x27;s improvements in both inference performance and training scalability will help us scale efficiently while maintaining the speed and reliability our customers expect.\"Google&#x27;s Axion processors target the computing workloads that make AI possibleAlongside Ironwood, Google introduced expanded options for its Axion processor family — custom Arm-based CPUs designed for general-purpose workloads that support AI applications but don&#x27;t require specialized accelerators.The N4A instance type, now entering preview, targets what Google describes as \"microservices, containerized applications, open-source databases, batch, data analytics, development environments, experimentation, data preparation and web serving jobs that make AI applications possible.\" The company claims N4A delivers up to 2X better price-performance than comparable current-generation x86-based virtual machines.Google is also previewing C4A metal, its first bare-metal Arm instance, which provides dedicated physical servers for specialized workloads such as Android development, automotive systems, and software with strict licensing requirements.The Axion strategy reflects a growing conviction that the future of computing infrastructure requires both specialized AI accelerators and highly efficient general-purpose processors. While a TPU handles the computationally intensive task of running an AI model, Axion-class processors manage data ingestion, preprocessing, application logic, API serving, and countless other tasks in a modern AI application stack.Early customer results suggest the approach delivers measurable economic benefits. Vimeo reported observing \"a 30% improvement in performance for our core transcoding workload compared to comparable x86 VMs\" in initial N4A tests. ZoomInfo measured \"a 60% improvement in price-performance\" for data processing pipelines running on Java services, according to Sergei Koren, the company&#x27;s chief infrastructure architect.Software tools turn raw silicon performance into developer productivityHardware performance means little if developers cannot easily harness it. Google emphasized that Ironwood and Axion are integrated into what it calls AI Hypercomputer — \"an integrated supercomputing system that brings together compute, networking, storage, and software to improve system-level performance and efficiency.\"According to an October 2025 IDC Business Value Snapshot study, AI Hypercomputer customers achieved on average 353% three-year return on investment, 28% lower IT costs, and 55% more efficient IT teams.Google disclosed several software enhancements designed to maximize Ironwood utilization. Google Kubernetes Engine now offers advanced maintenance and topology awareness for TPU clusters, enabling intelligent scheduling and highly resilient deployments. The company&#x27;s open-source MaxText framework now supports advanced training techniques including Supervised Fine-Tuning and Generative Reinforcement Policy Optimization.Perhaps most significant for production deployments, Google&#x27;s Inference Gateway intelligently load-balances requests across model servers to optimize critical metrics. According to Google, it can reduce time-to-first-token latency by 96% and serving costs by up to 30% through techniques like prefix-cache-aware routing.The Inference Gateway monitors key metrics including KV cache hits, GPU or TPU utilization, and request queue length, then routes incoming requests to the optimal replica. For conversational AI applications where multiple requests might share context, routing requests with shared prefixes to the same server instance can dramatically reduce redundant computation.The hidden challenge: powering and cooling one-megawatt server racksBehind these announcements lies a massive physical infrastructure challenge that Google addressed at the recent Open Compute Project EMEA Summit. The company disclosed that it&#x27;s implementing +/-400 volt direct current power delivery capable of supporting up to one megawatt per rack — a tenfold increase from typical deployments.\"The AI era requires even greater power delivery capabilities,\" explained Madhusudan Iyengar and Amber Huffman, Google principal engineers, in an April 2025 blog post. \"ML will require more than 500 kW per IT rack before 2030.\"Google is collaborating with Meta and Microsoft to standardize electrical and mechanical interfaces for high-voltage DC distribution. The company selected 400 VDC specifically to leverage the supply chain established by electric vehicles, \"for greater economies of scale, more efficient manufacturing, and improved quality and scale.\"On cooling, Google revealed it will contribute its fifth-generation cooling distribution unit design to the Open Compute Project. The company has deployed liquid cooling \"at GigaWatt scale across more than 2,000 TPU Pods in the past seven years\" with fleet-wide availability of approximately 99.999%.Water can transport approximately 4,000 times more heat per unit volume than air for a given temperature change — critical as individual AI accelerator chips increasingly dissipate 1,000 watts or more.Custom silicon gambit challenges Nvidia&#x27;s AI accelerator dominanceGoogle&#x27;s announcements come as the AI infrastructure market reaches an inflection point. While Nvidia maintains overwhelming dominance in AI accelerators — holding an estimated 80-95% market share — cloud providers are increasingly investing in custom silicon to differentiate their offerings and improve unit economics.Amazon Web Services pioneered this approach with Graviton Arm-based CPUs and Inferentia / Trainium AI chips. Microsoft has developed Cobalt processors and is reportedly working on AI accelerators. Google now offers the most comprehensive custom silicon portfolio among major cloud providers.The strategy faces inherent challenges. Custom chip development requires enormous upfront investment — often billions of dollars. The software ecosystem for specialized accelerators lags behind Nvidia&#x27;s CUDA platform, which benefits from 15+ years of developer tools. And rapid AI model architecture evolution creates risk that custom silicon optimized for today&#x27;s models becomes less relevant as new techniques emerge.Yet Google argues its approach delivers unique advantages. \"This is how we built the first TPU ten years ago, which in turn unlocked the invention of the Transformer eight years ago — the very architecture that powers most of modern AI,\" the company noted, referring to the seminal \"Attention Is All You Need\" paper from Google researchers in 2017.The argument is that tight integration — \"model research, software, and hardware development under one roof\" — enables optimizations impossible with off-the-shelf components.Beyond Anthropic, several other customers provided early feedback. Lightricks, which develops creative AI tools, reported that early Ironwood testing \"makes us highly enthusiastic\" about creating \"more nuanced, precise, and higher-fidelity image and video generation for our millions of global customers,\" said Yoav HaCohen, the company&#x27;s research director.Google&#x27;s announcements raise questions that will play out over coming quarters. Can the industry sustain current infrastructure spending, with major AI companies collectively committing hundreds of billions of dollars? Will custom silicon prove economically superior to Nvidia GPUs? How will model architectures evolve?For now, Google appears committed to a strategy that has defined the company for decades: building custom infrastructure to enable applications impossible on commodity hardware, then making that infrastructure available to customers who want similar capabilities without the capital investment.As the AI industry transitions from research labs to production deployments serving billions of users, that infrastructure layer — the silicon, software, networking, power, and cooling that make it all run — may prove as important as the models themselves.And if Anthropic&#x27;s willingness to commit to accessing up to one million chips is any indication, Google&#x27;s bet on custom silicon designed specifically for the age of inference may be paying off just as demand reaches its inflection point.",
          "feed_position": 1,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/3wiaJuUTbBrUaBp8cSXtO4/81095c7817da6a2967a961ed60356ed4/Ironwood_board.jpg?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/best-mesh-wifi-system-130028701.html",
          "published_at": "Thu, 06 Nov 2025 10:01:26 +0000",
          "title": "The best mesh Wi-Fi systems of 2025",
          "standfirst": "A Wi-Fi mesh system is one of the best upgrades you can make to improve your home’s internet coverage, especially if you’ve ever struggled with dead zones, buffering or dropped video calls. Unlike a single router that broadcasts from one central point, a mesh system uses multiple access points spread throughout your space to blanket your entire home with a strong, seamless Wi-Fi signal. Whether you're working on multiple laptops, streaming 4K video in the living room or gaming online in the basement, a mesh setup helps ensure you get reliable Wi-Fi wherever you are.These systems are designed to handle the demands of modern households, offering features like dual-band or even tri-band connectivity to balance your bandwidth across devices, and automatic updates to keep your firmware current. Many also support high-speed internet plans and include options for a wired connection if you need extra stability for gaming or work. With easy setup, smart app controls and long-term future-proofing, the best mesh Wi-Fi router systems can eliminate the need for clunky Wi-Fi extenders and give you fast, dependable Wi-Fi connections across your whole home. Table of contents Best mesh Wi-Fi systems for 2025 What to look for in a mesh Wi-Fi system Other mesh Wi-Fi router systems we tested How we test Wi-Fi routers Mesh Wi-Fi system FAQs Best mesh Wi-Fi systems for 2025 What to look for in a mesh Wi-Fi system Linksys’ CEO Jonathan Bettino told Engadget why mesh systems are an “advancement in Wi-Fi technology” over buying a single point router. With one transmitter, the signal can degrade the further away from the router you go, or the local environment isn’t ideal. “You can have a small [home], but there’s thick walls [...] or things in the way that just interfere with your wireless signal,” he said. Historically, the solution to a home’s Wi-Fi dead zone was to buy a Wi-Fi extender but Bettino said the hardware has both a “terrible user experience” and one of the highest return rates of any consumer electronics product. Mesh Wi-Fi, by comparison, offers “multiple nodes that can be placed anywhere in your home,” says Bettino, resulting in “ubiquitous Wi-Fi” that feels as if you have a “router in every room.” Rather than having one main router in your home, having a “router in every room” is the biggest selling point for mesh Wi-Fi given how reliant we all are on the internet. Each node is in constant contact with each other, broadcasting a single, seamless network to all of your connected devices. There’s no separate network for the 2.4GHz and 5GHz bands, just a single name that you connect to. It’s a good time to buy a mesh Wi-Fi system since the latest standard, Wi-Fi 6E, represents a big leap in the technology. Matt MacPherson, Cisco's Chief Technology Officer for Wireless, said Wi-Fi 6E is a big “inflection point,” using much more of the wireless spectrum than its predecessors. “If you’re using that spectrum with a Wi-Fi 6 [device],” he said, “you’re going to get significant gains [in speed.]” MacPherson added Wi-Fi 6E will likely “carry you for a long time” thanks to the fact its “top throughputs now typically exceed what people can actually connect their home to.” In short, with a top theoretical per-stream speed of 1.2 Gbps, Wi-Fi 6E is fast enough to outrun all but the fastest internet service. What do all these Wi-Fi numbers and letters mean? I’m sorry folks, we need to get boringly technical for one paragraph, but I promise you it’s worth it. Wi-Fi is governed by International Standard IEEE 802.11, and every few years a letter gets added onto that name when the technology evolves and improves. Until 2019, routers were sold under their IEEE name, leaving users to pick through the word soup of a product labeled 802.11 b/g/a/n/ac and so on. Mercifully, wiser heads opted to rebrand the letters as numbers, so rather than 802.11 b/g/a/n/ac, we have Wi-Fi 1, 2, 3 4 and 5. Right now, we’re in the middle of one of those Wi-Fi generations, with most of the gear on sale right now supporting either Wi-Fi 6 or Wi-Fi 6E. What’s the difference between Wi-Fi 6 and Wi-Fi 6E? Wi-Fi uses chunks of the radio frequency spectrum, with Wi-Fi 6 using the 2.4GHz and 5GHz bands to pump data around. In fact, back in the old days, it was likely your home router would offer you the choice of the 2.4GHz or the 5GHz network, as separate bands to access. These days, all of the spectrums are tied together as one thing, and Wi-Fi 6E has the added ability to use the 6GHz band as well. That’s a big chunk of extra wireless real estate that isn’t as cluttered up as the 2.4 and 5GHz bands. You’re going to talk about wireless frequencies now, aren’t you. Each Wi-Fi band had tradeoffs, because the slower radio frequencies have greater range but less speed. 2.4GHz signals will travel a long way in your home but aren’t quick, while 6GHz is blisteringly fast but can be defeated by a sturdy brick wall. A lot of Wi-Fi-enabled gear you own, like smart home products, only use the 2.4GHz band because the range is better and it’s a lot cheaper. But it means that the band is also overcrowded and slow, making it great for your doorbell and robovac, but lackluster for Twitch streaming. So, what am I looking for? Right now, the market is full of mature Wi-Fi 6 and 6E devices, and most new systems available to buy are capable of taking advantage of the faster speeds they offer. This guide focuses on Wi-Fi 6E gear since it’s what we think it’s more than enough to satisfy almost everyone’s at-home Wi-Fi needs. What about Wi-Fi 7? We’re now seeing the first generation of Wi-Fi 7 devices available to buy, but we don’t recommend you do so immediately. The Wi-Fi 7 standard is still so new that there’s little to no reason for you to rush out and buy one for your home. The hardware is tremendously expensive and while Wi-Fi 7 will, eventually, offer some great benefits over 6E, it’s not as transformative an upgrade as 6E. Not to mention, Wi-Fi 7 is so new that almost none of your home’s devices will be able to take advantage of its big-ticket features. I’d estimate you won’t need to worry about upgrading to Wi-Fi 7 for at least five years, if not longer. Range and speed All Wi-Fi routers boast a theoretical broadcast range and a theoretical top speed, and in some cases external antennas to boost signal directionality — but these figures don’t mean much. After all, manufacturers can’t control your ISP’s real speed, the materials and layout of your home or where you put your Wi-Fi gear. Raw speed isn’t everything, either, and you likely need a lot less than the internet speeds your provider is advertising. What matters more is how consistent your connection is between rooms and across devices.. After all, Netflix needs just 15 Mbps to push a single 4K video stream to your home. As cool as it is to say you’ve got all these hundreds of Mbps, factors like latency and reliability are far more crucial to a happy internet life. And unless you have Gigabit internet that can reach speeds of up to 1 Gbps, you won’t need a mesh router that offers that spec. Backhaul Mesh Wi-Fi systems work by connecting every hardware node to a single wireless network, letting them all communicate with each other. Imagine four people in a busy, noisy restaurant all trying to order their dinner from a weary staff member, all at once. Now imagine, while this is going on, that four more people at that same table are also trying to tell a funny anecdote. It’s no surprise that it might take a long time for the right information to reach its intended destination. To combat this, higher-end mesh routers offer dedicated wireless backhaul; a slice of the spectrum for node-to-node communication. So rather than everyone talking at once in the same space, the conversations are essentially separated, reducing the invisible clutter in the air. Because there’s less confusing cross-chatter, everything moves faster, offering a significant performance boost to those systems. Connectivity These days, even your washing machine can have a wireless connection, but that doesn’t mean you should ignore the joys of wired internet. No matter how fast Wi-Fi is, a hard line will always be faster, and some gear, like Philips’ Hue bridge, still needs an ethernet connection. Plenty of routers can also use these hard connections as backhaul, eliminating further wireless clutter. It’s convenient for spread-out systems and power users, but it will mean running more wires through your home. The most common standard is Cat 5e, or gigabit ethernet which, unsurprisingly, has a top speed of 1 Gigabit per second (Gbps). Since Ethernet cables are backward compatible, you should be able to easily find one that works with your system. However, to get the most out of your mesh routers, it’s worth investing in an Ethernet cable that meets the standard your router uses — if it’s Cat 5e, use a Cat 5e cable. You can check your router’s specs via the manufacturer’s website to be sure. Flexibility and scalability Mesh routers enable you to add (or subtract) modules from your home network to suit your needs. D-Link’s Alan Jones said users should “check how scalable the prospective product is” before you buy. This sense of scale doesn’t just apply to the number of nodes on the network, but how many simultaneous connections it can handle. It’s also worth looking at ASUS’ AiMesh products, which can combine mesh Wi-Fi gear and its standard “spider” Wi-Fi routers. If you’ve got a tricky part of your home, you can bolt on an ultra-power standalone Wi-Fi router to a compatible mesh. Placement Mesh networks replace one big piece of hardware with a series of identical nodes that you scatter around your home. You connect one to your modem (usually over ethernet), and then scatter the rest around the place for the best coverage. A good rule of thumb is to place each node no more than two rooms away from the last one, rather than sticking them at the far ends of your home. Bear in mind, every physical obstacle between a Wi-Fi node, its siblings and your devices will hurt your overall performance. You should aim to place them, at the very least, at waist height on furniture in open air, without too many obstructions. The reason many mesh Wi-Fi products are designed to look like an inoffensive white doodad is so you don’t feel compelled to hide them behind your TV. Other mesh Wi-Fi router systems we tested Amazon Eero Pro 7 Eero built its reputation on easy to use yet powerful mesh systems that offer a lot of good in a relatively small and affordable package. Setup is effortless, the app running things is clean and simple, and you get the added benefit of backwards compatibility with older hardware. Sadly, the issue with every Eero system is that so many basic management features, like parental controls, are paywalled behind the company’s Eero Plus subscription for $100 a year. Amazon Eero 6E Eero Pro 6E is an “easy” device, the sort a total novice can set up on their own and thrive with for years on end. There’s little brainwork required to get things set up, and the app has a clean UI with plenty of hand-holding. But, as with the Eero Pro 7, the fact that so many basic management tools are paywalled irks me, especially since you can get plenty of them for free with Google’s rival offering. Netgear Orbi 960 The Orbi 96T0 (RBKE963) is Netgear’s flagship mesh Wi-Fi product, which the company calls the “world’s most powerful Wi-Fi 6E system.” It’s also one of the most expensive consumer-level kits on the market, setting you back $1,499.99 for a three pack. It's a fantastic piece of gear, but it's worth saying that the subset of people who could, would or should buy it remains far smaller than you might expect. Ultimately, I feel that if you’re paying luxury prices, you should expect a luxury product. There were plenty of times during testing that I went looking for a feature that was either only available via the web client, or behind a paywall. While, yes, much of your cash is going to the superlative hardware, but for this sort of money, the fact you have to pay extra for some table-stakes features is insulting. If you’re looking for a new Wi-Fi system and aren’t prepared to spend almost $1,500, it’s worth considering our other top picks for the best Wi-Fi routers and mesh systems. How we test Wi-Fi routers My home covers around 2,200 square feet across three stories with the office on the third floor. It’s relatively long and thin, with the living room at the front of the house, the kitchen at the back and the three bedrooms on the first floor. Its age means there are a lot of solid brick walls, old-school lathe and plaster as well as aluminum foil-backed insulation boards to help with energy efficiency. There are two major Wi-Fi dead zones in the house: The bathroom and the third bedroom behind it, since there’s lots of old and new pipework in the walls and floors. For mesh routers with two nodes, I place the first in my living room, connected via ethernet to my cable modem with the second on the first floor landing in the (ostensible) center of the house. For three-node sets, the third goes in my kitchen, which I’ve found is the optimal layout to get the bulk of my house covered in Wi-Fi. Fundamentally, my home poses enough challenges that if it succeeds here, it stands a very good chance of succeeding in your place. Each mesh is judged on ease of setup, Wi-Fi coverage, reliability, speed and any additional features that it advertises. I look at how user-friendly each companion app is from the perspective of a novice rather than an expert given you shouldn’t need to be a network engineer to do this sort of thing. Tests I do include checking for dead zones, moving from room to room to measure consistency of connectivity and streaming multiple videos at once to replicate common usage patterns. Mesh Wi-Fi system FAQs This is the section of our mesh Wi-Fi buyer’s guide where we talk about the stuff that most people just glide past. If you’re not familiar with technology, it can be intimidating if people talk about these things as if you’re expected to already know. So here’s a very simple, very basic rundown of some of the stuff you might have missed in very basic terms. What’s the difference between a Wi-Fi router and a mesh router? A Wi-Fi router is a box that usually sits close to wherever the internet comes into your home and pumps out information over radio waves. A mesh router, meanwhile, is a set of smaller devices, one of which sits next to your internet connection while the rest are scattered around your home. A single Wi-Fi router is great if your home is small, your needs aren’t too demanding, or if your home doesn’t have many radio-blocking obstructions that mean those signals can’t reach every corner of your home. But, much like standing next to a radio transmitter and then walking away from it in a straight line, after a while, the signal will degrade. That’s the problem a mesh system is designed to solve, since it will take the signal from your modem and pump to the other mesh devices, known as nodes, in your home. That way, instead of having one big router in one part of your home, you have several small ones that ensure you have good Wi-Fi connectivity all over. It also helps ensure that there’s no risk of dropping your connection as you move around — a mesh router system makes it easy to, for instance, walk from room to room watching Netflix and know you won’t miss a single frame. What's the difference between a Wi-Fi extender and a mesh system? Oh boy. Wi-Fi extenders, or repeaters, are small devices designed to push Wi-Fi a little further than your Wi-Fi router can stretch. They’re cheap, compact and often come in the form of little boxes that sit on your plug sockets with the hope of pushing Wi-Fi to a signal-sparse corner of your home. They are, and I can’t put this delicately enough, often a big pile of rubbish and are often not worth your time. Especially since the price of mesh routers has fallen to within most people’s budgets. What is a wireless backhaul? As we explained above, mesh Wi-Fi systems work by connecting every hardware node to a single wireless network, letting them all communicate with each other. Imagine four people in a busy, noisy restaurant all trying to order their dinner from a weary staff member, all at once. Now imagine, while this is going on, that four more people at that same table are also trying to tell a funny anecdote. It’s no surprise that it might take a long time for the right information to reach its intended destination. To combat this, higher-end mesh routers offer dedicated wireless backhaul; a slice of the spectrum for node-to-node communication. So rather than everyone talking at once in the same space, the conversations are essentially separated, reducing the invisible clutter in the air. Because there’s less confusing cross-chatter, everything moves faster, offering a significant performance boost to those systems. Is it better to hard wire instead of using a mesh Wi-Fi system? This is a great question that doesn’t have a simple answer. It is (almost) always preferable to connect devices with a wire, in this case Ethernet, than to use Wi-Fi. The speeds are faster, it’s more reliable and your data is less vulnerable to the slings and arrows of the laws of physics. Hell, I spent about a year trying to work out how to build an iPhone to Ethernet connector back in the bad old days of Wi-Fi. But your ability to do so depends on your level of DIY skills and / or how much money you want to spend on contractors. Wiring your home for Ethernet if you don’t have the infrastructure already can be a costly and time-consuming process. Particularly if you don’t want ugly wires running along your baseboards and under your carpets or across your hardwood floors. If you’re building your own home or can do some serious DIY, then hard wiring is a fantastic thing to have. It goes wonderfully hand-in-glove with mesh networks too, since you’ll be able to hook up your nodes to the network for even better speeds. But if I’m honest, advances in Wi-Fi technology mean I’d only go for hard wiring if I really believed I needed the sort of speed it offers. Unless you’re a Twitch streamer running your own 24/7 content studio, it’s probably overkill. When we started renovating our 140-year-old home, I had Ethernet installed in the living room, the master and second bedroom and in my office, all at the front of the house. I can’t use it for my mesh since I’d need to put the wiring through the middle of the house. If I ever had the wiring done again, I would do so as I know I’ll instantly see a meaningful improvement in both my connection speed and reliability. But I wouldn’t spend several thousand pounds to have it done just for the sake of it.This article originally appeared on Engadget at https://www.engadget.com/best-mesh-wifi-system-130028701.html?src=rss",
          "content": "A Wi-Fi mesh system is one of the best upgrades you can make to improve your home’s internet coverage, especially if you’ve ever struggled with dead zones, buffering or dropped video calls. Unlike a single router that broadcasts from one central point, a mesh system uses multiple access points spread throughout your space to blanket your entire home with a strong, seamless Wi-Fi signal. Whether you're working on multiple laptops, streaming 4K video in the living room or gaming online in the basement, a mesh setup helps ensure you get reliable Wi-Fi wherever you are.These systems are designed to handle the demands of modern households, offering features like dual-band or even tri-band connectivity to balance your bandwidth across devices, and automatic updates to keep your firmware current. Many also support high-speed internet plans and include options for a wired connection if you need extra stability for gaming or work. With easy setup, smart app controls and long-term future-proofing, the best mesh Wi-Fi router systems can eliminate the need for clunky Wi-Fi extenders and give you fast, dependable Wi-Fi connections across your whole home. Table of contents Best mesh Wi-Fi systems for 2025 What to look for in a mesh Wi-Fi system Other mesh Wi-Fi router systems we tested How we test Wi-Fi routers Mesh Wi-Fi system FAQs Best mesh Wi-Fi systems for 2025 What to look for in a mesh Wi-Fi system Linksys’ CEO Jonathan Bettino told Engadget why mesh systems are an “advancement in Wi-Fi technology” over buying a single point router. With one transmitter, the signal can degrade the further away from the router you go, or the local environment isn’t ideal. “You can have a small [home], but there’s thick walls [...] or things in the way that just interfere with your wireless signal,” he said. Historically, the solution to a home’s Wi-Fi dead zone was to buy a Wi-Fi extender but Bettino said the hardware has both a “terrible user experience” and one of the highest return rates of any consumer electronics product. Mesh Wi-Fi, by comparison, offers “multiple nodes that can be placed anywhere in your home,” says Bettino, resulting in “ubiquitous Wi-Fi” that feels as if you have a “router in every room.” Rather than having one main router in your home, having a “router in every room” is the biggest selling point for mesh Wi-Fi given how reliant we all are on the internet. Each node is in constant contact with each other, broadcasting a single, seamless network to all of your connected devices. There’s no separate network for the 2.4GHz and 5GHz bands, just a single name that you connect to. It’s a good time to buy a mesh Wi-Fi system since the latest standard, Wi-Fi 6E, represents a big leap in the technology. Matt MacPherson, Cisco's Chief Technology Officer for Wireless, said Wi-Fi 6E is a big “inflection point,” using much more of the wireless spectrum than its predecessors. “If you’re using that spectrum with a Wi-Fi 6 [device],” he said, “you’re going to get significant gains [in speed.]” MacPherson added Wi-Fi 6E will likely “carry you for a long time” thanks to the fact its “top throughputs now typically exceed what people can actually connect their home to.” In short, with a top theoretical per-stream speed of 1.2 Gbps, Wi-Fi 6E is fast enough to outrun all but the fastest internet service. What do all these Wi-Fi numbers and letters mean? I’m sorry folks, we need to get boringly technical for one paragraph, but I promise you it’s worth it. Wi-Fi is governed by International Standard IEEE 802.11, and every few years a letter gets added onto that name when the technology evolves and improves. Until 2019, routers were sold under their IEEE name, leaving users to pick through the word soup of a product labeled 802.11 b/g/a/n/ac and so on. Mercifully, wiser heads opted to rebrand the letters as numbers, so rather than 802.11 b/g/a/n/ac, we have Wi-Fi 1, 2, 3 4 and 5. Right now, we’re in the middle of one of those Wi-Fi generations, with most of the gear on sale right now supporting either Wi-Fi 6 or Wi-Fi 6E. What’s the difference between Wi-Fi 6 and Wi-Fi 6E? Wi-Fi uses chunks of the radio frequency spectrum, with Wi-Fi 6 using the 2.4GHz and 5GHz bands to pump data around. In fact, back in the old days, it was likely your home router would offer you the choice of the 2.4GHz or the 5GHz network, as separate bands to access. These days, all of the spectrums are tied together as one thing, and Wi-Fi 6E has the added ability to use the 6GHz band as well. That’s a big chunk of extra wireless real estate that isn’t as cluttered up as the 2.4 and 5GHz bands. You’re going to talk about wireless frequencies now, aren’t you. Each Wi-Fi band had tradeoffs, because the slower radio frequencies have greater range but less speed. 2.4GHz signals will travel a long way in your home but aren’t quick, while 6GHz is blisteringly fast but can be defeated by a sturdy brick wall. A lot of Wi-Fi-enabled gear you own, like smart home products, only use the 2.4GHz band because the range is better and it’s a lot cheaper. But it means that the band is also overcrowded and slow, making it great for your doorbell and robovac, but lackluster for Twitch streaming. So, what am I looking for? Right now, the market is full of mature Wi-Fi 6 and 6E devices, and most new systems available to buy are capable of taking advantage of the faster speeds they offer. This guide focuses on Wi-Fi 6E gear since it’s what we think it’s more than enough to satisfy almost everyone’s at-home Wi-Fi needs. What about Wi-Fi 7? We’re now seeing the first generation of Wi-Fi 7 devices available to buy, but we don’t recommend you do so immediately. The Wi-Fi 7 standard is still so new that there’s little to no reason for you to rush out and buy one for your home. The hardware is tremendously expensive and while Wi-Fi 7 will, eventually, offer some great benefits over 6E, it’s not as transformative an upgrade as 6E. Not to mention, Wi-Fi 7 is so new that almost none of your home’s devices will be able to take advantage of its big-ticket features. I’d estimate you won’t need to worry about upgrading to Wi-Fi 7 for at least five years, if not longer. Range and speed All Wi-Fi routers boast a theoretical broadcast range and a theoretical top speed, and in some cases external antennas to boost signal directionality — but these figures don’t mean much. After all, manufacturers can’t control your ISP’s real speed, the materials and layout of your home or where you put your Wi-Fi gear. Raw speed isn’t everything, either, and you likely need a lot less than the internet speeds your provider is advertising. What matters more is how consistent your connection is between rooms and across devices.. After all, Netflix needs just 15 Mbps to push a single 4K video stream to your home. As cool as it is to say you’ve got all these hundreds of Mbps, factors like latency and reliability are far more crucial to a happy internet life. And unless you have Gigabit internet that can reach speeds of up to 1 Gbps, you won’t need a mesh router that offers that spec. Backhaul Mesh Wi-Fi systems work by connecting every hardware node to a single wireless network, letting them all communicate with each other. Imagine four people in a busy, noisy restaurant all trying to order their dinner from a weary staff member, all at once. Now imagine, while this is going on, that four more people at that same table are also trying to tell a funny anecdote. It’s no surprise that it might take a long time for the right information to reach its intended destination. To combat this, higher-end mesh routers offer dedicated wireless backhaul; a slice of the spectrum for node-to-node communication. So rather than everyone talking at once in the same space, the conversations are essentially separated, reducing the invisible clutter in the air. Because there’s less confusing cross-chatter, everything moves faster, offering a significant performance boost to those systems. Connectivity These days, even your washing machine can have a wireless connection, but that doesn’t mean you should ignore the joys of wired internet. No matter how fast Wi-Fi is, a hard line will always be faster, and some gear, like Philips’ Hue bridge, still needs an ethernet connection. Plenty of routers can also use these hard connections as backhaul, eliminating further wireless clutter. It’s convenient for spread-out systems and power users, but it will mean running more wires through your home. The most common standard is Cat 5e, or gigabit ethernet which, unsurprisingly, has a top speed of 1 Gigabit per second (Gbps). Since Ethernet cables are backward compatible, you should be able to easily find one that works with your system. However, to get the most out of your mesh routers, it’s worth investing in an Ethernet cable that meets the standard your router uses — if it’s Cat 5e, use a Cat 5e cable. You can check your router’s specs via the manufacturer’s website to be sure. Flexibility and scalability Mesh routers enable you to add (or subtract) modules from your home network to suit your needs. D-Link’s Alan Jones said users should “check how scalable the prospective product is” before you buy. This sense of scale doesn’t just apply to the number of nodes on the network, but how many simultaneous connections it can handle. It’s also worth looking at ASUS’ AiMesh products, which can combine mesh Wi-Fi gear and its standard “spider” Wi-Fi routers. If you’ve got a tricky part of your home, you can bolt on an ultra-power standalone Wi-Fi router to a compatible mesh. Placement Mesh networks replace one big piece of hardware with a series of identical nodes that you scatter around your home. You connect one to your modem (usually over ethernet), and then scatter the rest around the place for the best coverage. A good rule of thumb is to place each node no more than two rooms away from the last one, rather than sticking them at the far ends of your home. Bear in mind, every physical obstacle between a Wi-Fi node, its siblings and your devices will hurt your overall performance. You should aim to place them, at the very least, at waist height on furniture in open air, without too many obstructions. The reason many mesh Wi-Fi products are designed to look like an inoffensive white doodad is so you don’t feel compelled to hide them behind your TV. Other mesh Wi-Fi router systems we tested Amazon Eero Pro 7 Eero built its reputation on easy to use yet powerful mesh systems that offer a lot of good in a relatively small and affordable package. Setup is effortless, the app running things is clean and simple, and you get the added benefit of backwards compatibility with older hardware. Sadly, the issue with every Eero system is that so many basic management features, like parental controls, are paywalled behind the company’s Eero Plus subscription for $100 a year. Amazon Eero 6E Eero Pro 6E is an “easy” device, the sort a total novice can set up on their own and thrive with for years on end. There’s little brainwork required to get things set up, and the app has a clean UI with plenty of hand-holding. But, as with the Eero Pro 7, the fact that so many basic management tools are paywalled irks me, especially since you can get plenty of them for free with Google’s rival offering. Netgear Orbi 960 The Orbi 96T0 (RBKE963) is Netgear’s flagship mesh Wi-Fi product, which the company calls the “world’s most powerful Wi-Fi 6E system.” It’s also one of the most expensive consumer-level kits on the market, setting you back $1,499.99 for a three pack. It's a fantastic piece of gear, but it's worth saying that the subset of people who could, would or should buy it remains far smaller than you might expect. Ultimately, I feel that if you’re paying luxury prices, you should expect a luxury product. There were plenty of times during testing that I went looking for a feature that was either only available via the web client, or behind a paywall. While, yes, much of your cash is going to the superlative hardware, but for this sort of money, the fact you have to pay extra for some table-stakes features is insulting. If you’re looking for a new Wi-Fi system and aren’t prepared to spend almost $1,500, it’s worth considering our other top picks for the best Wi-Fi routers and mesh systems. How we test Wi-Fi routers My home covers around 2,200 square feet across three stories with the office on the third floor. It’s relatively long and thin, with the living room at the front of the house, the kitchen at the back and the three bedrooms on the first floor. Its age means there are a lot of solid brick walls, old-school lathe and plaster as well as aluminum foil-backed insulation boards to help with energy efficiency. There are two major Wi-Fi dead zones in the house: The bathroom and the third bedroom behind it, since there’s lots of old and new pipework in the walls and floors. For mesh routers with two nodes, I place the first in my living room, connected via ethernet to my cable modem with the second on the first floor landing in the (ostensible) center of the house. For three-node sets, the third goes in my kitchen, which I’ve found is the optimal layout to get the bulk of my house covered in Wi-Fi. Fundamentally, my home poses enough challenges that if it succeeds here, it stands a very good chance of succeeding in your place. Each mesh is judged on ease of setup, Wi-Fi coverage, reliability, speed and any additional features that it advertises. I look at how user-friendly each companion app is from the perspective of a novice rather than an expert given you shouldn’t need to be a network engineer to do this sort of thing. Tests I do include checking for dead zones, moving from room to room to measure consistency of connectivity and streaming multiple videos at once to replicate common usage patterns. Mesh Wi-Fi system FAQs This is the section of our mesh Wi-Fi buyer’s guide where we talk about the stuff that most people just glide past. If you’re not familiar with technology, it can be intimidating if people talk about these things as if you’re expected to already know. So here’s a very simple, very basic rundown of some of the stuff you might have missed in very basic terms. What’s the difference between a Wi-Fi router and a mesh router? A Wi-Fi router is a box that usually sits close to wherever the internet comes into your home and pumps out information over radio waves. A mesh router, meanwhile, is a set of smaller devices, one of which sits next to your internet connection while the rest are scattered around your home. A single Wi-Fi router is great if your home is small, your needs aren’t too demanding, or if your home doesn’t have many radio-blocking obstructions that mean those signals can’t reach every corner of your home. But, much like standing next to a radio transmitter and then walking away from it in a straight line, after a while, the signal will degrade. That’s the problem a mesh system is designed to solve, since it will take the signal from your modem and pump to the other mesh devices, known as nodes, in your home. That way, instead of having one big router in one part of your home, you have several small ones that ensure you have good Wi-Fi connectivity all over. It also helps ensure that there’s no risk of dropping your connection as you move around — a mesh router system makes it easy to, for instance, walk from room to room watching Netflix and know you won’t miss a single frame. What's the difference between a Wi-Fi extender and a mesh system? Oh boy. Wi-Fi extenders, or repeaters, are small devices designed to push Wi-Fi a little further than your Wi-Fi router can stretch. They’re cheap, compact and often come in the form of little boxes that sit on your plug sockets with the hope of pushing Wi-Fi to a signal-sparse corner of your home. They are, and I can’t put this delicately enough, often a big pile of rubbish and are often not worth your time. Especially since the price of mesh routers has fallen to within most people’s budgets. What is a wireless backhaul? As we explained above, mesh Wi-Fi systems work by connecting every hardware node to a single wireless network, letting them all communicate with each other. Imagine four people in a busy, noisy restaurant all trying to order their dinner from a weary staff member, all at once. Now imagine, while this is going on, that four more people at that same table are also trying to tell a funny anecdote. It’s no surprise that it might take a long time for the right information to reach its intended destination. To combat this, higher-end mesh routers offer dedicated wireless backhaul; a slice of the spectrum for node-to-node communication. So rather than everyone talking at once in the same space, the conversations are essentially separated, reducing the invisible clutter in the air. Because there’s less confusing cross-chatter, everything moves faster, offering a significant performance boost to those systems. Is it better to hard wire instead of using a mesh Wi-Fi system? This is a great question that doesn’t have a simple answer. It is (almost) always preferable to connect devices with a wire, in this case Ethernet, than to use Wi-Fi. The speeds are faster, it’s more reliable and your data is less vulnerable to the slings and arrows of the laws of physics. Hell, I spent about a year trying to work out how to build an iPhone to Ethernet connector back in the bad old days of Wi-Fi. But your ability to do so depends on your level of DIY skills and / or how much money you want to spend on contractors. Wiring your home for Ethernet if you don’t have the infrastructure already can be a costly and time-consuming process. Particularly if you don’t want ugly wires running along your baseboards and under your carpets or across your hardwood floors. If you’re building your own home or can do some serious DIY, then hard wiring is a fantastic thing to have. It goes wonderfully hand-in-glove with mesh networks too, since you’ll be able to hook up your nodes to the network for even better speeds. But if I’m honest, advances in Wi-Fi technology mean I’d only go for hard wiring if I really believed I needed the sort of speed it offers. Unless you’re a Twitch streamer running your own 24/7 content studio, it’s probably overkill. When we started renovating our 140-year-old home, I had Ethernet installed in the living room, the master and second bedroom and in my office, all at the front of the house. I can’t use it for my mesh since I’d need to put the wiring through the middle of the house. If I ever had the wiring done again, I would do so as I know I’ll instantly see a meaningful improvement in both my connection speed and reliability. But I wouldn’t spend several thousand pounds to have it done just for the sake of it.This article originally appeared on Engadget at https://www.engadget.com/best-mesh-wifi-system-130028701.html?src=rss",
          "feed_position": 32
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/best-streaming-service-deals-133028980.html",
          "published_at": "Thu, 06 Nov 2025 08:01:26 +0000",
          "title": "The best streaming deals: Save on DirecTV, Audible, Starz and more",
          "standfirst": "Whether you’re a true cord-cutter or you just want to watch the next season of Stranger Things when it drops, everyone’s on the lookout for streaming deals nowadays. Plenty have chosen VOD and live TV streaming services over traditional cable in recent years, but the savings that choice got you just a few years ago have somewhat evaporated now. Companies like Netflix, Disney, Max and others have been consistently raising prices to the point where you may question if streaming is even worth it anymore. We at Engadget still think so, for many reasons, but you can (and should) be smart with your money at the same time. Streaming deals are an option, even if they don’t come around with the same regularity as discounts on AirPods do. If you’re looking to save money and still stream all of the content you want, Engadget can help by laying out the best streaming deals you can get right now, how you can save with bundles and everything you should know before paying for yet another streaming service. Best streaming deals True streaming deals can be hard to come by. Most often, they’ll pop up during the Black Friday shopping period. On occasion, we’ll see them sparingly throughout the year and they usually take the form of a discounted monthly or annual rate for a limited period of time. Also, true streaming deals are typically on the ad-supported versions of a service, but once in a while you’ll find a unicorn of a deal on a tier that has ad-free viewing. If you’re able to wait for a deal before subscribing to a streaming service, we recommend doing so. You’ll save money upfront and in the long run, and you also have the option to cancel your subscription before the price goes back up to the normal rate. Audible subscription (three months) for $3 ($42 off): From now through mid-December, you can get Amazon’s audiobook subscription for just a dollar a month for three months. Note that it will auto-renew at $15 per month after that, but you can cancel at any point. Starz (one year) for $30 ($40 off): Pay upfront for one year and you can get $40 off a Stars annual subscription. There's a month-to-month option too, which costs $5 per month for the first three months if you don't want to commit to the full year. Either option gives you access to the entire Starz TV and movie library with offline viewing and no ads. Hulu + Live TV — $64.99/mo for 3 months ($25/mo off): New and eligible returning subscribers can get three months of Hulu + Live TV at a rate of $65 per month, which is much cheaper than the current $83-per-month rate and a whopping 27 percent off the new $90-per-month rate that kicks in on October 21. In addition to live TV content, unlimited DVR and access to more than 95 live TV channels, this service also includes Disney+ and ESPN Select, so you're essentially getting three separate streaming services under this umbrella. Just remember that your subscription will be billed at the new standard $90/month rate after the first three months. Fubo Pro for $55/month for the first month ($30 off): Fubo has introductory discounts on most of its packages, and the Pro package is the least expensive plan currently listed. It offers access to 224 channels, unlimited cloud DVR and up to 10 simultaneous streams. It even includes regional sports content from the NHL, MLB and NBA. Spotify Premium Individual (one month) for $0 ($12 off): This is our favorite music streaming service for podcasts and social features. The Premium Individual plan lets you listen ad-free and skip songs at will. You can also organize your listening queue and download content for offline listening. Just be aware, your subscription will auto-renew at the end of the trial period. So if you don't want to be on the hook for the $12 monthly fee, set a reminder to cancel and go back to the free version. Streaming bundle discounts There’s more consolidation happening now than ever before in the streaming space, and that means there are more streaming bundle options. These bundles offer you access to more content with one subscription price, but those prices are typically higher than paying for a single service by itself (obviously). It may be tempting to just get the bundle, but if only one of those services in the bundle speaks to you, you’ll spend less overall by just paying for the single service. Speaking of a deep love for a single streaming service: if all of your favorite shows are on Peacock or the latest releases on HBO Max consistently bring you joy, consider paying for one year upfront. Subscribing with an annual plan usually saves you money in the long term over paying on a monthly basis. Unfortunately, not all streaming services (looking at you, Netflix) have an annual subscription option. Disney+ If you feel like Charlie Kelly trying to figure out who Pepe Silvia is when you look at Disney's streaming prices chart, you're not alone. The confusion comes from the fact that Disney owns, or has a hand in, many streaming services including Hulu and ESPN. Throw in a partnership with HBO Max and you have a ton of options to consider and, probably, whiplash to match. Here's a quick overview of popular Disney+ bundle pricing. Disney+ and Hulu bundle (with ads) — $13/month Disney+ and Hulu bundle (without ads) — $20/month Disney+, Hulu and ESPN Select (with ads) — $20/month Disney+, Hulu and ESPN Select (without ads on Disney+ and Hulu only) — $30/month Disney+, Hulu and HBO Max (with ads) — $20/month Disney+, Hulu and HBO Max (without ads) — $33/month Peacock TV Peacock doesn't have any streaming bundles available all year round, but you can save if you pay for one year upfront. Peacock Select (with ads) — $8/month or $80/year Peacock Premium (with ads) — $11/month or $110/year Peacock Premium Plus (without ads) — $17/month or $170/year Paramount+ Paramount+ used to bill its tier with Showtime as a sort of bundle, but it has since renamed its plans and focused the Showtime inclusion in its premium tier as just another bonus of paying for the higher priced plan. Paramount+ Essential (with ads) —$8/month or $60/year Paramount Premium (without ads) — $13/month or $120/year Student discounts on streaming services It pays to be a student — sometimes, at least. A number of streaming services have student discounts you can take advantage of as long as you're actively studying. What that translates to most of the time is being able to verify your student status and signing up with your .edu email address. HBO Max student discount — subscribe for $5/month (50 percent off): HBO Max offers their ad-supported tier to students for half off the usual rate. You’ll just have to verify that you’re a student through Unidays, and make note that this offer is only good for up to 12 months of service. Hulu student discount — subscribe for $2/month (75 percent off): Those with a valid student ID can get Hulu’s ad-supported tier for 75 percent off the typical rate. They’ll keep the same sale price for as long as they’re a student as well. Spotify student discount — Premium + Hulu with ads for $6/month (72 percent off): Spotify’s student offer continues to be one of the best around, giving you access to the Premium tier of the music streamer and Hulu’s ad-supported plan for only $6 monthly. Purchased separately, you’d pay $22 per month for both of the services. Plus, the first month is free when you sign up. NBA League Pass student discount — one year for $120 (40 percent off): Students can get one year of League Pass for only $10 per month, which includes access to NBA TV and the ability to watch classic and archive games on-demand. On the NBA League Pass website, look for the student discount banner at the top and follow the instructions to verify your student status. Read more streaming coverage The best live TV streaming services to cut cable The best streaming services: Netflix, Hulu, HBO Max and more The best streaming devices Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/best-streaming-service-deals-133028980.html?src=rss",
          "content": "Whether you’re a true cord-cutter or you just want to watch the next season of Stranger Things when it drops, everyone’s on the lookout for streaming deals nowadays. Plenty have chosen VOD and live TV streaming services over traditional cable in recent years, but the savings that choice got you just a few years ago have somewhat evaporated now. Companies like Netflix, Disney, Max and others have been consistently raising prices to the point where you may question if streaming is even worth it anymore. We at Engadget still think so, for many reasons, but you can (and should) be smart with your money at the same time. Streaming deals are an option, even if they don’t come around with the same regularity as discounts on AirPods do. If you’re looking to save money and still stream all of the content you want, Engadget can help by laying out the best streaming deals you can get right now, how you can save with bundles and everything you should know before paying for yet another streaming service. Best streaming deals True streaming deals can be hard to come by. Most often, they’ll pop up during the Black Friday shopping period. On occasion, we’ll see them sparingly throughout the year and they usually take the form of a discounted monthly or annual rate for a limited period of time. Also, true streaming deals are typically on the ad-supported versions of a service, but once in a while you’ll find a unicorn of a deal on a tier that has ad-free viewing. If you’re able to wait for a deal before subscribing to a streaming service, we recommend doing so. You’ll save money upfront and in the long run, and you also have the option to cancel your subscription before the price goes back up to the normal rate. Audible subscription (three months) for $3 ($42 off): From now through mid-December, you can get Amazon’s audiobook subscription for just a dollar a month for three months. Note that it will auto-renew at $15 per month after that, but you can cancel at any point. Starz (one year) for $30 ($40 off): Pay upfront for one year and you can get $40 off a Stars annual subscription. There's a month-to-month option too, which costs $5 per month for the first three months if you don't want to commit to the full year. Either option gives you access to the entire Starz TV and movie library with offline viewing and no ads. Hulu + Live TV — $64.99/mo for 3 months ($25/mo off): New and eligible returning subscribers can get three months of Hulu + Live TV at a rate of $65 per month, which is much cheaper than the current $83-per-month rate and a whopping 27 percent off the new $90-per-month rate that kicks in on October 21. In addition to live TV content, unlimited DVR and access to more than 95 live TV channels, this service also includes Disney+ and ESPN Select, so you're essentially getting three separate streaming services under this umbrella. Just remember that your subscription will be billed at the new standard $90/month rate after the first three months. Fubo Pro for $55/month for the first month ($30 off): Fubo has introductory discounts on most of its packages, and the Pro package is the least expensive plan currently listed. It offers access to 224 channels, unlimited cloud DVR and up to 10 simultaneous streams. It even includes regional sports content from the NHL, MLB and NBA. Spotify Premium Individual (one month) for $0 ($12 off): This is our favorite music streaming service for podcasts and social features. The Premium Individual plan lets you listen ad-free and skip songs at will. You can also organize your listening queue and download content for offline listening. Just be aware, your subscription will auto-renew at the end of the trial period. So if you don't want to be on the hook for the $12 monthly fee, set a reminder to cancel and go back to the free version. Streaming bundle discounts There’s more consolidation happening now than ever before in the streaming space, and that means there are more streaming bundle options. These bundles offer you access to more content with one subscription price, but those prices are typically higher than paying for a single service by itself (obviously). It may be tempting to just get the bundle, but if only one of those services in the bundle speaks to you, you’ll spend less overall by just paying for the single service. Speaking of a deep love for a single streaming service: if all of your favorite shows are on Peacock or the latest releases on HBO Max consistently bring you joy, consider paying for one year upfront. Subscribing with an annual plan usually saves you money in the long term over paying on a monthly basis. Unfortunately, not all streaming services (looking at you, Netflix) have an annual subscription option. Disney+ If you feel like Charlie Kelly trying to figure out who Pepe Silvia is when you look at Disney's streaming prices chart, you're not alone. The confusion comes from the fact that Disney owns, or has a hand in, many streaming services including Hulu and ESPN. Throw in a partnership with HBO Max and you have a ton of options to consider and, probably, whiplash to match. Here's a quick overview of popular Disney+ bundle pricing. Disney+ and Hulu bundle (with ads) — $13/month Disney+ and Hulu bundle (without ads) — $20/month Disney+, Hulu and ESPN Select (with ads) — $20/month Disney+, Hulu and ESPN Select (without ads on Disney+ and Hulu only) — $30/month Disney+, Hulu and HBO Max (with ads) — $20/month Disney+, Hulu and HBO Max (without ads) — $33/month Peacock TV Peacock doesn't have any streaming bundles available all year round, but you can save if you pay for one year upfront. Peacock Select (with ads) — $8/month or $80/year Peacock Premium (with ads) — $11/month or $110/year Peacock Premium Plus (without ads) — $17/month or $170/year Paramount+ Paramount+ used to bill its tier with Showtime as a sort of bundle, but it has since renamed its plans and focused the Showtime inclusion in its premium tier as just another bonus of paying for the higher priced plan. Paramount+ Essential (with ads) —$8/month or $60/year Paramount Premium (without ads) — $13/month or $120/year Student discounts on streaming services It pays to be a student — sometimes, at least. A number of streaming services have student discounts you can take advantage of as long as you're actively studying. What that translates to most of the time is being able to verify your student status and signing up with your .edu email address. HBO Max student discount — subscribe for $5/month (50 percent off): HBO Max offers their ad-supported tier to students for half off the usual rate. You’ll just have to verify that you’re a student through Unidays, and make note that this offer is only good for up to 12 months of service. Hulu student discount — subscribe for $2/month (75 percent off): Those with a valid student ID can get Hulu’s ad-supported tier for 75 percent off the typical rate. They’ll keep the same sale price for as long as they’re a student as well. Spotify student discount — Premium + Hulu with ads for $6/month (72 percent off): Spotify’s student offer continues to be one of the best around, giving you access to the Premium tier of the music streamer and Hulu’s ad-supported plan for only $6 monthly. Purchased separately, you’d pay $22 per month for both of the services. Plus, the first month is free when you sign up. NBA League Pass student discount — one year for $120 (40 percent off): Students can get one year of League Pass for only $10 per month, which includes access to NBA TV and the ability to watch classic and archive games on-demand. On the NBA League Pass website, look for the student discount banner at the top and follow the instructions to verify your student status. Read more streaming coverage The best live TV streaming services to cut cable The best streaming services: Netflix, Hulu, HBO Max and more The best streaming devices Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/best-streaming-service-deals-133028980.html?src=rss",
          "feed_position": 34
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/from-prototype-to-production-what-vibe-coding-tools-must-fix-for-enterprise",
          "published_at": "Thu, 06 Nov 2025 05:00:00 GMT",
          "title": "From prototype to production: What vibe coding tools must fix for enterprise adoption",
          "standfirst": "Presented by Salesforce Vibe coding — the fast-growing trend of using generative AI to spin up code from plain-language prompts — is quick, creative, and great for instant prototypes. But many argue that it&#x27;s not cut out for building production-ready business apps with the security, governance, and trusted infrastructure that enterprises require. In other words, a few saved hours in development can mean a future full of security vulnerabilities, endless maintenance, and scalability headaches, says Mohith Shrivastava, principal developer advocate at Salesforce.\"For rapid experimentation, building minimum viable products, and tackling creative challenges, vibe coding is a game-changer,\" Shrivastava says. \"However, that same speed and improvisational nature are exactly what makes its application in a professional, enterprise setting a topic of intense debate. And the skepticism from the developer community is 100% justified.\"Risks and rewards of vibe coding The excitement is all about speed: going from a rough idea to a working prototype in hours, not weeks, is a massive advantage. But as Shrivastava shared, developers have been vocal about the potential downsides.\"When you apply vibe coding indiscriminately to an entire application stack, you’re not just moving fast; you’re accumulating risk at an unprecedented rate,\" Shrivastava explains. \"The cons are significant.\" That includes potential security nightmares, as AI models don&#x27;t typically take into consideration the company&#x27;s specific security policies. They can easily introduce vulnerabilities like hardcoded secrets or use insecure, hallucinated packages. Then there’s the issue of what Shrivastava calls \"spaghetti code on steroids,\" or verbose code that lacks a coherent architectural pattern, creating a mountain of technical debt.Equally concerning is the illusion of progress: vibe coding may complete 80% of a feature in record time, but the remaining 20% — the edge cases, performance tuning, and compliance work — becomes exponentially harder.But does this mean vibe coding has no place in the enterprise?\"The idea that you can just vibe your way to a complex, secure, and maintainable enterprise application is a dangerous fantasy,\" Shrivastava says. \"But — the pros are undeniable if it&#x27;s used correctly. The key is not to avoid vibe coding, but to apply it intelligently in your enterprise.\"Red and green zones: Enterprise-grade vibe codingYou can&#x27;t, and you absolutely should not, vibe code your entire enterprise stack with just any generic tool, Shrivastava warns. But when paired with no-, low-, or pro-code tools that are built for the enterprise, many of the gaps can be addressed. An enterprise-grade vibe coding solution, for example, can automatically scan for security issues, flag performance bottlenecks, and provide a safety net. It’s also critical to understand which parts of an application suit this approach — and which demand a higher level of trust and control. Shrivastava divides the stack into red and green zones to illustrate.The green zone is the presentation layer, or the UI and UX. It’s ideal for vibe coding, where developers can move fast and iterate quickly without much risk. In contrast is the red zone, which covers the foundational pillars of an application, including business logic and data layers.Empowering developers in the green zoneDeveloper expertise remains the foundation for effective and safe vibe coding. But developers can be amplified by AI tools and emerging agents that are grounded in business context, connected to real applications, integrations, and data flows.\"A generic AI agent can&#x27;t grasp your company&#x27;s unique processes, but a context-aware tool can act as a powerful pair programmer, helping a developer draft complex logic or model data with greater speed and accuracy,\" Shrivastava says. \"It’s about making the expert developer more efficient, not trying to do their job for them.\"Some areas will always be high risk for ungoverned AI — especially infrastructure and security. Letting a generic AI agent configure firewalls or Identity and Access Management [IAM] policies without oversight, Shrivastava warns, is a recipe for disaster. The solution isn’t to avoid the red zone entirely, but to approach it with the right tools — ones that embed governance, security, and context from the ground up.\"The winning strategy is clear: Vibe code the green zone for agility, approach the red zone by augmenting your developers with powerful, context-aware tools, and never, ever DIY your core infrastructure with AI,\" he says.Embracing enterprise vibe codingTo harness the power of enterprise vibe coding, Salesforce developed Agentforce Vibes. This new vibe coding offering for the enterprise includes Agentforce, an autonomous AI agent built to collaborate like a pair programmer on the Salesforce Platform. It’s designed precisely to provide developers with the right tools for the job, covering both the green and red zones. For the green zone, it offers the speed and agility to rapidly build UIs and prototypes. But its true power lies in how it augments developers in the red zone.\"Enterprise vibe coding like Agentforce lets organizations take AI-assisted development to the organizational level, accelerating coding, testing, and deployment, while ensuring consistency, security, and performance,\" says Dan Fernandez, VP of product, developer services at Salesforce. \"It&#x27;s not about throwing away governance for speed; it’s about integrating AI into every stage of the application lifecycle to work smarter.\"Because Agentforce Vibes’ tooling is deeply integrated with your business context on the platform, it can safely assist with business logic and data modeling. Most importantly, it operates on a trusted platform. Instead of a DIY approach — jury-rigging a generic AI agent to handle your networking — developers build on a foundation that has security and governance built in, so they can innovate safely, knowing the most critical layers of the stack are secure and compliant.Major enterprises are putting vibe coding to work Agentforce Vibes users are now tapping the tool to build around 20 to 25% of their new code base, according to Salesforce data, and users are accepting around 1.2 million lines of agentic code per month. That includes companies like Coinbase, CGI, Grupo Globo, and one of the top five banks in the U.S., which is using Agentforce Vibes capabilities to develop production-ready apps faster. Agentforce Vibes is part of a suite of tools in Agentforce 360 that span from no-code and low-code to pro-code development. These tools are together helping customers develop and deploy at speeds previously unheard of.With the low-code Agent Builder in Agentforce, the Secret Escapes team was able to build, test, and launch their agent to support customer service in just two weeks, compared to the six months it had previously taken the company to build and train a bot. With Agentforce, 1-800Accountant autonomously resolved 70% of customer chat engagements during tax week in 2025, without writing a line of code, using Salesforce’s low-code tools and AI assistance. Meanwhile, media company Grupo Globo deployed agents to identify subscribers at risk of lapsing, offer personalized upgrades, cross-sell, and convert non-subscribers. As a result, Agentforce boosted Globo’s retention rates by 22% in less than three months.Innovation meets discipline Enterprise tools show that disciplined engineering and creative experimentation can coexist — and that balance, Shrivastava says, is the key to lasting innovation.\"Vibe coding is not a fad, but it&#x27;s also not a silver bullet that will replace disciplined software engineering,\" Shrivastava says. \"The smart path forward is a hybrid approach where human software skills are augmented with agentic intelligence. This balanced approach is how you get the best of both worlds: radical innovation at the edge and unwavering stability at the core.\"Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.",
          "content": "Presented by Salesforce Vibe coding — the fast-growing trend of using generative AI to spin up code from plain-language prompts — is quick, creative, and great for instant prototypes. But many argue that it&#x27;s not cut out for building production-ready business apps with the security, governance, and trusted infrastructure that enterprises require. In other words, a few saved hours in development can mean a future full of security vulnerabilities, endless maintenance, and scalability headaches, says Mohith Shrivastava, principal developer advocate at Salesforce.\"For rapid experimentation, building minimum viable products, and tackling creative challenges, vibe coding is a game-changer,\" Shrivastava says. \"However, that same speed and improvisational nature are exactly what makes its application in a professional, enterprise setting a topic of intense debate. And the skepticism from the developer community is 100% justified.\"Risks and rewards of vibe coding The excitement is all about speed: going from a rough idea to a working prototype in hours, not weeks, is a massive advantage. But as Shrivastava shared, developers have been vocal about the potential downsides.\"When you apply vibe coding indiscriminately to an entire application stack, you’re not just moving fast; you’re accumulating risk at an unprecedented rate,\" Shrivastava explains. \"The cons are significant.\" That includes potential security nightmares, as AI models don&#x27;t typically take into consideration the company&#x27;s specific security policies. They can easily introduce vulnerabilities like hardcoded secrets or use insecure, hallucinated packages. Then there’s the issue of what Shrivastava calls \"spaghetti code on steroids,\" or verbose code that lacks a coherent architectural pattern, creating a mountain of technical debt.Equally concerning is the illusion of progress: vibe coding may complete 80% of a feature in record time, but the remaining 20% — the edge cases, performance tuning, and compliance work — becomes exponentially harder.But does this mean vibe coding has no place in the enterprise?\"The idea that you can just vibe your way to a complex, secure, and maintainable enterprise application is a dangerous fantasy,\" Shrivastava says. \"But — the pros are undeniable if it&#x27;s used correctly. The key is not to avoid vibe coding, but to apply it intelligently in your enterprise.\"Red and green zones: Enterprise-grade vibe codingYou can&#x27;t, and you absolutely should not, vibe code your entire enterprise stack with just any generic tool, Shrivastava warns. But when paired with no-, low-, or pro-code tools that are built for the enterprise, many of the gaps can be addressed. An enterprise-grade vibe coding solution, for example, can automatically scan for security issues, flag performance bottlenecks, and provide a safety net. It’s also critical to understand which parts of an application suit this approach — and which demand a higher level of trust and control. Shrivastava divides the stack into red and green zones to illustrate.The green zone is the presentation layer, or the UI and UX. It’s ideal for vibe coding, where developers can move fast and iterate quickly without much risk. In contrast is the red zone, which covers the foundational pillars of an application, including business logic and data layers.Empowering developers in the green zoneDeveloper expertise remains the foundation for effective and safe vibe coding. But developers can be amplified by AI tools and emerging agents that are grounded in business context, connected to real applications, integrations, and data flows.\"A generic AI agent can&#x27;t grasp your company&#x27;s unique processes, but a context-aware tool can act as a powerful pair programmer, helping a developer draft complex logic or model data with greater speed and accuracy,\" Shrivastava says. \"It’s about making the expert developer more efficient, not trying to do their job for them.\"Some areas will always be high risk for ungoverned AI — especially infrastructure and security. Letting a generic AI agent configure firewalls or Identity and Access Management [IAM] policies without oversight, Shrivastava warns, is a recipe for disaster. The solution isn’t to avoid the red zone entirely, but to approach it with the right tools — ones that embed governance, security, and context from the ground up.\"The winning strategy is clear: Vibe code the green zone for agility, approach the red zone by augmenting your developers with powerful, context-aware tools, and never, ever DIY your core infrastructure with AI,\" he says.Embracing enterprise vibe codingTo harness the power of enterprise vibe coding, Salesforce developed Agentforce Vibes. This new vibe coding offering for the enterprise includes Agentforce, an autonomous AI agent built to collaborate like a pair programmer on the Salesforce Platform. It’s designed precisely to provide developers with the right tools for the job, covering both the green and red zones. For the green zone, it offers the speed and agility to rapidly build UIs and prototypes. But its true power lies in how it augments developers in the red zone.\"Enterprise vibe coding like Agentforce lets organizations take AI-assisted development to the organizational level, accelerating coding, testing, and deployment, while ensuring consistency, security, and performance,\" says Dan Fernandez, VP of product, developer services at Salesforce. \"It&#x27;s not about throwing away governance for speed; it’s about integrating AI into every stage of the application lifecycle to work smarter.\"Because Agentforce Vibes’ tooling is deeply integrated with your business context on the platform, it can safely assist with business logic and data modeling. Most importantly, it operates on a trusted platform. Instead of a DIY approach — jury-rigging a generic AI agent to handle your networking — developers build on a foundation that has security and governance built in, so they can innovate safely, knowing the most critical layers of the stack are secure and compliant.Major enterprises are putting vibe coding to work Agentforce Vibes users are now tapping the tool to build around 20 to 25% of their new code base, according to Salesforce data, and users are accepting around 1.2 million lines of agentic code per month. That includes companies like Coinbase, CGI, Grupo Globo, and one of the top five banks in the U.S., which is using Agentforce Vibes capabilities to develop production-ready apps faster. Agentforce Vibes is part of a suite of tools in Agentforce 360 that span from no-code and low-code to pro-code development. These tools are together helping customers develop and deploy at speeds previously unheard of.With the low-code Agent Builder in Agentforce, the Secret Escapes team was able to build, test, and launch their agent to support customer service in just two weeks, compared to the six months it had previously taken the company to build and train a bot. With Agentforce, 1-800Accountant autonomously resolved 70% of customer chat engagements during tax week in 2025, without writing a line of code, using Salesforce’s low-code tools and AI assistance. Meanwhile, media company Grupo Globo deployed agents to identify subscribers at risk of lapsing, offer personalized upgrades, cross-sell, and convert non-subscribers. As a result, Agentforce boosted Globo’s retention rates by 22% in less than three months.Innovation meets discipline Enterprise tools show that disciplined engineering and creative experimentation can coexist — and that balance, Shrivastava says, is the key to lasting innovation.\"Vibe coding is not a fad, but it&#x27;s also not a silver bullet that will replace disciplined software engineering,\" Shrivastava says. \"The smart path forward is a hybrid approach where human software skills are augmented with agentic intelligence. This balanced approach is how you get the best of both worlds: radical innovation at the edge and unwavering stability at the core.\"Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.",
          "feed_position": 2,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/2Tb1BL75AhuG8odK9Kz0XF/5237845c829eb7c9f8d20b41fcdbd9da/AdobeStock_1628357042.jpeg?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/the-compute-rethink-scaling-ai-where-data-lives-at-the-edge",
          "published_at": "Thu, 06 Nov 2025 05:00:00 GMT",
          "title": "The compute rethink: Scaling AI where data lives, at the edge",
          "standfirst": "Presented by ArmAI is no longer confined to the cloud or data centers. Increasingly, it’s running directly where data is created — in devices, sensors, and networks at the edge. This shift toward on-device intelligence is being driven by latency, privacy, and cost concerns that companies are confronting as they continue their investments in AI. For leadership teams, the opportunity is clear, says Chris Bergey, SVP and GM, of Arm’s Client Business: Invest in AI-first platforms that complement cloud usage, deliver real-time responsiveness, and protect sensitive data. \"With the explosion of connected devices and the rise of IoT, edge AI provides a significant opportunity for organizations to gain a competitive edge through faster, more efficient AI,\" Bergey explains. \"Those who move first aren’t just improving efficiency, they’re redefining what customers expect. AI is becoming a differentiator in trust, responsiveness, and innovation. The sooner a business makes AI central to its workflows, the faster it compounds that advantage.\" Use cases: Deploying AI where data livesEnterprises are discovering that edge AI isn’t just a performance boost — it’s a new operational model. Processing locally means less dependency on the cloud and faster, safer decision-making in real time. For instance, a factory floor can analyze equipment data instantly to prevent downtime, while a hospital can run diagnostic models securely on-site. Retailers are deploying in-store analytics using vision systems while logistic companies are using on-device AI to optimize fleet operations. Instead of sending vast data volumes to the cloud, organizations can analyze and act on insights where they emerge. The result is a more responsive, privacy-preserving, and cost-effective AI architecture.The consumer expectation: Immediacy and trustWorking with Alibaba’s Taobao team, the largest Chinese ecommerce platform, Arm (Nasdaq:Arm) enabled on-device product recommendations that update instantly without depending on the cloud. This helped online shoppers find what they need faster while keeping browsing data private.Another example comes from consumer tech: Meta’s Ray-Ban smart glasses, which blend cloud and on-device AI. The glasses handle quick commands locally for faster responses, while heavier tasks like translation and visual recognition are processed in the cloud.\"Every major technology shift has created new ways to engage and monetize,\" Bergey says. \"As AI capabilities and user expectations grow, more intelligence will need to move closer to the edge to deliver this kind of immediacy and trust that people now expect.\" This shift is also taking place with the tools people use every day. Assistants like Microsoft Copilot and Google Gemini are blending cloud and on-device intelligence to bring generative AI closer to the user, delivering faster, more secure, and more context-aware experiences. That same principle applies across industries: the more intelligence you move safely and efficiently to the edge, the more responsive, private, and valuable your operations become. Building smarter for scaleThe explosion of AI at the edge demands not only smarter chips but smarter infrastructure. By aligning compute power with workload demands, enterprises can reduce energy consumption while maintaining high performance. This balance of sustainability and scale is fast becoming a competitive differentiator.\"Compute needs, whether in the cloud or on-premises, will continue to rise sharply. The question becomes, how do you maximize value from that compute?\" he said. \"You can only do this by investing in compute platforms and software that scale with your AI ambitions. The real measure of progress is enterprise value creation, not raw efficiency metrics.\"The intelligent foundationThe rapid evolution of AI models, especially those powering edge inferencing, multimodal applications, and low-latency responses, demands not just smarter algorithms, but a foundation of highly performant, energy-efficient hardware. As workloads grow more diverse and distributed, legacy architectures designed for traditional workloads are no longer adequate. The role of CPUs is evolving, and they now sit at the center of increasingly heterogenous systems that deliver advanced on-device AI experiences. Thanks to their flexibility, efficiency, and mature software support, modern CPUs can run everything from classic machine learning to complex generative AI workloads. When paired with accelerators such as NPUs or GPUs, they intelligently coordinate compute across the system — ensuring the right workload runs on the right engine for maximum performance and efficiency. The CPU continues to be the foundation that enables scalable, efficient AI everywhere.Technologies like Arm’s Scalable Matrix Extension 2 (SME2) bring advanced matrix acceleration to Armv9 CPUs. Meanwhile, Arm KleidiAI, its intelligent software layer, is extensively integrated across leading frameworks to automatically boost performance for a wide range of AI workloads, from language models to speech recognition to computer vision, running on Arm-based edge devices — without needing developers to rewrite their code.\"These technologies ensure that AI frameworks can tap into the full performance of Arm-based systems without extra developer effort,\" he says. \"It’s how we make AI both scalable and sustainable: by embedding intelligence into the foundation of modern compute, so innovation happens at the speed of software, not hardware cycles.\"That democratization of compute power is also what will facilitate the next wave of intelligent, real-time experiences across the enterprise, not just in flagship products, but across entire device portfolios. The evolution of edge AI As AI moves from isolated pilots to full-scale deployment, the enterprises that succeed will be those that connect intelligence across every layer of infrastructure. Agentic AI systems will depend on this seamless integration — enabling autonomous processes that can reason, coordinate, and deliver value instantly.\"The pattern is familiar as in every disruptive wave, incumbents that move slowly risk being overtaken by new entrants,\" he says. \"The companies that thrive will be the ones that wake up every morning asking how to make their organization AI-first. As with the rise of the internet and cloud computing, those who lean in and truly become AI-enabled will shape the next decade.\"Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.",
          "content": "Presented by ArmAI is no longer confined to the cloud or data centers. Increasingly, it’s running directly where data is created — in devices, sensors, and networks at the edge. This shift toward on-device intelligence is being driven by latency, privacy, and cost concerns that companies are confronting as they continue their investments in AI. For leadership teams, the opportunity is clear, says Chris Bergey, SVP and GM, of Arm’s Client Business: Invest in AI-first platforms that complement cloud usage, deliver real-time responsiveness, and protect sensitive data. \"With the explosion of connected devices and the rise of IoT, edge AI provides a significant opportunity for organizations to gain a competitive edge through faster, more efficient AI,\" Bergey explains. \"Those who move first aren’t just improving efficiency, they’re redefining what customers expect. AI is becoming a differentiator in trust, responsiveness, and innovation. The sooner a business makes AI central to its workflows, the faster it compounds that advantage.\" Use cases: Deploying AI where data livesEnterprises are discovering that edge AI isn’t just a performance boost — it’s a new operational model. Processing locally means less dependency on the cloud and faster, safer decision-making in real time. For instance, a factory floor can analyze equipment data instantly to prevent downtime, while a hospital can run diagnostic models securely on-site. Retailers are deploying in-store analytics using vision systems while logistic companies are using on-device AI to optimize fleet operations. Instead of sending vast data volumes to the cloud, organizations can analyze and act on insights where they emerge. The result is a more responsive, privacy-preserving, and cost-effective AI architecture.The consumer expectation: Immediacy and trustWorking with Alibaba’s Taobao team, the largest Chinese ecommerce platform, Arm (Nasdaq:Arm) enabled on-device product recommendations that update instantly without depending on the cloud. This helped online shoppers find what they need faster while keeping browsing data private.Another example comes from consumer tech: Meta’s Ray-Ban smart glasses, which blend cloud and on-device AI. The glasses handle quick commands locally for faster responses, while heavier tasks like translation and visual recognition are processed in the cloud.\"Every major technology shift has created new ways to engage and monetize,\" Bergey says. \"As AI capabilities and user expectations grow, more intelligence will need to move closer to the edge to deliver this kind of immediacy and trust that people now expect.\" This shift is also taking place with the tools people use every day. Assistants like Microsoft Copilot and Google Gemini are blending cloud and on-device intelligence to bring generative AI closer to the user, delivering faster, more secure, and more context-aware experiences. That same principle applies across industries: the more intelligence you move safely and efficiently to the edge, the more responsive, private, and valuable your operations become. Building smarter for scaleThe explosion of AI at the edge demands not only smarter chips but smarter infrastructure. By aligning compute power with workload demands, enterprises can reduce energy consumption while maintaining high performance. This balance of sustainability and scale is fast becoming a competitive differentiator.\"Compute needs, whether in the cloud or on-premises, will continue to rise sharply. The question becomes, how do you maximize value from that compute?\" he said. \"You can only do this by investing in compute platforms and software that scale with your AI ambitions. The real measure of progress is enterprise value creation, not raw efficiency metrics.\"The intelligent foundationThe rapid evolution of AI models, especially those powering edge inferencing, multimodal applications, and low-latency responses, demands not just smarter algorithms, but a foundation of highly performant, energy-efficient hardware. As workloads grow more diverse and distributed, legacy architectures designed for traditional workloads are no longer adequate. The role of CPUs is evolving, and they now sit at the center of increasingly heterogenous systems that deliver advanced on-device AI experiences. Thanks to their flexibility, efficiency, and mature software support, modern CPUs can run everything from classic machine learning to complex generative AI workloads. When paired with accelerators such as NPUs or GPUs, they intelligently coordinate compute across the system — ensuring the right workload runs on the right engine for maximum performance and efficiency. The CPU continues to be the foundation that enables scalable, efficient AI everywhere.Technologies like Arm’s Scalable Matrix Extension 2 (SME2) bring advanced matrix acceleration to Armv9 CPUs. Meanwhile, Arm KleidiAI, its intelligent software layer, is extensively integrated across leading frameworks to automatically boost performance for a wide range of AI workloads, from language models to speech recognition to computer vision, running on Arm-based edge devices — without needing developers to rewrite their code.\"These technologies ensure that AI frameworks can tap into the full performance of Arm-based systems without extra developer effort,\" he says. \"It’s how we make AI both scalable and sustainable: by embedding intelligence into the foundation of modern compute, so innovation happens at the speed of software, not hardware cycles.\"That democratization of compute power is also what will facilitate the next wave of intelligent, real-time experiences across the enterprise, not just in flagship products, but across entire device portfolios. The evolution of edge AI As AI moves from isolated pilots to full-scale deployment, the enterprises that succeed will be those that connect intelligence across every layer of infrastructure. Agentic AI systems will depend on this seamless integration — enabling autonomous processes that can reason, coordinate, and deliver value instantly.\"The pattern is familiar as in every disruptive wave, incumbents that move slowly risk being overtaken by new entrants,\" he says. \"The companies that thrive will be the ones that wake up every morning asking how to make their organization AI-first. As with the rise of the internet and cloud computing, those who lean in and truly become AI-enabled will shape the next decade.\"Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.",
          "feed_position": 3,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/5ERDb8KPErLVHncySz9rfu/b8f28a9fe706982175c4a8310f45ea90/AdobeStock_624772113.jpeg?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/apps/the-foursquare-founders-new-app-is-an-ai-powered-dj-for-neighborhood-updates-202326296.html",
          "published_at": "Wed, 05 Nov 2025 20:23:26 +0000",
          "title": "The Foursquare founder's new app is an AI-powered 'DJ' for neighborhood updates",
          "standfirst": "Foursquare founder Dennis Crowley has unveiled his latest venture and yes, it's another location-based social app. But, rather than the check-ins Crowley first popularized more than 15 years ago, \"BeeBot\" has a very 2025 take on the concept. Instead, the app is an AI-powered \"DJ\" that can deliver contextual audio updates to your ears as you move through your day. Crowley describes BeeBot as an \"app for AirPods,\" though it will work with any type of headphones, as well as smart glasses with audio capabilities like Meta's. \"Whenever you put your AirPods in, it turns on,\" Crowley explains in a post on Medium. \"Whenever you take your AirPods out it turns off. And when it’s 'on' it’ll push you snippets of audio about the people, places, and events that are nearby.'To do this, you'll need to give the app access to your location and share a handful of \"keywords\" about your interests. You can also share your contacts to get updates from friends who are using the app. The BeeBot \"DJ,\" which of course has an AI voice, will then be able to talk to you as you throughout the dat and alert you to interesting events, landmarks or updates from friends who happen to be nearby. In some ways, it sounds like Crowley is trying to re-create some of the serendipitous IRL social interactions enabled by the original version of Foursquare. BeeBot doesn't have \"mayorships,\" badges or any of the gamification features that helped popularize FourSquare, but it's meant to have some of the \"same playful spirit,\" of OG Foursquare, according to Crowley. (Foursquare shut down its city guide app of the same name earlier this year, though its check-in app, Swarm, lives on.) And, because it's 2025, there's also a whole bunch of AI thrown in, including \"a mix of different LLMs\" and \"synthetic voices.\" The app is \"powered by a TikTok-style algorithm,\" Crowley says, \"but one that’s focused on what’s happening nearby and IRL.\" There also seems to be a bit of DNA from Marsbot, the short-lived (and IMO very underrated) chat-based app Foursquare launched in 2016 that could proactively provide personalized restaurant recommendations. While BeeBot isn't as heavily focused on neighborhood recommendations as Foursquare, it is meant to proactively let you know about nearby happenings you might be interested in or even a bit of gossip from friends. In addition to friends' status updates, the app will draw on local Substacks and newsletters for relevant info about a given area. Crowley says the DJ's audio cues may \"occasionally\" interrupt your music or podcast to give an update, though users should expect to hear these only a couple times throughout the day. BeeBot won't interrupt voice or video calls. BeeBot, which is the inaugural project of Crowley's new company Hopscotch Labs, is out now in the App Store, though it's still \"very much a work in progress,\" according to Crowley. \"I feel like the version of the product we’re sharing with folks today is kind of where Foursquare was when it launched at SXSW in 2009 – an interesting vision, a good-enough execution, but something that still needs to be shaped by the people using it to fully blossom.\" This article originally appeared on Engadget at https://www.engadget.com/apps/the-foursquare-founders-new-app-is-an-ai-powered-dj-for-neighborhood-updates-202326296.html?src=rss",
          "content": "Foursquare founder Dennis Crowley has unveiled his latest venture and yes, it's another location-based social app. But, rather than the check-ins Crowley first popularized more than 15 years ago, \"BeeBot\" has a very 2025 take on the concept. Instead, the app is an AI-powered \"DJ\" that can deliver contextual audio updates to your ears as you move through your day. Crowley describes BeeBot as an \"app for AirPods,\" though it will work with any type of headphones, as well as smart glasses with audio capabilities like Meta's. \"Whenever you put your AirPods in, it turns on,\" Crowley explains in a post on Medium. \"Whenever you take your AirPods out it turns off. And when it’s 'on' it’ll push you snippets of audio about the people, places, and events that are nearby.'To do this, you'll need to give the app access to your location and share a handful of \"keywords\" about your interests. You can also share your contacts to get updates from friends who are using the app. The BeeBot \"DJ,\" which of course has an AI voice, will then be able to talk to you as you throughout the dat and alert you to interesting events, landmarks or updates from friends who happen to be nearby. In some ways, it sounds like Crowley is trying to re-create some of the serendipitous IRL social interactions enabled by the original version of Foursquare. BeeBot doesn't have \"mayorships,\" badges or any of the gamification features that helped popularize FourSquare, but it's meant to have some of the \"same playful spirit,\" of OG Foursquare, according to Crowley. (Foursquare shut down its city guide app of the same name earlier this year, though its check-in app, Swarm, lives on.) And, because it's 2025, there's also a whole bunch of AI thrown in, including \"a mix of different LLMs\" and \"synthetic voices.\" The app is \"powered by a TikTok-style algorithm,\" Crowley says, \"but one that’s focused on what’s happening nearby and IRL.\" There also seems to be a bit of DNA from Marsbot, the short-lived (and IMO very underrated) chat-based app Foursquare launched in 2016 that could proactively provide personalized restaurant recommendations. While BeeBot isn't as heavily focused on neighborhood recommendations as Foursquare, it is meant to proactively let you know about nearby happenings you might be interested in or even a bit of gossip from friends. In addition to friends' status updates, the app will draw on local Substacks and newsletters for relevant info about a given area. Crowley says the DJ's audio cues may \"occasionally\" interrupt your music or podcast to give an update, though users should expect to hear these only a couple times throughout the day. BeeBot won't interrupt voice or video calls. BeeBot, which is the inaugural project of Crowley's new company Hopscotch Labs, is out now in the App Store, though it's still \"very much a work in progress,\" according to Crowley. \"I feel like the version of the product we’re sharing with folks today is kind of where Foursquare was when it launched at SXSW in 2009 – an interesting vision, a good-enough execution, but something that still needs to be shaped by the people using it to fully blossom.\" This article originally appeared on Engadget at https://www.engadget.com/apps/the-foursquare-founders-new-app-is-an-ai-powered-dj-for-neighborhood-updates-202326296.html?src=rss",
          "feed_position": 40
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/fitbit-black-friday-deals-are-here-early-and-one-of-our-favorite-fitness-trackers-is-on-sale-for-100-185704387.html",
          "published_at": "Wed, 05 Nov 2025 18:57:04 +0000",
          "title": "Fitbit Black Friday deals are here early and one of our favorite fitness trackers is on sale for $100",
          "standfirst": "Fitbit is holding an early Black Friday and there are plenty of noteworthy deals. The Charge 6 fitness tracker is on sale for $100, which is a nice discount of $60 and 38 percent off. The deal applies to multiple colorways. The Charge 6 topped our list of the best fitness trackers, and for good reason. It features built-in GPS, which means it doesn't have to be tethered to a phone when doing some cardio. The heart rate monitor is much more accurate when compared to the Charge 5 and the battery lasts a full week before requiring a trip to the outlet. All told, it tracks 20 exercise types, in addition to sleep. The Charge 6 features a full-color AMOLED display that's easy on the eyes and a relatively thin design. This makes it feel fairly premium, especially when compared to rival devices. On the downside, there's no real integration with Apple Health. This could be a dealbreaker for some. Also, some health data is hidden behind a Fitbit Premium paywall. That service costs $10 each month. This isn't the only Fitbit product on sale right now. The Inspire 3 fitness tracker is available for $70, which is a discount of 30 percent.This article originally appeared on Engadget at https://www.engadget.com/deals/fitbit-black-friday-deals-are-here-early-and-one-of-our-favorite-fitness-trackers-is-on-sale-for-100-185704387.html?src=rss",
          "content": "Fitbit is holding an early Black Friday and there are plenty of noteworthy deals. The Charge 6 fitness tracker is on sale for $100, which is a nice discount of $60 and 38 percent off. The deal applies to multiple colorways. The Charge 6 topped our list of the best fitness trackers, and for good reason. It features built-in GPS, which means it doesn't have to be tethered to a phone when doing some cardio. The heart rate monitor is much more accurate when compared to the Charge 5 and the battery lasts a full week before requiring a trip to the outlet. All told, it tracks 20 exercise types, in addition to sleep. The Charge 6 features a full-color AMOLED display that's easy on the eyes and a relatively thin design. This makes it feel fairly premium, especially when compared to rival devices. On the downside, there's no real integration with Apple Health. This could be a dealbreaker for some. Also, some health data is hidden behind a Fitbit Premium paywall. That service costs $10 each month. This isn't the only Fitbit product on sale right now. The Inspire 3 fitness tracker is available for $70, which is a discount of 30 percent.This article originally appeared on Engadget at https://www.engadget.com/deals/fitbit-black-friday-deals-are-here-early-and-one-of-our-favorite-fitness-trackers-is-on-sale-for-100-185704387.html?src=rss",
          "feed_position": 45
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/the-best-vpn-deals-88-percent-discounts-on-protonvpn-expressvpn-surfshark-and-more-120056432.html",
          "published_at": "Wed, 05 Nov 2025 18:23:27 +0000",
          "title": "The best VPN deals: 88 percent discounts on ProtonVPN, ExpressVPN, Surfshark and more",
          "standfirst": "A virtual private network (VPN) is useful in several ways — a good one can stream foreign TV shows and events, save you from giving up information to hackers and keep you anonymous to protect against online tracking. Although we strongly recommend using a VPN, a bit of comparison shopping goes a long way in this market. VPN pricing can be opaque, and providers don't always portray their best deals accurately. Even so, there are genuinely great bargains on the table. VPN providers give out deep discounts to customers who sign up for a year or more at a time. This lets them boost their subscriber numbers, but it's a win for you as well — while you pay out more upfront, if you divide the cost by the months of service, it's significantly cheaper over time. Most of the deals we highlight below follow that pattern, so make sure you're comfortable with a longer commitment before you take the plunge. If you've been thinking about subscribing to a VPN service, read on for the best VPN deals we could find right now. Best VPN deals ExpressVPN Basic — $97.72 for a two-year subscription with four months free (73 percent off): This is one of the best VPNs, especially for new users, who will find its apps and website headache-free on all platforms. In tests for my ExpressVPN review, it dropped my download speeds by less than 7 percent and successfully changed my virtual location 14 out of 15 times. In short, it's an all-around excellent service that only suffers from being a little overpriced — which is why I'm so excited whenever I find it offering a decent deal. This deal, which gets you 28 months of ExpressVPN service, represents a 73 percent savings. ExpressVPN Advanced — $125.72 for a two-year subscription with four months free (67 percent off): ExpressVPN recently split its pricing into multiple tiers, but they all still come with similar discounts for going long. In addition to top-tier VPN service, advanced users get two additional simultaneous connections (for a total of 12), the ExpressVPN Keys password manager, advanced ad and tracker blocking, ID protection features and a 50 percent discount on an AirCove router. NordVPN Basic — $80.73 for a two-year subscription with three months free (74 percent off): NordVPN gets the most important parts of a VPN right. It's fast, it doesn't leak any of your data and it's great at changing your virtual location. I noted in my NordVPN review that it always connects quickly and includes a support page that makes it easy to get live help. Although I'm sad to see it shutting down Meshnet, NordVPN still includes a lot of cool features, like servers that instantly connect you to Tor. This early Black Friday deal gives you 74 percent off the two-year plan, which also comes with three extra months. NordVPN Plus — $105.03 for a two-year subscription with three months free (74 percent off): In another early Black Friday discount, NordVPN has also taken 74 percent off its Plus subscription. For only a little more, you get a powerful ad and tracker blocker that can also catch malware downloads, plus access to the NordPass password manager. A Plus plan also adds a data breach scanner that checks the dark web for your sensitive information. Surfshark Starter — $53.73 for a two-year subscription with three months free (87 percent off): This is the \"basic\" level of Surfshark, but it includes the entire VPN; everything on Surfshark One is an extra perk. With this subscription, you'll get some of the most envelope-pushing features in the VPN world right now. Surfshark can rotate your IP constantly to help you evade detection — it even lets you choose your own entry and exit nodes for a double-hop connection. That all comes with a near-invisible impact on download speeds. With this year-round deal, you can save 87 percent on 27 months of Surfshark. Surfshark One — $59.13 for a two-year subscription with three months free (88 percent off): A VPN is great, but it's not enough to protect your data all on its own. Surfshark One adds several apps that boost your security beyond just VPN service, including Surfshark Antivirus (scans devices and downloads for malware) and Surfshark Alert (alerts you whenever your sensitive information shows up in a data breach), plus Surfshark Search and Alternative ID from the tier below. This extra-low deal gives you 88 percent off all those features. If you bump up to Surfshark One+, you'll also get data removal through Incogni, but the price jumps enough that it's not quite worthwhile in my eyes. CyberGhost — $56.94 for a two-year subscription with two months free (83 percent off): CyberGhost has some of the best automation you'll see on any VPN. With its Smart Rules system, you can determine how its apps respond to different types of Wi-Fi networks, with exceptions for specific networks you know by name. Typically, you can set it to auto-connect, disconnect or send you a message asking what to do. CyberGhost's other best feature is its streaming servers — I've found both better video quality and more consistent unblocking when I use them on streaming sites. Currently, you can get 26 months of CyberGhost for 83 percent off the usual price. hide.me — $59.95 for a two-year subscription with five months free (79 percent off): Hide.me is an excellent free VPN — in fact, it's my favorite on the market, even with EventVPN and the free version of Proton VPN as competition. If you do want to upgrade to its paid plan, though, the two-year subscription offers great savings. Hide.me works well as a no-frills beginner VPN, with apps and a server network it should frankly be charging more for. Private Internet Access — $79 for a three-year subscription with three months free (83 percent off): It's a bit hard to find (the link at the start of this paragraph includes the coupon), but Private Internet Access (PIA) is giving out the best available price right now on a VPN I'd recommend using. With this deal, you can get 39 months of PIA for a little bit over $2 per month — an 83 percent discount on its monthly price. Despite being so cheap, PIA has plenty of features, coming with its own DNS servers, a built-in ad blocker and automation powers to rival CyberGhost. However, internet speeds can fluctuate while you're connected. What makes a good VPN deal Practically every VPN heavily discounts its long-term subscriptions year-round, with even sharper discounts around occasions like Black Friday/Cyber Monday. The only noteworthy exception is Mullvad, the Costco hot dog of VPNs (that's a compliment, to be clear). When there's constantly a huge discount going on, it can be hard to tell when you're actually getting a good deal. The best way to squeeze out more savings is to look for seasonal deals, student discounts or exclusive sales like Proton VPN's coupon for Engadget readers. One trick VPNs often use is to add extra months onto an introductory deal, pushing the average monthly price even lower. When it comes time to renew, you usually can't get these extra months again. You often can't even renew for the same basic period of time — for example, you may only be able to renew a two-year subscription for one year. If you're planning to hold onto a VPN indefinitely, check the fine print to see how much it will cost per month after the first renewal, and ensure that fits into your budget. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/the-best-vpn-deals-88-percent-discounts-on-protonvpn-expressvpn-surfshark-and-more-120056432.html?src=rss",
          "content": "A virtual private network (VPN) is useful in several ways — a good one can stream foreign TV shows and events, save you from giving up information to hackers and keep you anonymous to protect against online tracking. Although we strongly recommend using a VPN, a bit of comparison shopping goes a long way in this market. VPN pricing can be opaque, and providers don't always portray their best deals accurately. Even so, there are genuinely great bargains on the table. VPN providers give out deep discounts to customers who sign up for a year or more at a time. This lets them boost their subscriber numbers, but it's a win for you as well — while you pay out more upfront, if you divide the cost by the months of service, it's significantly cheaper over time. Most of the deals we highlight below follow that pattern, so make sure you're comfortable with a longer commitment before you take the plunge. If you've been thinking about subscribing to a VPN service, read on for the best VPN deals we could find right now. Best VPN deals ExpressVPN Basic — $97.72 for a two-year subscription with four months free (73 percent off): This is one of the best VPNs, especially for new users, who will find its apps and website headache-free on all platforms. In tests for my ExpressVPN review, it dropped my download speeds by less than 7 percent and successfully changed my virtual location 14 out of 15 times. In short, it's an all-around excellent service that only suffers from being a little overpriced — which is why I'm so excited whenever I find it offering a decent deal. This deal, which gets you 28 months of ExpressVPN service, represents a 73 percent savings. ExpressVPN Advanced — $125.72 for a two-year subscription with four months free (67 percent off): ExpressVPN recently split its pricing into multiple tiers, but they all still come with similar discounts for going long. In addition to top-tier VPN service, advanced users get two additional simultaneous connections (for a total of 12), the ExpressVPN Keys password manager, advanced ad and tracker blocking, ID protection features and a 50 percent discount on an AirCove router. NordVPN Basic — $80.73 for a two-year subscription with three months free (74 percent off): NordVPN gets the most important parts of a VPN right. It's fast, it doesn't leak any of your data and it's great at changing your virtual location. I noted in my NordVPN review that it always connects quickly and includes a support page that makes it easy to get live help. Although I'm sad to see it shutting down Meshnet, NordVPN still includes a lot of cool features, like servers that instantly connect you to Tor. This early Black Friday deal gives you 74 percent off the two-year plan, which also comes with three extra months. NordVPN Plus — $105.03 for a two-year subscription with three months free (74 percent off): In another early Black Friday discount, NordVPN has also taken 74 percent off its Plus subscription. For only a little more, you get a powerful ad and tracker blocker that can also catch malware downloads, plus access to the NordPass password manager. A Plus plan also adds a data breach scanner that checks the dark web for your sensitive information. Surfshark Starter — $53.73 for a two-year subscription with three months free (87 percent off): This is the \"basic\" level of Surfshark, but it includes the entire VPN; everything on Surfshark One is an extra perk. With this subscription, you'll get some of the most envelope-pushing features in the VPN world right now. Surfshark can rotate your IP constantly to help you evade detection — it even lets you choose your own entry and exit nodes for a double-hop connection. That all comes with a near-invisible impact on download speeds. With this year-round deal, you can save 87 percent on 27 months of Surfshark. Surfshark One — $59.13 for a two-year subscription with three months free (88 percent off): A VPN is great, but it's not enough to protect your data all on its own. Surfshark One adds several apps that boost your security beyond just VPN service, including Surfshark Antivirus (scans devices and downloads for malware) and Surfshark Alert (alerts you whenever your sensitive information shows up in a data breach), plus Surfshark Search and Alternative ID from the tier below. This extra-low deal gives you 88 percent off all those features. If you bump up to Surfshark One+, you'll also get data removal through Incogni, but the price jumps enough that it's not quite worthwhile in my eyes. CyberGhost — $56.94 for a two-year subscription with two months free (83 percent off): CyberGhost has some of the best automation you'll see on any VPN. With its Smart Rules system, you can determine how its apps respond to different types of Wi-Fi networks, with exceptions for specific networks you know by name. Typically, you can set it to auto-connect, disconnect or send you a message asking what to do. CyberGhost's other best feature is its streaming servers — I've found both better video quality and more consistent unblocking when I use them on streaming sites. Currently, you can get 26 months of CyberGhost for 83 percent off the usual price. hide.me — $59.95 for a two-year subscription with five months free (79 percent off): Hide.me is an excellent free VPN — in fact, it's my favorite on the market, even with EventVPN and the free version of Proton VPN as competition. If you do want to upgrade to its paid plan, though, the two-year subscription offers great savings. Hide.me works well as a no-frills beginner VPN, with apps and a server network it should frankly be charging more for. Private Internet Access — $79 for a three-year subscription with three months free (83 percent off): It's a bit hard to find (the link at the start of this paragraph includes the coupon), but Private Internet Access (PIA) is giving out the best available price right now on a VPN I'd recommend using. With this deal, you can get 39 months of PIA for a little bit over $2 per month — an 83 percent discount on its monthly price. Despite being so cheap, PIA has plenty of features, coming with its own DNS servers, a built-in ad blocker and automation powers to rival CyberGhost. However, internet speeds can fluctuate while you're connected. What makes a good VPN deal Practically every VPN heavily discounts its long-term subscriptions year-round, with even sharper discounts around occasions like Black Friday/Cyber Monday. The only noteworthy exception is Mullvad, the Costco hot dog of VPNs (that's a compliment, to be clear). When there's constantly a huge discount going on, it can be hard to tell when you're actually getting a good deal. The best way to squeeze out more savings is to look for seasonal deals, student discounts or exclusive sales like Proton VPN's coupon for Engadget readers. One trick VPNs often use is to add extra months onto an introductory deal, pushing the average monthly price even lower. When it comes time to renew, you usually can't get these extra months again. You often can't even renew for the same basic period of time — for example, you may only be able to renew a two-year subscription for one year. If you're planning to hold onto a VPN indefinitely, check the fine print to see how much it will cost per month after the first renewal, and ensure that fits into your budget. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/the-best-vpn-deals-88-percent-discounts-on-protonvpn-expressvpn-surfshark-and-more-120056432.html?src=rss",
          "feed_position": 48
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/the-agent-builder-arms-race-continues-as-google-cloud-pushes-deeper-into",
          "published_at": "Wed, 05 Nov 2025 17:44:00 GMT",
          "title": "Google Cloud updates its AI Agent Builder with new observability dashboard and faster build-and-deploy tools",
          "standfirst": "Google Cloud has introduced a big update in a bid to keep AI developers on its Vertex AI platform for concepting, designing, building, testing, deploying and modifying AI agents in enterprise use cases.The new features, announced today, include additional governance tools for enterprises and expanding the capabilities for creating agents with just a few lines of code, moving faster with state-of-the-art context management layers and one-click deployment, as well as managed services for scaling production and evaluation, and support for identifying agents.Agent Builder, released last year during its annual Cloud Next event, provides a no-code platform for enterprises to create agents and connect these to orchestration frameworks like LangChain.Google’s Agent Development Kit (ADK), which lets developers build agents “in under 100 lines of code,” can also be accessed through Agent Builder. “These new capabilities underscore our commitment to Agent Builder, and simplify the agent development process to meet developers where they are, no matter which tech stack they choose,” said Mike Clark, director of Product Management, Vertex AI Agent Builder. Build agents fasterPart of Google’s pitch for Agent Builder’s new features is that enterprises can bake in-orchestration even as they construct their agents. “Building an agent from a concept to a working product involves complex orchestration,” said Clark. The new capabilities, which are shipped with the ADK, include:SOTA context management layers including Static, Turn, User and Cache layers so enterprises have more control over the agents’ contextPrebuilt plugins with customizable logic. One of the new plugins allows agents to recognize failed tool calls and “self-heal” by retrying the task with a different approachAdditional language support in ADK, including Go, alongside Python and Java, that launched with ADKOne-click deployment through the ADK command line interface to move agents from a local environment to live testing with a single commandGovernance layerEnterprises require high accuracy; security; observability and auditability (what a program did and why); and steerability (control) in their production-grade AI agents.While Google had observability features in the local development environment at launch, developers can now access these tools through the Agent Engine managed runtime dashboard. The company said this brings cloud-based production monitoring to track token consumption, error rates and latency. Within this observability dashboard, enterprises can visualize the actions agents take and reproduce any issues. Agent Engine will also have a new Evaluation Layer to help “simulate agent performance across a vast array of user interactions and situations.”This governance layer will also include:Agent Identities that Google said give “agents their own unique, native identities within Google Cloud Model Armor, which would block prompt injections, screen tool calls and agent responsesSecurity Command Center, so admins can build an inventory of their agents to detect threats like unauthorized access“These native identities provide a deep, built-in layer of control and a clear audit trail for all agent actions. These certificate-backed identities further strengthen your security as they cannot be impersonated and are tied directly to the agent&#x27;s lifecycle, eliminating the risk of dormant accounts,” Clark said. The battle of agent builders It’s no surprise that model providers create platforms to build agents and bring them to production. The competition lies in how fast new tools and features are added.Google’s Agent Builder competes with OpenAI’s open-source Agent Development Kit, which enables developers to create AI agents using non-OpenAI models. Additionally, there is the recently announced AgentKit, which features an Agent Builder that enables companies to integrate agents into their applications easily. Microsoft has its Azure AI Foundry, launched last year around this time for AI agent creation, and AWS also offers agent builders on its Bedrock platform, but Google is hoping is suite of new features will help give it a competitive edge. However, it isn’t just companies with their own models that court developers to build their AI agents within their platforms. Any enterprise service provider with an agent library also wants clients to make agents on their systems. Capturing developer interest and keeping them within the ecosystem is the big battle between tech companies now, with features to make building and governing agents easier.",
          "content": "Google Cloud has introduced a big update in a bid to keep AI developers on its Vertex AI platform for concepting, designing, building, testing, deploying and modifying AI agents in enterprise use cases.The new features, announced today, include additional governance tools for enterprises and expanding the capabilities for creating agents with just a few lines of code, moving faster with state-of-the-art context management layers and one-click deployment, as well as managed services for scaling production and evaluation, and support for identifying agents.Agent Builder, released last year during its annual Cloud Next event, provides a no-code platform for enterprises to create agents and connect these to orchestration frameworks like LangChain.Google’s Agent Development Kit (ADK), which lets developers build agents “in under 100 lines of code,” can also be accessed through Agent Builder. “These new capabilities underscore our commitment to Agent Builder, and simplify the agent development process to meet developers where they are, no matter which tech stack they choose,” said Mike Clark, director of Product Management, Vertex AI Agent Builder. Build agents fasterPart of Google’s pitch for Agent Builder’s new features is that enterprises can bake in-orchestration even as they construct their agents. “Building an agent from a concept to a working product involves complex orchestration,” said Clark. The new capabilities, which are shipped with the ADK, include:SOTA context management layers including Static, Turn, User and Cache layers so enterprises have more control over the agents’ contextPrebuilt plugins with customizable logic. One of the new plugins allows agents to recognize failed tool calls and “self-heal” by retrying the task with a different approachAdditional language support in ADK, including Go, alongside Python and Java, that launched with ADKOne-click deployment through the ADK command line interface to move agents from a local environment to live testing with a single commandGovernance layerEnterprises require high accuracy; security; observability and auditability (what a program did and why); and steerability (control) in their production-grade AI agents.While Google had observability features in the local development environment at launch, developers can now access these tools through the Agent Engine managed runtime dashboard. The company said this brings cloud-based production monitoring to track token consumption, error rates and latency. Within this observability dashboard, enterprises can visualize the actions agents take and reproduce any issues. Agent Engine will also have a new Evaluation Layer to help “simulate agent performance across a vast array of user interactions and situations.”This governance layer will also include:Agent Identities that Google said give “agents their own unique, native identities within Google Cloud Model Armor, which would block prompt injections, screen tool calls and agent responsesSecurity Command Center, so admins can build an inventory of their agents to detect threats like unauthorized access“These native identities provide a deep, built-in layer of control and a clear audit trail for all agent actions. These certificate-backed identities further strengthen your security as they cannot be impersonated and are tied directly to the agent&#x27;s lifecycle, eliminating the risk of dormant accounts,” Clark said. The battle of agent builders It’s no surprise that model providers create platforms to build agents and bring them to production. The competition lies in how fast new tools and features are added.Google’s Agent Builder competes with OpenAI’s open-source Agent Development Kit, which enables developers to create AI agents using non-OpenAI models. Additionally, there is the recently announced AgentKit, which features an Agent Builder that enables companies to integrate agents into their applications easily. Microsoft has its Azure AI Foundry, launched last year around this time for AI agent creation, and AWS also offers agent builders on its Bedrock platform, but Google is hoping is suite of new features will help give it a competitive edge. However, it isn’t just companies with their own models that court developers to build their AI agents within their platforms. Any enterprise service provider with an agent library also wants clients to make agents on their systems. Capturing developer interest and keeping them within the ecosystem is the big battle between tech companies now, with features to make building and governing agents easier.",
          "feed_position": 4,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/T84hNL2FLkCHG8Qpdt8AR/8ad71c54daf9b7e5a06242ac44108a23/crimedy7_illustration_of_robots_constructing_a_building_vivid_40fa4859-c9cc-453b-ad1e-3f01969c44fe_2.png?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/security/anthropic-claude-speeds-soc-threat-analysis-43x",
          "published_at": "Wed, 05 Nov 2025 08:00:00 GMT",
          "title": "How Anthropic's Claude cuts SOC investigation time from 5 hours to 7 minutes",
          "standfirst": "Integrating AI models directly into extended detection and response (XDR) platforms is delivering breakthrough improvements in SOC investigation speed and accuracy.In an exclusive interview with VentureBeat, eSentire revealed that deploying Anthropic&#x27;s Claude across their Atlas XDR Platform compresses comprehensive threat investigations from five hours to seven minutes, delivering a 43x speed improvement, while matching senior SOC analyst decision-making with 95% accuracy.The typical enterprise SOC handles roughly 10,000 alerts daily, according to Dropzone AI&#x27;s research. SOC analysts tell VentureBeat that, on average, they can investigate just 22% to 25% of all alerts. Depending on how the SOC was configured and whether there&#x27;s too much reliance on legacy, non-integrated systems, false positives can reach 80%. The result: Critical threats go uninvestigated while analysts spend entire shifts on manual evidence-gathering workflows.\"We&#x27;re not looking to remove work but deliver better outcomes,\" Dustin Hillard, chief product and technology officer at eSentire, told VentureBeat. \"It really means understanding a threat better for our customers. When we say five hours of work in a few minutes, that&#x27;s 30 different evidence-gathering steps dynamically generated in the context of that specific security investigation.\"The breakthrough comes from integrating AI at the platform level. ESentire&#x27;s approach enables Anthropic&#x27;s Claude to orchestrate multi-tool workflows that correlate threat patterns across thousands of data points simultaneously, in essence replicating how senior analysts think but at machine speed.Platform integration represents XDR&#x27;s next evolution as AI adoption acceleratesSecurity copilots initially took aim at operational pains preventing SOC analysts from excelling at their work. They prove extremely useful for accelerating triage, alert de-duplication, noise suppression, firewall tuning and many other tasks. VentureBeat&#x27;s Security Copilot Guide, a comparative matrix of 16 vendors, reveals how copilots are designed to be tailored to the specific strengths of a given SOC&#x27;s analyst team.The next evolution moves beyond standalone copilots as major XDR vendors integrate third-party AI models directly into their platforms. ESentire&#x27;s approach with Anthropic&#x27;s Claude demonstrates how deeply-integrated AI can transform investigation workflows. The company&#x27;s DevOps and engineering teams discovered that platform-integrated AI can deliver comprehensive threat investigations matching senior SOC analyst decision-making with 95% accuracy, while reducing investigation time from five hours to under seven minutes, providing a 43x speed improvement.\"The ideal approach is typically to use AI as a force multiplier for human analysts rather than a replacement,\" Vineet Arora, CTO for WinWire, told VentureBeat. \"For example, AI can handle initial alert triage and routine responses to security issues, allowing analysts to focus their expertise on sophisticated threats and strategic work.\"eSentire&#x27;s Hillard noted: \"Earlier this year, around Claude 3.7, we started seeing the tool selection and the reasoning of conclusions across multiple evidence-gathering steps get to the point where it was matching our experts. That&#x27;s what really got us excited. We were hitting on something that would allow us to deliver better investigation quality for our customers, not just efficiency.\"The company compared Claude&#x27;s autonomous investigations against their most experienced Tier 3 SOC analysts across 1,000 diverse scenarios spanning ransomware, lateral movement, credential compromise and advanced persistent threats, finding that it achieved 95% alignment with expert judgment and 99.3% threat suppression on first contact.Multi-tool orchestration replicates senior analyst reasoning at machine speedeSentire&#x27;s DevOps and R&D teams integrated AI at the baseline of their Atlas XDR platform to deliver greater accuracy, speed and scale in SOC operations. Anthropic&#x27;s Claude handles the orchestration of multi-tool workflows that correlate threat patterns across thousands of data points. The system synthesizes evidence from endpoint telemetry, network traffic, log data, cloud environments, identity systems and vulnerability feeds simultaneously, all of which previously forced analysts into a series of serial investigation steps consuming entire shifts.Hillard explained that the deployment runs on Amazon Bedrock, with LangGraph providing the agentic orchestration framework that enables Anthropic&#x27;s Claude to select tools and reason through multi-step investigations dynamically. Each workflow inherits customer-specific access tokens that cascade through the Atlas Actions platform. By taking this approach, Hillard says every tool call, data query and vendor integration stays secure in tenant isolation.\"Using Bedrock was actually quite simple for us because we&#x27;ve been on AWS basically since the platform started,\" Hillard explained. \"The way Anthropic models are deployed within Bedrock makes everything locked tight in a way that we and our customers got comfortable with. A lot of our customers are critical infrastructure companies with extreme sensitivity around their data.\"When an incident triggers, a typical detection and response system has 15 minutes to contain it before lateral movement threatens broader infrastructure. That time window previously forced rapid-fire triage that precluded deep investigation. Hillard explains that Anthropic&#x27;s Claude is helping to turn that time pressure into an advantage by executing comprehensive evidence gathering across all telemetry sources — querying process trees, conducting log searches across stored telemetry, correlating related incidents from historical ticketing data and cross-referencing active threat intelligence.The Atlas XDR platform&#x27;s investigation process dynamically generates approximately 30 evidence-gathering steps tailored to each specific threat scenario across three dimensions: deeper analysis of security telemetry, context from related past incidents at the customer and threat landscape intelligence about what active threat actors are doing across eSentire&#x27;s entire customer base.Network effects amplify threat intelligence across customer deploymentsESentire&#x27;s Threat Response Unit uses Anthropic&#x27;s Claude to search across log, endpoint, network, cloud and identity data. When the team identifies emergent threat actor behaviors — through open-source intelligence or protecting critical infrastructure customers who see attacks first — they reflect those patterns against their 2,000-plus customers to identify repeated techniques before damage occurs.An attack against one customer strengthens defenses for all customers, as Claude enables the Atlas XDR platform to continually learn from new threats. Hillard told VentureBeat that the platform&#x27;s threat hunting stays ahead of commercial feeds 35% of the time and identifies threats never seen in commercial feeds 12% of the time.\"What used to take our experts a week to accomplish, they can now do in an hour,\" Hillard says. \"When they have a creative idea to test a new data analysis pattern, work that might have taken an engineering team a month to build, they can now do it directly in natural language.\"The velocity shift enables analysts to test hypotheses in hours rather than weeks, amplifying human expertise rather than replacing it. Hillard says the approach is working at scale across customers’ critical infrastructure deployments.Streamlined workflows prevent analyst burnout before it happensThe performance improvements address a growing workforce challenge before it becomes a crisis. SOC analysts tell VentureBeat the industry is decades away from a completely autonomous SOC, with many analysts relying on swivel chair integration (moving from one system to another to resolve alerts). This fragmented approach wastes time, introduces errors and contributes to analyst burnout.More than 70% of SOC analysts say they&#x27;re burned out, with 66% reporting that half their work is repetitive enough to be automated. In anonymous conversations VentureBeat conducts regularly via Signal, SOC analysts confide that a six-month to one-year tenure has become common. One analyst reported a 96% false positive rate in their environment — conditions that make effective threat detection nearly impossible.The U.S. Bureau of Labor Statistics projects information security analyst positions will grow 33% from 2023 to 2033, vastly outpacing the 4% average across all occupations. Using AI-based platforms to streamline SOC workflows represents a crucial strategy for enterprises to prevent burnout before it forces their best security talent to leave for less demanding roles.The strategic shift to platform-integrated AIAs enterprises face projected 33% growth in security analyst positions through 2033, platform-integrated AI offers a path to scale SOC operations without proportionally scaling headcount. The shift from five-hour investigations to seven-minute automated workflows doesn&#x27;t eliminate the need for senior analysts; it amplifies their expertise by enabling them to focus on sophisticated threat hunting and strategic work rather than repetitive evidence-gathering tasks.Platform-integrated AI represents a fundamental change in SOC economics. The 43x speed improvement that eSentire achieved demonstrates that AI can replicate elite analyst decision-making with 95% accuracy when integrated adequately at the platform level — not by replacing human expertise, but by automating the workflows that previously consumed entire shifts and left critical threats uninvestigated.The question for enterprise security leaders is how quickly organizations can integrate AI at the platform level to improve SOC performance before the combination of alert overload and manual workflows drives analysts to leave. For critical infrastructure protection, the ability to investigate threats 43 times faster while maintaining 95% accuracy while aligning with senior analysts represents the difference between staying ahead of adversaries and falling behind.",
          "content": "Integrating AI models directly into extended detection and response (XDR) platforms is delivering breakthrough improvements in SOC investigation speed and accuracy.In an exclusive interview with VentureBeat, eSentire revealed that deploying Anthropic&#x27;s Claude across their Atlas XDR Platform compresses comprehensive threat investigations from five hours to seven minutes, delivering a 43x speed improvement, while matching senior SOC analyst decision-making with 95% accuracy.The typical enterprise SOC handles roughly 10,000 alerts daily, according to Dropzone AI&#x27;s research. SOC analysts tell VentureBeat that, on average, they can investigate just 22% to 25% of all alerts. Depending on how the SOC was configured and whether there&#x27;s too much reliance on legacy, non-integrated systems, false positives can reach 80%. The result: Critical threats go uninvestigated while analysts spend entire shifts on manual evidence-gathering workflows.\"We&#x27;re not looking to remove work but deliver better outcomes,\" Dustin Hillard, chief product and technology officer at eSentire, told VentureBeat. \"It really means understanding a threat better for our customers. When we say five hours of work in a few minutes, that&#x27;s 30 different evidence-gathering steps dynamically generated in the context of that specific security investigation.\"The breakthrough comes from integrating AI at the platform level. ESentire&#x27;s approach enables Anthropic&#x27;s Claude to orchestrate multi-tool workflows that correlate threat patterns across thousands of data points simultaneously, in essence replicating how senior analysts think but at machine speed.Platform integration represents XDR&#x27;s next evolution as AI adoption acceleratesSecurity copilots initially took aim at operational pains preventing SOC analysts from excelling at their work. They prove extremely useful for accelerating triage, alert de-duplication, noise suppression, firewall tuning and many other tasks. VentureBeat&#x27;s Security Copilot Guide, a comparative matrix of 16 vendors, reveals how copilots are designed to be tailored to the specific strengths of a given SOC&#x27;s analyst team.The next evolution moves beyond standalone copilots as major XDR vendors integrate third-party AI models directly into their platforms. ESentire&#x27;s approach with Anthropic&#x27;s Claude demonstrates how deeply-integrated AI can transform investigation workflows. The company&#x27;s DevOps and engineering teams discovered that platform-integrated AI can deliver comprehensive threat investigations matching senior SOC analyst decision-making with 95% accuracy, while reducing investigation time from five hours to under seven minutes, providing a 43x speed improvement.\"The ideal approach is typically to use AI as a force multiplier for human analysts rather than a replacement,\" Vineet Arora, CTO for WinWire, told VentureBeat. \"For example, AI can handle initial alert triage and routine responses to security issues, allowing analysts to focus their expertise on sophisticated threats and strategic work.\"eSentire&#x27;s Hillard noted: \"Earlier this year, around Claude 3.7, we started seeing the tool selection and the reasoning of conclusions across multiple evidence-gathering steps get to the point where it was matching our experts. That&#x27;s what really got us excited. We were hitting on something that would allow us to deliver better investigation quality for our customers, not just efficiency.\"The company compared Claude&#x27;s autonomous investigations against their most experienced Tier 3 SOC analysts across 1,000 diverse scenarios spanning ransomware, lateral movement, credential compromise and advanced persistent threats, finding that it achieved 95% alignment with expert judgment and 99.3% threat suppression on first contact.Multi-tool orchestration replicates senior analyst reasoning at machine speedeSentire&#x27;s DevOps and R&D teams integrated AI at the baseline of their Atlas XDR platform to deliver greater accuracy, speed and scale in SOC operations. Anthropic&#x27;s Claude handles the orchestration of multi-tool workflows that correlate threat patterns across thousands of data points. The system synthesizes evidence from endpoint telemetry, network traffic, log data, cloud environments, identity systems and vulnerability feeds simultaneously, all of which previously forced analysts into a series of serial investigation steps consuming entire shifts.Hillard explained that the deployment runs on Amazon Bedrock, with LangGraph providing the agentic orchestration framework that enables Anthropic&#x27;s Claude to select tools and reason through multi-step investigations dynamically. Each workflow inherits customer-specific access tokens that cascade through the Atlas Actions platform. By taking this approach, Hillard says every tool call, data query and vendor integration stays secure in tenant isolation.\"Using Bedrock was actually quite simple for us because we&#x27;ve been on AWS basically since the platform started,\" Hillard explained. \"The way Anthropic models are deployed within Bedrock makes everything locked tight in a way that we and our customers got comfortable with. A lot of our customers are critical infrastructure companies with extreme sensitivity around their data.\"When an incident triggers, a typical detection and response system has 15 minutes to contain it before lateral movement threatens broader infrastructure. That time window previously forced rapid-fire triage that precluded deep investigation. Hillard explains that Anthropic&#x27;s Claude is helping to turn that time pressure into an advantage by executing comprehensive evidence gathering across all telemetry sources — querying process trees, conducting log searches across stored telemetry, correlating related incidents from historical ticketing data and cross-referencing active threat intelligence.The Atlas XDR platform&#x27;s investigation process dynamically generates approximately 30 evidence-gathering steps tailored to each specific threat scenario across three dimensions: deeper analysis of security telemetry, context from related past incidents at the customer and threat landscape intelligence about what active threat actors are doing across eSentire&#x27;s entire customer base.Network effects amplify threat intelligence across customer deploymentsESentire&#x27;s Threat Response Unit uses Anthropic&#x27;s Claude to search across log, endpoint, network, cloud and identity data. When the team identifies emergent threat actor behaviors — through open-source intelligence or protecting critical infrastructure customers who see attacks first — they reflect those patterns against their 2,000-plus customers to identify repeated techniques before damage occurs.An attack against one customer strengthens defenses for all customers, as Claude enables the Atlas XDR platform to continually learn from new threats. Hillard told VentureBeat that the platform&#x27;s threat hunting stays ahead of commercial feeds 35% of the time and identifies threats never seen in commercial feeds 12% of the time.\"What used to take our experts a week to accomplish, they can now do in an hour,\" Hillard says. \"When they have a creative idea to test a new data analysis pattern, work that might have taken an engineering team a month to build, they can now do it directly in natural language.\"The velocity shift enables analysts to test hypotheses in hours rather than weeks, amplifying human expertise rather than replacing it. Hillard says the approach is working at scale across customers’ critical infrastructure deployments.Streamlined workflows prevent analyst burnout before it happensThe performance improvements address a growing workforce challenge before it becomes a crisis. SOC analysts tell VentureBeat the industry is decades away from a completely autonomous SOC, with many analysts relying on swivel chair integration (moving from one system to another to resolve alerts). This fragmented approach wastes time, introduces errors and contributes to analyst burnout.More than 70% of SOC analysts say they&#x27;re burned out, with 66% reporting that half their work is repetitive enough to be automated. In anonymous conversations VentureBeat conducts regularly via Signal, SOC analysts confide that a six-month to one-year tenure has become common. One analyst reported a 96% false positive rate in their environment — conditions that make effective threat detection nearly impossible.The U.S. Bureau of Labor Statistics projects information security analyst positions will grow 33% from 2023 to 2033, vastly outpacing the 4% average across all occupations. Using AI-based platforms to streamline SOC workflows represents a crucial strategy for enterprises to prevent burnout before it forces their best security talent to leave for less demanding roles.The strategic shift to platform-integrated AIAs enterprises face projected 33% growth in security analyst positions through 2033, platform-integrated AI offers a path to scale SOC operations without proportionally scaling headcount. The shift from five-hour investigations to seven-minute automated workflows doesn&#x27;t eliminate the need for senior analysts; it amplifies their expertise by enabling them to focus on sophisticated threat hunting and strategic work rather than repetitive evidence-gathering tasks.Platform-integrated AI represents a fundamental change in SOC economics. The 43x speed improvement that eSentire achieved demonstrates that AI can replicate elite analyst decision-making with 95% accuracy when integrated adequately at the platform level — not by replacing human expertise, but by automating the workflows that previously consumed entire shifts and left critical threats uninvestigated.The question for enterprise security leaders is how quickly organizations can integrate AI at the platform level to improve SOC performance before the combination of alert overload and manual workflows drives analysts to leave. For critical infrastructure protection, the ability to investigate threats 43 times faster while maintaining 95% accuracy while aligning with senior analysts represents the difference between staying ahead of adversaries and falling behind.",
          "feed_position": 5,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/lvLrOoy7DefrKbTeNVJtE/6f41356a6eb02e08e3d95ba70ed4cc38/Anthropic.jpg?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/ais-capacity-crunch-latency-risk-escalating-costs-and-the-coming-surge",
          "published_at": "Wed, 05 Nov 2025 05:00:00 GMT",
          "title": "AI’s capacity crunch: Latency risk, escalating costs, and the coming surge-pricing breakpoint",
          "standfirst": "The latest big headline in AI isn’t model size or multimodality — it’s the capacity crunch. At VentureBeat’s latest AI Impact stop in NYC, Val Bercovici, chief AI officer at WEKA, joined Matt Marshall, VentureBeat CEO, to discuss what it really takes to scale AI amid rising latency, cloud lock-in, and runaway costs.Those forces, Bercovici argued, are pushing AI toward its own version of surge pricing. Uber famously introduced surge pricing, bringing real-time market rates to ridesharing for the first time. Now, Bercovici argued, AI is headed toward the same economic reckoning — especially for inference — when the focus turns to profitability.\"We don&#x27;t have real market rates today. We have subsidized rates. That’s been necessary to enable a lot of the innovation that’s been happening, but sooner or later — considering the trillions of dollars of capex we’re talking about right now, and the finite energy opex — real market rates are going to appear; perhaps next year, certainly by 2027,\" he said. \"When they do, it will fundamentally change this industry and drive an even deeper, keener focus on efficiency.\"The economics of the token explosion\"The first rule is that this is an industry where more is more. More tokens equal exponentially more business value,\" Bercovici said. But so far, no one&#x27;s figured out how to make that sustainable. The classic business triad — cost, quality, and speed — translates in AI to latency, cost, and accuracy (especially in output tokens). And accuracy is non-negotiable. That holds not only for consumer interactions with agents like ChatGPT, but for high-stakes use cases such as drug discovery and business workflows in heavily regulated industries like financial services and healthcare.\"That’s non-negotiable,\" Bercovici said. \"You have to have a high amount of tokens for high inference accuracy, especially when you add security into the mix, guardrail models, and quality models. Then you’re trading off latency and cost. That’s where you have some flexibility. If you can tolerate high latency, and sometimes you can for consumer use cases, then you can have lower cost, with free tiers and low cost-plus tiers.\" However, latency is a critical bottleneck for AI agents. “These agents now don&#x27;t operate in any singular sense. You either have an agent swarm or no agentic activity at all,” Bercovici noted.In a swarm, groups of agents work in parallel to complete a larger objective. An orchestrator agent — the smartest model — sits at the center, determining subtasks and key requirements: architecture choices, cloud vs. on-prem execution, performance constraints, and security considerations. The swarm then executes all subtasks, effectively spinning up numerous concurrent inference users in parallel sessions. Finally, evaluator models judge whether the overall task was successfully completed.“These swarms go through what&#x27;s called multiple turns, hundreds if not thousands of prompts and responses until the swarm convenes on an answer,” Bercovici said. “And if you have a compound delay in those thousand turns, it becomes untenable. So latency is really, really important. And that means typically having to pay a high price today that&#x27;s subsidized, and that&#x27;s what&#x27;s going to have to come down over time.”Reinforcement learning as the new paradigmUntil around May of this year, agents weren&#x27;t that performant, Bercovici explained. And then context windows became large enough, and GPUs available enough, to support agents that could complete advanced tasks, like writing reliable software. It&#x27;s now estimated that in some cases, 90% of software is generated by coding agents. Now that agents have essentially come of age, Bercovici noted, reinforcement learning is the new conversation among data scientists at some of the leading labs, like OpenAI, Anthropic, and Gemini, who view it as a critical path forward in AI innovation..\"The current AI season is reinforcement learning. It blends many of the elements of training and inference into one unified workflow,” Bercovici said. “It’s the latest and greatest scaling law to this mythical milestone we’re all trying to reach called AGI — artificial general intelligence,” he added. \"What’s fascinating to me is that you have to apply all the best practices of how you train models, plus all the best practices of how you infer models, to be able to iterate these thousands of reinforcement learning loops and advance the whole field.\"The path to AI profitability There’s no one answer when it comes to building an infrastructure foundation to make AI profitable, Bercovici said, since it&#x27;s still an emerging field. There’s no cookie-cutter approach. Going all on-prem may be the right choice for some — especially frontier model builders — while being cloud-native or running in a hybrid environment may be a better path for organizations looking to innovate agilely and responsively. Regardless of which path they choose initially, organizations will need to adapt their AI infrastructure strategy as their business needs evolve.\"Unit economics are what fundamentally matter here,\" said Bercovici. \"We are definitely in a boom, or even in a bubble, you could say, in some cases, since the underlying AI economics are being subsidized. But that doesn’t mean that if tokens get more expensive, you’ll stop using them. You’ll just get very fine-grained in terms of how you use them.\" Leaders should focus less on individual token pricing and more on transaction-level economics, where efficiency and impact become visible, Bercovici concludes. The pivotal question enterprises and AI companies should be asking, Bercovici said, is “What is the real cost for my unit economics?”Viewed through that lens, the path forward isn’t about doing less with AI — it’s about doing it smarter and more efficiently at scale.",
          "content": "The latest big headline in AI isn’t model size or multimodality — it’s the capacity crunch. At VentureBeat’s latest AI Impact stop in NYC, Val Bercovici, chief AI officer at WEKA, joined Matt Marshall, VentureBeat CEO, to discuss what it really takes to scale AI amid rising latency, cloud lock-in, and runaway costs.Those forces, Bercovici argued, are pushing AI toward its own version of surge pricing. Uber famously introduced surge pricing, bringing real-time market rates to ridesharing for the first time. Now, Bercovici argued, AI is headed toward the same economic reckoning — especially for inference — when the focus turns to profitability.\"We don&#x27;t have real market rates today. We have subsidized rates. That’s been necessary to enable a lot of the innovation that’s been happening, but sooner or later — considering the trillions of dollars of capex we’re talking about right now, and the finite energy opex — real market rates are going to appear; perhaps next year, certainly by 2027,\" he said. \"When they do, it will fundamentally change this industry and drive an even deeper, keener focus on efficiency.\"The economics of the token explosion\"The first rule is that this is an industry where more is more. More tokens equal exponentially more business value,\" Bercovici said. But so far, no one&#x27;s figured out how to make that sustainable. The classic business triad — cost, quality, and speed — translates in AI to latency, cost, and accuracy (especially in output tokens). And accuracy is non-negotiable. That holds not only for consumer interactions with agents like ChatGPT, but for high-stakes use cases such as drug discovery and business workflows in heavily regulated industries like financial services and healthcare.\"That’s non-negotiable,\" Bercovici said. \"You have to have a high amount of tokens for high inference accuracy, especially when you add security into the mix, guardrail models, and quality models. Then you’re trading off latency and cost. That’s where you have some flexibility. If you can tolerate high latency, and sometimes you can for consumer use cases, then you can have lower cost, with free tiers and low cost-plus tiers.\" However, latency is a critical bottleneck for AI agents. “These agents now don&#x27;t operate in any singular sense. You either have an agent swarm or no agentic activity at all,” Bercovici noted.In a swarm, groups of agents work in parallel to complete a larger objective. An orchestrator agent — the smartest model — sits at the center, determining subtasks and key requirements: architecture choices, cloud vs. on-prem execution, performance constraints, and security considerations. The swarm then executes all subtasks, effectively spinning up numerous concurrent inference users in parallel sessions. Finally, evaluator models judge whether the overall task was successfully completed.“These swarms go through what&#x27;s called multiple turns, hundreds if not thousands of prompts and responses until the swarm convenes on an answer,” Bercovici said. “And if you have a compound delay in those thousand turns, it becomes untenable. So latency is really, really important. And that means typically having to pay a high price today that&#x27;s subsidized, and that&#x27;s what&#x27;s going to have to come down over time.”Reinforcement learning as the new paradigmUntil around May of this year, agents weren&#x27;t that performant, Bercovici explained. And then context windows became large enough, and GPUs available enough, to support agents that could complete advanced tasks, like writing reliable software. It&#x27;s now estimated that in some cases, 90% of software is generated by coding agents. Now that agents have essentially come of age, Bercovici noted, reinforcement learning is the new conversation among data scientists at some of the leading labs, like OpenAI, Anthropic, and Gemini, who view it as a critical path forward in AI innovation..\"The current AI season is reinforcement learning. It blends many of the elements of training and inference into one unified workflow,” Bercovici said. “It’s the latest and greatest scaling law to this mythical milestone we’re all trying to reach called AGI — artificial general intelligence,” he added. \"What’s fascinating to me is that you have to apply all the best practices of how you train models, plus all the best practices of how you infer models, to be able to iterate these thousands of reinforcement learning loops and advance the whole field.\"The path to AI profitability There’s no one answer when it comes to building an infrastructure foundation to make AI profitable, Bercovici said, since it&#x27;s still an emerging field. There’s no cookie-cutter approach. Going all on-prem may be the right choice for some — especially frontier model builders — while being cloud-native or running in a hybrid environment may be a better path for organizations looking to innovate agilely and responsively. Regardless of which path they choose initially, organizations will need to adapt their AI infrastructure strategy as their business needs evolve.\"Unit economics are what fundamentally matter here,\" said Bercovici. \"We are definitely in a boom, or even in a bubble, you could say, in some cases, since the underlying AI economics are being subsidized. But that doesn’t mean that if tokens get more expensive, you’ll stop using them. You’ll just get very fine-grained in terms of how you use them.\" Leaders should focus less on individual token pricing and more on transaction-level economics, where efficiency and impact become visible, Bercovici concludes. The pivotal question enterprises and AI companies should be asking, Bercovici said, is “What is the real cost for my unit economics?”Viewed through that lens, the path forward isn’t about doing less with AI — it’s about doing it smarter and more efficiently at scale.",
          "feed_position": 6,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/4htZ4RXS4bpdlPbXEaKYhp/1dd9f6065c85eae3d9e3e44acda4c7fa/IMG_8825.jpg?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/from-logs-to-insights-the-ai-breakthrough-redefining-observability",
          "published_at": "Wed, 05 Nov 2025 05:00:00 GMT",
          "title": "From logs to insights: The AI breakthrough redefining observability",
          "standfirst": "Presented by Elastic Logs set to become the primary tool for finding the “why” in diagnosing network incidents Modern IT environments have a data problem: there’s too much of it. Organizations that need to manage a company’s environment are increasingly challenged to detect and diagnose issues in real-time, optimize performance, improve reliability, and ensure security and compliance — all within constrained budgets. The modern observability landscape has many tools that offer a solution. Most revolve around DevOps teams or Site Reliability Engineers (SREs) analyzing logs, metrics, and traces to uncover patterns and figure out what’s happening across the network, and diagnose why an issue or incident occurred. The problem is that the process creates information overload: A Kubernetes cluster alone can emit 30 to 50 gigabytes of logs a day, and suspicious behavior patterns can sneak past human eyes. \"It’s so anachronistic now, in the world of AI, to think about humans alone observing infrastructure,\" says Ken Exner, chief product officer at Elastic. \"I hate to break it to you, but machines are better than human beings at pattern matching.“An industry-wide focus on visualizing symptoms forces engineers to manually hunt for answers. The crucial \"why\" is buried in logs, but because they contain massive volumes of unstructured data, the industry tends to use them as a tool of last resort. This has forced teams into costly tradeoffs: either spend countless hours building complex data pipelines, drop valuable log data and risk critical visibility gaps, or log and forget.Elastic, the Search AI Company, recently released a new feature for observability called Streams, which aims to become the primary signal for investigations by taking noisy logs and turning them into patterns, context and meaning. Streams uses AI to automatically partition and parse raw logs to extract relevant fields, and greatly reduce the effort required of SREs to make logs usable. Streams also automatically surfaces significant events such as critical errors and anomalies from context-rich logs, giving SREs early warnings and a clear understanding of their workloads, enabling them to investigate and resolve issues faster. The ultimate goal is to show remediation steps.\"From raw, voluminous, messy data, Streams automatically creates structure, putting it into a form that is usable, automatically alerts you to issues and helps you remediate them,\" Exner says. \"That is the magic of Streams.\"A broken workflowStreams upends an observability process that some say is broken. Typically, SREs set up metrics, logs and traces. Then they set up alerts, and service level objectives (SLOs) — often hard-coded rules to show where a service or process has gone beyond a threshold, or a specific pattern has been detected. When an alert is triggered, it points to the metric that&#x27;s showing an anomaly. From there, SREs look at a metrics dashboard, where they can visualize the issue and compare the alert to other metrics, or CPU to memory to I/O, and start looking for patterns. They may then need to look at a trace, and examine upstream and downstream dependencies across the application to dig into the root cause of the issue. Once they figure out what&#x27;s causing the trouble, they jump into the logs for that database or service to try and debug the issue. Some companies simply seek to add more tools when current ones prove ineffective. That means SREs are hopping from tool to tool to keep on top of monitoring and troubleshooting across their infrastructure and applications.\"You’re hopping across different tools. You’re relying on a human to interpret these things, visually look at the relationship between systems in a service map, visually look at graphs on a metrics dashboard, to figure out what and where the issue is, \" Exner says. \"But AI automates that workflow away.\" With AI-powered Streams, logs are not just used reactively to resolve issues, but also to proactively process potential issues and create information-rich alerts that help teams jump straight to problem-solving, offering a solution for remediation or even fixing the issue entirely, before automatically notifying the team that it&#x27;s been taken care of.\"I believe that logs, the richest set of information, the original signal type, will start driving a lot of the automation that a service reliability engineer typically does today, and does very manually,\" he adds. \"A human should not be in that process, where they are doing this by digging into themselves, trying to figure out what is going on, where and what the issue is, and then once they find the root cause, they’re trying to figure out how to debug it.\"Observability’s future Large language models (LLMs) could be a key player in the future of observability. LLMs excel at recognizing patterns in vast quantities of repetitive data, which closely resembles log and telemetry data in complex, dynamic systems. And today’s LLMs can be trained for specific IT processes. With automation tooling, the LLM has the information and tools it needs to resolve database errors or Java heap issues, and more. Incorporating those into platforms that bring context and relevance will be essential. Automated remediation will still take some time, Exner says, but automated runbooks and playbooks generated by LLMs will become standard practice within the next couple of years. In other words, remediation steps will be driven by LLMs. The LLM will offer up fixes, and the human will verify and implement them, rather than calling in an expert.Addressing skill shortagesGoing all in on AI for observability would help address a major shortage in the talent needed to manage IT infrastructure. Hiring is slow because organizations need teams with a great deal of experience and understanding of potential issues, and how to resolve them fast. That experience can come from an LLM that is contextually grounded, Exner says.\"We can help deal with the skill shortage by augmenting people with LLMs that make them all instantly experts,\" he explains. \"I think this is going to make it much easier for us to take novice practitioners and make them expert practitioners in both security and observability, and it’s going to make it possible for a more novice practitioner to act like an expert.” Streams in Elastic Observability is available now. Get started by reading more on the Streams. Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.",
          "content": "Presented by Elastic Logs set to become the primary tool for finding the “why” in diagnosing network incidents Modern IT environments have a data problem: there’s too much of it. Organizations that need to manage a company’s environment are increasingly challenged to detect and diagnose issues in real-time, optimize performance, improve reliability, and ensure security and compliance — all within constrained budgets. The modern observability landscape has many tools that offer a solution. Most revolve around DevOps teams or Site Reliability Engineers (SREs) analyzing logs, metrics, and traces to uncover patterns and figure out what’s happening across the network, and diagnose why an issue or incident occurred. The problem is that the process creates information overload: A Kubernetes cluster alone can emit 30 to 50 gigabytes of logs a day, and suspicious behavior patterns can sneak past human eyes. \"It’s so anachronistic now, in the world of AI, to think about humans alone observing infrastructure,\" says Ken Exner, chief product officer at Elastic. \"I hate to break it to you, but machines are better than human beings at pattern matching.“An industry-wide focus on visualizing symptoms forces engineers to manually hunt for answers. The crucial \"why\" is buried in logs, but because they contain massive volumes of unstructured data, the industry tends to use them as a tool of last resort. This has forced teams into costly tradeoffs: either spend countless hours building complex data pipelines, drop valuable log data and risk critical visibility gaps, or log and forget.Elastic, the Search AI Company, recently released a new feature for observability called Streams, which aims to become the primary signal for investigations by taking noisy logs and turning them into patterns, context and meaning. Streams uses AI to automatically partition and parse raw logs to extract relevant fields, and greatly reduce the effort required of SREs to make logs usable. Streams also automatically surfaces significant events such as critical errors and anomalies from context-rich logs, giving SREs early warnings and a clear understanding of their workloads, enabling them to investigate and resolve issues faster. The ultimate goal is to show remediation steps.\"From raw, voluminous, messy data, Streams automatically creates structure, putting it into a form that is usable, automatically alerts you to issues and helps you remediate them,\" Exner says. \"That is the magic of Streams.\"A broken workflowStreams upends an observability process that some say is broken. Typically, SREs set up metrics, logs and traces. Then they set up alerts, and service level objectives (SLOs) — often hard-coded rules to show where a service or process has gone beyond a threshold, or a specific pattern has been detected. When an alert is triggered, it points to the metric that&#x27;s showing an anomaly. From there, SREs look at a metrics dashboard, where they can visualize the issue and compare the alert to other metrics, or CPU to memory to I/O, and start looking for patterns. They may then need to look at a trace, and examine upstream and downstream dependencies across the application to dig into the root cause of the issue. Once they figure out what&#x27;s causing the trouble, they jump into the logs for that database or service to try and debug the issue. Some companies simply seek to add more tools when current ones prove ineffective. That means SREs are hopping from tool to tool to keep on top of monitoring and troubleshooting across their infrastructure and applications.\"You’re hopping across different tools. You’re relying on a human to interpret these things, visually look at the relationship between systems in a service map, visually look at graphs on a metrics dashboard, to figure out what and where the issue is, \" Exner says. \"But AI automates that workflow away.\" With AI-powered Streams, logs are not just used reactively to resolve issues, but also to proactively process potential issues and create information-rich alerts that help teams jump straight to problem-solving, offering a solution for remediation or even fixing the issue entirely, before automatically notifying the team that it&#x27;s been taken care of.\"I believe that logs, the richest set of information, the original signal type, will start driving a lot of the automation that a service reliability engineer typically does today, and does very manually,\" he adds. \"A human should not be in that process, where they are doing this by digging into themselves, trying to figure out what is going on, where and what the issue is, and then once they find the root cause, they’re trying to figure out how to debug it.\"Observability’s future Large language models (LLMs) could be a key player in the future of observability. LLMs excel at recognizing patterns in vast quantities of repetitive data, which closely resembles log and telemetry data in complex, dynamic systems. And today’s LLMs can be trained for specific IT processes. With automation tooling, the LLM has the information and tools it needs to resolve database errors or Java heap issues, and more. Incorporating those into platforms that bring context and relevance will be essential. Automated remediation will still take some time, Exner says, but automated runbooks and playbooks generated by LLMs will become standard practice within the next couple of years. In other words, remediation steps will be driven by LLMs. The LLM will offer up fixes, and the human will verify and implement them, rather than calling in an expert.Addressing skill shortagesGoing all in on AI for observability would help address a major shortage in the talent needed to manage IT infrastructure. Hiring is slow because organizations need teams with a great deal of experience and understanding of potential issues, and how to resolve them fast. That experience can come from an LLM that is contextually grounded, Exner says.\"We can help deal with the skill shortage by augmenting people with LLMs that make them all instantly experts,\" he explains. \"I think this is going to make it much easier for us to take novice practitioners and make them expert practitioners in both security and observability, and it’s going to make it possible for a more novice practitioner to act like an expert.” Streams in Elastic Observability is available now. Get started by reading more on the Streams. Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.",
          "feed_position": 7,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/27gJHUFa8zIMnql7WcOsgp/ac5e9d96209e2fea9f36db14be5dd724/AdobeStock_1211315173.jpeg?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/databricks-research-reveals-that-building-better-ai-judges-isnt-just-a",
          "published_at": "Tue, 04 Nov 2025 20:00:00 GMT",
          "title": "Databricks research reveals that building better AI judges isn't just a technical concern, it's a people problem",
          "standfirst": "The intelligence of AI models isn&#x27;t what&#x27;s blocking enterprise deployments. It&#x27;s the inability to define and measure quality in the first place.That&#x27;s where AI judges are now playing an increasingly important role. In AI evaluation, a \"judge\" is an AI system that scores outputs from another AI system. Judge Builder is Databricks&#x27; framework for creating judges and was first deployed as part of the company&#x27;s Agent Bricks technology earlier this year. The framework has evolved significantly since its initial launch in response to direct user feedback and deployments.Early versions focused on technical implementation but customer feedback revealed the real bottleneck was organizational alignment. Databricks now offers a structured workshop process that guides teams through three core challenges: getting stakeholders to agree on quality criteria, capturing domain expertise from limited subject matter experts and deploying evaluation systems at scale.\"The intelligence of the model is typically not the bottleneck, the models are really smart,\" Jonathan Frankle, Databricks&#x27; chief AI scientist, told VentureBeat in an exclusive briefing. \"Instead, it&#x27;s really about asking, how do we get the models to do what we want, and how do we know if they did what we wanted?\"The &#x27;Ouroboros problem&#x27; of AI evaluationJudge Builder addresses what Pallavi Koppol, a Databricks research scientist who led the development, calls the \"Ouroboros problem.\" An Ouroboros is an ancient symbol that depicts a snake eating its own tail. Using AI systems to evaluate AI systems creates a circular validation challenge.\"You want a judge to see if your system is good, if your AI system is good, but then your judge is also an AI system,\" Koppol explained. \"And now you&#x27;re saying like, well, how do I know this judge is good?\"The solution is measuring \"distance to human expert ground truth\" as the primary scoring function. By minimizing the gap between how an AI judge scores outputs versus how domain experts would score them, organizations can trust these judges as scalable proxies for human evaluation.This approach differs fundamentally from traditional guardrail systems or single-metric evaluations. Rather than asking whether an AI output passed or failed on a generic quality check, Judge Builder creates highly specific evaluation criteria tailored to each organization&#x27;s domain expertise and business requirements.The technical implementation also sets it apart. Judge Builder integrates with Databricks&#x27; MLflow and prompt optimization tools and can work with any underlying model. Teams can version control their judges, track performance over time and deploy multiple judges simultaneously across different quality dimensions.Lessons learned: Building judges that actually workDatabricks&#x27; work with enterprise customers revealed three critical lessons that apply to anyone building AI judges.Lesson one: Your experts don&#x27;t agree as much as you think. When quality is subjective, organizations discover that even their own subject matter experts disagree on what constitutes acceptable output. A customer service response might be factually correct but use an inappropriate tone. A financial summary might be comprehensive but too technical for the intended audience.\"One of the biggest lessons of this whole process is that all problems become people problems,\" Frankle said. \"The hardest part is getting an idea out of a person&#x27;s brain and into something explicit. And the harder part is that companies are not one brain, but many brains.\"The fix is batched annotation with inter-rater reliability checks. Teams annotate examples in small groups, then measure agreement scores before proceeding. This catches misalignment early. In one case, three experts gave ratings of 1, 5 and neutral for the same output before discussion revealed they were interpreting the evaluation criteria differently.Companies using this approach achieve inter-rater reliability scores as high as 0.6 compared to typical scores of 0.3 from external annotation services. Higher agreement translates directly to better judge performance because the training data contains less noise.Lesson two: Break down vague criteria into specific judges. Instead of one judge evaluating whether a response is \"relevant, factual and concise,\" create three separate judges. Each targets a specific quality aspect. This granularity matters because a failing \"overall quality\" score reveals something is wrong but not what to fix.The best results come from combining top-down requirements such as regulatory constraints, stakeholder priorities, with bottom-up discovery of observed failure patterns. One customer built a top-down judge for correctness but discovered through data analysis that correct responses almost always cited the top two retrieval results. This insight became a new production-friendly judge that could proxy for correctness without requiring ground-truth labels.Lesson three: You need fewer examples than you think. Teams can create robust judges from just 20-30 well-chosen examples. The key is selecting edge cases that expose disagreement rather than obvious examples where everyone agrees.\"We&#x27;re able to run this process with some teams in as little as three hours, so it doesn&#x27;t really take that long to start getting a good judge,\" Koppol said.Production results: From pilots to seven-figure deploymentsFrankle shared three metrics Databricks uses to measure Judge Builder&#x27;s success: whether customers want to use it again, whether they increase AI spending and whether they progress further in their AI journey.On the first metric, one customer created more than a dozen judges after their initial workshop. \"This customer made more than a dozen judges after we walked them through doing this in a rigorous way for the first time with this framework,\" Frankle said. \"They really went to town on judges and are now measuring everything.\"For the second metric, the business impact is clear. \"There are multiple customers who have gone through this workshop and have become seven-figure spenders on GenAI at Databricks in a way that they weren&#x27;t before,\" Frankle said.The third metric reveals Judge Builder&#x27;s strategic value. Customers who previously hesitated to use advanced techniques like reinforcement learning now feel confident deploying them because they can measure whether improvements actually occurred.\"There are customers who have gone and done very advanced things after having had these judges where they were reluctant to do so before,\" Frankle said. \"They&#x27;ve moved from doing a little bit of prompt engineering to doing reinforcement learning with us. Why spend the money on reinforcement learning, and why spend the energy on reinforcement learning if you don&#x27;t know whether it actually made a difference?\"What enterprises should do nowThe teams successfully moving AI from pilot to production treat judges not as one-time artifacts but as evolving assets that grow with their systems.Databricks recommends three practical steps. First, focus on high-impact judges by identifying one critical regulatory requirement plus one observed failure mode. These become your initial judge portfolio.Second, create lightweight workflows with subject matter experts. A few hours reviewing 20-30 edge cases provides sufficient calibration for most judges. Use batched annotation and inter-rater reliability checks to denoise your data.Third, schedule regular judge reviews using production data. New failure modes will emerge as your system evolves. Your judge portfolio should evolve with them.\"A judge is a way to evaluate a model, it&#x27;s also a way to create guardrails, it&#x27;s also a way to have a metric against which you can do prompt optimization and it&#x27;s also a way to have a metric against which you can do reinforcement learning,\" Frankle said. \"Once you have a judge that you know represents your human taste in an empirical form that you can query as much as you want, you can use it in 10,000 different ways to measure or improve your agents.\"",
          "content": "The intelligence of AI models isn&#x27;t what&#x27;s blocking enterprise deployments. It&#x27;s the inability to define and measure quality in the first place.That&#x27;s where AI judges are now playing an increasingly important role. In AI evaluation, a \"judge\" is an AI system that scores outputs from another AI system. Judge Builder is Databricks&#x27; framework for creating judges and was first deployed as part of the company&#x27;s Agent Bricks technology earlier this year. The framework has evolved significantly since its initial launch in response to direct user feedback and deployments.Early versions focused on technical implementation but customer feedback revealed the real bottleneck was organizational alignment. Databricks now offers a structured workshop process that guides teams through three core challenges: getting stakeholders to agree on quality criteria, capturing domain expertise from limited subject matter experts and deploying evaluation systems at scale.\"The intelligence of the model is typically not the bottleneck, the models are really smart,\" Jonathan Frankle, Databricks&#x27; chief AI scientist, told VentureBeat in an exclusive briefing. \"Instead, it&#x27;s really about asking, how do we get the models to do what we want, and how do we know if they did what we wanted?\"The &#x27;Ouroboros problem&#x27; of AI evaluationJudge Builder addresses what Pallavi Koppol, a Databricks research scientist who led the development, calls the \"Ouroboros problem.\" An Ouroboros is an ancient symbol that depicts a snake eating its own tail. Using AI systems to evaluate AI systems creates a circular validation challenge.\"You want a judge to see if your system is good, if your AI system is good, but then your judge is also an AI system,\" Koppol explained. \"And now you&#x27;re saying like, well, how do I know this judge is good?\"The solution is measuring \"distance to human expert ground truth\" as the primary scoring function. By minimizing the gap between how an AI judge scores outputs versus how domain experts would score them, organizations can trust these judges as scalable proxies for human evaluation.This approach differs fundamentally from traditional guardrail systems or single-metric evaluations. Rather than asking whether an AI output passed or failed on a generic quality check, Judge Builder creates highly specific evaluation criteria tailored to each organization&#x27;s domain expertise and business requirements.The technical implementation also sets it apart. Judge Builder integrates with Databricks&#x27; MLflow and prompt optimization tools and can work with any underlying model. Teams can version control their judges, track performance over time and deploy multiple judges simultaneously across different quality dimensions.Lessons learned: Building judges that actually workDatabricks&#x27; work with enterprise customers revealed three critical lessons that apply to anyone building AI judges.Lesson one: Your experts don&#x27;t agree as much as you think. When quality is subjective, organizations discover that even their own subject matter experts disagree on what constitutes acceptable output. A customer service response might be factually correct but use an inappropriate tone. A financial summary might be comprehensive but too technical for the intended audience.\"One of the biggest lessons of this whole process is that all problems become people problems,\" Frankle said. \"The hardest part is getting an idea out of a person&#x27;s brain and into something explicit. And the harder part is that companies are not one brain, but many brains.\"The fix is batched annotation with inter-rater reliability checks. Teams annotate examples in small groups, then measure agreement scores before proceeding. This catches misalignment early. In one case, three experts gave ratings of 1, 5 and neutral for the same output before discussion revealed they were interpreting the evaluation criteria differently.Companies using this approach achieve inter-rater reliability scores as high as 0.6 compared to typical scores of 0.3 from external annotation services. Higher agreement translates directly to better judge performance because the training data contains less noise.Lesson two: Break down vague criteria into specific judges. Instead of one judge evaluating whether a response is \"relevant, factual and concise,\" create three separate judges. Each targets a specific quality aspect. This granularity matters because a failing \"overall quality\" score reveals something is wrong but not what to fix.The best results come from combining top-down requirements such as regulatory constraints, stakeholder priorities, with bottom-up discovery of observed failure patterns. One customer built a top-down judge for correctness but discovered through data analysis that correct responses almost always cited the top two retrieval results. This insight became a new production-friendly judge that could proxy for correctness without requiring ground-truth labels.Lesson three: You need fewer examples than you think. Teams can create robust judges from just 20-30 well-chosen examples. The key is selecting edge cases that expose disagreement rather than obvious examples where everyone agrees.\"We&#x27;re able to run this process with some teams in as little as three hours, so it doesn&#x27;t really take that long to start getting a good judge,\" Koppol said.Production results: From pilots to seven-figure deploymentsFrankle shared three metrics Databricks uses to measure Judge Builder&#x27;s success: whether customers want to use it again, whether they increase AI spending and whether they progress further in their AI journey.On the first metric, one customer created more than a dozen judges after their initial workshop. \"This customer made more than a dozen judges after we walked them through doing this in a rigorous way for the first time with this framework,\" Frankle said. \"They really went to town on judges and are now measuring everything.\"For the second metric, the business impact is clear. \"There are multiple customers who have gone through this workshop and have become seven-figure spenders on GenAI at Databricks in a way that they weren&#x27;t before,\" Frankle said.The third metric reveals Judge Builder&#x27;s strategic value. Customers who previously hesitated to use advanced techniques like reinforcement learning now feel confident deploying them because they can measure whether improvements actually occurred.\"There are customers who have gone and done very advanced things after having had these judges where they were reluctant to do so before,\" Frankle said. \"They&#x27;ve moved from doing a little bit of prompt engineering to doing reinforcement learning with us. Why spend the money on reinforcement learning, and why spend the energy on reinforcement learning if you don&#x27;t know whether it actually made a difference?\"What enterprises should do nowThe teams successfully moving AI from pilot to production treat judges not as one-time artifacts but as evolving assets that grow with their systems.Databricks recommends three practical steps. First, focus on high-impact judges by identifying one critical regulatory requirement plus one observed failure mode. These become your initial judge portfolio.Second, create lightweight workflows with subject matter experts. A few hours reviewing 20-30 edge cases provides sufficient calibration for most judges. Use batched annotation and inter-rater reliability checks to denoise your data.Third, schedule regular judge reviews using production data. New failure modes will emerge as your system evolves. Your judge portfolio should evolve with them.\"A judge is a way to evaluate a model, it&#x27;s also a way to create guardrails, it&#x27;s also a way to have a metric against which you can do prompt optimization and it&#x27;s also a way to have a metric against which you can do reinforcement learning,\" Frankle said. \"Once you have a judge that you know represents your human taste in an empirical form that you can query as much as you want, you can use it in 10,000 different ways to measure or improve your agents.\"",
          "feed_position": 8,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/75UWyAdt4L6TmmJjkbVDJu/96a54479b06fb94b7d13366ad4f046af/ouroboros-ai-smk.jpg?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/attention-isnt-all-you-need-new-qwen3-variant-brumby-14b-base-leverages",
          "published_at": "Tue, 04 Nov 2025 19:37:00 GMT",
          "title": "Attention ISN'T all you need?! New Qwen3 variant Brumby-14B-Base leverages Power Retention technique",
          "standfirst": "When the transformer architecture was introduced in 2017 in the now seminal Google paper \"Attention Is All You Need,\" it became an instant cornerstone of modern artificial intelligence. Every major large language model (LLM) — from OpenAI&#x27;s GPT series to Anthropic&#x27;s Claude, Google&#x27;s Gemini, and Meta&#x27;s Llama — has been built on some variation of its central mechanism: attention, the mathematical operation that allows a model to look back across its entire input and decide what information matters most.Eight years later, the same mechanism that defined AI’s golden age is now showing its limits. Attention is powerful, but it is also expensive — its computational and memory costs scale quadratically with context length, creating an increasingly unsustainable bottleneck for both research and industry. As models aim to reason across documents, codebases, or video streams lasting hours or days, attention becomes the architecture’s Achilles’ heel.On October 28, 2025, the little-known AI startup Manifest AI introduced a radical alternative. Their new model, Brumby-14B-Base, is a retrained variant of Qwen3-14B-Base, one of the leading open-source transformer models.But while many variants of Qwen have been trained already, Brumby-14B-Base is novel in that it abandons attention altogether. Instead, Brumby replaces those layers with a novel mechanism called Power Retention—a recurrent, hardware-efficient architecture that stores and updates information over arbitrarily long contexts without the exponential memory growth of attention.Trained at a stated cost of just $4,000, the 14-billion-parameter Brumby model performs on par with established transformer models like Qwen3-14B and GLM-4.5-Air, achieving near-state-of-the-art accuracy on a range of reasoning and comprehension benchmarks.From Attention to Retention: The Architectural ShiftThe core of Manifest AI’s innovation lies in what they call the Power Retention layer. In a traditional transformer, every token computes a set of queries (Q), keys (K), and values (V), then performs a matrix operation that measures the similarity between every token and every other token—essentially a full pairwise comparison across the sequence. This is what gives attention its flexibility, but also what makes it so costly: processing a sequence twice as long takes roughly four times the compute and memory.Power Retention keeps the same inputs (Q, K, V), but replaces the global similarity operation with a recurrent state update. Each layer maintains a memory matrix S, which is updated at each time step according to the incoming key, value, and a learned gating signal. The process looks more like an RNN (Recurrent Neural Network) than a transformer: instead of recomputing attention over the entire context, the model continuously compresses past information into a fixed-size latent state.This means the computational cost of Power Retention does not grow with context length. Whether the model is processing 1,000 or 1,000,000 tokens, the per-token cost remains constant. That property alone—constant-time per-token computation—marks a profound departure from transformer behavior.At the same time, Power Retention preserves the expressive power that made attention successful. Because the recurrence involves tensor powers of the input (hence the name “power retention”), it can represent higher-order dependencies between past and present tokens. The result is an architecture that can theoretically retain long-term dependencies indefinitely, while remaining as efficient as an RNN and as expressive as a transformer.Retraining, Not RebuildingPerhaps the most striking aspect of Brumby-14B’s training process is its efficiency. Manifest AI trained the model for only 60 hours on 32 Nvidia H100 GPUs, at a cost of roughly $4,000 — less than 2% of what a conventional model of this scale would cost to train from scratch.However, since it relied on a transformer-based model, it&#x27;s safe to say that this advance alone will not end the transformer AI-era.As Jacob Buckman, founder of Manifest AI, clarified in an email to VentureBeat: “The ability to train for $4,000 is indeed only possible when leveraging an existing transformer model,” he said. “Brumby could not be trained from scratch for that price.”Still, Buckman emphasized the significance of that result: “The reason this is important is that the ability to build on the weights of the previous generation of model architectures is a critical accelerant for the adoption of a new modeling paradigm.” He argues this demonstrates how attention-free systems can catch up to transformer performance “for orders-of-magnitude less” investment.In the loss curves released by Manifest AI, Brumby’s training loss quickly converges to that of the Qwen3 baseline within 3,000 training steps, even as the architecture diverges significantly from its transformer origins. Although Brumby-14B-Base began life as Qwen3-14B-Base, it did not remain identical for long. Manifest AI fundamentally altered Qwen3’s architecture by removing its attention layers—the mathematical engine that defines how a transformer model processes information—and replacing them with their new “power retention” mechanism. This change restructured the model’s internal wiring, effectively giving it a new brain while preserving much of its prior knowledge.Because of that architectural swap, the existing Qwen3 weights no longer fit perfectly. They were trained to operate within a transformer’s attention dynamics, not the new retention-based system. As a result, the Brumby model initially “forgot” how to apply some of its learned knowledge effectively. The retraining process—about 3,000 steps of additional learning—served to recalibrate those weights, aligning them with the power retention framework without having to start from zero.A helpful way to think about this is to imagine taking a world-class pianist and handing them a guitar. They already understand rhythm, harmony, and melody, but their hands must learn entirely new patterns to produce the same music. Similarly, Brumby had to relearn how to use its existing knowledge through a new computational instrument. Those 3,000 training steps were, in effect, its crash course in guitar lessons.By the end of this short retraining phase, Brumby had regained its full performance, reaching the same accuracy as the original Qwen3 model. That quick recovery is what makes the result so significant: it shows that an attention-free system can inherit and adapt the capabilities of a transformer model with only a fraction of the training time and cost.The benchmark progression plots show a similar trend: the model rapidly approaches its target accuracy on core evaluations like GSM8K, HellaSwag, and MMLU after only a few thousand steps, matching or even slightly surpassing Qwen3 on several tasks.Benchmarking the BrumbyAcross standard evaluation tasks, Brumby-14B-Base consistently performs at or near parity with transformer baselines of comparable scale.TaskBrumby-14BQwen3-14BGLM-4.5-AirNemotron Nano (12B)ARC0.890.940.920.93GSM8K0.880.840.830.84GSM8K (Platinum)0.870.880.850.87HellaSwag0.770.810.850.82MATH0.620.540.470.26MBPP0.570.750.730.71MMLU0.710.780.770.78MMLU (Pro)0.360.550.510.53While it lags slightly behind transformers on knowledge-heavy evaluations like MMLU-Pro, it matches or outperforms them on mathematical reasoning and long-context reasoning tasks—precisely where attention architectures tend to falter. This pattern reinforces the idea that recurrent or retention-based systems may hold a structural advantage for reasoning over extended temporal or logical dependencies.Hardware Efficiency and Inference PerformanceBrumby’s power retention design offers another major advantage: hardware efficiency.Because the state update involves only local matrix operations, inference can be implemented with linear complexity in sequence length. Manifest AI reports that their fastest kernels, developed through their in-house CUDA framework Vidrial, can deliver hundreds-fold speedups over attention on very long contexts.Buckman said the alpha-stage Power Retention kernels “achieve typical hardware utilization of 80–85%, which is higher than FlashAttention2’s 70–75% or Mamba’s 50–60%.” (Mamba is another emerging “post-transformer” architecture developed by Carnegie Mellon scientists back in 2023 that, like Power Retention, seeks to eliminate the computational bottleneck of attention. It replaces attention with a state-space mechanism that processes sequences linearly — updating an internal state over time rather than comparing every token to every other one. This makes it far more efficient for long inputs, though it typically achieves lower hardware utilization than Power Retention in early tests.)Both Power Retention and Mamba, he added, “expend meaningfully fewer total FLOPs than FlashAttention2 on long contexts, as well as far less memory.” According to Buckman, the reported 100× speedup comes from this combined improvement in utilization and computational efficiency, though he noted that “we have not yet stress-tested it on production-scale workloads.”Training and Scaling EconomicsPerhaps no statistic in the Brumby release generated more attention than the training cost.A 14-billion-parameter model, trained for $4,000, represents a two-order-of-magnitude reduction in the cost of foundation model development.Buckman confirmed that the low cost reflects a broader scaling pattern. “Far from diminishing returns, we have found that ease of retraining improves with scale,” he said. “The number of steps required to successfully retrain a model decreases with its parameter count.” Manifest has not yet validated the cost of retraining models at 700B parameters, but Buckman projected a range of $10,000–$20,000 for models of that magnitude—still far below transformer training budgets.He also reiterated that this approach could democratize large-scale experimentation by allowing smaller research groups or companies to retrain or repurpose existing transformer checkpoints without prohibitive compute costs.Integration and DeploymentAccording to Buckman, converting an existing transformer into a Power Retention model is designed to be simple. “It is straightforward for any company that is already retraining, post-training, or fine-tuning open-source models,” he said. “Simply pip install retention, change one line of your architecture code, and resume training where you left off.”He added that after only a small number of GPU-hours, the model typically recovers its original performance—at which point it gains the efficiency benefits of the attention-free design. “The resulting architecture will permit far faster long-context training and inference than previously,” Buckman noted.On infrastructure, Buckman said the main Brumby kernels are written in Triton, compatible with both NVIDIA and AMD accelerators. Specialized CUDA kernels are also available through the team’s in-house Vidrial framework. Integration with vLLM and other inference engines remains a work in progress: “We have not yet integrated Power Retention into inference engines, but doing so is a major ongoing initiative at Manifest.”As for distributed inference, Buckman dismissed concerns about instability: “We have not found this difficulty to be exacerbated in any way by our recurrent-state architecture. In fact, context-parallel training and GPU partitioning for multi-user inference both become significantly cleaner technically when using our approach.”Mission and Long-Term VisionBeyond the engineering details, Buckman also described Manifest’s broader mission. “Our mission is to train a neural network to model all human output,” he said. The team’s goal, he explained, is to move beyond modeling “artifacts of intelligence” toward modeling “the intelligent processes that generated them.” This shift, he argued, requires “fundamentally rethinking” how models are designed and trained—work that Power Retention represents only the beginning of.The Brumby-14B release, he said, is “one step forward in a long march” toward architectures that can model thought processes continuously and efficiently.Public Debate and Industry ReceptionThe launch of Brumby-14B sparked immediate discussion on X (formerly Twitter), where researchers debated the framing of Manifest AI’s announcement. Some, including Meta researcher Ariel (@redtachyon), argued that the “$4,000 foundation model” tagline was misleading, since the training involved reusing pretrained transformer weights rather than training from scratch.“They shuffled around the weights of Qwen, fine-tuned it a bit, and called it ‘training a foundation model for $4k,’” Ariel wrote.Buckman responded publicly, clarifying that the initial tweet had been part of a longer thread explaining the retraining approach. “It’s not like I was being deceptive about it,” he wrote. “I broke it up into separate tweets, and now everyone is mad about the first one.”In a follow-up email, Buckman took a measured view of the controversy. “The end of the transformer era is not yet here,” he reiterated, “but the march has begun.” He also acknowledged that the $4,000 claim, though technically accurate in context, had drawn attention precisely because it challenged expectations about what it costs to experiment at frontier scale.Conclusion: A Crack in the Transformer’s Wall?The release of Brumby-14B-Base is more than an engineering milestone; it is a proof of concept that the transformer’s dominance may finally face credible competition. By replacing attention with power retention, Manifest AI has demonstrated that performance parity with state-of-the-art transformers is possible at a fraction of the computational cost—and that the long-context bottleneck can be broken without exotic hardware.The broader implications are twofold. First, the economics of training and serving large models could shift dramatically, lowering the barrier to entry for open research and smaller organizations. Second, the architectural diversity of AI models may expand again, reigniting theoretical and empirical exploration after half a decade of transformer monoculture.As Buckman put it: “The end of the transformer era is not yet here. Our release is just one step forward in a long march toward the future.”",
          "content": "When the transformer architecture was introduced in 2017 in the now seminal Google paper \"Attention Is All You Need,\" it became an instant cornerstone of modern artificial intelligence. Every major large language model (LLM) — from OpenAI&#x27;s GPT series to Anthropic&#x27;s Claude, Google&#x27;s Gemini, and Meta&#x27;s Llama — has been built on some variation of its central mechanism: attention, the mathematical operation that allows a model to look back across its entire input and decide what information matters most.Eight years later, the same mechanism that defined AI’s golden age is now showing its limits. Attention is powerful, but it is also expensive — its computational and memory costs scale quadratically with context length, creating an increasingly unsustainable bottleneck for both research and industry. As models aim to reason across documents, codebases, or video streams lasting hours or days, attention becomes the architecture’s Achilles’ heel.On October 28, 2025, the little-known AI startup Manifest AI introduced a radical alternative. Their new model, Brumby-14B-Base, is a retrained variant of Qwen3-14B-Base, one of the leading open-source transformer models.But while many variants of Qwen have been trained already, Brumby-14B-Base is novel in that it abandons attention altogether. Instead, Brumby replaces those layers with a novel mechanism called Power Retention—a recurrent, hardware-efficient architecture that stores and updates information over arbitrarily long contexts without the exponential memory growth of attention.Trained at a stated cost of just $4,000, the 14-billion-parameter Brumby model performs on par with established transformer models like Qwen3-14B and GLM-4.5-Air, achieving near-state-of-the-art accuracy on a range of reasoning and comprehension benchmarks.From Attention to Retention: The Architectural ShiftThe core of Manifest AI’s innovation lies in what they call the Power Retention layer. In a traditional transformer, every token computes a set of queries (Q), keys (K), and values (V), then performs a matrix operation that measures the similarity between every token and every other token—essentially a full pairwise comparison across the sequence. This is what gives attention its flexibility, but also what makes it so costly: processing a sequence twice as long takes roughly four times the compute and memory.Power Retention keeps the same inputs (Q, K, V), but replaces the global similarity operation with a recurrent state update. Each layer maintains a memory matrix S, which is updated at each time step according to the incoming key, value, and a learned gating signal. The process looks more like an RNN (Recurrent Neural Network) than a transformer: instead of recomputing attention over the entire context, the model continuously compresses past information into a fixed-size latent state.This means the computational cost of Power Retention does not grow with context length. Whether the model is processing 1,000 or 1,000,000 tokens, the per-token cost remains constant. That property alone—constant-time per-token computation—marks a profound departure from transformer behavior.At the same time, Power Retention preserves the expressive power that made attention successful. Because the recurrence involves tensor powers of the input (hence the name “power retention”), it can represent higher-order dependencies between past and present tokens. The result is an architecture that can theoretically retain long-term dependencies indefinitely, while remaining as efficient as an RNN and as expressive as a transformer.Retraining, Not RebuildingPerhaps the most striking aspect of Brumby-14B’s training process is its efficiency. Manifest AI trained the model for only 60 hours on 32 Nvidia H100 GPUs, at a cost of roughly $4,000 — less than 2% of what a conventional model of this scale would cost to train from scratch.However, since it relied on a transformer-based model, it&#x27;s safe to say that this advance alone will not end the transformer AI-era.As Jacob Buckman, founder of Manifest AI, clarified in an email to VentureBeat: “The ability to train for $4,000 is indeed only possible when leveraging an existing transformer model,” he said. “Brumby could not be trained from scratch for that price.”Still, Buckman emphasized the significance of that result: “The reason this is important is that the ability to build on the weights of the previous generation of model architectures is a critical accelerant for the adoption of a new modeling paradigm.” He argues this demonstrates how attention-free systems can catch up to transformer performance “for orders-of-magnitude less” investment.In the loss curves released by Manifest AI, Brumby’s training loss quickly converges to that of the Qwen3 baseline within 3,000 training steps, even as the architecture diverges significantly from its transformer origins. Although Brumby-14B-Base began life as Qwen3-14B-Base, it did not remain identical for long. Manifest AI fundamentally altered Qwen3’s architecture by removing its attention layers—the mathematical engine that defines how a transformer model processes information—and replacing them with their new “power retention” mechanism. This change restructured the model’s internal wiring, effectively giving it a new brain while preserving much of its prior knowledge.Because of that architectural swap, the existing Qwen3 weights no longer fit perfectly. They were trained to operate within a transformer’s attention dynamics, not the new retention-based system. As a result, the Brumby model initially “forgot” how to apply some of its learned knowledge effectively. The retraining process—about 3,000 steps of additional learning—served to recalibrate those weights, aligning them with the power retention framework without having to start from zero.A helpful way to think about this is to imagine taking a world-class pianist and handing them a guitar. They already understand rhythm, harmony, and melody, but their hands must learn entirely new patterns to produce the same music. Similarly, Brumby had to relearn how to use its existing knowledge through a new computational instrument. Those 3,000 training steps were, in effect, its crash course in guitar lessons.By the end of this short retraining phase, Brumby had regained its full performance, reaching the same accuracy as the original Qwen3 model. That quick recovery is what makes the result so significant: it shows that an attention-free system can inherit and adapt the capabilities of a transformer model with only a fraction of the training time and cost.The benchmark progression plots show a similar trend: the model rapidly approaches its target accuracy on core evaluations like GSM8K, HellaSwag, and MMLU after only a few thousand steps, matching or even slightly surpassing Qwen3 on several tasks.Benchmarking the BrumbyAcross standard evaluation tasks, Brumby-14B-Base consistently performs at or near parity with transformer baselines of comparable scale.TaskBrumby-14BQwen3-14BGLM-4.5-AirNemotron Nano (12B)ARC0.890.940.920.93GSM8K0.880.840.830.84GSM8K (Platinum)0.870.880.850.87HellaSwag0.770.810.850.82MATH0.620.540.470.26MBPP0.570.750.730.71MMLU0.710.780.770.78MMLU (Pro)0.360.550.510.53While it lags slightly behind transformers on knowledge-heavy evaluations like MMLU-Pro, it matches or outperforms them on mathematical reasoning and long-context reasoning tasks—precisely where attention architectures tend to falter. This pattern reinforces the idea that recurrent or retention-based systems may hold a structural advantage for reasoning over extended temporal or logical dependencies.Hardware Efficiency and Inference PerformanceBrumby’s power retention design offers another major advantage: hardware efficiency.Because the state update involves only local matrix operations, inference can be implemented with linear complexity in sequence length. Manifest AI reports that their fastest kernels, developed through their in-house CUDA framework Vidrial, can deliver hundreds-fold speedups over attention on very long contexts.Buckman said the alpha-stage Power Retention kernels “achieve typical hardware utilization of 80–85%, which is higher than FlashAttention2’s 70–75% or Mamba’s 50–60%.” (Mamba is another emerging “post-transformer” architecture developed by Carnegie Mellon scientists back in 2023 that, like Power Retention, seeks to eliminate the computational bottleneck of attention. It replaces attention with a state-space mechanism that processes sequences linearly — updating an internal state over time rather than comparing every token to every other one. This makes it far more efficient for long inputs, though it typically achieves lower hardware utilization than Power Retention in early tests.)Both Power Retention and Mamba, he added, “expend meaningfully fewer total FLOPs than FlashAttention2 on long contexts, as well as far less memory.” According to Buckman, the reported 100× speedup comes from this combined improvement in utilization and computational efficiency, though he noted that “we have not yet stress-tested it on production-scale workloads.”Training and Scaling EconomicsPerhaps no statistic in the Brumby release generated more attention than the training cost.A 14-billion-parameter model, trained for $4,000, represents a two-order-of-magnitude reduction in the cost of foundation model development.Buckman confirmed that the low cost reflects a broader scaling pattern. “Far from diminishing returns, we have found that ease of retraining improves with scale,” he said. “The number of steps required to successfully retrain a model decreases with its parameter count.” Manifest has not yet validated the cost of retraining models at 700B parameters, but Buckman projected a range of $10,000–$20,000 for models of that magnitude—still far below transformer training budgets.He also reiterated that this approach could democratize large-scale experimentation by allowing smaller research groups or companies to retrain or repurpose existing transformer checkpoints without prohibitive compute costs.Integration and DeploymentAccording to Buckman, converting an existing transformer into a Power Retention model is designed to be simple. “It is straightforward for any company that is already retraining, post-training, or fine-tuning open-source models,” he said. “Simply pip install retention, change one line of your architecture code, and resume training where you left off.”He added that after only a small number of GPU-hours, the model typically recovers its original performance—at which point it gains the efficiency benefits of the attention-free design. “The resulting architecture will permit far faster long-context training and inference than previously,” Buckman noted.On infrastructure, Buckman said the main Brumby kernels are written in Triton, compatible with both NVIDIA and AMD accelerators. Specialized CUDA kernels are also available through the team’s in-house Vidrial framework. Integration with vLLM and other inference engines remains a work in progress: “We have not yet integrated Power Retention into inference engines, but doing so is a major ongoing initiative at Manifest.”As for distributed inference, Buckman dismissed concerns about instability: “We have not found this difficulty to be exacerbated in any way by our recurrent-state architecture. In fact, context-parallel training and GPU partitioning for multi-user inference both become significantly cleaner technically when using our approach.”Mission and Long-Term VisionBeyond the engineering details, Buckman also described Manifest’s broader mission. “Our mission is to train a neural network to model all human output,” he said. The team’s goal, he explained, is to move beyond modeling “artifacts of intelligence” toward modeling “the intelligent processes that generated them.” This shift, he argued, requires “fundamentally rethinking” how models are designed and trained—work that Power Retention represents only the beginning of.The Brumby-14B release, he said, is “one step forward in a long march” toward architectures that can model thought processes continuously and efficiently.Public Debate and Industry ReceptionThe launch of Brumby-14B sparked immediate discussion on X (formerly Twitter), where researchers debated the framing of Manifest AI’s announcement. Some, including Meta researcher Ariel (@redtachyon), argued that the “$4,000 foundation model” tagline was misleading, since the training involved reusing pretrained transformer weights rather than training from scratch.“They shuffled around the weights of Qwen, fine-tuned it a bit, and called it ‘training a foundation model for $4k,’” Ariel wrote.Buckman responded publicly, clarifying that the initial tweet had been part of a longer thread explaining the retraining approach. “It’s not like I was being deceptive about it,” he wrote. “I broke it up into separate tweets, and now everyone is mad about the first one.”In a follow-up email, Buckman took a measured view of the controversy. “The end of the transformer era is not yet here,” he reiterated, “but the march has begun.” He also acknowledged that the $4,000 claim, though technically accurate in context, had drawn attention precisely because it challenged expectations about what it costs to experiment at frontier scale.Conclusion: A Crack in the Transformer’s Wall?The release of Brumby-14B-Base is more than an engineering milestone; it is a proof of concept that the transformer’s dominance may finally face credible competition. By replacing attention with power retention, Manifest AI has demonstrated that performance parity with state-of-the-art transformers is possible at a fraction of the computational cost—and that the long-context bottleneck can be broken without exotic hardware.The broader implications are twofold. First, the economics of training and serving large models could shift dramatically, lowering the barrier to entry for open research and smaller organizations. Second, the architectural diversity of AI models may expand again, reigniting theoretical and empirical exploration after half a decade of transformer monoculture.As Buckman put it: “The end of the transformer era is not yet here. Our release is just one step forward in a long march toward the future.”",
          "feed_position": 9,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/4w8pJoJCpKW8g1eJxxgy3f/c8d0f3a8431956228510e551a2b474f1/aRNZNKxpXqqt3S_iXlQfh_71e40940f61b4bf89d6b5f5cbeafd63e.png?w=300&q=30"
        }
      ],
      "featured_image": "https://images.ctfassets.net/jdtwqhzvc2n1/1HGuj3BsedMoBgz8md53tP/b48cd315b64ffaf658892d572c61d188/cfr0z3n_fisheye_view_surreal_bold_deep_color_vibrant_high_contr_5795af91-597f-4686-8f07-ff43e6922e8c.png?w=300&q=30",
      "popularity_score": 2019.1819608333333,
      "ai_summary": [
        "Moonshot AI released Kimi K2 Thinking, an open-source AI model.",
        "The model outperforms GPT-5 and Claude Sonnet 4.5 in benchmarks.",
        "Kimi K2 Thinking is available on platform.moonshot.ai and kimi.com.",
        "The model is released under a Modified MIT License.",
        "The license includes a usage restriction for high-traffic applications."
      ]
    },
    {
      "id": "cluster_72",
      "coverage": 2,
      "updated_at": "Thu, 06 Nov 2025 09:55:05 -0500",
      "title": "Fomo, a consumer crypto trading app, raised a $17M Series A led by Benchmark, taking its total funding to $19M, and reports $20M to $40M in daily trading volume (Julie Bort/TechCrunch)",
      "neutral_headline": "Fomo, a consumer crypto trading app, raised a $17M Series A led by Benchmark, taking its total funding to $19M, and reports $20M to $40M in daily trading volume (Julie Bort/TechCrunch)",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/251106/p28#a251106p28",
          "published_at": "Thu, 06 Nov 2025 09:55:05 -0500",
          "title": "Fomo, a consumer crypto trading app, raised a $17M Series A led by Benchmark, taking its total funding to $19M, and reports $20M to $40M in daily trading volume (Julie Bort/TechCrunch)",
          "standfirst": "Julie Bort / TechCrunch: Fomo, a consumer crypto trading app, raised a $17M Series A led by Benchmark, taking its total funding to $19M, and reports $20M to $40M in daily trading volume &mdash; Paul Erlanger and Se Yong Park, co-founders of consumer crypto trading app Fomo, took an unusual route to raising capital that is paying off for them.",
          "content": "Julie Bort / TechCrunch: Fomo, a consumer crypto trading app, raised a $17M Series A led by Benchmark, taking its total funding to $19M, and reports $20M to $40M in daily trading volume &mdash; Paul Erlanger and Se Yong Park, co-founders of consumer crypto trading app Fomo, took an unusual route to raising capital that is paying off for them.",
          "feed_position": 10,
          "image_url": "http://www.techmeme.com/251106/i28.jpg"
        },
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2025/11/06/why-benchmark-made-a-rare-crypto-bet-on-trading-app-fomo-with-17m-series-a/",
          "published_at": "Thu, 06 Nov 2025 14:00:00 +0000",
          "title": "Why Benchmark made a rare crypto bet on trading app Fomo, with $17M Series A",
          "standfirst": "The founders of Fomo, a consumer crypto trading app that launched in May, took an unusual route to raising capital.",
          "content": "The founders of Fomo, a consumer crypto trading app that launched in May, took an unusual route to raising capital.",
          "feed_position": 6
        }
      ],
      "featured_image": "http://www.techmeme.com/251106/i28.jpg",
      "popularity_score": 2015.6500163888888,
      "ai_summary": [
        "Fomo, a crypto trading app, raised a $17M Series A led by Benchmark.",
        "Total funding for Fomo is now $19 million.",
        "The app reports $20M to $40M in daily trading volume.",
        "The founders took an unusual route to raise capital.",
        "The app launched in May."
      ]
    },
    {
      "id": "cluster_25",
      "coverage": 1,
      "updated_at": "Thu, 06 Nov 2025 17:53:35 +0000",
      "title": "After Russian spaceport firm fails to pay bills, electric company turns the lights off",
      "neutral_headline": "After Russian spaceport firm fails to pay bills, electric company turns the lights off",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2025/11/after-russian-spaceport-firm-fails-to-pay-bills-electric-company-turns-the-lights-off/",
          "published_at": "Thu, 06 Nov 2025 17:53:35 +0000",
          "title": "After Russian spaceport firm fails to pay bills, electric company turns the lights off",
          "standfirst": "\"If you increase the cost, you’ll get everything in two years. If not, I’m sorry.”",
          "content": "One of Russia’s most important projects over the last 15 years has been the construction of the Vostochny spaceport as the country seeks to fly its rockets from native soil and modernize its launch operations. However, the initiative has been a fiasco from the start. After construction began in 2011, the project was beset by hunger strikes, claims of unpaid workers, and the theft of $126 million. Additionally, a man driving a diamond-encrusted Mercedes was arrested after embezzling $75,000. Five years ago, there was another purge of top officials after another round of corruption. Through it all, there has been some progress. In 2016, a Soyuz-2 rocket launched from the first pad, “1S.” And eight years later, a second pad, “1A,” opened with a successful Angara rocket launch. Eventually, the Russian space corporation, Roscosmos, would like to operate seven launch pads at the Vostochny in the far eastern area of Russia, so development work continues.Read full article Comments",
          "feed_position": 1,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-1661794528-1024x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-1661794528-1024x648.jpg",
      "popularity_score": 370.62501638888887,
      "ai_summary": [
        "A Russian spaceport firm failed to pay its bills.",
        "An electric company disconnected the power.",
        "The electric company issued a warning.",
        "The warning stated the consequences of non-payment.",
        "The spaceport's operations are affected."
      ]
    },
    {
      "id": "cluster_20",
      "coverage": 1,
      "updated_at": "Thu, 06 Nov 2025 17:59:07 +0000",
      "title": "“It’s only a matter of time before people die”: Trump cuts hit food inspections",
      "neutral_headline": "“It’s only a matter of time before people die”: Trump cuts hit food inspections",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/health/2025/11/its-only-a-matter-of-time-before-people-die-trump-cuts-hit-food-inspections/",
          "published_at": "Thu, 06 Nov 2025 17:59:07 +0000",
          "title": "“It’s only a matter of time before people die”: Trump cuts hit food inspections",
          "standfirst": "American inspections of foreign food facilities hit historic lows this year.",
          "content": "American inspections of foreign food facilities—which produce everything from crawfish to cookies for the US market—have plummeted to historic lows this year, a ProPublica analysis of federal data shows, even as inspections reveal alarming conditions at some manufacturers. About two dozen current and former Food and Drug Administration officials blame the pullback on deep staffing cuts under the Trump administration. The stark reduction marks a dramatic shift in oversight at a time when the United States has never been more dependent on foreign food, which accounts for the vast majority of the nation’s seafood and more than half its fresh fruit. The stakes are high: Foreign products have been increasingly linked to outbreaks of foodborne illness. In recent years, FDA investigators have uncovered disturbing lapses in facilities producing food bound for American supermarkets. In Indonesia, cookie factory workers hauled dough in soiled buckets. In China, seafood processors slid crawfish along cracked, stained conveyor belts. Investigators have reported crawling insects, dripping pipes, and fake testing data purporting to show food products were pathogen free.Read full article Comments",
          "feed_position": 0,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/shrimp-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/shrimp-1152x648.jpg",
      "popularity_score": 351.7172386111111,
      "ai_summary": [
        "American inspections of foreign food facilities are at historic lows.",
        "The cuts were implemented during the Trump administration.",
        "The situation raises concerns about food safety.",
        "The inspections are crucial for public health.",
        "The cuts may increase the risk of illness."
      ]
    },
    {
      "id": "cluster_31",
      "coverage": 1,
      "updated_at": "Thu, 06 Nov 2025 17:24:51 +0000",
      "title": "Google plans secret AI military outpost on tiny island overrun by crabs",
      "neutral_headline": "Google plans secret AI military outpost on tiny island overrun by crabs",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/ai/2025/11/google-plans-secret-ai-military-outpost-on-tiny-island-overrun-by-crabs/",
          "published_at": "Thu, 06 Nov 2025 17:24:51 +0000",
          "title": "Google plans secret AI military outpost on tiny island overrun by crabs",
          "standfirst": "Christmas Island facility would support naval surveillance in strategic Indo-Pacific waters.",
          "content": "On Wednesday, Reuters reported that Google is planning to build a large AI data center on Christmas Island, a 52-square-mile Australian territory in the Indian Ocean, following a cloud computing deal with Australia’s military. The previously undisclosed project will reportedly position advanced AI infrastructure a mere 220 miles south of Indonesia at a location military strategists consider critical for monitoring Chinese naval activity. Aside from its strategic military position, the island is famous for its massive annual crab migration, where over 100 million of red crabs make their way across the island to spawn in the ocean. That’s notable because the tech giant has applied for environmental approvals to build a subsea cable connecting the 135-square-kilometer island to Darwin, where US Marines are stationed for six months each year. The project follows a three-year cloud agreement Google signed with Australia’s military in July 2025, but many details about the new facility’s size, cost, and specific capabilities remain “secret,” according to Reuters. Both Google and Australia’s Department of Defense declined to comment when contacted by the news agency.Read full article Comments",
          "feed_position": 3,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/crabs_hero-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/crabs_hero-1152x648.jpg",
      "popularity_score": 348.1461275,
      "ai_summary": [
        "Google plans an AI military facility on Christmas Island.",
        "The facility will support naval surveillance.",
        "The location is in strategic Indo-Pacific waters.",
        "The island is known for its crab population.",
        "The project is a secret initiative."
      ]
    },
    {
      "id": "cluster_29",
      "coverage": 1,
      "updated_at": "Thu, 06 Nov 2025 17:33:46 +0000",
      "title": "Lego boldly goes into the Star Trek universe with $400, 3,600-piece Enterprise-D",
      "neutral_headline": "Lego boldly goes into the Star Trek universe with $400, 3,600-piece Enterprise-D",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/culture/2025/11/lego-boldly-goes-into-the-star-trek-universe-with-400-3600-piece-enterprise-d/",
          "published_at": "Thu, 06 Nov 2025 17:33:46 +0000",
          "title": "Lego boldly goes into the Star Trek universe with $400, 3,600-piece Enterprise-D",
          "standfirst": "Kit includes the ship and accessory-toting minifigs of the Enterprise crew.",
          "content": "Star Trek fans who have long envied the Star Wars franchise’s collaboration with Lego are finally getting something to celebrate: Lego is introducing a version of Star Trek’s USS Enterprise, specifically the Enterprise-D from Star Trek: The Next Generation. Because we don’t live in the post-money utopian society of the 24th century, the kit will cost you, and unfortunately, it’s priced well into the for-superfans-only zone. The 3,600-piece starship and collection of minifigs will run you $400 when the set officially leaves spacedock on November 28. Though the Enterprise-D is far from our favorite Enterprise, it does make sense as a starting point for the Lego Group. The Next Generation‘s seven-year run in the late ’80s and early ’90s represents a creative and cultural peak for the franchise, and a 2010s-era remaster that painstakingly re-scanned and upgraded all of the original footage and effects for high-definition TVs has kept the old episodes looking fresher than other ’90s Trek shows like Deep Space Nine and Voyager.Read full article Comments",
          "feed_position": 2,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/10356_WEB_SEC01_NOBG_en-gb-1152x648.jpeg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/10356_WEB_SEC01_NOBG_en-gb-1152x648.jpeg",
      "popularity_score": 331.2947386111111,
      "ai_summary": [
        "Lego is releasing a Star Trek Enterprise-D set.",
        "The set contains 3,600 pieces.",
        "The kit includes minifigs of the Enterprise crew.",
        "The set will cost $400.",
        "The release marks Lego's entry into Star Trek."
      ]
    },
    {
      "id": "cluster_110",
      "coverage": 1,
      "updated_at": "Wed, 05 Nov 2025 23:00:46 +0000",
      "title": "5 AI-developed malware families analyzed by Google fail to work and are easily detected",
      "neutral_headline": "AI Malware Fails, Easily Detected, According to Google Analysis",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/security/2025/11/ai-generated-malware-poses-little-real-world-threat-contrary-to-hype/",
          "published_at": "Wed, 05 Nov 2025 23:00:46 +0000",
          "title": "5 AI-developed malware families analyzed by Google fail to work and are easily detected",
          "standfirst": "You wouldn't know it from the hype, but the results fail to impress.",
          "content": "Google on Wednesday revealed five recent malware samples that were built using generative AI. The end results of each one were far below par with professional malware development, a finding that shows that vibe coding of malicious wares lags behind more traditional forms of development and, thus, still has a long way to go before it poses a real-world threat. One of the samples, for instance, tracked under the name PromptLock, was part of an academic study analyzing how effective the use of large language models can be “to autonomously plan, adapt, and execute the ransomware attack lifecycle.” The researchers, however, reported the malware had “clear limitations: it omits persistence, lateral movement, and advanced evasion tactics” and served as little more than a demonstration of the feasibility of AI for such purposes. Prior to the paper’s release, security firm ESET said it had discovered the sample and hailed it as “the first AI-powered ransomware.” Don’t believe the hype Like the other four samples Google analyzed—FruitShell, PromptFlux, PromptSteal, and QuietVault—PromptLock was easy to detect, even by less-sophisticated endpoint protections that rely on static signatures. All samples also employed previously seen methods in malware samples, making them easy to counteract. They also had no operational impact, meaning they didn’t require defenders to adopt new defenses.Read full article Comments",
          "feed_position": 6,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/05/malware-threat-1000x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/05/malware-threat-1000x648.jpg",
      "popularity_score": 306,
      "ai_summary": [
        "Google analyzed five AI-developed malware families, finding they were ineffective.",
        "The malware was easily detected by existing security measures.",
        "The results of the analysis did not align with the hype surrounding AI malware.",
        "The study suggests current AI malware is not a significant threat.",
        "The findings indicate a gap between expectations and actual performance."
      ]
    },
    {
      "id": "cluster_107",
      "coverage": 1,
      "updated_at": "Wed, 05 Nov 2025 23:50:26 +0000",
      "title": "Musk and Trump both went to Penn—now hacked by someone sympathetic to their cause",
      "neutral_headline": "Penn University Hacked After Musk, Trump Visits",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/security/2025/11/musk-and-trump-both-went-to-penn-now-hacked-by-someone-sympathetic-to-their-cause/",
          "published_at": "Wed, 05 Nov 2025 23:50:26 +0000",
          "title": "Musk and Trump both went to Penn—now hacked by someone sympathetic to their cause",
          "standfirst": "Social engineering strikes again.",
          "content": "The University of Pennsylvania has a somewhat unusual distinction: It is the alma mater of two of the planet’s most polarizing figures, Elon Musk and Donald Trump. As the political power of both men rose over the last year, the US government began to pressure Penn, first by pulling its research funding and then by targeting the school for past actions related to a transgender swimmer. After the “sticks” were deployed, a “carrot” was offered. Penn became one of just nine schools nationally to be offered a special “compact” with the federal government, which would give the feds broad control over the school and its speech in return for preferential access to federal funds. Penn declined to sign the deal. (Making the whole surreal situation stranger was the fact that one of Penn’s own wealthy boosters apparently helped the Trump administration write the compact.) In other words, Penn has become an obvious target of the Trump administration; now it has been targeted by a hacker claiming to share Trump and Musk’s grievances over affirmative action and “wokeness.”Read full article Comments",
          "feed_position": 4,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-1405238501-1152x648-1762385719.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-1405238501-1152x648-1762385719.jpg",
      "popularity_score": 303.5725163888889,
      "ai_summary": [
        "The University of Pennsylvania was recently targeted by a social engineering attack.",
        "The attack occurred after visits from Elon Musk and Donald Trump.",
        "The hackers reportedly sympathized with the causes of Musk and Trump.",
        "Details of the attack and its impact are still emerging.",
        "The incident highlights the vulnerability of institutions to social engineering."
      ]
    },
    {
      "id": "cluster_109",
      "coverage": 1,
      "updated_at": "Wed, 05 Nov 2025 23:29:42 +0000",
      "title": "83-year-old man married 50 years nearly stumps doctors with surprise STI",
      "neutral_headline": "Doctors Puzzled by STI in 83-Year-Old Married Man",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/health/2025/11/rare-form-of-syphilis-in-married-elderly-man-nearly-stumps-doctors/",
          "published_at": "Wed, 05 Nov 2025 23:29:42 +0000",
          "title": "83-year-old man married 50 years nearly stumps doctors with surprise STI",
          "standfirst": "Man said he was in a monogamous 50-year marriage, but doctors aren't so sure now.",
          "content": "Syphilis can be a tricky disease to diagnose—especially when a patient may not be sharing the whole story. Doctors in Belgium met with a real head-scratcher when an 83-year-old married man came in with a rare form of secondary syphilis—the second of four stages of the sexually transmitted bacterial infection that has been called a “master of disguise.” The man told doctors up front that he was in a monogamous 50-year-long marriage and had been sexually inactive in recent years following treatment for cancer. In a Clinical Problem-Solving report published today in the New England Journal of Medicine, doctors laid out the step-by-step tests and reasoning they used to get to the right diagnosis, which still didn’t answer all their questions.Read full article Comments",
          "feed_position": 5,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-2157231571-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-2157231571-1152x648.jpg",
      "popularity_score": 283.22696083333335,
      "ai_summary": [
        "An 83-year-old man presented with a sexually transmitted infection.",
        "The man claimed to have been in a monogamous marriage for fifty years.",
        "Doctors are investigating the source of the infection.",
        "The case has raised questions about the man's claims.",
        "The situation presents a medical mystery for the doctors."
      ]
    },
    {
      "id": "cluster_115",
      "coverage": 1,
      "updated_at": "Wed, 05 Nov 2025 21:03:03 +0000",
      "title": "DHS offers “disturbing new excuses” to seize kids’ biometric data, expert says",
      "neutral_headline": "DHS Seeks Biometric Data of Immigrant Children, Expert Says",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/11/dhs-wants-to-use-biometrics-to-track-immigrant-kids-throughout-their-lives/",
          "published_at": "Wed, 05 Nov 2025 21:03:03 +0000",
          "title": "DHS offers “disturbing new excuses” to seize kids’ biometric data, expert says",
          "standfirst": "Sweeping DHS power grab would collect face, iris, voice scans of all immigrants.",
          "content": "Civil and digital rights experts are horrified by a proposed rule change that would allow the Department of Homeland Security to collect a wide range of sensitive biometric data on all immigrants, without age restrictions, and store that data throughout each person’s “lifecycle” in the immigration system. If adopted, the rule change would allow DHS agencies, including Immigration and Customs Enforcement (ICE), to broadly collect facial imagery, finger and palm prints, iris scans, and voice prints. They may also request DNA, which DHS claimed “would only be collected in limited circumstances,” like to verify family relations. These updates would cost taxpayers $288.7 million annually, DHS estimated, including $57.1 million for DNA collection alone. Annual individual charges to immigrants submitting data will likely be similarly high, estimated at around $231.5 million. Costs could be higher, DHS admitted, especially if DNA testing is conducted more widely than projected.Read full article Comments",
          "feed_position": 7,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-511484858-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-511484858-1152x648.jpg",
      "popularity_score": 263,
      "ai_summary": [
        "The Department of Homeland Security plans to collect biometric data from children.",
        "This data includes face, iris, and voice scans of all immigrants.",
        "Experts are criticizing the DHS plan as a power grab.",
        "The plan would significantly expand the government's data collection.",
        "The DHS has offered explanations for the data collection."
      ]
    },
    {
      "id": "cluster_119",
      "coverage": 1,
      "updated_at": "Wed, 05 Nov 2025 20:00:26 +0000",
      "title": "New quantum hardware puts the mechanics in quantum mechanics",
      "neutral_headline": "New Quantum Hardware Tests Quantum Mechanics Models",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2025/11/new-quantum-computing-hardware-sorts-ions-for-computation/",
          "published_at": "Wed, 05 Nov 2025 20:00:26 +0000",
          "title": "New quantum hardware puts the mechanics in quantum mechanics",
          "standfirst": "As a test case, the machine was used to test a model of superconductivity.",
          "content": "Quantum computers based on ions or atoms have one major advantage: The qubits themselves aren’t manufactured, and there’s no device-to-device among atoms. Every atom is the same and should perform similarly every time. And since the qubits themselves can be moved around, it’s theoretically possible to entangle any atom or ion with any other in the system, allowing for a lot of flexibility in how algorithms and error correction are performed. This combination of consistent, high-fidelity performance with all-to-all connectivity has led many key demonstrations of quantum computing to be done on trapped-ion hardware. Unfortunately, the hardware has been held back a bit by relatively low qubit counts—a few dozen compared to the hundred or more seen in other technologies. But on Wednesday, a company called Quantinuum announced a new version of its trapped-ion hardware that significantly boosts the qubit count and uses some interesting technology to manage their operation. Trapped-ion computing Both neutral atom and trapped-ion computers store their qubits in the spin of the nucleus. That spin is somewhat shielded from the environment by the cloud of electrons around the nucleus, giving these qubits a relatively long coherence time. While neutral atoms are held in place by a network of lasers, trapped ions are manipulated via electromagnetic control based on the ion’s charge. This means that key components of the hardware can be built using standard electronic manufacturing, although lasers are still needed for manipulations and readout.Read full article Comments",
          "feed_position": 8,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/helios-chip_top-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/helios-chip_top-1152x648.jpg",
      "popularity_score": 253,
      "ai_summary": [
        "New quantum hardware has been developed for testing quantum mechanics.",
        "The machine was used to test a model of superconductivity.",
        "The hardware allows for experimental verification of quantum theories.",
        "The research provides insights into quantum phenomena.",
        "The technology could advance quantum computing research."
      ]
    },
    {
      "id": "cluster_121",
      "coverage": 1,
      "updated_at": "Wed, 05 Nov 2025 19:10:45 +0000",
      "title": "YouTube TV’s Disney blackout reminds users that they don’t own what they stream",
      "neutral_headline": "YouTube TV Disney Blackout Highlights Streaming Ownership",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2025/11/youtube-tvs-disney-blackout-reminds-users-that-they-dont-own-what-they-stream/",
          "published_at": "Wed, 05 Nov 2025 19:10:45 +0000",
          "title": "YouTube TV’s Disney blackout reminds users that they don’t own what they stream",
          "standfirst": "“This is a hard lesson for us all.”",
          "content": "Google and Disney have been in a contract dispute since October 30 that has resulted in YouTube TV subscribers losing access to 21 Disney-owned TV channels, including ABC, ESPN, and The Disney Channel. In addition to reducing access to popular live content, the corporate conflict is highlighting another frustration in the streaming era. As Google and Disney continue duking it out, their customers have lost some access to content they thought was permanent: DVR files and digital movie purchases. A perk of subscribing to YouTube TV, per Google’s marketing, is the ability to “record it all with unlimited DVR space.” A footnote on the YouTube TV homepage notes that unlimited DVR is subject to “device, regional, and Internet restrictions” but overlooks an additional restriction in the form of multi-conglomerate spats.Read full article Comments",
          "feed_position": 9,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2021/12/getty-youtube-tv-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2021/12/getty-youtube-tv-1152x648.jpg",
      "popularity_score": 243,
      "ai_summary": [
        "YouTube TV users experienced a blackout of Disney content.",
        "The blackout served as a reminder that users do not own streamed content.",
        "The situation highlighted the limitations of streaming services.",
        "The event prompted discussion about content ownership.",
        "Users learned a lesson about the nature of streaming services."
      ]
    },
    {
      "id": "cluster_134",
      "coverage": 1,
      "updated_at": "Wed, 05 Nov 2025 16:36:12 +0000",
      "title": "If you want to satiate AI’s hunger for power, Google suggests going to space",
      "neutral_headline": "Google Suggests Space Data Centers to Power AI",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2025/11/if-you-want-to-satiate-ais-hunger-for-power-google-suggests-going-to-space/",
          "published_at": "Wed, 05 Nov 2025 16:36:12 +0000",
          "title": "If you want to satiate AI’s hunger for power, Google suggests going to space",
          "standfirst": "Google engineers think they already have all the pieces needed to build a data center in orbit.",
          "content": "It was probably always when, not if, Google would add its name to the list of companies intrigued by the potential of orbiting data centers. Google announced Tuesday a new initiative, named Project Suncatcher, to examine the feasibility of bringing artificial intelligence to space. The idea is to deploy swarms of satellites in low-Earth orbit, each carrying Google’s AI accelerator chips designed for training, content generation, synthetic speech and vision, and predictive modeling. Google calls these chips Tensor Processing Units, or TPUs. “Project Suncatcher is a moonshot exploring a new frontier: equipping solar-powered satellite constellations with TPUs and free-space optical links to one day scale machine learning compute in space,” Google wrote in a blog post.Read full article Comments",
          "feed_position": 11,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/googletpu-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/googletpu-1152x648.jpg",
      "popularity_score": 168,
      "ai_summary": [
        "Google engineers propose building data centers in space.",
        "They believe they have the necessary components for an orbital data center.",
        "The proposal aims to meet the power demands of AI.",
        "The plan involves placing data centers in orbit.",
        "The idea is to address the energy needs of AI development."
      ]
    },
    {
      "id": "cluster_138",
      "coverage": 1,
      "updated_at": "Wed, 05 Nov 2025 15:42:57 +0000",
      "title": "How to declutter, quiet down, and take the AI out of Windows 11 25H2",
      "neutral_headline": "Guide to Decluttering and Quieting Windows 11 25H2",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2025/11/what-i-do-to-clean-up-a-clean-install-of-windows-11-23h2-and-edge/",
          "published_at": "Wed, 05 Nov 2025 15:42:57 +0000",
          "title": "How to declutter, quiet down, and take the AI out of Windows 11 25H2",
          "standfirst": "A new major Windows 11 release means a new guide for cleaning up the OS.",
          "content": "It’s that time of year again—temperatures are dropping, leaves are changing color, and Microsoft is gradually rolling out another major yearly update to Windows 11. The Windows 11 25H2 update is relatively minor compared to last year’s 24H2 update (the “25” here is a reference to the year the update was released, while the “H2” denotes that it was released in the second half of the year, a vestigial suffix from when Microsoft would release two major Windows updates per year). The 24H2 update came with some major under-the-hood overhauls of core Windows components and significant performance improvements for the Arm version; 25H2 is largely 24H2, but with a rolled-over version number to keep it in line with Microsoft’s timeline for security updates and tech support. But Microsoft’s continuous update cadence for Windows 11 means that even the 24H2 version as it currently exists isn’t the same one Microsoft released a year ago.Read full article Comments",
          "feed_position": 13,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2024/02/windows-11-cleanup-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2024/02/windows-11-cleanup-1152x648.jpg",
      "popularity_score": 148,
      "ai_summary": [
        "A new major Windows 11 release requires a cleanup guide.",
        "The guide focuses on decluttering and optimizing the operating system.",
        "It provides instructions for removing unwanted features.",
        "The guide aims to improve user experience.",
        "The guide helps users take AI out of Windows 11."
      ]
    },
    {
      "id": "cluster_136",
      "coverage": 1,
      "updated_at": "Wed, 05 Nov 2025 16:26:02 +0000",
      "title": "Google settlement with Epic caps Play Store fees, boosts other Android app stores",
      "neutral_headline": "Google Settlement Caps Play Store Fees, Boosts App Stores",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2025/11/google-settlement-with-epic-caps-play-store-fees-boosts-other-android-app-stores/",
          "published_at": "Wed, 05 Nov 2025 16:26:02 +0000",
          "title": "Google settlement with Epic caps Play Store fees, boosts other Android app stores",
          "standfirst": "Google will make several changes to Android app support globally, supported through at least 2032.",
          "content": "Google has spent the last few years waging a losing battle against Epic Games, which accused the Android maker of illegally stifling competition in mobile apps. Losses in court left Google to make sweeping changes to the Play Store, but Google appeared poised to take the case all the way to the Supreme Court. That is unlikely now that Epic and Google have reached a settlement in the case. It still needs to be approved by the judge, but the agreement provides a framework for long-term changes to Android app distribution that would apply globally. Late last month, Google was forced to make the first round of mandated changes to the Play Store to comply with the court’s ruling. It grudgingly began allowing developers to direct users to alternative payment options and app downloads outside of Google’s ecosystem. By next summer, Google was supposed to open up Android to third-party app stores in a big way. These changes were only mandated for three years and in the United States. The new agreement includes a different vision for third-party stores on Android—one that Google finds more palatable and that still gives Epic what it wants. If approved, the settlement will lower Google’s standard fee for developers. There will also be new support in Android for third-party app stores that will reduce the friction of leaving the Google bubble. Under the terms of the settlement, Google will support these changes through at least June 2032.Read full article Comments",
          "feed_position": 12,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/08/google-logo-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/08/google-logo-1152x648.jpg",
      "popularity_score": 145,
      "ai_summary": [
        "Google reached a settlement with Epic Games regarding Play Store fees.",
        "The settlement includes changes to Android app support globally.",
        "These changes will be supported through at least the year 2032.",
        "The settlement caps fees for developers on the Play Store.",
        "The agreement will boost other Android app stores."
      ]
    },
    {
      "id": "cluster_144",
      "coverage": 1,
      "updated_at": "Wed, 05 Nov 2025 14:00:26 +0000",
      "title": "So long, Assistant—Gemini is taking over Google Maps",
      "neutral_headline": "Gemini Takes Over Google Maps from Assistant",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/google/2025/11/so-long-assistant-gemini-is-taking-over-google-maps/",
          "published_at": "Wed, 05 Nov 2025 14:00:26 +0000",
          "title": "So long, Assistant—Gemini is taking over Google Maps",
          "standfirst": "Gemini is rolling out to Maps on Android and iOS, with Android Auto coming soon.",
          "content": "Google is in the process of purging Assistant across its products, and the next target is Google Maps. Starting today, Gemini will begin rolling out in Maps, powering new experiences for navigation, location info, and more. This update will eventually completely usurp Google Assistant’s hands-free role in Maps, but the rollout will take time. So for now, the smart assistant in Google Maps will still depend on how you’re running the app. Across all Gemini’s incarnations, Google stresses its conversational abilities. Whereas Assistant was hard-pressed to keep one or two balls in the air, you can theoretically give Gemini much more complex instructions. Google’s demo includes someone asking for nearby restaurants with cheap vegan food, but instead of just providing a list, it suggests something based on the user’s input. Gemini can also offer more information about the location. Maps will also get its own Gemini-infused version of Lens for after you park. You will be able to point the camera at a landmark, restaurant, or other business to get instant answers to your questions. This experience will be distinct from the version of Lens available in the Google app, focused on giving you location-based information. Maybe you want to know about the menu at a restaurant or what it’s like inside. Sure, you could open the door… but AI!Read full article Comments",
          "feed_position": 16,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/Gemini-in-navigation-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/Gemini-in-navigation-1152x648.jpg",
      "popularity_score": 145,
      "ai_summary": [
        "Gemini is replacing Assistant in Google Maps.",
        "The change is rolling out on Android and iOS devices.",
        "Android Auto will receive Gemini support soon.",
        "Gemini will provide navigation and other features.",
        "The transition marks a shift in Google's AI integration."
      ]
    },
    {
      "id": "cluster_156",
      "coverage": 1,
      "updated_at": "Wed, 05 Nov 2025 09:15:41 +0000",
      "title": "Space junk may have struck a Chinese crew ship in low-Earth orbit",
      "neutral_headline": "Space Junk May Have Struck Chinese Crew Ship",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2025/11/landing-postponed-for-chinese-astronauts-after-suspected-space-debris-strike/",
          "published_at": "Wed, 05 Nov 2025 09:15:41 +0000",
          "title": "Space junk may have struck a Chinese crew ship in low-Earth orbit",
          "standfirst": "The three-man crew was supposed to return to Earth on Wednesday to wrap up six months in space.",
          "content": "Three Chinese astronauts were due to depart the Tiangong space station, reenter the atmosphere, and land in the remote desert of Inner Mongolia on Wednesday. Instead, officials ordered the crew to remain at the station while engineers investigate a potential problem with their landing craft. The China Manned Space Agency, run by the country’s military, announced the change late Tuesday in a brief statement posted to Weibo, the Chinese social media platform. “The Shenzhou 20 manned spacecraft is suspected of being impacted by small space debris,” the statement said. “Impact analysis and risk assessment are underway. To ensure the safety and health of the astronauts and the complete success of the mission, it has been decided that the Shenzhou 20 return mission, originally scheduled for November 5, will be postponed.”Read full article Comments",
          "feed_position": 18,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-2211173685-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-2211173685-1152x648.jpg",
      "popularity_score": 141,
      "ai_summary": [
        "Space debris may have struck a Chinese crew ship.",
        "The crew was scheduled to return to Earth on Wednesday.",
        "The crew had been in space for six months.",
        "The incident occurred in low-Earth orbit.",
        "The impact's severity is currently unknown."
      ]
    },
    {
      "id": "cluster_139",
      "coverage": 1,
      "updated_at": "Wed, 05 Nov 2025 15:40:58 +0000",
      "title": "Tesla’s European and Chinese customers are staying away in droves",
      "neutral_headline": "Tesla Sales Decline in Europe and China",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/cars/2025/11/after-a-great-q3-tesla-sees-double-digit-declines-all-over-europe/",
          "published_at": "Wed, 05 Nov 2025 15:40:58 +0000",
          "title": "Tesla’s European and Chinese customers are staying away in droves",
          "standfirst": "Sales tank as investors get ready to decide whether to make Musk a trillionaire.",
          "content": "Tesla’s shareholders are ready to vote tomorrow on whether to give Elon Musk an even more vast slice of the company in an effort to keep him focused on selling electric vehicles. Currently, the trolling tycoon appears a little obsessed with the UK, a place he appears to conflate with Middle Earth, which investors may or may not take into account when making their decision. What they ought to take into account is how many cars Tesla sold last month. Although Tesla only publishes quarterly sales figures and does not divide those up by region, slightly more granular data is available from some countries via monthly new car registrations. And the numbers for October, when compared year on year to the same month in 2024, should be alarming. Sales fell by double-digit margins in Sweden (89 percent), Denmark (86 percent), Belgium (69 percent), Finland (68 percent), Austria (65 percent), Switzerland (60 percent), Portugal (59 percent), Germany (54 percent), Norway (50 percent), the Netherlands (48 percent), the UK (47 percent), Italy (47 percent), and Spain (31 percent).Read full article Comments",
          "feed_position": 14,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/09/GettyImages-2211638677-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/09/GettyImages-2211638677-1152x648.jpg",
      "popularity_score": 139,
      "ai_summary": [
        "Tesla sales are decreasing in Europe and China.",
        "Investors are preparing for a decision regarding Elon Musk.",
        "The sales decline is impacting investor confidence.",
        "The situation is affecting Tesla's financial performance.",
        "The future of Tesla is being assessed by investors."
      ]
    },
    {
      "id": "cluster_132",
      "coverage": 1,
      "updated_at": "Wed, 05 Nov 2025 16:45:22 +0000",
      "title": "Flock haters cross political divides to remove error-prone cameras",
      "neutral_headline": "Flock Camera Critics Remove Error-Prone Cameras",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/11/flock-haters-cross-political-divides-to-remove-error-prone-cameras/",
          "published_at": "Wed, 05 Nov 2025 16:45:22 +0000",
          "title": "Flock haters cross political divides to remove error-prone cameras",
          "standfirst": "Lawmakers' calls for Flock probe may help kill local contracts, expert says.",
          "content": "Flock Safety—the surveillance company behind the country’s largest network of automated license plate readers (ALPRs)—currently faces attacks on multiple fronts seeking to tear down the invasive and error-prone cameras across the US. This week, two lawmakers, Sen. Ron Wyden (D-Ore.) and Rep. Raja Krishnamoorthi (D-Ill.), called for a federal investigation, alleging that Flock has been “negligently handling Americans’ personal data” by failing to use cybersecurity best practices. The month prior, Wyden wrote a letter to Flock CEO Garrett Langley, alleging that Flock’s security failures mean that “abuse of Flock cameras is inevitable” and that they threaten to expose billions of people’s harvested data should a catastrophic breach occur. “In my view, local elected officials can best protect their constituents from the inevitable abuses of Flock cameras by removing Flock from their communities,” Wyden wrote.Read full article Comments",
          "feed_position": 10,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-2205485838-1024x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-2205485838-1024x648.jpg",
      "popularity_score": 133,
      "ai_summary": [
        "Critics of Flock cameras are working to remove them.",
        "The criticism crosses political divides.",
        "Lawmakers' calls for a Flock probe may end local contracts.",
        "An expert is commenting on the situation.",
        "The cameras are being removed due to errors."
      ]
    },
    {
      "id": "cluster_143",
      "coverage": 1,
      "updated_at": "Wed, 05 Nov 2025 14:42:10 +0000",
      "title": "Why being too attractive can hurt fitness influencers",
      "neutral_headline": "Attractive Fitness Influencers Face \"Beauty Backfire",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/health/2025/11/why-being-too-attractive-can-hurt-fitness-influencers/",
          "published_at": "Wed, 05 Nov 2025 14:42:10 +0000",
          "title": "Why being too attractive can hurt fitness influencers",
          "standfirst": "The \"beauty backfire effect\" is especially strong in the fitness space.",
          "content": "“Sex sells” has been a mantra in marketing for decades. As researchers who study consumer behavior, we’ve seen plenty of evidence to support it: Attractive models and spokespeople have been shown to reliably grab attention, boost clicks, and make products seem more desirable. But our new research suggests that in a digital world full of influencers—trusted tastemakers with large online followings—being too attractive can actually backfire, particularly in the fitness space. We call this the “beauty backfire effect,” and we put it to the test in a series of laboratory experiments.Read full article Comments",
          "feed_position": 15,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/influencer-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/influencer-1152x648.jpg",
      "popularity_score": 133,
      "ai_summary": [
        "Being too attractive can negatively impact fitness influencers.",
        "The \"beauty backfire effect\" is especially strong in fitness.",
        "The effect can undermine credibility and trust.",
        "The phenomenon affects how audiences perceive influencers.",
        "The effect can hinder the influencer's success."
      ]
    },
    {
      "id": "cluster_147",
      "coverage": 1,
      "updated_at": "Wed, 05 Nov 2025 13:00:50 +0000",
      "title": "“So much more menacing”: Formula E’s new Gen4 car breaks cover",
      "neutral_headline": "“So much more menacing”: Formula E’s new Gen4 car breaks cover",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/cars/2025/11/formula-e-gets-2x-the-power-and-awd-with-new-gen4-car/",
          "published_at": "Wed, 05 Nov 2025 13:00:50 +0000",
          "title": "“So much more menacing”: Formula E’s new Gen4 car breaks cover",
          "standfirst": "Up to 700 kW regen braking, new Bridgestone tires, and it's even fully recyclable.",
          "content": "Formula E officially revealed its next electric racing car today. At first glance, the Gen4 machine looks similar to machinery of seasons past, but looks are deceiving—it’s “so much more menacing,” according to Formula E CEO Jeff Dodds. The new car is not only longer and wider, it’s far more powerful. The wings and bodywork now generate meaningful aerodynamic downforce. There will be a new tire supplier as Bridgestone returns to single-seat racing. The car is even completely recyclable. I’m not sure that everyone who attended a Formula E race in its first season would have bet on the sport’s continued existence more than a decade down the line. When the cars took their green flag for the first time in Beijing in 2014, as many people derided it for being too slow or for the mid-race car swaps as praised it for trying something new in the world of motorsport. Despite that, the racing was mostly entertaining, and it got better with the introduction of the Gen2 car, which made car swapping a thing of the past. Gen3 added more power, then temporary all-wheel drive with the advent of the Gen3 Evo days. That car will continue to race in season 12, which kicks off in Brazil on December 6 and ends in mid-August in London. When season 13 picks up in late 2026, we might see a pretty different kind of Formula E racing.Read full article Comments",
          "feed_position": 17,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/FORMULAE_15-1152x648.jpeg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/FORMULAE_15-1152x648.jpeg",
      "popularity_score": 133,
      "ai_summary": [
        "Formula E's new Gen4 car has been revealed.",
        "The car features up to 700 kW regen braking.",
        "It includes new Bridgestone tires.",
        "The car is fully recyclable.",
        "The new car is designed to be more menacing."
      ]
    },
    {
      "id": "cluster_157",
      "coverage": 1,
      "updated_at": "Wed, 05 Nov 2025 00:50:43 +0000",
      "title": "In a stunning comeback, Jared Isaacman is renominated to lead NASA",
      "neutral_headline": "Jared Isaacman Renominated to Lead NASA, Acknowledges Expectations",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2025/11/in-a-stunning-comeback-jared-isaacman-is-renominated-to-lead-nasa/",
          "published_at": "Wed, 05 Nov 2025 00:50:43 +0000",
          "title": "In a stunning comeback, Jared Isaacman is renominated to lead NASA",
          "standfirst": "\"I will do everything I can to live up to those expectations.\"",
          "content": "President Trump announced Tuesday evening that he is renominating private astronaut Jared Isaacman to lead NASA. “Jared’s passion for space, astronaut experience, and dedication to pushing the boundaries of exploration, unlocking the mysteries of the universe, and advancing the new space economy make him ideally suited to lead NASA into a bold new era,” Trump wrote on his social media network, Truth Social. In his statement, Trump did not offer an explanation for why he found Isaacman acceptable now after pulling his original nomination in late May.Read full article Comments",
          "feed_position": 19,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2021/02/Jared_Isaacman_at_SpaceX_2-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2021/02/Jared_Isaacman_at_SpaceX_2-1152x648.jpg",
      "popularity_score": 133,
      "ai_summary": [
        "Jared Isaacman received renomination to lead NASA, according to an Ars Technica article.",
        "Isaacman stated he would strive to meet the expectations associated with the role.",
        "The article highlights Isaacman's commitment to fulfilling the responsibilities.",
        "The nomination reflects confidence in Isaacman's leadership capabilities.",
        "Isaacman's response indicates his dedication to the NASA mission."
      ]
    }
  ]
}