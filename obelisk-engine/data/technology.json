{
  "updated_at": "2025-10-19T19:14:15.684Z",
  "clusters": [
    {
      "id": "cluster_19",
      "coverage": 2,
      "updated_at": "Sun, 19 Oct 2025 13:00:00 +0000",
      "title": "Engadget review recap: New Pixel devices, Meta Ray-Ban Display, ASUS ROG Xbox Ally X and more",
      "neutral_headline": "Engadget review recap: New Pixel devices, Meta Ray-Ban Display, ASUS ROG Xbox Ally X and more",
      "items": [
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/engadget-review-recap-new-pixel-devices-meta-ray-ban-display-asus-rog-xbox-ally-x-and-more-130000215.html",
          "published_at": "Sun, 19 Oct 2025 13:00:00 +0000",
          "title": "Engadget review recap: New Pixel devices, Meta Ray-Ban Display, ASUS ROG Xbox Ally X and more",
          "standfirst": "Techtober is a busy time for our reviews team as a deluge of new devices arrive before the holiday season. We’ve been hard at work conducting our in-depth testing, but it’s understandable if you missed a review or two over the last few weeks. Read on to catch up all the reviews you might’ve missed, including the latest trio of Google Pixel devices. Google Pixel 10 Pro Fold, Pixel Watch 4 and Pixel Buds 2a Earlier this month, a trio of new Pixel devices arrived for us to put through their paces. The Pixel 10 Pro Fold headlines the group, but the Pixel Watch 4 and Pixel Buds 2a are also highly capable devices in their own categories. Senior reviews writer Sam Rutherford wrote that Google has removed one of the last remaining issues with foldable phones: durability. “The addition of proper dust and water resistance on the Pixel 10 Pro Fold has removed one of the last two shortcomings of modern foldables, which is no mean feat,” he said. However, at $1,799, the price barrier is still hard to overcome for most folks.” Meta Ray-Ban Display The second-gen Meta Ray-Ban smart glasses may be the best option for most people right now, but the Meta Ray-Ban Display is the more exciting product for many. If you can get past the chunky look and in-progress features, you still have to contend with limited availability and a $800 price tag. “The Meta Ray-Ban Display glasses, on the other hand, still look very much like a first-gen product,” senior reporter Karissa Bell wrote. “There are some really compelling use cases for the display, but its functionality is limited.” ASUS ROG Xbox Ally X Microsoft’s collaboration with ASUS has produced a compelling gaming handheld that’s decidedly Xbox. The combination of familiar controls and a full-screen Xbox experience give this device an advantage over its Windows-based competition. “While Microsoft’s first real foray into PC gaming handhelds isn’t upending the status quo and it’s way too early to say if this gadget will save Xbox as a whole, it is bringing some notable advancements,” Sam said. “The new full-screen experience makes launching and playing games on Windows-based devices so much more seamless that it’s kind of wild it took so long to get here.” Bose QuietComfort Headphones (2nd gen) Bose never leave the top few spots on our best noise canceling headphones list, but its refresh of the QC Ultra Headphones put the company firmly back in first place. Updates to active noise cancellation, sound performance, battery life and power management are enough to recommend this model over the previous version and the Sony WH-1000XM6. “Similar to the second-gen QC Ultra Earbuds over the summer, Bose didn’t make huge upgrades for the updated version of the QC Ultra Headphones,” I explained. “But what you do get here is a decent improvement over its predecessor.” Razer Blade 18 (2025) Razer’s massive 18-inch gaming laptop has impressive performance, but it’s size and limited battery life don’t offer a lot of convenience. “Personally, if I had to choose between Razer’s current lineup, I’d go with the Blade 16 so that I could actually carry it around and occasionally use it as a productivity machine,” senior reviews editor Devindra Hardawar wrote. “Not so with the Blade 18 — its short two hour and 17 minute battery life (in PCMark 10’s battery benchmark) means you’ll always need to lug around its beefy power adapter.”This article originally appeared on Engadget at https://www.engadget.com/engadget-review-recap-new-pixel-devices-meta-ray-ban-display-asus-rog-xbox-ally-x-and-more-130000215.html?src=rss",
          "content": "Techtober is a busy time for our reviews team as a deluge of new devices arrive before the holiday season. We’ve been hard at work conducting our in-depth testing, but it’s understandable if you missed a review or two over the last few weeks. Read on to catch up all the reviews you might’ve missed, including the latest trio of Google Pixel devices. Google Pixel 10 Pro Fold, Pixel Watch 4 and Pixel Buds 2a Earlier this month, a trio of new Pixel devices arrived for us to put through their paces. The Pixel 10 Pro Fold headlines the group, but the Pixel Watch 4 and Pixel Buds 2a are also highly capable devices in their own categories. Senior reviews writer Sam Rutherford wrote that Google has removed one of the last remaining issues with foldable phones: durability. “The addition of proper dust and water resistance on the Pixel 10 Pro Fold has removed one of the last two shortcomings of modern foldables, which is no mean feat,” he said. However, at $1,799, the price barrier is still hard to overcome for most folks.” Meta Ray-Ban Display The second-gen Meta Ray-Ban smart glasses may be the best option for most people right now, but the Meta Ray-Ban Display is the more exciting product for many. If you can get past the chunky look and in-progress features, you still have to contend with limited availability and a $800 price tag. “The Meta Ray-Ban Display glasses, on the other hand, still look very much like a first-gen product,” senior reporter Karissa Bell wrote. “There are some really compelling use cases for the display, but its functionality is limited.” ASUS ROG Xbox Ally X Microsoft’s collaboration with ASUS has produced a compelling gaming handheld that’s decidedly Xbox. The combination of familiar controls and a full-screen Xbox experience give this device an advantage over its Windows-based competition. “While Microsoft’s first real foray into PC gaming handhelds isn’t upending the status quo and it’s way too early to say if this gadget will save Xbox as a whole, it is bringing some notable advancements,” Sam said. “The new full-screen experience makes launching and playing games on Windows-based devices so much more seamless that it’s kind of wild it took so long to get here.” Bose QuietComfort Headphones (2nd gen) Bose never leave the top few spots on our best noise canceling headphones list, but its refresh of the QC Ultra Headphones put the company firmly back in first place. Updates to active noise cancellation, sound performance, battery life and power management are enough to recommend this model over the previous version and the Sony WH-1000XM6. “Similar to the second-gen QC Ultra Earbuds over the summer, Bose didn’t make huge upgrades for the updated version of the QC Ultra Headphones,” I explained. “But what you do get here is a decent improvement over its predecessor.” Razer Blade 18 (2025) Razer’s massive 18-inch gaming laptop has impressive performance, but it’s size and limited battery life don’t offer a lot of convenience. “Personally, if I had to choose between Razer’s current lineup, I’d go with the Blade 16 so that I could actually carry it around and occasionally use it as a productivity machine,” senior reviews editor Devindra Hardawar wrote. “Not so with the Blade 18 — its short two hour and 17 minute battery life (in PCMark 10’s battery benchmark) means you’ll always need to lug around its beefy power adapter.”This article originally appeared on Engadget at https://www.engadget.com/engadget-review-recap-new-pixel-devices-meta-ray-ban-display-asus-rog-xbox-ally-x-and-more-130000215.html?src=rss",
          "feed_position": 2
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/the-teacher-is-the-new-engineer-inside-the-rise-of-ai-enablement-and",
          "published_at": "Sun, 19 Oct 2025 03:45:00 GMT",
          "title": "The teacher is the new engineer: Inside the rise of AI enablement and PromptOps",
          "standfirst": "As more companies quickly begin using gen AI, it’s important to avoid a big mistake that could impact its effectiveness: Proper onboarding. Companies spend time and money training new human workers to succeed, but when they use large language model (LLM) helpers, many treat them like simple tools that need no explanation. This isn&#x27;t just a waste of resources; it&#x27;s risky. Research shows that AI has advanced quickly from testing to actual use in 2024 to 2025, with almost a third of companies reporting a sharp increase in usage and acceptance from the previous year.Probabilistic systems need governance, not wishful thinkingUnlike traditional software, gen AI is probabilistic and adaptive. It learns from interaction, can drift as data or usage changes and operates in the gray zone between automation and agency. Treating it like static software ignores reality: Without monitoring and updates, models degrade and produce faulty outputs: A phenomenon widely known as model drift. Gen AI also lacks built-in organizational intelligence. A model trained on internet data may write a Shakespearean sonnet, but it won’t know your escalation paths and compliance constraints unless you teach it. Regulators and standards bodies have begun pushing guidance precisely because these systems behave dynamically and can hallucinate, mislead or leak data if left unchecked.The real-world costs of skipping onboardingWhen LLMs hallucinate, misinterpret tone, leak sensitive information or amplify bias, the costs are tangible.Misinformation and liability: A Canadian tribunal held Air Canada liable after its website chatbot gave a passenger incorrect policy information. The ruling made it clear that companies remain responsible for their AI agents’ statements.Embarrassing hallucinations: In 2025, a syndicated “summer reading list” carried by the Chicago Sun-Times and Philadelphia Inquirer recommended books that didn’t exist; the writer had used AI without adequate verification, prompting retractions and firings.Bias at scale: The Equal Employment Opportunity Commission (EEOCs) first AI-discrimination settlement involved a recruiting algorithm that auto-rejected older applicants, underscoring how unmonitored systems can amplify bias and create legal risk.Data leakage: After employees pasted sensitive code into ChatGPT, Samsung temporarily banned public gen AI tools on corporate devices — an avoidable misstep with better policy and training.The message is simple: Un-onboarded AI and un-governed usage create legal, security and reputational exposure.Treat AI agents like new hiresEnterprises should onboard AI agents as deliberately as they onboard people — with job descriptions, training curricula, feedback loops and performance reviews. This is a cross-functional effort across data science, security, compliance, design, HR and the end users who will work with the system daily.Role definition. Spell out scope, inputs/outputs, escalation paths and acceptable failure modes. A legal copilot, for instance, can summarize contracts and surface risky clauses, but should avoid final legal judgments and must escalate edge cases.Contextual training. Fine-tuning has its place, but for many teams, retrieval-augmented generation (RAG) and tool adapters are safer, cheaper and more auditable. RAG keeps models grounded in your latest, vetted knowledge (docs, policies, knowledge bases), reducing hallucinations and improving traceability. Emerging Model Context Protocol (MCP) integrations make it easier to connect copilots to enterprise systems in a controlled way — bridging models with tools and data while preserving separation of concerns. Salesforce’s Einstein Trust Layer illustrates how vendors are formalizing secure grounding, masking, and audit controls for enterprise AI.Simulation before production. Don’t let your AI’s first “training” be with real customers. Build high-fidelity sandboxes and stress-test tone, reasoning and edge cases — then evaluate with human graders. Morgan Stanley built an evaluation regimen for its GPT-4 assistant, having advisors and prompt engineers grade answers and refine prompts before broad rollout. The result: >98% adoption among advisor teams once quality thresholds were met. Vendors are also moving to simulation: Salesforce recently highlighted digital-twin testing to rehearse agents safely against realistic scenarios. 4) Cross-functional mentorship. Treat early usage as a two-way learning loop: Domain experts and front-line users give feedback on tone, correctness and usefulness; security and compliance teams enforce boundaries and red lines; designers shape frictionless UIs that encourage proper use.Feedback loops and performance reviews—foreverOnboarding doesn’t end at go-live. The most meaningful learning begins after deployment.Monitoring and observability: Log outputs, track KPIs (accuracy, satisfaction, escalation rates) and watch for degradation. Cloud providers now ship observability/evaluation tooling to help teams detect drift and regressions in production, especially for RAG systems whose knowledge changes over time.User feedback channels. Provide in-product flagging and structured review queues so humans can coach the model — then close the loop by feeding these signals into prompts, RAG sources or fine-tuning sets.Regular audits. Schedule alignment checks, factual audits and safety evaluations. Microsoft’s enterprise responsible-AI playbooks, for instance, emphasize governance and staged rollouts with executive visibility and clear guardrails.Succession planning for models. As laws, products and models evolve, plan upgrades and retirement the way you would plan people transitions — run overlap tests and port institutional knowledge (prompts, eval sets, retrieval sources).Why this is urgent nowGen AI is no longer an “innovation shelf” project — it’s embedded in CRMs, support desks, analytics pipelines and executive workflows. Banks like Morgan Stanley and Bank of America are focusing AI on internal copilot use cases to boost employee efficiency while constraining customer-facing risk, an approach that hinges on structured onboarding and careful scoping. Meanwhile, security leaders say gen AI is everywhere, yet one-third of adopters haven’t implemented basic risk mitigations, a gap that invites shadow AI and data exposure.The AI-native workforce also expects better: Transparency, traceability, and the ability to shape the tools they use. Organizations that provide this — through training, clear UX affordances and responsive product teams — see faster adoption and fewer workarounds. When users trust a copilot, they use it; when they don’t, they bypass it.As onboarding matures, expect to see AI enablement managers and PromptOps specialists in more org charts, curating prompts, managing retrieval sources, running eval suites and coordinating cross-functional updates. Microsoft’s internal Copilot rollout points to this operational discipline: Centers of excellence, governance templates and executive-ready deployment playbooks. These practitioners are the “teachers” who keep AI aligned with fast-moving business goals.A practical onboarding checklistIf you’re introducing (or rescuing) an enterprise copilot, start here:Write the job description. Scope, inputs/outputs, tone, red lines, escalation rules.Ground the model. Implement RAG (and/or MCP-style adapters) to connect to authoritative, access-controlled sources; prefer dynamic grounding over broad fine-tuning where possible.Build the simulator. Create scripted and seeded scenarios; measure accuracy, coverage, tone, safety; require human sign-offs to graduate stages.Ship with guardrails. DLP, data masking, content filters and audit trails (see vendor trust layers and responsible-AI standards).Instrument feedback. In-product flagging, analytics and dashboards; schedule weekly triage.Review and retrain. Monthly alignment checks, quarterly factual audits and planned model upgrades — with side-by-side A/Bs to prevent regressions.In a future where every employee has an AI teammate, the organizations that take onboarding seriously will move faster, safer and with greater purpose. Gen AI doesn’t just need data or compute; it needs guidance, goals, and growth plans. Treating AI systems as teachable, improvable and accountable team members turns hype into habitual value.Dhyey Mavani is accelerating generative AI at LinkedIn.",
          "content": "As more companies quickly begin using gen AI, it’s important to avoid a big mistake that could impact its effectiveness: Proper onboarding. Companies spend time and money training new human workers to succeed, but when they use large language model (LLM) helpers, many treat them like simple tools that need no explanation. This isn&#x27;t just a waste of resources; it&#x27;s risky. Research shows that AI has advanced quickly from testing to actual use in 2024 to 2025, with almost a third of companies reporting a sharp increase in usage and acceptance from the previous year.Probabilistic systems need governance, not wishful thinkingUnlike traditional software, gen AI is probabilistic and adaptive. It learns from interaction, can drift as data or usage changes and operates in the gray zone between automation and agency. Treating it like static software ignores reality: Without monitoring and updates, models degrade and produce faulty outputs: A phenomenon widely known as model drift. Gen AI also lacks built-in organizational intelligence. A model trained on internet data may write a Shakespearean sonnet, but it won’t know your escalation paths and compliance constraints unless you teach it. Regulators and standards bodies have begun pushing guidance precisely because these systems behave dynamically and can hallucinate, mislead or leak data if left unchecked.The real-world costs of skipping onboardingWhen LLMs hallucinate, misinterpret tone, leak sensitive information or amplify bias, the costs are tangible.Misinformation and liability: A Canadian tribunal held Air Canada liable after its website chatbot gave a passenger incorrect policy information. The ruling made it clear that companies remain responsible for their AI agents’ statements.Embarrassing hallucinations: In 2025, a syndicated “summer reading list” carried by the Chicago Sun-Times and Philadelphia Inquirer recommended books that didn’t exist; the writer had used AI without adequate verification, prompting retractions and firings.Bias at scale: The Equal Employment Opportunity Commission (EEOCs) first AI-discrimination settlement involved a recruiting algorithm that auto-rejected older applicants, underscoring how unmonitored systems can amplify bias and create legal risk.Data leakage: After employees pasted sensitive code into ChatGPT, Samsung temporarily banned public gen AI tools on corporate devices — an avoidable misstep with better policy and training.The message is simple: Un-onboarded AI and un-governed usage create legal, security and reputational exposure.Treat AI agents like new hiresEnterprises should onboard AI agents as deliberately as they onboard people — with job descriptions, training curricula, feedback loops and performance reviews. This is a cross-functional effort across data science, security, compliance, design, HR and the end users who will work with the system daily.Role definition. Spell out scope, inputs/outputs, escalation paths and acceptable failure modes. A legal copilot, for instance, can summarize contracts and surface risky clauses, but should avoid final legal judgments and must escalate edge cases.Contextual training. Fine-tuning has its place, but for many teams, retrieval-augmented generation (RAG) and tool adapters are safer, cheaper and more auditable. RAG keeps models grounded in your latest, vetted knowledge (docs, policies, knowledge bases), reducing hallucinations and improving traceability. Emerging Model Context Protocol (MCP) integrations make it easier to connect copilots to enterprise systems in a controlled way — bridging models with tools and data while preserving separation of concerns. Salesforce’s Einstein Trust Layer illustrates how vendors are formalizing secure grounding, masking, and audit controls for enterprise AI.Simulation before production. Don’t let your AI’s first “training” be with real customers. Build high-fidelity sandboxes and stress-test tone, reasoning and edge cases — then evaluate with human graders. Morgan Stanley built an evaluation regimen for its GPT-4 assistant, having advisors and prompt engineers grade answers and refine prompts before broad rollout. The result: >98% adoption among advisor teams once quality thresholds were met. Vendors are also moving to simulation: Salesforce recently highlighted digital-twin testing to rehearse agents safely against realistic scenarios. 4) Cross-functional mentorship. Treat early usage as a two-way learning loop: Domain experts and front-line users give feedback on tone, correctness and usefulness; security and compliance teams enforce boundaries and red lines; designers shape frictionless UIs that encourage proper use.Feedback loops and performance reviews—foreverOnboarding doesn’t end at go-live. The most meaningful learning begins after deployment.Monitoring and observability: Log outputs, track KPIs (accuracy, satisfaction, escalation rates) and watch for degradation. Cloud providers now ship observability/evaluation tooling to help teams detect drift and regressions in production, especially for RAG systems whose knowledge changes over time.User feedback channels. Provide in-product flagging and structured review queues so humans can coach the model — then close the loop by feeding these signals into prompts, RAG sources or fine-tuning sets.Regular audits. Schedule alignment checks, factual audits and safety evaluations. Microsoft’s enterprise responsible-AI playbooks, for instance, emphasize governance and staged rollouts with executive visibility and clear guardrails.Succession planning for models. As laws, products and models evolve, plan upgrades and retirement the way you would plan people transitions — run overlap tests and port institutional knowledge (prompts, eval sets, retrieval sources).Why this is urgent nowGen AI is no longer an “innovation shelf” project — it’s embedded in CRMs, support desks, analytics pipelines and executive workflows. Banks like Morgan Stanley and Bank of America are focusing AI on internal copilot use cases to boost employee efficiency while constraining customer-facing risk, an approach that hinges on structured onboarding and careful scoping. Meanwhile, security leaders say gen AI is everywhere, yet one-third of adopters haven’t implemented basic risk mitigations, a gap that invites shadow AI and data exposure.The AI-native workforce also expects better: Transparency, traceability, and the ability to shape the tools they use. Organizations that provide this — through training, clear UX affordances and responsive product teams — see faster adoption and fewer workarounds. When users trust a copilot, they use it; when they don’t, they bypass it.As onboarding matures, expect to see AI enablement managers and PromptOps specialists in more org charts, curating prompts, managing retrieval sources, running eval suites and coordinating cross-functional updates. Microsoft’s internal Copilot rollout points to this operational discipline: Centers of excellence, governance templates and executive-ready deployment playbooks. These practitioners are the “teachers” who keep AI aligned with fast-moving business goals.A practical onboarding checklistIf you’re introducing (or rescuing) an enterprise copilot, start here:Write the job description. Scope, inputs/outputs, tone, red lines, escalation rules.Ground the model. Implement RAG (and/or MCP-style adapters) to connect to authoritative, access-controlled sources; prefer dynamic grounding over broad fine-tuning where possible.Build the simulator. Create scripted and seeded scenarios; measure accuracy, coverage, tone, safety; require human sign-offs to graduate stages.Ship with guardrails. DLP, data masking, content filters and audit trails (see vendor trust layers and responsible-AI standards).Instrument feedback. In-product flagging, analytics and dashboards; schedule weekly triage.Review and retrain. Monthly alignment checks, quarterly factual audits and planned model upgrades — with side-by-side A/Bs to prevent regressions.In a future where every employee has an AI teammate, the organizations that take onboarding seriously will move faster, safer and with greater purpose. Gen AI doesn’t just need data or compute; it needs guidance, goals, and growth plans. Treating AI systems as teachable, improvable and accountable team members turns hype into habitual value.Dhyey Mavani is accelerating generative AI at LinkedIn.",
          "feed_position": 0,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/3LM1cCj4dJrbJGr6K5iiIe/b18688f1a0faf3befb7be85063da4801/u7277289442_A_sophisticated_AI_agent_is_standing_at_the_front_a862de2f-78d6-4330-8b28-8957b27aa9e0_0.png"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/gaming/a-spooky-nes-platformer-more-n-and-other-new-indie-games-worth-checking-out-110000259.html",
          "published_at": "Sat, 18 Oct 2025 11:00:00 +0000",
          "title": "A spooky NES platformer, more N++ and other new indie games worth checking out",
          "standfirst": "Welcome to our latest roundup of what's going on in the indie game space. I've reluctantly paused Ball x Pit for long enough to share some neat new releases and more details on upcoming games — some of which are arriving very soon. We've got a notable update for a classic as well. Steam Next Fest is taking place at the minute, and you still have until Monday to join in by checking out some of the many, many demos that have gone live for the event. Thanks partially (okay, almost entirely) to being unable to escape Ball x Pit, I've only tried a fewm Next Fest demos so far. I'm a fan of Aerial_Knight's Never Yield and after last year's sequel, it's cool to see solo developer Neil Jones (aka Aerial_Knight) trying something totally different. Aerial_Knight’s DropShot is a skydiving first-person shooter with finger guns and dragons. It’s a single-player game in which the aim is to take out your opponents and reach the ground first. Like Jones' previous games, it's stylish and fast-paced. I'm planning to check out the full game when it arrives down the line.It certainly helps to be a fast, accurate typer when you put words together for a living, but I wasn't quick or precise enough to win any rounds in the Final Sentence demo. This is a battle royale for up to 100 players in which you're at a typewriter and have to bash out sentences (or other strings of letters, numbers and symbols) in a race to the finish. If you run out of time, make too many mistakes or don't win, it's lights out, courtesy of the masked figure with a revolver who’s standing in front of you. There are some nice touches here. Having to type out the rules in the first few rounds is a clever idea on the part of developer Button Mash. I haven't won a round myself yet (I finished in second place a couple of times), but watched some streamers play. It's very funny when the winning player flips the bird at the guy holding a revolver in front of them. Final Sentence is coming to Steam later this year. Maybe I'll have learned how to spell \"sphinx\" by then.There are a few other Next Fest demos I'd like to try this weekend, namely:Crashout Crew (Overcooked-style co-op chaos with forklifts)Reanimal (co-op horror from the team behind Little Nightmares and Little Nightmares 2)Slots & Daggers (as if I need another slot-machine-based roguelite in my life right now) The Last Caretaker (sci-fi survival)Goodnight Universe (we'll get to that)I've been looking forward to Skate Story for forever, but I think I'm going to skip that demo. I'm already sold and I'm fine with waiting a couple more months before playing the whole thing.There are a couple of showcases coming up next week that might be worth keeping an eye on. The third annual edition of DreadXP's indie horror showcase is set for 1PM ET on October 23. You can catch that on the publisher's YouTube channel.Two hours later, you'll be able to tune into the Galaxies Autumn showcase. This will feature more than 50 games, including world premieres, gameplay trailers and other announcements. Games that will be featured include PowerWash Simulator 2, Mouse: PI For Hire and Denshattack, all of which are firmly on my to-play list.New releasesMister Scary is a weird little guy. I love when you get to play a game as a weird little guy. The game of the same name is a spooky NES homebrew platformer from Calgames. Mister Scary can stomp on his enemies, or freeze or burn them after eating a snack. When Mister Scary ducks, he becomes immune to damage because he's taking a nap. I appreciate that. Nothing scary can happen while you're snoozing.Mister Scary is $10 on Itch. You'll need to plug the ROM into a NES emulator to become Mister Scary.The only reason I still have Flash Player installed on my PC is so I can open N, which sits on my desktop, once in a while. I've been playing that classic freeware platformer for a long time, and now there's a good reason for many people to revisit the third entry in the series. As a thank you to the N++ community, Metanet Software is releasing a free update to mark the 10th anniversary of the game's PS4 debut. TEN++ is said to include the developer's \"most challenging levels yet.\" Given how darn tough these games are already, that's saying something. The update is available now on Steam and it's coming to the console versions of N++ soon.The Cabin Factory is an anomaly-hunting (i.e. spot the difference) game in the style of The Exit 8. You'll examine horror-themed cabins that are built for use in movies and theme parks to make sure they aren't actually haunted. If you spot an anomaly, you'll want to get out of the cabin post haste.This $3 horror walking sim from International Cat Studios and publisher Future Friends Games debuted on Steam last year, and it just hit consoles in time for Halloween. It's out now on PS4, PS5, Xbox One, Xbox Series X/S and Nintendo Switch.Upcoming After CloverPit and Ball x Pit, I was planning to take a break from roguelikes/roguelites before diving into Hades 2. Alas, the latest game from the legendary Ron Gilbert now has a release date, and it's very soon!In Death by Scrolling, the aim is to collect enough gold to pay a ferryman so you can escape purgatory. However, there's a wall of fire coming after you the whole time, so you'll need to keep moving in order to try to stay alive. You'll also need to avoid or stun an unkillable grim reaper as you collect gold and gems that unlock upgrades. Death by Scrolling is from Gilbert's Terrible Toybox and MicroProse Software. It's coming to Steam on October 28.There's a lot going on in Silly Polly Beast. It's safe to say this game is a shooter, but the release date trailer rapidly flits between perspectives and genres. There's an emphasis on survival horror, along with puzzles and stealth segments. Polly will even sometimes remove the board that's strapped to her back for some skateboarding sequences. This is said to have a story that morphs and evolves as much as the gameplay does. After escaping her hellish orphanage, Polly lands right in the underworld and has to navigate her way out of that too. Developer Andrei Chernyshov and publisher Top Hat Studios are behind Silly Polly Beast. It's coming to Steam, PS4, PS5, Xbox One, Xbox Series X/S and Nintendo Switch on October 28.Also coming to Steam on October 28 is a project from Autoscopia Interactive that's designed to be played in a single sitting. As Long As You’re Here is a first-person game that places you in the role of a woman with Alzheimer’s disease. Her memories of the past, including her late brother, blend into the present as Annie settles into living with her family. As Long As You’re Here started as a student project by Marlène Delrive, who was trying to better understand what her grandmother was experiencing in her final years. \"The aim is to create a mature and nuanced experience that shows the difficult repercussions of losing not only your memory, but also your agency and sense of time and place,\" the developers said.Let's close things out with a new trailer for Goodnight Universe. This is a cinematic adventure in which you play as a six-month-old baby. This particular infant is incredibly intelligent and has psychic powers. Isaac simply desires familial love and acceptance but (shock horror!) a tech company wants to take away the tot.As with Nice Dreams' last game (the stupendous Before Your Eyes), you control Goodnight Universe with your peepers via your device's camera. It seems fascinating, and I really have to check out the Next Fest demo. Publisher Skybound Games is bringing it to Steam, Xbox Series X/S, PS5, Nintendo Switch and Switch 2 on November 11.This article originally appeared on Engadget at https://www.engadget.com/gaming/a-spooky-nes-platformer-more-n-and-other-new-indie-games-worth-checking-out-110000259.html?src=rss",
          "content": "Welcome to our latest roundup of what's going on in the indie game space. I've reluctantly paused Ball x Pit for long enough to share some neat new releases and more details on upcoming games — some of which are arriving very soon. We've got a notable update for a classic as well. Steam Next Fest is taking place at the minute, and you still have until Monday to join in by checking out some of the many, many demos that have gone live for the event. Thanks partially (okay, almost entirely) to being unable to escape Ball x Pit, I've only tried a fewm Next Fest demos so far. I'm a fan of Aerial_Knight's Never Yield and after last year's sequel, it's cool to see solo developer Neil Jones (aka Aerial_Knight) trying something totally different. Aerial_Knight’s DropShot is a skydiving first-person shooter with finger guns and dragons. It’s a single-player game in which the aim is to take out your opponents and reach the ground first. Like Jones' previous games, it's stylish and fast-paced. I'm planning to check out the full game when it arrives down the line.It certainly helps to be a fast, accurate typer when you put words together for a living, but I wasn't quick or precise enough to win any rounds in the Final Sentence demo. This is a battle royale for up to 100 players in which you're at a typewriter and have to bash out sentences (or other strings of letters, numbers and symbols) in a race to the finish. If you run out of time, make too many mistakes or don't win, it's lights out, courtesy of the masked figure with a revolver who’s standing in front of you. There are some nice touches here. Having to type out the rules in the first few rounds is a clever idea on the part of developer Button Mash. I haven't won a round myself yet (I finished in second place a couple of times), but watched some streamers play. It's very funny when the winning player flips the bird at the guy holding a revolver in front of them. Final Sentence is coming to Steam later this year. Maybe I'll have learned how to spell \"sphinx\" by then.There are a few other Next Fest demos I'd like to try this weekend, namely:Crashout Crew (Overcooked-style co-op chaos with forklifts)Reanimal (co-op horror from the team behind Little Nightmares and Little Nightmares 2)Slots & Daggers (as if I need another slot-machine-based roguelite in my life right now) The Last Caretaker (sci-fi survival)Goodnight Universe (we'll get to that)I've been looking forward to Skate Story for forever, but I think I'm going to skip that demo. I'm already sold and I'm fine with waiting a couple more months before playing the whole thing.There are a couple of showcases coming up next week that might be worth keeping an eye on. The third annual edition of DreadXP's indie horror showcase is set for 1PM ET on October 23. You can catch that on the publisher's YouTube channel.Two hours later, you'll be able to tune into the Galaxies Autumn showcase. This will feature more than 50 games, including world premieres, gameplay trailers and other announcements. Games that will be featured include PowerWash Simulator 2, Mouse: PI For Hire and Denshattack, all of which are firmly on my to-play list.New releasesMister Scary is a weird little guy. I love when you get to play a game as a weird little guy. The game of the same name is a spooky NES homebrew platformer from Calgames. Mister Scary can stomp on his enemies, or freeze or burn them after eating a snack. When Mister Scary ducks, he becomes immune to damage because he's taking a nap. I appreciate that. Nothing scary can happen while you're snoozing.Mister Scary is $10 on Itch. You'll need to plug the ROM into a NES emulator to become Mister Scary.The only reason I still have Flash Player installed on my PC is so I can open N, which sits on my desktop, once in a while. I've been playing that classic freeware platformer for a long time, and now there's a good reason for many people to revisit the third entry in the series. As a thank you to the N++ community, Metanet Software is releasing a free update to mark the 10th anniversary of the game's PS4 debut. TEN++ is said to include the developer's \"most challenging levels yet.\" Given how darn tough these games are already, that's saying something. The update is available now on Steam and it's coming to the console versions of N++ soon.The Cabin Factory is an anomaly-hunting (i.e. spot the difference) game in the style of The Exit 8. You'll examine horror-themed cabins that are built for use in movies and theme parks to make sure they aren't actually haunted. If you spot an anomaly, you'll want to get out of the cabin post haste.This $3 horror walking sim from International Cat Studios and publisher Future Friends Games debuted on Steam last year, and it just hit consoles in time for Halloween. It's out now on PS4, PS5, Xbox One, Xbox Series X/S and Nintendo Switch.Upcoming After CloverPit and Ball x Pit, I was planning to take a break from roguelikes/roguelites before diving into Hades 2. Alas, the latest game from the legendary Ron Gilbert now has a release date, and it's very soon!In Death by Scrolling, the aim is to collect enough gold to pay a ferryman so you can escape purgatory. However, there's a wall of fire coming after you the whole time, so you'll need to keep moving in order to try to stay alive. You'll also need to avoid or stun an unkillable grim reaper as you collect gold and gems that unlock upgrades. Death by Scrolling is from Gilbert's Terrible Toybox and MicroProse Software. It's coming to Steam on October 28.There's a lot going on in Silly Polly Beast. It's safe to say this game is a shooter, but the release date trailer rapidly flits between perspectives and genres. There's an emphasis on survival horror, along with puzzles and stealth segments. Polly will even sometimes remove the board that's strapped to her back for some skateboarding sequences. This is said to have a story that morphs and evolves as much as the gameplay does. After escaping her hellish orphanage, Polly lands right in the underworld and has to navigate her way out of that too. Developer Andrei Chernyshov and publisher Top Hat Studios are behind Silly Polly Beast. It's coming to Steam, PS4, PS5, Xbox One, Xbox Series X/S and Nintendo Switch on October 28.Also coming to Steam on October 28 is a project from Autoscopia Interactive that's designed to be played in a single sitting. As Long As You’re Here is a first-person game that places you in the role of a woman with Alzheimer’s disease. Her memories of the past, including her late brother, blend into the present as Annie settles into living with her family. As Long As You’re Here started as a student project by Marlène Delrive, who was trying to better understand what her grandmother was experiencing in her final years. \"The aim is to create a mature and nuanced experience that shows the difficult repercussions of losing not only your memory, but also your agency and sense of time and place,\" the developers said.Let's close things out with a new trailer for Goodnight Universe. This is a cinematic adventure in which you play as a six-month-old baby. This particular infant is incredibly intelligent and has psychic powers. Isaac simply desires familial love and acceptance but (shock horror!) a tech company wants to take away the tot.As with Nice Dreams' last game (the stupendous Before Your Eyes), you control Goodnight Universe with your peepers via your device's camera. It seems fascinating, and I really have to check out the Next Fest demo. Publisher Skybound Games is bringing it to Steam, Xbox Series X/S, PS5, Nintendo Switch and Switch 2 on November 11.This article originally appeared on Engadget at https://www.engadget.com/gaming/a-spooky-nes-platformer-more-n-and-other-new-indie-games-worth-checking-out-110000259.html?src=rss",
          "feed_position": 11
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/abstract-or-die-why-ai-enterprises-cant-afford-rigid-vector-stacks",
          "published_at": "Sat, 18 Oct 2025 09:00:00 GMT",
          "title": "Abstract or die: Why AI enterprises can't afford rigid vector stacks",
          "standfirst": "Vector databases (DBs), once specialist research instruments, have become widely used infrastructure in just a few years. They power today&#x27;s semantic search, recommendation engines, anti-fraud measures and gen AI applications across industries. There are a deluge of options: PostgreSQL with pgvector, MySQL HeatWave, DuckDB VSS, SQLite VSS, Pinecone, Weaviate, Milvus and several others.The riches of choices sound like a boon to companies. But just beneath, a growing problem looms: Stack instability. New vector DBs appear each quarter, with disparate APIs, indexing schemes and performance trade-offs. Today&#x27;s ideal choice may look dated or limiting tomorrow.To business AI teams, volatility translates into lock-in risks and migration hell. Most projects begin life with lightweight engines like DuckDB or SQLite for prototyping, then move to Postgres, MySQL or a cloud-native service in production. Each switch involves rewriting queries, reshaping pipelines, and slowing down deployments.This re-engineering merry-go-round undermines the very speed and agility that AI adoption is supposed to bring.Why portability matters nowCompanies have a tricky balancing act:Experiment quickly with minimal overhead, in hopes of trying and getting early value; Scale safely on stable, production-quality infrastructure without months of refactoring;Be nimble in a world where new and better backends arrive nearly every month. Without portability, organizations stagnate. They have technical debt from recursive code paths, are hesitant to adopt new technology and cannot move prototypes to production at pace. In effect, the database is a bottleneck rather than an accelerator.Portability, or the ability to move underlying infrastructure without re-encoding the application, is ever more a strategic requirement for enterprises rolling out AI at scale.Abstraction as infrastructureThe solution is not to pick the \"perfect\" vector database (there isn&#x27;t one), but to change how enterprises think about the problem.In software engineering, the adapter pattern provides a stable interface while hiding underlying complexity. Historically, we&#x27;ve seen how this principle reshaped entire industries:ODBC/JDBC gave enterprises a single way to query relational databases, reducing the risk of being tied to Oracle, MySQL or SQL Server; Apache Arrow standardized columnar data formats, so data systems could play nice together; ONNX created a vendor-agnostic format for machine learning (ML) models, bringing TensorFlow, PyTorch, etc. together; Kubernetes abstracted infrastructure details, so workloads could run the same everywhere on clouds;any-llm (Mozilla AI) now makes it possible to have one API across lots of large language model (LLM) vendors, so playing with AI is safer. All these abstractions led to adoption by lowering switching costs. They turned broken ecosystems into solid, enterprise-level infrastructure.Vector databases are also at the same tipping point.The adapter approach to vectorsInstead of having application code directly bound to some specific vector backend, companies can compile against an abstraction layer that normalizes operations like inserts, queries and filtering.This doesn&#x27;t necessarily eliminate the need to choose a backend; it makes that choice less rigid. Development teams can start with DuckDB or SQLite in the lab, then scale up to Postgres or MySQL for production and ultimately adopt a special-purpose cloud vector DB without having to re-architect the application.Open source efforts like Vectorwrap are early examples of this approach, presenting a single Python API to Postgres, MySQL, DuckDB and SQLite. They demonstrate the power of abstraction to accelerate prototyping, reduce lock-in risk and support hybrid architectures employing numerous backends.Why businesses should careFor leaders of data infrastructure and decision-makers for AI, abstraction offers three benefits:Speed from prototype to productionTeams are able to prototype on lightweight local environments and scale without expensive rewrites.Reduced vendor riskOrganizations can adopt new backends as they emerge without long migration projects by decoupling app code from specific databases.Hybrid flexibilityCompanies can mix transactional, analytical and specialized vector DBs under one architecture, all behind an aggregated interface.The result is data layer agility, and that&#x27;s more and more the difference between fast and slow companies.A broader movement in open sourceWhat&#x27;s happening in the vector space is one example of a bigger trend: Open-source abstractions as critical infrastructure.In data formats: Apache ArrowIn ML models: ONNXIn orchestration: KubernetesIn AI APIs: Any-LLM and other such frameworksThese projects succeed, not by adding new capability, but by removing friction. They enable enterprises to move more quickly, hedge bets and evolve along with the ecosystem.Vector DB adapters continue this legacy, transforming a high-speed, fragmented space into infrastructure that enterprises can truly depend on.The future of vector DB portabilityThe landscape of vector DBs will not converge anytime soon. Instead, the number of options will grow, and every vendor will tune for different use cases, scale, latency, hybrid search, compliance or cloud platform integration.Abstraction becomes strategy in this case. Companies adopting portable approaches will be capable of:Prototyping boldlyDeploying in a flexible mannerScaling rapidly to new techIt&#x27;s possible we&#x27;ll eventually see a \"JDBC for vectors,\" a universal standard that codifies queries and operations across backends. Until then, open-source abstractions are laying the groundwork.ConclusionEnterprises adopting AI cannot afford to be slowed by database lock-in. As the vector ecosystem evolves, the winners will be those who treat abstraction as infrastructure, building against portable interfaces rather than binding themselves to any single backend.The decades-long lesson of software engineering is simple: Standards and abstractions lead to adoption. For vector DBs, that revolution has already begun.Mihir Ahuja is an AI/ML engineer and open-source contributor based in San Francisco.",
          "content": "Vector databases (DBs), once specialist research instruments, have become widely used infrastructure in just a few years. They power today&#x27;s semantic search, recommendation engines, anti-fraud measures and gen AI applications across industries. There are a deluge of options: PostgreSQL with pgvector, MySQL HeatWave, DuckDB VSS, SQLite VSS, Pinecone, Weaviate, Milvus and several others.The riches of choices sound like a boon to companies. But just beneath, a growing problem looms: Stack instability. New vector DBs appear each quarter, with disparate APIs, indexing schemes and performance trade-offs. Today&#x27;s ideal choice may look dated or limiting tomorrow.To business AI teams, volatility translates into lock-in risks and migration hell. Most projects begin life with lightweight engines like DuckDB or SQLite for prototyping, then move to Postgres, MySQL or a cloud-native service in production. Each switch involves rewriting queries, reshaping pipelines, and slowing down deployments.This re-engineering merry-go-round undermines the very speed and agility that AI adoption is supposed to bring.Why portability matters nowCompanies have a tricky balancing act:Experiment quickly with minimal overhead, in hopes of trying and getting early value; Scale safely on stable, production-quality infrastructure without months of refactoring;Be nimble in a world where new and better backends arrive nearly every month. Without portability, organizations stagnate. They have technical debt from recursive code paths, are hesitant to adopt new technology and cannot move prototypes to production at pace. In effect, the database is a bottleneck rather than an accelerator.Portability, or the ability to move underlying infrastructure without re-encoding the application, is ever more a strategic requirement for enterprises rolling out AI at scale.Abstraction as infrastructureThe solution is not to pick the \"perfect\" vector database (there isn&#x27;t one), but to change how enterprises think about the problem.In software engineering, the adapter pattern provides a stable interface while hiding underlying complexity. Historically, we&#x27;ve seen how this principle reshaped entire industries:ODBC/JDBC gave enterprises a single way to query relational databases, reducing the risk of being tied to Oracle, MySQL or SQL Server; Apache Arrow standardized columnar data formats, so data systems could play nice together; ONNX created a vendor-agnostic format for machine learning (ML) models, bringing TensorFlow, PyTorch, etc. together; Kubernetes abstracted infrastructure details, so workloads could run the same everywhere on clouds;any-llm (Mozilla AI) now makes it possible to have one API across lots of large language model (LLM) vendors, so playing with AI is safer. All these abstractions led to adoption by lowering switching costs. They turned broken ecosystems into solid, enterprise-level infrastructure.Vector databases are also at the same tipping point.The adapter approach to vectorsInstead of having application code directly bound to some specific vector backend, companies can compile against an abstraction layer that normalizes operations like inserts, queries and filtering.This doesn&#x27;t necessarily eliminate the need to choose a backend; it makes that choice less rigid. Development teams can start with DuckDB or SQLite in the lab, then scale up to Postgres or MySQL for production and ultimately adopt a special-purpose cloud vector DB without having to re-architect the application.Open source efforts like Vectorwrap are early examples of this approach, presenting a single Python API to Postgres, MySQL, DuckDB and SQLite. They demonstrate the power of abstraction to accelerate prototyping, reduce lock-in risk and support hybrid architectures employing numerous backends.Why businesses should careFor leaders of data infrastructure and decision-makers for AI, abstraction offers three benefits:Speed from prototype to productionTeams are able to prototype on lightweight local environments and scale without expensive rewrites.Reduced vendor riskOrganizations can adopt new backends as they emerge without long migration projects by decoupling app code from specific databases.Hybrid flexibilityCompanies can mix transactional, analytical and specialized vector DBs under one architecture, all behind an aggregated interface.The result is data layer agility, and that&#x27;s more and more the difference between fast and slow companies.A broader movement in open sourceWhat&#x27;s happening in the vector space is one example of a bigger trend: Open-source abstractions as critical infrastructure.In data formats: Apache ArrowIn ML models: ONNXIn orchestration: KubernetesIn AI APIs: Any-LLM and other such frameworksThese projects succeed, not by adding new capability, but by removing friction. They enable enterprises to move more quickly, hedge bets and evolve along with the ecosystem.Vector DB adapters continue this legacy, transforming a high-speed, fragmented space into infrastructure that enterprises can truly depend on.The future of vector DB portabilityThe landscape of vector DBs will not converge anytime soon. Instead, the number of options will grow, and every vendor will tune for different use cases, scale, latency, hybrid search, compliance or cloud platform integration.Abstraction becomes strategy in this case. Companies adopting portable approaches will be capable of:Prototyping boldlyDeploying in a flexible mannerScaling rapidly to new techIt&#x27;s possible we&#x27;ll eventually see a \"JDBC for vectors,\" a universal standard that codifies queries and operations across backends. Until then, open-source abstractions are laying the groundwork.ConclusionEnterprises adopting AI cannot afford to be slowed by database lock-in. As the vector ecosystem evolves, the winners will be those who treat abstraction as infrastructure, building against portable interfaces rather than binding themselves to any single backend.The decades-long lesson of software engineering is simple: Standards and abstractions lead to adoption. For vector DBs, that revolution has already begun.Mihir Ahuja is an AI/ML engineer and open-source contributor based in San Francisco.",
          "feed_position": 1,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/2Xu1YqwqTUMJMVWnRrKy7M/23b8a69e94d864a0306f259b1cd29397/u7277289442_A_rendering_of_animated_friendly_AI_agents_with_h_9b63c83c-c489-4154-a498-3bdfdc3a5c5b_3.png"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/developers-can-now-add-live-google-maps-data-to-gemini-powered-ai-app",
          "published_at": "Fri, 17 Oct 2025 22:31:00 GMT",
          "title": "Developers can now add live Google Maps data to Gemini-powered AI app outputs",
          "standfirst": "Google is adding a new feature for third-party developers building atop its Gemini AI models that rivals like OpenAI&#x27;s ChatGPT, Anthropic&#x27;s Claude, and the growing array of Chinese open source options are unlikely to get anytime soon: grounding with Google Maps.This addition allows developers to connect Google&#x27;s Gemini AI models&#x27; reasoning capabilities with live geospatial data from Google Maps, enabling applications to deliver detailed, location-relevant responses to user queries—such as business hours, reviews, or the atmosphere of a specific venue. By tapping into data from over 250 million places, developers can now build more intelligent and responsive location-aware experiences.This is particularly useful for applications where proximity, real-time availability, or location-specific personalization matter—such as local search, delivery services, real estate, and travel planning. When the user’s location is known, developers can pass latitude and longitude into the request to enhance the response quality.By tightly integrating real-time and historical Maps data into the Gemini API, Google enables applications to generate grounded, location-specific responses with factual accuracy and contextual depth that are uniquely possible through its mapping infrastructure.Merging AI and Geospatial IntelligenceThe new feature is accessible in Google AI Studio, where developers can try a live demo powered by the Gemini Live API. Models that support the grounding with Google Maps include:Gemini 2.5 ProGemini 2.5 FlashGemini 2.5 Flash-LiteGemini 2.0 FlashIn one demonstration, a user asked for Italian restaurant recommendations in Chicago. The assistant, leveraging Maps data, retrieved top-rated options and clarified a misspelled restaurant name before locating the correct venue with accurate business details.Developers can also retrieve a context token to embed a Google Maps widget in their app’s user interface. This interactive component displays photos, reviews, and other familiar content typically found in Google Maps.Integration is handled via the generateContent method in the Gemini API, where developers include googleMaps as a tool. They can also enable a Maps widget by setting a parameter in the request. The widget, rendered using a returned context token, can provide a visual layer alongside the AI-generated text.Use Cases Across IndustriesThe Maps grounding tool is designed to support a wide range of practical use cases:Itinerary generation: Travel apps can create detailed daily plans with routing, timing, and venue information.Personalized local recommendations: Real estate platforms can highlight listings near kid-friendly amenities like schools and parks.Detailed location queries: Applications can provide specific information, such as whether a cafe offers outdoor seating, using community reviews and Maps metadata.Developers are encouraged to only enable the tool when geographic context is relevant, to optimize both performance and cost. According to the developer documentation, pricing starts at $25 per 1,000 grounded prompts — a steep sum for those trafficking in numerous queries.Combining Search and Maps for Enhanced ContextDevelopers can use Grounding with Google Maps alongside Grounding with Google Search in the same request.While the Maps tool contributes factual data—like addresses, hours, and ratings—the Search tool adds broader context from web content, such as news or event listings.For example, when asked about live music on Beale Street, the combined tools provide venue details from Maps and event times from Search. According to Google, internal testing shows that using both tools together leads to significantly improved response quality.Unfortunately, it doesn&#x27;t appear that the Google Maps grounding includes live vehicular traffic data — at least not yet.Customization and Developer FlexibilityThe experience is built for customization. Developers can tweak system prompts, choose from different Gemini models, and configure voice settings to tailor interactions. The demo app in Google AI Studio is also remixable, enabling developers to test ideas, add features, and iterate on designs within a flexible development environment.The API returns structured metadata—including source links, place IDs, and citation spans—that developers can use to build inline citations or verify the AI-generated outputs. This supports transparency and enhances trust in user-facing applications. Google also requires that Maps-based sources be attributed clearly and linked back to the source using their URI.Implementation Considerations for AI BuildersFor technical teams integrating this capability, Google recommends:Passing user location context when known, for better results.Displaying Google Maps source links directly beneath the relevant content.Only enabling the tool when the query clearly involves geographic context.Monitoring latency and disabling grounding when performance is critical.Grounding with Google Maps is currently available globally, though prohibited in several territories (including China, Iran, North Korea, and Cuba), and not permitted for emergency response use cases.Availability and AccessGrounding with Google Maps is now generally available through the Gemini API. With this release, Google continues to expand the capabilities of the Gemini API, empowering developers to build AI-driven applications that understand and respond to the world around them.",
          "content": "Google is adding a new feature for third-party developers building atop its Gemini AI models that rivals like OpenAI&#x27;s ChatGPT, Anthropic&#x27;s Claude, and the growing array of Chinese open source options are unlikely to get anytime soon: grounding with Google Maps.This addition allows developers to connect Google&#x27;s Gemini AI models&#x27; reasoning capabilities with live geospatial data from Google Maps, enabling applications to deliver detailed, location-relevant responses to user queries—such as business hours, reviews, or the atmosphere of a specific venue. By tapping into data from over 250 million places, developers can now build more intelligent and responsive location-aware experiences.This is particularly useful for applications where proximity, real-time availability, or location-specific personalization matter—such as local search, delivery services, real estate, and travel planning. When the user’s location is known, developers can pass latitude and longitude into the request to enhance the response quality.By tightly integrating real-time and historical Maps data into the Gemini API, Google enables applications to generate grounded, location-specific responses with factual accuracy and contextual depth that are uniquely possible through its mapping infrastructure.Merging AI and Geospatial IntelligenceThe new feature is accessible in Google AI Studio, where developers can try a live demo powered by the Gemini Live API. Models that support the grounding with Google Maps include:Gemini 2.5 ProGemini 2.5 FlashGemini 2.5 Flash-LiteGemini 2.0 FlashIn one demonstration, a user asked for Italian restaurant recommendations in Chicago. The assistant, leveraging Maps data, retrieved top-rated options and clarified a misspelled restaurant name before locating the correct venue with accurate business details.Developers can also retrieve a context token to embed a Google Maps widget in their app’s user interface. This interactive component displays photos, reviews, and other familiar content typically found in Google Maps.Integration is handled via the generateContent method in the Gemini API, where developers include googleMaps as a tool. They can also enable a Maps widget by setting a parameter in the request. The widget, rendered using a returned context token, can provide a visual layer alongside the AI-generated text.Use Cases Across IndustriesThe Maps grounding tool is designed to support a wide range of practical use cases:Itinerary generation: Travel apps can create detailed daily plans with routing, timing, and venue information.Personalized local recommendations: Real estate platforms can highlight listings near kid-friendly amenities like schools and parks.Detailed location queries: Applications can provide specific information, such as whether a cafe offers outdoor seating, using community reviews and Maps metadata.Developers are encouraged to only enable the tool when geographic context is relevant, to optimize both performance and cost. According to the developer documentation, pricing starts at $25 per 1,000 grounded prompts — a steep sum for those trafficking in numerous queries.Combining Search and Maps for Enhanced ContextDevelopers can use Grounding with Google Maps alongside Grounding with Google Search in the same request.While the Maps tool contributes factual data—like addresses, hours, and ratings—the Search tool adds broader context from web content, such as news or event listings.For example, when asked about live music on Beale Street, the combined tools provide venue details from Maps and event times from Search. According to Google, internal testing shows that using both tools together leads to significantly improved response quality.Unfortunately, it doesn&#x27;t appear that the Google Maps grounding includes live vehicular traffic data — at least not yet.Customization and Developer FlexibilityThe experience is built for customization. Developers can tweak system prompts, choose from different Gemini models, and configure voice settings to tailor interactions. The demo app in Google AI Studio is also remixable, enabling developers to test ideas, add features, and iterate on designs within a flexible development environment.The API returns structured metadata—including source links, place IDs, and citation spans—that developers can use to build inline citations or verify the AI-generated outputs. This supports transparency and enhances trust in user-facing applications. Google also requires that Maps-based sources be attributed clearly and linked back to the source using their URI.Implementation Considerations for AI BuildersFor technical teams integrating this capability, Google recommends:Passing user location context when known, for better results.Displaying Google Maps source links directly beneath the relevant content.Only enabling the tool when the query clearly involves geographic context.Monitoring latency and disabling grounding when performance is critical.Grounding with Google Maps is currently available globally, though prohibited in several territories (including China, Iran, North Korea, and Cuba), and not permitted for emergency response use cases.Availability and AccessGrounding with Google Maps is now generally available through the Gemini API. With this release, Google continues to expand the capabilities of the Gemini API, empowering developers to build AI-driven applications that understand and respond to the world around them.",
          "feed_position": 2,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/2ehvkDsQ7NC7wQuECOcDGS/cb89da9ccaf13f2888f005ac93cecffa/cfr0z3n_realistic_graphic_novel_hyper_detailed_flat_illustratio_8aae43d5-1e50-4ebd-8760-ea28dbc0f328.png"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/wearables/meta-ray-ban-display-review-chunky-frames-with-impressive-abilities-193127070.html",
          "published_at": "Fri, 17 Oct 2025 19:31:27 +0000",
          "title": "Meta Ray-Ban Display review: Chunky frames with impressive abilities",
          "standfirst": "I've been wearing the $800 Meta Ray-Ban Display glasses daily for ten days and I'm still a bit conflicted. On one hand, I'm still not entirely comfortable with how they look. I've worn them on the bus, at the office, on walks around my neighborhood and during hangouts with friends. Each time, I'm very aware that I probably look a bit strange. On the other hand, there's a lot I really like about using these glasses. The built-in display has helped me look at my phone less throughout the day. The neural band feels more innovative than any wrist-based device I've tried. Together, it feels like a significant milestone for smart glasses overall. But it's also very much a first-generation device with some issues that still need to be worked out. Chunky statement glasses or hideously nerdy? To once again state the obvious: The frames are extremely chunky and too wide for my face. The dark black frames I tried for this review unfortunately accentuate the extra thickness. I won't pretend it's my best look and I did feel a bit self-conscious at times wearing these in public. Meta also makes a light brown \"sand\" color that I tried at the Connect event, and I think that color is a bit more flattering, even if the frames are just as oversized. (Sidenote: Smart glasses companies, please, please make your frames available in something other than black!) But, everyone has a different face shape, skin tone and general ability to \"pull off\" what one of my friends charitably described as \"chunky statement glasses.\" What looks not-great on my face, may look good on someone else. I really wish Meta could have squeezed this tech into slightly smaller frames, but I did get more used to the look the more I wore them. Overall, I do think the size is a reasonable tradeoff for a first-generation product that's pretty clearly aimed at early adopters. Here's how they look in the lighter \"sand\" color. Karissa Bell for Engadget The reason the glasses are so thick compared with Meta's other frames is because there are a lot of extra components to power the display, including a mini projector and waveguide. And, at 69 grams, the display glasses are noticeably heavier. I didn't find it particularly uncomfortable at first, but there is a noticeable pressure after six or seven hours of wear. Plus, the extra weight and width also made them consistently slide down my nose. I'm not sure I'd feel comfortable wearing these on a bike ride or a jog as I'd worry about them falling off. While I tested these, I was very interested to get reactions from friends and family. I didn't get many positive comments about how they looked on my face, though a few particularly generous colleagues assured me I was \"pulling them off.\" But seeing people's reactions as soon as the display activated was another matter. Almost everyone has had the same initial reaction: \"whoa.\" Quality display with some limitations As I discussed in my initial impressions, these glasses have a monocular display on the right side, so it doesn't offer the kind of immersive AR I experienced with the Orion prototype last year. You have to look slightly up and to the right to focus on the full-color display. It's impressively bright and clear, but doesn't overtake your vision. At 20 degrees, the field of view is small, but it never felt like a limitation. Because the content you see isn't meant to be immersive, it never feels like what's on the display is being cut off or like you have to adjust where you're looking to properly see it. The display itself has three main menus: an app launcher, a kind of home screen where you can access Meta AI and view notifications and a settings page for adjusting brightness, volume and other preferences. The display is in the right lens. Karissa Bell for Engadget For now, there are only a handful of Meta-created \"apps\" available. You can check your Instagram, WhatsApp and Messenger inboxes and chat with Meta AI. There's also a simple maps app for walking navigation, a music/audio player, camera and live translation and captioning features. There's also a mini puzzle game called \"Hypertrail.\" One of my favorite integrations was the ability to check Instagram DMs. Not only can you quickly read and respond to messages, you can watch Reels sent by your friends. While the video quality isn't as high as what you'd see on your phone, there's something very cool about quickly watching a clip without having to pull out your phone. Meta is also working on a standalone Reels experience that I'm very much looking forward to. I also enjoyed being able to view media sent in my family group chats on WhatsApp. I often would end up revisiting the photos on videos once I pulled out my phone, but being able to instantly see these messages as they came in tickled whatever part of my brain responds to instant gratification. There's some impressive tech inside those thick frames. Karissa Bell for Engadget The display also solves one of my biggest complaints with Meta's other smart glasses: that it's really difficult to frame photos. When you open the camera app on the display model, you can see a preview of the photo and even use a gesture to zoom in to properly frame your shot. Similarly, if you're on a WhatsApp video call you can see both the other person's video as well as a small preview of your own like you would on your phone's screen. It's a cool trick but the small display felt too cramped for a proper video call. People I used this with also told me that my video feed had some quality issues despite being on Wi-Fi. The glasses' live captioning and translation features are probably the best examples of Meta bringing its existing AI features into the display. I've written before about how Meta AI's translation abilities are one of my favorite features of the Ray-Ban smart glasses. Live translation on the display is even better, because it delivers a real-time text feed of what the person in front of you is saying. I tried it out with my husband, a native Spanish speaker, and it was even more natural than the non-display glasses because I didn't have to pause and wait for the audio to relay what he was saying. It still wasn't an exactly perfect translation, and there were still a few occasions when it didn't catch everything he said, but it made the process so much simpler overall. Likewise, live captions transcribes conversations in real-time into a similar text feed. I've found that it's a cool way to demo these glasses' capabilities, but I haven't yet found an occasion to use this in anything other than a demo. However, I still think it could be useful as an accessibility aid for anyone who has trouble hearing or processing audio. Another feature that's useful for travel is walking navigation. Dictate an address or location (you can say something like \"take me to the closest Starbucks\") and the glasses' display will guide you on your route. The first time I tried this was the roughly 10-minute walk from my bus stop to Yahoo's San Francisco office. The route only required two turns, but it didn't quite work. My glasses confidently navigated me to an alleyway behind the office building rather than the entrance. These kinds of mishaps happen with lots of mapping tools — Meta's maps rely on data from OpenStreetMap and Overture — but it was a good reminder that it's still early days for this product. I don't use Meta AI a ton on any of my smart glasses, but having a bit of visual feedback for these interactions was a nice change. I retain information much better from reading than listening, so seeing text-based output to my queries felt a lot more helpful. It's also nice that for longer responses from the assistant, you can stop the audio playback and swipe through informational cards instead. Meta AI on the glasses' display delivers information in a card-like interface. Meta While cooking dinner one night, I asked for a quick recipe for teriyaki salmon and Meta AI supplied what seemed like a passable recipe onto the display. The only drawback was the display goes to sleep pretty quickly unless you continue to interact with the content you're seeing, so the recipe I liked disappeared before I could actually attempt it. (You can view your Meta AI history in the Meta AI app if you really want to revisit something.) My main complaint is that I want to be able to do much more with the display. Messaging app integrations are nice, but I wish the display worked with more of the apps on my phone. When it worked best, I was happy to be able to view and dismiss messaging notifications without having to touch my phone; I just wish it worked with all my phone's notifications. There are also some frustrating limitations on sending and receiving texts. For example, there's no simple way to take a photo on your glasses and text it to a friend with the glasses. You have to wait for the glasses to send a \"preview\" of your message to your phone and then manually send the text. Or, you can opt in to Meta's cloud services and send the photo immediately as a link, but I'm not sure many of my friends would readily open a \"media.meta.com\" URL. The glasses also don't really support non-WhatsApp group chats, at least on iOS. You can receive messages sent in group chats, but there's no indication the message originated in a group thread. And, it's impossible to reply in the same thread; instead, replies are sent directly to the person who texted, which can get confusing if you're not checking your phone. It was also a little annoying that reading and even replying to texts from my glasses wouldn't mark the text as read in my phone's inbox. Meta blames all this on Apple's iOS restrictions, and says it's hoping to work with the company to improve the experience. The company tells me that group messaging should work normally for people with Android devices and that there is also a dedicated inbox for checking texts on the glasses. I haven’t tested this out yet. The band + battery life The glasses are controlled using Meta's Neural Band, which can translate subtle gestures like finger taps into actions on the display. Because the band relies on electromyography (EMG), you do need a fairly snug fit for it to work properly. I didn't find it uncomfortable, but, like the glasses, I don't love how it looks as a daily accessory. It also requires daily charging if you wear the glasses all day. But the band does work surprisingly well. In more than a week, it almost never missed a gesture, and it never falsely registered a gesture, despite my efforts to confuse it by fidgeting or rubbing my fingers together. The gestures themselves are also pretty intuitive and don't take long to get used to: double tapping your thumb and middle fingers wakes up or puts the display to sleep, single taps of your index and middle fingers allow you to select an item or go back, and swiping your thumb along the side of your index finger lets you navigate around the display. There are a few others, but those are the ones I used most often. The Meta Neural Band requires a snug fit to work properly. Karissa Bell for Engadget Each time you make a gesture, the band emits a small vibration so you get a bit of haptic feedback letting you know it registered. I've used hand tracking-based navigation in various VR, AR and mixed reality devices and I've always felt a bit goofy waving my hands around. But the neural band gestures work when your hand is by your side or in your pocket. The other major drawback of these glasses is that heavy use of the display drains the battery pretty quickly. Meta says the Ray-Ban Display’s battery can go about six hours on a single charge, but it really depends on how much you're using the display. With very limited use, l was able to stretch the battery to about seven hours, but if you're doing display-intensive tasks like video calling or live translation, it will die much, much more quickly. The Meta Ray-Ban Display glasses, charging case and neural band. Karissa Bell for Engadget The glasses do come with a charging case that can deliver a few extra charges on-the-go, but I was a bit surprised at how often I had to recharge the case. With my normal Ray-Ban Meta glasses I can go several days without topping up the charging case, but with the Meta Ray-Ban Display case, I'm charging it at least every other day. Privacy and safety Whenever I write or post on social media about a pair of Meta-branded glasses, I inevitably hear from people concerned about the privacy implications of these devices. As I wrote in my recent review of Meta's second-gen Ray-Ban glasses, I share a lot of these concerns. Meta has made subtle but meaningful changes to its glasses' privacy policy over the last year, and its track record suggests these devices will inevitably scoop up more of our data over time. In terms of privacy implications of the display-enabled glasses, there isn't a meaningful difference compared to their counterparts. Meta's policies are the same for all its wearables. I suppose you could use live translation to surreptitiously eavesdrop on a conversation you wouldn't typically understand, though that's technically possible with Meta's other glasses too. And the addition of a wrist-based controller means taking photos is a bit less obvious, but there's still an LED indicator that lights up when the camera is on. The neural band allows you to snap photos without touching the capture button or using a voice command. Karissa Bell for Engadget I have been surprised at how many people have asked me if these glasses have some kind of facial recognition abilities. I'm not sure if that's a sign of people's general distrust of Meta, or an assumption based on seeing similar glasses in sci-fi flicks, but I do think it's telling. (They don't, to be clear. Meta currently only uses facial recognition for two safety-related features on Facebook and Instagram.) Meta hasn't done much to earn people's trust when it comes to privacy, and I wish the company would use its growing wearables business to try to prove otherwise. On a more practical level, I have some safety concerns. The display didn't hinder my situational awareness while walking, but I could see how it might for others. And I'm definitely not comfortable using the display while driving. Meta does have an audio-only \"driving detection\" setting that can automatically kick in when you're traveling in a car, but the feature is optional, which seems potentially problematic. Should you buy these? In short: probably not. As much as I've been genuinely impressed with Meta's display tech, I don't think these glasses make sense for most people right now. And, at $800, the Meta Ray-Ban Display glasses are more than twice as much as the company's very good second-generation Ray-Ban glasses, which come in a wide range of much more normal-looking frame styles and colors. The Meta Ray-Ban Display glasses, on the other hand, still look very much like a first-gen product. There are some really compelling use cases for the display, but its functionality is limited. The glasses are also too thick and bulky for what's meant to be an everyday accessory. At the end of the day, most people want glasses that make them look good. There’s also the fact that right now, these glasses are somewhat difficult to actually buy. They are only available at a handful of physical retailers, which currently have a very limited supply, Meta is also requiring would-be buyers to schedule demo appointments in order to buy, though some stores — like the LensCrafters where I bought my pair — aren’t enforcing this. Still, there's a lot to be excited about. Watching people's reactions to trying these has been almost as much fun as using them myself. Meta also has a solid lineup of new features already in the works, including a standalone Reels app, a teleprompter and gesture-based handwriting for message replies. If you're already all-in on smart glasses or, like me, you've been patiently waiting for glasses with a high quality, usable display, then the Meta Ray-Ban Display glasses are worth the investment now — as long as you can accept the thick frames. Update, October 17, 2025, 3:42PM PT: Added more information about group text functionality on Android. This article originally appeared on Engadget at https://www.engadget.com/wearables/meta-ray-ban-display-review-chunky-frames-with-impressive-abilities-193127070.html?src=rss",
          "content": "I've been wearing the $800 Meta Ray-Ban Display glasses daily for ten days and I'm still a bit conflicted. On one hand, I'm still not entirely comfortable with how they look. I've worn them on the bus, at the office, on walks around my neighborhood and during hangouts with friends. Each time, I'm very aware that I probably look a bit strange. On the other hand, there's a lot I really like about using these glasses. The built-in display has helped me look at my phone less throughout the day. The neural band feels more innovative than any wrist-based device I've tried. Together, it feels like a significant milestone for smart glasses overall. But it's also very much a first-generation device with some issues that still need to be worked out. Chunky statement glasses or hideously nerdy? To once again state the obvious: The frames are extremely chunky and too wide for my face. The dark black frames I tried for this review unfortunately accentuate the extra thickness. I won't pretend it's my best look and I did feel a bit self-conscious at times wearing these in public. Meta also makes a light brown \"sand\" color that I tried at the Connect event, and I think that color is a bit more flattering, even if the frames are just as oversized. (Sidenote: Smart glasses companies, please, please make your frames available in something other than black!) But, everyone has a different face shape, skin tone and general ability to \"pull off\" what one of my friends charitably described as \"chunky statement glasses.\" What looks not-great on my face, may look good on someone else. I really wish Meta could have squeezed this tech into slightly smaller frames, but I did get more used to the look the more I wore them. Overall, I do think the size is a reasonable tradeoff for a first-generation product that's pretty clearly aimed at early adopters. Here's how they look in the lighter \"sand\" color. Karissa Bell for Engadget The reason the glasses are so thick compared with Meta's other frames is because there are a lot of extra components to power the display, including a mini projector and waveguide. And, at 69 grams, the display glasses are noticeably heavier. I didn't find it particularly uncomfortable at first, but there is a noticeable pressure after six or seven hours of wear. Plus, the extra weight and width also made them consistently slide down my nose. I'm not sure I'd feel comfortable wearing these on a bike ride or a jog as I'd worry about them falling off. While I tested these, I was very interested to get reactions from friends and family. I didn't get many positive comments about how they looked on my face, though a few particularly generous colleagues assured me I was \"pulling them off.\" But seeing people's reactions as soon as the display activated was another matter. Almost everyone has had the same initial reaction: \"whoa.\" Quality display with some limitations As I discussed in my initial impressions, these glasses have a monocular display on the right side, so it doesn't offer the kind of immersive AR I experienced with the Orion prototype last year. You have to look slightly up and to the right to focus on the full-color display. It's impressively bright and clear, but doesn't overtake your vision. At 20 degrees, the field of view is small, but it never felt like a limitation. Because the content you see isn't meant to be immersive, it never feels like what's on the display is being cut off or like you have to adjust where you're looking to properly see it. The display itself has three main menus: an app launcher, a kind of home screen where you can access Meta AI and view notifications and a settings page for adjusting brightness, volume and other preferences. The display is in the right lens. Karissa Bell for Engadget For now, there are only a handful of Meta-created \"apps\" available. You can check your Instagram, WhatsApp and Messenger inboxes and chat with Meta AI. There's also a simple maps app for walking navigation, a music/audio player, camera and live translation and captioning features. There's also a mini puzzle game called \"Hypertrail.\" One of my favorite integrations was the ability to check Instagram DMs. Not only can you quickly read and respond to messages, you can watch Reels sent by your friends. While the video quality isn't as high as what you'd see on your phone, there's something very cool about quickly watching a clip without having to pull out your phone. Meta is also working on a standalone Reels experience that I'm very much looking forward to. I also enjoyed being able to view media sent in my family group chats on WhatsApp. I often would end up revisiting the photos on videos once I pulled out my phone, but being able to instantly see these messages as they came in tickled whatever part of my brain responds to instant gratification. There's some impressive tech inside those thick frames. Karissa Bell for Engadget The display also solves one of my biggest complaints with Meta's other smart glasses: that it's really difficult to frame photos. When you open the camera app on the display model, you can see a preview of the photo and even use a gesture to zoom in to properly frame your shot. Similarly, if you're on a WhatsApp video call you can see both the other person's video as well as a small preview of your own like you would on your phone's screen. It's a cool trick but the small display felt too cramped for a proper video call. People I used this with also told me that my video feed had some quality issues despite being on Wi-Fi. The glasses' live captioning and translation features are probably the best examples of Meta bringing its existing AI features into the display. I've written before about how Meta AI's translation abilities are one of my favorite features of the Ray-Ban smart glasses. Live translation on the display is even better, because it delivers a real-time text feed of what the person in front of you is saying. I tried it out with my husband, a native Spanish speaker, and it was even more natural than the non-display glasses because I didn't have to pause and wait for the audio to relay what he was saying. It still wasn't an exactly perfect translation, and there were still a few occasions when it didn't catch everything he said, but it made the process so much simpler overall. Likewise, live captions transcribes conversations in real-time into a similar text feed. I've found that it's a cool way to demo these glasses' capabilities, but I haven't yet found an occasion to use this in anything other than a demo. However, I still think it could be useful as an accessibility aid for anyone who has trouble hearing or processing audio. Another feature that's useful for travel is walking navigation. Dictate an address or location (you can say something like \"take me to the closest Starbucks\") and the glasses' display will guide you on your route. The first time I tried this was the roughly 10-minute walk from my bus stop to Yahoo's San Francisco office. The route only required two turns, but it didn't quite work. My glasses confidently navigated me to an alleyway behind the office building rather than the entrance. These kinds of mishaps happen with lots of mapping tools — Meta's maps rely on data from OpenStreetMap and Overture — but it was a good reminder that it's still early days for this product. I don't use Meta AI a ton on any of my smart glasses, but having a bit of visual feedback for these interactions was a nice change. I retain information much better from reading than listening, so seeing text-based output to my queries felt a lot more helpful. It's also nice that for longer responses from the assistant, you can stop the audio playback and swipe through informational cards instead. Meta AI on the glasses' display delivers information in a card-like interface. Meta While cooking dinner one night, I asked for a quick recipe for teriyaki salmon and Meta AI supplied what seemed like a passable recipe onto the display. The only drawback was the display goes to sleep pretty quickly unless you continue to interact with the content you're seeing, so the recipe I liked disappeared before I could actually attempt it. (You can view your Meta AI history in the Meta AI app if you really want to revisit something.) My main complaint is that I want to be able to do much more with the display. Messaging app integrations are nice, but I wish the display worked with more of the apps on my phone. When it worked best, I was happy to be able to view and dismiss messaging notifications without having to touch my phone; I just wish it worked with all my phone's notifications. There are also some frustrating limitations on sending and receiving texts. For example, there's no simple way to take a photo on your glasses and text it to a friend with the glasses. You have to wait for the glasses to send a \"preview\" of your message to your phone and then manually send the text. Or, you can opt in to Meta's cloud services and send the photo immediately as a link, but I'm not sure many of my friends would readily open a \"media.meta.com\" URL. The glasses also don't really support non-WhatsApp group chats, at least on iOS. You can receive messages sent in group chats, but there's no indication the message originated in a group thread. And, it's impossible to reply in the same thread; instead, replies are sent directly to the person who texted, which can get confusing if you're not checking your phone. It was also a little annoying that reading and even replying to texts from my glasses wouldn't mark the text as read in my phone's inbox. Meta blames all this on Apple's iOS restrictions, and says it's hoping to work with the company to improve the experience. The company tells me that group messaging should work normally for people with Android devices and that there is also a dedicated inbox for checking texts on the glasses. I haven’t tested this out yet. The band + battery life The glasses are controlled using Meta's Neural Band, which can translate subtle gestures like finger taps into actions on the display. Because the band relies on electromyography (EMG), you do need a fairly snug fit for it to work properly. I didn't find it uncomfortable, but, like the glasses, I don't love how it looks as a daily accessory. It also requires daily charging if you wear the glasses all day. But the band does work surprisingly well. In more than a week, it almost never missed a gesture, and it never falsely registered a gesture, despite my efforts to confuse it by fidgeting or rubbing my fingers together. The gestures themselves are also pretty intuitive and don't take long to get used to: double tapping your thumb and middle fingers wakes up or puts the display to sleep, single taps of your index and middle fingers allow you to select an item or go back, and swiping your thumb along the side of your index finger lets you navigate around the display. There are a few others, but those are the ones I used most often. The Meta Neural Band requires a snug fit to work properly. Karissa Bell for Engadget Each time you make a gesture, the band emits a small vibration so you get a bit of haptic feedback letting you know it registered. I've used hand tracking-based navigation in various VR, AR and mixed reality devices and I've always felt a bit goofy waving my hands around. But the neural band gestures work when your hand is by your side or in your pocket. The other major drawback of these glasses is that heavy use of the display drains the battery pretty quickly. Meta says the Ray-Ban Display’s battery can go about six hours on a single charge, but it really depends on how much you're using the display. With very limited use, l was able to stretch the battery to about seven hours, but if you're doing display-intensive tasks like video calling or live translation, it will die much, much more quickly. The Meta Ray-Ban Display glasses, charging case and neural band. Karissa Bell for Engadget The glasses do come with a charging case that can deliver a few extra charges on-the-go, but I was a bit surprised at how often I had to recharge the case. With my normal Ray-Ban Meta glasses I can go several days without topping up the charging case, but with the Meta Ray-Ban Display case, I'm charging it at least every other day. Privacy and safety Whenever I write or post on social media about a pair of Meta-branded glasses, I inevitably hear from people concerned about the privacy implications of these devices. As I wrote in my recent review of Meta's second-gen Ray-Ban glasses, I share a lot of these concerns. Meta has made subtle but meaningful changes to its glasses' privacy policy over the last year, and its track record suggests these devices will inevitably scoop up more of our data over time. In terms of privacy implications of the display-enabled glasses, there isn't a meaningful difference compared to their counterparts. Meta's policies are the same for all its wearables. I suppose you could use live translation to surreptitiously eavesdrop on a conversation you wouldn't typically understand, though that's technically possible with Meta's other glasses too. And the addition of a wrist-based controller means taking photos is a bit less obvious, but there's still an LED indicator that lights up when the camera is on. The neural band allows you to snap photos without touching the capture button or using a voice command. Karissa Bell for Engadget I have been surprised at how many people have asked me if these glasses have some kind of facial recognition abilities. I'm not sure if that's a sign of people's general distrust of Meta, or an assumption based on seeing similar glasses in sci-fi flicks, but I do think it's telling. (They don't, to be clear. Meta currently only uses facial recognition for two safety-related features on Facebook and Instagram.) Meta hasn't done much to earn people's trust when it comes to privacy, and I wish the company would use its growing wearables business to try to prove otherwise. On a more practical level, I have some safety concerns. The display didn't hinder my situational awareness while walking, but I could see how it might for others. And I'm definitely not comfortable using the display while driving. Meta does have an audio-only \"driving detection\" setting that can automatically kick in when you're traveling in a car, but the feature is optional, which seems potentially problematic. Should you buy these? In short: probably not. As much as I've been genuinely impressed with Meta's display tech, I don't think these glasses make sense for most people right now. And, at $800, the Meta Ray-Ban Display glasses are more than twice as much as the company's very good second-generation Ray-Ban glasses, which come in a wide range of much more normal-looking frame styles and colors. The Meta Ray-Ban Display glasses, on the other hand, still look very much like a first-gen product. There are some really compelling use cases for the display, but its functionality is limited. The glasses are also too thick and bulky for what's meant to be an everyday accessory. At the end of the day, most people want glasses that make them look good. There’s also the fact that right now, these glasses are somewhat difficult to actually buy. They are only available at a handful of physical retailers, which currently have a very limited supply, Meta is also requiring would-be buyers to schedule demo appointments in order to buy, though some stores — like the LensCrafters where I bought my pair — aren’t enforcing this. Still, there's a lot to be excited about. Watching people's reactions to trying these has been almost as much fun as using them myself. Meta also has a solid lineup of new features already in the works, including a standalone Reels app, a teleprompter and gesture-based handwriting for message replies. If you're already all-in on smart glasses or, like me, you've been patiently waiting for glasses with a high quality, usable display, then the Meta Ray-Ban Display glasses are worth the investment now — as long as you can accept the thick frames. Update, October 17, 2025, 3:42PM PT: Added more information about group text functionality on Android. This article originally appeared on Engadget at https://www.engadget.com/wearables/meta-ray-ban-display-review-chunky-frames-with-impressive-abilities-193127070.html?src=rss",
          "feed_position": 15,
          "image_url": "https://d29szjachogqwa.cloudfront.net/videos/user-uploaded/meta_display_sand_on_kb.jpg"
        }
      ],
      "featured_image": "https://images.ctfassets.net/jdtwqhzvc2n1/3LM1cCj4dJrbJGr6K5iiIe/b18688f1a0faf3befb7be85063da4801/u7277289442_A_sophisticated_AI_agent_is_standing_at_the_front_a862de2f-78d6-4330-8b28-8957b27aa9e0_0.png",
      "popularity_score": 2013.76231,
      "ai_summary": [
        "Engadget reviewed new Pixel devices, including the Pixel 10 Pro Fold.",
        "The Pixel 10 Pro Fold addresses durability issues in foldable phones.",
        "Meta Ray-Ban Display smart glasses are the second generation.",
        "The ASUS ROG Xbox Ally X is a gaming handheld with Xbox controls.",
        "The reviews cover a range of new tech devices released recently."
      ]
    },
    {
      "id": "cluster_1",
      "coverage": 1,
      "updated_at": "Sun, 19 Oct 2025 18:50:10 +0000",
      "title": "Something from ‘space’ may have just struck a United Airlines flight over Utah",
      "neutral_headline": "Something from ‘space’ may have just struck a United Airlines flight over Utah",
      "items": [
        {
          "source": "Ars Technica Main",
          "url": "https://arstechnica.com/space/2025/10/something-from-space-may-have-just-struck-a-united-airlines-flight-over-utah/",
          "published_at": "Sun, 19 Oct 2025 18:50:10 +0000",
          "title": "Something from ‘space’ may have just struck a United Airlines flight over Utah",
          "standfirst": "\"NTSB gathering radar, weather, flight recorder data.\"",
          "content": "The National Transportation Safety Board confirmed Sunday that it is investigating an airliner that was struck by an object in its windscreen, mid-flight, over Utah. “NTSB gathering radar, weather, flight recorder data,” the federal agency said on the social media site X. “Windscreen being sent to NTSB laboratories for examination.” The strike occurred Thursday, during a United Airlines flight from Denver to Los Angeles. Images shared on social media showed that one of the two large windows at the front of a 737 MAX aircraft was significantly cracked. Related images also reveal a pilot’s arm that has been cut multiple times by what appear to be small shards of glass.Read full article Comments",
          "feed_position": 0,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2019/03/Boeing_737_MAX_7-1-1-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2019/03/Boeing_737_MAX_7-1-1-1152x648.jpg",
      "popularity_score": 375.5984211111111,
      "ai_summary": [
        "An object from space may have struck a United Airlines flight.",
        "The incident occurred over Utah.",
        "The NTSB is gathering data on the event.",
        "Radar, weather, and flight recorder data are being collected.",
        "The investigation is ongoing to determine the cause."
      ]
    },
    {
      "id": "cluster_72",
      "coverage": 1,
      "updated_at": "Sat, 18 Oct 2025 12:15:59 +0000",
      "title": "Roberta Williams’ The Colonel’s Bequest was a different type of adventure game",
      "neutral_headline": "Roberta Williams’ The Colonel’s Bequest was a different type of adventure game",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gaming/2025/10/roberta-williams-the-colonels-bequest-was-a-different-type-of-adventure-game/",
          "published_at": "Sat, 18 Oct 2025 12:15:59 +0000",
          "title": "Roberta Williams’ The Colonel’s Bequest was a different type of adventure game",
          "standfirst": "What if point-and-click games weren't about the puzzles?",
          "content": "Even in my youth, I always loved the idea of point-and-click adventure games more than I did the reality. I appreciated how they transported me to other worlds, each with its own rules, histories, and interesting characters. However, like many people, I often ran up against the harsh reality of solving bizarre and obtuse puzzles in a time before Internet walkthroughs. I almost never actually finished point-and-click adventure games for that reason—but there is one major exception: I completed Roberta Williams’ The Colonel’s Bequest several times. One of the last Sierra adventure games to still use a text parser, The Colonel’s Bequest follows a young woman named Laura Bow as she visits a mansion in the Southern US belonging to her college friend’s grandfather, Colonel Henri Dijon. While she’s there, a dispute breaks out over the colonel’s will, and it becomes clear a murderer is on the loose.Read full article Comments",
          "feed_position": 0,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/Colonels-Bequest-1-1152x648-1760651707.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/Colonels-Bequest-1-1152x648-1760651707.jpg",
      "popularity_score": 333,
      "ai_summary": [
        "The article discusses Roberta Williams' game, The Colonel's Bequest.",
        "The game is described as a different type of adventure game.",
        "It challenges the typical focus on puzzles in the genre.",
        "The game's approach to adventure is being examined.",
        "The article explores the game's unique qualities."
      ]
    },
    {
      "id": "cluster_93",
      "coverage": 1,
      "updated_at": "Fri, 17 Oct 2025 22:14:59 +0000",
      "title": "With deadline looming, 4 of 9 universities reject Trump’s “compact” to remake higher ed",
      "neutral_headline": "With deadline looming, 4 of 9 universities reject Trump’s “compact” to remake higher ed",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/culture/2025/10/with-deadline-looming-4-of-9-universities-reject-trumps-compact-to-remake-higher-ed/",
          "published_at": "Fri, 17 Oct 2025 22:14:59 +0000",
          "title": "With deadline looming, 4 of 9 universities reject Trump’s “compact” to remake higher ed",
          "standfirst": "But Trump is pressuring the other five.",
          "content": "Earlier this month, the Trump administration made nine elite universities an offer they couldn’t refuse: bring in more conservatives while shutting down “institutional units that purposefully punish, belittle, and even spark violence against conservative ideas,” give up control of admissions and hiring decisions, agree to “biological” definitions of sex and gender, don’t raise tuition for five years, clamp down on student protests, and stay institutionally “neutral” on current events. Do this and you won’t be cut off from “federal benefits,” which could include research funding, student loans, federal contracts, and even student and faculty immigration visas. Instead, you may gain “substantial and meaningful federal grants.” But the universities are refusing. With the initial deadline of October 20 approaching, four of the nine universities—the University of Pennsylvania, Brown, University of Southern California, and MIT—that received the federal “compact” have announced that they will not sign it. In addition, the American Council on Education, which represents more than 1,600 colleges and universities, today issued a statement calling for the compact to be completely withdrawn.Read full article Comments",
          "feed_position": 1,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/05/GettyImages-588709290-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/05/GettyImages-588709290-1152x648.jpg",
      "popularity_score": 323,
      "ai_summary": [
        "Four out of nine universities rejected Trump's \"compact.\"",
        "The compact aims to remake higher education.",
        "A deadline is approaching for the universities.",
        "Trump is pressuring the remaining five universities.",
        "The article discusses the resistance to the compact."
      ]
    },
    {
      "id": "cluster_94",
      "coverage": 1,
      "updated_at": "Fri, 17 Oct 2025 22:02:54 +0000",
      "title": "Vaginal condition treatment update: Men should get treated, too",
      "neutral_headline": "Vaginal condition treatment update: Men should get treated, too",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/health/2025/10/vaginal-condition-treatment-update-men-should-get-treated-too/",
          "published_at": "Fri, 17 Oct 2025 22:02:54 +0000",
          "title": "Vaginal condition treatment update: Men should get treated, too",
          "standfirst": "For bacterial vaginosis, partners are part of the problem—and the solution.",
          "content": "For some cases of bacterial vaginosis, treatment should include a package deal, doctors now say. The American College of Obstetricians & Gynecologists (ACOG) updated its clinical guidance Friday to fit with recent data indicating that treatment for recurring bacterial vaginosis (BV) in women is significantly more effective if their male partners are also treated at the same time—with both an oral antibiotic and an antibiotic cream directly onto the potentially offending member. “Partner therapy offers us another avenue for hopefully preventing recurrence and helping people feel better faster,” Christopher Zahn, chief of clinical practice and health equity and quality at ACOG, said in a statement.Read full article Comments",
          "feed_position": 2,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2185753669-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2185753669-1152x648.jpg",
      "popularity_score": 313,
      "ai_summary": [
        "The article discusses treatment for a vaginal condition.",
        "It emphasizes that partners, including men, should be treated.",
        "Bacterial vaginosis is the focus of the discussion.",
        "The article highlights the importance of partner treatment.",
        "The solution involves treating both partners."
      ]
    },
    {
      "id": "cluster_95",
      "coverage": 1,
      "updated_at": "Fri, 17 Oct 2025 21:50:10 +0000",
      "title": "Ring cameras are about to get increasingly chummy with law enforcement",
      "neutral_headline": "Ring cameras are about to get increasingly chummy with law enforcement",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2025/10/ring-cameras-are-about-to-get-increasingly-chummy-with-law-enforcement/",
          "published_at": "Fri, 17 Oct 2025 21:50:10 +0000",
          "title": "Ring cameras are about to get increasingly chummy with law enforcement",
          "standfirst": "Amazon's Ring partners with company whose tech has reportedly been used by ICE.",
          "content": "Law enforcement agencies will soon have easier access to footage captured by Amazon’s Ring smart cameras. In a partnership announced this week, Amazon will allow approximately 5,000 local law enforcement agencies to request access to Ring camera footage via surveillance platforms from Flock Safety. Ring cooperating with law enforcement and the reported use of Flock technologies by federal agencies, including US Immigration and Customs Enforcement (ICE), has resurfaced privacy concerns that have followed the devices for years. According to Flock’s announcement, its Ring partnership allows local law enforcement members to use Flock software “to send a direct post in the Ring Neighbors app with details about the investigation and request voluntary assistance.” Requests must include “specific location and timeframe of the incident, a unique investigation code, and details about what is being investigated,” and users can look at the requests anonymously, Flock said. “Any footage a Ring customer chooses to submit will be securely packaged by Flock and shared directly with the requesting local public safety agency through the FlockOS or Flock Nova platform,” the announcement reads.Read full article Comments",
          "feed_position": 3,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/20250611-lifestyle-outdoorcampro-plugin-home-exterior-wall-wht-rgb-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/20250611-lifestyle-outdoorcampro-plugin-home-exterior-wall-wht-rgb-1152x648.jpg",
      "popularity_score": 303,
      "ai_summary": [
        "Amazon's Ring cameras are partnering with a company.",
        "The company's tech has reportedly been used by ICE.",
        "The partnership raises concerns about law enforcement.",
        "The article discusses the implications of the partnership.",
        "The focus is on the relationship between Ring and law enforcement."
      ]
    }
  ]
}