{
  "updated_at": "2026-02-04T07:44:12.226Z",
  "clusters": [
    {
      "id": "cluster_34",
      "coverage": 2,
      "updated_at": "Tue, 03 Feb 2026 22:47:00 GMT",
      "title": "Qwen3-Coder-Next offers vibe coders a powerful open source, ultra-sparse model with 10x higher throughput for repo tasks",
      "neutral_headline": "Qwen3 Coder Next Offers Powerful Open Source Model",
      "items": [
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/technology/qwen3-coder-next-offers-vibe-coders-a-powerful-open-source-ultra-sparse",
          "published_at": "Tue, 03 Feb 2026 22:47:00 GMT",
          "title": "Qwen3-Coder-Next offers vibe coders a powerful open source, ultra-sparse model with 10x higher throughput for repo tasks",
          "standfirst": "Chinese e-commerce giant Alibaba&#x27;s Qwen team of AI researchers has emerged in the last year as one of the global leaders of open source AI development, releasing a host of powerful large language models and specialized multimodal models that approach, and in some cases, surpass the performance of the proprietary U.S. leaders such as OpenAI, Anthropic, Google and xAI.Now the Qwen team is back again this week with a compelling release that matches the \"vibe coding\" frenzy that has arisen in recent months: Qwen3-Coder-Next, a specialized 80-billion-parameter model designed to deliver elite agentic performance within a lightweight active footprint. It&#x27;s been released on a permissive Apache 2.0 license, enabling commercial usage by large enterprises and indie developers alike, with the model weights available on Hugging Face in four variants and a technical report describing some of its training approach and innovations. The release marks a major escalation in the global arms race for the ultimate coding assistant, following a week that has seen the space explode with new entrants. From the massive efficiency gains of Anthropic’s Claude Code harness to the high-profile launch of the OpenAI Codex app and the rapid community adoption of open-source frameworks like OpenClaw, the competitive landscape has never been more crowded. In this high-stakes environment, Alibaba isn&#x27;t just keeping pace — it is attempting to set a new standard for open-weight intelligence.For LLM decision-makers, Qwen3-Coder-Next represents a fundamental shift in the economics of AI engineering. While the model houses 80 billion total parameters, it utilizes an ultra-sparse Mixture-of-Experts (MoE) architecture that activates only 3 billion parameters per forward pass. This design allows it to deliver reasoning capabilities that rival massive proprietary systems while maintaining the low deployment costs and high throughput of a lightweight local model.Solving the long-context bottleneckThe core technical breakthrough behind Qwen3-Coder-Next is a hybrid architecture designed specifically to circumvent the quadratic scaling issues that plague traditional Transformers. As context windows expand — and this model supports a massive 262,144 tokens — traditional attention mechanisms become computationally prohibitive. Standard Transformers suffer from a \"memory wall\" where the cost of processing context grows quadratically with sequence length. Qwen addresses this by combining Gated DeltaNet with Gated Attention.Gated DeltaNet acts as a linear-complexity alternative to standard softmax attention. It allows the model to maintain state across its quarter-million-token window without the exponential latency penalties typical of long-horizon reasoning. When paired with the ultra-sparse MoE, the result is a theoretical 10x higher throughput for repository-level tasks compared to dense models of similar total capacity. This architecture ensures an agent can \"read\" an entire Python library or complex JavaScript framework and respond with the speed of a 3B model, yet with the structural understanding of an 80B system. To prevent context hallucination during training, the team utilized Best-Fit Packing (BFP), a strategy that maintains efficiency without the truncation errors found in traditional document concatenation.Trained to be agent-firstThe \"Next\" in the model&#x27;s nomenclature refers to a fundamental pivot in training methodology. Historically, coding models were trained on static code-text pairs—essentially a \"read-only\" education. Qwen3-Coder-Next was instead developed through a massive \"agentic training\" pipeline. The technical report details a synthesis pipeline that produced 800,000 verifiable coding tasks. These were not mere snippets; they were real-world bug-fixing scenarios mined from GitHub pull requests and paired with fully executable environments.The training infrastructure, known as MegaFlow, is a cloud-native orchestration system based on Alibaba Cloud Kubernetes. In MegaFlow, each agentic task is expressed as a three-stage workflow: agent rollout, evaluation, and post-processing. During rollout, the model interacts with a live containerized environment. If it generates code that fails a unit test or crashes a container, it receives immediate feedback through mid-training and reinforcement learning. This \"closed-loop\" education allows the model to learn from environment feedback, teaching it to recover from faults and refine solutions in real-time.Product specifications include:Support for 370 Programming Languages: An expansion from 92 in previous versions.XML-Style Tool Calling: A new qwen3_coder format designed for string-heavy arguments, allowing the model to emit long code snippets without the nested quoting and escaping overhead typical of JSON.Repository-Level Focus: Mid-training was expanded to approximately 600B tokens of repository-level data, proving more impactful for cross-file dependency logic than file-level datasets alone.Specialization via expert modelsA key differentiator in the Qwen3-Coder-Next pipeline is its use of specialized Expert Models. Rather than training one generalist model for all tasks, the team developed domain-specific experts for Web Development and User Experience (UX).The Web Development Expert targets full-stack tasks like UI construction and component composition. All code samples were rendered in a Playwright-controlled Chromium environment. For React samples, a Vite server was deployed to ensure all dependencies were correctly initialized. A Vision-Language Model (VLM) then judged the rendered pages for layout integrity and UI quality.The User Experience Expert was optimized for tool-call format adherence across diverse CLI/IDE scaffolds such as Cline and OpenCode. The team found that training on diverse tool chat templates significantly improved the model&#x27;s robustness to unseen schemas at deployment time. Once these experts achieved peak performance, their capabilities were distilled back into the single 80B/3B MoE model. This ensures the lightweight deployment version retains the nuanced knowledge of much larger teacher models.Punching up on benchmarks while offering high securityThe results of this specialized training are evident in the model&#x27;s competitive standing against industry giants. In benchmark evaluations conducted using the SWE-Agent scaffold, Qwen3-Coder-Next demonstrated exceptional efficiency relative to its active parameter count. On SWE-Bench Verified, the model achieved a score of 70.6%. This performance is notably competitive when placed alongside significantly larger models; it outpaces DeepSeek-V3.2, which scores 70.2%, and trails only slightly behind the 74.2% score of GLM-4.7.Crucially, the model demonstrates robust inherent security awareness. On SecCodeBench, which evaluates a model&#x27;s ability to repair vulnerabilities, Qwen3-Coder-Next outperformed Claude-Opus-4.5 in code generation scenarios (61.2% vs. 52.5%). Notably, it maintained high scores even when provided with no security hints, indicating it has learned to anticipate common security pitfalls during its 800k-task agentic training phase. In multilingual multilingual security evaluations, the model also demonstrated a competitive balance between functional and secure code generation, outperforming both DeepSeek-V3.2 and GLM-4.7 on the CWEval benchmark with a func-sec@1 score of 56.32%.Challenging the proprietary giantsThe release represents the most significant challenge to the dominance of closed-source coding models in 2026. By proving that a model with only 3B active parameters can navigate the complexities of real-world software engineering as effectively as a \"giant,\" Alibaba has effectively democratized agentic coding.The \"aha!\" moment for the industry is the realization that context length and throughput are the two most important levers for agentic success. A model that can process 262k tokens of a repository in seconds and verify its own work in a Docker container is fundamentally more useful than a larger model that is too slow or expensive to iterate. As the Qwen team concludes in their report: \"Scaling agentic training, rather than model size alone, is a key driver for advancing real-world coding agent capability\". With Qwen3-Coder-Next, the era of the \"mammoth\" coding model may be coming to an end, replaced by ultra-fast, sparse experts that can think as deeply as they can run.",
          "content": "Chinese e-commerce giant Alibaba&#x27;s Qwen team of AI researchers has emerged in the last year as one of the global leaders of open source AI development, releasing a host of powerful large language models and specialized multimodal models that approach, and in some cases, surpass the performance of the proprietary U.S. leaders such as OpenAI, Anthropic, Google and xAI.Now the Qwen team is back again this week with a compelling release that matches the \"vibe coding\" frenzy that has arisen in recent months: Qwen3-Coder-Next, a specialized 80-billion-parameter model designed to deliver elite agentic performance within a lightweight active footprint. It&#x27;s been released on a permissive Apache 2.0 license, enabling commercial usage by large enterprises and indie developers alike, with the model weights available on Hugging Face in four variants and a technical report describing some of its training approach and innovations. The release marks a major escalation in the global arms race for the ultimate coding assistant, following a week that has seen the space explode with new entrants. From the massive efficiency gains of Anthropic’s Claude Code harness to the high-profile launch of the OpenAI Codex app and the rapid community adoption of open-source frameworks like OpenClaw, the competitive landscape has never been more crowded. In this high-stakes environment, Alibaba isn&#x27;t just keeping pace — it is attempting to set a new standard for open-weight intelligence.For LLM decision-makers, Qwen3-Coder-Next represents a fundamental shift in the economics of AI engineering. While the model houses 80 billion total parameters, it utilizes an ultra-sparse Mixture-of-Experts (MoE) architecture that activates only 3 billion parameters per forward pass. This design allows it to deliver reasoning capabilities that rival massive proprietary systems while maintaining the low deployment costs and high throughput of a lightweight local model.Solving the long-context bottleneckThe core technical breakthrough behind Qwen3-Coder-Next is a hybrid architecture designed specifically to circumvent the quadratic scaling issues that plague traditional Transformers. As context windows expand — and this model supports a massive 262,144 tokens — traditional attention mechanisms become computationally prohibitive. Standard Transformers suffer from a \"memory wall\" where the cost of processing context grows quadratically with sequence length. Qwen addresses this by combining Gated DeltaNet with Gated Attention.Gated DeltaNet acts as a linear-complexity alternative to standard softmax attention. It allows the model to maintain state across its quarter-million-token window without the exponential latency penalties typical of long-horizon reasoning. When paired with the ultra-sparse MoE, the result is a theoretical 10x higher throughput for repository-level tasks compared to dense models of similar total capacity. This architecture ensures an agent can \"read\" an entire Python library or complex JavaScript framework and respond with the speed of a 3B model, yet with the structural understanding of an 80B system. To prevent context hallucination during training, the team utilized Best-Fit Packing (BFP), a strategy that maintains efficiency without the truncation errors found in traditional document concatenation.Trained to be agent-firstThe \"Next\" in the model&#x27;s nomenclature refers to a fundamental pivot in training methodology. Historically, coding models were trained on static code-text pairs—essentially a \"read-only\" education. Qwen3-Coder-Next was instead developed through a massive \"agentic training\" pipeline. The technical report details a synthesis pipeline that produced 800,000 verifiable coding tasks. These were not mere snippets; they were real-world bug-fixing scenarios mined from GitHub pull requests and paired with fully executable environments.The training infrastructure, known as MegaFlow, is a cloud-native orchestration system based on Alibaba Cloud Kubernetes. In MegaFlow, each agentic task is expressed as a three-stage workflow: agent rollout, evaluation, and post-processing. During rollout, the model interacts with a live containerized environment. If it generates code that fails a unit test or crashes a container, it receives immediate feedback through mid-training and reinforcement learning. This \"closed-loop\" education allows the model to learn from environment feedback, teaching it to recover from faults and refine solutions in real-time.Product specifications include:Support for 370 Programming Languages: An expansion from 92 in previous versions.XML-Style Tool Calling: A new qwen3_coder format designed for string-heavy arguments, allowing the model to emit long code snippets without the nested quoting and escaping overhead typical of JSON.Repository-Level Focus: Mid-training was expanded to approximately 600B tokens of repository-level data, proving more impactful for cross-file dependency logic than file-level datasets alone.Specialization via expert modelsA key differentiator in the Qwen3-Coder-Next pipeline is its use of specialized Expert Models. Rather than training one generalist model for all tasks, the team developed domain-specific experts for Web Development and User Experience (UX).The Web Development Expert targets full-stack tasks like UI construction and component composition. All code samples were rendered in a Playwright-controlled Chromium environment. For React samples, a Vite server was deployed to ensure all dependencies were correctly initialized. A Vision-Language Model (VLM) then judged the rendered pages for layout integrity and UI quality.The User Experience Expert was optimized for tool-call format adherence across diverse CLI/IDE scaffolds such as Cline and OpenCode. The team found that training on diverse tool chat templates significantly improved the model&#x27;s robustness to unseen schemas at deployment time. Once these experts achieved peak performance, their capabilities were distilled back into the single 80B/3B MoE model. This ensures the lightweight deployment version retains the nuanced knowledge of much larger teacher models.Punching up on benchmarks while offering high securityThe results of this specialized training are evident in the model&#x27;s competitive standing against industry giants. In benchmark evaluations conducted using the SWE-Agent scaffold, Qwen3-Coder-Next demonstrated exceptional efficiency relative to its active parameter count. On SWE-Bench Verified, the model achieved a score of 70.6%. This performance is notably competitive when placed alongside significantly larger models; it outpaces DeepSeek-V3.2, which scores 70.2%, and trails only slightly behind the 74.2% score of GLM-4.7.Crucially, the model demonstrates robust inherent security awareness. On SecCodeBench, which evaluates a model&#x27;s ability to repair vulnerabilities, Qwen3-Coder-Next outperformed Claude-Opus-4.5 in code generation scenarios (61.2% vs. 52.5%). Notably, it maintained high scores even when provided with no security hints, indicating it has learned to anticipate common security pitfalls during its 800k-task agentic training phase. In multilingual multilingual security evaluations, the model also demonstrated a competitive balance between functional and secure code generation, outperforming both DeepSeek-V3.2 and GLM-4.7 on the CWEval benchmark with a func-sec@1 score of 56.32%.Challenging the proprietary giantsThe release represents the most significant challenge to the dominance of closed-source coding models in 2026. By proving that a model with only 3B active parameters can navigate the complexities of real-world software engineering as effectively as a \"giant,\" Alibaba has effectively democratized agentic coding.The \"aha!\" moment for the industry is the realization that context length and throughput are the two most important levers for agentic success. A model that can process 262k tokens of a repository in seconds and verify its own work in a Docker container is fundamentally more useful than a larger model that is too slow or expensive to iterate. As the Qwen team concludes in their report: \"Scaling agentic training, rather than model size alone, is a key driver for advancing real-world coding agent capability\". With Qwen3-Coder-Next, the era of the \"mammoth\" coding model may be coming to an end, replaced by ultra-fast, sparse experts that can think as deeply as they can run.",
          "feed_position": 0,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/7ErEzHma6LbYLaznwE3rtA/39944c4739dbecaa3f8a336826de352d/sVJFWPtv0RFGdOIc_fKWG_C0uLbr6A__1_.png?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/technology/apple-integrates-anthropics-claude-and-openais-codex-into-xcode-26-3-in-push",
          "published_at": "Tue, 03 Feb 2026 20:45:00 GMT",
          "title": "Apple integrates Anthropic’s Claude and OpenAI’s Codex into Xcode 26.3 in push for ‘agentic coding’",
          "standfirst": "Apple on Tuesday announced a major update to its flagship developer tool that gives artificial intelligence agents unprecedented control over the app-building process, a move that signals the iPhone maker&#x27;s aggressive push into an emerging and controversial practice known as \"agentic coding.\"Xcode 26.3, available immediately as a release candidate, integrates Anthropic&#x27;s Claude Agent and OpenAI&#x27;s Codex directly into Apple&#x27;s development environment, allowing the AI systems to autonomously write code, build projects, run tests, and visually verify their own work — all with minimal human oversight.The update is Apple&#x27;s most significant embrace of AI-assisted software development since introducing intelligence features in Xcode 26 last year, and arrives as \"vibe coding\" — the practice of delegating software creation to large language models — has become one of the most debated topics in technology.Apple says that while integrating intelligence into the Xcode developer workflow is powerful, the model itself still has a somewhat limited aperture. It answers questions based on what the developer provides, but it doesn&#x27;t have access to the full context of the project, and it&#x27;s not able to take action on its own. That changes with this update, the company said during a press conference Tuesday morning.How Apple&#x27;s new AI coding features let developers build apps faster than everThe key innovation in Xcode 26.3 is the depth of integration between AI agents and Apple&#x27;s development tools. Unlike previous iterations that offered code suggestions and autocomplete features, the new system grants AI agents access to nearly every aspect of the development process.During a live demonstration, an Apple engineer showed how the Claude agent could receive a simple prompt — \"add a new feature to show the weather at a landmark\" — and then independently analyze the project&#x27;s file structure, consult Apple&#x27;s documentation, write the necessary code, build the project, and take screenshots of the running application to verify its work matched the requested design.According to Apple, the agent is able to use tools like build and screenshot previews to verify its work, visually analyze the image, and confirm that everything has been built accordingly. Before, when interacting with a model, it would provide an answer and just stop there.The system creates automatic checkpoints as developers interact with the AI, allowing them to roll back changes if results prove unsatisfactory — a safeguard that acknowledges the unpredictable nature of AI-generated code.Apple says it worked directly with Anthropic and OpenAI to optimize the experience with particular attention paid to reducing token usage — the computational units that determine costs when using cloud-based AI models — and improving the efficiency of tool calling.According to the company, developers can download new agents with a single click, and they update automatically.Why Apple&#x27;s adoption of the Model Context Protocol could reshape the AI development landscapeUnderlying the integration is the Model Context Protocol, or MCP, an open standard that Anthropic developed for connecting AI agents with external tools. Apple&#x27;s adoption of MCP means that any compatible agent — not just Claude or Codex — can now interact with Xcode&#x27;s capabilities.Apple says this also works for agents that are running outside of Xcode. Any agent that is compatible with MCP can now work with Xcode to do all the same things — project discovery and change management, building and testing apps, working with previews and code snippets, and accessing the latest documentation.The decision to embrace an open protocol, rather than building a proprietary system, represents a notable departure for Apple, which has historically favored closed ecosystems. It also positions Xcode as a potential hub for a growing universe of AI development tools.Xcode&#x27;s troubled history with AI tools — and why Apple says this time is differentThe announcement comes against a backdrop of mixed experiences with AI-assisted coding in Apple&#x27;s tools. During the press conference, one developer described previous attempts to use AI agents with Xcode as \"horrible,\" citing constant crashes and an inability to complete basic tasks.Apple acknowledged the concerns while arguing that the new integration addresses fundamental limitations of earlier approaches.The company says the big shift is that Claude and Codex have so much more visibility into the breadth of the project. If they hallucinate and write code that doesn&#x27;t work, they can now build, see the compile errors, and iterate in real time to fix those issues — in some cases before presenting it as a finished work.Apple argues that the power of IDE integration extends beyond error correction. Agents can now automatically add entitlements to projects when needed to access protected APIs — a task the company says would be otherwise very difficult for an AI operating outside the development environment dealing with binary files it may not have the format for.From Andrej Karpathy&#x27;s tweet to LinkedIn certifications: The unstoppable rise of vibe codingApple&#x27;s announcement arrives at a crucial moment in the evolution of AI-assisted development. The term \"vibe coding,\" coined by AI researcher Andrej Karpathy in early 2025, has transformed from a curiosity into a genuine cultural phenomenon that is reshaping how software gets built.LinkedIn announced last week that it will begin offering official certifications in AI coding skills, drawing on usage data from platforms like Lovable and Replit. Job postings requiring AI proficiency doubled in the past year, according to edX research, with Indeed&#x27;s Hiring Lab reporting that 4.2% of U.S. job listings now mention AI-related keywords.The enthusiasm is driven by genuine productivity gains. Casey Newton, the technology journalist, recently described building a complete personal website using Claude Code in about an hour — a task that previously required expensive Squarespace subscriptions and years of frustrated attempts with various website builders.More dramatically, Jaana Dogan, a principal engineer at Google, posted that she gave Claude Code \"a description of the problem\" and \"it generated what we built last year in an hour.\" Her post, which accumulated more than 8 million views, began with the disclaimer: \"I&#x27;m not joking and this isn&#x27;t funny.\"Security experts warn that AI-generated code could lead to &#x27;catastrophic explosions&#x27;But the rapid adoption of agentic coding has also sparked significant concerns among security researchers and software engineers.David Mytton, founder and CEO of developer security provider Arcjet, warned last month that the proliferation of vibe-coded applications \"into production will lead to catastrophic problems for organizations that don&#x27;t properly review AI-developed software.\"\"In 2026, I expect more and more vibe-coded applications hitting production in a big way,\" Mytton wrote. \"That&#x27;s going to be great for velocity… but you&#x27;ve still got to pay attention. There&#x27;s going to be some big explosions coming!\"Simon Willison, co-creator of the Django web framework, drew an even starker comparison. \"I think we&#x27;re due a Challenger disaster with respect to coding agent security,\" he said, referring to the 1986 space shuttle explosion that killed all seven crew members. \"So many people, myself included, are running these coding agents practically as root. We&#x27;re letting them do all of this stuff.\"A pre-print paper from researchers this week warned that vibe coding could pose existential risks to the open-source software ecosystem. The study found that AI-assisted development pulls user interaction away from community projects, reduces visits to documentation websites and forums, and makes launching new open-source initiatives significantly harder.Stack Overflow usage has plummeted as developers increasingly turn to AI chatbots for answers—a shift that could ultimately starve the very knowledge bases that trained the AI models in the first place.Previous research painted an even more troubling picture: a 2024 report found that vibe coding using tools like GitHub Copilot \"offered no real benefits unless adding 41% more bugs is a measure of success.\"The hidden mental health cost of letting AI write your codeEven enthusiastic adopters have begun acknowledging the darker aspects of AI-assisted development.Peter Steinberger, creator of the viral AI agent originally known as Clawdbot (now OpenClaw), recently revealed that he had to step back from vibe coding after it consumed his life.\"I was out with my friends and instead of joining the conversation in the restaurant, I was just like, vibe coding on my phone,\" Steinberger said in a recent podcast interview. \"I decided, OK, I have to stop this more for my mental health than for anything else.\"Steinberger warned that the constant building of increasingly powerful AI tools creates the \"illusion of making you more productive\" without necessarily advancing real goals. \"If you don&#x27;t have a vision of what you&#x27;re going to build, it&#x27;s still going to be slop,\" he added.Google CEO Sundar Pichai has expressed similar reservations, saying he won&#x27;t vibe code on \"large codebases where you really have to get it right.\"\"The security has to be there,\" Pichai said in a November podcast interview.Boris Cherny, the Anthropic engineer who created Claude Code, acknowledged that vibe coding works best for \"prototypes or throwaway code, not software that sits at the core of a business.\"\"You want maintainable code sometimes. You want to be very thoughtful about every line sometimes,\" Cherny said.Apple is gambling that deep IDE integration can make AI coding safe for productionApple appears to be betting that the benefits of deep IDE integration can mitigate many of these concerns. By giving AI agents access to build systems, test suites, and visual verification tools, the company is essentially arguing that Xcode can serve as a quality control mechanism for AI-generated code.Susan Prescott, Apple&#x27;s vice president of Worldwide Developer Relations, framed the update as part of Apple&#x27;s broader mission.In a statement, Apple said its goal is to make tools that put industry-leading technologies directly in developers&#x27; hands so they can build the very best apps. The company says agentic coding supercharges productivity and creativity, streamlining the development workflow so developers can focus on innovation.But the question remains whether the safeguards will prove sufficient as AI agents grow more autonomous. Asked about debugging capabilities, Apple noted that while Xcode has a powerful debugger built in, there is no direct MCP tool for debugging.Developers can run the debugger and manually relay information to the agent, but the AI cannot yet independently investigate runtime issues — a limitation that could prove significant as the complexity of AI-generated code increases.The update also does not currently support running multiple agents simultaneously on the same project, though Apple noted that developers can open projects in multiple Xcode windows using Git worktrees as a workaround.The future of software development hangs in the balance — and Apple just raised the stakesXcode 26.3 is available immediately as a release candidate for members of the Apple Developer Program, with a general release expected soon on the App Store. The release candidate designation — Apple&#x27;s final beta before production — means developers who download today will automatically receive the finished version when it ships.The integration supports both API keys and direct account credentials from OpenAI and Anthropic, offering developers flexibility in managing their AI subscriptions. But those conveniences belie the magnitude of what Apple is attempting: nothing less than a fundamental reimagining of how software comes into existence.For the world&#x27;s most valuable company, the calculus is straightforward. Apple&#x27;s ability to attract and retain developers has always underpinned its platform dominance. If agentic coding delivers on its promise of radical productivity gains, early and deep integration could cement Apple&#x27;s position for another generation. If it doesn&#x27;t — if the security disasters and \"catastrophic explosions\" that critics predict come to pass — Cupertino could find itself at the epicenter of a very different kind of transformation.The technology industry has spent decades building systems to catch human errors before they reach users. Now it must answer a more unsettling question: What happens when the errors aren&#x27;t human at all?As Apple conceded during Tuesday&#x27;s press conference, with what may prove to be unintentional understatement: \"Large language models, as agents sometimes do, sometimes hallucinate.\"Millions of lines of code are about to find out how often.",
          "content": "Apple on Tuesday announced a major update to its flagship developer tool that gives artificial intelligence agents unprecedented control over the app-building process, a move that signals the iPhone maker&#x27;s aggressive push into an emerging and controversial practice known as \"agentic coding.\"Xcode 26.3, available immediately as a release candidate, integrates Anthropic&#x27;s Claude Agent and OpenAI&#x27;s Codex directly into Apple&#x27;s development environment, allowing the AI systems to autonomously write code, build projects, run tests, and visually verify their own work — all with minimal human oversight.The update is Apple&#x27;s most significant embrace of AI-assisted software development since introducing intelligence features in Xcode 26 last year, and arrives as \"vibe coding\" — the practice of delegating software creation to large language models — has become one of the most debated topics in technology.Apple says that while integrating intelligence into the Xcode developer workflow is powerful, the model itself still has a somewhat limited aperture. It answers questions based on what the developer provides, but it doesn&#x27;t have access to the full context of the project, and it&#x27;s not able to take action on its own. That changes with this update, the company said during a press conference Tuesday morning.How Apple&#x27;s new AI coding features let developers build apps faster than everThe key innovation in Xcode 26.3 is the depth of integration between AI agents and Apple&#x27;s development tools. Unlike previous iterations that offered code suggestions and autocomplete features, the new system grants AI agents access to nearly every aspect of the development process.During a live demonstration, an Apple engineer showed how the Claude agent could receive a simple prompt — \"add a new feature to show the weather at a landmark\" — and then independently analyze the project&#x27;s file structure, consult Apple&#x27;s documentation, write the necessary code, build the project, and take screenshots of the running application to verify its work matched the requested design.According to Apple, the agent is able to use tools like build and screenshot previews to verify its work, visually analyze the image, and confirm that everything has been built accordingly. Before, when interacting with a model, it would provide an answer and just stop there.The system creates automatic checkpoints as developers interact with the AI, allowing them to roll back changes if results prove unsatisfactory — a safeguard that acknowledges the unpredictable nature of AI-generated code.Apple says it worked directly with Anthropic and OpenAI to optimize the experience with particular attention paid to reducing token usage — the computational units that determine costs when using cloud-based AI models — and improving the efficiency of tool calling.According to the company, developers can download new agents with a single click, and they update automatically.Why Apple&#x27;s adoption of the Model Context Protocol could reshape the AI development landscapeUnderlying the integration is the Model Context Protocol, or MCP, an open standard that Anthropic developed for connecting AI agents with external tools. Apple&#x27;s adoption of MCP means that any compatible agent — not just Claude or Codex — can now interact with Xcode&#x27;s capabilities.Apple says this also works for agents that are running outside of Xcode. Any agent that is compatible with MCP can now work with Xcode to do all the same things — project discovery and change management, building and testing apps, working with previews and code snippets, and accessing the latest documentation.The decision to embrace an open protocol, rather than building a proprietary system, represents a notable departure for Apple, which has historically favored closed ecosystems. It also positions Xcode as a potential hub for a growing universe of AI development tools.Xcode&#x27;s troubled history with AI tools — and why Apple says this time is differentThe announcement comes against a backdrop of mixed experiences with AI-assisted coding in Apple&#x27;s tools. During the press conference, one developer described previous attempts to use AI agents with Xcode as \"horrible,\" citing constant crashes and an inability to complete basic tasks.Apple acknowledged the concerns while arguing that the new integration addresses fundamental limitations of earlier approaches.The company says the big shift is that Claude and Codex have so much more visibility into the breadth of the project. If they hallucinate and write code that doesn&#x27;t work, they can now build, see the compile errors, and iterate in real time to fix those issues — in some cases before presenting it as a finished work.Apple argues that the power of IDE integration extends beyond error correction. Agents can now automatically add entitlements to projects when needed to access protected APIs — a task the company says would be otherwise very difficult for an AI operating outside the development environment dealing with binary files it may not have the format for.From Andrej Karpathy&#x27;s tweet to LinkedIn certifications: The unstoppable rise of vibe codingApple&#x27;s announcement arrives at a crucial moment in the evolution of AI-assisted development. The term \"vibe coding,\" coined by AI researcher Andrej Karpathy in early 2025, has transformed from a curiosity into a genuine cultural phenomenon that is reshaping how software gets built.LinkedIn announced last week that it will begin offering official certifications in AI coding skills, drawing on usage data from platforms like Lovable and Replit. Job postings requiring AI proficiency doubled in the past year, according to edX research, with Indeed&#x27;s Hiring Lab reporting that 4.2% of U.S. job listings now mention AI-related keywords.The enthusiasm is driven by genuine productivity gains. Casey Newton, the technology journalist, recently described building a complete personal website using Claude Code in about an hour — a task that previously required expensive Squarespace subscriptions and years of frustrated attempts with various website builders.More dramatically, Jaana Dogan, a principal engineer at Google, posted that she gave Claude Code \"a description of the problem\" and \"it generated what we built last year in an hour.\" Her post, which accumulated more than 8 million views, began with the disclaimer: \"I&#x27;m not joking and this isn&#x27;t funny.\"Security experts warn that AI-generated code could lead to &#x27;catastrophic explosions&#x27;But the rapid adoption of agentic coding has also sparked significant concerns among security researchers and software engineers.David Mytton, founder and CEO of developer security provider Arcjet, warned last month that the proliferation of vibe-coded applications \"into production will lead to catastrophic problems for organizations that don&#x27;t properly review AI-developed software.\"\"In 2026, I expect more and more vibe-coded applications hitting production in a big way,\" Mytton wrote. \"That&#x27;s going to be great for velocity… but you&#x27;ve still got to pay attention. There&#x27;s going to be some big explosions coming!\"Simon Willison, co-creator of the Django web framework, drew an even starker comparison. \"I think we&#x27;re due a Challenger disaster with respect to coding agent security,\" he said, referring to the 1986 space shuttle explosion that killed all seven crew members. \"So many people, myself included, are running these coding agents practically as root. We&#x27;re letting them do all of this stuff.\"A pre-print paper from researchers this week warned that vibe coding could pose existential risks to the open-source software ecosystem. The study found that AI-assisted development pulls user interaction away from community projects, reduces visits to documentation websites and forums, and makes launching new open-source initiatives significantly harder.Stack Overflow usage has plummeted as developers increasingly turn to AI chatbots for answers—a shift that could ultimately starve the very knowledge bases that trained the AI models in the first place.Previous research painted an even more troubling picture: a 2024 report found that vibe coding using tools like GitHub Copilot \"offered no real benefits unless adding 41% more bugs is a measure of success.\"The hidden mental health cost of letting AI write your codeEven enthusiastic adopters have begun acknowledging the darker aspects of AI-assisted development.Peter Steinberger, creator of the viral AI agent originally known as Clawdbot (now OpenClaw), recently revealed that he had to step back from vibe coding after it consumed his life.\"I was out with my friends and instead of joining the conversation in the restaurant, I was just like, vibe coding on my phone,\" Steinberger said in a recent podcast interview. \"I decided, OK, I have to stop this more for my mental health than for anything else.\"Steinberger warned that the constant building of increasingly powerful AI tools creates the \"illusion of making you more productive\" without necessarily advancing real goals. \"If you don&#x27;t have a vision of what you&#x27;re going to build, it&#x27;s still going to be slop,\" he added.Google CEO Sundar Pichai has expressed similar reservations, saying he won&#x27;t vibe code on \"large codebases where you really have to get it right.\"\"The security has to be there,\" Pichai said in a November podcast interview.Boris Cherny, the Anthropic engineer who created Claude Code, acknowledged that vibe coding works best for \"prototypes or throwaway code, not software that sits at the core of a business.\"\"You want maintainable code sometimes. You want to be very thoughtful about every line sometimes,\" Cherny said.Apple is gambling that deep IDE integration can make AI coding safe for productionApple appears to be betting that the benefits of deep IDE integration can mitigate many of these concerns. By giving AI agents access to build systems, test suites, and visual verification tools, the company is essentially arguing that Xcode can serve as a quality control mechanism for AI-generated code.Susan Prescott, Apple&#x27;s vice president of Worldwide Developer Relations, framed the update as part of Apple&#x27;s broader mission.In a statement, Apple said its goal is to make tools that put industry-leading technologies directly in developers&#x27; hands so they can build the very best apps. The company says agentic coding supercharges productivity and creativity, streamlining the development workflow so developers can focus on innovation.But the question remains whether the safeguards will prove sufficient as AI agents grow more autonomous. Asked about debugging capabilities, Apple noted that while Xcode has a powerful debugger built in, there is no direct MCP tool for debugging.Developers can run the debugger and manually relay information to the agent, but the AI cannot yet independently investigate runtime issues — a limitation that could prove significant as the complexity of AI-generated code increases.The update also does not currently support running multiple agents simultaneously on the same project, though Apple noted that developers can open projects in multiple Xcode windows using Git worktrees as a workaround.The future of software development hangs in the balance — and Apple just raised the stakesXcode 26.3 is available immediately as a release candidate for members of the Apple Developer Program, with a general release expected soon on the App Store. The release candidate designation — Apple&#x27;s final beta before production — means developers who download today will automatically receive the finished version when it ships.The integration supports both API keys and direct account credentials from OpenAI and Anthropic, offering developers flexibility in managing their AI subscriptions. But those conveniences belie the magnitude of what Apple is attempting: nothing less than a fundamental reimagining of how software comes into existence.For the world&#x27;s most valuable company, the calculus is straightforward. Apple&#x27;s ability to attract and retain developers has always underpinned its platform dominance. If agentic coding delivers on its promise of radical productivity gains, early and deep integration could cement Apple&#x27;s position for another generation. If it doesn&#x27;t — if the security disasters and \"catastrophic explosions\" that critics predict come to pass — Cupertino could find itself at the epicenter of a very different kind of transformation.The technology industry has spent decades building systems to catch human errors before they reach users. Now it must answer a more unsettling question: What happens when the errors aren&#x27;t human at all?As Apple conceded during Tuesday&#x27;s press conference, with what may prove to be unintentional understatement: \"Large language models, as agents sometimes do, sometimes hallucinate.\"Millions of lines of code are about to find out how often.",
          "feed_position": 1,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/2Q8OGhbv336AGtWlkPE9wO/51480cf6096208fab81a5f6c184a9a20/nuneybits_Vector_art_of_Apple_logo_inside_code_brackets_63cd7c00-dd68-44a7-b323-831a5a2226c0.webp?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/data/databricks-serverless-database-slashes-app-development-from-months-to-days",
          "published_at": "Tue, 03 Feb 2026 17:00:00 GMT",
          "title": "Databricks' serverless database slashes app development from months to days as companies prep for agentic AI",
          "standfirst": "Five years ago, Databricks coined the term &#x27;data lakehouse&#x27; to describe a new type of data architecture that combines a data lake with a data warehouse. That term and data architecture are now commonplace across the data industry for analytics workloads.Now, Databricks is once again looking to create a new category with its Lakebase service, now generally available today. While the data lakehouse construct deals with OLAP (online analytical processing) databases, Lakebase is all about OLTP (online transaction processing) and operational databases. The Lakebase service has been in development since June 2025 and is based on technology Databricks gained via its acquisition of PostgreSQL database provider Neon. It was further enhanced in October of 2025 with the acquisition of Mooncake, which brought capabilities to help bridge PostgreSQL with lakehouse data formats.Lakebase is a serverless operational database that represents a fundamental rethinking of how databases work in the age of autonomous AI agents. Early adopters, including easyJet, Hafnia and Warner Music Group, are cutting application delivery times by 75 to 95%, but the deeper architectural innovation positions databases as ephemeral, self-service infrastructure that AI agents can provision and manage without human intervention.This isn&#x27;t just another managed Postgres service. Lakebase treats operational databases as lightweight, disposable compute running on data lake storage rather than monolithic systems requiring careful capacity planning and database administrator (DBA) oversight. \"Really, for the vibe coding trend to take off, you need developers to believe they can actually create new apps very quickly, but you also need the central IT team, or DBAs, to be comfortable with the tsunami of apps and databases,\" Databricks co-founder Reynold Xin told VentureBeat. \"Classic databases simply won&#x27;t scale to that because they can&#x27;t afford to put a DBA per database and per app.\"92% faster delivery: From two months to five daysThe production numbers demonstrate immediate impact beyond the agent provisioning vision. Hafnia reduced delivery time for production-ready applications from two months to five days — or 92% — using Lakebase as the transactional engine for their internal operations portal. The shipping company moved beyond static BI reports to real-time business applications for fleet, commercial and finance workflows.EasyJet consolidated more than 100 Git repositories into just two and cut development cycles from nine months to four months — a 56% reduction — while building a web-based revenue management hub on Lakebase to replace a decade-old desktop app and one of Europe&#x27;s largest legacy SQL Server environments.Warner Music Group is moving insights directly into production systems using the unified foundation, while Quantum Capital Group uses it to maintain consistent, governed data for identifying and evaluating oil and gas investments — eliminating the data duplication that previously forced teams to maintain multiple copies in different formats.The acceleration stems from the elimination of two major bottlenecks: database cloning for test environments and ETL pipeline maintenance for syncing operational and analytical data.Technical architecture: Why this isn&#x27;t just managed PostgresTraditional databases couple storage and compute — organizations provision a database instance with attached storage and scale by adding more instances or storage. AWS Aurora innovated by separating these layers using proprietary storage, but the storage remained locked inside AWS&#x27;s ecosystem and wasn&#x27;t independently accessible for analytics.Lakebase takes the separation of storage and compute to its logical conclusion by putting storage directly in the data lakehouse. The compute layer runs essentially vanilla PostgreSQL— maintaining full compatibility with the Postgres ecosystem — but every write goes to lakehouse storage in formats that Spark, Databricks SQL and other analytics engines can immediately query without ETL.\"The unique technical insight was that data lakes decouple storage from compute, which was great, but we need to introduce data management capabilities like governance and transaction management into the data lake,\" Xin explained. \"We&#x27;re actually not that different from the lakehouse concept, but we&#x27;re building lightweight, ephemeral compute for OLTP databases on top.\"Databricks built Lakebase with the technology it gained from the acquisition of Neon. But Xin emphasized that Databricks significantly expanded Neon&#x27;s original capabilities to create something fundamentally different.\"They didn’t have the enterprise experience, and they didn’t have the cloud scale,\" Xin said. \"We brought the Neon team&#x27;s novel architectural idea together with the robustness of the Databricks infrastructure and combined them. So now we&#x27;ve created a super scalable platform.\"From hundreds of databases to millions built for agentic AIXin outlined a vision directly tied to the economics of AI coding tools that explains why the Lakebase construct matters beyond current use cases. As development costs plummet, enterprises will shift from buying hundreds of SaaS applications to building millions of bespoke internal applications.\"As the cost of software development goes down, which we&#x27;re seeing today because of AI coding tools, it will shift from the proliferation of SaaS in the last 10 to 15 years to the proliferation of in-house application development,\" Xin said. \"Instead of building maybe hundreds of applications, they&#x27;ll be building millions of bespoke apps over time.\"This creates an impossible fleet management problem with traditional approaches. You cannot hire enough DBAs to manually provision, monitor and troubleshoot thousands of databases. Xin&#x27;s solution: Treat database management itself as a data problem rather than an operations problem.Lakebase stores all telemetry and metadata — query performance, resource utilization, connection patterns, error rates — directly in the lakehouse, where it can be analyzed using standard data engineering and data science tools. Instead of configuring dashboards in database-specific monitoring tools, data teams query telemetry data with SQL or analyze it with machine learning models to identify outliers and predict issues.\"Instead of creating a dashboard for every 50 or 100 databases, you can actually look at the chart to understand if something has misbehaved,\" Xin explained. \"Database management will look very similar to an analytics problem. You look at outliers, you look at trends, you try to understand why things happen. This is how you manage at scale when agents are creating and destroying databases programmatically.\"The implications extend to autonomous agents themselves. An AI agent experiencing performance issues could query the telemetry data to diagnose problems — treating database operations as just another analytics task rather than requiring specialized DBA knowledge. Database management becomes something agents can do for themselves using the same data analysis capabilities they already have.What this means for enterprise data teamsThe Lakebase construct signals a fundamental shift in how enterprises should think about operational databases — not as precious, carefully managed infrastructure requiring specialized DBAs, but as ephemeral, self-service resources that scale programmatically like cloud compute. This matters whether or not autonomous agents materialize as quickly as Databricks envisions, because the underlying architectural principle — treating database management as an analytics problem rather than an operations problem — changes the skill sets and team structures enterprises need.Data leaders should pay attention to the convergence of operational and analytical data happening across the industry. When writes to an operational database are immediately queryable by analytics engines without ETL, the traditional boundaries between transactional systems and data warehouses blur. This unified architecture reduces the operational overhead of maintaining separate systems, but it also requires rethinking data team structures built around those boundaries.When lakehouse launched, competitors rejected the concept before eventually adopting it themselves. Xin expects the same trajectory for Lakebase. \"It just makes sense to separate storage and compute and put all the storage in the lake — it enables so many capabilities and possibilities,\" he said.",
          "content": "Five years ago, Databricks coined the term &#x27;data lakehouse&#x27; to describe a new type of data architecture that combines a data lake with a data warehouse. That term and data architecture are now commonplace across the data industry for analytics workloads.Now, Databricks is once again looking to create a new category with its Lakebase service, now generally available today. While the data lakehouse construct deals with OLAP (online analytical processing) databases, Lakebase is all about OLTP (online transaction processing) and operational databases. The Lakebase service has been in development since June 2025 and is based on technology Databricks gained via its acquisition of PostgreSQL database provider Neon. It was further enhanced in October of 2025 with the acquisition of Mooncake, which brought capabilities to help bridge PostgreSQL with lakehouse data formats.Lakebase is a serverless operational database that represents a fundamental rethinking of how databases work in the age of autonomous AI agents. Early adopters, including easyJet, Hafnia and Warner Music Group, are cutting application delivery times by 75 to 95%, but the deeper architectural innovation positions databases as ephemeral, self-service infrastructure that AI agents can provision and manage without human intervention.This isn&#x27;t just another managed Postgres service. Lakebase treats operational databases as lightweight, disposable compute running on data lake storage rather than monolithic systems requiring careful capacity planning and database administrator (DBA) oversight. \"Really, for the vibe coding trend to take off, you need developers to believe they can actually create new apps very quickly, but you also need the central IT team, or DBAs, to be comfortable with the tsunami of apps and databases,\" Databricks co-founder Reynold Xin told VentureBeat. \"Classic databases simply won&#x27;t scale to that because they can&#x27;t afford to put a DBA per database and per app.\"92% faster delivery: From two months to five daysThe production numbers demonstrate immediate impact beyond the agent provisioning vision. Hafnia reduced delivery time for production-ready applications from two months to five days — or 92% — using Lakebase as the transactional engine for their internal operations portal. The shipping company moved beyond static BI reports to real-time business applications for fleet, commercial and finance workflows.EasyJet consolidated more than 100 Git repositories into just two and cut development cycles from nine months to four months — a 56% reduction — while building a web-based revenue management hub on Lakebase to replace a decade-old desktop app and one of Europe&#x27;s largest legacy SQL Server environments.Warner Music Group is moving insights directly into production systems using the unified foundation, while Quantum Capital Group uses it to maintain consistent, governed data for identifying and evaluating oil and gas investments — eliminating the data duplication that previously forced teams to maintain multiple copies in different formats.The acceleration stems from the elimination of two major bottlenecks: database cloning for test environments and ETL pipeline maintenance for syncing operational and analytical data.Technical architecture: Why this isn&#x27;t just managed PostgresTraditional databases couple storage and compute — organizations provision a database instance with attached storage and scale by adding more instances or storage. AWS Aurora innovated by separating these layers using proprietary storage, but the storage remained locked inside AWS&#x27;s ecosystem and wasn&#x27;t independently accessible for analytics.Lakebase takes the separation of storage and compute to its logical conclusion by putting storage directly in the data lakehouse. The compute layer runs essentially vanilla PostgreSQL— maintaining full compatibility with the Postgres ecosystem — but every write goes to lakehouse storage in formats that Spark, Databricks SQL and other analytics engines can immediately query without ETL.\"The unique technical insight was that data lakes decouple storage from compute, which was great, but we need to introduce data management capabilities like governance and transaction management into the data lake,\" Xin explained. \"We&#x27;re actually not that different from the lakehouse concept, but we&#x27;re building lightweight, ephemeral compute for OLTP databases on top.\"Databricks built Lakebase with the technology it gained from the acquisition of Neon. But Xin emphasized that Databricks significantly expanded Neon&#x27;s original capabilities to create something fundamentally different.\"They didn’t have the enterprise experience, and they didn’t have the cloud scale,\" Xin said. \"We brought the Neon team&#x27;s novel architectural idea together with the robustness of the Databricks infrastructure and combined them. So now we&#x27;ve created a super scalable platform.\"From hundreds of databases to millions built for agentic AIXin outlined a vision directly tied to the economics of AI coding tools that explains why the Lakebase construct matters beyond current use cases. As development costs plummet, enterprises will shift from buying hundreds of SaaS applications to building millions of bespoke internal applications.\"As the cost of software development goes down, which we&#x27;re seeing today because of AI coding tools, it will shift from the proliferation of SaaS in the last 10 to 15 years to the proliferation of in-house application development,\" Xin said. \"Instead of building maybe hundreds of applications, they&#x27;ll be building millions of bespoke apps over time.\"This creates an impossible fleet management problem with traditional approaches. You cannot hire enough DBAs to manually provision, monitor and troubleshoot thousands of databases. Xin&#x27;s solution: Treat database management itself as a data problem rather than an operations problem.Lakebase stores all telemetry and metadata — query performance, resource utilization, connection patterns, error rates — directly in the lakehouse, where it can be analyzed using standard data engineering and data science tools. Instead of configuring dashboards in database-specific monitoring tools, data teams query telemetry data with SQL or analyze it with machine learning models to identify outliers and predict issues.\"Instead of creating a dashboard for every 50 or 100 databases, you can actually look at the chart to understand if something has misbehaved,\" Xin explained. \"Database management will look very similar to an analytics problem. You look at outliers, you look at trends, you try to understand why things happen. This is how you manage at scale when agents are creating and destroying databases programmatically.\"The implications extend to autonomous agents themselves. An AI agent experiencing performance issues could query the telemetry data to diagnose problems — treating database operations as just another analytics task rather than requiring specialized DBA knowledge. Database management becomes something agents can do for themselves using the same data analysis capabilities they already have.What this means for enterprise data teamsThe Lakebase construct signals a fundamental shift in how enterprises should think about operational databases — not as precious, carefully managed infrastructure requiring specialized DBAs, but as ephemeral, self-service resources that scale programmatically like cloud compute. This matters whether or not autonomous agents materialize as quickly as Databricks envisions, because the underlying architectural principle — treating database management as an analytics problem rather than an operations problem — changes the skill sets and team structures enterprises need.Data leaders should pay attention to the convergence of operational and analytical data happening across the industry. When writes to an operational database are immediately queryable by analytics engines without ETL, the traditional boundaries between transactional systems and data warehouses blur. This unified architecture reduces the operational overhead of maintaining separate systems, but it also requires rethinking data team structures built around those boundaries.When lakehouse launched, competitors rejected the concept before eventually adopting it themselves. Xin expects the same trajectory for Lakebase. \"It just makes sense to separate storage and compute and put all the storage in the lake — it enables so many capabilities and possibilities,\" he said.",
          "feed_position": 2,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/469ZkXofQMti2royetGH8u/756e2b7f0b026e2b48d72bf886cd7868/database-in-a-lake-smk1.jpg?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/infrastructure/vercel-rebuilt-v0-to-tackle-the-90-problem-connecting-ai-generated-code-to",
          "published_at": "Tue, 03 Feb 2026 14:00:00 GMT",
          "title": "Vercel rebuilt v0 to tackle the 90% problem: Connecting AI-generated code to existing production infrastructure, not prototypes",
          "standfirst": "Before Claude Code wrote its first line of code, Vercel was already in the vibe coding space with its v0 service.The basic idea behind the original v0, which launched in 2024, was essentially to be version 0. That is, the earliest version of an application, helping developers solve the blank canvas problem. Developers could prompt their way to a user interface (UI) scaffolding that looked good, but the code was disposable. Getting those prototypes into production required rewrites.More than 4 million people have used v0 to build millions of prototypes, but the platform was missing elements required to get into production. The challenge is a familiar one with vibe coding tools, as there is a gap in what tools provide and what enterprise builders require. Claude Code, for instance, generates backend logic and scripts effectively, but does not deploy production UIs within existing company design systems while enforcing security policiesThis creates what Vercel CPO Tom Occhino calls \"the world&#x27;s largest shadow IT problem.\" AI-enabled software creation is already happening inside every enterprise. Credentials are copied into prompts. Company data flows to unmanaged tools. Apps deploy outside approved infrastructure. There&#x27;s no audit trail.Vercel rebuilt v0 to address this production deployment gap. The new version, generally available today, imports existing GitHub repositories and automatically pulls environment variables and configurations. It generates code in a sandbox-based runtime that maps directly to real Vercel deployments and enforces security controls and proper git workflows while allowing non-engineers to ship production code.\"What&#x27;s really nice about v0 is that you still have the code visible and reviewable and governed,\" Occhino told VentureBeat in an exclusive interview. \"Teams end up collaborating on the product, not on PRDs and stuff.\"This shift matters because most enterprise software work happens on existing applications, not new prototypes. Teams need tools that integrate with their current codebases and infrastructure.How v0&#x27;s sandbox runtime connects AI-generated code to existing repositoriesThe original v0 generated UI scaffolding from prompts and let users iterate through conversations. But the code lived in v0&#x27;s isolated environment, which meant moving it to production required copying files, rewriting imports and manually wiring everything together.The rebuilt v0 fundamentally changes this by directly importing existing GitHub repositories. A sandbox-based runtime automatically pulls environment variables, deployments and configurations from Vercel, so every prompt generates production-ready code that already understands the company&#x27;s infrastructure. The code lives in the repository, not a separate prototyping tool.Previously, v0 was a separate prototyping environment. Now, it&#x27;s connected to the actual codebase with full VS Code built into the interface, which means developers can edit code directly without switching tools.A new git panel handles proper workflows. Anyone on a team can create branches from within v0, open pull requests against main and deploy on merge. Pull requests are first-class citizens and previews map directly to real Vercel deployments, not isolated demos.This matters because product managers and marketers can now ship production code through proper git workflows without needing local development environments or handing code snippets to engineers for integration. The new version also adds direct integrations with Snowflake and AWS databases, so teams can wire apps to production data sources with proper access controls built in, rather than requiring manual work.Vercel&#x27;s React and Next.js experience explains v0&#x27;s deployment infrastructurePrior to joining Vercel in 2023, Occhino spent a dozen years as an engineer at Meta (formerly Facebook) and helped lead that company&#x27;s development of the widely-used React JavaScript framework.Vercel&#x27;s claim to fame is that its company founder, Guillermo Rauch, is the creator of Next.js, a full-stack framework built on top of React. In the vibe coding era, Next.js has become an increasingly popular framework. The company recently published a list of React best practices specifically designed to help AI agents and LLMs work.The Vercel platform encapsulates best practices and learnings from Next.js and React. That decade of building frameworks and infrastructure together means v0 outputs production-ready code that deploys on the same infrastructure Vercel uses for millions of deployments annually. The platform includes agentic workflow support, MCP integration, web application firewall, SSO and deployment protections. Teams can open any project in a cloud dev environment and push changes in a single click to a Vercel preview or production deployment.With no shortage of competitive offerings in the vibe coding space, including Replit, Lovable and Cursor among others, it&#x27;s the core foundational infrastructure that Occhino sees as standing out.\"The biggest differentiator for us is the Vercel infrastructure,\" Occhino said. \"It&#x27;s been building managed infrastructure, framework-defined infrastructure, now self-driving infrastructure for the past 10 years.\"Why vibe coding security requires infrastructure control, not just policyThe shadow IT problem isn&#x27;t that employees are using AI tools. It&#x27;s that most vibe coding tools operate entirely outside enterprise infrastructure. Credentials are copied into prompts because there&#x27;s no secure way to connect generated code to enterprise databases. Apps deploy to public URLs because the tools don&#x27;t integrate with company deployment pipelines. Data leaks happen because visibility controls don&#x27;t exist.The technical challenge is that securing AI-generated code requires controlling where it runs and what it can access. Policy documents don&#x27;t help if the tooling itself can&#x27;t enforce those policies.This is where infrastructure matters. When vibe coding tools operate on separate platforms, enterprises face a choice: Block the tools entirely or accept the security risks. When the vibe coding tool runs on the same infrastructure as production deployments, security controls can be enforced automatically.v0 runs on Vercel&#x27;s infrastructure, which means enterprises can set deployment protections, visibility controls and access policies that apply to AI-generated code the same way they apply to hand-written code. Direct integrations with Snowflake and AWS databases let teams connect to production data with proper access controls rather than copying credentials into prompts.\"IT teams are comfortable with what their teams are building because they have control over who has access,\" Occhino said. \"They have control over what those applications have access to from Snowflake or data systems.\"Generative UI vs. generative softwareIn addition to the new version of v0, Vercel has recently introduced a generative UI technology called json-render.v0 is what Vercel calls generative software. This differs from the company&#x27;s json-render framework for a true generative UI. Vercel software engineer Chris Tate explained that v0 builds full-stack apps and agents, not just UIs or frontends. In contrast, json-render is a framework that enables AI to generate UI components directly at runtime by outputting JSON instead of code. \"The AI doesn&#x27;t write software,\" Tate told VentureBeat. \"It plugs directly into the rendering layer to create spontaneous, personalized interfaces on demand.\"The distinction matters for enterprise use cases. Teams use v0 when they need to build complete applications, custom components or production software.They use JSON-render for dynamic, personalized UI elements within applications, dashboards that adapt to individual users, contextual widgets and interfaces that respond to changing data without code changes.Both leverage the AI SDK infrastructure that Vercel has built for streaming and structured outputs.Three lessons enterprises learned from vibe coding adoptionAs enterprises adopted vibe coding tools over the past two years, several patterns emerged about AI-generated code in production environments.Lesson 1: Prototyping without production deployment creates false progress. Enterprises saw teams generate impressive demos in v0&#x27;s early versions, then hit a wall moving those demos to production. The problem wasn&#x27;t the quality of generated code. It was that prototypes lived in isolated environments disconnected from production infrastructure.\"While demos are easy to generate, I think most of the iteration that&#x27;s happening on these code bases is happening on real production apps,\" Occhino said. \"90% of what we need to do is make changes to an existing code base.\"Lesson 2: The software development lifecycle has already changed, whether enterprises planned for it or not. Domain experts are building software directly instead of writing product requirement documents (PRDs) for engineers to interpret. Product managers and marketers ship features without waiting for engineering sprints.This shift means enterprises need tools that maintain code visibility and governance while enabling non-engineers to ship. The alternative is creating bottlenecks by forcing all AI-generated code through traditional development workflows.Lesson 3: Blocking vibe coding tools doesn&#x27;t stop vibe coding. It just pushes the activity outside IT&#x27;s visibility. Enterprises that try to restrict AI-powered development find employees using tools anyway, creating the shadow IT problem at scale.The practical implication is that enterprises should focus less on whether to allow vibe coding and more on ensuring it happens within infrastructure that can enforce existing security and deployment policies.",
          "content": "Before Claude Code wrote its first line of code, Vercel was already in the vibe coding space with its v0 service.The basic idea behind the original v0, which launched in 2024, was essentially to be version 0. That is, the earliest version of an application, helping developers solve the blank canvas problem. Developers could prompt their way to a user interface (UI) scaffolding that looked good, but the code was disposable. Getting those prototypes into production required rewrites.More than 4 million people have used v0 to build millions of prototypes, but the platform was missing elements required to get into production. The challenge is a familiar one with vibe coding tools, as there is a gap in what tools provide and what enterprise builders require. Claude Code, for instance, generates backend logic and scripts effectively, but does not deploy production UIs within existing company design systems while enforcing security policiesThis creates what Vercel CPO Tom Occhino calls \"the world&#x27;s largest shadow IT problem.\" AI-enabled software creation is already happening inside every enterprise. Credentials are copied into prompts. Company data flows to unmanaged tools. Apps deploy outside approved infrastructure. There&#x27;s no audit trail.Vercel rebuilt v0 to address this production deployment gap. The new version, generally available today, imports existing GitHub repositories and automatically pulls environment variables and configurations. It generates code in a sandbox-based runtime that maps directly to real Vercel deployments and enforces security controls and proper git workflows while allowing non-engineers to ship production code.\"What&#x27;s really nice about v0 is that you still have the code visible and reviewable and governed,\" Occhino told VentureBeat in an exclusive interview. \"Teams end up collaborating on the product, not on PRDs and stuff.\"This shift matters because most enterprise software work happens on existing applications, not new prototypes. Teams need tools that integrate with their current codebases and infrastructure.How v0&#x27;s sandbox runtime connects AI-generated code to existing repositoriesThe original v0 generated UI scaffolding from prompts and let users iterate through conversations. But the code lived in v0&#x27;s isolated environment, which meant moving it to production required copying files, rewriting imports and manually wiring everything together.The rebuilt v0 fundamentally changes this by directly importing existing GitHub repositories. A sandbox-based runtime automatically pulls environment variables, deployments and configurations from Vercel, so every prompt generates production-ready code that already understands the company&#x27;s infrastructure. The code lives in the repository, not a separate prototyping tool.Previously, v0 was a separate prototyping environment. Now, it&#x27;s connected to the actual codebase with full VS Code built into the interface, which means developers can edit code directly without switching tools.A new git panel handles proper workflows. Anyone on a team can create branches from within v0, open pull requests against main and deploy on merge. Pull requests are first-class citizens and previews map directly to real Vercel deployments, not isolated demos.This matters because product managers and marketers can now ship production code through proper git workflows without needing local development environments or handing code snippets to engineers for integration. The new version also adds direct integrations with Snowflake and AWS databases, so teams can wire apps to production data sources with proper access controls built in, rather than requiring manual work.Vercel&#x27;s React and Next.js experience explains v0&#x27;s deployment infrastructurePrior to joining Vercel in 2023, Occhino spent a dozen years as an engineer at Meta (formerly Facebook) and helped lead that company&#x27;s development of the widely-used React JavaScript framework.Vercel&#x27;s claim to fame is that its company founder, Guillermo Rauch, is the creator of Next.js, a full-stack framework built on top of React. In the vibe coding era, Next.js has become an increasingly popular framework. The company recently published a list of React best practices specifically designed to help AI agents and LLMs work.The Vercel platform encapsulates best practices and learnings from Next.js and React. That decade of building frameworks and infrastructure together means v0 outputs production-ready code that deploys on the same infrastructure Vercel uses for millions of deployments annually. The platform includes agentic workflow support, MCP integration, web application firewall, SSO and deployment protections. Teams can open any project in a cloud dev environment and push changes in a single click to a Vercel preview or production deployment.With no shortage of competitive offerings in the vibe coding space, including Replit, Lovable and Cursor among others, it&#x27;s the core foundational infrastructure that Occhino sees as standing out.\"The biggest differentiator for us is the Vercel infrastructure,\" Occhino said. \"It&#x27;s been building managed infrastructure, framework-defined infrastructure, now self-driving infrastructure for the past 10 years.\"Why vibe coding security requires infrastructure control, not just policyThe shadow IT problem isn&#x27;t that employees are using AI tools. It&#x27;s that most vibe coding tools operate entirely outside enterprise infrastructure. Credentials are copied into prompts because there&#x27;s no secure way to connect generated code to enterprise databases. Apps deploy to public URLs because the tools don&#x27;t integrate with company deployment pipelines. Data leaks happen because visibility controls don&#x27;t exist.The technical challenge is that securing AI-generated code requires controlling where it runs and what it can access. Policy documents don&#x27;t help if the tooling itself can&#x27;t enforce those policies.This is where infrastructure matters. When vibe coding tools operate on separate platforms, enterprises face a choice: Block the tools entirely or accept the security risks. When the vibe coding tool runs on the same infrastructure as production deployments, security controls can be enforced automatically.v0 runs on Vercel&#x27;s infrastructure, which means enterprises can set deployment protections, visibility controls and access policies that apply to AI-generated code the same way they apply to hand-written code. Direct integrations with Snowflake and AWS databases let teams connect to production data with proper access controls rather than copying credentials into prompts.\"IT teams are comfortable with what their teams are building because they have control over who has access,\" Occhino said. \"They have control over what those applications have access to from Snowflake or data systems.\"Generative UI vs. generative softwareIn addition to the new version of v0, Vercel has recently introduced a generative UI technology called json-render.v0 is what Vercel calls generative software. This differs from the company&#x27;s json-render framework for a true generative UI. Vercel software engineer Chris Tate explained that v0 builds full-stack apps and agents, not just UIs or frontends. In contrast, json-render is a framework that enables AI to generate UI components directly at runtime by outputting JSON instead of code. \"The AI doesn&#x27;t write software,\" Tate told VentureBeat. \"It plugs directly into the rendering layer to create spontaneous, personalized interfaces on demand.\"The distinction matters for enterprise use cases. Teams use v0 when they need to build complete applications, custom components or production software.They use JSON-render for dynamic, personalized UI elements within applications, dashboards that adapt to individual users, contextual widgets and interfaces that respond to changing data without code changes.Both leverage the AI SDK infrastructure that Vercel has built for streaming and structured outputs.Three lessons enterprises learned from vibe coding adoptionAs enterprises adopted vibe coding tools over the past two years, several patterns emerged about AI-generated code in production environments.Lesson 1: Prototyping without production deployment creates false progress. Enterprises saw teams generate impressive demos in v0&#x27;s early versions, then hit a wall moving those demos to production. The problem wasn&#x27;t the quality of generated code. It was that prototypes lived in isolated environments disconnected from production infrastructure.\"While demos are easy to generate, I think most of the iteration that&#x27;s happening on these code bases is happening on real production apps,\" Occhino said. \"90% of what we need to do is make changes to an existing code base.\"Lesson 2: The software development lifecycle has already changed, whether enterprises planned for it or not. Domain experts are building software directly instead of writing product requirement documents (PRDs) for engineers to interpret. Product managers and marketers ship features without waiting for engineering sprints.This shift means enterprises need tools that maintain code visibility and governance while enabling non-engineers to ship. The alternative is creating bottlenecks by forcing all AI-generated code through traditional development workflows.Lesson 3: Blocking vibe coding tools doesn&#x27;t stop vibe coding. It just pushes the activity outside IT&#x27;s visibility. Enterprises that try to restrict AI-powered development find employees using tools anyway, creating the shadow IT problem at scale.The practical implication is that enterprises should focus less on whether to allow vibe coding and more on ensuring it happens within infrastructure that can enforce existing security and deployment policies.",
          "feed_position": 3,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/2U5I1bHA9QfvqoZdPB6qk0/7e4945d1392da8bd955144193be626f7/enterprise-vibe-coder-smk1.jpg?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/computing/accessories/best-bluetooth-tracker-140028377.html",
          "published_at": "Tue, 03 Feb 2026 08:00:35 +0000",
          "title": "The best Bluetooth trackers for 2026",
          "standfirst": "Most people think of AirTags when they picture a Bluetooth tracker. And indeed, Apple’s little white discs were once the only capable option, relying on a vast finding network of nearby iPhones to pinpoint lost tags. But now Google has a finding network of its own, and third party brands like Chipolo, Hyper and Pebblebee have trackers that pair with your choice of Google or Apple’s network. That means you’ve got a lot of options for tagging and tracking your keys, backpacks, luggage and more. We tested the major brands out there to see how well they work, how loud they are and how they look to put together a guide to help you get the most out of your chosen tracker. Here are the best Bluetooth trackers you can buy.Editor's note: Apple just released a new version of its AirTag trackers. We are in the process of testing the new model and will update this guide once we're done. Best Bluetooth trackers for 2026 What to look for in a Bluetooth tracking device Bluetooth trackers are small discs or cards that rely on short-range, low-energy wireless signals to communicate with your smartphone. Attach one of these gadgets your stuff and, if it’s in range, your phone can “ring” the chip so you can find it. These tracking devices offer other features like separation alerts to tell you when you’ve left a tagged item behind, or where a lost item was last detected. Some can even tap into a larger network of smartphones to track down your device when you’re out of range. Depending on what you want the tracker to do, there are a few specs to look for when deciding which to get. Device compatibility Like most things from the folks in Cupertino, AirTags only work with products in the Apple ecosystem. Both Apple and Google have opened up access to the Find My and Find Hub networks to third-party manufacturers, including Chipolo and Pebblebee. Those two companies make device-agnostic models that will work with the larger tracking network from either brand, so iPhone and Android users can buy the same tag. Tile trackers work with either Android or Apple devices, but use Tile’s own Life 360 finding network. Samsung’s latest fob, the Galaxy SmartTag2, only works with Samsung phones and taps into a finding system that relies on other Samsung devices to locate lost tags. Finding network Crowd-sourced finding capabilities are what make headlines, with stories about recovering stolen equipment or tracking lost luggage across the globe. Using anonymous signals that ping other people’s devices, these Bluetooth tracking devices can potentially tell you where a tagged item is, even if your smartphone is out of Bluetooth range. Apple’s Find My network is the largest, with over a billion iPhones and iPads in service all running Apple’s Find My app by default. So unless an iPhone user opts out, their phone silently acts as a location detector for any nearby AirTags. Apple recently increased the AirTag’s finding power by enabling you to share the tracker's location with a third party, party, like an airline. Chipolo fobs that work on Apple’s network have the same ability. Google launched its Find My Device network in 2024 and has since renamed it Find Hub, which, like Apple's fining app, combines devices and people finding in one place. That network is now a close second for the largest in the US Now that Google’s Find Hub network is up and running, it’s a close second for the largest in the US. Like Apple, Android users are automatically part of the network, but can opt-out by selecting the Google services option in their phone’s Settings app and toggling the option in the Find Hub menu. Samsung’s SmartTag 2 and related network also defaults to an opt-in status for finding tags and other devices. Tile offers a large finding grid that includes Tile users, Amazon Sidewalk customers and people running the Life360 network. Life360 acquired Tile in 2021, and, according to the company, the Life360 network has more than 70 million monthly active users. In our tests, AirTags and third-party tags using its network, like the Chipolo Loop and Pop and the Pebblebee Clip 5, were the fastest to track down lost items. They offered nearly real-time location data in moderately to heavily trafficked spots around Albuquerque, including a bar, bookstore and coffee shops in Nob Hill, along with various outdoor hangouts on UNM’s campus. Samsung's SmartTags were able to locate our lost items most of the time, though not with the same precision finding accuracy as AirTags. When we tested Google’s Find Hub (then called Find My Device) network right after launch, it was noticeably slower than Apple’s network when using the community finding feature. Testing it again in mid 2025, the time it took to locate a lost item was considerably improved, taking less than 20 minutes on average for the community to track a fob. In our tests, Tile’s finding network wasn’t able to consistently locate its lost fobs. Amy Skorheim / Engadget Separation alerts A tracker’s day-to-day utility becomes really apparent when it prevents you from losing something in the first place. Separation alerts tell you when you’ve traveled too far from your tagged items. Useful if you want to make sure your laptop bag, jacket or umbrella always comes with you when you leave the house. Apple’s Find My app delivers these notifications, but Google’s Find Hub does not. However, if you have a Chipolo device and allow its companion app to run in the background on your Android phone, left-behind alerts are enabled. Tile trackers require a yearly subscription to enable the alerts (currently $7 to $25 monthly). Both AirTags and Tiles allow you to turn off separation alerts at certain locations, meaning you can set your home as a “safe” place where items can be left behind, but alerts will still trigger elsewhere. In our tests, AirTags and others using the Find My network alerted us between the 600- and 1,400-foot mark. Tiles sent a notification after about an average of 1,500 feet and were more consistent when using an Android phone than an iPhone. Chipolo Pop tags paired with an Android phone and using its own app sent an alert when we got around 450 feet away from our tagged item. Connectivity and volume The feature you may use most often is the key finder function, which makes the tracker ring when you hit a button in the app. With Apple's AirTags, you can say \"Hey Siri, where are my keys?\" and the assistant will ring the tag (assuming it doesn't mistakenly think you're asking for directions to the Floridian archipelago). You can also use the Find Item app in your Apple Watch to ring your fob. Asking smart home/personal assistants like Alexa or the Google Assistant to find your keys will work with Chipolo, Tile and Pebblebee trackers linked to your Android device. If you have your tag but can’t find your phone, some trackers will let you ring them to find your handset. SmartTag2 fobs reliably rang our Galaxy phone when we double-pressed it. Tile trackers have the same feature. Chipolo Pop and Loop trackers can ring your phone, but uses the Chipolo app to do so, which can run concurrently with the Find My or Find Hub connection. AirTags and third-party tags using Google’s network don’t offer this feature. The volume of the Bluetooth tracking device may determine whether you can find an item buried in your couch cushions or in a noisy room. AirTags have a reputation for being on the quiet side, and that aligned with what we saw (measuring roughly 65 decibels). Chipolo’s Pop tags and Tile’s Pro model measure between 83 and 86 decibels on average. Pebblebee’s new Clip 5 was the loudest of any tag we’ve tested, clocking in at 97 ear-splitting decibels. Design and alternative formats Design will determine what you can attach the tracker to. AirTags are small, smooth discs that can’t be secured to anything without accessories, which are numerous, but that is an additional cost to consider. Chipolo, Pebblebee and Tile offer trackers with holes that easily attach to your key ring, and all three companies also offer card-shaped versions designed to fit in your wallet. Pebblebee Clip 5 tags come with a handy carabiner-style key ring. You can even get trackers embedded into useful items like luggage locks. The SmartLock from KeySmart is a TSA-approved luggage lock, but in addition to the three digit code, it’s also a Bluetooth tracker that’s compatible with Apple Find My. It wasn’t quite as loud as other trackers in my tests, and the range wasn’t as long, but it paired easily and worked with Apple’s finding network just like an AirTag. Battery life AirTag, Tile Pro, SmartTag2, HyperShield and Chipolo Pop fobs use replaceable batteries and each should go for at least a year before needing to be swapped. Pebblebee Clip 5 and Chipolo Loop trackers are rechargeable via a standard USB-C port. The Clip 5 has a long battery life claim at 12 months. The Loop should go for six months on a charge. Trackers shaped like credit cards, aka wallet trackers, don't have replaceable batteries, but some, like the Chipolo Card and the Pebblebee Card 5 are USB-C rechargeable. Stalking, theft and data privacy AirTags have gotten a lot of attention and even prompted some lawsuits for Apple due to bad actors planting them on people in order to stalk them. While this fact may not influence your buying decision, any discussion of Bluetooth trackers should note what steps Apple, Google and Tile have taken to address the issue. Last year, all the major players in the Bluetooth tracker business teamed up to combat misuse and standardize how unauthorized tracking detection and alerts work for iOS and Android. Last year, Tile launched a feature called Anti-Theft Mode, which enables you to render one of its trackers undetectable by others. That means if someone steals your tagged item, they won’t be able to use the anti-stalking features to find and disable the tracker. That sort of negates one of the major ways potential stalking victims can stay safe, so Tile hopes ID verification and a $1 million penalty will deter misuse. As a theft deterrent, a Bluetooth tracker may or may not be the best option. Anecdotal stories abound in which people have recovered stolen goods using a tracker — but other tales are more cautionary. Neither Apple nor Google promotes its trackers or finding networks as a way to deal with theft. GPS trackers, on the other hand, are typically marketed for just that purpose. How we tested Bluetooth trackers Before deciding on which trackers to test, we researched the field, looking at user reviews on Amazon, Best Buy and other retailers, along with discussions on sites like Reddit. We also checked out what other publications had to say on the matter before narrowing down our options. Here’s the full list of every tracker we tested: Apple AirTag Chipolo Card Spot Chipolo One Spot Chipolo One Chipolo Card Chipolo Loop Chipolo Pop HyperShield KeySmart SmartLock Motorola Moto Tag Pebblebee Clip 5 Pebblebee Clip Universal Pebblebee Clip Samsung SmartTag 2 Chipolo One Point Pebblebee Clip for Android Tile Pro (2024) Tile Mate (2024) Tile Mate (2022) Tile Pro (2022) Tile Slim (2022) After acquiring the trackers, I tested each one over the course of a few weeks using both an iPhone 11 followed by an iPhone 16 and a Samsung Galaxy S22 then an S23 Ultra. I recreated likely user experiences, such as losing and leaving items behind at home and out in the city. I planted trackers at different spots near downtown Albuquerque, mostly concentrated in and around the University of New Mexico and the surrounding neighborhood of Nob Hill. Later, I conducted tests in the Queen Anne neighborhood of Seattle. Each test was performed multiple times, both while walking and driving and I used the measure distance feature on Google Maps to track footage for alerts. I paid attention to how easy the app was to use, how reliable the phone-to-tracker connection was and any other perks and drawbacks that came up during regular use. As new trackers come to market, or as we learn of worthy models to try, I'll test them and add the results to this guide. Other Bluetooth trackers we tested Motorola Moto Tag The Moto Tag haunts me. At this very moment, my Galaxy phone says the fob is “Near you right now.” But I don’t know where. I tap to play a sound and the Find Hub tries, but ultimately says it can’t. I tap the Find Nearby function that’s supposed to visually guide you to the tag. I parade my phone around the house like a divining rod, take it down into the basement, walk it all over the garage. Nothing. But the Hub app unendingly says the Moto Tag is “Near you right now” and I get flashes of every old-school horror movie where the telephone operator tells the soon-to-be victim that the call is coming from inside the house. It’s partly my fault. I tend to keep good tabs on the gadgets I test for work. But during my most recent move, the tiny green disc didn’t make it into the safety of my review unit cabinet after relocation. Perhaps in retribution for my neglect, the Moto Tag keeps itself just out of reach. Taunting me. I’ll let you know if I ever find it, but in the meantime, it’s clear this finding device doesn’t want to be found. The recommended tags in this guide will serve you better. Tile Pro and Tile Mate (2024) Tile recently came out with a new suite of trackers, replacing the Tile Mate, Tile Pro, Tile Sticker and Tile Slim with updated models. In addition to fun new colors for the Mate and Slim, Tile added an SOS feature that can send a notification to your Life360 Circle when you triple press the button on the tracker. It’s a clever addition that turns your keys into a panic button, something offered by personal safety companies as standalone devices. There are a few caveats: You and the people you want to notify in an emergency will need the Life360 app installed on your phones. If you want your Tile to also trigger a call to emergency services, you’ll need a $15-per-month Life360 subscription (that’s in addition to a Tile membership, which starts at $3/month or $30 annually). And enabling the SOS triple-press disables the ability to ring your phone with the fob. I tested the SOS feature and it did indeed send a text message to my Circle, with the message that I had triggered an SOS and a link to a website that showed my current location. I thought it odd that the link didn’t open the Life360 app (which shows the location of users' phones), but I wasn’t as much concerned with Tile’s personal safety features as I was with the tracking capabilities, which turned out to be less than ideal. For my tests, I planted Tile trackers in a densely populated area of Seattle (about 15,000 people per square mile). After setting the trackers to “lost” in the Tile app, I waited. After four hours, one of the trackers was not discovered by the finding community, so I went and retrieved it. Another fob I planted alerted me that the tracker had been found by the Tile community after three hours — but the location it gave me was off by a third of a mile. I then decided to plant a tracker in the busiest place I could think of — the dried fruit and nuts aisle of a Trader Joes on a Friday evening before a major holiday. It still took over a half an hour before another Tile user anonymously pinged my lost tracker. In my tests with Samsung’s trackers and the fobs on Google’s Find Hub network, it took around ten minutes for them to be discovered. AirTags took half that time and all were tested in a far less populated city. Tile's four hours with no ping and over a half hour before getting a hit in a crowded TJs were pretty long stretches. Tile devices work with both mobile operating systems and its latest models are indeed louder than they were before. But they aren’t as quick to connect and you need to pay for a membership to activate left-behind alerts. And when you do, those notifications don’t kick in as quickly as they do with competing trackers. Bluetooth tracker FAQs Which Bluetooth tracker has the longest range? Both the Tile Pro and the Samsung Galaxy SmartTag2 claim a maximum range of around 400 feet, which is longer than the 300-foot claim for Chipolo’s Pop tags. The Pebblebee Clip 5 claims a 500-foot range, though other trackers with a shorter claimed range often performed better in our tests. Apple doesn’t make range claims for AirTags, but 30 meters (100 feet) seems to be the general consensus for those fobs. Any Bluetooth signal, of course, is dependent on a few factors. Obstacles like walls and people can block the signal, so a clear line of sight is the only way to achieve the maximum range. Other signals, like Wi-Fi, can also interfere with Bluetooth connections. Even high humidity can have an effect and lessen the distance at which your phone will connect to your tracker. Remember, when considering the range of Bluetooth trackers, the size of the “finding network” also comes into play. This is the number of nearby phones that can be used to anonymously ping your tracker when your own phone is out of Bluetooth range. As of now, Apple AirTags have the largest network, followed by Google’s Find Hub, Samsung’s finding community and finally, Tile’s Life360 members. What is the best Bluetooth tracker for a car? Bluetooth trackers are designed to track small, personal items like keys, jackets, backpacks and the like. All trackers have safeguards to prohibit the tag from being used to stalk people, so most will alert someone if a tracker that does not belong to them is detected following them. That means a car thief may get tipped off that there’s a tracker in the car they’re trying to steal. That said, you’ll see plenty of stories about people finding their car thanks to a Bluetooth tracker. Some police departments have even handed out trackers to combat high rates of carjacking. In most instances, the tracker of choice has been AirTags thanks to their wide finding network. If you’re looking for a tracker for your car, you may want to look into GPS trackers, some of which are designed for just that purpose. How accurate are Bluetooth trackers? Accuracy for Bluetooth trackers can be looked at in two ways: Finding items nearby and finding items misplaced outside your home. For nearby items, you’ll most often use the ring function on the device to hunt it down. Apple’s AirTags also use ultra-wideband technology, which creates directional navigation on your phone to get you within a foot of the tracker. Accurately finding lost items outside your home depends on the size of the finding network. Since this relies on the serendipity of a random phone passing within Bluetooth range of your tracker, the more phones on a given network, the better. And since Bluetooth ranges and distance estimates are only precise within about a meter or so, getting pings from more than one phone will help locating items. Here again, it’s worth noting that Apple’s Find My network is the largest, followed by Google, Samsung and Tile (both Chipolo and Pebblebee have fobs that work with the Apple and Google networks). Recent Updates February 2026: Added Pebblebee Clip 5 as the best rechargeable device. Added HyperShield tag as a budget pick. Updated FAQs for accuracy. October 2025: Added Chipolo Loop as a new pick for best rechargeable Bluetooth tracker. Detailed our experience with the Moto Tag and KeySmart SmartLock. Updated details about separation alerts and Ultra Wideband tech. August 2025: Updated the name of Google's finding network to Find Hub, instead of Find My Device. Added details about Pebblebee's new Alert feature. Added a table of contents. This article originally appeared on Engadget at https://www.engadget.com/computing/accessories/best-bluetooth-tracker-140028377.html?src=rss",
          "content": "Most people think of AirTags when they picture a Bluetooth tracker. And indeed, Apple’s little white discs were once the only capable option, relying on a vast finding network of nearby iPhones to pinpoint lost tags. But now Google has a finding network of its own, and third party brands like Chipolo, Hyper and Pebblebee have trackers that pair with your choice of Google or Apple’s network. That means you’ve got a lot of options for tagging and tracking your keys, backpacks, luggage and more. We tested the major brands out there to see how well they work, how loud they are and how they look to put together a guide to help you get the most out of your chosen tracker. Here are the best Bluetooth trackers you can buy.Editor's note: Apple just released a new version of its AirTag trackers. We are in the process of testing the new model and will update this guide once we're done. Best Bluetooth trackers for 2026 What to look for in a Bluetooth tracking device Bluetooth trackers are small discs or cards that rely on short-range, low-energy wireless signals to communicate with your smartphone. Attach one of these gadgets your stuff and, if it’s in range, your phone can “ring” the chip so you can find it. These tracking devices offer other features like separation alerts to tell you when you’ve left a tagged item behind, or where a lost item was last detected. Some can even tap into a larger network of smartphones to track down your device when you’re out of range. Depending on what you want the tracker to do, there are a few specs to look for when deciding which to get. Device compatibility Like most things from the folks in Cupertino, AirTags only work with products in the Apple ecosystem. Both Apple and Google have opened up access to the Find My and Find Hub networks to third-party manufacturers, including Chipolo and Pebblebee. Those two companies make device-agnostic models that will work with the larger tracking network from either brand, so iPhone and Android users can buy the same tag. Tile trackers work with either Android or Apple devices, but use Tile’s own Life 360 finding network. Samsung’s latest fob, the Galaxy SmartTag2, only works with Samsung phones and taps into a finding system that relies on other Samsung devices to locate lost tags. Finding network Crowd-sourced finding capabilities are what make headlines, with stories about recovering stolen equipment or tracking lost luggage across the globe. Using anonymous signals that ping other people’s devices, these Bluetooth tracking devices can potentially tell you where a tagged item is, even if your smartphone is out of Bluetooth range. Apple’s Find My network is the largest, with over a billion iPhones and iPads in service all running Apple’s Find My app by default. So unless an iPhone user opts out, their phone silently acts as a location detector for any nearby AirTags. Apple recently increased the AirTag’s finding power by enabling you to share the tracker's location with a third party, party, like an airline. Chipolo fobs that work on Apple’s network have the same ability. Google launched its Find My Device network in 2024 and has since renamed it Find Hub, which, like Apple's fining app, combines devices and people finding in one place. That network is now a close second for the largest in the US Now that Google’s Find Hub network is up and running, it’s a close second for the largest in the US. Like Apple, Android users are automatically part of the network, but can opt-out by selecting the Google services option in their phone’s Settings app and toggling the option in the Find Hub menu. Samsung’s SmartTag 2 and related network also defaults to an opt-in status for finding tags and other devices. Tile offers a large finding grid that includes Tile users, Amazon Sidewalk customers and people running the Life360 network. Life360 acquired Tile in 2021, and, according to the company, the Life360 network has more than 70 million monthly active users. In our tests, AirTags and third-party tags using its network, like the Chipolo Loop and Pop and the Pebblebee Clip 5, were the fastest to track down lost items. They offered nearly real-time location data in moderately to heavily trafficked spots around Albuquerque, including a bar, bookstore and coffee shops in Nob Hill, along with various outdoor hangouts on UNM’s campus. Samsung's SmartTags were able to locate our lost items most of the time, though not with the same precision finding accuracy as AirTags. When we tested Google’s Find Hub (then called Find My Device) network right after launch, it was noticeably slower than Apple’s network when using the community finding feature. Testing it again in mid 2025, the time it took to locate a lost item was considerably improved, taking less than 20 minutes on average for the community to track a fob. In our tests, Tile’s finding network wasn’t able to consistently locate its lost fobs. Amy Skorheim / Engadget Separation alerts A tracker’s day-to-day utility becomes really apparent when it prevents you from losing something in the first place. Separation alerts tell you when you’ve traveled too far from your tagged items. Useful if you want to make sure your laptop bag, jacket or umbrella always comes with you when you leave the house. Apple’s Find My app delivers these notifications, but Google’s Find Hub does not. However, if you have a Chipolo device and allow its companion app to run in the background on your Android phone, left-behind alerts are enabled. Tile trackers require a yearly subscription to enable the alerts (currently $7 to $25 monthly). Both AirTags and Tiles allow you to turn off separation alerts at certain locations, meaning you can set your home as a “safe” place where items can be left behind, but alerts will still trigger elsewhere. In our tests, AirTags and others using the Find My network alerted us between the 600- and 1,400-foot mark. Tiles sent a notification after about an average of 1,500 feet and were more consistent when using an Android phone than an iPhone. Chipolo Pop tags paired with an Android phone and using its own app sent an alert when we got around 450 feet away from our tagged item. Connectivity and volume The feature you may use most often is the key finder function, which makes the tracker ring when you hit a button in the app. With Apple's AirTags, you can say \"Hey Siri, where are my keys?\" and the assistant will ring the tag (assuming it doesn't mistakenly think you're asking for directions to the Floridian archipelago). You can also use the Find Item app in your Apple Watch to ring your fob. Asking smart home/personal assistants like Alexa or the Google Assistant to find your keys will work with Chipolo, Tile and Pebblebee trackers linked to your Android device. If you have your tag but can’t find your phone, some trackers will let you ring them to find your handset. SmartTag2 fobs reliably rang our Galaxy phone when we double-pressed it. Tile trackers have the same feature. Chipolo Pop and Loop trackers can ring your phone, but uses the Chipolo app to do so, which can run concurrently with the Find My or Find Hub connection. AirTags and third-party tags using Google’s network don’t offer this feature. The volume of the Bluetooth tracking device may determine whether you can find an item buried in your couch cushions or in a noisy room. AirTags have a reputation for being on the quiet side, and that aligned with what we saw (measuring roughly 65 decibels). Chipolo’s Pop tags and Tile’s Pro model measure between 83 and 86 decibels on average. Pebblebee’s new Clip 5 was the loudest of any tag we’ve tested, clocking in at 97 ear-splitting decibels. Design and alternative formats Design will determine what you can attach the tracker to. AirTags are small, smooth discs that can’t be secured to anything without accessories, which are numerous, but that is an additional cost to consider. Chipolo, Pebblebee and Tile offer trackers with holes that easily attach to your key ring, and all three companies also offer card-shaped versions designed to fit in your wallet. Pebblebee Clip 5 tags come with a handy carabiner-style key ring. You can even get trackers embedded into useful items like luggage locks. The SmartLock from KeySmart is a TSA-approved luggage lock, but in addition to the three digit code, it’s also a Bluetooth tracker that’s compatible with Apple Find My. It wasn’t quite as loud as other trackers in my tests, and the range wasn’t as long, but it paired easily and worked with Apple’s finding network just like an AirTag. Battery life AirTag, Tile Pro, SmartTag2, HyperShield and Chipolo Pop fobs use replaceable batteries and each should go for at least a year before needing to be swapped. Pebblebee Clip 5 and Chipolo Loop trackers are rechargeable via a standard USB-C port. The Clip 5 has a long battery life claim at 12 months. The Loop should go for six months on a charge. Trackers shaped like credit cards, aka wallet trackers, don't have replaceable batteries, but some, like the Chipolo Card and the Pebblebee Card 5 are USB-C rechargeable. Stalking, theft and data privacy AirTags have gotten a lot of attention and even prompted some lawsuits for Apple due to bad actors planting them on people in order to stalk them. While this fact may not influence your buying decision, any discussion of Bluetooth trackers should note what steps Apple, Google and Tile have taken to address the issue. Last year, all the major players in the Bluetooth tracker business teamed up to combat misuse and standardize how unauthorized tracking detection and alerts work for iOS and Android. Last year, Tile launched a feature called Anti-Theft Mode, which enables you to render one of its trackers undetectable by others. That means if someone steals your tagged item, they won’t be able to use the anti-stalking features to find and disable the tracker. That sort of negates one of the major ways potential stalking victims can stay safe, so Tile hopes ID verification and a $1 million penalty will deter misuse. As a theft deterrent, a Bluetooth tracker may or may not be the best option. Anecdotal stories abound in which people have recovered stolen goods using a tracker — but other tales are more cautionary. Neither Apple nor Google promotes its trackers or finding networks as a way to deal with theft. GPS trackers, on the other hand, are typically marketed for just that purpose. How we tested Bluetooth trackers Before deciding on which trackers to test, we researched the field, looking at user reviews on Amazon, Best Buy and other retailers, along with discussions on sites like Reddit. We also checked out what other publications had to say on the matter before narrowing down our options. Here’s the full list of every tracker we tested: Apple AirTag Chipolo Card Spot Chipolo One Spot Chipolo One Chipolo Card Chipolo Loop Chipolo Pop HyperShield KeySmart SmartLock Motorola Moto Tag Pebblebee Clip 5 Pebblebee Clip Universal Pebblebee Clip Samsung SmartTag 2 Chipolo One Point Pebblebee Clip for Android Tile Pro (2024) Tile Mate (2024) Tile Mate (2022) Tile Pro (2022) Tile Slim (2022) After acquiring the trackers, I tested each one over the course of a few weeks using both an iPhone 11 followed by an iPhone 16 and a Samsung Galaxy S22 then an S23 Ultra. I recreated likely user experiences, such as losing and leaving items behind at home and out in the city. I planted trackers at different spots near downtown Albuquerque, mostly concentrated in and around the University of New Mexico and the surrounding neighborhood of Nob Hill. Later, I conducted tests in the Queen Anne neighborhood of Seattle. Each test was performed multiple times, both while walking and driving and I used the measure distance feature on Google Maps to track footage for alerts. I paid attention to how easy the app was to use, how reliable the phone-to-tracker connection was and any other perks and drawbacks that came up during regular use. As new trackers come to market, or as we learn of worthy models to try, I'll test them and add the results to this guide. Other Bluetooth trackers we tested Motorola Moto Tag The Moto Tag haunts me. At this very moment, my Galaxy phone says the fob is “Near you right now.” But I don’t know where. I tap to play a sound and the Find Hub tries, but ultimately says it can’t. I tap the Find Nearby function that’s supposed to visually guide you to the tag. I parade my phone around the house like a divining rod, take it down into the basement, walk it all over the garage. Nothing. But the Hub app unendingly says the Moto Tag is “Near you right now” and I get flashes of every old-school horror movie where the telephone operator tells the soon-to-be victim that the call is coming from inside the house. It’s partly my fault. I tend to keep good tabs on the gadgets I test for work. But during my most recent move, the tiny green disc didn’t make it into the safety of my review unit cabinet after relocation. Perhaps in retribution for my neglect, the Moto Tag keeps itself just out of reach. Taunting me. I’ll let you know if I ever find it, but in the meantime, it’s clear this finding device doesn’t want to be found. The recommended tags in this guide will serve you better. Tile Pro and Tile Mate (2024) Tile recently came out with a new suite of trackers, replacing the Tile Mate, Tile Pro, Tile Sticker and Tile Slim with updated models. In addition to fun new colors for the Mate and Slim, Tile added an SOS feature that can send a notification to your Life360 Circle when you triple press the button on the tracker. It’s a clever addition that turns your keys into a panic button, something offered by personal safety companies as standalone devices. There are a few caveats: You and the people you want to notify in an emergency will need the Life360 app installed on your phones. If you want your Tile to also trigger a call to emergency services, you’ll need a $15-per-month Life360 subscription (that’s in addition to a Tile membership, which starts at $3/month or $30 annually). And enabling the SOS triple-press disables the ability to ring your phone with the fob. I tested the SOS feature and it did indeed send a text message to my Circle, with the message that I had triggered an SOS and a link to a website that showed my current location. I thought it odd that the link didn’t open the Life360 app (which shows the location of users' phones), but I wasn’t as much concerned with Tile’s personal safety features as I was with the tracking capabilities, which turned out to be less than ideal. For my tests, I planted Tile trackers in a densely populated area of Seattle (about 15,000 people per square mile). After setting the trackers to “lost” in the Tile app, I waited. After four hours, one of the trackers was not discovered by the finding community, so I went and retrieved it. Another fob I planted alerted me that the tracker had been found by the Tile community after three hours — but the location it gave me was off by a third of a mile. I then decided to plant a tracker in the busiest place I could think of — the dried fruit and nuts aisle of a Trader Joes on a Friday evening before a major holiday. It still took over a half an hour before another Tile user anonymously pinged my lost tracker. In my tests with Samsung’s trackers and the fobs on Google’s Find Hub network, it took around ten minutes for them to be discovered. AirTags took half that time and all were tested in a far less populated city. Tile's four hours with no ping and over a half hour before getting a hit in a crowded TJs were pretty long stretches. Tile devices work with both mobile operating systems and its latest models are indeed louder than they were before. But they aren’t as quick to connect and you need to pay for a membership to activate left-behind alerts. And when you do, those notifications don’t kick in as quickly as they do with competing trackers. Bluetooth tracker FAQs Which Bluetooth tracker has the longest range? Both the Tile Pro and the Samsung Galaxy SmartTag2 claim a maximum range of around 400 feet, which is longer than the 300-foot claim for Chipolo’s Pop tags. The Pebblebee Clip 5 claims a 500-foot range, though other trackers with a shorter claimed range often performed better in our tests. Apple doesn’t make range claims for AirTags, but 30 meters (100 feet) seems to be the general consensus for those fobs. Any Bluetooth signal, of course, is dependent on a few factors. Obstacles like walls and people can block the signal, so a clear line of sight is the only way to achieve the maximum range. Other signals, like Wi-Fi, can also interfere with Bluetooth connections. Even high humidity can have an effect and lessen the distance at which your phone will connect to your tracker. Remember, when considering the range of Bluetooth trackers, the size of the “finding network” also comes into play. This is the number of nearby phones that can be used to anonymously ping your tracker when your own phone is out of Bluetooth range. As of now, Apple AirTags have the largest network, followed by Google’s Find Hub, Samsung’s finding community and finally, Tile’s Life360 members. What is the best Bluetooth tracker for a car? Bluetooth trackers are designed to track small, personal items like keys, jackets, backpacks and the like. All trackers have safeguards to prohibit the tag from being used to stalk people, so most will alert someone if a tracker that does not belong to them is detected following them. That means a car thief may get tipped off that there’s a tracker in the car they’re trying to steal. That said, you’ll see plenty of stories about people finding their car thanks to a Bluetooth tracker. Some police departments have even handed out trackers to combat high rates of carjacking. In most instances, the tracker of choice has been AirTags thanks to their wide finding network. If you’re looking for a tracker for your car, you may want to look into GPS trackers, some of which are designed for just that purpose. How accurate are Bluetooth trackers? Accuracy for Bluetooth trackers can be looked at in two ways: Finding items nearby and finding items misplaced outside your home. For nearby items, you’ll most often use the ring function on the device to hunt it down. Apple’s AirTags also use ultra-wideband technology, which creates directional navigation on your phone to get you within a foot of the tracker. Accurately finding lost items outside your home depends on the size of the finding network. Since this relies on the serendipity of a random phone passing within Bluetooth range of your tracker, the more phones on a given network, the better. And since Bluetooth ranges and distance estimates are only precise within about a meter or so, getting pings from more than one phone will help locating items. Here again, it’s worth noting that Apple’s Find My network is the largest, followed by Google, Samsung and Tile (both Chipolo and Pebblebee have fobs that work with the Apple and Google networks). Recent Updates February 2026: Added Pebblebee Clip 5 as the best rechargeable device. Added HyperShield tag as a budget pick. Updated FAQs for accuracy. October 2025: Added Chipolo Loop as a new pick for best rechargeable Bluetooth tracker. Detailed our experience with the Moto Tag and KeySmart SmartLock. Updated details about separation alerts and Ultra Wideband tech. August 2025: Updated the name of Google's finding network to Find Hub, instead of Find My Device. Added details about Pebblebee's new Alert feature. Added a table of contents. This article originally appeared on Engadget at https://www.engadget.com/computing/accessories/best-bluetooth-tracker-140028377.html?src=rss",
          "feed_position": 30,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2023-02/94489620-abc7-11ed-b375-842957054bf1"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/orchestration/openai-launches-a-codex-desktop-app-for-macos-to-run-multiple-ai-coding",
          "published_at": "Mon, 02 Feb 2026 18:00:00 GMT",
          "title": "OpenAI launches a Codex desktop app for macOS to run multiple AI coding agents in parallel",
          "standfirst": "OpenAI on Monday released a new desktop application for its Codex artificial intelligence coding system, a tool the company says transforms software development from a collaborative exercise with a single AI assistant into something more akin to managing a team of autonomous workers.The Codex app for macOS functions as what OpenAI executives describe as a \"command center for agents,\" allowing developers to delegate multiple coding tasks simultaneously, automate repetitive work, and supervise AI systems that can run for up to 30 minutes independently before returning completed code.\"This is the most loved internal product we&#x27;ve ever had,\" Sam Altman, OpenAI&#x27;s chief executive, told VentureBeat in a press briefing ahead of Monday&#x27;s launch. \"It&#x27;s been totally an amazing thing for us to be using recently at OpenAI.\"The release arrives at a pivotal moment for the enterprise AI market. According to a survey of 100 Global 2000 companies published last week by venture capital firm Andreessen Horowitz, 78% of enterprise CIOs now use OpenAI models in production, though competitors Anthropic and Google are gaining ground rapidly. Anthropic posted the largest share increase of any frontier lab since May 2025, growing 25% in enterprise penetration, with 44% of enterprises now using Anthropic in production.The timing of OpenAI&#x27;s Codex app launch — with its focus on professional software engineering workflows — appears designed to defend the company&#x27;s position in what has become the most contested segment of the AI market: coding tools.Why developers are abandoning their IDEs for AI agent managementThe Codex app introduces a fundamentally different approach to AI-assisted coding. While previous tools like GitHub Copilot focused on autocompleting lines of code in real-time, the new application enables developers to \"effortlessly manage multiple agents at once, run work in parallel, and collaborate with agents over long-running tasks.\"Alexander Embiricos, the product lead for Codex, explained the evolution during the press briefing by tracing the product&#x27;s lineage back to 2021, when OpenAI first introduced a model called Codex that powered GitHub Copilot.\"Back then, people were using AI to write small chunks of code in their IDEs,\" Embiricos said. \"GPT-5 in August last year was a big jump, and then 5.2 in December was another massive jump, where people started doing longer and longer tasks, asking models to do work end to end. So what we saw is that developers, instead of working closely with the model, pair coding, they started delegating entire features.\"The shift has been so profound that Altman said he recently completed a substantial coding project without ever opening a traditional integrated development environment.\"I was astonished by this…I did this fairly big project in a few days earlier this week and over the weekend. I did not open an IDE during the process. Not a single time,\" Altman said. \"I did look at some code, but I was not doing it the old-fashioned way, and I did not think that was going to be happening by now.\"How skills and automations extend AI coding beyond simple code generationThe Codex app introduces several new capabilities designed to extend AI coding beyond writing lines of code. Chief among these are \"Skills,\" which bundle instructions, resources, and scripts so that Codex can \"reliably connect to tools, run workflows, and complete tasks according to your team&#x27;s preferences.\"The app includes a dedicated interface for creating and managing skills, and users can explicitly invoke specific skills or allow the system to automatically select them based on the task at hand. OpenAI has published a library of skills for common workflows, including tools to fetch design context from Figma, manage projects in Linear, deploy web applications to cloud hosts like Cloudflare and Vercel, generate images using GPT Image, and create professional documents in PDF, spreadsheet, and Word formats.To demonstrate the system&#x27;s capabilities, OpenAI asked Codex to build a racing game from a single prompt. Using an image generation skill and a web game development skill, Codex built the game by working independently using more than 7 million tokens with just one initial user prompt, taking on \"the roles of designer, game developer, and QA tester to validate its work by actually playing the game.\"The company has also introduced \"Automations,\" which allow developers to schedule Codex to work in the background on an automatic schedule. \"When an Automation finishes, the results land in a review queue so you can jump back in and continue working if needed.\"Thibault Sottiaux, who leads the Codex team at OpenAI, described how the company uses these automations internally: \"We&#x27;ve been using Automations to handle the repetitive but important tasks, like daily issue triage, finding and summarizing CI failures, generating daily release briefs, checking for bugs, and more.\"The app also includes built-in support for \"worktrees,\" allowing multiple agents to work on the same repository without conflicts. \"Each agent works on an isolated copy of your code, allowing you to explore different paths without needing to track how they impact your codebase.\"OpenAI battles Anthropic and Google for control of enterprise AI spendingThe launch comes as enterprise spending on AI coding tools accelerates dramatically. According to the Andreessen Horowitz survey, average enterprise AI spend on large language models has risen from approximately $4.5 million to $7 million over the last two years, with enterprises expecting growth of another 65% this year to approximately $11.6 million.Leadership in the enterprise AI market varies significantly by use case. OpenAI dominates \"early, horizontal use cases like general purpose chatbots, enterprise knowledge management and customer support,\" while Anthropic leads in \"software development and data analysis, where CIOs consistently cite rapid capability gains since the second half of 2024.\"When asked during the press briefing how Codex differentiates from Anthropic&#x27;s Claude Code, which has been described as having its \"ChatGPT moment,\" Sottiaux emphasized OpenAI&#x27;s focus on model capability for long-running tasks.\"One of the things that our models are extremely good at—they really sit at the frontier of intelligence and doing reliable work for long periods of time,\" Sottiaux said. \"This is also what we&#x27;re optimizing this new surface to be very good at, so that you can start many parallel agents and coordinate them over long periods of time and not get lost.\"Altman added that while many tools can handle \"vibe coding front ends,\" OpenAI&#x27;s 5.2 model remains \"the strongest model by far\" for sophisticated work on complex systems.\"Taking that level of model capability and putting it in an interface where you can do what Thibault was saying, we think is going to matter quite a bit,\" Altman said. \"That&#x27;s probably the, at least listening to users and sort of looking at the chatter on social that&#x27;s that&#x27;s the single biggest differentiator.\"The surprising satisfies on AI progress: how fast humans can typeThe philosophical underpinning of the Codex app reflects a view that OpenAI executives have been articulating for months: that human limitations — not AI capabilities — now constitute the primary constraint on productivity.In a December appearance on Lenny’s Podcast, Embiricos described human typing speed as \"the current underappreciated limiting factor\" to achieving artificial general intelligence. The logic: if AI can perform complex coding tasks but humans can&#x27;t write prompts or review outputs fast enough, progress stalls.The Codex app attempts to address this by enabling what the team calls an \"abundance mindset\" — running multiple tasks in parallel rather than perfecting single requests. During the briefing, Embiricos described how power users at OpenAI work with the tool.\"Last night, I was working on the app, and I was making a few changes, and all of these changes are able to run in parallel together. And I was just sort of going between them, managing them,\" Embiricos said. \"Behind the scenes, all these tasks are running on something called gate work trees, which means that the agents are running independently, and you don&#x27;t have to manage them.\"In the Sequoia Capital podcast \"Training Data,\" Embiricos elaborated on this mindset shift: \"The mindset that works really well for Codex is, like, kind of like this abundance mindset and, like, hey, let&#x27;s try anything. Let&#x27;s try anything even multiple times and see what works.\" He noted that when users run 20 or more tasks in a day or an hour, \"they&#x27;ve probably understood basically how to use the tool.\"Building trust through sandboxes: how OpenAI secures autonomous coding agentsOpenAI has built security measures into the Codex architecture from the ground up. The app uses \"native, open-source and configurable system-level sandboxing,\" and by default, \"Codex agents are limited to editing files in the folder or branch where they&#x27;re working and using cached web search, then asking for permission to run commands that require elevated permissions like network access.\"Embiricos elaborated on the security approach during the briefing, noting that OpenAI has open-sourced its sandbox technology.\"Codex has this sandbox that we&#x27;re actually incredibly proud of, and it&#x27;s open source, so you can go check it out,\" Embiricos said. The sandbox \"basically ensures that when the agent is working on your computer, it can only make writes in a specific folder that you want it to make rights into, and it doesn&#x27;t access network without information.\"The system also includes a granular permission model that allows users to configure persistent approvals for specific actions, avoiding the need to repeatedly authorize routine operations. \"If the agent wants to do something and you find yourself annoyed that you&#x27;re constantly having to approve it, instead of just saying, &#x27;All right, you can do everything,&#x27; you can just say, &#x27;Hey, remember this one thing — I&#x27;m actually okay with you doing this going forward,&#x27;\" Embiricos explained.Altman emphasized that the permission architecture signals a broader philosophy about AI safety in agentic systems.\"I think this is going to be really important. I mean, it&#x27;s been so clear to us using this, how much you want it to have control of your computer, and how much you need it,\" Altman said. \"And the way the team built Codex such that you can sensibly limit what&#x27;s happening and also pick the level of control you&#x27;re comfortable with is important.\"He also acknowledged the dual-use nature of the technology. \"We do expect to get to our internal cybersecurity high moment of our models very soon. We&#x27;ve been preparing for this. We&#x27;ve talked about our mitigation plan,\" Altman said. \"A real thing for the world to contend with is going to be defending against a lot of capable cybersecurity threats using these models very quickly.\"The same capabilities that make Codex valuable for fixing bugs and refactoring code could, in the wrong hands, be used to discover vulnerabilities or write malicious software—a tension that will only intensify as AI coding agents become more capable.From Android apps to research breakthroughs: how Codex transformed OpenAI&#x27;s own operationsPerhaps the most compelling evidence for Codex&#x27;s capabilities comes from OpenAI&#x27;s own use of the tool. Sottiaux described how the system has accelerated internal development.\"A Sora Android app is an example of that where four engineers shipped in only 18 days internally, and then within the month we give access to the world,\" Sottiaux said. \"I had never noticed such speed at this scale before.\"Beyond product development, Sottiaux described how Codex has become integral to OpenAI&#x27;s research operations.\"Codex is really involved in all parts of the research — making new data sets, investigating its own screening runs,\" he said. \"When I sit in meetings with researchers, they all send Codex off to do an investigation while we&#x27;re having a chat, and then it will come back with useful information, and we&#x27;re able to debug much faster.\"The tool has also begun contributing to its own development. \"Codex also is starting to build itself,\" Sottiaux noted. \"There&#x27;s no screen within the Codex engineering team that doesn&#x27;t have Codex running on multiple, six, eight, ten, tasks at a time.\"When asked whether this constitutes evidence of \"recursive self-improvement\" — a concept that has long concerned AI safety researchers — Sottiaux was measured in his response.\"There is a human in the loop at all times,\" he said. \"I wouldn&#x27;t necessarily call it recursive self-improvement, a glimpse into the future there.\"Altman offered a more expansive view of the research implications.\"There&#x27;s two parts of what people talk about when they talk about automating research to a degree where you can imagine that happening,\" Altman said. \"One is, can you write software, extremely complex infrastructure, software to run training jobs across hundreds of thousands of GPUs and babysit them. And the second is, can you come up with the new scientific ideas that make algorithms more efficient.\"He noted that OpenAI is \"seeing early but promising signs on both of those.\"The end of technical debt? AI agents take on the work engineers hate mostOne of the more unexpected applications of Codex has been addressing technical debt — the accumulated maintenance burden that plagues most software projects.Altman described how AI coding agents excel at the unglamorous work that human engineers typically avoid.\"The kind of work that human engineers hate to do — go refactor this, clean up this code base, rewrite this, write this test — this is where the model doesn&#x27;t care. The model will do anything, whether it&#x27;s fun or not,\" Altman said.He reported that some infrastructure teams at OpenAI that \"had sort of like, given up hope that you were ever really going to long term win the war against tech debt, are now like, we&#x27;re going to win this, because the model is going to constantly be working behind us, making sure we have great test coverage, making sure that we refactor when we&#x27;re supposed to.\"The observation speaks to a broader theme that emerged repeatedly during the briefing: AI coding agents don&#x27;t experience the motivational fluctuations that affect human programmers. As Altman noted, a team member recently observed that \"the hardest mental adjustment to make about working with these sort of like aI coding teammates, unlike a human, is the models just don&#x27;t run out of dopamine. They keep trying. They don&#x27;t run out of motivation. They don&#x27;t get, you know, they don&#x27;t lose energy when something&#x27;s not working. They just keep going and, you know, they figure out how to get it done.\"What the Codex app costs and who can use it starting todayThe Codex app launches today on macOS and is available to anyone with a ChatGPT Plus, Pro, Business, Enterprise, or Edu subscription. Usage is included in ChatGPT subscriptions, with the option to purchase additional credits if needed.In a promotional push, OpenAI is temporarily making Codex available to ChatGPT Free and Go users \"to help more people try agentic workflows.\" The company is also doubling rate limits for existing Codex users across all paid plans during this promotional period.The pricing strategy reflects OpenAI&#x27;s determination to establish Codex as the default tool for AI-assisted development before competitors can gain further traction. More than a million developers have used Codex in the past month, and usage has nearly doubled since the launch of GPT-5.2-Codex in mid-December, building on more than 20x usage growth since August 2025.Customers using Codex include large enterprises like Cisco, Ramp, Virgin Atlantic, Vanta, Duolingo, and Gap, as well as startups like Harvey, Sierra, and Wonderful. Individual developers have also embraced the tool: Peter Steinberger, creator of OpenClaw, built the project entirely with Codex and reports that since fully switching to the tool, his productivity has roughly doubled across more than 82,000 GitHub contributions.OpenAI&#x27;s ambitious roadmap: Windows support, cloud triggers, and continuous background agentsOpenAI outlined an aggressive development roadmap for Codex. The company plans to make the app available on Windows, continue pushing \"the frontier of model capabilities,\" and roll out faster inference.Within the app, OpenAI will \"keep refining multi-agent workflows based on real-world feedback\" and is \"building out Automations with support for cloud-based triggers, so Codex can run continuously in the background—not just when your computer is open.\"The company also announced a new \"plan mode\" feature that allows Codex to read through complex changes in read-only mode, then discuss with the user before executing. \"This means that it lets you build a lot of confidence before, again, sending it to do a lot of work by itself, independently, in parallel to you,\" Embiricos explained.Additionally, OpenAI is introducing customizable personalities for Codex. \"The default personality for Codex has been quite terse. A lot of people love it, but some people want something more engaging,\" Embiricos said. Users can access the new personalities using the /personality command.Altman also hinted at future integration with ChatGPT&#x27;s broader ecosystem.\"There will be all kinds of cool things we can do over time to connect people&#x27;s ChatGPT accounts and leverage sort of all the history they&#x27;ve built up there,\" Altman said.Microsoft still dominates enterprise AI, but the window for disruption is openThe Codex app launch occurs as most enterprises have moved beyond single-vendor strategies. According to the Andreessen Horowitz survey, \"81% now use three or more model families in testing or production, up from 68% less than a year ago.\"Despite the proliferation of AI coding tools, Microsoft continues to dominate enterprise adoption through its existing relationships. \"Microsoft 365 Copilot leads enterprise chat though ChatGPT has closed the gap meaningfully,\" and \"Github Copilot is still the coding leader for enterprises.\" The survey found that \"65% of enterprises noted they preferred to go with incumbent solutions when available,\" citing trust, integration, and procurement simplicity.However, the survey also suggests significant opportunity for challengers: \"Enterprises consistently say they value faster innovation, deeper AI focus, and greater flexibility paired with cutting edge capabilities that AI native startups bring.\"OpenAI appears to be positioning Codex as a bridge between these worlds. \"Codex is built on a simple premise: everything is controlled by code,\" the company stated. \"The better an agent is at reasoning about and producing code, the more capable it becomes across all forms of technical and knowledge work.\"The company&#x27;s ambition extends beyond coding. \"We&#x27;ve focused on making Codex the best coding agent, which has also laid the foundation for it to become a strong agent for a broad range of knowledge work tasks that extend beyond writing code.\"When asked whether AI coding tools could eventually move beyond early adopters to become mainstream, Altman suggested the transition may be closer than many expect.\"Can it go from vibe coding to serious software engineering? That&#x27;s what this is about,\" Altman said. \"I think we are over the bar on that. I think this will be the way that most serious coders do their job — and very rapidly from now.\"He then pivoted to an even bolder prediction: that code itself could become the universal interface for all computer-based work.\"Code is a universal language to get computers to do what you want. And it&#x27;s gotten so good that I think, very quickly, we can go not just from vibe coding silly apps but to doing all the non-coding knowledge work,\" Altman said.At the close of the briefing, Altman urged journalists to try the product themselves: \"Please try the app. There&#x27;s no way to get this across just by talking about it. It&#x27;s a crazy amount of power.\"For developers who have spent careers learning to write code, the message was clear: the future belongs to those who learn to manage the machines that write it for them.",
          "content": "OpenAI on Monday released a new desktop application for its Codex artificial intelligence coding system, a tool the company says transforms software development from a collaborative exercise with a single AI assistant into something more akin to managing a team of autonomous workers.The Codex app for macOS functions as what OpenAI executives describe as a \"command center for agents,\" allowing developers to delegate multiple coding tasks simultaneously, automate repetitive work, and supervise AI systems that can run for up to 30 minutes independently before returning completed code.\"This is the most loved internal product we&#x27;ve ever had,\" Sam Altman, OpenAI&#x27;s chief executive, told VentureBeat in a press briefing ahead of Monday&#x27;s launch. \"It&#x27;s been totally an amazing thing for us to be using recently at OpenAI.\"The release arrives at a pivotal moment for the enterprise AI market. According to a survey of 100 Global 2000 companies published last week by venture capital firm Andreessen Horowitz, 78% of enterprise CIOs now use OpenAI models in production, though competitors Anthropic and Google are gaining ground rapidly. Anthropic posted the largest share increase of any frontier lab since May 2025, growing 25% in enterprise penetration, with 44% of enterprises now using Anthropic in production.The timing of OpenAI&#x27;s Codex app launch — with its focus on professional software engineering workflows — appears designed to defend the company&#x27;s position in what has become the most contested segment of the AI market: coding tools.Why developers are abandoning their IDEs for AI agent managementThe Codex app introduces a fundamentally different approach to AI-assisted coding. While previous tools like GitHub Copilot focused on autocompleting lines of code in real-time, the new application enables developers to \"effortlessly manage multiple agents at once, run work in parallel, and collaborate with agents over long-running tasks.\"Alexander Embiricos, the product lead for Codex, explained the evolution during the press briefing by tracing the product&#x27;s lineage back to 2021, when OpenAI first introduced a model called Codex that powered GitHub Copilot.\"Back then, people were using AI to write small chunks of code in their IDEs,\" Embiricos said. \"GPT-5 in August last year was a big jump, and then 5.2 in December was another massive jump, where people started doing longer and longer tasks, asking models to do work end to end. So what we saw is that developers, instead of working closely with the model, pair coding, they started delegating entire features.\"The shift has been so profound that Altman said he recently completed a substantial coding project without ever opening a traditional integrated development environment.\"I was astonished by this…I did this fairly big project in a few days earlier this week and over the weekend. I did not open an IDE during the process. Not a single time,\" Altman said. \"I did look at some code, but I was not doing it the old-fashioned way, and I did not think that was going to be happening by now.\"How skills and automations extend AI coding beyond simple code generationThe Codex app introduces several new capabilities designed to extend AI coding beyond writing lines of code. Chief among these are \"Skills,\" which bundle instructions, resources, and scripts so that Codex can \"reliably connect to tools, run workflows, and complete tasks according to your team&#x27;s preferences.\"The app includes a dedicated interface for creating and managing skills, and users can explicitly invoke specific skills or allow the system to automatically select them based on the task at hand. OpenAI has published a library of skills for common workflows, including tools to fetch design context from Figma, manage projects in Linear, deploy web applications to cloud hosts like Cloudflare and Vercel, generate images using GPT Image, and create professional documents in PDF, spreadsheet, and Word formats.To demonstrate the system&#x27;s capabilities, OpenAI asked Codex to build a racing game from a single prompt. Using an image generation skill and a web game development skill, Codex built the game by working independently using more than 7 million tokens with just one initial user prompt, taking on \"the roles of designer, game developer, and QA tester to validate its work by actually playing the game.\"The company has also introduced \"Automations,\" which allow developers to schedule Codex to work in the background on an automatic schedule. \"When an Automation finishes, the results land in a review queue so you can jump back in and continue working if needed.\"Thibault Sottiaux, who leads the Codex team at OpenAI, described how the company uses these automations internally: \"We&#x27;ve been using Automations to handle the repetitive but important tasks, like daily issue triage, finding and summarizing CI failures, generating daily release briefs, checking for bugs, and more.\"The app also includes built-in support for \"worktrees,\" allowing multiple agents to work on the same repository without conflicts. \"Each agent works on an isolated copy of your code, allowing you to explore different paths without needing to track how they impact your codebase.\"OpenAI battles Anthropic and Google for control of enterprise AI spendingThe launch comes as enterprise spending on AI coding tools accelerates dramatically. According to the Andreessen Horowitz survey, average enterprise AI spend on large language models has risen from approximately $4.5 million to $7 million over the last two years, with enterprises expecting growth of another 65% this year to approximately $11.6 million.Leadership in the enterprise AI market varies significantly by use case. OpenAI dominates \"early, horizontal use cases like general purpose chatbots, enterprise knowledge management and customer support,\" while Anthropic leads in \"software development and data analysis, where CIOs consistently cite rapid capability gains since the second half of 2024.\"When asked during the press briefing how Codex differentiates from Anthropic&#x27;s Claude Code, which has been described as having its \"ChatGPT moment,\" Sottiaux emphasized OpenAI&#x27;s focus on model capability for long-running tasks.\"One of the things that our models are extremely good at—they really sit at the frontier of intelligence and doing reliable work for long periods of time,\" Sottiaux said. \"This is also what we&#x27;re optimizing this new surface to be very good at, so that you can start many parallel agents and coordinate them over long periods of time and not get lost.\"Altman added that while many tools can handle \"vibe coding front ends,\" OpenAI&#x27;s 5.2 model remains \"the strongest model by far\" for sophisticated work on complex systems.\"Taking that level of model capability and putting it in an interface where you can do what Thibault was saying, we think is going to matter quite a bit,\" Altman said. \"That&#x27;s probably the, at least listening to users and sort of looking at the chatter on social that&#x27;s that&#x27;s the single biggest differentiator.\"The surprising satisfies on AI progress: how fast humans can typeThe philosophical underpinning of the Codex app reflects a view that OpenAI executives have been articulating for months: that human limitations — not AI capabilities — now constitute the primary constraint on productivity.In a December appearance on Lenny’s Podcast, Embiricos described human typing speed as \"the current underappreciated limiting factor\" to achieving artificial general intelligence. The logic: if AI can perform complex coding tasks but humans can&#x27;t write prompts or review outputs fast enough, progress stalls.The Codex app attempts to address this by enabling what the team calls an \"abundance mindset\" — running multiple tasks in parallel rather than perfecting single requests. During the briefing, Embiricos described how power users at OpenAI work with the tool.\"Last night, I was working on the app, and I was making a few changes, and all of these changes are able to run in parallel together. And I was just sort of going between them, managing them,\" Embiricos said. \"Behind the scenes, all these tasks are running on something called gate work trees, which means that the agents are running independently, and you don&#x27;t have to manage them.\"In the Sequoia Capital podcast \"Training Data,\" Embiricos elaborated on this mindset shift: \"The mindset that works really well for Codex is, like, kind of like this abundance mindset and, like, hey, let&#x27;s try anything. Let&#x27;s try anything even multiple times and see what works.\" He noted that when users run 20 or more tasks in a day or an hour, \"they&#x27;ve probably understood basically how to use the tool.\"Building trust through sandboxes: how OpenAI secures autonomous coding agentsOpenAI has built security measures into the Codex architecture from the ground up. The app uses \"native, open-source and configurable system-level sandboxing,\" and by default, \"Codex agents are limited to editing files in the folder or branch where they&#x27;re working and using cached web search, then asking for permission to run commands that require elevated permissions like network access.\"Embiricos elaborated on the security approach during the briefing, noting that OpenAI has open-sourced its sandbox technology.\"Codex has this sandbox that we&#x27;re actually incredibly proud of, and it&#x27;s open source, so you can go check it out,\" Embiricos said. The sandbox \"basically ensures that when the agent is working on your computer, it can only make writes in a specific folder that you want it to make rights into, and it doesn&#x27;t access network without information.\"The system also includes a granular permission model that allows users to configure persistent approvals for specific actions, avoiding the need to repeatedly authorize routine operations. \"If the agent wants to do something and you find yourself annoyed that you&#x27;re constantly having to approve it, instead of just saying, &#x27;All right, you can do everything,&#x27; you can just say, &#x27;Hey, remember this one thing — I&#x27;m actually okay with you doing this going forward,&#x27;\" Embiricos explained.Altman emphasized that the permission architecture signals a broader philosophy about AI safety in agentic systems.\"I think this is going to be really important. I mean, it&#x27;s been so clear to us using this, how much you want it to have control of your computer, and how much you need it,\" Altman said. \"And the way the team built Codex such that you can sensibly limit what&#x27;s happening and also pick the level of control you&#x27;re comfortable with is important.\"He also acknowledged the dual-use nature of the technology. \"We do expect to get to our internal cybersecurity high moment of our models very soon. We&#x27;ve been preparing for this. We&#x27;ve talked about our mitigation plan,\" Altman said. \"A real thing for the world to contend with is going to be defending against a lot of capable cybersecurity threats using these models very quickly.\"The same capabilities that make Codex valuable for fixing bugs and refactoring code could, in the wrong hands, be used to discover vulnerabilities or write malicious software—a tension that will only intensify as AI coding agents become more capable.From Android apps to research breakthroughs: how Codex transformed OpenAI&#x27;s own operationsPerhaps the most compelling evidence for Codex&#x27;s capabilities comes from OpenAI&#x27;s own use of the tool. Sottiaux described how the system has accelerated internal development.\"A Sora Android app is an example of that where four engineers shipped in only 18 days internally, and then within the month we give access to the world,\" Sottiaux said. \"I had never noticed such speed at this scale before.\"Beyond product development, Sottiaux described how Codex has become integral to OpenAI&#x27;s research operations.\"Codex is really involved in all parts of the research — making new data sets, investigating its own screening runs,\" he said. \"When I sit in meetings with researchers, they all send Codex off to do an investigation while we&#x27;re having a chat, and then it will come back with useful information, and we&#x27;re able to debug much faster.\"The tool has also begun contributing to its own development. \"Codex also is starting to build itself,\" Sottiaux noted. \"There&#x27;s no screen within the Codex engineering team that doesn&#x27;t have Codex running on multiple, six, eight, ten, tasks at a time.\"When asked whether this constitutes evidence of \"recursive self-improvement\" — a concept that has long concerned AI safety researchers — Sottiaux was measured in his response.\"There is a human in the loop at all times,\" he said. \"I wouldn&#x27;t necessarily call it recursive self-improvement, a glimpse into the future there.\"Altman offered a more expansive view of the research implications.\"There&#x27;s two parts of what people talk about when they talk about automating research to a degree where you can imagine that happening,\" Altman said. \"One is, can you write software, extremely complex infrastructure, software to run training jobs across hundreds of thousands of GPUs and babysit them. And the second is, can you come up with the new scientific ideas that make algorithms more efficient.\"He noted that OpenAI is \"seeing early but promising signs on both of those.\"The end of technical debt? AI agents take on the work engineers hate mostOne of the more unexpected applications of Codex has been addressing technical debt — the accumulated maintenance burden that plagues most software projects.Altman described how AI coding agents excel at the unglamorous work that human engineers typically avoid.\"The kind of work that human engineers hate to do — go refactor this, clean up this code base, rewrite this, write this test — this is where the model doesn&#x27;t care. The model will do anything, whether it&#x27;s fun or not,\" Altman said.He reported that some infrastructure teams at OpenAI that \"had sort of like, given up hope that you were ever really going to long term win the war against tech debt, are now like, we&#x27;re going to win this, because the model is going to constantly be working behind us, making sure we have great test coverage, making sure that we refactor when we&#x27;re supposed to.\"The observation speaks to a broader theme that emerged repeatedly during the briefing: AI coding agents don&#x27;t experience the motivational fluctuations that affect human programmers. As Altman noted, a team member recently observed that \"the hardest mental adjustment to make about working with these sort of like aI coding teammates, unlike a human, is the models just don&#x27;t run out of dopamine. They keep trying. They don&#x27;t run out of motivation. They don&#x27;t get, you know, they don&#x27;t lose energy when something&#x27;s not working. They just keep going and, you know, they figure out how to get it done.\"What the Codex app costs and who can use it starting todayThe Codex app launches today on macOS and is available to anyone with a ChatGPT Plus, Pro, Business, Enterprise, or Edu subscription. Usage is included in ChatGPT subscriptions, with the option to purchase additional credits if needed.In a promotional push, OpenAI is temporarily making Codex available to ChatGPT Free and Go users \"to help more people try agentic workflows.\" The company is also doubling rate limits for existing Codex users across all paid plans during this promotional period.The pricing strategy reflects OpenAI&#x27;s determination to establish Codex as the default tool for AI-assisted development before competitors can gain further traction. More than a million developers have used Codex in the past month, and usage has nearly doubled since the launch of GPT-5.2-Codex in mid-December, building on more than 20x usage growth since August 2025.Customers using Codex include large enterprises like Cisco, Ramp, Virgin Atlantic, Vanta, Duolingo, and Gap, as well as startups like Harvey, Sierra, and Wonderful. Individual developers have also embraced the tool: Peter Steinberger, creator of OpenClaw, built the project entirely with Codex and reports that since fully switching to the tool, his productivity has roughly doubled across more than 82,000 GitHub contributions.OpenAI&#x27;s ambitious roadmap: Windows support, cloud triggers, and continuous background agentsOpenAI outlined an aggressive development roadmap for Codex. The company plans to make the app available on Windows, continue pushing \"the frontier of model capabilities,\" and roll out faster inference.Within the app, OpenAI will \"keep refining multi-agent workflows based on real-world feedback\" and is \"building out Automations with support for cloud-based triggers, so Codex can run continuously in the background—not just when your computer is open.\"The company also announced a new \"plan mode\" feature that allows Codex to read through complex changes in read-only mode, then discuss with the user before executing. \"This means that it lets you build a lot of confidence before, again, sending it to do a lot of work by itself, independently, in parallel to you,\" Embiricos explained.Additionally, OpenAI is introducing customizable personalities for Codex. \"The default personality for Codex has been quite terse. A lot of people love it, but some people want something more engaging,\" Embiricos said. Users can access the new personalities using the /personality command.Altman also hinted at future integration with ChatGPT&#x27;s broader ecosystem.\"There will be all kinds of cool things we can do over time to connect people&#x27;s ChatGPT accounts and leverage sort of all the history they&#x27;ve built up there,\" Altman said.Microsoft still dominates enterprise AI, but the window for disruption is openThe Codex app launch occurs as most enterprises have moved beyond single-vendor strategies. According to the Andreessen Horowitz survey, \"81% now use three or more model families in testing or production, up from 68% less than a year ago.\"Despite the proliferation of AI coding tools, Microsoft continues to dominate enterprise adoption through its existing relationships. \"Microsoft 365 Copilot leads enterprise chat though ChatGPT has closed the gap meaningfully,\" and \"Github Copilot is still the coding leader for enterprises.\" The survey found that \"65% of enterprises noted they preferred to go with incumbent solutions when available,\" citing trust, integration, and procurement simplicity.However, the survey also suggests significant opportunity for challengers: \"Enterprises consistently say they value faster innovation, deeper AI focus, and greater flexibility paired with cutting edge capabilities that AI native startups bring.\"OpenAI appears to be positioning Codex as a bridge between these worlds. \"Codex is built on a simple premise: everything is controlled by code,\" the company stated. \"The better an agent is at reasoning about and producing code, the more capable it becomes across all forms of technical and knowledge work.\"The company&#x27;s ambition extends beyond coding. \"We&#x27;ve focused on making Codex the best coding agent, which has also laid the foundation for it to become a strong agent for a broad range of knowledge work tasks that extend beyond writing code.\"When asked whether AI coding tools could eventually move beyond early adopters to become mainstream, Altman suggested the transition may be closer than many expect.\"Can it go from vibe coding to serious software engineering? That&#x27;s what this is about,\" Altman said. \"I think we are over the bar on that. I think this will be the way that most serious coders do their job — and very rapidly from now.\"He then pivoted to an even bolder prediction: that code itself could become the universal interface for all computer-based work.\"Code is a universal language to get computers to do what you want. And it&#x27;s gotten so good that I think, very quickly, we can go not just from vibe coding silly apps but to doing all the non-coding knowledge work,\" Altman said.At the close of the briefing, Altman urged journalists to try the product themselves: \"Please try the app. There&#x27;s no way to get this across just by talking about it. It&#x27;s a crazy amount of power.\"For developers who have spent careers learning to write code, the message was clear: the future belongs to those who learn to manage the machines that write it for them.",
          "feed_position": 5,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/5ixq6lS6l5yrO3RJxCtr5w/c11ddd394c8969826452a5a30312234b/nuneybits_Vector_art_of_an_Apple_iMac_monitor_displaying_cascad_ecce1621-251d-41d9-ac6e-72eb25b2fd35.webp?w=300&q=30"
        }
      ],
      "featured_image": "https://images.ctfassets.net/jdtwqhzvc2n1/7ErEzHma6LbYLaznwE3rtA/39944c4739dbecaa3f8a336826de352d/sVJFWPtv0RFGdOIc_fKWG_C0uLbr6A__1_.png?w=300&q=30",
      "popularity_score": 2011.046603888889,
      "ai_summary": [
        "Qwen3 Coder Next is a new open source model",
        "The model is designed for coding tasks",
        "It has 80 billion parameters",
        "The model uses an ultra sparse architecture",
        "It is released on a permissive Apache 2.0 license"
      ]
    },
    {
      "id": "cluster_33",
      "coverage": 1,
      "updated_at": "Tue, 03 Feb 2026 22:55:59 +0000",
      "title": "Godlike Titan threatens humanity in Monarch: Legacy of Monsters S2 trailer",
      "neutral_headline": "Godlike Titan Threatens Humanity In Monarch",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/culture/2026/02/meet-the-new-tentacled-titan-x-in-monarch-legacy-of-monsters-s2-trailer/",
          "published_at": "Tue, 03 Feb 2026 22:55:59 +0000",
          "title": "Godlike Titan threatens humanity in Monarch: Legacy of Monsters S2 trailer",
          "standfirst": "\"This Titan is like a god, and the sea creatures worship it.\"",
          "content": "Last month, Apple TV released a teaser for the second season of Monarch: Legacy of Monsters, part of Legendary Entertainment’s MonsterVerse, which brought Godzilla, King Kong, and various other monsters (kaiju) created by Toho Co., Ltd into a shared narrative. But we only got the most fleeting glimpse of the promised new mythical Titan threatening the human race. The full trailer just dropped and rectifies that: it's a gigantic tentacled undersea being dubbed Titan X—and only Kong and Godzilla can stop it. (Spoilers for Season 1 below.) As previously reported, the first season picked up where 2014’s Godzilla left off, specifically the introduction of Project Monarch, a secret organization established in the 1950s to study Godzilla and other kaiju—after attempts to kill Godzilla with nuclear weapons failed. In the S1 finale, Godzilla fights off an Ion Dragon, tossing it through a rift back to the Hollow Earth, and Lee Shaw (Kurt Russell) seemingly sacrifices himself to save his colleagues. Per the official Season 2 premise:Read full article Comments",
          "feed_position": 1,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/monarch1-1152x648-1770156538.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/monarch1-1152x648-1770156538.jpg",
      "popularity_score": 349.1963261111111,
      "ai_summary": [
        "A godlike Titan threatens humanity in a show",
        "The show is Monarch Legacy of Monsters",
        "The Titan is worshipped by sea creatures",
        "The show's second season is upcoming",
        "The trailer reveals the Titan's threat"
      ]
    },
    {
      "id": "cluster_28",
      "coverage": 1,
      "updated_at": "Tue, 03 Feb 2026 23:41:39 +0000",
      "title": "Netflix says users can cancel service if HBO Max merger makes it too expensive",
      "neutral_headline": "Netflix Users Can Cancel Service",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2026/02/netflix-claims-subscribers-will-get-more-content-for-less-if-it-buys-hbo-max/",
          "published_at": "Tue, 03 Feb 2026 23:41:39 +0000",
          "title": "Netflix says users can cancel service if HBO Max merger makes it too expensive",
          "standfirst": "80 percent of HBO Max subscribers subscribe to Netflix, Sarandos tells Senate.",
          "content": "There is concern that subscribers might be negatively affected if Netflix acquires Warner Bros. Discovery’s (WBD's) streaming and movie studios businesses. One of the biggest fears is that the merger would lead to higher prices due to Netflix having less competition. During a Senate hearing today, Netflix co-CEO Ted Sarandos suggested that the merger would have an opposite effect. Sarandos was speaking at a hearing held by the US Senate Judiciary Committee’s Subcommittee on Antitrust, Competition Policy, and Consumer Rights, “Examining the Competitive Impact of the Proposed Netflix-Warner Brothers Transaction.” Sarandos aimed to convince the subcommittee that Netflix wouldn’t become a monopoly in streaming or in movie and TV production if regulators allowed its acquisition to close. Netflix is the largest subscription video-on-demand (SVOD) provider by subscribers (301.63 million as of January 2025), and WBD is the third (128 million streaming subscribers, including users of HBO Max and, to a smaller degree, Discovery+).Read full article Comments",
          "feed_position": 0,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-2259767834-1024x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-2259767834-1024x648.jpg",
      "popularity_score": 344.9574372222222,
      "ai_summary": [
        "Netflix users can cancel their service",
        "The reason is a potential HBO Max merger",
        "The merger could make the service expensive",
        "80 percent of HBO Max subscribers use Netflix",
        "Netflix's CEO made the statement"
      ]
    },
    {
      "id": "cluster_35",
      "coverage": 1,
      "updated_at": "Tue, 03 Feb 2026 22:44:15 +0000",
      "title": "Nvidia's $100 billion OpenAI deal has seemingly vanished",
      "neutral_headline": "Nvidia OpenAI Deal Has Vanished",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/information-technology/2026/02/five-months-later-nvidias-100-billion-openai-investment-plan-has-fizzled-out/",
          "published_at": "Tue, 03 Feb 2026 22:44:15 +0000",
          "title": "Nvidia's $100 billion OpenAI deal has seemingly vanished",
          "standfirst": "Two AI giants shake market confidence after investment fails to materialize.",
          "content": "In September 2025, Nvidia and OpenAI announced a letter of intent for Nvidia to invest up to $100 billion in OpenAI's AI infrastructure. At the time, the companies said they expected to finalize details \"in the coming weeks.\" Five months later, no deal has closed, Nvidia's CEO now says the $100 billion figure was \"never a commitment,\" and Reuters reports that OpenAI has been quietly seeking alternatives to Nvidia chips since last year. Reuters also wrote that OpenAI is unsatisfied with the speed of some Nvidia chips for inference tasks, citing eight sources familiar with the matter. Inference is the process by which a trained AI model generates responses to user queries. According to the report, the issue became apparent in OpenAI's Codex, an AI code-generation tool. OpenAI staff reportedly attributed some of Codex's performance limitations to Nvidia's GPU-based hardware. After the Reuters story published and Nvidia's stock price took a dive, Nvidia and OpenAI have tried to smooth things over publicly. OpenAI CEO Sam Altman posted on X: \"We love working with NVIDIA and they make the best AI chips in the world. We hope to be a gigantic customer for a very long time. I don't get where all this insanity is coming from.\"Read full article Comments",
          "feed_position": 2,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/09/nvidia-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/09/nvidia-1152x648.jpg",
      "popularity_score": 339.00077055555556,
      "ai_summary": [
        "A deal between Nvidia and OpenAI has vanished",
        "The deal was worth $100 billion",
        "The investment failed to materialize",
        "The AI giants are shaking market confidence",
        "The deal's disappearance is unexpected"
      ]
    },
    {
      "id": "cluster_50",
      "coverage": 1,
      "updated_at": "Tue, 03 Feb 2026 20:13:08 +0000",
      "title": "X office raided in France's Grok probe; Elon Musk summoned for questioning",
      "neutral_headline": "X Office Raided In Grok Probe",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2026/02/x-office-raided-in-frances-grok-probe-elon-musk-summoned-for-questioning/",
          "published_at": "Tue, 03 Feb 2026 20:13:08 +0000",
          "title": "X office raided in France's Grok probe; Elon Musk summoned for questioning",
          "standfirst": "Paris prosecutor: Illegal content probe includes pornographic images of minors.",
          "content": "French law enforcement authorities today raided X's Paris office and summoned Elon Musk for questioning as part of an investigation into illegal content. The Paris public prosecutor’s office said the yearlong probe was recently expanded because the Grok chatbot was disseminating Holocaust-denial claims and sexually explicit deepfakes. Europol, which is assisting French authorities, said today the \"investigation concerns a range of suspected criminal offenses linked to the functioning and use of the platform, including the dissemination of illegal content and other forms of online criminal activity.\" Europol's cybercrime center provided \"an analyst on the ground in Paris to assist national authorities.\" The French Gendarmerie’s cybercrime unit is also aiding the investigation. French authorities want to question Musk and former X CEO Linda Yaccarino, who quit last year amid a controversy over Grok's praise of Hitler. Prosecutors summoned Musk and Yaccarino for interviews in April 2026, though the interviews are being described as voluntary.Read full article Comments",
          "feed_position": 4,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2024/02/Elon-Musk-X-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2024/02/Elon-Musk-X-1152x648.jpg",
      "popularity_score": 316.48215944444445,
      "ai_summary": [
        "The X office was raided in a probe",
        "The probe is into illegal content",
        "Elon Musk was summoned for questioning",
        "The probe includes pornographic images of minors",
        "The raid was conducted by French authorities"
      ]
    },
    {
      "id": "cluster_43",
      "coverage": 1,
      "updated_at": "Tue, 03 Feb 2026 21:15:57 +0000",
      "title": "Newborn dies after mother drinks raw milk during pregnancy",
      "neutral_headline": "Newborn Dies After Mother Drinks Raw Milk",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/health/2026/02/newborns-death-spurs-raw-milk-warning-in-new-mexico/",
          "published_at": "Tue, 03 Feb 2026 21:15:57 +0000",
          "title": "Newborn dies after mother drinks raw milk during pregnancy",
          "standfirst": "Raw milk is promoted by anti-vaccine Health Secretary Kennedy.",
          "content": "A newborn baby has died in New Mexico from a Listeria infection that state health officials say was likely contracted from raw (unpasteurized) milk that the baby's mother drank during pregnancy. In a news release Tuesday, officials warned people not to consume any raw dairy, highlighting that it can be teeming with a variety of pathogens. Those germs are especially dangerous to pregnant women, as well as young children, the elderly, and people with weakened immune systems. \"Raw milk can contain numerous disease-causing germs, including Listeria, which is bacteria that can cause miscarriage, stillbirth, preterm birth, or fatal infection in newborns, even if the mother is only mildly ill,\" the New Mexico Department of Health said in the press release.Read full article Comments",
          "feed_position": 3,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-2211793949-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-2211793949-1152x648.jpg",
      "popularity_score": 312.52910388888887,
      "ai_summary": [
        "A newborn died after the mother drank raw milk",
        "Raw milk is promoted by an anti vaccine Health Secretary",
        "The mother consumed the milk during pregnancy",
        "The incident highlights the risks of raw milk",
        "The Health Secretary's promotion is criticized"
      ]
    },
    {
      "id": "cluster_66",
      "coverage": 1,
      "updated_at": "Tue, 03 Feb 2026 18:56:20 +0000",
      "title": "Nintendo Switch is the second-bestselling game console ever, behind only the PS2",
      "neutral_headline": "Nintendo Switch is second best selling console",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2026/02/original-nintendo-switch-passes-the-ds-to-become-nintendos-bestselling-console/",
          "published_at": "Tue, 03 Feb 2026 18:56:20 +0000",
          "title": "Nintendo Switch is the second-bestselling game console ever, behind only the PS2",
          "standfirst": "Switch 2 has already beaten the Wii U and is on its way to overtaking GameCube.",
          "content": "Although it was finally replaced last year by the new Switch 2, the orginal switch isn't done just yet. Many recent Switch games (and a handful of major updates, like the one for Animal Crossing) have been released in both Switch and Switch 2 editions, and Nintendo continues to sell all editions of the original console as entry-level systems for those who can't pay $450 for a Switch 2. The 9-year-old Switch's continued availability has helped it clear a milestone, according to the company's third-quarter financial results (PDF). As of December 31, 2025, Nintendo says the Switch \"has reached the highest sales volume of any Nintendo hardware\" with a total of 155.37 million units sold, surpassing the original DS's lifetime total of 154.02 million units. The console has sold 3.25 million units in Nintendo's fiscal 2026 so far, including 1.36 million units over the holidays. Those consoles have sold despite price hikes that Nintendo introduced in August of 2025, citing \"market conditions.\" That makes the Switch the second-bestselling game console of all time, just three years after it became the third-bestselling game console of all time. The only frontier left for the Switch to conquer is Sony's PlayStation 2, which Sony says sold \"over 160 million units\" over its long life. At its current sales rate (Nintendo predicts it will sell roughly 750,000 Switches in the next quarter), it would take the Switch another couple of years to cross that line, but those numbers are likely to taper off as we get deeper into the Switch 2 era.Read full article Comments",
          "feed_position": 5,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2021/07/NintendoSwitchOLEDmodel_02-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2021/07/NintendoSwitchOLEDmodel_02-1152x648.jpg",
      "popularity_score": 296.2021594444444,
      "ai_summary": [
        "Nintendo Switch sales surpass 100 million units",
        "Switch sales exceed Wii U and GameCube sales",
        "PlayStation 2 remains the best selling console",
        "Nintendo Switch has a large game library",
        "Switch sales continue to grow steadily"
      ]
    },
    {
      "id": "cluster_72",
      "coverage": 1,
      "updated_at": "Tue, 03 Feb 2026 18:25:18 +0000",
      "title": "Google court filings suggest ChromeOS has an expiration date",
      "neutral_headline": "Google ChromeOS has an expiration date",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/google/2026/02/google-court-filings-suggest-googles-chromeos-has-an-expiration-date/",
          "published_at": "Tue, 03 Feb 2026 18:25:18 +0000",
          "title": "Google court filings suggest ChromeOS has an expiration date",
          "standfirst": "ChromeOS may be canned once the current support guarantee has run its course.",
          "content": "Chromebooks debuted 16 years ago with the limited release of Google's Cr-48, an unassuming compact laptop that was provided free to select users. From there, Chromebooks became one of the most popular budget computing options and a common fixture in schools and businesses. According to some newly uncovered court documents, Google's shift to Android PCs means Chromebooks have an expiration date in 2034. The documents were filed as part of Google's long-running search antitrust case, which began in 2020 and reached a verdict in 2024. While Google is still seeking to have the guilty verdict overturned, it has escaped most of the remedies that government prosecutors requested. According to The Verge, the company's plans for Chromebooks and the upcoming Android-based Aluminium came up in filings from the remedy phase of the trial. As Google moves toward releasing Aluminium, it sought to keep the upcoming machines above the fray and retain the Chrome browser (which it did). In Judge Amit Mehta's final order, devices running ChromeOS or a ChromeOS successor are excluded. To get there, Google had to provide a little more detail on its plans.Read full article Comments",
          "feed_position": 6,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/06/9_Lenovo-Chromebook-Plus-14-copy-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/06/9_Lenovo-Chromebook-Plus-14-copy-1152x648.jpg",
      "popularity_score": 291.68493722222223,
      "ai_summary": [
        "ChromeOS support guarantee will eventually expire",
        "Google may discontinue ChromeOS after guarantee",
        "ChromeOS is used in various Google devices",
        "Google has not announced an official end date",
        "ChromeOS users may need to switch to alternative"
      ]
    },
    {
      "id": "cluster_75",
      "coverage": 1,
      "updated_at": "Tue, 03 Feb 2026 18:01:49 +0000",
      "title": "Xcode 26.3 adds support for Claude, Codex, and other agentic tools via MCP",
      "neutral_headline": "Xcode 26.3 adds support for agentic tools",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/apple/2026/02/xcode-26-3-adds-support-for-claude-codex-and-other-agentic-tools-via-mcp/",
          "published_at": "Tue, 03 Feb 2026 18:01:49 +0000",
          "title": "Xcode 26.3 adds support for Claude, Codex, and other agentic tools via MCP",
          "standfirst": "With Model Context Protocol (MCP), this works with more than Codex/Claude, too.",
          "content": "Apple has announced a new version of Xcode, the latest version of its integrated development environment (IDE) for building software for its own platforms, like the iPhone and Mac. The key feature of 26.3 is support for full-fledged agentic coding tools, like OpenAI's Codex or Claude Agent, with a side panel interface for assigning tasks to agents with prompts and tracking their progress and changes. This is achieved via Model Context Protocol (MCP), an open protocol that lets AI agents work with external tools and structured resources. Xcode acts as an MCP endpoint that exposes a bunch of machine-invocable interfaces and gives AI tools like Codex or Claude Agent access to a wide range of IDE primitives like file graph, docs search, project settings, and so on. While AI chat and workflows were supported in Xcode before, this release gives them much deeper access to the features and capabilities of Xcode. This approach is notable because it means that even though OpenAI and Anthropic's model integrations are privileged with a dedicated spot in Xcode's settings, it's possible to connect other tooling that supports MCP, which also allows doing some of this with models running locally.Read full article Comments",
          "feed_position": 7,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/Xcode-1152x648.png"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/Xcode-1152x648.png",
      "popularity_score": 269.2935483333333,
      "ai_summary": [
        "Xcode 26.3 includes Model Context Protocol",
        "MCP supports Claude and Codex tools",
        "Agentic tools enhance developer experience",
        "Xcode update improves overall performance",
        "MCP works with multiple tools and platforms"
      ]
    },
    {
      "id": "cluster_81",
      "coverage": 1,
      "updated_at": "Tue, 03 Feb 2026 17:29:16 +0000",
      "title": "Wing Commander III: \"Isn't that the guy from Star Wars?\"",
      "neutral_headline": "Wing Commander III is a classic game",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gaming/2026/02/wing-commander-iii-the-game-kind-of-sucked-but-the-experience-blew-me-away/",
          "published_at": "Tue, 03 Feb 2026 17:29:16 +0000",
          "title": "Wing Commander III: \"Isn't that the guy from Star Wars?\"",
          "standfirst": "C:\\ArsGames looks at a vanguard of the multimedia FMV future that never quite came to pass.",
          "content": "It's Christmas of 1994, and I am 16 years old. Sitting on the table in our family room next to a pile of cow-spotted boxes is the most incredible thing in the world: a brand-new Gateway 66MHz Pentium tower, with a 540MB hard disk drive, 8MB of RAM, and, most importantly, a CD-ROM drive. I am agog, practically trembling with barely suppressed joy, my bored Gen-X teenager mask threatening to slip and let actual feelings out. My life was about to change—at least where games were concerned. I'd been working for several months at Babbage's store No. 9, near Baybrook Mall in southeast suburban Houston. Although the Gateway PC's arrival on Christmas morning was utterly unexpected, the choice of what game to buy required no planning at all. I'd already decided a few weeks earlier, when Chris Roberts' latest opus had been drop-shipped to our shelves, just in time for the holiday season. The choice made itself, really. Gimli and Luke, together at last! Credit: Origin Systems / Electronic Arts The moment Babbage's opened its doors on December 26—a day I had off, fortunately—I was there, checkbook in hand. One entire paycheck's worth of capitalism later, I was sprinting out to my creaky 280-Z, sweatily clutching two boxes—one an impulse buy, The Star Trek: The Next Generation Interactive Technical Manual, and the other a game I felt sure would be the best thing I'd ever played or ever would play: Origin's Wing Commander III: The Heart of the Tiger. On the backs of Wing Commander I and Wing Commander II, how could it not be?!Read full article Comments",
          "feed_position": 8,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/wc3_boxart_sm-1152x648-1770132197.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/wc3_boxart_sm-1152x648-1770132197.jpg",
      "popularity_score": 258.75104833333336,
      "ai_summary": [
        "Wing Commander III is a vintage game title",
        "Game features full motion video content",
        "Wing Commander III was released in the 1990s",
        "Game has a unique blend of action and story",
        "Wing Commander III is still remembered today"
      ]
    },
    {
      "id": "cluster_85",
      "coverage": 1,
      "updated_at": "Tue, 03 Feb 2026 17:08:34 +0000",
      "title": "Upset at reports that he'd given up, Trump now wants $1B from Harvard",
      "neutral_headline": "Trump demands 1 billion from Harvard",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2026/02/upset-at-reports-that-hed-given-up-trump-now-wants-1b-from-harvard/",
          "published_at": "Tue, 03 Feb 2026 17:08:34 +0000",
          "title": "Upset at reports that he'd given up, Trump now wants $1B from Harvard",
          "standfirst": "Hefty \"fine\" comes in wake of NY Times reporting of money-free settlement.",
          "content": "Amid the Trump administration's attack on universities, Harvard has emerged as a particular target. Early on, the administration put $2.2 billion in research money on hold and shortly thereafter blocked all future funding while demanding intrusive control over Harvard's hiring and admissions. Unlike many of its peer institutions, Harvard fought back, filing and ultimately winning a lawsuit that restored the cut funds. Despite Harvard's victory, the Trump administration continued to push for some sort of formal agreement that would settle the administration's accusations that Harvard created an environment that allowed antisemitism to flourish. In fact, it had become a running joke among some journalists that The New York Times had devoted a monthly column to reporting that a settlement between the two parties was near. Given the government's loss of leverage, it was no surprise that the latest installment of said column included the detail that the latest negotiations had dropped demands that Harvard pay any money as part of a final agreement. The Trump administration had extracted hundreds of millions of dollars from some other universities and had demanded over a billion dollars from UCLA, so this appeared to be a major concession to Harvard.Read full article Comments",
          "feed_position": 9,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-578751544-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-578751544-1152x648.jpg",
      "popularity_score": 248.40604833333333,
      "ai_summary": [
        "Trump seeks 1 billion dollar payment",
        "Demand comes after New York Times report",
        "Trump claims Harvard owes him money",
        "Harvard has not commented on the demand",
        "Trump has made similar demands in past"
      ]
    },
    {
      "id": "cluster_124",
      "coverage": 1,
      "updated_at": "Tue, 03 Feb 2026 12:00:01 +0000",
      "title": "The rise of Moltbook suggests viral AI prompts may be the next big security threat",
      "neutral_headline": "Moltbook suggests AI prompts are a threat",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/ai/2026/02/the-rise-of-moltbook-suggests-viral-ai-prompts-may-be-the-next-big-security-threat/",
          "published_at": "Tue, 03 Feb 2026 12:00:01 +0000",
          "title": "The rise of Moltbook suggests viral AI prompts may be the next big security threat",
          "standfirst": "We don't need self-replicating AI models to have problems, just self-replicating prompts.",
          "content": "On November 2, 1988, graduate student Robert Morris released a self-replicating program into the early Internet. Within 24 hours, the Morris worm had infected roughly 10 percent of all connected computers, crashing systems at Harvard, Stanford, NASA, and Lawrence Livermore National Laboratory. The worm exploited security flaws in Unix systems that administrators knew existed but had not bothered to patch. Morris did not intend to cause damage. He wanted to measure the size of the Internet. But a coding error caused the worm to replicate far faster than expected, and by the time he tried to send instructions for removing it, the network was too clogged to deliver the message. History may soon repeat itself with a novel new platform: networks of AI agents carrying out instructions from prompts and sharing them with other AI agents, which could spread the instructions further.Read full article Comments",
          "feed_position": 12,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/moltbook-chest-burster-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/moltbook-chest-burster-1152x648.jpg",
      "popularity_score": 158.26354833333335,
      "ai_summary": [
        "Moltbook is a potential security risk",
        "AI prompts can be used for malicious purposes",
        "Self replicating prompts are a concern",
        "AI models can be vulnerable to prompts",
        "Moltbook highlights need for AI security"
      ]
    },
    {
      "id": "cluster_109",
      "coverage": 1,
      "updated_at": "Tue, 03 Feb 2026 14:02:57 +0000",
      "title": "Senior staff departing OpenAI as firm prioritizes ChatGPT development",
      "neutral_headline": "OpenAI senior staff are departing",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/ai/2026/02/senior-staff-departing-openai-as-firm-prioritizes-chatgpt-development/",
          "published_at": "Tue, 03 Feb 2026 14:02:57 +0000",
          "title": "Senior staff departing OpenAI as firm prioritizes ChatGPT development",
          "standfirst": "Resources are redirected from long-term research toward improving the flagship chatbot.",
          "content": "OpenAI is prioritizing the advancement of ChatGPT over more long-term research, prompting the departure of senior staff as the $500 billion company adapts to stiff competition from rivals such as Google and Anthropic. The San Francisco-based start-up has reallocated resources for experimental work in favor of advances to the large language models that power its flagship chatbot, according to 10 current and former employees. Among those to leave OpenAI in recent months over the strategic shift are vice-president of research Jerry Tworek, model policy researcher Andrea Vallone, and economist Tom Cunningham.Read full article Comments",
          "feed_position": 11,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/03/openai-logo-1152x648-1741196873.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/03/openai-logo-1152x648-1741196873.jpg",
      "popularity_score": 156.31243722222223,
      "ai_summary": [
        "Senior staff are leaving OpenAI",
        "Firm is prioritizing ChatGPT development",
        "Resources are being redirected to ChatGPT",
        "OpenAI is focusing on short term goals",
        "Departures may impact long term research"
      ]
    },
    {
      "id": "cluster_144",
      "coverage": 1,
      "updated_at": "Mon, 02 Feb 2026 21:55:44 +0000",
      "title": "SpaceX acquires xAI, plans to launch a massive satellite constellation to power it",
      "neutral_headline": "SpaceX acquires xAI company",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/ai/2026/02/spacex-acquires-xai-plans-1-million-satellite-constellation-to-power-it/",
          "published_at": "Mon, 02 Feb 2026 21:55:44 +0000",
          "title": "SpaceX acquires xAI, plans to launch a massive satellite constellation to power it",
          "standfirst": "\"This marks not just the next chapter, but the next book in SpaceX and xAI's mission.\"",
          "content": "SpaceX has formally acquired another one of Elon Musk's companies, xAi, the space company announced on Monday afternoon. \"SpaceX has acquired xAI to form the most ambitious, vertically-integrated innovation engine on (and off) Earth, with AI, rockets, space-based internet, direct-to-mobile device communications and the world’s foremost real-time information and free speech platform,\" the company said. \"This marks not just the next chapter, but the next book in SpaceX and xAI's mission: scaling to make a sentient sun to understand the Universe and extend the light of consciousness to the stars!\" The merging of what is arguably Musk's most successful company, SpaceX, with the more speculative xAI venture is a risk. Founded in 2023, xAI's main products are the generative AI chatbot Grok and the social media site X, formerly known as Twitter. The company aims to compete with OpenAI and other artificial intelligence firms. However, Grok has been controversial, including the sexualization of women and children through AI-generated images, as has Musk's management of Twitter.Read full article Comments",
          "feed_position": 16,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/starship_march2025-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/starship_march2025-1152x648.jpg",
      "popularity_score": 156,
      "ai_summary": [
        "SpaceX has acquired xAI company",
        "Acquisition will enhance SpaceX capabilities",
        "xAI will help power SpaceX satellite",
        "Satellite constellation will be launched",
        "Acquisition marks new chapter for SpaceX"
      ]
    },
    {
      "id": "cluster_142",
      "coverage": 1,
      "updated_at": "Mon, 02 Feb 2026 22:31:46 +0000",
      "title": "Streaming service Crunchyroll raises prices weeks after killing its free tier",
      "neutral_headline": "Crunchyroll raises streaming prices",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2026/02/streaming-service-crunchyroll-raises-prices-weeks-after-killing-its-free-tier/",
          "published_at": "Mon, 02 Feb 2026 22:31:46 +0000",
          "title": "Streaming service Crunchyroll raises prices weeks after killing its free tier",
          "standfirst": "Sony has made streaming anime pricier since buying Crunchyroll.",
          "content": "Crunchyroll is one of the most popular streaming platforms for anime viewers. Over the past six years, the service has raised prices for fans, and today, it announced that it's increasing monthly subscription prices by up to 25 percent. Sony bought Crunchyroll from AT&T in 2020. At the time, Crunchyroll had 3 million paid subscribers and an additional 197 million users with free accounts, which let people watch a limited number of titles with commercials. At the time, Crunchyroll monthly subscription tiers cost $8, $10, or $15. After its acquisition by Sony, like many large technology companies that buy a smaller, beloved product, the company made controversial changes. The Tokyo-based company folded rival Funimation into Crunchyroll; Sony shut down Funimation, which it bought in 2017, in April 2024.Read full article Comments",
          "feed_position": 15,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-2258531948-1024x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-2258531948-1024x648.jpg",
      "popularity_score": 148,
      "ai_summary": [
        "Crunchyroll has increased its prices",
        "Price hike comes after free tier removal",
        "Sony owns Crunchyroll streaming service",
        "Price increase affects all subscribers",
        "Crunchyroll offers various anime content"
      ]
    },
    {
      "id": "cluster_145",
      "coverage": 1,
      "updated_at": "Mon, 02 Feb 2026 21:32:18 +0000",
      "title": "Russian drones use Starlink, but Ukraine has plan to block their Internet access",
      "neutral_headline": "Ukraine plans to block Russian drones",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2026/02/russian-drones-use-starlink-but-ukraine-has-plan-to-block-their-internet-access/",
          "published_at": "Mon, 02 Feb 2026 21:32:18 +0000",
          "title": "Russian drones use Starlink, but Ukraine has plan to block their Internet access",
          "standfirst": "Defense chief: \"No Ukrainians have been killed by Russian drones using Starlink.\"",
          "content": "Ukraine and SpaceX say they recently collaborated to stop strikes by Russian drones using Starlink and will soon block all unregistered use of Starlink terminals in an attempt to stop Russia's military from using the satellite broadband network over Ukraine territory. Ukrainians will soon be required to register their Starlink terminals to get on a whitelist. After that, \"only verified and registered terminals will be allowed to operate in the country. All others will be disconnected,\" the Ukraine Ministry of Defense said in a press release today. Ukraine Minister of Defense Mykhailo Fedorov \"emphasized that the only technical solution to counter this threat is to introduce a 'whitelist' and authorize all terminals,\" according to the ministry. \"This is a necessary step by the Government to save Ukrainian lives and protect critical energy infrastructure,\" Fedorov said.Read full article Comments",
          "feed_position": 17,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2023/02/getty-ukraine-starlink-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2023/02/getty-ukraine-starlink-1152x648.jpg",
      "popularity_score": 148,
      "ai_summary": [
        "Ukraine will block Russian drone access",
        "Russian drones use Starlink internet",
        "Ukraine has a plan to counter drones",
        "No Ukrainians killed by Russian drones",
        "Defense chief comments on drone situation"
      ]
    },
    {
      "id": "cluster_151",
      "coverage": 1,
      "updated_at": "Mon, 02 Feb 2026 20:30:56 +0000",
      "title": "Notepad++ users take note: It's time to check if you're hacked",
      "neutral_headline": "Notepad++ users may be hacked",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/security/2026/02/notepad-updater-was-compromised-for-6-months-in-supply-chain-attack/",
          "published_at": "Mon, 02 Feb 2026 20:30:56 +0000",
          "title": "Notepad++ users take note: It's time to check if you're hacked",
          "standfirst": "Suspected China-state hackers used update infrastructure to deliver backdoored version.",
          "content": "Infrastructure delivering updates for Notepad++—a widely used text editor for Windows—was compromised for six months by suspected China-state hackers who used their control to deliver backdoored versions of the app to select targets, developers said Monday. “I deeply apologize to all users affected by this hijacking,” the author of a post published to the official notepad-plus-plus.org site wrote Monday. The post said that the attack began last June with an “infrastructure-level compromise that allowed malicious actors to intercept and redirect update traffic destined for notepad-plus-plus.org.” The attackers, whom multiple investigators tied to the Chinese government, then selectively redirected certain targeted users to malicious update servers where they received backdoored updates. Notepad++ didn’t regain control of its infrastructure until December. The attackers used their access to install a never-before-seen payload that has been dubbed Chrysalis. Security firm Rapid 7 descrbed it as a \"custom, feature-rich backdoor.\"Read full article Comments",
          "feed_position": 19,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2023/07/exploit-vulnerability-security.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2023/07/exploit-vulnerability-security.jpg",
      "popularity_score": 143,
      "ai_summary": [
        "Notepad++ users should check for hacks",
        "Suspected China state hackers involved",
        "Hackers used update infrastructure",
        "Users should update Notepad++ software",
        "Hackers delivered backdoored version"
      ]
    },
    {
      "id": "cluster_101",
      "coverage": 1,
      "updated_at": "Tue, 03 Feb 2026 15:33:06 +0000",
      "title": "China bans all retractable car door handles, starting next year",
      "neutral_headline": "China bans retractable car door handles",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/cars/2026/02/china-bans-all-retractable-car-door-handles-starting-next-year/",
          "published_at": "Tue, 03 Feb 2026 15:33:06 +0000",
          "title": "China bans all retractable car door handles, starting next year",
          "standfirst": "The pop-out door handle ban starts in 2027 for new cars, 2029 for existing models.",
          "content": "Flush door handles have been quite the automotive design trend of late. Stylists like them because they don't add visual noise to the side of a car. And aerodynamicists like them because they make a vehicle more slippery through the air. When Tesla designed its Model S, it needed a car that was both desirable and as efficient as possible, so flush door handles were a no-brainer. Since then, as electric vehicles have proliferated, so too have flush door handles. But as of next year, China says no. Just like pop-up headlights, despite the aesthetic and aerodynamic advantages, there are safety downsides. Tesla's handles are an extreme example: In the event of a crash and a loss of 12 V power, there is no way for first responders to open the door from the outside, which has resulted in at least 15 deaths. Those deaths prompted the National Highway Traffic Safety Administration to open an investigation last year, but China is being a little more proactive. It has been looking at whether retractable car door handles are safe since mid-2024, according to Bloomberg, and has concluded that no, they are not.Read full article Comments",
          "feed_position": 10,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/09/GettyImages-1277594023-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/09/GettyImages-1277594023-1152x648.jpg",
      "popularity_score": 136.81493722222223,
      "ai_summary": [
        "China has banned retractable door handles",
        "Ban starts in 2027 for new cars",
        "Existing models have until 2029",
        "Ban applies to all car manufacturers",
        "Reason for ban not fully explained"
      ]
    },
    {
      "id": "cluster_133",
      "coverage": 1,
      "updated_at": "Tue, 03 Feb 2026 08:06:50 +0000",
      "title": "Unable to tame hydrogen leaks, NASA delays launch of Artemis II until March",
      "neutral_headline": "NASA delays Artemis II launch",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2026/02/unable-to-tame-hydrogen-leaks-nasa-delays-launch-of-artemis-ii-until-march/",
          "published_at": "Tue, 03 Feb 2026 08:06:50 +0000",
          "title": "Unable to tame hydrogen leaks, NASA delays launch of Artemis II until March",
          "standfirst": "NASA spent most of Monday trying to overcome hydrogen leaks on the Artemis II rocket.",
          "content": "The launch of NASA's Artemis II mission, the first flight of astronauts to the Moon in more than 53 years, will have to wait another month after a fueling test Monday uncovered hydrogen leaks in the connection between the rocket and its launch platform at Kennedy Space Center in Florida. \"Engineers pushed through several challenges during the two-day test and met many of the planned objectives,\" NASA said in a statement following the conclusion of the mock countdown, or wet dress rehearsal (WDR), early Tuesday morning. \"To allow teams to review data and conduct a second Wet Dress Rehearsal, NASA now will target March as the earliest possible launch opportunity for the flight test.\" The practice countdown was designed to identify problems and provide NASA an opportunity to fix them before launch. Most importantly, the test revealed NASA still has not fully resolved recurring hydrogen leaks that delayed the launch of the unpiloted Artemis I test flight by several months in 2022. Artemis I finally launched successfully after engineers revised their hydrogen loading procedures to overcome the leak.Read full article Comments",
          "feed_position": 13,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/AFRC2026-0017-15orig-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/AFRC2026-0017-15orig-1152x648.jpg",
      "popularity_score": 133,
      "ai_summary": [
        "NASA has delayed Artemis II launch",
        "Delay due to hydrogen leaks",
        "NASA spent Monday trying to fix leaks",
        "Launch rescheduled for March",
        "Artemis II is a crucial mission"
      ]
    },
    {
      "id": "cluster_141",
      "coverage": 1,
      "updated_at": "Mon, 02 Feb 2026 22:57:56 +0000",
      "title": "Looking back at Catacomb 3D, the game that led to Wolfenstein 3D",
      "neutral_headline": "Catacomb 3D is a classic game",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gaming/2026/02/looking-back-at-catacomb-3d-the-game-that-led-to-wolfenstein-3d/",
          "published_at": "Mon, 02 Feb 2026 22:57:56 +0000",
          "title": "Looking back at Catacomb 3D, the game that led to Wolfenstein 3D",
          "standfirst": "Romero, Carmack, and colleagues discuss an oft-forgotten piece of PC gaming history.",
          "content": "If you know anything about the history of id Software, you know how 1992's Wolfenstein 3D helped establish the company's leadership in the burgeoning first-person shooter genre, leading directly to subsequent hits like Doom and Quake. But only the serious id Software nerds remember Catacomb 3D, id's first-person adventure game that directly preceded and inspired work on Wolfenstein 3D. Now, nearly 35 years after Catacomb 3D's initial release, id co-founder John Romero brought the company's founding members together for an informative retrospective video on the creation of the oft-forgotten game. But the pioneering game—which included mouse support, color-coded keys, and shooting walls to find secrets—almost ended up being a gimmicky dead end for the company. id Software's founders look back at an oft-forgotten piece of gaming history. Texture maps and \"undo\" animation Catacomb 3D was a follow-up to id's earlier Catacomb, which was a simplified clone of the popular arcade hit Gauntlet. As such, the 3D game still has some of that \"quarter eater\" mentality that was not very fashionable in PC gaming at the time, as John Carmack remembered.Read full article Comments",
          "feed_position": 14,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/c3d-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/c3d-1152x648.jpg",
      "popularity_score": 133,
      "ai_summary": [
        "Catacomb 3D is an old game title",
        "Game led to development of Wolfenstein",
        "Game was created by Romero and Carmack",
        "Catacomb 3D is a piece of gaming history",
        "Game has been remembered by developers"
      ]
    },
    {
      "id": "cluster_148",
      "coverage": 1,
      "updated_at": "Mon, 02 Feb 2026 20:43:31 +0000",
      "title": "Court orders restart of all US offshore wind construction",
      "neutral_headline": "Court orders restart of US wind construction",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2026/02/court-orders-restart-of-all-us-offshore-wind-construction/",
          "published_at": "Mon, 02 Feb 2026 20:43:31 +0000",
          "title": "Court orders restart of all US offshore wind construction",
          "standfirst": "Trump admin's \"it's classified\" ploy put on hold in five different cases.",
          "content": "The Trump administration is no fan of renewable energy, but it reserves special ire for wind power. Trump himself has repeatedly made false statements about the cost of wind power, its use around the world, and its environmental impacts. That animosity was paired with an executive order that blocked all permitting for offshore wind and some land-based projects, an order that has since been thrown out by a court that ruled it arbitrary and capricious. Not content to block all future developments, the administration has also gone after the five offshore wind projects currently under construction. After temporarily blocking two of them for reasons that were never fully elaborated, the Department of the Interior settled on a single justification for blocking turbine installation: a classified national security risk. The response to that late-December announcement has been uniform: The companies building each of the projects sued the administration. As of Monday, every single one of them has achieved the same result: a temporary injunction that allows them to continue construction. This, despite the fact that the suits were filed in three different courts and heard by four different judges.Read full article Comments",
          "feed_position": 18,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-1623091024.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-1623091024.jpg",
      "popularity_score": 133,
      "ai_summary": [
        "Court has ordered wind construction restart",
        "Trump administration had halted construction",
        "Construction will resume immediately",
        "Court ruling applies to five cases",
        "Wind construction is crucial for energy goals"
      ]
    }
  ]
}