{
  "updated_at": "2026-01-13T04:02:26.271Z",
  "clusters": [
    {
      "id": "cluster_24",
      "coverage": 2,
      "updated_at": "Mon, 12 Jan 2026 18:05:00 -0500",
      "title": "Personal finance app Betterment says an individual accessed certain systems to send fake crypto scam notification and believes the person accessed user info (Emily Mason/Bloomberg)",
      "neutral_headline": "Personal finance app Betterment says an individual accessed certain systems to send fake crypto scam notification and...",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/260112/p36#a260112p36",
          "published_at": "Mon, 12 Jan 2026 18:05:00 -0500",
          "title": "Personal finance app Betterment says an individual accessed certain systems to send fake crypto scam notification and believes the person accessed user info (Emily Mason/Bloomberg)",
          "standfirst": "Emily Mason / Bloomberg: Personal finance app Betterment says an individual accessed certain systems to send fake crypto scam notification and believes the person accessed user info &mdash; Personal finance platform Betterment fell victim to an online attack that allowed an unauthorized individual to send some customers a &hellip;",
          "content": "Emily Mason / Bloomberg: Personal finance app Betterment says an individual accessed certain systems to send fake crypto scam notification and believes the person accessed user info &mdash; Personal finance platform Betterment fell victim to an online attack that allowed an unauthorized individual to send some customers a &hellip;",
          "feed_position": 5,
          "image_url": "http://www.techmeme.com/260112/i36.jpg"
        },
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2026/01/12/fintech-firm-betterment-confirms-data-breach-after-hackers-send-fake-crypto-scam-notification-to-users/",
          "published_at": "Mon, 12 Jan 2026 18:14:10 +0000",
          "title": "Fintech firm Betterment confirms data breach after hackers send fake crypto scam notification to users",
          "standfirst": "Hackers gained access to some Betterment customers’ personal information through a social engineering attack, then targeted some of them with a crypto-related phishing message.",
          "content": "Hackers gained access to some Betterment customers’ personal information through a social engineering attack, then targeted some of them with a crypto-related phishing message.",
          "feed_position": 11
        }
      ],
      "featured_image": "http://www.techmeme.com/260112/i36.jpg",
      "popularity_score": 2015.0427025
    },
    {
      "id": "cluster_32",
      "coverage": 2,
      "updated_at": "2026-01-12T17:14:58-05:00",
      "title": "Meta plans to lay off hundreds of metaverse employees this week",
      "neutral_headline": "Meta plans to lay off hundreds of metaverse employees this week",
      "items": [
        {
          "source": "The Verge",
          "url": "https://www.theverge.com/news/860984/meta-reality-labs-layoffs-metaverse",
          "published_at": "2026-01-12T17:14:58-05:00",
          "title": "Meta plans to lay off hundreds of metaverse employees this week",
          "standfirst": "Meta's Reality Labs team is expected to lose around 10 percent of its staff, with layoffs concentrated on the division's metaverse employees, as reported by The New York Times. The layoffs are apparently a side effect of Meta's AI ambitions, which are pulling focus away from its virtual reality division. According to the Times, Meta's [&#8230;]",
          "content": "Meta's Reality Labs team is expected to lose around 10 percent of its staff, with layoffs concentrated on the division's metaverse employees, as reported by The New York Times. The layoffs are apparently a side effect of Meta's AI ambitions, which are pulling focus away from its virtual reality division. According to the Times, Meta's chief technology officer, Andrew Bosworth, called a meeting for Wednesday that he \"urged staff to attend in person,\" saying it will be the \"most important\" meeting of the year. Bosworth oversees the Reality Labs division, which employs about 15,000 people. Unfortunately, layoffs to Meta's VR team may not come … Read the full story at The Verge.",
          "feed_position": 3
        },
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/260112/p31#a260112p31",
          "published_at": "Mon, 12 Jan 2026 15:42:14 -0500",
          "title": "Sources: Meta plans to lay off ~10% of its 15,000-person Reality Labs division, disproportionately affecting those working on VR headsets and Horizon Worlds (New York Times)",
          "standfirst": "New York Times: Sources: Meta plans to lay off ~10% of its 15,000-person Reality Labs division, disproportionately affecting those working on VR headsets and Horizon Worlds &mdash; The layoffs are set to be announced this week and would affect Meta's work on the metaverse, as the company spends heavily on building artificial intelligence.",
          "content": "New York Times: Sources: Meta plans to lay off ~10% of its 15,000-person Reality Labs division, disproportionately affecting those working on VR headsets and Horizon Worlds &mdash; The layoffs are set to be announced this week and would affect Meta's work on the metaverse, as the company spends heavily on building artificial intelligence.",
          "feed_position": 10,
          "image_url": "http://www.techmeme.com/260112/i31.jpg"
        }
      ],
      "featured_image": "http://www.techmeme.com/260112/i31.jpg",
      "popularity_score": 2014.208813611111
    },
    {
      "id": "cluster_37",
      "coverage": 2,
      "updated_at": "Mon, 12 Jan 2026 21:47:32 +0000",
      "title": "Google removes some AI health summaries after investigation finds “dangerous” flaws",
      "neutral_headline": "Google removes some AI health summaries after investigation finds “dangerous” flaws",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/ai/2026/01/google-removes-some-ai-health-summaries-after-investigation-finds-dangerous-flaws/",
          "published_at": "Mon, 12 Jan 2026 21:47:32 +0000",
          "title": "Google removes some AI health summaries after investigation finds “dangerous” flaws",
          "standfirst": "AI Overviews provided false liver test information experts called alarming.",
          "content": "On Sunday, Google removed some of its AI Overviews health summaries after a Guardian investigation found people were being put at risk by false and misleading information. The removals came after the newspaper found that Google's generative AI feature delivered inaccurate health information at the top of search results, potentially leading seriously ill patients to mistakenly conclude they are in good health. Google disabled specific queries, such as \"what is the normal range for liver blood tests,\" after experts contacted by The Guardian flagged the results as dangerous. The report also highlighted a critical error regarding pancreatic cancer: The AI suggested patients avoid high-fat foods, a recommendation that contradicts standard medical guidance to maintain weight and could jeopardize patient health. Despite these findings, Google only deactivated the summaries for the liver test queries, leaving other potentially harmful answers accessible. The investigation revealed that searching for liver test norms generated raw data tables (listing specific enzymes like ALT, AST, and alkaline phosphatase) that lacked essential context. The AI feature also failed to adjust these figures for patient demographics such as age, sex, and ethnicity. Experts warned that because the AI model's definition of \"normal\" often differed from actual medical standards, patients with serious liver conditions might mistakenly believe they are healthy and skip necessary follow-up care.Read full article Comments",
          "feed_position": 5,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2024/05/GettyImages-1488311999-1152x648.jpg"
        },
        {
          "source": "Guardian Tech",
          "url": "https://www.theguardian.com/technology/2026/jan/11/google-ai-overviews-health-guardian-investigation",
          "published_at": "Sun, 11 Jan 2026 07:00:19 GMT",
          "title": "‘Dangerous and alarming’: Google removes some of its AI summaries after users’ health put at risk",
          "standfirst": "Exclusive: Guardian investigation finds AI Overviews provided inaccurate and false information when queried over blood testsGoogle has removed some of its artificial intelligence health summaries after a Guardian investigation found people were being put at risk of harm by false and misleading information.The company has said its AI Overviews, which use generative AI to provide snapshots of essential information about a topic or question, are “helpful” and “reliable”. Continue reading...",
          "content": "Exclusive: Guardian investigation finds AI Overviews provided inaccurate and false information when queried over blood testsGoogle has removed some of its artificial intelligence health summaries after a Guardian investigation found people were being put at risk of harm by false and misleading information.The company has said its AI Overviews, which use generative AI to provide snapshots of essential information about a topic or question, are “helpful” and “reliable”. Continue reading...",
          "feed_position": 2
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2024/05/GettyImages-1488311999-1152x648.jpg",
      "popularity_score": 2013.751591388889
    },
    {
      "id": "cluster_38",
      "coverage": 2,
      "updated_at": "Mon, 12 Jan 2026 21:47:07 +0000",
      "title": "Our favorite UGreen 3-in-1 wireless charger is 32 percent off right now",
      "neutral_headline": "Our favorite UGreen 3-in-1 wireless charger is 32 percent off right now",
      "items": [
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/our-favorite-ugreen-3-in-1-wireless-charger-is-32-percent-off-right-now-214707069.html",
          "published_at": "Mon, 12 Jan 2026 21:47:07 +0000",
          "title": "Our favorite UGreen 3-in-1 wireless charger is 32 percent off right now",
          "standfirst": "Now that the winter holidays are well and truly past, now's the perfect time to take stock of your tech setup. If you were gifted (or gifted yourself) some new gear in December, make sure that you've got the proper accessories to keep that gear performing at its best. If a new way to power all those batteries would be a benefit, Amazon's currently running a discount on an excellent wireless charging pad. The UGREEN MagFlow Qi2 3-in-1 Charger Station 25W is on sale for $95. That's only a little bit above the lowest price we've ever seen for the product (which was $90), and it's still a 32 percent discount off its usual cost. This is our top pick for a 3-in-1 charging pad thanks to its versatility. The UGREEN can work equally well as a permanent fixture in your home or act as a portable charging station. It boasts a foldable design and has smart little design details to keep it feeling like a premium product. The Qi2 25W charging works across a range of iPhone models and accessories, such as AirPods. There's also a dedicated part of the pad's design for an Apple Watch, which uses a proprietary charging standard, to power up too. Just note that you'll need a newer model of phone and the latest iOS 26 in order to take full advantage of the 25W charging capability. The wireless pad also comes with both a charging plug and a cable. We felt this UGREEN model was a great value at $140, so being able to snag one for a third of the usual price is an even better deal. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/our-favorite-ugreen-3-in-1-wireless-charger-is-32-percent-off-right-now-214707069.html?src=rss",
          "content": "Now that the winter holidays are well and truly past, now's the perfect time to take stock of your tech setup. If you were gifted (or gifted yourself) some new gear in December, make sure that you've got the proper accessories to keep that gear performing at its best. If a new way to power all those batteries would be a benefit, Amazon's currently running a discount on an excellent wireless charging pad. The UGREEN MagFlow Qi2 3-in-1 Charger Station 25W is on sale for $95. That's only a little bit above the lowest price we've ever seen for the product (which was $90), and it's still a 32 percent discount off its usual cost. This is our top pick for a 3-in-1 charging pad thanks to its versatility. The UGREEN can work equally well as a permanent fixture in your home or act as a portable charging station. It boasts a foldable design and has smart little design details to keep it feeling like a premium product. The Qi2 25W charging works across a range of iPhone models and accessories, such as AirPods. There's also a dedicated part of the pad's design for an Apple Watch, which uses a proprietary charging standard, to power up too. Just note that you'll need a newer model of phone and the latest iOS 26 in order to take full advantage of the 25W charging capability. The wireless pad also comes with both a charging plug and a cable. We felt this UGREEN model was a great value at $140, so being able to snag one for a third of the usual price is an even better deal. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/our-favorite-ugreen-3-in-1-wireless-charger-is-32-percent-off-right-now-214707069.html?src=rss",
          "feed_position": 1
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/computing/laptops/dell-revives-its-xps-laptops-after-a-boneheaded-rebranding-001028029.html",
          "published_at": "Mon, 12 Jan 2026 20:25:47 +0000",
          "title": "Dell revives its XPS laptops after a boneheaded rebranding",
          "standfirst": "Last year, Dell killed off all of its PC brands, including the iconic XPS lineup, and replaced them with a simplified naming scheme. It was a move meant to make it easier for people to discern between the company's many brands, but in reality, it just just made the company's lineup even more confusing. We called it an unforced error at the time, but after seeing how much Dell's PC market share fell over 2025, it's fair to say that rebranding was an absolute marketing disaster. So, with its tail between its legs, Dell has returned to CES some welcome news for its fans: XPS lives! And the company plans to double-down on the brand in ways it never did before. Today, Dell revealed the new XPS 14 and 16 notebooks, which feature a more practical design than the previous models. There's a new function row with traditional keys, instead of the odd capacitive buttons that disappeared in sunlight. And while the company is sticking with its \"invisible\" trackpad, which sits flush alongside the wrist rest, there's now a light border around the edges that lets you feel exactly where the trackpad begins and ends.So, in short, Dell seems to have solved most of our recent complaints about the XPS lineup. To signify its commitment to the brand, it's also emblazoning the XPS logo on all of these new machines, replacing the previous Dell name. That’s something I could never imagine a less humbled Dell doing. The redesign also gave Dell room to shave off some weight and thickness from both machines. The XPS 14 weighs around three pounds now, a half-pound lighter than the previous generation, while the XPS 16 weighs 3.6 pounds, a whole pound lighter than before. The new cases make both machines look a lot more like Microsoft’s extra-subtle Surface Laptop, but that’s not necessarily a bad thing. Both systems are powered by Intel’s new Panther Lake Core Ultra Series 3 chips, and they also offer tandem OLED display options.Dell also briefly teased the return of a new XPS 13 later this year, which is set to be the company’s thinnest and lightest notebook ever. Dell says it’ll be cheaper than the XPS has been in the past.The new XPS 14 and 16 will be available on January 6, starting at $2,050 and $2,200, respectively. A Dell representative tells us these aren’t entry-level configurations, instead we can expect to see cheaper prices with lower specs in February.Update 1/6/26, 12:30p: Pricing updated to reflecrt new numbers from Dell. Originally, we were told they would start at $1,650 and $1,850.Update 1/12, 3:00p: Added a mention of lower entry-level configurations coming eventually.This article originally appeared on Engadget at https://www.engadget.com/computing/laptops/dell-revives-its-xps-laptops-after-a-boneheaded-rebranding-001028029.html?src=rss",
          "content": "Last year, Dell killed off all of its PC brands, including the iconic XPS lineup, and replaced them with a simplified naming scheme. It was a move meant to make it easier for people to discern between the company's many brands, but in reality, it just just made the company's lineup even more confusing. We called it an unforced error at the time, but after seeing how much Dell's PC market share fell over 2025, it's fair to say that rebranding was an absolute marketing disaster. So, with its tail between its legs, Dell has returned to CES some welcome news for its fans: XPS lives! And the company plans to double-down on the brand in ways it never did before. Today, Dell revealed the new XPS 14 and 16 notebooks, which feature a more practical design than the previous models. There's a new function row with traditional keys, instead of the odd capacitive buttons that disappeared in sunlight. And while the company is sticking with its \"invisible\" trackpad, which sits flush alongside the wrist rest, there's now a light border around the edges that lets you feel exactly where the trackpad begins and ends.So, in short, Dell seems to have solved most of our recent complaints about the XPS lineup. To signify its commitment to the brand, it's also emblazoning the XPS logo on all of these new machines, replacing the previous Dell name. That’s something I could never imagine a less humbled Dell doing. The redesign also gave Dell room to shave off some weight and thickness from both machines. The XPS 14 weighs around three pounds now, a half-pound lighter than the previous generation, while the XPS 16 weighs 3.6 pounds, a whole pound lighter than before. The new cases make both machines look a lot more like Microsoft’s extra-subtle Surface Laptop, but that’s not necessarily a bad thing. Both systems are powered by Intel’s new Panther Lake Core Ultra Series 3 chips, and they also offer tandem OLED display options.Dell also briefly teased the return of a new XPS 13 later this year, which is set to be the company’s thinnest and lightest notebook ever. Dell says it’ll be cheaper than the XPS has been in the past.The new XPS 14 and 16 will be available on January 6, starting at $2,050 and $2,200, respectively. A Dell representative tells us these aren’t entry-level configurations, instead we can expect to see cheaper prices with lower specs in February.Update 1/6/26, 12:30p: Pricing updated to reflecrt new numbers from Dell. Originally, we were told they would start at $1,650 and $1,850.Update 1/12, 3:00p: Added a mention of lower entry-level configurations coming eventually.This article originally appeared on Engadget at https://www.engadget.com/computing/laptops/dell-revives-its-xps-laptops-after-a-boneheaded-rebranding-001028029.html?src=rss",
          "feed_position": 3
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/computing/ces-2026-proved-the-pc-industry-is-hosed-this-year-174500314.html",
          "published_at": "Mon, 12 Jan 2026 20:25:38 +0000",
          "title": "CES 2026 proved the PC industry is hosed this year",
          "standfirst": "Dell's XPS 14 currently costs over $2,000. An AMD executive predicts that PC builders will likely make piecemeal upgrades this year, instead of building entirely new systems. And new AI supercomputers from NVIDIA and AMD are gobbling up the RAM market. At CES 2026, it was hard not to notice the dire year ahead for the computing industry, one that will likely lead to higher prices and more limited availability for consumer goods across the board.Really, though, the show just confirmed what was apparent since RAM prices skyrocketed over the last few months, driven by demand from AI datacenters. As Samsung's marketing leader, Wonjin Lee, told Bloomberg at CES: \"There's going to be issues around semiconductor supplies, and it's going to affect everyone. Prices are going up even as we speak.\"At first, it appeared that Dell's new XPS 14 and XPS 16 were among the earliest systems hit by these demands. Last year's models started at $1,699 and $1,899, respectively, and we were initially told the new models would actually come in cheaper at $1,650 and $1,850. At the moment, the XPS 14 starts at $2,050, while the XPS 16 is $2,200. A Dell representative tells us these aren’t entry-level configurations, instead we can expect to see cheaper systems below $2,000 in February. While those prices haven’t been finalized, the reps say it should be similar to the earlier figures we were given.It’s also worth noting that it didn't take much to configure the earlier models upwards of $2,000. It’s just unfortunate that Dell doesn’t have cheaper configurations available for the launch if its new systems, especially since they look so compelling. Meanwhile, Apple still hasn't budged its $1,599 MacBook Pro 14-inch pricing. At least Dell still comes in cheaper than the $2,499 MacBook Pro 16-inch.On the desktop front, AMD's David McAfee, Corporate Vice President and GM of Client Channel Business, noted that the longevity of the company's AM4 and AM5 platforms might be a boon for gamers, since they can upgrade their CPUs without buying new RAM kits and motherboards. That allows for a pathway to better performance without paying out the nose for over-priced RAM.\"I think that will be potentially a trend that we see in 2026 with more component upgrades, as opposed to full system swap outs and, and altogether rebuilds,\" he said in a group interview with Engadget and other outlets. \"Some of the most popular CPUs that are still running in gamers’ platforms are parts like the 2600 back to the Pinnacle Ridge days, or 3000 series... Stepping even from there into a little bit more modern 5,000 series processors in an AM4 socket and motherboard, there's a pretty big boost there.\"McAfee added that around 30 to 40 percent of AMD's business still revolves around the AM4 platform, even without the specter of a wild memory market.\"There's no product that has memory in it that's immune to some of these forces around DRAM pricing and, and what it's doing to the market,\" he said, when asked about potential GPU price increases. \"I think the, the truth is the volatility that we've seen over the past two months or so has really been unprecedented.\" Looking ahead, he said he expects prices to settle within the first three to six months of the year, but he didn't discuss his reasoning further. As an aside, he also noted that AMD's X3D chips, which feature 3D V-cache, actually don't see much of a hit from slower RAM. Their high amounts of onboard L2 and L3 cache make up for less ideal memory transfer speeds, McAfee said.That McAfee commented at all about the state of RAM is noteworthy. Every PC maker I’ve asked, including Dell and Acer, refused to comment on the volatile state of the memory industry ahead of CES. Perhaps they were hoping things would calm down before they had to price their new systems. Ultimately, they’re beholden to an increasingly limited supply of RAM. And where is all that memory going? At CES, NVIDIA announced its new Vera Rubin AI supercomputer, which supports up to 54TB of RAM across 36 Vera CPUs and 20.7TB of memory across 72 GPUs. AMD, as well, announced its new Helios AI rack, which supports up to 31TB of memory across 72 AMD Instinct MI455X GPUs. Given the endless appetite for computing to power AI model building and inferencing, there’s likely going to be a significant demand for these beastly systems.Put simply: Our global supply of memory is being sacrificed to appease the AI industry. That’s good news for the likes of OpenAI, Microsoft and NVIDIA, but bad news for anyone who cares about PCs and the consumer products we use every day. Get ready for a year of price hikes. Update 1/12, 3:00p: Added a mention of lower entry-level configurations coming eventually.This article originally appeared on Engadget at https://www.engadget.com/computing/ces-2026-proved-the-pc-industry-is-hosed-this-year-174500314.html?src=rss",
          "content": "Dell's XPS 14 currently costs over $2,000. An AMD executive predicts that PC builders will likely make piecemeal upgrades this year, instead of building entirely new systems. And new AI supercomputers from NVIDIA and AMD are gobbling up the RAM market. At CES 2026, it was hard not to notice the dire year ahead for the computing industry, one that will likely lead to higher prices and more limited availability for consumer goods across the board.Really, though, the show just confirmed what was apparent since RAM prices skyrocketed over the last few months, driven by demand from AI datacenters. As Samsung's marketing leader, Wonjin Lee, told Bloomberg at CES: \"There's going to be issues around semiconductor supplies, and it's going to affect everyone. Prices are going up even as we speak.\"At first, it appeared that Dell's new XPS 14 and XPS 16 were among the earliest systems hit by these demands. Last year's models started at $1,699 and $1,899, respectively, and we were initially told the new models would actually come in cheaper at $1,650 and $1,850. At the moment, the XPS 14 starts at $2,050, while the XPS 16 is $2,200. A Dell representative tells us these aren’t entry-level configurations, instead we can expect to see cheaper systems below $2,000 in February. While those prices haven’t been finalized, the reps say it should be similar to the earlier figures we were given.It’s also worth noting that it didn't take much to configure the earlier models upwards of $2,000. It’s just unfortunate that Dell doesn’t have cheaper configurations available for the launch if its new systems, especially since they look so compelling. Meanwhile, Apple still hasn't budged its $1,599 MacBook Pro 14-inch pricing. At least Dell still comes in cheaper than the $2,499 MacBook Pro 16-inch.On the desktop front, AMD's David McAfee, Corporate Vice President and GM of Client Channel Business, noted that the longevity of the company's AM4 and AM5 platforms might be a boon for gamers, since they can upgrade their CPUs without buying new RAM kits and motherboards. That allows for a pathway to better performance without paying out the nose for over-priced RAM.\"I think that will be potentially a trend that we see in 2026 with more component upgrades, as opposed to full system swap outs and, and altogether rebuilds,\" he said in a group interview with Engadget and other outlets. \"Some of the most popular CPUs that are still running in gamers’ platforms are parts like the 2600 back to the Pinnacle Ridge days, or 3000 series... Stepping even from there into a little bit more modern 5,000 series processors in an AM4 socket and motherboard, there's a pretty big boost there.\"McAfee added that around 30 to 40 percent of AMD's business still revolves around the AM4 platform, even without the specter of a wild memory market.\"There's no product that has memory in it that's immune to some of these forces around DRAM pricing and, and what it's doing to the market,\" he said, when asked about potential GPU price increases. \"I think the, the truth is the volatility that we've seen over the past two months or so has really been unprecedented.\" Looking ahead, he said he expects prices to settle within the first three to six months of the year, but he didn't discuss his reasoning further. As an aside, he also noted that AMD's X3D chips, which feature 3D V-cache, actually don't see much of a hit from slower RAM. Their high amounts of onboard L2 and L3 cache make up for less ideal memory transfer speeds, McAfee said.That McAfee commented at all about the state of RAM is noteworthy. Every PC maker I’ve asked, including Dell and Acer, refused to comment on the volatile state of the memory industry ahead of CES. Perhaps they were hoping things would calm down before they had to price their new systems. Ultimately, they’re beholden to an increasingly limited supply of RAM. And where is all that memory going? At CES, NVIDIA announced its new Vera Rubin AI supercomputer, which supports up to 54TB of RAM across 36 Vera CPUs and 20.7TB of memory across 72 GPUs. AMD, as well, announced its new Helios AI rack, which supports up to 31TB of memory across 72 AMD Instinct MI455X GPUs. Given the endless appetite for computing to power AI model building and inferencing, there’s likely going to be a significant demand for these beastly systems.Put simply: Our global supply of memory is being sacrificed to appease the AI industry. That’s good news for the likes of OpenAI, Microsoft and NVIDIA, but bad news for anyone who cares about PCs and the consumer products we use every day. Get ready for a year of price hikes. Update 1/12, 3:00p: Added a mention of lower entry-level configurations coming eventually.This article originally appeared on Engadget at https://www.engadget.com/computing/ces-2026-proved-the-pc-industry-is-hosed-this-year-174500314.html?src=rss",
          "feed_position": 4
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/ai/anthropic-made-a-version-of-its-coding-ai-for-regular-people-193000363.html",
          "published_at": "Mon, 12 Jan 2026 19:30:00 +0000",
          "title": "Anthropic made a version of its coding AI for regular people",
          "standfirst": "If you follow Anthropic, you're probably familiar with Claude Code. Since the fall of 2024, the company has been training its AI models to use and navigate computers like a human would, and the coding agent has been the most practical expression of that work, giving developers a way to automate rote programming tasks. Starting today, Anthropic is giving regular people a way to take advantage of those capabilities, with the release of a new preview feature called Claude Cowork.The company is billing Cowork as \"a simpler way for anyone — not just developers — to work with Claude.\" After you give the system access to a folder on your computer, it can read, edit or create new files in that folder on your behalf. Anthropic gives a few different example use cases for Cowork. For instance, you could ask Claude to organize your downloads folder, telling it to rename the files contained within to something that's easier to parse at a glance. Another example: you could use Claude to turn screenshots of receipts and invoices into a spreadsheet for tracking expenses. Cowork can also navigate websites — provided you install Claude’s Chrome plugin — and make can use Anthropic's Connectors framework to access third-party apps like Canva. \"Cowork is designed to make using Claude for new work as simple as possible. You don’t need to keep manually providing context or converting Claude’s outputs into the right format,\" the company said. \"Nor do you have to wait for Claude to finish before offering further ideas or feedback: you can queue up tasks and let Claude work through them in parallel.\" If the idea of granting Claude access to your computer sounds ill-advised, Anthropic says Claude \"can’t read or edit anything you don’t give it explicit access to.\" However, the company does note the system can \"take potentially destructive actions,\" such as deleting a file that is important to you or misinterpreting your instructions. For that reason, Anthropic suggests it's best to give \"very clear\" guidance to Claude. Anthropic isn’t the first to offer a computer agent. Microsoft, for example, has been pushing Copilot hard for nearly three years, despite seemingly limited adoption. For Anthropic, the challenge will be convincing people these tools are useful where others have failed. The fact Claude Code has been universally loved by programmers may make that task easier. For now, Anthropic is giving users of its pricey Claude Max subscription first access to the preview. If you want to try Cowork for yourself, you'll also need a Mac with the Claude macOS app installed. For everyone else, you’ll need to join a wait list. This article originally appeared on Engadget at https://www.engadget.com/ai/anthropic-made-a-version-of-its-coding-ai-for-regular-people-193000363.html?src=rss",
          "content": "If you follow Anthropic, you're probably familiar with Claude Code. Since the fall of 2024, the company has been training its AI models to use and navigate computers like a human would, and the coding agent has been the most practical expression of that work, giving developers a way to automate rote programming tasks. Starting today, Anthropic is giving regular people a way to take advantage of those capabilities, with the release of a new preview feature called Claude Cowork.The company is billing Cowork as \"a simpler way for anyone — not just developers — to work with Claude.\" After you give the system access to a folder on your computer, it can read, edit or create new files in that folder on your behalf. Anthropic gives a few different example use cases for Cowork. For instance, you could ask Claude to organize your downloads folder, telling it to rename the files contained within to something that's easier to parse at a glance. Another example: you could use Claude to turn screenshots of receipts and invoices into a spreadsheet for tracking expenses. Cowork can also navigate websites — provided you install Claude’s Chrome plugin — and make can use Anthropic's Connectors framework to access third-party apps like Canva. \"Cowork is designed to make using Claude for new work as simple as possible. You don’t need to keep manually providing context or converting Claude’s outputs into the right format,\" the company said. \"Nor do you have to wait for Claude to finish before offering further ideas or feedback: you can queue up tasks and let Claude work through them in parallel.\" If the idea of granting Claude access to your computer sounds ill-advised, Anthropic says Claude \"can’t read or edit anything you don’t give it explicit access to.\" However, the company does note the system can \"take potentially destructive actions,\" such as deleting a file that is important to you or misinterpreting your instructions. For that reason, Anthropic suggests it's best to give \"very clear\" guidance to Claude. Anthropic isn’t the first to offer a computer agent. Microsoft, for example, has been pushing Copilot hard for nearly three years, despite seemingly limited adoption. For Anthropic, the challenge will be convincing people these tools are useful where others have failed. The fact Claude Code has been universally loved by programmers may make that task easier. For now, Anthropic is giving users of its pricey Claude Max subscription first access to the preview. If you want to try Cowork for yourself, you'll also need a Mac with the Claude macOS app installed. For everyone else, you’ll need to join a wait list. This article originally appeared on Engadget at https://www.engadget.com/ai/anthropic-made-a-version-of-its-coding-ai-for-regular-people-193000363.html?src=rss",
          "feed_position": 5
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/the-disney-hulu-bundle-is-on-sale-for-10-for-one-month-right-now-192814784.html",
          "published_at": "Mon, 12 Jan 2026 19:28:14 +0000",
          "title": "The Disney+ Hulu bundle is on sale for $10 for one month right now",
          "standfirst": "The peak time for deals on streaming services — the holiday shopping season — has come and gone, but Disney is back with a fresh offer for the new year. New and eligible returning subscribers can get one month of the ad-supported Disney+ Hulu bundle for just $10. That's $3 off the usual monthly rate for the bundle, and more than 58 percent off if you consider the prices for each service individually (Disney+ at $12 per month and, separately, Hulu also at $12 per month). We'd be remiss if we didn't mention that this isn't quite as good as the Black Friday deal we saw last year, which offered the same bundle for $5 per month for one year. However, if you missed that offer or just want to try out Disney+ and Hulu for a brief period of time, this is a good way to do so. Disney+ and Hulu make one of the most balanced streaming pairs available, blending family-friendly favorites with acclaimed originals and network TV staples. Disney+ brings a vast library of animated classics, blockbuster franchises and exclusive content from Marvel, Pixar, Star Wars and National Geographic. It’s the place to stream nearly every Star Wars film and series, plus the full Marvel Cinematic Universe lineup and Disney’s most recent theatrical releases. Hulu balances things out with a more adult-oriented lineup of current TV shows, next-day network episodes and a growing roster of award-winning originals. The platform hosts series like The Bear, The Handmaid’s Tale and Only Murders in the Building, alongside comedies, thrillers and documentaries that regularly feature in awards conversations. It’s also the home for next-day streaming of ABC and FX shows, making it especially useful if you’ve already cut the cable cord but still want to keep up with primetime TV. The Duo Basic bundle ties these two services together under a single subscription, offering a simple way to expand your library without juggling multiple accounts. This tier includes ads on both platforms, but the trade-off is significant savings compared with paying for each service separately. For many households, that’s an acceptable compromise when it means access to such a wide range of content. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/the-disney-hulu-bundle-is-on-sale-for-10-for-one-month-right-now-192814784.html?src=rss",
          "content": "The peak time for deals on streaming services — the holiday shopping season — has come and gone, but Disney is back with a fresh offer for the new year. New and eligible returning subscribers can get one month of the ad-supported Disney+ Hulu bundle for just $10. That's $3 off the usual monthly rate for the bundle, and more than 58 percent off if you consider the prices for each service individually (Disney+ at $12 per month and, separately, Hulu also at $12 per month). We'd be remiss if we didn't mention that this isn't quite as good as the Black Friday deal we saw last year, which offered the same bundle for $5 per month for one year. However, if you missed that offer or just want to try out Disney+ and Hulu for a brief period of time, this is a good way to do so. Disney+ and Hulu make one of the most balanced streaming pairs available, blending family-friendly favorites with acclaimed originals and network TV staples. Disney+ brings a vast library of animated classics, blockbuster franchises and exclusive content from Marvel, Pixar, Star Wars and National Geographic. It’s the place to stream nearly every Star Wars film and series, plus the full Marvel Cinematic Universe lineup and Disney’s most recent theatrical releases. Hulu balances things out with a more adult-oriented lineup of current TV shows, next-day network episodes and a growing roster of award-winning originals. The platform hosts series like The Bear, The Handmaid’s Tale and Only Murders in the Building, alongside comedies, thrillers and documentaries that regularly feature in awards conversations. It’s also the home for next-day streaming of ABC and FX shows, making it especially useful if you’ve already cut the cable cord but still want to keep up with primetime TV. The Duo Basic bundle ties these two services together under a single subscription, offering a simple way to expand your library without juggling multiple accounts. This tier includes ads on both platforms, but the trade-off is significant savings compared with paying for each service separately. For many households, that’s an acceptable compromise when it means access to such a wide range of content. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/the-disney-hulu-bundle-is-on-sale-for-10-for-one-month-right-now-192814784.html?src=rss",
          "feed_position": 6
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/orchestration/why-your-llm-bill-is-exploding-and-how-semantic-caching-can-cut-it-by-73",
          "published_at": "Mon, 12 Jan 2026 19:00:00 GMT",
          "title": "Why your LLM bill is exploding — and how semantic caching can cut it by 73%",
          "standfirst": "Our LLM API bill was growing 30% month-over-month. Traffic was increasing, but not that fast. When I analyzed our query logs, I found the real problem: Users ask the same questions in different ways.\"What&#x27;s your return policy?,\" \"How do I return something?\", and \"Can I get a refund?\" were all hitting our LLM separately, generating nearly identical responses, each incurring full API costs.Exact-match caching, the obvious first solution, captured only 18% of these redundant calls. The same semantic question, phrased differently, bypassed the cache entirely.So, I implemented semantic caching based on what queries mean, not how they&#x27;re worded. After implementing it, our cache hit rate increased to 67%, reducing LLM API costs by 73%. But getting there requires solving problems that naive implementations miss.Why exact-match caching falls shortTraditional caching uses query text as the cache key. This works when queries are identical:# Exact-match cachingcache_key = hash(query_text)if cache_key in cache: return cache[cache_key]But users don&#x27;t phrase questions identically. My analysis of 100,000 production queries found:Only 18% were exact duplicates of previous queries47% were semantically similar to previous queries (same intent, different wording)35% were genuinely novel queriesThat 47% represented massive cost savings we were missing. Each semantically-similar query triggered a full LLM call, generating a response nearly identical to one we&#x27;d already computed.Semantic caching architectureSemantic caching replaces text-based keys with embedding-based similarity lookup:class SemanticCache: def __init__(self, embedding_model, similarity_threshold=0.92): self.embedding_model = embedding_model self.threshold = similarity_threshold self.vector_store = VectorStore() # FAISS, Pinecone, etc. self.response_store = ResponseStore() # Redis, DynamoDB, etc. def get(self, query: str) -> Optional[str]: \"\"\"Return cached response if semantically similar query exists.\"\"\" query_embedding = self.embedding_model.encode(query) # Find most similar cached query matches = self.vector_store.search(query_embedding, top_k=1) if matches and matches[0].similarity >= self.threshold: cache_id = matches[0].id return self.response_store.get(cache_id) return None def set(self, query: str, response: str): \"\"\"Cache query-response pair.\"\"\" query_embedding = self.embedding_model.encode(query) cache_id = generate_id() self.vector_store.add(cache_id, query_embedding) self.response_store.set(cache_id, { &#x27;query&#x27;: query, &#x27;response&#x27;: response, &#x27;timestamp&#x27;: datetime.utcnow() })The key insight: Instead of hashing query text, I embed queries into vector space and find cached queries within a similarity threshold.The threshold problemThe similarity threshold is the critical parameter. Set it too high, and you miss valid cache hits. Set it too low, and you return wrong responses.Our initial threshold of 0.85 seemed reasonable; 85% similar should be \"the same question,\" right?Wrong. At 0.85, we got cache hits like:Query: \"How do I cancel my subscription?\"Cached: \"How do I cancel my order?\"Similarity: 0.87These are different questions with different answers. Returning the cached response would be incorrect.I discovered that optimal thresholds vary by query type:Query typeOptimal thresholdRationaleFAQ-style questions0.94High precision needed; wrong answers damage trustProduct searches0.88More tolerance for near-matchesSupport queries0.92Balance between coverage and accuracyTransactional queries0.97Very low tolerance for errorsI implemented query-type-specific thresholds:class AdaptiveSemanticCache: def __init__(self): self.thresholds = { &#x27;faq&#x27;: 0.94, &#x27;search&#x27;: 0.88, &#x27;support&#x27;: 0.92, &#x27;transactional&#x27;: 0.97, &#x27;default&#x27;: 0.92 } self.query_classifier = QueryClassifier() def get_threshold(self, query: str) -> float: query_type = self.query_classifier.classify(query) return self.thresholds.get(query_type, self.thresholds[&#x27;default&#x27;]) def get(self, query: str) -> Optional[str]: threshold = self.get_threshold(query) query_embedding = self.embedding_model.encode(query) matches = self.vector_store.search(query_embedding, top_k=1) if matches and matches[0].similarity >= threshold: return self.response_store.get(matches[0].id) return NoneThreshold tuning methodologyI couldn&#x27;t tune thresholds blindly. I needed ground truth on which query pairs were actually \"the same.\"Our methodology:Step 1: Sample query pairs. I sampled 5,000 query pairs at various similarity levels (0.80-0.99).Step 2: Human labeling. Annotators labeled each pair as \"same intent\" or \"different intent.\" I used three annotators per pair and took a majority vote.Step 3: Compute precision/recall curves. For each threshold, we computed:Precision: Of cache hits, what fraction had the same intent?Recall: Of same-intent pairs, what fraction did we cache-hit?def compute_precision_recall(pairs, labels, threshold): \"\"\"Compute precision and recall at given similarity threshold.\"\"\" predictions = [1 if pair.similarity >= threshold else 0 for pair in pairs] true_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 1) false_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 0) false_negatives = sum(1 for p, l in zip(predictions, labels) if p == 0 and l == 1) precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0 recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0 return precision, recallStep 4: Select threshold based on cost of errors. For FAQ queries where wrong answers damage trust, I optimized for precision (0.94 threshold gave 98% precision). For search queries where missing a cache hit just costs money, I optimized for recall (0.88 threshold).Latency overheadSemantic caching adds latency: You must embed the query and search the vector store before knowing whether to call the LLM.Our measurements:OperationLatency (p50)Latency (p99)Query embedding12ms28msVector search8ms19msTotal cache lookup20ms47msLLM API call850ms2400msThe 20ms overhead is negligible compared to the 850ms LLM call we avoid on cache hits. Even at p99, the 47ms overhead is acceptable.However, cache misses now take 20ms longer than before (embedding + search + LLM call). At our 67% hit rate, the math works out favorably:Before: 100% of queries × 850ms = 850ms averageAfter: (33% × 870ms) + (67% × 20ms) = 287ms + 13ms = 300ms averageNet latency improvement of 65% alongside the cost reduction.Cache invalidationCached responses go stale. Product information changes, policies update and yesterday&#x27;s correct answer becomes today&#x27;s wrong answer.I implemented three invalidation strategies:Time-based TTLSimple expiration based on content type:TTL_BY_CONTENT_TYPE = { &#x27;pricing&#x27;: timedelta(hours=4), # Changes frequently &#x27;policy&#x27;: timedelta(days=7), # Changes rarely &#x27;product_info&#x27;: timedelta(days=1), # Daily refresh &#x27;general_faq&#x27;: timedelta(days=14), # Very stable}Event-based invalidationWhen underlying data changes, invalidate related cache entries:class CacheInvalidator: def on_content_update(self, content_id: str, content_type: str): \"\"\"Invalidate cache entries related to updated content.\"\"\" # Find cached queries that referenced this content affected_queries = self.find_queries_referencing(content_id) for query_id in affected_queries: self.cache.invalidate(query_id) self.log_invalidation(content_id, len(affected_queries))Staleness detectionFor responses that might become stale without explicit events, I implemented periodic freshness checks:def check_freshness(self, cached_response: dict) -> bool: \"\"\"Verify cached response is still valid.\"\"\" # Re-run the query against current data fresh_response = self.generate_response(cached_response[&#x27;query&#x27;]) # Compare semantic similarity of responses cached_embedding = self.embed(cached_response[&#x27;response&#x27;]) fresh_embedding = self.embed(fresh_response) similarity = cosine_similarity(cached_embedding, fresh_embedding) # If responses diverged significantly, invalidate if similarity < 0.90: self.cache.invalidate(cached_response[&#x27;id&#x27;]) return False return TrueWe run freshness checks on a sample of cached entries daily, catching staleness that TTL and event-based invalidation miss.Production resultsAfter three months in production:MetricBeforeAfterChangeCache hit rate18%67%+272%LLM API costs$47K/month$12.7K/month-73%Average latency850ms300ms-65%False-positive rateN/A0.8%—Customer complaints (wrong answers)Baseline+0.3%Minimal increaseThe 0.8% false-positive rate (queries where we returned a cached response that was semantically incorrect) was within acceptable bounds. These cases occurred primarily at the boundaries of our threshold, where similarity was just above the cutoff but intent differed slightly.Pitfalls to avoidDon&#x27;t use a single global threshold. Different query types have different tolerance for errors. Tune thresholds per category.Don&#x27;t skip the embedding step on cache hits. You might be tempted to skip embedding overhead when returning cached responses, but you need the embedding for cache key generation. The overhead is unavoidable.Don&#x27;t forget invalidation. Semantic caching without invalidation strategy leads to stale responses that erode user trust. Build invalidation from day one.Don&#x27;t cache everything. Some queries shouldn&#x27;t be cached: Personalized responses, time-sensitive information, transactional confirmations. Build exclusion rules.def should_cache(self, query: str, response: str) -> bool: \"\"\"Determine if response should be cached.\"\" # Don&#x27;t cache personalized responses if self.contains_personal_info(response): return False # Don&#x27;t cache time-sensitive information if self.is_time_sensitive(query): return False # Don&#x27;t cache transactional confirmations if self.is_transactional(query): return False return TrueKey takeawaysSemantic caching is a practical pattern for LLM cost control that captures redundancy exact-match caching misses. The key challenges are threshold tuning (use query-type-specific thresholds based on precision/recall analysis) and cache invalidation (combine TTL, event-based and staleness detection).At 73% cost reduction, this was our highest-ROI optimization for production LLM systems. The implementation complexity is moderate, but the threshold tuning requires careful attention to avoid quality degradation.Sreenivasa Reddy Hulebeedu Reddy is a lead software engineer.",
          "content": "Our LLM API bill was growing 30% month-over-month. Traffic was increasing, but not that fast. When I analyzed our query logs, I found the real problem: Users ask the same questions in different ways.\"What&#x27;s your return policy?,\" \"How do I return something?\", and \"Can I get a refund?\" were all hitting our LLM separately, generating nearly identical responses, each incurring full API costs.Exact-match caching, the obvious first solution, captured only 18% of these redundant calls. The same semantic question, phrased differently, bypassed the cache entirely.So, I implemented semantic caching based on what queries mean, not how they&#x27;re worded. After implementing it, our cache hit rate increased to 67%, reducing LLM API costs by 73%. But getting there requires solving problems that naive implementations miss.Why exact-match caching falls shortTraditional caching uses query text as the cache key. This works when queries are identical:# Exact-match cachingcache_key = hash(query_text)if cache_key in cache: return cache[cache_key]But users don&#x27;t phrase questions identically. My analysis of 100,000 production queries found:Only 18% were exact duplicates of previous queries47% were semantically similar to previous queries (same intent, different wording)35% were genuinely novel queriesThat 47% represented massive cost savings we were missing. Each semantically-similar query triggered a full LLM call, generating a response nearly identical to one we&#x27;d already computed.Semantic caching architectureSemantic caching replaces text-based keys with embedding-based similarity lookup:class SemanticCache: def __init__(self, embedding_model, similarity_threshold=0.92): self.embedding_model = embedding_model self.threshold = similarity_threshold self.vector_store = VectorStore() # FAISS, Pinecone, etc. self.response_store = ResponseStore() # Redis, DynamoDB, etc. def get(self, query: str) -> Optional[str]: \"\"\"Return cached response if semantically similar query exists.\"\"\" query_embedding = self.embedding_model.encode(query) # Find most similar cached query matches = self.vector_store.search(query_embedding, top_k=1) if matches and matches[0].similarity >= self.threshold: cache_id = matches[0].id return self.response_store.get(cache_id) return None def set(self, query: str, response: str): \"\"\"Cache query-response pair.\"\"\" query_embedding = self.embedding_model.encode(query) cache_id = generate_id() self.vector_store.add(cache_id, query_embedding) self.response_store.set(cache_id, { &#x27;query&#x27;: query, &#x27;response&#x27;: response, &#x27;timestamp&#x27;: datetime.utcnow() })The key insight: Instead of hashing query text, I embed queries into vector space and find cached queries within a similarity threshold.The threshold problemThe similarity threshold is the critical parameter. Set it too high, and you miss valid cache hits. Set it too low, and you return wrong responses.Our initial threshold of 0.85 seemed reasonable; 85% similar should be \"the same question,\" right?Wrong. At 0.85, we got cache hits like:Query: \"How do I cancel my subscription?\"Cached: \"How do I cancel my order?\"Similarity: 0.87These are different questions with different answers. Returning the cached response would be incorrect.I discovered that optimal thresholds vary by query type:Query typeOptimal thresholdRationaleFAQ-style questions0.94High precision needed; wrong answers damage trustProduct searches0.88More tolerance for near-matchesSupport queries0.92Balance between coverage and accuracyTransactional queries0.97Very low tolerance for errorsI implemented query-type-specific thresholds:class AdaptiveSemanticCache: def __init__(self): self.thresholds = { &#x27;faq&#x27;: 0.94, &#x27;search&#x27;: 0.88, &#x27;support&#x27;: 0.92, &#x27;transactional&#x27;: 0.97, &#x27;default&#x27;: 0.92 } self.query_classifier = QueryClassifier() def get_threshold(self, query: str) -> float: query_type = self.query_classifier.classify(query) return self.thresholds.get(query_type, self.thresholds[&#x27;default&#x27;]) def get(self, query: str) -> Optional[str]: threshold = self.get_threshold(query) query_embedding = self.embedding_model.encode(query) matches = self.vector_store.search(query_embedding, top_k=1) if matches and matches[0].similarity >= threshold: return self.response_store.get(matches[0].id) return NoneThreshold tuning methodologyI couldn&#x27;t tune thresholds blindly. I needed ground truth on which query pairs were actually \"the same.\"Our methodology:Step 1: Sample query pairs. I sampled 5,000 query pairs at various similarity levels (0.80-0.99).Step 2: Human labeling. Annotators labeled each pair as \"same intent\" or \"different intent.\" I used three annotators per pair and took a majority vote.Step 3: Compute precision/recall curves. For each threshold, we computed:Precision: Of cache hits, what fraction had the same intent?Recall: Of same-intent pairs, what fraction did we cache-hit?def compute_precision_recall(pairs, labels, threshold): \"\"\"Compute precision and recall at given similarity threshold.\"\"\" predictions = [1 if pair.similarity >= threshold else 0 for pair in pairs] true_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 1) false_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 0) false_negatives = sum(1 for p, l in zip(predictions, labels) if p == 0 and l == 1) precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0 recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0 return precision, recallStep 4: Select threshold based on cost of errors. For FAQ queries where wrong answers damage trust, I optimized for precision (0.94 threshold gave 98% precision). For search queries where missing a cache hit just costs money, I optimized for recall (0.88 threshold).Latency overheadSemantic caching adds latency: You must embed the query and search the vector store before knowing whether to call the LLM.Our measurements:OperationLatency (p50)Latency (p99)Query embedding12ms28msVector search8ms19msTotal cache lookup20ms47msLLM API call850ms2400msThe 20ms overhead is negligible compared to the 850ms LLM call we avoid on cache hits. Even at p99, the 47ms overhead is acceptable.However, cache misses now take 20ms longer than before (embedding + search + LLM call). At our 67% hit rate, the math works out favorably:Before: 100% of queries × 850ms = 850ms averageAfter: (33% × 870ms) + (67% × 20ms) = 287ms + 13ms = 300ms averageNet latency improvement of 65% alongside the cost reduction.Cache invalidationCached responses go stale. Product information changes, policies update and yesterday&#x27;s correct answer becomes today&#x27;s wrong answer.I implemented three invalidation strategies:Time-based TTLSimple expiration based on content type:TTL_BY_CONTENT_TYPE = { &#x27;pricing&#x27;: timedelta(hours=4), # Changes frequently &#x27;policy&#x27;: timedelta(days=7), # Changes rarely &#x27;product_info&#x27;: timedelta(days=1), # Daily refresh &#x27;general_faq&#x27;: timedelta(days=14), # Very stable}Event-based invalidationWhen underlying data changes, invalidate related cache entries:class CacheInvalidator: def on_content_update(self, content_id: str, content_type: str): \"\"\"Invalidate cache entries related to updated content.\"\"\" # Find cached queries that referenced this content affected_queries = self.find_queries_referencing(content_id) for query_id in affected_queries: self.cache.invalidate(query_id) self.log_invalidation(content_id, len(affected_queries))Staleness detectionFor responses that might become stale without explicit events, I implemented periodic freshness checks:def check_freshness(self, cached_response: dict) -> bool: \"\"\"Verify cached response is still valid.\"\"\" # Re-run the query against current data fresh_response = self.generate_response(cached_response[&#x27;query&#x27;]) # Compare semantic similarity of responses cached_embedding = self.embed(cached_response[&#x27;response&#x27;]) fresh_embedding = self.embed(fresh_response) similarity = cosine_similarity(cached_embedding, fresh_embedding) # If responses diverged significantly, invalidate if similarity < 0.90: self.cache.invalidate(cached_response[&#x27;id&#x27;]) return False return TrueWe run freshness checks on a sample of cached entries daily, catching staleness that TTL and event-based invalidation miss.Production resultsAfter three months in production:MetricBeforeAfterChangeCache hit rate18%67%+272%LLM API costs$47K/month$12.7K/month-73%Average latency850ms300ms-65%False-positive rateN/A0.8%—Customer complaints (wrong answers)Baseline+0.3%Minimal increaseThe 0.8% false-positive rate (queries where we returned a cached response that was semantically incorrect) was within acceptable bounds. These cases occurred primarily at the boundaries of our threshold, where similarity was just above the cutoff but intent differed slightly.Pitfalls to avoidDon&#x27;t use a single global threshold. Different query types have different tolerance for errors. Tune thresholds per category.Don&#x27;t skip the embedding step on cache hits. You might be tempted to skip embedding overhead when returning cached responses, but you need the embedding for cache key generation. The overhead is unavoidable.Don&#x27;t forget invalidation. Semantic caching without invalidation strategy leads to stale responses that erode user trust. Build invalidation from day one.Don&#x27;t cache everything. Some queries shouldn&#x27;t be cached: Personalized responses, time-sensitive information, transactional confirmations. Build exclusion rules.def should_cache(self, query: str, response: str) -> bool: \"\"\"Determine if response should be cached.\"\" # Don&#x27;t cache personalized responses if self.contains_personal_info(response): return False # Don&#x27;t cache time-sensitive information if self.is_time_sensitive(query): return False # Don&#x27;t cache transactional confirmations if self.is_transactional(query): return False return TrueKey takeawaysSemantic caching is a practical pattern for LLM cost control that captures redundancy exact-match caching misses. The key challenges are threshold tuning (use query-type-specific thresholds based on precision/recall analysis) and cache invalidation (combine TTL, event-based and staleness detection).At 73% cost reduction, this was our highest-ROI optimization for production LLM systems. The implementation complexity is moderate, but the threshold tuning requires careful attention to avoid quality degradation.Sreenivasa Reddy Hulebeedu Reddy is a lead software engineer.",
          "feed_position": 0,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/7iyQoeSwdOqqpfcE0PFWgF/48db7d0305019eee107028d9f018d2ac/Semantic_caching.png?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/entertainment/paramount-wont-quit-files-suit-against-warner-bros-discovery-over-rejected-bid-175317166.html",
          "published_at": "Mon, 12 Jan 2026 17:53:17 +0000",
          "title": "Paramount won't quit, files suit against Warner Bros. Discovery over rejected bid",
          "standfirst": "Paramount Skydance just does not want to take no for an answer. After having multiple bids to acquire Warner Bros. Discovery (WBD) rejected, including a recent hostile bid that the WBD board recommended that shareholders reject, Paramount is turning to the courts and mounting a proxy fight. In a letter to shareholders on Monday, Paramount CEO David Ellison said the company has filed suit in Delaware Chancery Court seeking more disclosure about WBD’s pending Netflix deal and the process that led to its acceptance. Paramount argues WBD hasn’t provided “basic information” shareholders need to evaluate competing offers, including how WBD valued the planned cable-networks spinout Discovery Global (or Global Networks, depending on the filing). The Netflix acquisition would leave Discovery Global to become its own publicly traded company, while the Paramount offer included these assets. Paramount is also escalating the corporate pressure campaign, with Ellison saying it intends to nominate a slate of directors for election at WBD’s 2026 annual meeting. The end goal would be installing a board that would “engage” on Paramount’s offer under the terms of WBD’s merger agreement with Netflix. If WBD were to call a special meeting to approve the Netflix transaction before the annual meeting, Paramount says it will solicit proxy votes against the deal. It also plans to push a bylaw change requiring shareholders to approve any separation of Discovery Global. This change seems like Paramount stoking the flames (whether real or imagined) surrounding shareholders having their WBD shares bought out without the value of Discovery Global built-in under the Netflix merger. Paramount remains convinced that its offer is \"superior\" to that of Netflix, while WBD maintains Paramount's bid offers \"insufficient value\" and that Paramount has failed to submit a true best proposal \"despite clear direction from WBD on both the deficiencies and potential solutions.\" The lawsuit now aims to force WBD to spell out exactly how it arrived at recommending the Netflix deal over Paramount's bid. WBD expressed concerns over whether a potential Paramount deal would even reach closing, citing the substantial debt the smaller studio would have to take on to pull off a leveraged buyout.This article originally appeared on Engadget at https://www.engadget.com/entertainment/paramount-wont-quit-files-suit-against-warner-bros-discovery-over-rejected-bid-175317166.html?src=rss",
          "content": "Paramount Skydance just does not want to take no for an answer. After having multiple bids to acquire Warner Bros. Discovery (WBD) rejected, including a recent hostile bid that the WBD board recommended that shareholders reject, Paramount is turning to the courts and mounting a proxy fight. In a letter to shareholders on Monday, Paramount CEO David Ellison said the company has filed suit in Delaware Chancery Court seeking more disclosure about WBD’s pending Netflix deal and the process that led to its acceptance. Paramount argues WBD hasn’t provided “basic information” shareholders need to evaluate competing offers, including how WBD valued the planned cable-networks spinout Discovery Global (or Global Networks, depending on the filing). The Netflix acquisition would leave Discovery Global to become its own publicly traded company, while the Paramount offer included these assets. Paramount is also escalating the corporate pressure campaign, with Ellison saying it intends to nominate a slate of directors for election at WBD’s 2026 annual meeting. The end goal would be installing a board that would “engage” on Paramount’s offer under the terms of WBD’s merger agreement with Netflix. If WBD were to call a special meeting to approve the Netflix transaction before the annual meeting, Paramount says it will solicit proxy votes against the deal. It also plans to push a bylaw change requiring shareholders to approve any separation of Discovery Global. This change seems like Paramount stoking the flames (whether real or imagined) surrounding shareholders having their WBD shares bought out without the value of Discovery Global built-in under the Netflix merger. Paramount remains convinced that its offer is \"superior\" to that of Netflix, while WBD maintains Paramount's bid offers \"insufficient value\" and that Paramount has failed to submit a true best proposal \"despite clear direction from WBD on both the deficiencies and potential solutions.\" The lawsuit now aims to force WBD to spell out exactly how it arrived at recommending the Netflix deal over Paramount's bid. WBD expressed concerns over whether a potential Paramount deal would even reach closing, citing the substantial debt the smaller studio would have to take on to pull off a leveraged buyout.This article originally appeared on Engadget at https://www.engadget.com/entertainment/paramount-wont-quit-files-suit-against-warner-bros-discovery-over-rejected-bid-175317166.html?src=rss",
          "feed_position": 8
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/security/nvidia-rubin-rack-scale-encryption-enterprise-ai-security",
          "published_at": "Mon, 12 Jan 2026 16:00:00 GMT",
          "title": "Nvidia Rubin's rack-scale encryption signals a turning point for enterprise AI security",
          "standfirst": "Nvidia&#x27;s Vera Rubin NVL72, announced at CES 2026, encrypts every bus across 72 GPUs, 36 CPUs, and the entire NVLink fabric. It&#x27;s the first rack-scale platform to deliver confidential computing across CPU, GPU, and NVLink domains.For security leaders, this fundamentally shifts the conversation. Rather than attempting to secure complex hybrid cloud configurations through contractual trust with cloud providers, they can verify them cryptographically. That’s a critical distinction that matters when nation-state adversaries have proven they are capable of launching targeted cyberattacks at machine speed.The brutal economics of unprotected AIEpoch AI research shows frontier training costs have grown at 2.4x annually since 2016, which means billion-dollar training runs could be a reality within a few short years. Yet the infrastructure protecting these investments remains fundamentally insecure in most deployments. Security budgets created to protect frontier training models aren&#x27;t keeping up with the exceptionally fast pace of model training. The result is that more models are under threat as existing approaches can&#x27;t scale and keep up with adversaries&#x27; tradecraft.IBM&#x27;s 2025 Cost of Data Breach Report found that 13% of organizations experienced breaches of AI models or applications. Among those breached, 97% lacked proper AI access controls. Shadow AI incidents cost $4.63 million on average, or $670,000 more than standard breaches, with one in five breaches now involving unsanctioned tools that disproportionately expose customer PII (65%) and intellectual property (40%).Think about what this means for organizations spending $50 million or $500 million on a training run. Their model weights sit in multi-tenant environments where cloud providers can inspect the data. Hardware-level encryption that proves the environment hasn&#x27;t been tampered with changes that financial equation entirely.The GTG-1002 wake-up callIn November 2025, Anthropic disclosed something unprecedented: A Chinese state-sponsored group designated GTG-1002 had manipulated Claude Code to conduct what the company described as the first documented case of a large-scale cyberattack executed without substantial human intervention.State-sponsored adversaries turned it into an autonomous intrusion agent that discovered vulnerabilities, crafted exploits, harvested credentials, moved laterally through networks, and categorized stolen data by intelligence value. Human operators stepped in only at critical junctures. According to Anthropic&#x27;s analysis, the AI executed around 80 to 90% of all tactical work independently.The implications extend beyond this single incident. Attack surfaces that once required teams of experienced attackers can now be probed at machine speed by opponents with access to foundation models.Comparing the performance of Blackwell vs. RubinSpecificationBlackwell GB300 NVL72Rubin NVL72Inference compute (FP4)1.44 exaFLOPS3.6 exaFLOPSNVFP4 per GPU (inference)20 PFLOPS50 PFLOPSPer-GPU NVLink bandwidth1.8 TB/s3.6 TB/sRack NVLink bandwidth130 TB/s260 TB/sHBM bandwidth per GPU~8 TB/s~22 TB/sIndustry momentum and AMD&#x27;s alternativeNvidia isn&#x27;t operating in isolation. Research from the Confidential Computing Consortium and IDC, released in December, found that 75% of organizations are adopting confidential computing, with 18% already in production and 57% piloting deployments.\"Confidential Computing has grown from a niche concept into a vital strategy for data security and trusted AI innovation,\" said Nelly Porter, governing board chair of the Confidential Computing Consortium. Real barriers remain: attestation validation challenges affect 84% of respondents, and a skills gap hampers 75%.AMD&#x27;s Helios rack takes a different approach. Built on Meta&#x27;s Open Rack Wide specification, announced at OCP Global Summit in October 2025, it delivers approximately 2.9 exaflops of FP4 compute with 31 TB of HBM4 memory and 1.4 PB/s aggregate bandwidth. Where Nvidia designs confidential computing into every component, AMD prioritizes open standards through the Ultra Accelerator Link and Ultra Ethernet consortia. The competition between Nvidia and AMD is giving security leaders more of a choice than they otherwise would have had. Comparing the tradeoffs of Nvidia&#x27;s integrated approach versus AMD&#x27;s open-standards flexibility for their specific infrastructures and business-specific threat models is key.What security leaders are doing nowHardware-level confidentiality doesn&#x27;t replace zero-trust principles; it gives them teeth. What Nvidia and AMD are building lets security leaders verify trust cryptographically rather than assume it contractually. That&#x27;s a meaningful shift for anyone running sensitive workloads on shared infrastructure. And if the attestation claims hold up in production, this approach could let enterprises extend zero-trust enforcement across thousands of nodes without the policy sprawl and agent overhead that software-only implementations require.Before deployment: Verify attestation to confirm environments haven&#x27;t been tampered with. Cryptographic proof of compliance should be a prerequisite for signing contracts, not an afterthought or worse, a nice-to-have. If your cloud provider can&#x27;t demonstrate attestation capabilities, that&#x27;s a question worth raising in your next QBR.During operation: Maintain separate enclaves for training and inference, and include security teams in the model pipeline from the very start. IBM&#x27;s research showed 63% of breached organizations had no AI governance policy. You can&#x27;t bolt security on after development; that translates into an onramp for mediocre security design-ins and lengthy red teaming that catches bugs that needed to be engineered out of a model or app early.Across the organization: Run joint exercises between security and data science teams to surface vulnerabilities before attackers find them. Shadow AI accounted for 20% of breaches and exposed customer PII and IP at higher rates than other breach types.Bottom line The GTG-1002 campaign demonstrated that adversaries can now automate large-scale intrusions with minimal human oversight at scale. Nearly every organization that experienced an AI-related breach lacked proper access controls.Nvidia&#x27;s Vera Rubin NVL72 transforms racks from potential liabilities into cryptographically attested assets by encrypting every bus. AMD&#x27;s Helios offers an open-standards alternative. Hardware confidentiality alone won&#x27;t stop a determined adversary, but combined with strong governance and realistic threat exercises, rack-scale encryption gives security leaders the foundation they need to protect investments measured in hundreds of millions of dollars.The question facing CISOs isn&#x27;t whether attested infrastructure is worth it. It&#x27;s whether organizations building high-value AI models can afford to operate without it.",
          "content": "Nvidia&#x27;s Vera Rubin NVL72, announced at CES 2026, encrypts every bus across 72 GPUs, 36 CPUs, and the entire NVLink fabric. It&#x27;s the first rack-scale platform to deliver confidential computing across CPU, GPU, and NVLink domains.For security leaders, this fundamentally shifts the conversation. Rather than attempting to secure complex hybrid cloud configurations through contractual trust with cloud providers, they can verify them cryptographically. That’s a critical distinction that matters when nation-state adversaries have proven they are capable of launching targeted cyberattacks at machine speed.The brutal economics of unprotected AIEpoch AI research shows frontier training costs have grown at 2.4x annually since 2016, which means billion-dollar training runs could be a reality within a few short years. Yet the infrastructure protecting these investments remains fundamentally insecure in most deployments. Security budgets created to protect frontier training models aren&#x27;t keeping up with the exceptionally fast pace of model training. The result is that more models are under threat as existing approaches can&#x27;t scale and keep up with adversaries&#x27; tradecraft.IBM&#x27;s 2025 Cost of Data Breach Report found that 13% of organizations experienced breaches of AI models or applications. Among those breached, 97% lacked proper AI access controls. Shadow AI incidents cost $4.63 million on average, or $670,000 more than standard breaches, with one in five breaches now involving unsanctioned tools that disproportionately expose customer PII (65%) and intellectual property (40%).Think about what this means for organizations spending $50 million or $500 million on a training run. Their model weights sit in multi-tenant environments where cloud providers can inspect the data. Hardware-level encryption that proves the environment hasn&#x27;t been tampered with changes that financial equation entirely.The GTG-1002 wake-up callIn November 2025, Anthropic disclosed something unprecedented: A Chinese state-sponsored group designated GTG-1002 had manipulated Claude Code to conduct what the company described as the first documented case of a large-scale cyberattack executed without substantial human intervention.State-sponsored adversaries turned it into an autonomous intrusion agent that discovered vulnerabilities, crafted exploits, harvested credentials, moved laterally through networks, and categorized stolen data by intelligence value. Human operators stepped in only at critical junctures. According to Anthropic&#x27;s analysis, the AI executed around 80 to 90% of all tactical work independently.The implications extend beyond this single incident. Attack surfaces that once required teams of experienced attackers can now be probed at machine speed by opponents with access to foundation models.Comparing the performance of Blackwell vs. RubinSpecificationBlackwell GB300 NVL72Rubin NVL72Inference compute (FP4)1.44 exaFLOPS3.6 exaFLOPSNVFP4 per GPU (inference)20 PFLOPS50 PFLOPSPer-GPU NVLink bandwidth1.8 TB/s3.6 TB/sRack NVLink bandwidth130 TB/s260 TB/sHBM bandwidth per GPU~8 TB/s~22 TB/sIndustry momentum and AMD&#x27;s alternativeNvidia isn&#x27;t operating in isolation. Research from the Confidential Computing Consortium and IDC, released in December, found that 75% of organizations are adopting confidential computing, with 18% already in production and 57% piloting deployments.\"Confidential Computing has grown from a niche concept into a vital strategy for data security and trusted AI innovation,\" said Nelly Porter, governing board chair of the Confidential Computing Consortium. Real barriers remain: attestation validation challenges affect 84% of respondents, and a skills gap hampers 75%.AMD&#x27;s Helios rack takes a different approach. Built on Meta&#x27;s Open Rack Wide specification, announced at OCP Global Summit in October 2025, it delivers approximately 2.9 exaflops of FP4 compute with 31 TB of HBM4 memory and 1.4 PB/s aggregate bandwidth. Where Nvidia designs confidential computing into every component, AMD prioritizes open standards through the Ultra Accelerator Link and Ultra Ethernet consortia. The competition between Nvidia and AMD is giving security leaders more of a choice than they otherwise would have had. Comparing the tradeoffs of Nvidia&#x27;s integrated approach versus AMD&#x27;s open-standards flexibility for their specific infrastructures and business-specific threat models is key.What security leaders are doing nowHardware-level confidentiality doesn&#x27;t replace zero-trust principles; it gives them teeth. What Nvidia and AMD are building lets security leaders verify trust cryptographically rather than assume it contractually. That&#x27;s a meaningful shift for anyone running sensitive workloads on shared infrastructure. And if the attestation claims hold up in production, this approach could let enterprises extend zero-trust enforcement across thousands of nodes without the policy sprawl and agent overhead that software-only implementations require.Before deployment: Verify attestation to confirm environments haven&#x27;t been tampered with. Cryptographic proof of compliance should be a prerequisite for signing contracts, not an afterthought or worse, a nice-to-have. If your cloud provider can&#x27;t demonstrate attestation capabilities, that&#x27;s a question worth raising in your next QBR.During operation: Maintain separate enclaves for training and inference, and include security teams in the model pipeline from the very start. IBM&#x27;s research showed 63% of breached organizations had no AI governance policy. You can&#x27;t bolt security on after development; that translates into an onramp for mediocre security design-ins and lengthy red teaming that catches bugs that needed to be engineered out of a model or app early.Across the organization: Run joint exercises between security and data science teams to surface vulnerabilities before attackers find them. Shadow AI accounted for 20% of breaches and exposed customer PII and IP at higher rates than other breach types.Bottom line The GTG-1002 campaign demonstrated that adversaries can now automate large-scale intrusions with minimal human oversight at scale. Nearly every organization that experienced an AI-related breach lacked proper access controls.Nvidia&#x27;s Vera Rubin NVL72 transforms racks from potential liabilities into cryptographically attested assets by encrypting every bus. AMD&#x27;s Helios offers an open-standards alternative. Hardware confidentiality alone won&#x27;t stop a determined adversary, but combined with strong governance and realistic threat exercises, rack-scale encryption gives security leaders the foundation they need to protect investments measured in hundreds of millions of dollars.The question facing CISOs isn&#x27;t whether attested infrastructure is worth it. It&#x27;s whether organizations building high-value AI models can afford to operate without it.",
          "feed_position": 1,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/2YSkElI5OHmat9celGziVv/a23ebd445f5a38cd5062055b9dd3b15e/jenson_at_ces.jpg?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/apps/monarch-money-is-offering-50-percent-off-its-budgeting-app-for-new-users-204507767.html",
          "published_at": "Mon, 12 Jan 2026 15:45:37 +0000",
          "title": "Monarch Money is offering 50 percent off its budgeting app for new users",
          "standfirst": "The start of the new year is a great time to get your finances in order, and a good budgeting app can help with that. Instead of laboring over a spreadsheet, you can try one of our favorite budgeting apps for less than usual. Monarch Money is running a sale that gives new users 50 percent off one year of the service, bringing the final cost down to just $50. Just use the code NEWYEAR2026 at checkout to get the discount. Monarch Money makes for a capable and detailed budgeting companion. You can use the service via apps for iOS, Android, iPadOS or the web, and Monarch also offers a Chrome extension that can sync your Amazon and Target transactions and automatically categorize them. Like other budgeting apps, Monarch Money lets you connect multiple financial accounts and track your money based on where you spend it over time. Monarch offers two different approaches to tracking budgeting (flexible and category budgeting) depending on what fits your life best, and the ability to add a budget widget on your phone so you can know how you're tracking that month. How budgeting apps turn your raw transactions into visuals you can understand at a glance is one of the big things that differentiates one app from another, and Monarch Money offers multiple graphs and charts to look at for things like spending, investments or categories of your choice based on how you've labelled your expenses. The app can also monitor the spending of you and your partner all in one place, to make it easier to plan together. The main drawbacks Engadget found in testing Monarch Money were the app's learning curve, and the differences in features (and bugginess) between Monarch's web and mobile versions. Still, for 50 percent off, the Monarch Money is well worth experimenting with if you're trying to save money in 2026, especially if you want to do it collaboratively with a partner. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/apps/monarch-money-is-offering-50-percent-off-its-budgeting-app-for-new-users-204507767.html?src=rss",
          "content": "The start of the new year is a great time to get your finances in order, and a good budgeting app can help with that. Instead of laboring over a spreadsheet, you can try one of our favorite budgeting apps for less than usual. Monarch Money is running a sale that gives new users 50 percent off one year of the service, bringing the final cost down to just $50. Just use the code NEWYEAR2026 at checkout to get the discount. Monarch Money makes for a capable and detailed budgeting companion. You can use the service via apps for iOS, Android, iPadOS or the web, and Monarch also offers a Chrome extension that can sync your Amazon and Target transactions and automatically categorize them. Like other budgeting apps, Monarch Money lets you connect multiple financial accounts and track your money based on where you spend it over time. Monarch offers two different approaches to tracking budgeting (flexible and category budgeting) depending on what fits your life best, and the ability to add a budget widget on your phone so you can know how you're tracking that month. How budgeting apps turn your raw transactions into visuals you can understand at a glance is one of the big things that differentiates one app from another, and Monarch Money offers multiple graphs and charts to look at for things like spending, investments or categories of your choice based on how you've labelled your expenses. The app can also monitor the spending of you and your partner all in one place, to make it easier to plan together. The main drawbacks Engadget found in testing Monarch Money were the app's learning curve, and the differences in features (and bugginess) between Monarch's web and mobile versions. Still, for 50 percent off, the Monarch Money is well worth experimenting with if you're trying to save money in 2026, especially if you want to do it collaboratively with a partner. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/apps/monarch-money-is-offering-50-percent-off-its-budgeting-app-for-new-users-204507767.html?src=rss",
          "feed_position": 12
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/audible-deal-get-three-months-of-access-for-only-3-193859213.html",
          "published_at": "Mon, 12 Jan 2026 14:35:35 +0000",
          "title": "Audible deal: Get three months of access for only $3",
          "standfirst": "One way to read more in the new year is to incorporate audiobooks as part of your reading habit. Audible is having a sale right now that makes that easier and cheaper to do: you can get three months of access for only $1 per month, or a total of $3. The promotion runs through January 21. An Audible subscription grants one audiobook per month to keep. This can be selected from a massive catalog of new releases and bestsellers. The collection here has just about everything. However, it's easy to plow through a single book in a month. Users also get streaming access to thousands of curated titles. Think of it like Netflix for audiobooks. The catalog is limited, but it gets the job done in a pinch. Subscribers do get access to all Audible original content and they will receive discounts on purchasing audiobooks outright. In other words, it's a neat little service and well worth a buck. The regular price is $15, so make sure to cancel at the end of that three months if you aren't enjoying the platform. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/audible-deal-get-three-months-of-access-for-only-3-193859213.html?src=rss",
          "content": "One way to read more in the new year is to incorporate audiobooks as part of your reading habit. Audible is having a sale right now that makes that easier and cheaper to do: you can get three months of access for only $1 per month, or a total of $3. The promotion runs through January 21. An Audible subscription grants one audiobook per month to keep. This can be selected from a massive catalog of new releases and bestsellers. The collection here has just about everything. However, it's easy to plow through a single book in a month. Users also get streaming access to thousands of curated titles. Think of it like Netflix for audiobooks. The catalog is limited, but it gets the job done in a pinch. Subscribers do get access to all Audible original content and they will receive discounts on purchasing audiobooks outright. In other words, it's a neat little service and well worth a buck. The regular price is $15, so make sure to cancel at the end of that three months if you aren't enjoying the platform. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/audible-deal-get-three-months-of-access-for-only-3-193859213.html?src=rss",
          "feed_position": 14
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/apples-mac-mini-m4-is-back-on-sale-for-499-141615231.html",
          "published_at": "Mon, 12 Jan 2026 14:16:16 +0000",
          "title": "Apple's Mac mini M4 is back on sale for $499",
          "standfirst": "The holiday season is fully in the rear view mirror and real life is here to stay. But that doesn't mean the time for gifts is over — especially ones for yourself. You can still take advantage of great January sales on some awesome tech products. Take the Apple Mac mini M4, which is down to $500 from $599. The 17 percent discount gives you 16GB of RAM and 256GB of SSD for only about $20 more than the computer's Black Friday sale. Its beefier models are also on sale: opting for 512GB of SSD will cost you $690, down from $799, while also upping your RAM to 24GB is available for $890, dropping from $999. We gave the Apple Mac mini M4 a 90 in our review thanks in large part to its powerful chip. The M4 works very fast despite being in such a small device. It also offers front-facing headphone and USB-C ports. You can further upgrade to the Apple M4 Pro chip for $1,270, down from $1,399 — a nine percent discount. The Pro model also has Thunderbolt 5 support. Check out our coverage of the best Apple deals for more discounts, and follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/apples-mac-mini-m4-is-back-on-sale-for-499-141615231.html?src=rss",
          "content": "The holiday season is fully in the rear view mirror and real life is here to stay. But that doesn't mean the time for gifts is over — especially ones for yourself. You can still take advantage of great January sales on some awesome tech products. Take the Apple Mac mini M4, which is down to $500 from $599. The 17 percent discount gives you 16GB of RAM and 256GB of SSD for only about $20 more than the computer's Black Friday sale. Its beefier models are also on sale: opting for 512GB of SSD will cost you $690, down from $799, while also upping your RAM to 24GB is available for $890, dropping from $999. We gave the Apple Mac mini M4 a 90 in our review thanks in large part to its powerful chip. The M4 works very fast despite being in such a small device. It also offers front-facing headphone and USB-C ports. You can further upgrade to the Apple M4 Pro chip for $1,270, down from $1,399 — a nine percent discount. The Pro model also has Thunderbolt 5 support. Check out our coverage of the best Apple deals for more discounts, and follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/apples-mac-mini-m4-is-back-on-sale-for-499-141615231.html?src=rss",
          "feed_position": 15
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/get-apples-25w-magsafe-charger-for-only-30-right-now-141707104.html",
          "published_at": "Mon, 12 Jan 2026 13:36:26 +0000",
          "title": "Get Apple's 25W MagSafe charger for only $30 right now",
          "standfirst": "One way you can reduce the number of cables you have to deal with on the regular is by investing in a few wireless chargers. Those with iPhones should consider Apple's own MagSafe charger not only because of its sleek and effective design, but also because it's on sale right now at Amazon. The Qi2.2-rated MagSafe charger is down to $30 for the one-meter version, or $40 for the two-meter version. If you have an iPhone 16, iPhone 17 or iPhone Air, this cable can charge your device at 25W as long as it's connected to a 30W power adapter on the other end. While you'll need a more recent iPhone to get the fastest MagSafe charging speeds, the charger can wirelessly top up the battery of any iPhone from the last eight years (iPhone 8 and later). With older iPhones, the charging speed tops out at 15W. The cable works with AirPods wireless charging cases too — it's certified for Qi2.2 and Qi charging. The MagSafe charger is one of our favorite iPhone accessories, and would pair quite nicely with your new iPhone if you're picking up one of the latest models. If you're on the fence about that, be sure to check out our reviews of the iPhone 17, iPhone Pro/Pro Max and iPhone Air. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/get-apples-25w-magsafe-charger-for-only-30-right-now-141707104.html?src=rss",
          "content": "One way you can reduce the number of cables you have to deal with on the regular is by investing in a few wireless chargers. Those with iPhones should consider Apple's own MagSafe charger not only because of its sleek and effective design, but also because it's on sale right now at Amazon. The Qi2.2-rated MagSafe charger is down to $30 for the one-meter version, or $40 for the two-meter version. If you have an iPhone 16, iPhone 17 or iPhone Air, this cable can charge your device at 25W as long as it's connected to a 30W power adapter on the other end. While you'll need a more recent iPhone to get the fastest MagSafe charging speeds, the charger can wirelessly top up the battery of any iPhone from the last eight years (iPhone 8 and later). With older iPhones, the charging speed tops out at 15W. The cable works with AirPods wireless charging cases too — it's certified for Qi2.2 and Qi charging. The MagSafe charger is one of our favorite iPhone accessories, and would pair quite nicely with your new iPhone if you're picking up one of the latest models. If you're on the fence about that, be sure to check out our reviews of the iPhone 17, iPhone Pro/Pro Max and iPhone Air. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/get-apples-25w-magsafe-charger-for-only-30-right-now-141707104.html?src=rss",
          "feed_position": 17
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/this-elevationlabs-10-year-extended-battery-case-for-airtags-is-on-sale-for-only-16-162308983.html",
          "published_at": "Mon, 12 Jan 2026 13:14:15 +0000",
          "title": "This ElevationLabs 10-year extended battery case for AirTags is on sale for only $16",
          "standfirst": "ElevationLab makes a battery case for your AirTag that can power it for 10 years and the accessory is on sale now for 30 percent off. Normally retailing for $23, you can pick one up for $16. The TimeCapsule case uses two AA batteries to offer up to 14 times the lifespan of the CR2032 battery that powers an AirTag. The company based those estimates on Energizer Ultimate Lithium batteries, so your mileage may vary. Once an AirTag is seated inside the case, which is a compact 4.45 x 1.57 inches, it is sealed shut with four screws at the corners. The case is fiber-reinforced, according to Elevation Lab, and rated IP69 waterproof. The company says it’s intended for use cases where you might place an AirTag for long periods of time, like in a vehicle, a piece of luggage or a work bag. We've already got a couple of Elevation Lab products on our list for best AirTag accessories, so while we haven't reviewed the battery case, we tend to like this company's products. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/this-elevationlabs-10-year-extended-battery-case-for-airtags-is-on-sale-for-only-16-162308983.html?src=rss",
          "content": "ElevationLab makes a battery case for your AirTag that can power it for 10 years and the accessory is on sale now for 30 percent off. Normally retailing for $23, you can pick one up for $16. The TimeCapsule case uses two AA batteries to offer up to 14 times the lifespan of the CR2032 battery that powers an AirTag. The company based those estimates on Energizer Ultimate Lithium batteries, so your mileage may vary. Once an AirTag is seated inside the case, which is a compact 4.45 x 1.57 inches, it is sealed shut with four screws at the corners. The case is fiber-reinforced, according to Elevation Lab, and rated IP69 waterproof. The company says it’s intended for use cases where you might place an AirTag for long periods of time, like in a vehicle, a piece of luggage or a work bag. We've already got a couple of Elevation Lab products on our list for best AirTag accessories, so while we haven't reviewed the battery case, we tend to like this company's products. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/this-elevationlabs-10-year-extended-battery-case-for-airtags-is-on-sale-for-only-16-162308983.html?src=rss",
          "feed_position": 18
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/social-media/meta-closes-550000-accounts-to-comply-with-australias-kids-social-media-ban-130041356.html",
          "published_at": "Mon, 12 Jan 2026 13:00:41 +0000",
          "title": "Meta closes 550,000 accounts to comply with Australia's kids social media ban",
          "standfirst": "To comply with Australia's under-16 social media ban, Meta said on Medium that it has shut down nearly 550,00 accounts. That number includes 330,000 Instagram, 173,000 Facebook and 40,000 Threads accounts deemed to belong to children. \"Ongoing compliance with the law will be a multi-layered process that we will continue to refine, though our concerns about determining age online without an industry standard remain,\" the company wrote. Australia's minimum age social media ban, the first of its kind in the world for a democracy, went into effect on December 10. The ten platforms affected, including Facebook, Instagram, TikTok, Snapchat, X, Reddit and Twitch, must bar underage users or face a fine of up to $AUD 49.5 million ($33 million). Platforms are using a variety of means to determine age, including age inference based on activity and selfies. Some of those platforms aren't taking the ban lying down. Reddit, which launched a lawsuit against the Australian government, argued that it shouldn't have been included in the ban since it isn't a social media site, while adding that it comes with some \"serious privacy and political expression issues\" for users. Meta also expressed its opposition to the ban, citing a number of factors. It says taking social media out of the hands of teens can isolate them from getting support from online communities, and that the ban is only driving them to \"less regulated parts of the internet.\" It also sites inconsistent age verification methods and a lack of interest in compliance from teens and parents. However, the fact that Meta has removed almost 550,000 accounts just a month after the ban took affect shows that it is also affecting the company's bottom line. And Meta doesn't have a sterling record when it comes to teen safety, having previously downplayed the frequency of harm to children. This article originally appeared on Engadget at https://www.engadget.com/social-media/meta-closes-550000-accounts-to-comply-with-australias-kids-social-media-ban-130041356.html?src=rss",
          "content": "To comply with Australia's under-16 social media ban, Meta said on Medium that it has shut down nearly 550,00 accounts. That number includes 330,000 Instagram, 173,000 Facebook and 40,000 Threads accounts deemed to belong to children. \"Ongoing compliance with the law will be a multi-layered process that we will continue to refine, though our concerns about determining age online without an industry standard remain,\" the company wrote. Australia's minimum age social media ban, the first of its kind in the world for a democracy, went into effect on December 10. The ten platforms affected, including Facebook, Instagram, TikTok, Snapchat, X, Reddit and Twitch, must bar underage users or face a fine of up to $AUD 49.5 million ($33 million). Platforms are using a variety of means to determine age, including age inference based on activity and selfies. Some of those platforms aren't taking the ban lying down. Reddit, which launched a lawsuit against the Australian government, argued that it shouldn't have been included in the ban since it isn't a social media site, while adding that it comes with some \"serious privacy and political expression issues\" for users. Meta also expressed its opposition to the ban, citing a number of factors. It says taking social media out of the hands of teens can isolate them from getting support from online communities, and that the ban is only driving them to \"less regulated parts of the internet.\" It also sites inconsistent age verification methods and a lack of interest in compliance from teens and parents. However, the fact that Meta has removed almost 550,000 accounts just a month after the ban took affect shows that it is also affecting the company's bottom line. And Meta doesn't have a sterling record when it comes to teen safety, having previously downplayed the frequency of harm to children. This article originally appeared on Engadget at https://www.engadget.com/social-media/meta-closes-550000-accounts-to-comply-with-australias-kids-social-media-ban-130041356.html?src=rss",
          "feed_position": 19
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/big-tech/uk-regulator-ofcom-opens-a-formal-investigation-into-x-over-csam-scandal-120000312.html",
          "published_at": "Mon, 12 Jan 2026 12:00:00 +0000",
          "title": "UK regulator Ofcom opens a formal investigation into X over CSAM scandal",
          "standfirst": "The UK’s media regulator has opened a formal investigation into X under the Online Safety Act. \"There have been deeply concerning reports of the Grok AI chatbot account on X being used to create and share undressed images of people — which may amount to intimate image abuse or pornography — and sexualized images of children that may amount to child sexual abuse material (CSAM),\" Ofcom said.The investigation will focus on whether X has \"has complied with its duties to protect people in the UK from content that is illegal in the UK.\" That includes whether X is taking appropriate measures to prevent UK users from seeing \"priority\" illegal content, such as CSAM and non-consensual intimate images; if the platform is removing illegal content quickly after becoming aware of it; and whether X carried out an updated risk assessment before making \"any significant changes\" to the platform. The probe will also consider whether X assessed the risk that its platform poses to UK children and if it has ”highly effective age assurance to protect UK children from seeing pornography.”The regulator said it contacted X on January 5 and received a response by its January 9 deadline. Ofcom is conducting an \"expedited assessment of available evidence as a matter of urgency\" and added that it has asked xAI for \"urgent clarification\" on the steps the company is taking to protect UK users.\"Reports of Grok being used to create and share illegal non-consensual intimate images and child sexual abuse material on X have been deeply concerning,\" an Ofcom spokesperson said. \"Platforms must protect people in the UK from content that’s illegal in the UK, and we won’t hesitate to investigate where we suspect companies are failing in their duties, especially where there’s a risk of harm to children. We’ll progress this investigation as a matter of the highest priority, while ensuring we follow due process. As the UK’s independent online safety enforcement agency, it’s important we make sure our investigations are legally robust and fairly decided.\"If Ofcom deems that a company has broken the law, it can \"require platforms to take specific steps to come into compliance or to remedy harm caused by the breach.\" The regulator can additionally impose fines of up to £18 million ($24.3 million) or 10 percent of \"qualifying\" worldwide revenue, whichever of the two figures is higher. It can also seek a court order to stop payment providers or advertisers from working with a platform, or to require internet service providers to block a site in the UK. The UK government has said it would back any action that Ofcom takes against X. Reports over the weekend suggested that the UK had held discussions with allies over a coordinated response to Grok-generated deepfakes. Regulators elsewhere, including in India and the European Union, are also investigating X.Last week, the Grok account on X started telling users that its image generation and editing tools were being limited to paying subscribers. But as of Monday it was still possible for non-paying users to generate images through the Grok tab on the X website and app. Meanwhile, Malaysia and Indonesia became the first countries to block Grok, claiming that X’s chatbot does not have sufficient safeguards in place to prevent explicit AI-generated deepfakes of women and children from being created and disseminated on X. Indonesia temporarily blocked access to Grok on Saturday, as did Malaysia on Sunday, the Associated Press reports. \"The government sees non-consensual sexual deepfakes as a serious violation of human rights, dignity and the safety of citizens in the digital space,\" Indonesia’s Communication and Digital Affairs Minister Meutya Hafid said in a statement. Officials in the country said initial findings showed that Grok lacks effective controls to prevent users from creating and sharing sexually explicit deepfakes based on photos of Indonesian residents. The country's director general of digital space supervision, Alexander Sabar, said generating deepfakes can violate individuals' image and privacy rights when photos are shared or manipulated without consent, adding that they can lead to reputational, social and psychological harm.The Malaysian Communications and Multimedia Commission cited \"repeated misuse\" of Grok to generate explicit and non-consensual deepfakes, some of which involved women and children. The regulator said Grok will remain blocked in the country until X Corp and parent xAI establish strong enough safeguards. This article originally appeared on Engadget at https://www.engadget.com/big-tech/uk-regulator-ofcom-opens-a-formal-investigation-into-x-over-csam-scandal-120000312.html?src=rss",
          "content": "The UK’s media regulator has opened a formal investigation into X under the Online Safety Act. \"There have been deeply concerning reports of the Grok AI chatbot account on X being used to create and share undressed images of people — which may amount to intimate image abuse or pornography — and sexualized images of children that may amount to child sexual abuse material (CSAM),\" Ofcom said.The investigation will focus on whether X has \"has complied with its duties to protect people in the UK from content that is illegal in the UK.\" That includes whether X is taking appropriate measures to prevent UK users from seeing \"priority\" illegal content, such as CSAM and non-consensual intimate images; if the platform is removing illegal content quickly after becoming aware of it; and whether X carried out an updated risk assessment before making \"any significant changes\" to the platform. The probe will also consider whether X assessed the risk that its platform poses to UK children and if it has ”highly effective age assurance to protect UK children from seeing pornography.”The regulator said it contacted X on January 5 and received a response by its January 9 deadline. Ofcom is conducting an \"expedited assessment of available evidence as a matter of urgency\" and added that it has asked xAI for \"urgent clarification\" on the steps the company is taking to protect UK users.\"Reports of Grok being used to create and share illegal non-consensual intimate images and child sexual abuse material on X have been deeply concerning,\" an Ofcom spokesperson said. \"Platforms must protect people in the UK from content that’s illegal in the UK, and we won’t hesitate to investigate where we suspect companies are failing in their duties, especially where there’s a risk of harm to children. We’ll progress this investigation as a matter of the highest priority, while ensuring we follow due process. As the UK’s independent online safety enforcement agency, it’s important we make sure our investigations are legally robust and fairly decided.\"If Ofcom deems that a company has broken the law, it can \"require platforms to take specific steps to come into compliance or to remedy harm caused by the breach.\" The regulator can additionally impose fines of up to £18 million ($24.3 million) or 10 percent of \"qualifying\" worldwide revenue, whichever of the two figures is higher. It can also seek a court order to stop payment providers or advertisers from working with a platform, or to require internet service providers to block a site in the UK. The UK government has said it would back any action that Ofcom takes against X. Reports over the weekend suggested that the UK had held discussions with allies over a coordinated response to Grok-generated deepfakes. Regulators elsewhere, including in India and the European Union, are also investigating X.Last week, the Grok account on X started telling users that its image generation and editing tools were being limited to paying subscribers. But as of Monday it was still possible for non-paying users to generate images through the Grok tab on the X website and app. Meanwhile, Malaysia and Indonesia became the first countries to block Grok, claiming that X’s chatbot does not have sufficient safeguards in place to prevent explicit AI-generated deepfakes of women and children from being created and disseminated on X. Indonesia temporarily blocked access to Grok on Saturday, as did Malaysia on Sunday, the Associated Press reports. \"The government sees non-consensual sexual deepfakes as a serious violation of human rights, dignity and the safety of citizens in the digital space,\" Indonesia’s Communication and Digital Affairs Minister Meutya Hafid said in a statement. Officials in the country said initial findings showed that Grok lacks effective controls to prevent users from creating and sharing sexually explicit deepfakes based on photos of Indonesian residents. The country's director general of digital space supervision, Alexander Sabar, said generating deepfakes can violate individuals' image and privacy rights when photos are shared or manipulated without consent, adding that they can lead to reputational, social and psychological harm.The Malaysian Communications and Multimedia Commission cited \"repeated misuse\" of Grok to generate explicit and non-consensual deepfakes, some of which involved women and children. The regulator said Grok will remain blocked in the country until X Corp and parent xAI establish strong enough safeguards. This article originally appeared on Engadget at https://www.engadget.com/big-tech/uk-regulator-ofcom-opens-a-formal-investigation-into-x-over-csam-scandal-120000312.html?src=rss",
          "feed_position": 20
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/technology/anthropic-launches-cowork-a-claude-desktop-agent-that-works-in-your-files-no",
          "published_at": "Mon, 12 Jan 2026 11:30:00 GMT",
          "title": "Anthropic launches Cowork, a Claude Desktop agent that works in your files — no coding required",
          "standfirst": "Anthropic released Cowork on Monday, a new AI agent capability that extends the power of its wildly successful Claude Code tool to non-technical users — and according to company insiders, the team built the entire feature in approximately a week and a half, largely using Claude Code itself.The launch marks a major inflection point in the race to deliver practical AI agents to mainstream users, positioning Anthropic to compete not just with OpenAI and Google in conversational AI, but with Microsoft&#x27;s Copilot in the burgeoning market for AI-powered productivity tools.\"Cowork lets you complete non-technical tasks much like how developers use Claude Code,\" the company announced via its official Claude account on X. The feature arrives as a research preview available exclusively to Claude Max subscribers — Anthropic&#x27;s power-user tier priced between $100 and $200 per month — through the macOS desktop application.For the past year, the industry narrative has focused on large language models that can write poetry or debug code. With Cowork, Anthropic is betting that the real enterprise value lies in an AI that can open a folder, read a messy pile of receipts, and generate a structured expense report without human hand-holding.How developers using a coding tool for vacation research inspired Anthropic&#x27;s latest productThe genesis of Cowork lies in Anthropic&#x27;s recent success with the developer community. In late 2024, the company released Claude Code, a terminal-based tool that allowed software engineers to automate rote programming tasks. The tool was a hit, but Anthropic noticed a peculiar trend: users were forcing the coding tool to perform non-coding labor.According to Boris Cherny, an engineer at Anthropic, the company observed users deploying the developer tool for an unexpectedly diverse array of tasks.\"Since we launched Claude Code, we saw people using it for all sorts of non-coding work: doing vacation research, building slide decks, cleaning up your email, cancelling subscriptions, recovering wedding photos from a hard drive, monitoring plant growth, controlling your oven,\" Cherny wrote on X. \"These use cases are diverse and surprising — the reason is that the underlying Claude Agent is the best agent, and Opus 4.5 is the best model.\"Recognizing this shadow usage, Anthropic effectively stripped the command-line complexity from their developer tool to create a consumer-friendly interface. In its blog post announcing the feature, Anthropic explained that developers \"quickly began using it for almost everything else,\" which \"prompted us to build Cowork: a simpler way for anyone — not just developers — to work with Claude in the very same way.\"Inside the folder-based architecture that lets Claude read, edit, and create files on your computerUnlike a standard chat interface where a user pastes text for analysis, Cowork requires a different level of trust and access. Users designate a specific folder on their local machine that Claude can access. Within that sandbox, the AI agent can read existing files, modify them, or create entirely new ones.Anthropic offers several illustrative examples: reorganizing a cluttered downloads folder by sorting and intelligently renaming each file, generating a spreadsheet of expenses from a collection of receipt screenshots, or drafting a report from scattered notes across multiple documents.\"In Cowork, you give Claude access to a folder on your computer. Claude can then read, edit, or create files in that folder,\" the company explained on X. \"Try it to create a spreadsheet from a pile of screenshots, or produce a first draft from scattered notes.\"The architecture relies on what is known as an \"agentic loop.\" When a user assigns a task, the AI does not merely generate a text response. Instead, it formulates a plan, executes steps in parallel, checks its own work, and asks for clarification if it hits a roadblock. Users can queue multiple tasks and let Claude process them simultaneously — a workflow Anthropic describes as feeling \"much less like a back-and-forth and much more like leaving messages for a coworker.\"The system is built on Anthropic&#x27;s Claude Agent SDK, meaning it shares the same underlying architecture as Claude Code. Anthropic notes that Cowork \"can take on many of the same tasks that Claude Code can handle, but in a more approachable form for non-coding tasks.\"The recursive loop where AI builds AI: Claude Code reportedly wrote much of Claude CoworkPerhaps the most remarkable detail surrounding Cowork&#x27;s launch is the speed at which the tool was reportedly built — highlighting a recursive feedback loop where AI tools are being used to build better AI tools.During a livestream hosted by Dan Shipper, Felix Rieseberg, an Anthropic employee, confirmed that the team built Cowork in approximately a week and a half.Alex Volkov, who covers AI developments, expressed surprise at the timeline: \"Holy shit Anthropic built &#x27;Cowork&#x27; in the last... week and a half?!\"This prompted immediate speculation about how much of Cowork was itself built by Claude Code. Simon Smith, EVP of Generative AI at Klick Health, put it bluntly on X: \"Claude Code wrote all of Claude Cowork. Can we all agree that we&#x27;re in at least somewhat of a recursive improvement loop here?\"The implication is profound: Anthropic&#x27;s AI coding agent may have substantially contributed to building its own non-technical sibling product. If true, this is one of the most visible examples yet of AI systems being used to accelerate their own development and expansion — a strategy that could widen the gap between AI labs that successfully deploy their own agents internally and those that do not.Connectors, browser automation, and skills extend Cowork&#x27;s reach beyond the local file systemCowork doesn&#x27;t operate in isolation. The feature integrates with Anthropic&#x27;s existing ecosystem of connectors — tools that link Claude to external information sources and services such as Asana, Notion, PayPal, and other supported partners. Users who have configured these connections in the standard Claude interface can leverage them within Cowork sessions.Additionally, Cowork can pair with Claude in Chrome, Anthropic&#x27;s browser extension, to execute tasks requiring web access. This combination allows the agent to navigate websites, click buttons, fill forms, and extract information from the internet — all while operating from the desktop application.\"Cowork includes a number of novel UX and safety features that we think make the product really special,\" Cherny explained, highlighting \"a built-in VM [virtual machine] for isolation, out of the box support for browser automation, support for all your claude.ai data connectors, asking you for clarification when it&#x27;s unsure.\"Anthropic has also introduced an initial set of \"skills\" specifically designed for Cowork that enhance Claude&#x27;s ability to create documents, presentations, and other files. These build on the Skills for Claude framework the company announced in October, which provides specialized instruction sets Claude can load for particular types of tasks.Why Anthropic is warning users that its own AI agent could delete their filesThe transition from a chatbot that suggests edits to an agent that makes edits introduces significant risk. An AI that can organize files can, theoretically, delete them.In a notable display of transparency, Anthropic devoted considerable space in its announcement to warning users about Cowork&#x27;s potential dangers — an unusual approach for a product launch.The company explicitly acknowledges that Claude \"can take potentially destructive actions (such as deleting local files) if it&#x27;s instructed to.\" Because Claude might occasionally misinterpret instructions, Anthropic urges users to provide \"very clear guidance\" about sensitive operations.More concerning is the risk of prompt injection attacks — a technique where malicious actors embed hidden instructions in content Claude might encounter online, potentially causing the agent to bypass safeguards or take harmful actions.\"We&#x27;ve built sophisticated defenses against prompt injections,\" Anthropic wrote, \"but agent safety — that is, the task of securing Claude&#x27;s real-world actions — is still an active area of development in the industry.\"The company characterized these risks as inherent to the current state of AI agent technology rather than unique to Cowork. \"These risks aren&#x27;t new with Cowork, but it might be the first time you&#x27;re using a more advanced tool that moves beyond a simple conversation,\" the announcement notes.Anthropic&#x27;s desktop agent strategy sets up a direct challenge to Microsoft CopilotThe launch of Cowork places Anthropic in direct competition with Microsoft, which has spent years attempting to integrate its Copilot AI into the fabric of the Windows operating system with mixed adoption results.However, Anthropic&#x27;s approach differs in its isolation. By confining the agent to specific folders and requiring explicit connectors, they are attempting to strike a balance between the utility of an OS-level agent and the security of a sandboxed application.What distinguishes Anthropic&#x27;s approach is its bottom-up evolution. Rather than designing an AI assistant and retrofitting agent capabilities, Anthropic built a powerful coding agent first — Claude Code — and is now abstracting its capabilities for broader audiences. This technical lineage may give Cowork more robust agentic behavior from the start.Claude Code has generated significant enthusiasm among developers since its initial launch as a command-line tool in late 2024. The company expanded access with a web interface in October 2025, followed by a Slack integration in December. Cowork is the next logical step: bringing the same agentic architecture to users who may never touch a terminal.Who can access Cowork now, and what&#x27;s coming next for Windows and other platformsFor now, Cowork remains exclusive to Claude Max subscribers using the macOS desktop application. Users on other subscription tiers — Free, Pro, Team, or Enterprise — can join a waitlist for future access.Anthropic has signaled clear intentions to expand the feature&#x27;s reach. The blog post explicitly mentions plans to add cross-device sync and bring Cowork to Windows as the company learns from the research preview.Cherny set expectations appropriately, describing the product as \"early and raw, similar to what Claude Code felt like when it first launched.\"To access Cowork, Max subscribers can download or update the Claude macOS app and click on \"Cowork\" in the sidebar.The real question facing enterprise AI adoptionFor technical decision-makers, the implications of Cowork extend beyond any single product launch. The bottleneck for AI adoption is shifting — no longer is model intelligence the limiting factor, but rather workflow integration and user trust.Anthropic&#x27;s goal, as the company puts it, is to make working with Claude feel less like operating a tool and more like delegating to a colleague. Whether mainstream users are ready to hand over folder access to an AI that might misinterpret their instructions remains an open question.But the speed of Cowork&#x27;s development — a major feature built in ten days, possibly by the company&#x27;s own AI — previews a future where the capabilities of these systems compound faster than organizations can evaluate them. The chatbot has learned to use a file manager. What it learns to use next is anyone&#x27;s guess.",
          "content": "Anthropic released Cowork on Monday, a new AI agent capability that extends the power of its wildly successful Claude Code tool to non-technical users — and according to company insiders, the team built the entire feature in approximately a week and a half, largely using Claude Code itself.The launch marks a major inflection point in the race to deliver practical AI agents to mainstream users, positioning Anthropic to compete not just with OpenAI and Google in conversational AI, but with Microsoft&#x27;s Copilot in the burgeoning market for AI-powered productivity tools.\"Cowork lets you complete non-technical tasks much like how developers use Claude Code,\" the company announced via its official Claude account on X. The feature arrives as a research preview available exclusively to Claude Max subscribers — Anthropic&#x27;s power-user tier priced between $100 and $200 per month — through the macOS desktop application.For the past year, the industry narrative has focused on large language models that can write poetry or debug code. With Cowork, Anthropic is betting that the real enterprise value lies in an AI that can open a folder, read a messy pile of receipts, and generate a structured expense report without human hand-holding.How developers using a coding tool for vacation research inspired Anthropic&#x27;s latest productThe genesis of Cowork lies in Anthropic&#x27;s recent success with the developer community. In late 2024, the company released Claude Code, a terminal-based tool that allowed software engineers to automate rote programming tasks. The tool was a hit, but Anthropic noticed a peculiar trend: users were forcing the coding tool to perform non-coding labor.According to Boris Cherny, an engineer at Anthropic, the company observed users deploying the developer tool for an unexpectedly diverse array of tasks.\"Since we launched Claude Code, we saw people using it for all sorts of non-coding work: doing vacation research, building slide decks, cleaning up your email, cancelling subscriptions, recovering wedding photos from a hard drive, monitoring plant growth, controlling your oven,\" Cherny wrote on X. \"These use cases are diverse and surprising — the reason is that the underlying Claude Agent is the best agent, and Opus 4.5 is the best model.\"Recognizing this shadow usage, Anthropic effectively stripped the command-line complexity from their developer tool to create a consumer-friendly interface. In its blog post announcing the feature, Anthropic explained that developers \"quickly began using it for almost everything else,\" which \"prompted us to build Cowork: a simpler way for anyone — not just developers — to work with Claude in the very same way.\"Inside the folder-based architecture that lets Claude read, edit, and create files on your computerUnlike a standard chat interface where a user pastes text for analysis, Cowork requires a different level of trust and access. Users designate a specific folder on their local machine that Claude can access. Within that sandbox, the AI agent can read existing files, modify them, or create entirely new ones.Anthropic offers several illustrative examples: reorganizing a cluttered downloads folder by sorting and intelligently renaming each file, generating a spreadsheet of expenses from a collection of receipt screenshots, or drafting a report from scattered notes across multiple documents.\"In Cowork, you give Claude access to a folder on your computer. Claude can then read, edit, or create files in that folder,\" the company explained on X. \"Try it to create a spreadsheet from a pile of screenshots, or produce a first draft from scattered notes.\"The architecture relies on what is known as an \"agentic loop.\" When a user assigns a task, the AI does not merely generate a text response. Instead, it formulates a plan, executes steps in parallel, checks its own work, and asks for clarification if it hits a roadblock. Users can queue multiple tasks and let Claude process them simultaneously — a workflow Anthropic describes as feeling \"much less like a back-and-forth and much more like leaving messages for a coworker.\"The system is built on Anthropic&#x27;s Claude Agent SDK, meaning it shares the same underlying architecture as Claude Code. Anthropic notes that Cowork \"can take on many of the same tasks that Claude Code can handle, but in a more approachable form for non-coding tasks.\"The recursive loop where AI builds AI: Claude Code reportedly wrote much of Claude CoworkPerhaps the most remarkable detail surrounding Cowork&#x27;s launch is the speed at which the tool was reportedly built — highlighting a recursive feedback loop where AI tools are being used to build better AI tools.During a livestream hosted by Dan Shipper, Felix Rieseberg, an Anthropic employee, confirmed that the team built Cowork in approximately a week and a half.Alex Volkov, who covers AI developments, expressed surprise at the timeline: \"Holy shit Anthropic built &#x27;Cowork&#x27; in the last... week and a half?!\"This prompted immediate speculation about how much of Cowork was itself built by Claude Code. Simon Smith, EVP of Generative AI at Klick Health, put it bluntly on X: \"Claude Code wrote all of Claude Cowork. Can we all agree that we&#x27;re in at least somewhat of a recursive improvement loop here?\"The implication is profound: Anthropic&#x27;s AI coding agent may have substantially contributed to building its own non-technical sibling product. If true, this is one of the most visible examples yet of AI systems being used to accelerate their own development and expansion — a strategy that could widen the gap between AI labs that successfully deploy their own agents internally and those that do not.Connectors, browser automation, and skills extend Cowork&#x27;s reach beyond the local file systemCowork doesn&#x27;t operate in isolation. The feature integrates with Anthropic&#x27;s existing ecosystem of connectors — tools that link Claude to external information sources and services such as Asana, Notion, PayPal, and other supported partners. Users who have configured these connections in the standard Claude interface can leverage them within Cowork sessions.Additionally, Cowork can pair with Claude in Chrome, Anthropic&#x27;s browser extension, to execute tasks requiring web access. This combination allows the agent to navigate websites, click buttons, fill forms, and extract information from the internet — all while operating from the desktop application.\"Cowork includes a number of novel UX and safety features that we think make the product really special,\" Cherny explained, highlighting \"a built-in VM [virtual machine] for isolation, out of the box support for browser automation, support for all your claude.ai data connectors, asking you for clarification when it&#x27;s unsure.\"Anthropic has also introduced an initial set of \"skills\" specifically designed for Cowork that enhance Claude&#x27;s ability to create documents, presentations, and other files. These build on the Skills for Claude framework the company announced in October, which provides specialized instruction sets Claude can load for particular types of tasks.Why Anthropic is warning users that its own AI agent could delete their filesThe transition from a chatbot that suggests edits to an agent that makes edits introduces significant risk. An AI that can organize files can, theoretically, delete them.In a notable display of transparency, Anthropic devoted considerable space in its announcement to warning users about Cowork&#x27;s potential dangers — an unusual approach for a product launch.The company explicitly acknowledges that Claude \"can take potentially destructive actions (such as deleting local files) if it&#x27;s instructed to.\" Because Claude might occasionally misinterpret instructions, Anthropic urges users to provide \"very clear guidance\" about sensitive operations.More concerning is the risk of prompt injection attacks — a technique where malicious actors embed hidden instructions in content Claude might encounter online, potentially causing the agent to bypass safeguards or take harmful actions.\"We&#x27;ve built sophisticated defenses against prompt injections,\" Anthropic wrote, \"but agent safety — that is, the task of securing Claude&#x27;s real-world actions — is still an active area of development in the industry.\"The company characterized these risks as inherent to the current state of AI agent technology rather than unique to Cowork. \"These risks aren&#x27;t new with Cowork, but it might be the first time you&#x27;re using a more advanced tool that moves beyond a simple conversation,\" the announcement notes.Anthropic&#x27;s desktop agent strategy sets up a direct challenge to Microsoft CopilotThe launch of Cowork places Anthropic in direct competition with Microsoft, which has spent years attempting to integrate its Copilot AI into the fabric of the Windows operating system with mixed adoption results.However, Anthropic&#x27;s approach differs in its isolation. By confining the agent to specific folders and requiring explicit connectors, they are attempting to strike a balance between the utility of an OS-level agent and the security of a sandboxed application.What distinguishes Anthropic&#x27;s approach is its bottom-up evolution. Rather than designing an AI assistant and retrofitting agent capabilities, Anthropic built a powerful coding agent first — Claude Code — and is now abstracting its capabilities for broader audiences. This technical lineage may give Cowork more robust agentic behavior from the start.Claude Code has generated significant enthusiasm among developers since its initial launch as a command-line tool in late 2024. The company expanded access with a web interface in October 2025, followed by a Slack integration in December. Cowork is the next logical step: bringing the same agentic architecture to users who may never touch a terminal.Who can access Cowork now, and what&#x27;s coming next for Windows and other platformsFor now, Cowork remains exclusive to Claude Max subscribers using the macOS desktop application. Users on other subscription tiers — Free, Pro, Team, or Enterprise — can join a waitlist for future access.Anthropic has signaled clear intentions to expand the feature&#x27;s reach. The blog post explicitly mentions plans to add cross-device sync and bring Cowork to Windows as the company learns from the research preview.Cherny set expectations appropriately, describing the product as \"early and raw, similar to what Claude Code felt like when it first launched.\"To access Cowork, Max subscribers can download or update the Claude macOS app and click on \"Cowork\" in the sidebar.The real question facing enterprise AI adoptionFor technical decision-makers, the implications of Cowork extend beyond any single product launch. The bottleneck for AI adoption is shifting — no longer is model intelligence the limiting factor, but rather workflow integration and user trust.Anthropic&#x27;s goal, as the company puts it, is to make working with Claude feel less like operating a tool and more like delegating to a colleague. Whether mainstream users are ready to hand over folder access to an AI that might misinterpret their instructions remains an open question.But the speed of Cowork&#x27;s development — a major feature built in ten days, possibly by the company&#x27;s own AI — previews a future where the capabilities of these systems compound faster than organizations can evaluate them. The chatbot has learned to use a file manager. What it learns to use next is anyone&#x27;s guess.",
          "feed_position": 2,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/wHv1Wez7Ps9wYVYAo9fwT/14b41f606dbf1f5b17994be510407449/nuneybits_Hyper-realistic_image_of_a_retro_computer_with_a_glos_61ffb6e2-7c33-4d45-85f7-69c28693b3ec.webp?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/computing/accessories/best-laptop-power-bank-120040388.html",
          "published_at": "Mon, 12 Jan 2026 10:01:26 +0000",
          "title": "The best laptop power banks for 2026",
          "standfirst": "Laptop power banks come in handy if you often travel or work away from your desk. These larger batteries have enough capacity to refill your computer to around 75 percent, giving you many more productive hours. Of course, they can also charge smaller devices like your phone, camera or tablet a few times over. Nearly all portable laptop chargers fall below the 100-watt-hour mark so you can bring them in your carry-on when you fly. Additional features, like built-in cables and digital displays make these battery banks easier to use, too. We tested over a dozen options and put our thoughts below, so you can find the best laptop power bank for your next trip away from an outlet. Table of contents Best laptop power banks for 2026 What to look for in a laptop power bank How we test portable laptop chargers Other laptop power banks we tested Laptop power bank FAQs Recent updates Best laptop power banks for 2026 What to look for in a laptop power bank Flying with a laptop power bank Most portable batteries top out at around 27,000mAh so you can fly with them. The TSA currently limits the capacity carry-on batteries to 100Wh, which works out to around 27,500mAh for 3.6 volt lithium-ion batteries. Note that you’re not allowed to pack any batteries in your checked luggage, regardless of capacity. The TSA rules are intended to limit fire danger — and some airlines are implementing further restrictions due to recent on-board incidents. In March 2025, a Hong Kong flight was grounded after a battery pack caught fire in an overhead bin. A similar situation happened that same year in July on a domestic Delta flight, and again in August on a transatlantic KLM flight. As a result, some airlines, including Emirates, Southwest and others have announced further restrictions on flying with battery packs. Rules include limiting the number of allowed portable chargers and requiring flyers to keep power banks in clear view when using them to recharge a device. If the battery pack isn’t actively in use, however, most rules allow them to stay in your carry-on bag in the overhead bin. Before flying, it’s wise to check your airline’s policies. Capacity If you just need to keep a smartphone from dying before you can make it home, just about any of the best power banks will do. But if you need to revive multiple devices or the substantial battery of a laptop, you’ll want something with a high milliamp-hour​​ (mAh) capacity. A power bank capable of delivering enough power to a laptop will have a capacity between 20,000 and 27,000 mAh. If you want something even bigger than a laptop power bank, and don’t need to fly with it, you’ll likely want to look into portable power stations. These can be the size of a car battery or larger and can potentially fuel an entire weekend away. Another thing to keep in mind is that the capacity listed in a power bank's specs is not what will be delivered to your devices. As I mentioned, the capacity of these banks is around 25,000mAh. Even the huge battery on a 16-inch MacBook Pro or a Dell XPS 16 has a mAh rating of around 5,000 - 6,000mAh, so you might think you’d get five full charges but in reality, you only get about a single 70-percent charge. The voltage is different (typically 3.7V for the power bank and 11.4V for a laptop) which makes the watt-hours, or the amount of energy each battery can hold, different (working out to 92Wh for the battery and 72Wh for the built-in laptop batteries). On top of that, in order to feed a charge from a power bank to a laptop, a voltage conversion takes place and that dissipates a decent amount of energy. Without turning this into a physics lesson, this all means that a power bank with a 25,000mAh (or 92Wh) capacity will typically fill a 5,000mAh (or 72Wh) laptop battery to about 75 percent. In my tests, I averaged about a 60-percent efficiency rate between a power bank’s listed capacity and the actual charge delivered. Ports Every large power bank I’ve tested has at least three USB ports, with a mix of USB-C and USB-A, which should cover nearly any portable device you need to recharge — earbuds, phones, tablets, laptops, you name it. In addition to the different plug formats, some ports supply power at different wattages. For example, one built-in USB-C port might be rated for 60 watts, while the one next to it is rated for 100 watts. So if you’ve got a device that’s capable of 70W fast charging, such as the new MacBook Air, you’d want to opt for the 100W port to get the best charging speeds possible. Note that devices with a smaller wattage draw won’t be negatively affected by connecting to ports with high ratings. For example, a Galaxy S24 Ultra, capable of 45W super fast charging, is perfectly compatible with the 100W port. A device will only draw what it can take, regardless of what a port can supply. Just remember that the port, device and charging cable need to be at or above the desired wattage rating to achieve maximum charging rates. Some of these larger batteries also have AC ports. It might seem like a natural fit to plug in your laptop’s power adapter for a recharge. But really, the AC port should only be for devices that can’t use USB — such as a lamp or a printer. Plugging a power adapter into the AC port only wastes energy through conversion. First, the battery converts its DC power to supply the port with AC power, then the power adapter converts that AC power back to DC so your laptop can take it in. And as you’ll remember from physics class, each time energy is converted, some is lost to heat and other dissipations. Better to cut out the middleman and just send that DC power straight from the battery to the device. Also, you can use more than one port at a time with these devices; just remember that the speed of whatever you’re charging will likely go down, and of course, the battery is going to drain proportionally to what you’re refilling. Wireless charging Since I first started testing portable power banks a few years ago, wireless charging capabilities have noticeably improved. The first few I tried were painfully slow and not worth recommending. Now the wireless pads built into power banks are impressively fast — particularly, in my experience, when charging Samsung Galaxy phones (though the lack of a stabilizing magnetic connection like Apple’s MagSafe means they only work when rested flat on a pad). Most wireless charging connections can be used while other ports are also being employed, making them convenient for some mobile battlestation setups. Of course, wireless charging is always less efficient than wired, and recharging from an external battery is less efficient in general. If you want to waste as little energy as possible, you’re better off sticking to wired connections. Design All power banks are designed to be portable, but there’s a big difference between a pocket-friendly 5,000mAh battery and one of these laptop-compatible bruisers. Most of the latter weigh between a pound and a half to two pounds, which is a considerable addition to a backpack. Many of the options listed here have a display to tell you how much charge remains in the battery, which is helpful when you’re trying to judiciously meet out charges to your devices. If a bank has a wireless connection, the pad is usually on the flat top and any available AC connection is usually at one end. Both may require you to engage those charging methods. Don’t be like me and grumble loudly that you got a bum unit without pressing (and sometimes double pressing) all the buttons first. How we test portable laptop chargers For the past three years, I’ve been testing and using dozens of portable batteries for our other battery guide. Some of those batteries include the higher-capacity power banks you see here. I also got a hold of a few extra banks just for this guide to make sure we covered what’s available. I went for brands I’m already familiar with, as well as battery packs from well-received manufacturers I hadn’t tried before (like UGREEN and Lion Energy). I only considered banks with at least a 20,000mAh capacity and mostly stuck with those that rated 25,000mAh and higher. Here’s everything we tested: Zendure Supertank Pro Mophie Powerstation Pro XL Mophie Powerstation Pro AC Lion Energy Eclipse Mag Lion Energy Trek Baseus Blade Laptop Anker Prime 27,650mAh Goal Zero Sherpa 100 AC Anker Retractable Cable Laptop Bank HyperJuice 245W Anker Prime Power Bank (26K, 300W) UGreen Power Bank 25,000mAh 145W I tested each power bank with an Apple phone (iPhone 15 or 16), an Android phone (Galaxy S23 Ultra), a tablet (M1 iPad Air) and a laptop (16-inch MacBook Pro with the M1 Pro chip). Even though these banks can charge multiple devices at once, I refilled one at a time, to make side-by-side comparisons more straightforward. I drained the batteries of the phones and tablets to between zero and five percent and then didn’t use any device as it refilled. For the MacBook, I let it run down to 10 percent before plugging in the power bank. That's when most laptops give display a “connect to power” warning, as draining any battery to empty will compromise the battery life. I then used it as one might in a mobile office, with a Bluetooth keyboard and mouse, while connected to Wi-Fi and a VPN. For each test, I noted how long a completely charged battery took to get a device back to full and how much of the battery’s capacity was used up in one charge. I also noted things like portability, apparent durability, helpful features and overall design. For reference, here are the battery capacities of the devices I used: iPhone 15: 3,349mAh Galaxy S23 Ultra: 4,855mAh iPad Air (5th gen): 7,729mAh 16-inch M1 Pro MacBook Pro: 27,027mAh Other laptop power banks we tested HyperJuice 245W Hyper’s HyperJuice 245W brick looks great and has a hefty 27,000mAh capacity. The four USB-C ports can combine to output 245W of power and it got my MacBook Pro from nearly dead to 75 percent before depleting itself. When testing it with a Samsung Galaxy S23 Ultra, the handset got back up to a full charge in just over an hour. The screen tells you what each port is doing as well as displaying the amount of charge remaining in the pack itself. But the lack of port variety makes it feel less versatile than other picks on this list — the price is higher than our other options, too. Laptop power bank FAQs How do laptop power banks differ from phone power banks? The main difference is size. Phone power banks tend to have a capacity ranging from 5,000mAh to 20,000mAh and laptop powerbanks are typically rated between 20,000mAh and 27,000mAh. There’s no official definition, however. Laptop batteries are simply larger and need a bigger supply of power to give them a meaningful charge. How do you fast charge a power bank? You can charge a power bank exactly as fast as the power bank’s internal mechanisms will allow. Most batteries are limited in how quickly they can accept and deliver a charge to avoid dangerously overheating. But to make sure you’re charging a bank as quickly as possible, make sure the wall adapter and the USB-C cable you are using have a high wattage rating — using a 5W power brick and a 10W cable will take a lot longer to refill your bank than a 65W wall charger and a 100W cord. What size power bank do I need for a laptop? Look for a power bank with a rating of at least 20,000mAh. Slightly smaller batteries may work, but they won’t deliver a significant charge laptops. How many mAh to charge a laptop? A milliamp hour (mAh) is how much a battery can hold, and most portable batteries list their capacity using mAh. If you get a battery rated at 20,000mAh or above, it should be able to charge your laptop. Using mAh to discuss laptop batteries can be confusing. Due to differing voltages, you can’t directly compare the mAh ratings of a power bank battery to a laptop battery. Using watt-hours is a better gauge, as that calculation takes voltage into account. Recent updates November 2025: Updated our overall top pick to the Anker Laptop Power bank. Added a premium power bank pick. This article originally appeared on Engadget at https://www.engadget.com/computing/accessories/best-laptop-power-bank-120040388.html?src=rss",
          "content": "Laptop power banks come in handy if you often travel or work away from your desk. These larger batteries have enough capacity to refill your computer to around 75 percent, giving you many more productive hours. Of course, they can also charge smaller devices like your phone, camera or tablet a few times over. Nearly all portable laptop chargers fall below the 100-watt-hour mark so you can bring them in your carry-on when you fly. Additional features, like built-in cables and digital displays make these battery banks easier to use, too. We tested over a dozen options and put our thoughts below, so you can find the best laptop power bank for your next trip away from an outlet. Table of contents Best laptop power banks for 2026 What to look for in a laptop power bank How we test portable laptop chargers Other laptop power banks we tested Laptop power bank FAQs Recent updates Best laptop power banks for 2026 What to look for in a laptop power bank Flying with a laptop power bank Most portable batteries top out at around 27,000mAh so you can fly with them. The TSA currently limits the capacity carry-on batteries to 100Wh, which works out to around 27,500mAh for 3.6 volt lithium-ion batteries. Note that you’re not allowed to pack any batteries in your checked luggage, regardless of capacity. The TSA rules are intended to limit fire danger — and some airlines are implementing further restrictions due to recent on-board incidents. In March 2025, a Hong Kong flight was grounded after a battery pack caught fire in an overhead bin. A similar situation happened that same year in July on a domestic Delta flight, and again in August on a transatlantic KLM flight. As a result, some airlines, including Emirates, Southwest and others have announced further restrictions on flying with battery packs. Rules include limiting the number of allowed portable chargers and requiring flyers to keep power banks in clear view when using them to recharge a device. If the battery pack isn’t actively in use, however, most rules allow them to stay in your carry-on bag in the overhead bin. Before flying, it’s wise to check your airline’s policies. Capacity If you just need to keep a smartphone from dying before you can make it home, just about any of the best power banks will do. But if you need to revive multiple devices or the substantial battery of a laptop, you’ll want something with a high milliamp-hour​​ (mAh) capacity. A power bank capable of delivering enough power to a laptop will have a capacity between 20,000 and 27,000 mAh. If you want something even bigger than a laptop power bank, and don’t need to fly with it, you’ll likely want to look into portable power stations. These can be the size of a car battery or larger and can potentially fuel an entire weekend away. Another thing to keep in mind is that the capacity listed in a power bank's specs is not what will be delivered to your devices. As I mentioned, the capacity of these banks is around 25,000mAh. Even the huge battery on a 16-inch MacBook Pro or a Dell XPS 16 has a mAh rating of around 5,000 - 6,000mAh, so you might think you’d get five full charges but in reality, you only get about a single 70-percent charge. The voltage is different (typically 3.7V for the power bank and 11.4V for a laptop) which makes the watt-hours, or the amount of energy each battery can hold, different (working out to 92Wh for the battery and 72Wh for the built-in laptop batteries). On top of that, in order to feed a charge from a power bank to a laptop, a voltage conversion takes place and that dissipates a decent amount of energy. Without turning this into a physics lesson, this all means that a power bank with a 25,000mAh (or 92Wh) capacity will typically fill a 5,000mAh (or 72Wh) laptop battery to about 75 percent. In my tests, I averaged about a 60-percent efficiency rate between a power bank’s listed capacity and the actual charge delivered. Ports Every large power bank I’ve tested has at least three USB ports, with a mix of USB-C and USB-A, which should cover nearly any portable device you need to recharge — earbuds, phones, tablets, laptops, you name it. In addition to the different plug formats, some ports supply power at different wattages. For example, one built-in USB-C port might be rated for 60 watts, while the one next to it is rated for 100 watts. So if you’ve got a device that’s capable of 70W fast charging, such as the new MacBook Air, you’d want to opt for the 100W port to get the best charging speeds possible. Note that devices with a smaller wattage draw won’t be negatively affected by connecting to ports with high ratings. For example, a Galaxy S24 Ultra, capable of 45W super fast charging, is perfectly compatible with the 100W port. A device will only draw what it can take, regardless of what a port can supply. Just remember that the port, device and charging cable need to be at or above the desired wattage rating to achieve maximum charging rates. Some of these larger batteries also have AC ports. It might seem like a natural fit to plug in your laptop’s power adapter for a recharge. But really, the AC port should only be for devices that can’t use USB — such as a lamp or a printer. Plugging a power adapter into the AC port only wastes energy through conversion. First, the battery converts its DC power to supply the port with AC power, then the power adapter converts that AC power back to DC so your laptop can take it in. And as you’ll remember from physics class, each time energy is converted, some is lost to heat and other dissipations. Better to cut out the middleman and just send that DC power straight from the battery to the device. Also, you can use more than one port at a time with these devices; just remember that the speed of whatever you’re charging will likely go down, and of course, the battery is going to drain proportionally to what you’re refilling. Wireless charging Since I first started testing portable power banks a few years ago, wireless charging capabilities have noticeably improved. The first few I tried were painfully slow and not worth recommending. Now the wireless pads built into power banks are impressively fast — particularly, in my experience, when charging Samsung Galaxy phones (though the lack of a stabilizing magnetic connection like Apple’s MagSafe means they only work when rested flat on a pad). Most wireless charging connections can be used while other ports are also being employed, making them convenient for some mobile battlestation setups. Of course, wireless charging is always less efficient than wired, and recharging from an external battery is less efficient in general. If you want to waste as little energy as possible, you’re better off sticking to wired connections. Design All power banks are designed to be portable, but there’s a big difference between a pocket-friendly 5,000mAh battery and one of these laptop-compatible bruisers. Most of the latter weigh between a pound and a half to two pounds, which is a considerable addition to a backpack. Many of the options listed here have a display to tell you how much charge remains in the battery, which is helpful when you’re trying to judiciously meet out charges to your devices. If a bank has a wireless connection, the pad is usually on the flat top and any available AC connection is usually at one end. Both may require you to engage those charging methods. Don’t be like me and grumble loudly that you got a bum unit without pressing (and sometimes double pressing) all the buttons first. How we test portable laptop chargers For the past three years, I’ve been testing and using dozens of portable batteries for our other battery guide. Some of those batteries include the higher-capacity power banks you see here. I also got a hold of a few extra banks just for this guide to make sure we covered what’s available. I went for brands I’m already familiar with, as well as battery packs from well-received manufacturers I hadn’t tried before (like UGREEN and Lion Energy). I only considered banks with at least a 20,000mAh capacity and mostly stuck with those that rated 25,000mAh and higher. Here’s everything we tested: Zendure Supertank Pro Mophie Powerstation Pro XL Mophie Powerstation Pro AC Lion Energy Eclipse Mag Lion Energy Trek Baseus Blade Laptop Anker Prime 27,650mAh Goal Zero Sherpa 100 AC Anker Retractable Cable Laptop Bank HyperJuice 245W Anker Prime Power Bank (26K, 300W) UGreen Power Bank 25,000mAh 145W I tested each power bank with an Apple phone (iPhone 15 or 16), an Android phone (Galaxy S23 Ultra), a tablet (M1 iPad Air) and a laptop (16-inch MacBook Pro with the M1 Pro chip). Even though these banks can charge multiple devices at once, I refilled one at a time, to make side-by-side comparisons more straightforward. I drained the batteries of the phones and tablets to between zero and five percent and then didn’t use any device as it refilled. For the MacBook, I let it run down to 10 percent before plugging in the power bank. That's when most laptops give display a “connect to power” warning, as draining any battery to empty will compromise the battery life. I then used it as one might in a mobile office, with a Bluetooth keyboard and mouse, while connected to Wi-Fi and a VPN. For each test, I noted how long a completely charged battery took to get a device back to full and how much of the battery’s capacity was used up in one charge. I also noted things like portability, apparent durability, helpful features and overall design. For reference, here are the battery capacities of the devices I used: iPhone 15: 3,349mAh Galaxy S23 Ultra: 4,855mAh iPad Air (5th gen): 7,729mAh 16-inch M1 Pro MacBook Pro: 27,027mAh Other laptop power banks we tested HyperJuice 245W Hyper’s HyperJuice 245W brick looks great and has a hefty 27,000mAh capacity. The four USB-C ports can combine to output 245W of power and it got my MacBook Pro from nearly dead to 75 percent before depleting itself. When testing it with a Samsung Galaxy S23 Ultra, the handset got back up to a full charge in just over an hour. The screen tells you what each port is doing as well as displaying the amount of charge remaining in the pack itself. But the lack of port variety makes it feel less versatile than other picks on this list — the price is higher than our other options, too. Laptop power bank FAQs How do laptop power banks differ from phone power banks? The main difference is size. Phone power banks tend to have a capacity ranging from 5,000mAh to 20,000mAh and laptop powerbanks are typically rated between 20,000mAh and 27,000mAh. There’s no official definition, however. Laptop batteries are simply larger and need a bigger supply of power to give them a meaningful charge. How do you fast charge a power bank? You can charge a power bank exactly as fast as the power bank’s internal mechanisms will allow. Most batteries are limited in how quickly they can accept and deliver a charge to avoid dangerously overheating. But to make sure you’re charging a bank as quickly as possible, make sure the wall adapter and the USB-C cable you are using have a high wattage rating — using a 5W power brick and a 10W cable will take a lot longer to refill your bank than a 65W wall charger and a 100W cord. What size power bank do I need for a laptop? Look for a power bank with a rating of at least 20,000mAh. Slightly smaller batteries may work, but they won’t deliver a significant charge laptops. How many mAh to charge a laptop? A milliamp hour (mAh) is how much a battery can hold, and most portable batteries list their capacity using mAh. If you get a battery rated at 20,000mAh or above, it should be able to charge your laptop. Using mAh to discuss laptop batteries can be confusing. Due to differing voltages, you can’t directly compare the mAh ratings of a power bank battery to a laptop battery. Using watt-hours is a better gauge, as that calculation takes voltage into account. Recent updates November 2025: Updated our overall top pick to the Anker Laptop Power bank. Added a premium power bank pick. This article originally appeared on Engadget at https://www.engadget.com/computing/accessories/best-laptop-power-bank-120040388.html?src=rss",
          "feed_position": 21
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/infrastructure/how-doordash-scaled-without-a-costly-erp-overhaul",
          "published_at": "Mon, 12 Jan 2026 05:00:00 GMT",
          "title": "How DoorDash scaled without a costly ERP overhaul",
          "standfirst": "Presented by NetSuiteMost companies racing from startup to an industry leader face a choice: limp along with scrappy early systems or endure a costly platform migration.DoorDash did neither. The local-commerce giant scaled from its 2013 founding through IPO and global expansion — acquiring the Helsiniki-based technology company Wolt in 2022 and UK-based Deliveroo in 2025 — while keeping its original Oracle NetSuite business system. Today, it serves over 50 million consumers in more than 40 countries.*Chief Accounting Officer Gordon Lee says the secret is building a scalable ecosystem that allows teams to use tools that work best for them.Choosing flexibility over uniformityWhen DoorDash selected NetSuite as its corporate financial control center, it wasn&#x27;t looking for a system to enforce uniformity. It sought a scalable platform that could connect all its systems, from ERP, CRM, HR, sourcing, and more. \"Our philosophy has been to create a platform that allows our customers and business partners to use whatever tools work best for them,\" Lee says. \"When we&#x27;re managing growth, the majority of the conversation is about managing expectations — what people expect when you grow from A to B.\"The migration questionTwo years after its founding, DoorDash surpassed one million deliveries and expanded into Canada. As the company scaled, Lee faced growing pressure from vendors insisting that rapid growth required a new enterprise platform.He ran the numbers. The move to another platform could cost millions and consume months of his team&#x27;s focus.Instead, DoorDash stayed with NetSuite, which continued to scale alongside the company’s growth. Built on Oracle Cloud Infrastructure, NetSuite delivers the performance and reliability of an enterprise platform without the cost or disruption of migration. Lee concluded: \"Why do I bother to move? I already have the scalability I need from NetSuite.\"Today, DoorDash’s NetSuite backend provides enterprise-grade security while its familiar front end provides the team flexibility, creating a stable, modern foundation for sustained, high-velocity growth.Expanding the menu without the technical indigestionThat flexibility soon proved invaluable. The ability to add new applications quickly — without long, costly integrations — became a major advantage during hypergrowth.For example, as DoorDash expanded from restaurant delivery into grocery, convenience, and retail, Lee turned to NetSuite’s inventory modules to handle the distinct demands of those new categories.“The flexibility to have and not have, and turn the switch on and off, is easy because it’s all integrated,” he explains.Today, DoorDash’s technology stack spans multiple systems — all integrating seamlessly with NetSuite as the financial hub. “They do it, and you’re done,” Lee says.Embedding expertise to scale smarter, not biggerFor Lee, true partnerships turn vendors into part of the team — and that’s exactly how he describes NetSuite Advanced Customer Support (ACS).\"They are here with us every week. They know all my schematics, they know all my data infrastructure, they know all my database structure within NetSuite. Essentially, they are an extension of my team,\" Lee explains.Close collaboration benefits both parties. DoorDash keeps NetSuite attuned to the realities of hypergrowth and gets instant feedback on technology capability and scalability. In turn, NetSuite stays close to a marquee customer. Interaction is ongoing — and frank, according to Lee. “We work directly with NetSuite ACS and often ask, &#x27;Can NetSuite do this?’ If they can prove it can, we stay with NetSuite.\"Another benefit is the ability to extend DoorDash&#x27;s expertise without expanding headcount. \"If someone says to me, &#x27;Gordon, you&#x27;re just an accountant. How do you know about systems? I say, I don&#x27;t. I have a network guy with us, an expert.’ That&#x27;s the kind of partner I want to surround myself with, so that I can grow beyond what I am.”By embedding expertise within our partnerships, DoorDash scales with precision and control. Lee says the model applies to other companies preparing for IPOs or global expansion. He adds that sustainable growth depends as much on shared understanding as on technology itself. Too often, finance and IT “look at the same requirement but see completely different things,” Lee says, describing what he calls the “blue versus purple” problem. “The accountant doesn’t understand the configuration of the system,” he explains. “The IT guy doesn’t understand what the accountant was trying to tell them.”NetSuite bridges that gap. With a unified data model and built-in best practices across finance, operations, and more, it keeps teams aligned and information consistent. That close collaboration, Lee notes, is what keeps rollouts smooth, data clean, and growth sustainable at any stage.AI strategy: Trust only internal data, get data ducks in a rowLee plans to test the NetSuite AI Connector Service — which supports Model Context Protocol (MCP) and lets customers connect their own AI to NetSuite — to see how faster access to accurate data can drive growth.By implementing an internal instance, Lee is less worried about disruptive errors from LLMs trained on public data sources. \"Think about a generative AI chatbot. When you ask a question, it can reflect many perspectives,” he explains. On the other hand, a chatbot trained on private enterprise systems benefits from “a clean data infrastructure.”Lee is taking a methodical approach: first get data pristine, then train AI on domain-specific terminology, and finally see how internal AI can both find the right information and automate downstream accounting processes to save resources and accelerate growth.Betting long-term on its original financial coreFrom early growth to major acquisitions that helped expand its footprint across the globe, DoorDash has relied on NetSuite as a consistent foundation for innovation and scale.Lee credits NetSuite’s flexible architecture and close partnership with helping enable DoorDash as it continued to scale and cement itself as a leader in local commerce globally.His mantra is simple: “Focus on growth instead of churning through vendors.”* Based on the combined numbers for DoorDash, Wolt, and Deliveroo, measured as of September 2025.Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.",
          "content": "Presented by NetSuiteMost companies racing from startup to an industry leader face a choice: limp along with scrappy early systems or endure a costly platform migration.DoorDash did neither. The local-commerce giant scaled from its 2013 founding through IPO and global expansion — acquiring the Helsiniki-based technology company Wolt in 2022 and UK-based Deliveroo in 2025 — while keeping its original Oracle NetSuite business system. Today, it serves over 50 million consumers in more than 40 countries.*Chief Accounting Officer Gordon Lee says the secret is building a scalable ecosystem that allows teams to use tools that work best for them.Choosing flexibility over uniformityWhen DoorDash selected NetSuite as its corporate financial control center, it wasn&#x27;t looking for a system to enforce uniformity. It sought a scalable platform that could connect all its systems, from ERP, CRM, HR, sourcing, and more. \"Our philosophy has been to create a platform that allows our customers and business partners to use whatever tools work best for them,\" Lee says. \"When we&#x27;re managing growth, the majority of the conversation is about managing expectations — what people expect when you grow from A to B.\"The migration questionTwo years after its founding, DoorDash surpassed one million deliveries and expanded into Canada. As the company scaled, Lee faced growing pressure from vendors insisting that rapid growth required a new enterprise platform.He ran the numbers. The move to another platform could cost millions and consume months of his team&#x27;s focus.Instead, DoorDash stayed with NetSuite, which continued to scale alongside the company’s growth. Built on Oracle Cloud Infrastructure, NetSuite delivers the performance and reliability of an enterprise platform without the cost or disruption of migration. Lee concluded: \"Why do I bother to move? I already have the scalability I need from NetSuite.\"Today, DoorDash’s NetSuite backend provides enterprise-grade security while its familiar front end provides the team flexibility, creating a stable, modern foundation for sustained, high-velocity growth.Expanding the menu without the technical indigestionThat flexibility soon proved invaluable. The ability to add new applications quickly — without long, costly integrations — became a major advantage during hypergrowth.For example, as DoorDash expanded from restaurant delivery into grocery, convenience, and retail, Lee turned to NetSuite’s inventory modules to handle the distinct demands of those new categories.“The flexibility to have and not have, and turn the switch on and off, is easy because it’s all integrated,” he explains.Today, DoorDash’s technology stack spans multiple systems — all integrating seamlessly with NetSuite as the financial hub. “They do it, and you’re done,” Lee says.Embedding expertise to scale smarter, not biggerFor Lee, true partnerships turn vendors into part of the team — and that’s exactly how he describes NetSuite Advanced Customer Support (ACS).\"They are here with us every week. They know all my schematics, they know all my data infrastructure, they know all my database structure within NetSuite. Essentially, they are an extension of my team,\" Lee explains.Close collaboration benefits both parties. DoorDash keeps NetSuite attuned to the realities of hypergrowth and gets instant feedback on technology capability and scalability. In turn, NetSuite stays close to a marquee customer. Interaction is ongoing — and frank, according to Lee. “We work directly with NetSuite ACS and often ask, &#x27;Can NetSuite do this?’ If they can prove it can, we stay with NetSuite.\"Another benefit is the ability to extend DoorDash&#x27;s expertise without expanding headcount. \"If someone says to me, &#x27;Gordon, you&#x27;re just an accountant. How do you know about systems? I say, I don&#x27;t. I have a network guy with us, an expert.’ That&#x27;s the kind of partner I want to surround myself with, so that I can grow beyond what I am.”By embedding expertise within our partnerships, DoorDash scales with precision and control. Lee says the model applies to other companies preparing for IPOs or global expansion. He adds that sustainable growth depends as much on shared understanding as on technology itself. Too often, finance and IT “look at the same requirement but see completely different things,” Lee says, describing what he calls the “blue versus purple” problem. “The accountant doesn’t understand the configuration of the system,” he explains. “The IT guy doesn’t understand what the accountant was trying to tell them.”NetSuite bridges that gap. With a unified data model and built-in best practices across finance, operations, and more, it keeps teams aligned and information consistent. That close collaboration, Lee notes, is what keeps rollouts smooth, data clean, and growth sustainable at any stage.AI strategy: Trust only internal data, get data ducks in a rowLee plans to test the NetSuite AI Connector Service — which supports Model Context Protocol (MCP) and lets customers connect their own AI to NetSuite — to see how faster access to accurate data can drive growth.By implementing an internal instance, Lee is less worried about disruptive errors from LLMs trained on public data sources. \"Think about a generative AI chatbot. When you ask a question, it can reflect many perspectives,” he explains. On the other hand, a chatbot trained on private enterprise systems benefits from “a clean data infrastructure.”Lee is taking a methodical approach: first get data pristine, then train AI on domain-specific terminology, and finally see how internal AI can both find the right information and automate downstream accounting processes to save resources and accelerate growth.Betting long-term on its original financial coreFrom early growth to major acquisitions that helped expand its footprint across the globe, DoorDash has relied on NetSuite as a consistent foundation for innovation and scale.Lee credits NetSuite’s flexible architecture and close partnership with helping enable DoorDash as it continued to scale and cement itself as a leader in local commerce globally.His mantra is simple: “Focus on growth instead of churning through vendors.”* Based on the combined numbers for DoorDash, Wolt, and Deliveroo, measured as of September 2025.Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.",
          "feed_position": 3,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/6ZnGG4zYxTpmnj2txjQwJD/9b3b125e58398fe9ab2b4865aade5e62/AdobeStock_792967914_Editorial_Use_Only.jpeg?w=300&q=30"
        }
      ],
      "featured_image": "https://images.ctfassets.net/jdtwqhzvc2n1/7iyQoeSwdOqqpfcE0PFWgF/48db7d0305019eee107028d9f018d2ac/Semantic_caching.png?w=300&q=30",
      "popularity_score": 2013.7446469444444
    },
    {
      "id": "cluster_39",
      "coverage": 2,
      "updated_at": "Mon, 12 Jan 2026 21:44:43 +0000",
      "title": "Mark Zuckerberg says Meta is launching its own AI infrastructure initiative",
      "neutral_headline": "Mark Zuckerberg says Meta is launching its own AI infrastructure initiative",
      "items": [
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2026/01/12/mark-zuckerberg-says-meta-is-launching-its-own-ai-infrastructure-initiative/",
          "published_at": "Mon, 12 Jan 2026 21:44:43 +0000",
          "title": "Mark Zuckerberg says Meta is launching its own AI infrastructure initiative",
          "standfirst": "Meta is ramping up its efforts to build out its AI capacity — Zuckerberg said the company intended to drastically expand its energy footprint in the coming years.",
          "content": "Meta is ramping up its efforts to build out its AI capacity — Zuckerberg said the company intended to drastically expand its energy footprint in the coming years.",
          "feed_position": 5
        },
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/260112/p28#a260112p28",
          "published_at": "Mon, 12 Jan 2026 13:20:04 -0500",
          "title": "Mark Zuckerberg says Meta is establishing a new \"top-level\" initiative called Meta Compute to build \"tens of gigawatts\" of AI infrastructure during this decade (Sara Fischer/Axios)",
          "standfirst": "Sara Fischer / Axios: Mark Zuckerberg says Meta is establishing a new &ldquo;top-level&rdquo; initiative called Meta Compute to build &ldquo;tens of gigawatts&rdquo; of AI infrastructure during this decade &mdash; Meta CEO Mark Zuckerberg on Monday said his company is establishing a new &ldquo;top-level&rdquo; &hellip;",
          "content": "Sara Fischer / Axios: Mark Zuckerberg says Meta is establishing a new &ldquo;top-level&rdquo; initiative called Meta Compute to build &ldquo;tens of gigawatts&rdquo; of AI infrastructure during this decade &mdash; Meta CEO Mark Zuckerberg on Monday said his company is establishing a new &ldquo;top-level&rdquo; &hellip;",
          "feed_position": 13,
          "image_url": "http://www.techmeme.com/260112/i28.jpg"
        }
      ],
      "featured_image": "http://www.techmeme.com/260112/i28.jpg",
      "popularity_score": 2013.7046469444444
    },
    {
      "id": "cluster_44",
      "coverage": 2,
      "updated_at": "Mon, 12 Jan 2026 20:53:36 GMT",
      "title": "Can Google save Apple AI? Gemini to power a new, personalized Siri",
      "neutral_headline": "Can Google save Apple AI? Gemini to power a new, personalized Siri",
      "items": [
        {
          "source": "ZDNet",
          "url": "https://www.zdnet.com/article/gemini-apple-intelligence-siri-upgrade/",
          "published_at": "Mon, 12 Jan 2026 20:53:36 GMT",
          "title": "Can Google save Apple AI? Gemini to power a new, personalized Siri",
          "standfirst": "A new deal between Apple and Google makes Gemini the cloud-based technology driving Apple Intelligence and Siri. Here's what that could look like.",
          "content": "A new deal between Apple and Google makes Gemini the cloud-based technology driving Apple Intelligence and Siri. Here's what that could look like.",
          "feed_position": 11
        },
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2026/01/12/googles-gemini-to-power-apples-ai-features-like-siri/",
          "published_at": "Mon, 12 Jan 2026 17:12:41 +0000",
          "title": "Google’s Gemini to power Apple’s AI features like Siri",
          "standfirst": "Apple and Google have embarked on a non-exclusive, multi-year partnership that will involve Apple using Gemini models and Google cloud technology for future foundational models.",
          "content": "Apple and Google have embarked on a non-exclusive, multi-year partnership that will involve Apple using Gemini models and Google cloud technology for future foundational models.",
          "feed_position": 13
        }
      ],
      "popularity_score": 2012.8527025
    },
    {
      "id": "cluster_52",
      "coverage": 2,
      "updated_at": "Mon, 12 Jan 2026 15:00:48 -0500",
      "title": "Anthropic launches Cowork for Claude, built on Claude Code to automate complex tasks with minimal prompting, as a research preview for Claude Max subscribers (Webb Wright/ZDNET)",
      "neutral_headline": "Claude Cowork automates complex tasks for you now - at your own risk",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/260112/p30#a260112p30",
          "published_at": "Mon, 12 Jan 2026 15:00:48 -0500",
          "title": "Anthropic launches Cowork for Claude, built on Claude Code to automate complex tasks with minimal prompting, as a research preview for Claude Max subscribers (Webb Wright/ZDNET)",
          "standfirst": "Webb Wright / ZDNET: Anthropic launches Cowork for Claude, built on Claude Code to automate complex tasks with minimal prompting, as a research preview for Claude Max subscribers &mdash; ZDNET's key takeaways &mdash; Anthropic is launching Cowork for Claude as a research preview. &mdash; It's built upon Claude Code and can automate complex tasks.",
          "content": "Webb Wright / ZDNET: Anthropic launches Cowork for Claude, built on Claude Code to automate complex tasks with minimal prompting, as a research preview for Claude Max subscribers &mdash; ZDNET's key takeaways &mdash; Anthropic is launching Cowork for Claude as a research preview. &mdash; It's built upon Claude Code and can automate complex tasks.",
          "feed_position": 11,
          "image_url": "http://www.techmeme.com/260112/i30.jpg"
        },
        {
          "source": "ZDNet",
          "url": "https://www.zdnet.com/article/anthropic-cowork-for-claude-complex-actions-security-risks/",
          "published_at": "Mon, 12 Jan 2026 19:30:40 GMT",
          "title": "Claude Cowork automates complex tasks for you now - at your own risk",
          "standfirst": "Available first to Claude Max subscribers, the research preview empowers Anthropic's chatbot to handle complex tasks.",
          "content": "Available first to Claude Max subscribers, the research preview empowers Anthropic's chatbot to handle complex tasks.",
          "feed_position": 14
        }
      ],
      "featured_image": "http://www.techmeme.com/260112/i30.jpg",
      "popularity_score": 2011.9727025
    },
    {
      "id": "cluster_23",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 23:42:09 +0000",
      "title": "Anthropic launches Cowork, a Claude Code-like for general computing",
      "neutral_headline": "Anthropic launches Cowork, a Claude Code-like for general computing",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/ai/2026/01/anthropic-launches-cowork-a-claude-code-like-for-general-computing/",
          "published_at": "Mon, 12 Jan 2026 23:42:09 +0000",
          "title": "Anthropic launches Cowork, a Claude Code-like for general computing",
          "standfirst": "Users can give Claude access to a folder and tell it what to do for them.",
          "content": "Anthropic's agentic tool Claude Code has been an enormous hit with some software developers and hobbyists, and now the company is bringing that modality to more general office work with a new feature called Cowork. Built on the same foundations as Claude Code and baked into the macOS Claude desktop app, Cowork allows users to give Claude access to a specific folder on their computer and then give plain language instructions for tasks. Anthropic gave examples like filling out an expense report from a folder full of receipt photos, writing reports based on a big stack of digital notes, or reorganizing a folder (or cleaning up your desktop) based on a prompt.Read full article Comments",
          "feed_position": 0,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/Claude-Cowork-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/Claude-Cowork-1152x648.jpg",
      "popularity_score": 348.6618691666667
    },
    {
      "id": "cluster_25",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 23:04:33 +0000",
      "title": "You can now reserve a hotel room on the Moon for $250,000",
      "neutral_headline": "You can now reserve a hotel room on the Moon for $250,000",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2026/01/you-can-now-reserve-a-hotel-room-on-the-moon-for-250000/",
          "published_at": "Mon, 12 Jan 2026 23:04:33 +0000",
          "title": "You can now reserve a hotel room on the Moon for $250,000",
          "standfirst": "\"We can't keep everyone living on that first ship that sailed to North America.\"",
          "content": "A company called GRU Space publicly announced its intent to construct a series of increasingly sophisticated habitats on the Moon, culminating in a hotel inspired by the Palace of the Fine Arts in San Francisco. On Monday, the company invited those interested in a berth to plunk down a deposit between $250,000 and $1 million, qualifying them for a spot on one of its early lunar surface missions in as little as six years from now. It sounds crazy, doesn't it? After all, GRU Space had, as of late December when I spoke to founder Skyler Chan, a single full-time employee aside from himself. And Chan, in fact, only recently graduated from the University of California, Berkeley.Read full article Comments",
          "feed_position": 1,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/lunar-hotel-1-1152x648-1768258910.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/lunar-hotel-1-1152x648-1768258910.jpg",
      "popularity_score": 338.0352025
    },
    {
      "id": "cluster_26",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 22:49:31 +0000",
      "title": "Paramount sues WBD over Netflix deal. WBD says Paramount’s price is still inadequate.",
      "neutral_headline": "Paramount sues WBD over Netflix deal. WBD says Paramount’s price is still inadequate.",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2026/01/paramount-sues-wbd-over-netflix-deal-wbd-says-paramounts-price-is-still-inadequate/",
          "published_at": "Mon, 12 Jan 2026 22:49:31 +0000",
          "title": "Paramount sues WBD over Netflix deal. WBD says Paramount’s price is still inadequate.",
          "standfirst": "WBD calls Paramount's lawsuit \"meritless\" and its offer deficient.",
          "content": "Paramount Skydance escalated its hostile takeover bid of Warner Bros. Discovery (WBD) today by filing a lawsuit in Delaware Chancery Court against WBD, declaring its intention to fight Netflix’s acquisition. In December, WBD agreed to sell its streaming and movie businesses to Netflix for $82.7 billion. The deal would see WBD’s Global Networks division, comprised of WBD's legacy cable networks, spun out into a separate company called Discovery Global. But in December, Paramount submitted a hostile takeover bid and amended its bid for WBD. Subsequently, the company has aggressively tried to convince WBD’s shareholders that its $108.4 billion offer for all of WBD is superior to the Netflix deal. Today, Paramount CEO David Ellison wrote a letter to WBD shareholders informing them of Paramount’s lawsuit. The lawsuit requests the court to force WBD to disclose “how it valued the Global Networks stub equity, how it valued the overall Netflix transaction, how the purchase price reduction for debt works in the Netflix transaction, or even what the basis is for its ‘risk adjustment’” of Paramount’s $30 per share all-cash offer. Netflix’s offer equates to $27.72 per share, including $23.25 in cash and shares of Netflix common stock. Paramount hopes the information will encourage more WBD shareholders to tender their shares under Paramount's offer by the January 21 deadline.Read full article Comments",
          "feed_position": 2,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2215193098-1152x648-1768255617.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2215193098-1152x648-1768255617.jpg",
      "popularity_score": 327.78464694444443
    },
    {
      "id": "cluster_28",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 22:27:50 +0000",
      "title": "Even Linus Torvalds is trying his hand at vibe coding (but just a little)",
      "neutral_headline": "Even Linus Torvalds is trying his hand at vibe coding (but just a little)",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/ai/2026/01/hobby-github-repo-shows-linus-torvalds-vibe-codes-sometimes/",
          "published_at": "Mon, 12 Jan 2026 22:27:50 +0000",
          "title": "Even Linus Torvalds is trying his hand at vibe coding (but just a little)",
          "standfirst": "\"But then I cut out the middle man—me.\"",
          "content": "Linux and Git creator Linus Torvalds' latest project contains code that was \"basically written by vibe coding,\" but you shouldn't read that to mean that Torvalds is embracing that approach for anything and everything. Torvalds sometimes works on a small hobby projects over holiday breaks. Last year, he made guitar pedals. This year, he did some work on AudioNoise, which he calls \"another silly guitar-pedal-related repo.\" It creates random digital audio effects. Torvalds revealed that he had used an AI coding tool in the README for the repo:Read full article Comments",
          "feed_position": 3,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2015/08/LinuxCon_Europe_Linus_Torvalds_05-1152x648-1768254932.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2015/08/LinuxCon_Europe_Linus_Torvalds_05-1152x648-1768254932.jpg",
      "popularity_score": 323.42325805555555
    },
    {
      "id": "cluster_34",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 22:07:23 +0000",
      "title": "Verizon to stop automatic unlocking of phones as FCC ends 60-day unlock rule",
      "neutral_headline": "Verizon to stop automatic unlocking of phones as FCC ends 60-day unlock rule",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2026/01/fcc-lets-verizon-lock-phones-for-longer-making-it-harder-to-switch-carriers/",
          "published_at": "Mon, 12 Jan 2026 22:07:23 +0000",
          "title": "Verizon to stop automatic unlocking of phones as FCC ends 60-day unlock rule",
          "standfirst": "FCC waives rule that forced Verizon to unlock phones 60 days after activation.",
          "content": "The Federal Communications Commission is letting Verizon lock phones to its network for longer periods, eliminating a requirement to unlock handsets 60 days after they are activated on its network. The change will make it harder for people to switch from Verizon to other carriers. The FCC today granted Verizon's petition for a waiver of the 60-day unlocking requirement. While the waiver is in effect, Verizon only has to comply with the CTIA trade group's voluntary unlocking policy. The CTIA policy calls for unlocking prepaid mobile devices one year after activation, while devices on postpaid plans can be unlocked after a contract, device financing plan, or early termination fee is paid. Unlocking a phone allows it to be used on another carrier's network. While Verizon was previously required to unlock phones automatically after 60 days, the CTIA code says carriers only have to unlock phones \"upon request\" from consumers. The FCC said the Verizon waiver will remain in effect until the agency \"decides on an appropriate industry-wide approach for the unlocking of handsets.\"Read full article Comments",
          "feed_position": 4,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/verizon-jerks-locked-phone-1152x648-1765486982.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/verizon-jerks-locked-phone-1152x648-1765486982.jpg",
      "popularity_score": 307.08242472222224
    },
    {
      "id": "cluster_40",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 21:27:35 +0000",
      "title": "Judge: Trump violated Fifth Amendment by ending energy grants in only blue states",
      "neutral_headline": "Judge: Trump violated Fifth Amendment by ending energy grants in only blue states",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2026/01/judge-trump-violated-fifth-amendment-by-ending-energy-grants-in-only-blue-states/",
          "published_at": "Mon, 12 Jan 2026 21:27:35 +0000",
          "title": "Judge: Trump violated Fifth Amendment by ending energy grants in only blue states",
          "standfirst": "Donald Trump’s social media post triggers rare Fifth Amendment ruling.",
          "content": "The Trump administration violated the Fifth Amendment when canceling billions of dollars in environmental grants for projects in \"blue states\" that didn't vote for him in the last election, a judge ruled Monday. Trump's blatant discrimination came on the same day as the government shut down last fall. In total, 315 grants were terminated in October, ending support for 223 projects worth approximately $7.5 billion, the Department of Energy confirmed. All the awardees, except for one, were based in states where Donald Trump lost the majority vote to Kamala Harris in 2024. Only seven awardees sued, defending projects that helped states with \"electric vehicle development, updating building energy codes, and addressing methane emissions.\" They accused Trump officials of clearly discriminating against Democratic voters by pointing to their social media posts boasting about punishing blue states.Read full article Comments",
          "feed_position": 6,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2254848310-1024x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2254848310-1024x648.jpg",
      "popularity_score": 286.4190913888889
    },
    {
      "id": "cluster_51",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 20:00:54 +0000",
      "title": "Switching water sources improved hygiene of Pompeii’s public baths",
      "neutral_headline": "Switching water sources improved hygiene of Pompeii’s public baths",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2026/01/switching-water-sources-improved-hygiene-of-pompeiis-public-baths/",
          "published_at": "Mon, 12 Jan 2026 20:00:54 +0000",
          "title": "Switching water sources improved hygiene of Pompeii’s public baths",
          "standfirst": "Scientists analyzed carbonate deposits from baths, aqueduct to learn more about city's changing water supply.",
          "content": "The eruption of Mount Vesuvius in 79 CE released thermal energy roughly equivalent to 100,000 times the atomic bombs dropped on Hiroshima and Nagasaki at the end of World War II, spewing molten rock, pumice, and hot ash over Pompeii. Pompeii's public baths, aqueduct, and water towers were among the preserved structures frozen in time. A new paper published in the Proceedings of the National Academy of Sciences analyzed calcium carbonate deposits from those structures to learn more about the city's water supply and how it changed over time. Pompeii was founded in the sixth century BCE. Prior research revealed that, early on, the city relied on rainwater stored in cisterns and wells for its water supply. The public baths used weight-lifting machinery to lift water up well shafts that were as deep as 40 meters. As the city developed, so did the complexity of its water supply system, most notably with the construction of an aqueduct between 27 BCE and 14 CE. The authors of this latest paper were interested in the calcium carbonate deposits left by water in well shafts as well as the baths and aqueduct. The different layers have \"different chemical and isotope composition, calcite crystal size, and shape,\" which in turn could reveal information about seasonal changes in temperature, as well as changes over time in the chemical composition of the water. Analyzing those properties would enable them to \"reconstruct the history of such systems—particularly public baths—revealing aspects of their maintenance and the adaptations made during their period of use,\" the authors wrote.Read full article Comments",
          "feed_position": 7,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/bath1-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/bath1-1152x648.jpg",
      "popularity_score": 274.9743691666667
    },
    {
      "id": "cluster_55",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 19:36:17 +0000",
      "title": "Apps like Grok are explicitly banned under Google’s rules—why is it still in the Play Store?",
      "neutral_headline": "Apps like Grok are explicitly banned under Google’s rules—why is it...",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/google/2026/01/apps-like-grok-are-explicitly-banned-under-googles-rules-why-is-it-still-in-the-play-store/",
          "published_at": "Mon, 12 Jan 2026 19:36:17 +0000",
          "title": "Apps like Grok are explicitly banned under Google’s rules—why is it still in the Play Store?",
          "standfirst": "Google describes apps exactly like Grok and says they are banned from Google Play.",
          "content": "Elon Musk's xAI recently weakened content guard rails for image generation in the Grok AI bot. This led to a new spate of non-consensual sexual imagery on X, much of it aimed at silencing women on the platform. This, along with the creation of sexualized images of children in the more compliant Grok, has led regulators to begin investigating xAI. In the meantime, Google has rules in place for exactly this eventuality—it's just not enforcing them. It really could not be more clear from Google's publicly available policies that Grok should have been banned yesterday. And yet, it remains in the Play Store. Not only that—it enjoys a T for Teen rating, one notch below the M-rated X app. Apple also still offers the Grok app on its platform, but its rules actually leave more wiggle room. App content restrictions at Apple and Google have evolved in very different ways. From the start, Apple has been prone to removing apps on a whim, so developers have come to expect that Apple's guidelines may not mention every possible eventuality. As Google has shifted from a laissez-faire attitude to more hard-nosed control of the Play Store, it has progressively piled on clarifications in the content policy. As a result, Google's rules are spelled out in no uncertain terms, and Grok runs afoul of them.Read full article Comments",
          "feed_position": 9,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/07/GettyImages-2225386195-1024x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/07/GettyImages-2225386195-1024x648.jpg",
      "popularity_score": 266.56409138888887
    },
    {
      "id": "cluster_54",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 19:56:01 +0000",
      "title": "Supreme Court takes case that could strip FCC of authority to issue fines",
      "neutral_headline": "Supreme Court takes case that could strip FCC of authority to issue fines",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2026/01/supreme-court-takes-case-that-could-strip-fcc-of-authority-to-issue-fines/",
          "published_at": "Mon, 12 Jan 2026 19:56:01 +0000",
          "title": "Supreme Court takes case that could strip FCC of authority to issue fines",
          "standfirst": "AT&#038;T and Verizon claim right to a jury trial was violated by FCC fines.",
          "content": "The Supreme Court will hear a case that could invalidate the Federal Communications Commission's authority to issue fines against companies regulated by the FCC. AT&T, Verizon, and T-Mobile challenged the FCC's ability to punish them after the commission fined the carriers for selling customer location data without their users’ consent. AT&T convinced the US Court of Appeals for the 5th Circuit to overturn its fine, while Verizon lost in the 2nd Circuit and T-Mobile lost in the District of Columbia Circuit. Verizon petitioned the Supreme Court to reverse its loss, while the FCC and Justice Department petitioned the court to overturn AT&T's victory in the 5th Circuit. The Supreme Court granted both petitions to hear the challenges and consolidated the cases in a list of orders released Friday. Oral arguments will be held.Read full article Comments",
          "feed_position": 8,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2023/09/getty-supreme-court-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2023/09/getty-supreme-court-1152x648.jpg",
      "popularity_score": 264.8929802777778
    },
    {
      "id": "cluster_74",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 17:57:32 +0000",
      "title": "Apple chooses Google’s Gemini over OpenAI’s ChatGPT to power next-gen Siri",
      "neutral_headline": "Apple chooses Google’s Gemini over OpenAI’s ChatGPT to power next-gen Siri",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/apple/2026/01/apple-says-its-new-ai-powered-siri-will-use-googles-gemini-language-models/",
          "published_at": "Mon, 12 Jan 2026 17:57:32 +0000",
          "title": "Apple chooses Google’s Gemini over OpenAI’s ChatGPT to power next-gen Siri",
          "standfirst": "Apple goes with Google's tech despite using OpenAI's ChatGPT elsewhere in iOS.",
          "content": "The \"more intelligent\" version of Siri that Apple plans to release later this year will be backed by Google's Gemini language models, the company announced today. CNBC reports that the deal is part of a \"multi-year partnership\" between Apple and Google that will allow Apple to use Google's AI models in its own software. \"After careful evaluation, we determined that Google’s technology provides the most capable foundation for Apple Foundation Models and we’re excited about the innovative new experiences it will unlock for our users,” reads an Apple statement given to CNBC. Today's announcement confirms reporting by Bloomberg's Mark Gurman late last year that Apple and Google were nearing a deal. Apple didn't disclose terms, but Gurman said that Apple would be paying Google \"about $1 billion a year\" for access to its AI models \"following an extensive evaluation period.\"Read full article Comments",
          "feed_position": 11,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/apple_google_hero_3-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/apple_google_hero_3-1152x648.jpg",
      "popularity_score": 169.91825805555555
    },
    {
      "id": "cluster_89",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 15:21:38 +0000",
      "title": "NASA topples towers used to test Saturn rockets, space shuttle",
      "neutral_headline": "NASA topples towers used to test Saturn rockets, space shuttle",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2026/01/nasa-topples-towers-used-to-test-saturn-rockets-space-shuttle/",
          "published_at": "Mon, 12 Jan 2026 15:21:38 +0000",
          "title": "NASA topples towers used to test Saturn rockets, space shuttle",
          "standfirst": "The Propulsion and Structural Test Facility and Dynamic Test Facility are no more.",
          "content": "Two historic NASA test facilities used in the development of the Saturn V and space shuttle launch vehicles have been demolished after towering over the Marshall Space Flight Center in Alabama since the start of the Space Age. The Propulsion and Structural Test Facility, which was erected in 1957—the same year the first artificial satellite entered Earth orbit—and the Dynamic Test Facility, which has stood since 1964, were brought down by a coordinated series of implosions on Saturday, January 10. Located in Marshall's East Test Area on the US Army's Redstone Arsenal in Huntsville, the two structures were no longer in use and, according to NASA, had a backlog of $25 million in needed repairs. \"This work reflects smart stewardship of taxpayer resources,\" Jared Isaacman, NASA administrator, said in a statement. \"Clearing outdated infrastructure allows NASA to safely modernize, streamline operations and fully leverage the infrastructure investments signed into law by President Trump to keep Marshall positioned at the forefront of aerospace innovation.\"Read full article Comments",
          "feed_position": 16,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/news-010826c-lg-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/news-010826c-lg-1152x648.jpg",
      "popularity_score": 153.31992472222223
    },
    {
      "id": "cluster_60",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 19:25:17 +0000",
      "title": "NASA launches new mission to get the most out of the James Webb Space Telescope",
      "neutral_headline": "NASA launches new mission to get the most out of the James Webb Space Telescope",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2026/01/nasas-newest-telescope-will-play-an-outsize-role-in-finding-earth-2-0/",
          "published_at": "Mon, 12 Jan 2026 19:25:17 +0000",
          "title": "NASA launches new mission to get the most out of the James Webb Space Telescope",
          "standfirst": "\"It was not recognized how serious a problem that is until... about 2017 or 2018.\"",
          "content": "Among other things, the James Webb Space Telescope is designed to get us closer to finding habitable worlds around faraway stars. From its perch a million miles from Earth, Webb's huge gold-coated mirror collects more light than any other telescope put into space. The Webb telescope, launched in 2021 at a cost of more than $10 billion, has the sensitivity to peer into distant planetary systems and detect the telltale chemical fingerprints of molecules critical to or indicative of potential life, like water vapor, carbon dioxide, and methane. Webb can do this while also observing the oldest observable galaxies in the Universe and studying planets, moons, and smaller objects within our own Solar System. Naturally, astronomers want to get the most out of their big-budget observatory. That's where NASA's Pandora mission comes in.Read full article Comments",
          "feed_position": 10,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/Pandora_integrated_blue_BCT-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/Pandora_integrated_blue_BCT-1152x648.jpg",
      "popularity_score": 152.38075805555556
    },
    {
      "id": "cluster_85",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 15:59:15 +0000",
      "title": "The Chevrolet Bolt is back... but for how long?",
      "neutral_headline": "The Chevrolet Bolt is back... but for how long",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/cars/2026/01/the-chevrolet-bolt-is-back-but-for-how-long/",
          "published_at": "Mon, 12 Jan 2026 15:59:15 +0000",
          "title": "The Chevrolet Bolt is back... but for how long?",
          "standfirst": "The new LFP battery pack has 262 miles of range and fast-charges at 150 kW.",
          "content": "The new Chevrolet Equinox EV is a solid entry into the compact crossover market, and with a (just) sub-$35,000 starting price, it also counts as affordable by the standards of 2026. But if you think that's too rich for your blood, or that the Equinox is still too large for your needs, take heart—the Chevrolet Bolt is back in dealerships now as well. The Bolt was GM's first modern electric vehicle, following on from the hand-built, pre-lithium ion EV1 and the compliance car that was the Spark EV. We're big fans of the Bolt here at Ars Technica. It offered well more than 200 miles of range in a mass-produced EV at a reasonable price well before Tesla's Model 3 started clogging up our roads, it got more efficient over time, and it managed to be fun to drive in the process. General Motors (which owns Chevrolet) probably feels less well-disposed toward the Bolt. It lost thousands of dollars on each car it sold, even before the entire fleet had to be recalled for a costly battery replacement. The issue was due to improperly folded tabs on some cells that could cause a battery fire, giving GM (and its battery partner LG) plenty of bad press in the process. That recall alone cost $1.8 billion.Read full article Comments",
          "feed_position": 14,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/Chevrolet-Bolt-2027-DriverRear-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/Chevrolet-Bolt-2027-DriverRear-1152x648.jpg",
      "popularity_score": 143.94686916666666
    },
    {
      "id": "cluster_83",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 16:32:21 +0000",
      "title": "UK probes X over Grok CSAM scandal; Elon Musk cries censorship",
      "neutral_headline": "UK probes X over Grok CSAM scandal; Elon Musk cries censorship",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2026/01/uk-investigating-x-after-grok-undressed-thousands-of-women-and-children/",
          "published_at": "Mon, 12 Jan 2026 16:32:21 +0000",
          "title": "UK probes X over Grok CSAM scandal; Elon Musk cries censorship",
          "standfirst": "Grok tests if UK can penalize platforms for sexualized deepfakes generated by AI.",
          "content": "Elon Musk's X is currently under investigation in the United Kingdom after failing to stop the platform's chatbot, Grok, from generating thousands of sexualized images of women and children. On Monday, UK media regulator Ofcom confirmed that X may have violated the UK's Online Safety Act, which requires platforms to block illegal content. The proliferation of \"undressed images of people\" by X users may amount to intimate image abuse, pornography, and child sexual abuse material (CSAM), the regulator said. And X may also have neglected its duty to stop kids from seeing porn. \"Reports of Grok being used to create and share illegal non-consensual intimate images and child sexual abuse material on X have been deeply concerning,\" an Ofcom spokesperson said. \"Platforms must protect people in the UK from content that’s illegal in the UK, and we won’t hesitate to investigate where we suspect companies are failing in their duties, especially where there’s a risk of harm to children.\"Read full article Comments",
          "feed_position": 13,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2246892016-1024x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2246892016-1024x648.jpg",
      "popularity_score": 141.49853583333334
    },
    {
      "id": "cluster_86",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 15:55:20 +0000",
      "title": "New research shows how shunning ultraprocessed foods helps with aging",
      "neutral_headline": "New research shows how shunning ultraprocessed foods helps with aging",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/health/2026/01/avoiding-ultraprocessed-foods-supports-healthier-aging/",
          "published_at": "Mon, 12 Jan 2026 15:55:20 +0000",
          "title": "New research shows how shunning ultraprocessed foods helps with aging",
          "standfirst": "Studies have linked ultraprocessed foods to poor health outcomes.",
          "content": "Older adults can dramatically reduce the amount of ultraprocessed foods they eat while keeping a familiar, balanced diet—and this shift leads to improvements across several key markers related to how the body regulates appetite and metabolism. That’s the main finding of a new study my colleagues and I published in the journal Clinical Nutrition. Ultraprocessed foods are made using industrial techniques and ingredients that aren’t typically used in home cooking. They often contain additives such as emulsifiers, flavorings, colors, and preservatives. Common examples include packaged snacks, ready-to-eat meals, and some processed meats. Studies have linked diets high in ultraprocessed foods to poorer health outcomes. My team and I enrolled Americans ages 65 and older in our study, many of whom were overweight or had metabolic risk factors such as insulin resistance or high cholesterol. Participants followed two diets low in ultraprocessed foods for eight weeks each. One included lean red meat (pork); the other was vegetarian with milk and eggs. For two weeks in between, participants returned to their usual diets.Read full article Comments",
          "feed_position": 15,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/ultraprocessed-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/ultraprocessed-1152x648.jpg",
      "popularity_score": 140.8815913888889
    },
    {
      "id": "cluster_78",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 17:08:56 +0000",
      "title": "Is this the beginning of the end for GameStop?",
      "neutral_headline": "Is this the beginning of the end for GameStop",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gaming/2026/01/is-this-the-beginning-of-the-end-for-gamestop/",
          "published_at": "Mon, 12 Jan 2026 17:08:56 +0000",
          "title": "Is this the beginning of the end for GameStop?",
          "standfirst": "The sudden closure of hundreds of storefronts isn't exactly a great sign...",
          "content": "Six and a half years ago—after a failed corporate sale attempt, massive financial losses, and the departure/layoff of many key staff—I wrote about what seemed at the time like the \"imminent demise\" of GameStop. Now, after five years of meme stock mania that helped prop up the company's finances a bit, I'll admit the video game and Funko Pop retailer has lasted much longer as a relevant entity than I anticipated. GameStop's surprisingly extended run may be coming to an end, though, with Polygon reporting late last week that GameStop has abruptly shut down 400 stores across the US, with even more closures expected before the end of the month. That comes on top of 590 US stores that were shuttered in fiscal 2024 (which ended in January 2025) and stated plans to close hundreds of remaining international stores across Canada, Australia, and Europe in the coming months, per SEC filings. GameStop still had just over 3,200 stores worldwide as of February 1, 2025, so even hundreds of new and planned store closures don't literally mean the immediate end of the company as a going concern. But when you consider that there were still nearly 6,000 GameStop locations worldwide as of 2019—nearly 4,000 of which were in the US—the long-term trend is clear.Read full article Comments",
          "feed_position": 12,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2019/08/GettyImages-1135950796-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2019/08/GettyImages-1135950796-1152x648.jpg",
      "popularity_score": 139.10825805555555
    },
    {
      "id": "cluster_95",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 11:30:06 +0000",
      "title": "The most fascinating monitors at CES 2026",
      "neutral_headline": "The most fascinating monitors at CES 2026",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2026/01/the-most-fascinating-monitors-at-ces-2026/",
          "published_at": "Mon, 12 Jan 2026 11:30:06 +0000",
          "title": "The most fascinating monitors at CES 2026",
          "standfirst": "Big sizes, big resolution, and big ideas.",
          "content": "CES 2026 took place in Las Vegas last week, and as usual, we're looking at the most interesting monitors from the show. Not every display is a monitor in the strictest sense, but they all provide a display for computers and have a unique twist that make them worth exploring. Dell’s massive UltraSharp Dell's biggest UltraSharp has a 21:9 aspect ratio. Credit: Dell It was a pretty safe bet that Dell would announce new UltraSharp monitors at CES. The displays are a solid recommendation for reliable USB-C monitors, including for Mac users and people needing something polished for professional or creative work. In recent years, UltraSharp monitors have also boasted more modern features, including integrated web cameras and IPS Black tech. This year, the strategy was clear: Bigger is better.Read full article Comments",
          "feed_position": 17,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/TCXAIO.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/TCXAIO.jpg",
      "popularity_score": 133.46103583333334
    },
    {
      "id": "cluster_101",
      "coverage": 1,
      "updated_at": "Sun, 11 Jan 2026 20:35:33 +0000",
      "title": "That time Will Smith helped discover new species of anaconda",
      "neutral_headline": "That time Will Smith helped discover new species of anaconda",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2026/01/that-time-will-smith-helped-discover-new-species-of-anaconda/",
          "published_at": "Sun, 11 Jan 2026 20:35:33 +0000",
          "title": "That time Will Smith helped discover new species of anaconda",
          "standfirst": "Footage of the 2024 discovery appears in NatGeo's new documentary series Pole to Pole with Will Smith.",
          "content": "In 2024, scientists announced the discovery of a new species of giant anaconda in South America. A National Geographic camera crew was on hand for the 2022 expedition that documented the new species—and so was actor Will Smith, since they were filming for NatGeo's new documentary series, Pole to Pole with Will Smith. Now we can all share in Smith's Amazon experience, courtesy of the three-minute clip above. Along with venom expert Bryan Fry, we follow Smith's journey by boat with a team of indigenous Waorani guides, scouring the river banks for anacondas. And they find one: a female green anaconda about 16 to 17 feet long, \"pure muscle.\" The Waorani secure the giant snake—anacondas aren't venomous but they do bite—so that Fry (with Smith's understandably reluctant help) can collect a scale sample for further analysis. Fry says that this will enable him to determine the accumulation of pollutants in the water. That and other collected samples also enabled scientists to conduct the genetic analysis that resulted in the declaration of a new species: the northern green anaconda (Eunectes akayama, which roughly translates to \"the great snake\"). It is genetically distinct from the southern green anaconda (Eunectes murinus); the two species likely diverged some 10 million years ago. The northern green anaconda's turf includes Venezuela, Colombia, Suriname, French Guyana, and the northern part of Brazil.Read full article Comments",
          "feed_position": 18,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/anaconda3-1152x648-1768069746.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/anaconda3-1152x648-1768069746.jpg",
      "popularity_score": 133
    },
    {
      "id": "cluster_108",
      "coverage": 1,
      "updated_at": "Sun, 11 Jan 2026 12:00:50 +0000",
      "title": "The oceans just keep getting hotter",
      "neutral_headline": "The oceans just keep getting hotter",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2026/01/the-oceans-just-keep-getting-hotter/",
          "published_at": "Sun, 11 Jan 2026 12:00:50 +0000",
          "title": "The oceans just keep getting hotter",
          "standfirst": "For the eighth year in a row, the world’s oceans absorbed a record-breaking amount of heat in 2025.",
          "content": "Since 2018, a group of researchers from around the world has crunched the numbers on how much heat the world’s oceans are absorbing each year. In 2025, their measurements broke records once again, making this the eighth year in a row that the world’s oceans have absorbed more heat than in the years before. The study, which was published Friday in the journal Advances in Atmospheric Science, found that the world’s oceans absorbed an additional 23 zettajoules’ worth of heat in 2025, the most in any year since modern measurements began in the 1960s. That’s significantly higher than the 16 additional zettajoules they absorbed in 2024. The research comes from a team of more than 50 scientists across the United States, Europe, and China. A joule is a common way to measure energy. A single joule is a relatively small unit of measurement—it’s about enough to power a tiny lightbulb for a second, or slightly heat a gram of water. But a zettajoule is one sextillion joules; numerically, the 23 zettajoules the oceans absorbed this year can be written out as 23,000,000,000,000,000,000,000.Read full article Comments",
          "feed_position": 19,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2023/04/sun-over-ocean-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2023/04/sun-over-ocean-1152x648.jpg",
      "popularity_score": 130
    }
  ]
}