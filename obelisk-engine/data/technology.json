{
  "updated_at": "2026-01-12T19:20:41.905Z",
  "clusters": [
    {
      "id": "cluster_24",
      "coverage": 4,
      "updated_at": "Mon, 12 Jan 2026 17:12:41 +0000",
      "title": "Google’s Gemini to power Apple’s AI features like Siri",
      "neutral_headline": "Google’s Gemini to power Apple’s AI features like Siri",
      "items": [
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2026/01/12/googles-gemini-to-power-apples-ai-features-like-siri/",
          "published_at": "Mon, 12 Jan 2026 17:12:41 +0000",
          "title": "Google’s Gemini to power Apple’s AI features like Siri",
          "standfirst": "Apple and Google have embarked on a non-exclusive, multi-year partnership that will involve Apple using Gemini models and Google cloud technology for future foundational models.",
          "content": "Apple and Google have embarked on a non-exclusive, multi-year partnership that will involve Apple using Gemini models and Google cloud technology for future foundational models.",
          "feed_position": 3
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/ai/apples-siri-ai-will-be-powered-by-gemini-153636649.html",
          "published_at": "Mon, 12 Jan 2026 17:03:13 +0000",
          "title": "Apple's Siri AI will be powered by Gemini",
          "standfirst": "Apple and Google have confirmed reports that it will use Google Gemini’s models to help it power the new version of Siri and other generative AI features. CNBC first reported the news; Apple and Google subsequently released a joint statement which reads:“Apple and Google have entered into a multi-year collaboration under which the next generation of Apple Foundation Models will be based on Google's Gemini models and cloud technology. These models will help power future Apple Intelligence features, including a more personalized Siri coming this year.After careful evaluation, Apple determined that Google's Al technology provides the most capable foundation for Apple Foundation Models and is excited about the innovative new experiences it will unlock for Apple users. Apple Intelligence will continue to run on Apple devices and Private Cloud Compute, while maintaining Apple's industry-leading privacy standards.”Apple first demoed a genAI version of Siri back at WWDC 2024. In March 2025, the company said it was delaying a major Siri update until this year, but it appears that Apple is not quite ready to publicly release a more capable version of the voice assistant. In June, it was reported that Apple was considering partnerships with OpenAI and Anthropic for Siri (the voice assistant can currently tap ChatGPT for certain queries as part of Apple Intelligence). Two months later, Google emerged as a possible contender. Those rumors intensified in November, when it was reported that Apple might build the new Siri using a custom version of Gemini that runs on its Private Cloud Compute servers — and that it would pay Google around $1 billion a year for the privilege. Update, January 12, 2026, 12:03PM ET: This story has been updated with a full joint statement from Apple and Google.This article originally appeared on Engadget at https://www.engadget.com/ai/apples-siri-ai-will-be-powered-by-gemini-153636649.html?src=rss",
          "content": "Apple and Google have confirmed reports that it will use Google Gemini’s models to help it power the new version of Siri and other generative AI features. CNBC first reported the news; Apple and Google subsequently released a joint statement which reads:“Apple and Google have entered into a multi-year collaboration under which the next generation of Apple Foundation Models will be based on Google's Gemini models and cloud technology. These models will help power future Apple Intelligence features, including a more personalized Siri coming this year.After careful evaluation, Apple determined that Google's Al technology provides the most capable foundation for Apple Foundation Models and is excited about the innovative new experiences it will unlock for Apple users. Apple Intelligence will continue to run on Apple devices and Private Cloud Compute, while maintaining Apple's industry-leading privacy standards.”Apple first demoed a genAI version of Siri back at WWDC 2024. In March 2025, the company said it was delaying a major Siri update until this year, but it appears that Apple is not quite ready to publicly release a more capable version of the voice assistant. In June, it was reported that Apple was considering partnerships with OpenAI and Anthropic for Siri (the voice assistant can currently tap ChatGPT for certain queries as part of Apple Intelligence). Two months later, Google emerged as a possible contender. Those rumors intensified in November, when it was reported that Apple might build the new Siri using a custom version of Gemini that runs on its Private Cloud Compute servers — and that it would pay Google around $1 billion a year for the privilege. Update, January 12, 2026, 12:03PM ET: This story has been updated with a full joint statement from Apple and Google.This article originally appeared on Engadget at https://www.engadget.com/ai/apples-siri-ai-will-be-powered-by-gemini-153636649.html?src=rss",
          "feed_position": 1
        },
        {
          "source": "The Verge",
          "url": "https://www.theverge.com/news/860521/apple-siri-google-gemini-ai-personalization",
          "published_at": "2026-01-12T10:38:19-05:00",
          "title": "Apple picks Google’s Gemini AI for its big Siri upgrade",
          "standfirst": "Apple will use Google's Gemini AI model to power a more personalized Siri coming this year. \"After careful evaluation, Apple determined that Google's Al technology provides the most capable foundation for Apple Foundation Models and is excited about the innovative new experiences it will unlock for Apple users,\" Google and Apple announced on Monday. CNBC [&#8230;]",
          "content": "Apple will use Google's Gemini AI model to power a more personalized Siri coming this year. \"After careful evaluation, Apple determined that Google's Al technology provides the most capable foundation for Apple Foundation Models and is excited about the innovative new experiences it will unlock for Apple users,\" Google and Apple announced on Monday. CNBC first reported on the multi-year partnership, which will allow Apple to use Google Gemini and the company's cloud technology for its future models. \"These models will help power future Apple Intelligence features, including a more personalized Siri coming this year,\" Apple and Google say, a … Read the full story at The Verge.",
          "feed_position": 9
        },
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/260112/p20#a260112p20",
          "published_at": "Mon, 12 Jan 2026 10:24:44 -0500",
          "title": "Apple signs a multiyear Google deal to use Gemini and Google Cloud to power Siri's features in 2026, saying Google's tech \"provides the most capable foundation\" (Samantha Subin/CNBC)",
          "standfirst": "Samantha Subin / CNBC: Apple signs a multiyear Google deal to use Gemini and Google Cloud to power Siri's features in 2026, saying Google's tech &ldquo;provides the most capable foundation&rdquo; &mdash; Apple is joining forces with Google to power its artificial intelligence features for products such as Siri later this year.",
          "content": "Samantha Subin / CNBC: Apple signs a multiyear Google deal to use Gemini and Google Cloud to power Siri's features in 2026, saying Google's tech &ldquo;provides the most capable foundation&rdquo; &mdash; Apple is joining forces with Google to power its artificial intelligence features for products such as Siri later this year.",
          "feed_position": 9,
          "image_url": "http://www.techmeme.com/260112/i20.jpg"
        }
      ],
      "featured_image": "http://www.techmeme.com/260112/i20.jpg",
      "popularity_score": 4017.866415277778
    },
    {
      "id": "cluster_5",
      "coverage": 2,
      "updated_at": "Mon, 12 Jan 2026 19:00:00 GMT",
      "title": "Why your LLM bill is exploding — and how semantic caching can cut it by 73%",
      "neutral_headline": "Why your LLM bill is exploding — and how semantic caching can cut it by 73%",
      "items": [
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/orchestration/why-your-llm-bill-is-exploding-and-how-semantic-caching-can-cut-it-by-73",
          "published_at": "Mon, 12 Jan 2026 19:00:00 GMT",
          "title": "Why your LLM bill is exploding — and how semantic caching can cut it by 73%",
          "standfirst": "Our LLM API bill was growing 30% month-over-month. Traffic was increasing, but not that fast. When I analyzed our query logs, I found the real problem: Users ask the same questions in different ways.\"What&#x27;s your return policy?,\" \"How do I return something?\", and \"Can I get a refund?\" were all hitting our LLM separately, generating nearly identical responses, each incurring full API costs.Exact-match caching, the obvious first solution, captured only 18% of these redundant calls. The same semantic question, phrased differently, bypassed the cache entirely.So, I implemented semantic caching based on what queries mean, not how they&#x27;re worded. After implementing it, our cache hit rate increased to 67%, reducing LLM API costs by 73%. But getting there requires solving problems that naive implementations miss.Why exact-match caching falls shortTraditional caching uses query text as the cache key. This works when queries are identical:# Exact-match cachingcache_key = hash(query_text)if cache_key in cache: return cache[cache_key]But users don&#x27;t phrase questions identically. My analysis of 100,000 production queries found:Only 18% were exact duplicates of previous queries47% were semantically similar to previous queries (same intent, different wording)35% were genuinely novel queriesThat 47% represented massive cost savings we were missing. Each semantically-similar query triggered a full LLM call, generating a response nearly identical to one we&#x27;d already computed.Semantic caching architectureSemantic caching replaces text-based keys with embedding-based similarity lookup:class SemanticCache: def __init__(self, embedding_model, similarity_threshold=0.92): self.embedding_model = embedding_model self.threshold = similarity_threshold self.vector_store = VectorStore() # FAISS, Pinecone, etc. self.response_store = ResponseStore() # Redis, DynamoDB, etc. def get(self, query: str) -> Optional[str]: \"\"\"Return cached response if semantically similar query exists.\"\"\" query_embedding = self.embedding_model.encode(query) # Find most similar cached query matches = self.vector_store.search(query_embedding, top_k=1) if matches and matches[0].similarity >= self.threshold: cache_id = matches[0].id return self.response_store.get(cache_id) return None def set(self, query: str, response: str): \"\"\"Cache query-response pair.\"\"\" query_embedding = self.embedding_model.encode(query) cache_id = generate_id() self.vector_store.add(cache_id, query_embedding) self.response_store.set(cache_id, { &#x27;query&#x27;: query, &#x27;response&#x27;: response, &#x27;timestamp&#x27;: datetime.utcnow() })The key insight: Instead of hashing query text, I embed queries into vector space and find cached queries within a similarity threshold.The threshold problemThe similarity threshold is the critical parameter. Set it too high, and you miss valid cache hits. Set it too low, and you return wrong responses.Our initial threshold of 0.85 seemed reasonable; 85% similar should be \"the same question,\" right?Wrong. At 0.85, we got cache hits like:Query: \"How do I cancel my subscription?\"Cached: \"How do I cancel my order?\"Similarity: 0.87These are different questions with different answers. Returning the cached response would be incorrect.I discovered that optimal thresholds vary by query type:Query typeOptimal thresholdRationaleFAQ-style questions0.94High precision needed; wrong answers damage trustProduct searches0.88More tolerance for near-matchesSupport queries0.92Balance between coverage and accuracyTransactional queries0.97Very low tolerance for errorsI implemented query-type-specific thresholds:class AdaptiveSemanticCache: def __init__(self): self.thresholds = { &#x27;faq&#x27;: 0.94, &#x27;search&#x27;: 0.88, &#x27;support&#x27;: 0.92, &#x27;transactional&#x27;: 0.97, &#x27;default&#x27;: 0.92 } self.query_classifier = QueryClassifier() def get_threshold(self, query: str) -> float: query_type = self.query_classifier.classify(query) return self.thresholds.get(query_type, self.thresholds[&#x27;default&#x27;]) def get(self, query: str) -> Optional[str]: threshold = self.get_threshold(query) query_embedding = self.embedding_model.encode(query) matches = self.vector_store.search(query_embedding, top_k=1) if matches and matches[0].similarity >= threshold: return self.response_store.get(matches[0].id) return NoneThreshold tuning methodologyI couldn&#x27;t tune thresholds blindly. I needed ground truth on which query pairs were actually \"the same.\"Our methodology:Step 1: Sample query pairs. I sampled 5,000 query pairs at various similarity levels (0.80-0.99).Step 2: Human labeling. Annotators labeled each pair as \"same intent\" or \"different intent.\" I used three annotators per pair and took a majority vote.Step 3: Compute precision/recall curves. For each threshold, we computed:Precision: Of cache hits, what fraction had the same intent?Recall: Of same-intent pairs, what fraction did we cache-hit?def compute_precision_recall(pairs, labels, threshold): \"\"\"Compute precision and recall at given similarity threshold.\"\"\" predictions = [1 if pair.similarity >= threshold else 0 for pair in pairs] true_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 1) false_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 0) false_negatives = sum(1 for p, l in zip(predictions, labels) if p == 0 and l == 1) precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0 recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0 return precision, recallStep 4: Select threshold based on cost of errors. For FAQ queries where wrong answers damage trust, I optimized for precision (0.94 threshold gave 98% precision). For search queries where missing a cache hit just costs money, I optimized for recall (0.88 threshold).Latency overheadSemantic caching adds latency: You must embed the query and search the vector store before knowing whether to call the LLM.Our measurements:OperationLatency (p50)Latency (p99)Query embedding12ms28msVector search8ms19msTotal cache lookup20ms47msLLM API call850ms2400msThe 20ms overhead is negligible compared to the 850ms LLM call we avoid on cache hits. Even at p99, the 47ms overhead is acceptable.However, cache misses now take 20ms longer than before (embedding + search + LLM call). At our 67% hit rate, the math works out favorably:Before: 100% of queries × 850ms = 850ms averageAfter: (33% × 870ms) + (67% × 20ms) = 287ms + 13ms = 300ms averageNet latency improvement of 65% alongside the cost reduction.Cache invalidationCached responses go stale. Product information changes, policies update and yesterday&#x27;s correct answer becomes today&#x27;s wrong answer.I implemented three invalidation strategies:Time-based TTLSimple expiration based on content type:TTL_BY_CONTENT_TYPE = { &#x27;pricing&#x27;: timedelta(hours=4), # Changes frequently &#x27;policy&#x27;: timedelta(days=7), # Changes rarely &#x27;product_info&#x27;: timedelta(days=1), # Daily refresh &#x27;general_faq&#x27;: timedelta(days=14), # Very stable}Event-based invalidationWhen underlying data changes, invalidate related cache entries:class CacheInvalidator: def on_content_update(self, content_id: str, content_type: str): \"\"\"Invalidate cache entries related to updated content.\"\"\" # Find cached queries that referenced this content affected_queries = self.find_queries_referencing(content_id) for query_id in affected_queries: self.cache.invalidate(query_id) self.log_invalidation(content_id, len(affected_queries))Staleness detectionFor responses that might become stale without explicit events, I implemented periodic freshness checks:def check_freshness(self, cached_response: dict) -> bool: \"\"\"Verify cached response is still valid.\"\"\" # Re-run the query against current data fresh_response = self.generate_response(cached_response[&#x27;query&#x27;]) # Compare semantic similarity of responses cached_embedding = self.embed(cached_response[&#x27;response&#x27;]) fresh_embedding = self.embed(fresh_response) similarity = cosine_similarity(cached_embedding, fresh_embedding) # If responses diverged significantly, invalidate if similarity < 0.90: self.cache.invalidate(cached_response[&#x27;id&#x27;]) return False return TrueWe run freshness checks on a sample of cached entries daily, catching staleness that TTL and event-based invalidation miss.Production resultsAfter three months in production:MetricBeforeAfterChangeCache hit rate18%67%+272%LLM API costs$47K/month$12.7K/month-73%Average latency850ms300ms-65%False-positive rateN/A0.8%—Customer complaints (wrong answers)Baseline+0.3%Minimal increaseThe 0.8% false-positive rate (queries where we returned a cached response that was semantically incorrect) was within acceptable bounds. These cases occurred primarily at the boundaries of our threshold, where similarity was just above the cutoff but intent differed slightly.Pitfalls to avoidDon&#x27;t use a single global threshold. Different query types have different tolerance for errors. Tune thresholds per category.Don&#x27;t skip the embedding step on cache hits. You might be tempted to skip embedding overhead when returning cached responses, but you need the embedding for cache key generation. The overhead is unavoidable.Don&#x27;t forget invalidation. Semantic caching without invalidation strategy leads to stale responses that erode user trust. Build invalidation from day one.Don&#x27;t cache everything. Some queries shouldn&#x27;t be cached: Personalized responses, time-sensitive information, transactional confirmations. Build exclusion rules.def should_cache(self, query: str, response: str) -> bool: \"\"\"Determine if response should be cached.\"\" # Don&#x27;t cache personalized responses if self.contains_personal_info(response): return False # Don&#x27;t cache time-sensitive information if self.is_time_sensitive(query): return False # Don&#x27;t cache transactional confirmations if self.is_transactional(query): return False return TrueKey takeawaysSemantic caching is a practical pattern for LLM cost control that captures redundancy exact-match caching misses. The key challenges are threshold tuning (use query-type-specific thresholds based on precision/recall analysis) and cache invalidation (combine TTL, event-based and staleness detection).At 73% cost reduction, this was our highest-ROI optimization for production LLM systems. The implementation complexity is moderate, but the threshold tuning requires careful attention to avoid quality degradation.Sreenivasa Reddy Hulebeedu Reddy is a lead software engineer.",
          "content": "Our LLM API bill was growing 30% month-over-month. Traffic was increasing, but not that fast. When I analyzed our query logs, I found the real problem: Users ask the same questions in different ways.\"What&#x27;s your return policy?,\" \"How do I return something?\", and \"Can I get a refund?\" were all hitting our LLM separately, generating nearly identical responses, each incurring full API costs.Exact-match caching, the obvious first solution, captured only 18% of these redundant calls. The same semantic question, phrased differently, bypassed the cache entirely.So, I implemented semantic caching based on what queries mean, not how they&#x27;re worded. After implementing it, our cache hit rate increased to 67%, reducing LLM API costs by 73%. But getting there requires solving problems that naive implementations miss.Why exact-match caching falls shortTraditional caching uses query text as the cache key. This works when queries are identical:# Exact-match cachingcache_key = hash(query_text)if cache_key in cache: return cache[cache_key]But users don&#x27;t phrase questions identically. My analysis of 100,000 production queries found:Only 18% were exact duplicates of previous queries47% were semantically similar to previous queries (same intent, different wording)35% were genuinely novel queriesThat 47% represented massive cost savings we were missing. Each semantically-similar query triggered a full LLM call, generating a response nearly identical to one we&#x27;d already computed.Semantic caching architectureSemantic caching replaces text-based keys with embedding-based similarity lookup:class SemanticCache: def __init__(self, embedding_model, similarity_threshold=0.92): self.embedding_model = embedding_model self.threshold = similarity_threshold self.vector_store = VectorStore() # FAISS, Pinecone, etc. self.response_store = ResponseStore() # Redis, DynamoDB, etc. def get(self, query: str) -> Optional[str]: \"\"\"Return cached response if semantically similar query exists.\"\"\" query_embedding = self.embedding_model.encode(query) # Find most similar cached query matches = self.vector_store.search(query_embedding, top_k=1) if matches and matches[0].similarity >= self.threshold: cache_id = matches[0].id return self.response_store.get(cache_id) return None def set(self, query: str, response: str): \"\"\"Cache query-response pair.\"\"\" query_embedding = self.embedding_model.encode(query) cache_id = generate_id() self.vector_store.add(cache_id, query_embedding) self.response_store.set(cache_id, { &#x27;query&#x27;: query, &#x27;response&#x27;: response, &#x27;timestamp&#x27;: datetime.utcnow() })The key insight: Instead of hashing query text, I embed queries into vector space and find cached queries within a similarity threshold.The threshold problemThe similarity threshold is the critical parameter. Set it too high, and you miss valid cache hits. Set it too low, and you return wrong responses.Our initial threshold of 0.85 seemed reasonable; 85% similar should be \"the same question,\" right?Wrong. At 0.85, we got cache hits like:Query: \"How do I cancel my subscription?\"Cached: \"How do I cancel my order?\"Similarity: 0.87These are different questions with different answers. Returning the cached response would be incorrect.I discovered that optimal thresholds vary by query type:Query typeOptimal thresholdRationaleFAQ-style questions0.94High precision needed; wrong answers damage trustProduct searches0.88More tolerance for near-matchesSupport queries0.92Balance between coverage and accuracyTransactional queries0.97Very low tolerance for errorsI implemented query-type-specific thresholds:class AdaptiveSemanticCache: def __init__(self): self.thresholds = { &#x27;faq&#x27;: 0.94, &#x27;search&#x27;: 0.88, &#x27;support&#x27;: 0.92, &#x27;transactional&#x27;: 0.97, &#x27;default&#x27;: 0.92 } self.query_classifier = QueryClassifier() def get_threshold(self, query: str) -> float: query_type = self.query_classifier.classify(query) return self.thresholds.get(query_type, self.thresholds[&#x27;default&#x27;]) def get(self, query: str) -> Optional[str]: threshold = self.get_threshold(query) query_embedding = self.embedding_model.encode(query) matches = self.vector_store.search(query_embedding, top_k=1) if matches and matches[0].similarity >= threshold: return self.response_store.get(matches[0].id) return NoneThreshold tuning methodologyI couldn&#x27;t tune thresholds blindly. I needed ground truth on which query pairs were actually \"the same.\"Our methodology:Step 1: Sample query pairs. I sampled 5,000 query pairs at various similarity levels (0.80-0.99).Step 2: Human labeling. Annotators labeled each pair as \"same intent\" or \"different intent.\" I used three annotators per pair and took a majority vote.Step 3: Compute precision/recall curves. For each threshold, we computed:Precision: Of cache hits, what fraction had the same intent?Recall: Of same-intent pairs, what fraction did we cache-hit?def compute_precision_recall(pairs, labels, threshold): \"\"\"Compute precision and recall at given similarity threshold.\"\"\" predictions = [1 if pair.similarity >= threshold else 0 for pair in pairs] true_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 1) false_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 0) false_negatives = sum(1 for p, l in zip(predictions, labels) if p == 0 and l == 1) precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0 recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0 return precision, recallStep 4: Select threshold based on cost of errors. For FAQ queries where wrong answers damage trust, I optimized for precision (0.94 threshold gave 98% precision). For search queries where missing a cache hit just costs money, I optimized for recall (0.88 threshold).Latency overheadSemantic caching adds latency: You must embed the query and search the vector store before knowing whether to call the LLM.Our measurements:OperationLatency (p50)Latency (p99)Query embedding12ms28msVector search8ms19msTotal cache lookup20ms47msLLM API call850ms2400msThe 20ms overhead is negligible compared to the 850ms LLM call we avoid on cache hits. Even at p99, the 47ms overhead is acceptable.However, cache misses now take 20ms longer than before (embedding + search + LLM call). At our 67% hit rate, the math works out favorably:Before: 100% of queries × 850ms = 850ms averageAfter: (33% × 870ms) + (67% × 20ms) = 287ms + 13ms = 300ms averageNet latency improvement of 65% alongside the cost reduction.Cache invalidationCached responses go stale. Product information changes, policies update and yesterday&#x27;s correct answer becomes today&#x27;s wrong answer.I implemented three invalidation strategies:Time-based TTLSimple expiration based on content type:TTL_BY_CONTENT_TYPE = { &#x27;pricing&#x27;: timedelta(hours=4), # Changes frequently &#x27;policy&#x27;: timedelta(days=7), # Changes rarely &#x27;product_info&#x27;: timedelta(days=1), # Daily refresh &#x27;general_faq&#x27;: timedelta(days=14), # Very stable}Event-based invalidationWhen underlying data changes, invalidate related cache entries:class CacheInvalidator: def on_content_update(self, content_id: str, content_type: str): \"\"\"Invalidate cache entries related to updated content.\"\"\" # Find cached queries that referenced this content affected_queries = self.find_queries_referencing(content_id) for query_id in affected_queries: self.cache.invalidate(query_id) self.log_invalidation(content_id, len(affected_queries))Staleness detectionFor responses that might become stale without explicit events, I implemented periodic freshness checks:def check_freshness(self, cached_response: dict) -> bool: \"\"\"Verify cached response is still valid.\"\"\" # Re-run the query against current data fresh_response = self.generate_response(cached_response[&#x27;query&#x27;]) # Compare semantic similarity of responses cached_embedding = self.embed(cached_response[&#x27;response&#x27;]) fresh_embedding = self.embed(fresh_response) similarity = cosine_similarity(cached_embedding, fresh_embedding) # If responses diverged significantly, invalidate if similarity < 0.90: self.cache.invalidate(cached_response[&#x27;id&#x27;]) return False return TrueWe run freshness checks on a sample of cached entries daily, catching staleness that TTL and event-based invalidation miss.Production resultsAfter three months in production:MetricBeforeAfterChangeCache hit rate18%67%+272%LLM API costs$47K/month$12.7K/month-73%Average latency850ms300ms-65%False-positive rateN/A0.8%—Customer complaints (wrong answers)Baseline+0.3%Minimal increaseThe 0.8% false-positive rate (queries where we returned a cached response that was semantically incorrect) was within acceptable bounds. These cases occurred primarily at the boundaries of our threshold, where similarity was just above the cutoff but intent differed slightly.Pitfalls to avoidDon&#x27;t use a single global threshold. Different query types have different tolerance for errors. Tune thresholds per category.Don&#x27;t skip the embedding step on cache hits. You might be tempted to skip embedding overhead when returning cached responses, but you need the embedding for cache key generation. The overhead is unavoidable.Don&#x27;t forget invalidation. Semantic caching without invalidation strategy leads to stale responses that erode user trust. Build invalidation from day one.Don&#x27;t cache everything. Some queries shouldn&#x27;t be cached: Personalized responses, time-sensitive information, transactional confirmations. Build exclusion rules.def should_cache(self, query: str, response: str) -> bool: \"\"\"Determine if response should be cached.\"\" # Don&#x27;t cache personalized responses if self.contains_personal_info(response): return False # Don&#x27;t cache time-sensitive information if self.is_time_sensitive(query): return False # Don&#x27;t cache transactional confirmations if self.is_transactional(query): return False return TrueKey takeawaysSemantic caching is a practical pattern for LLM cost control that captures redundancy exact-match caching misses. The key challenges are threshold tuning (use query-type-specific thresholds based on precision/recall analysis) and cache invalidation (combine TTL, event-based and staleness detection).At 73% cost reduction, this was our highest-ROI optimization for production LLM systems. The implementation complexity is moderate, but the threshold tuning requires careful attention to avoid quality degradation.Sreenivasa Reddy Hulebeedu Reddy is a lead software engineer.",
          "feed_position": 0,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/7iyQoeSwdOqqpfcE0PFWgF/48db7d0305019eee107028d9f018d2ac/Semantic_caching.png?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/security/nvidia-rubin-rack-scale-encryption-enterprise-ai-security",
          "published_at": "Mon, 12 Jan 2026 16:00:00 GMT",
          "title": "Nvidia Rubin's rack-scale encryption signals a turning point for enterprise AI security",
          "standfirst": "Nvidia&#x27;s Vera Rubin NVL72, announced at CES 2026, encrypts every bus across 72 GPUs, 36 CPUs, and the entire NVLink fabric. It&#x27;s the first rack-scale platform to deliver confidential computing across CPU, GPU, and NVLink domains.For security leaders, this fundamentally shifts the conversation. Rather than attempting to secure complex hybrid cloud configurations through contractual trust with cloud providers, they can verify them cryptographically. That’s a critical distinction that matters when nation-state adversaries have proven they are capable of launching targeted cyberattacks at machine speed.The brutal economics of unprotected AIEpoch AI research shows frontier training costs have grown at 2.4x annually since 2016, which means billion-dollar training runs could be a reality within a few short years. Yet the infrastructure protecting these investments remains fundamentally insecure in most deployments. Security budgets created to protect frontier training models aren&#x27;t keeping up with the exceptionally fast pace of model training. The result is that more models are under threat as existing approaches can&#x27;t scale and keep up with adversaries&#x27; tradecraft.IBM&#x27;s 2025 Cost of Data Breach Report found that 13% of organizations experienced breaches of AI models or applications. Among those breached, 97% lacked proper AI access controls. Shadow AI incidents cost $4.63 million on average, or $670,000 more than standard breaches, with one in five breaches now involving unsanctioned tools that disproportionately expose customer PII (65%) and intellectual property (40%).Think about what this means for organizations spending $50 million or $500 million on a training run. Their model weights sit in multi-tenant environments where cloud providers can inspect the data. Hardware-level encryption that proves the environment hasn&#x27;t been tampered with changes that financial equation entirely.The GTG-1002 wake-up callIn November 2025, Anthropic disclosed something unprecedented: A Chinese state-sponsored group designated GTG-1002 had manipulated Claude Code to conduct what the company described as the first documented case of a large-scale cyberattack executed without substantial human intervention.State-sponsored adversaries turned it into an autonomous intrusion agent that discovered vulnerabilities, crafted exploits, harvested credentials, moved laterally through networks, and categorized stolen data by intelligence value. Human operators stepped in only at critical junctures. According to Anthropic&#x27;s analysis, the AI executed around 80 to 90% of all tactical work independently.The implications extend beyond this single incident. Attack surfaces that once required teams of experienced attackers can now be probed at machine speed by opponents with access to foundation models.Comparing the performance of Blackwell vs. RubinSpecificationBlackwell GB300 NVL72Rubin NVL72Inference compute (FP4)1.44 exaFLOPS3.6 exaFLOPSNVFP4 per GPU (inference)20 PFLOPS50 PFLOPSPer-GPU NVLink bandwidth1.8 TB/s3.6 TB/sRack NVLink bandwidth130 TB/s260 TB/sHBM bandwidth per GPU~8 TB/s~22 TB/sIndustry momentum and AMD&#x27;s alternativeNvidia isn&#x27;t operating in isolation. Research from the Confidential Computing Consortium and IDC, released in December, found that 75% of organizations are adopting confidential computing, with 18% already in production and 57% piloting deployments.\"Confidential Computing has grown from a niche concept into a vital strategy for data security and trusted AI innovation,\" said Nelly Porter, governing board chair of the Confidential Computing Consortium. Real barriers remain: attestation validation challenges affect 84% of respondents, and a skills gap hampers 75%.AMD&#x27;s Helios rack takes a different approach. Built on Meta&#x27;s Open Rack Wide specification, announced at OCP Global Summit in October 2025, it delivers approximately 2.9 exaflops of FP4 compute with 31 TB of HBM4 memory and 1.4 PB/s aggregate bandwidth. Where Nvidia designs confidential computing into every component, AMD prioritizes open standards through the Ultra Accelerator Link and Ultra Ethernet consortia. The competition between Nvidia and AMD is giving security leaders more of a choice than they otherwise would have had. Comparing the tradeoffs of Nvidia&#x27;s integrated approach versus AMD&#x27;s open-standards flexibility for their specific infrastructures and business-specific threat models is key.What security leaders are doing nowHardware-level confidentiality doesn&#x27;t replace zero-trust principles; it gives them teeth. What Nvidia and AMD are building lets security leaders verify trust cryptographically rather than assume it contractually. That&#x27;s a meaningful shift for anyone running sensitive workloads on shared infrastructure. And if the attestation claims hold up in production, this approach could let enterprises extend zero-trust enforcement across thousands of nodes without the policy sprawl and agent overhead that software-only implementations require.Before deployment: Verify attestation to confirm environments haven&#x27;t been tampered with. Cryptographic proof of compliance should be a prerequisite for signing contracts, not an afterthought or worse, a nice-to-have. If your cloud provider can&#x27;t demonstrate attestation capabilities, that&#x27;s a question worth raising in your next QBR.During operation: Maintain separate enclaves for training and inference, and include security teams in the model pipeline from the very start. IBM&#x27;s research showed 63% of breached organizations had no AI governance policy. You can&#x27;t bolt security on after development; that translates into an onramp for mediocre security design-ins and lengthy red teaming that catches bugs that needed to be engineered out of a model or app early.Across the organization: Run joint exercises between security and data science teams to surface vulnerabilities before attackers find them. Shadow AI accounted for 20% of breaches and exposed customer PII and IP at higher rates than other breach types.Bottom line The GTG-1002 campaign demonstrated that adversaries can now automate large-scale intrusions with minimal human oversight at scale. Nearly every organization that experienced an AI-related breach lacked proper access controls.Nvidia&#x27;s Vera Rubin NVL72 transforms racks from potential liabilities into cryptographically attested assets by encrypting every bus. AMD&#x27;s Helios offers an open-standards alternative. Hardware confidentiality alone won&#x27;t stop a determined adversary, but combined with strong governance and realistic threat exercises, rack-scale encryption gives security leaders the foundation they need to protect investments measured in hundreds of millions of dollars.The question facing CISOs isn&#x27;t whether attested infrastructure is worth it. It&#x27;s whether organizations building high-value AI models can afford to operate without it.",
          "content": "Nvidia&#x27;s Vera Rubin NVL72, announced at CES 2026, encrypts every bus across 72 GPUs, 36 CPUs, and the entire NVLink fabric. It&#x27;s the first rack-scale platform to deliver confidential computing across CPU, GPU, and NVLink domains.For security leaders, this fundamentally shifts the conversation. Rather than attempting to secure complex hybrid cloud configurations through contractual trust with cloud providers, they can verify them cryptographically. That’s a critical distinction that matters when nation-state adversaries have proven they are capable of launching targeted cyberattacks at machine speed.The brutal economics of unprotected AIEpoch AI research shows frontier training costs have grown at 2.4x annually since 2016, which means billion-dollar training runs could be a reality within a few short years. Yet the infrastructure protecting these investments remains fundamentally insecure in most deployments. Security budgets created to protect frontier training models aren&#x27;t keeping up with the exceptionally fast pace of model training. The result is that more models are under threat as existing approaches can&#x27;t scale and keep up with adversaries&#x27; tradecraft.IBM&#x27;s 2025 Cost of Data Breach Report found that 13% of organizations experienced breaches of AI models or applications. Among those breached, 97% lacked proper AI access controls. Shadow AI incidents cost $4.63 million on average, or $670,000 more than standard breaches, with one in five breaches now involving unsanctioned tools that disproportionately expose customer PII (65%) and intellectual property (40%).Think about what this means for organizations spending $50 million or $500 million on a training run. Their model weights sit in multi-tenant environments where cloud providers can inspect the data. Hardware-level encryption that proves the environment hasn&#x27;t been tampered with changes that financial equation entirely.The GTG-1002 wake-up callIn November 2025, Anthropic disclosed something unprecedented: A Chinese state-sponsored group designated GTG-1002 had manipulated Claude Code to conduct what the company described as the first documented case of a large-scale cyberattack executed without substantial human intervention.State-sponsored adversaries turned it into an autonomous intrusion agent that discovered vulnerabilities, crafted exploits, harvested credentials, moved laterally through networks, and categorized stolen data by intelligence value. Human operators stepped in only at critical junctures. According to Anthropic&#x27;s analysis, the AI executed around 80 to 90% of all tactical work independently.The implications extend beyond this single incident. Attack surfaces that once required teams of experienced attackers can now be probed at machine speed by opponents with access to foundation models.Comparing the performance of Blackwell vs. RubinSpecificationBlackwell GB300 NVL72Rubin NVL72Inference compute (FP4)1.44 exaFLOPS3.6 exaFLOPSNVFP4 per GPU (inference)20 PFLOPS50 PFLOPSPer-GPU NVLink bandwidth1.8 TB/s3.6 TB/sRack NVLink bandwidth130 TB/s260 TB/sHBM bandwidth per GPU~8 TB/s~22 TB/sIndustry momentum and AMD&#x27;s alternativeNvidia isn&#x27;t operating in isolation. Research from the Confidential Computing Consortium and IDC, released in December, found that 75% of organizations are adopting confidential computing, with 18% already in production and 57% piloting deployments.\"Confidential Computing has grown from a niche concept into a vital strategy for data security and trusted AI innovation,\" said Nelly Porter, governing board chair of the Confidential Computing Consortium. Real barriers remain: attestation validation challenges affect 84% of respondents, and a skills gap hampers 75%.AMD&#x27;s Helios rack takes a different approach. Built on Meta&#x27;s Open Rack Wide specification, announced at OCP Global Summit in October 2025, it delivers approximately 2.9 exaflops of FP4 compute with 31 TB of HBM4 memory and 1.4 PB/s aggregate bandwidth. Where Nvidia designs confidential computing into every component, AMD prioritizes open standards through the Ultra Accelerator Link and Ultra Ethernet consortia. The competition between Nvidia and AMD is giving security leaders more of a choice than they otherwise would have had. Comparing the tradeoffs of Nvidia&#x27;s integrated approach versus AMD&#x27;s open-standards flexibility for their specific infrastructures and business-specific threat models is key.What security leaders are doing nowHardware-level confidentiality doesn&#x27;t replace zero-trust principles; it gives them teeth. What Nvidia and AMD are building lets security leaders verify trust cryptographically rather than assume it contractually. That&#x27;s a meaningful shift for anyone running sensitive workloads on shared infrastructure. And if the attestation claims hold up in production, this approach could let enterprises extend zero-trust enforcement across thousands of nodes without the policy sprawl and agent overhead that software-only implementations require.Before deployment: Verify attestation to confirm environments haven&#x27;t been tampered with. Cryptographic proof of compliance should be a prerequisite for signing contracts, not an afterthought or worse, a nice-to-have. If your cloud provider can&#x27;t demonstrate attestation capabilities, that&#x27;s a question worth raising in your next QBR.During operation: Maintain separate enclaves for training and inference, and include security teams in the model pipeline from the very start. IBM&#x27;s research showed 63% of breached organizations had no AI governance policy. You can&#x27;t bolt security on after development; that translates into an onramp for mediocre security design-ins and lengthy red teaming that catches bugs that needed to be engineered out of a model or app early.Across the organization: Run joint exercises between security and data science teams to surface vulnerabilities before attackers find them. Shadow AI accounted for 20% of breaches and exposed customer PII and IP at higher rates than other breach types.Bottom line The GTG-1002 campaign demonstrated that adversaries can now automate large-scale intrusions with minimal human oversight at scale. Nearly every organization that experienced an AI-related breach lacked proper access controls.Nvidia&#x27;s Vera Rubin NVL72 transforms racks from potential liabilities into cryptographically attested assets by encrypting every bus. AMD&#x27;s Helios offers an open-standards alternative. Hardware confidentiality alone won&#x27;t stop a determined adversary, but combined with strong governance and realistic threat exercises, rack-scale encryption gives security leaders the foundation they need to protect investments measured in hundreds of millions of dollars.The question facing CISOs isn&#x27;t whether attested infrastructure is worth it. It&#x27;s whether organizations building high-value AI models can afford to operate without it.",
          "feed_position": 1,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/2YSkElI5OHmat9celGziVv/a23ebd445f5a38cd5062055b9dd3b15e/jenson_at_ces.jpg?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/computing/accessories/best-laptop-power-bank-120040388.html",
          "published_at": "Mon, 12 Jan 2026 10:01:26 +0000",
          "title": "The best laptop power banks for 2026",
          "standfirst": "Laptop power banks come in handy if you often travel or work away from your desk. These larger batteries have enough capacity to refill your computer to around 75 percent, giving you many more productive hours. Of course, they can also charge smaller devices like your phone, camera or tablet a few times over. Nearly all portable laptop chargers fall below the 100-watt-hour mark so you can bring them in your carry-on when you fly. Additional features, like built-in cables and digital displays make these battery banks easier to use, too. We tested over a dozen options and put our thoughts below, so you can find the best laptop power bank for your next trip away from an outlet. Table of contents Best laptop power banks for 2026 What to look for in a laptop power bank How we test portable laptop chargers Other laptop power banks we tested Laptop power bank FAQs Recent updates Best laptop power banks for 2026 What to look for in a laptop power bank Flying with a laptop power bank Most portable batteries top out at around 27,000mAh so you can fly with them. The TSA currently limits the capacity carry-on batteries to 100Wh, which works out to around 27,500mAh for 3.6 volt lithium-ion batteries. Note that you’re not allowed to pack any batteries in your checked luggage, regardless of capacity. The TSA rules are intended to limit fire danger — and some airlines are implementing further restrictions due to recent on-board incidents. In March 2025, a Hong Kong flight was grounded after a battery pack caught fire in an overhead bin. A similar situation happened that same year in July on a domestic Delta flight, and again in August on a transatlantic KLM flight. As a result, some airlines, including Emirates, Southwest and others have announced further restrictions on flying with battery packs. Rules include limiting the number of allowed portable chargers and requiring flyers to keep power banks in clear view when using them to recharge a device. If the battery pack isn’t actively in use, however, most rules allow them to stay in your carry-on bag in the overhead bin. Before flying, it’s wise to check your airline’s policies. Capacity If you just need to keep a smartphone from dying before you can make it home, just about any of the best power banks will do. But if you need to revive multiple devices or the substantial battery of a laptop, you’ll want something with a high milliamp-hour​​ (mAh) capacity. A power bank capable of delivering enough power to a laptop will have a capacity between 20,000 and 27,000 mAh. If you want something even bigger than a laptop power bank, and don’t need to fly with it, you’ll likely want to look into portable power stations. These can be the size of a car battery or larger and can potentially fuel an entire weekend away. Another thing to keep in mind is that the capacity listed in a power bank's specs is not what will be delivered to your devices. As I mentioned, the capacity of these banks is around 25,000mAh. Even the huge battery on a 16-inch MacBook Pro or a Dell XPS 16 has a mAh rating of around 5,000 - 6,000mAh, so you might think you’d get five full charges but in reality, you only get about a single 70-percent charge. The voltage is different (typically 3.7V for the power bank and 11.4V for a laptop) which makes the watt-hours, or the amount of energy each battery can hold, different (working out to 92Wh for the battery and 72Wh for the built-in laptop batteries). On top of that, in order to feed a charge from a power bank to a laptop, a voltage conversion takes place and that dissipates a decent amount of energy. Without turning this into a physics lesson, this all means that a power bank with a 25,000mAh (or 92Wh) capacity will typically fill a 5,000mAh (or 72Wh) laptop battery to about 75 percent. In my tests, I averaged about a 60-percent efficiency rate between a power bank’s listed capacity and the actual charge delivered. Ports Every large power bank I’ve tested has at least three USB ports, with a mix of USB-C and USB-A, which should cover nearly any portable device you need to recharge — earbuds, phones, tablets, laptops, you name it. In addition to the different plug formats, some ports supply power at different wattages. For example, one built-in USB-C port might be rated for 60 watts, while the one next to it is rated for 100 watts. So if you’ve got a device that’s capable of 70W fast charging, such as the new MacBook Air, you’d want to opt for the 100W port to get the best charging speeds possible. Note that devices with a smaller wattage draw won’t be negatively affected by connecting to ports with high ratings. For example, a Galaxy S24 Ultra, capable of 45W super fast charging, is perfectly compatible with the 100W port. A device will only draw what it can take, regardless of what a port can supply. Just remember that the port, device and charging cable need to be at or above the desired wattage rating to achieve maximum charging rates. Some of these larger batteries also have AC ports. It might seem like a natural fit to plug in your laptop’s power adapter for a recharge. But really, the AC port should only be for devices that can’t use USB — such as a lamp or a printer. Plugging a power adapter into the AC port only wastes energy through conversion. First, the battery converts its DC power to supply the port with AC power, then the power adapter converts that AC power back to DC so your laptop can take it in. And as you’ll remember from physics class, each time energy is converted, some is lost to heat and other dissipations. Better to cut out the middleman and just send that DC power straight from the battery to the device. Also, you can use more than one port at a time with these devices; just remember that the speed of whatever you’re charging will likely go down, and of course, the battery is going to drain proportionally to what you’re refilling. Wireless charging Since I first started testing portable power banks a few years ago, wireless charging capabilities have noticeably improved. The first few I tried were painfully slow and not worth recommending. Now the wireless pads built into power banks are impressively fast — particularly, in my experience, when charging Samsung Galaxy phones (though the lack of a stabilizing magnetic connection like Apple’s MagSafe means they only work when rested flat on a pad). Most wireless charging connections can be used while other ports are also being employed, making them convenient for some mobile battlestation setups. Of course, wireless charging is always less efficient than wired, and recharging from an external battery is less efficient in general. If you want to waste as little energy as possible, you’re better off sticking to wired connections. Design All power banks are designed to be portable, but there’s a big difference between a pocket-friendly 5,000mAh battery and one of these laptop-compatible bruisers. Most of the latter weigh between a pound and a half to two pounds, which is a considerable addition to a backpack. Many of the options listed here have a display to tell you how much charge remains in the battery, which is helpful when you’re trying to judiciously meet out charges to your devices. If a bank has a wireless connection, the pad is usually on the flat top and any available AC connection is usually at one end. Both may require you to engage those charging methods. Don’t be like me and grumble loudly that you got a bum unit without pressing (and sometimes double pressing) all the buttons first. How we test portable laptop chargers For the past three years, I’ve been testing and using dozens of portable batteries for our other battery guide. Some of those batteries include the higher-capacity power banks you see here. I also got a hold of a few extra banks just for this guide to make sure we covered what’s available. I went for brands I’m already familiar with, as well as battery packs from well-received manufacturers I hadn’t tried before (like UGREEN and Lion Energy). I only considered banks with at least a 20,000mAh capacity and mostly stuck with those that rated 25,000mAh and higher. Here’s everything we tested: Zendure Supertank Pro Mophie Powerstation Pro XL Mophie Powerstation Pro AC Lion Energy Eclipse Mag Lion Energy Trek Baseus Blade Laptop Anker Prime 27,650mAh Goal Zero Sherpa 100 AC Anker Retractable Cable Laptop Bank HyperJuice 245W Anker Prime Power Bank (26K, 300W) UGreen Power Bank 25,000mAh 145W I tested each power bank with an Apple phone (iPhone 15 or 16), an Android phone (Galaxy S23 Ultra), a tablet (M1 iPad Air) and a laptop (16-inch MacBook Pro with the M1 Pro chip). Even though these banks can charge multiple devices at once, I refilled one at a time, to make side-by-side comparisons more straightforward. I drained the batteries of the phones and tablets to between zero and five percent and then didn’t use any device as it refilled. For the MacBook, I let it run down to 10 percent before plugging in the power bank. That's when most laptops give display a “connect to power” warning, as draining any battery to empty will compromise the battery life. I then used it as one might in a mobile office, with a Bluetooth keyboard and mouse, while connected to Wi-Fi and a VPN. For each test, I noted how long a completely charged battery took to get a device back to full and how much of the battery’s capacity was used up in one charge. I also noted things like portability, apparent durability, helpful features and overall design. For reference, here are the battery capacities of the devices I used: iPhone 15: 3,349mAh Galaxy S23 Ultra: 4,855mAh iPad Air (5th gen): 7,729mAh 16-inch M1 Pro MacBook Pro: 27,027mAh Other laptop power banks we tested HyperJuice 245W Hyper’s HyperJuice 245W brick looks great and has a hefty 27,000mAh capacity. The four USB-C ports can combine to output 245W of power and it got my MacBook Pro from nearly dead to 75 percent before depleting itself. When testing it with a Samsung Galaxy S23 Ultra, the handset got back up to a full charge in just over an hour. The screen tells you what each port is doing as well as displaying the amount of charge remaining in the pack itself. But the lack of port variety makes it feel less versatile than other picks on this list — the price is higher than our other options, too. Laptop power bank FAQs How do laptop power banks differ from phone power banks? The main difference is size. Phone power banks tend to have a capacity ranging from 5,000mAh to 20,000mAh and laptop powerbanks are typically rated between 20,000mAh and 27,000mAh. There’s no official definition, however. Laptop batteries are simply larger and need a bigger supply of power to give them a meaningful charge. How do you fast charge a power bank? You can charge a power bank exactly as fast as the power bank’s internal mechanisms will allow. Most batteries are limited in how quickly they can accept and deliver a charge to avoid dangerously overheating. But to make sure you’re charging a bank as quickly as possible, make sure the wall adapter and the USB-C cable you are using have a high wattage rating — using a 5W power brick and a 10W cable will take a lot longer to refill your bank than a 65W wall charger and a 100W cord. What size power bank do I need for a laptop? Look for a power bank with a rating of at least 20,000mAh. Slightly smaller batteries may work, but they won’t deliver a significant charge laptops. How many mAh to charge a laptop? A milliamp hour (mAh) is how much a battery can hold, and most portable batteries list their capacity using mAh. If you get a battery rated at 20,000mAh or above, it should be able to charge your laptop. Using mAh to discuss laptop batteries can be confusing. Due to differing voltages, you can’t directly compare the mAh ratings of a power bank battery to a laptop battery. Using watt-hours is a better gauge, as that calculation takes voltage into account. Recent updates November 2025: Updated our overall top pick to the Anker Laptop Power bank. Added a premium power bank pick. This article originally appeared on Engadget at https://www.engadget.com/computing/accessories/best-laptop-power-bank-120040388.html?src=rss",
          "content": "Laptop power banks come in handy if you often travel or work away from your desk. These larger batteries have enough capacity to refill your computer to around 75 percent, giving you many more productive hours. Of course, they can also charge smaller devices like your phone, camera or tablet a few times over. Nearly all portable laptop chargers fall below the 100-watt-hour mark so you can bring them in your carry-on when you fly. Additional features, like built-in cables and digital displays make these battery banks easier to use, too. We tested over a dozen options and put our thoughts below, so you can find the best laptop power bank for your next trip away from an outlet. Table of contents Best laptop power banks for 2026 What to look for in a laptop power bank How we test portable laptop chargers Other laptop power banks we tested Laptop power bank FAQs Recent updates Best laptop power banks for 2026 What to look for in a laptop power bank Flying with a laptop power bank Most portable batteries top out at around 27,000mAh so you can fly with them. The TSA currently limits the capacity carry-on batteries to 100Wh, which works out to around 27,500mAh for 3.6 volt lithium-ion batteries. Note that you’re not allowed to pack any batteries in your checked luggage, regardless of capacity. The TSA rules are intended to limit fire danger — and some airlines are implementing further restrictions due to recent on-board incidents. In March 2025, a Hong Kong flight was grounded after a battery pack caught fire in an overhead bin. A similar situation happened that same year in July on a domestic Delta flight, and again in August on a transatlantic KLM flight. As a result, some airlines, including Emirates, Southwest and others have announced further restrictions on flying with battery packs. Rules include limiting the number of allowed portable chargers and requiring flyers to keep power banks in clear view when using them to recharge a device. If the battery pack isn’t actively in use, however, most rules allow them to stay in your carry-on bag in the overhead bin. Before flying, it’s wise to check your airline’s policies. Capacity If you just need to keep a smartphone from dying before you can make it home, just about any of the best power banks will do. But if you need to revive multiple devices or the substantial battery of a laptop, you’ll want something with a high milliamp-hour​​ (mAh) capacity. A power bank capable of delivering enough power to a laptop will have a capacity between 20,000 and 27,000 mAh. If you want something even bigger than a laptop power bank, and don’t need to fly with it, you’ll likely want to look into portable power stations. These can be the size of a car battery or larger and can potentially fuel an entire weekend away. Another thing to keep in mind is that the capacity listed in a power bank's specs is not what will be delivered to your devices. As I mentioned, the capacity of these banks is around 25,000mAh. Even the huge battery on a 16-inch MacBook Pro or a Dell XPS 16 has a mAh rating of around 5,000 - 6,000mAh, so you might think you’d get five full charges but in reality, you only get about a single 70-percent charge. The voltage is different (typically 3.7V for the power bank and 11.4V for a laptop) which makes the watt-hours, or the amount of energy each battery can hold, different (working out to 92Wh for the battery and 72Wh for the built-in laptop batteries). On top of that, in order to feed a charge from a power bank to a laptop, a voltage conversion takes place and that dissipates a decent amount of energy. Without turning this into a physics lesson, this all means that a power bank with a 25,000mAh (or 92Wh) capacity will typically fill a 5,000mAh (or 72Wh) laptop battery to about 75 percent. In my tests, I averaged about a 60-percent efficiency rate between a power bank’s listed capacity and the actual charge delivered. Ports Every large power bank I’ve tested has at least three USB ports, with a mix of USB-C and USB-A, which should cover nearly any portable device you need to recharge — earbuds, phones, tablets, laptops, you name it. In addition to the different plug formats, some ports supply power at different wattages. For example, one built-in USB-C port might be rated for 60 watts, while the one next to it is rated for 100 watts. So if you’ve got a device that’s capable of 70W fast charging, such as the new MacBook Air, you’d want to opt for the 100W port to get the best charging speeds possible. Note that devices with a smaller wattage draw won’t be negatively affected by connecting to ports with high ratings. For example, a Galaxy S24 Ultra, capable of 45W super fast charging, is perfectly compatible with the 100W port. A device will only draw what it can take, regardless of what a port can supply. Just remember that the port, device and charging cable need to be at or above the desired wattage rating to achieve maximum charging rates. Some of these larger batteries also have AC ports. It might seem like a natural fit to plug in your laptop’s power adapter for a recharge. But really, the AC port should only be for devices that can’t use USB — such as a lamp or a printer. Plugging a power adapter into the AC port only wastes energy through conversion. First, the battery converts its DC power to supply the port with AC power, then the power adapter converts that AC power back to DC so your laptop can take it in. And as you’ll remember from physics class, each time energy is converted, some is lost to heat and other dissipations. Better to cut out the middleman and just send that DC power straight from the battery to the device. Also, you can use more than one port at a time with these devices; just remember that the speed of whatever you’re charging will likely go down, and of course, the battery is going to drain proportionally to what you’re refilling. Wireless charging Since I first started testing portable power banks a few years ago, wireless charging capabilities have noticeably improved. The first few I tried were painfully slow and not worth recommending. Now the wireless pads built into power banks are impressively fast — particularly, in my experience, when charging Samsung Galaxy phones (though the lack of a stabilizing magnetic connection like Apple’s MagSafe means they only work when rested flat on a pad). Most wireless charging connections can be used while other ports are also being employed, making them convenient for some mobile battlestation setups. Of course, wireless charging is always less efficient than wired, and recharging from an external battery is less efficient in general. If you want to waste as little energy as possible, you’re better off sticking to wired connections. Design All power banks are designed to be portable, but there’s a big difference between a pocket-friendly 5,000mAh battery and one of these laptop-compatible bruisers. Most of the latter weigh between a pound and a half to two pounds, which is a considerable addition to a backpack. Many of the options listed here have a display to tell you how much charge remains in the battery, which is helpful when you’re trying to judiciously meet out charges to your devices. If a bank has a wireless connection, the pad is usually on the flat top and any available AC connection is usually at one end. Both may require you to engage those charging methods. Don’t be like me and grumble loudly that you got a bum unit without pressing (and sometimes double pressing) all the buttons first. How we test portable laptop chargers For the past three years, I’ve been testing and using dozens of portable batteries for our other battery guide. Some of those batteries include the higher-capacity power banks you see here. I also got a hold of a few extra banks just for this guide to make sure we covered what’s available. I went for brands I’m already familiar with, as well as battery packs from well-received manufacturers I hadn’t tried before (like UGREEN and Lion Energy). I only considered banks with at least a 20,000mAh capacity and mostly stuck with those that rated 25,000mAh and higher. Here’s everything we tested: Zendure Supertank Pro Mophie Powerstation Pro XL Mophie Powerstation Pro AC Lion Energy Eclipse Mag Lion Energy Trek Baseus Blade Laptop Anker Prime 27,650mAh Goal Zero Sherpa 100 AC Anker Retractable Cable Laptop Bank HyperJuice 245W Anker Prime Power Bank (26K, 300W) UGreen Power Bank 25,000mAh 145W I tested each power bank with an Apple phone (iPhone 15 or 16), an Android phone (Galaxy S23 Ultra), a tablet (M1 iPad Air) and a laptop (16-inch MacBook Pro with the M1 Pro chip). Even though these banks can charge multiple devices at once, I refilled one at a time, to make side-by-side comparisons more straightforward. I drained the batteries of the phones and tablets to between zero and five percent and then didn’t use any device as it refilled. For the MacBook, I let it run down to 10 percent before plugging in the power bank. That's when most laptops give display a “connect to power” warning, as draining any battery to empty will compromise the battery life. I then used it as one might in a mobile office, with a Bluetooth keyboard and mouse, while connected to Wi-Fi and a VPN. For each test, I noted how long a completely charged battery took to get a device back to full and how much of the battery’s capacity was used up in one charge. I also noted things like portability, apparent durability, helpful features and overall design. For reference, here are the battery capacities of the devices I used: iPhone 15: 3,349mAh Galaxy S23 Ultra: 4,855mAh iPad Air (5th gen): 7,729mAh 16-inch M1 Pro MacBook Pro: 27,027mAh Other laptop power banks we tested HyperJuice 245W Hyper’s HyperJuice 245W brick looks great and has a hefty 27,000mAh capacity. The four USB-C ports can combine to output 245W of power and it got my MacBook Pro from nearly dead to 75 percent before depleting itself. When testing it with a Samsung Galaxy S23 Ultra, the handset got back up to a full charge in just over an hour. The screen tells you what each port is doing as well as displaying the amount of charge remaining in the pack itself. But the lack of port variety makes it feel less versatile than other picks on this list — the price is higher than our other options, too. Laptop power bank FAQs How do laptop power banks differ from phone power banks? The main difference is size. Phone power banks tend to have a capacity ranging from 5,000mAh to 20,000mAh and laptop powerbanks are typically rated between 20,000mAh and 27,000mAh. There’s no official definition, however. Laptop batteries are simply larger and need a bigger supply of power to give them a meaningful charge. How do you fast charge a power bank? You can charge a power bank exactly as fast as the power bank’s internal mechanisms will allow. Most batteries are limited in how quickly they can accept and deliver a charge to avoid dangerously overheating. But to make sure you’re charging a bank as quickly as possible, make sure the wall adapter and the USB-C cable you are using have a high wattage rating — using a 5W power brick and a 10W cable will take a lot longer to refill your bank than a 65W wall charger and a 100W cord. What size power bank do I need for a laptop? Look for a power bank with a rating of at least 20,000mAh. Slightly smaller batteries may work, but they won’t deliver a significant charge laptops. How many mAh to charge a laptop? A milliamp hour (mAh) is how much a battery can hold, and most portable batteries list their capacity using mAh. If you get a battery rated at 20,000mAh or above, it should be able to charge your laptop. Using mAh to discuss laptop batteries can be confusing. Due to differing voltages, you can’t directly compare the mAh ratings of a power bank battery to a laptop battery. Using watt-hours is a better gauge, as that calculation takes voltage into account. Recent updates November 2025: Updated our overall top pick to the Anker Laptop Power bank. Added a premium power bank pick. This article originally appeared on Engadget at https://www.engadget.com/computing/accessories/best-laptop-power-bank-120040388.html?src=rss",
          "feed_position": 13
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/infrastructure/how-doordash-scaled-without-a-costly-erp-overhaul",
          "published_at": "Mon, 12 Jan 2026 05:00:00 GMT",
          "title": "How DoorDash scaled without a costly ERP overhaul",
          "standfirst": "Presented by NetSuiteMost companies racing from startup to an industry leader face a choice: limp along with scrappy early systems or endure a costly platform migration.DoorDash did neither. The local-commerce giant scaled from its 2013 founding through IPO and global expansion — acquiring the Helsiniki-based technology company Wolt in 2022 and UK-based Deliveroo in 2025 — while keeping its original Oracle NetSuite business system. Today, it serves over 50 million consumers in more than 40 countries.*Chief Accounting Officer Gordon Lee says the secret is building a scalable ecosystem that allows teams to use tools that work best for them.Choosing flexibility over uniformityWhen DoorDash selected NetSuite as its corporate financial control center, it wasn&#x27;t looking for a system to enforce uniformity. It sought a scalable platform that could connect all its systems, from ERP, CRM, HR, sourcing, and more. \"Our philosophy has been to create a platform that allows our customers and business partners to use whatever tools work best for them,\" Lee says. \"When we&#x27;re managing growth, the majority of the conversation is about managing expectations — what people expect when you grow from A to B.\"The migration questionTwo years after its founding, DoorDash surpassed one million deliveries and expanded into Canada. As the company scaled, Lee faced growing pressure from vendors insisting that rapid growth required a new enterprise platform.He ran the numbers. The move to another platform could cost millions and consume months of his team&#x27;s focus.Instead, DoorDash stayed with NetSuite, which continued to scale alongside the company’s growth. Built on Oracle Cloud Infrastructure, NetSuite delivers the performance and reliability of an enterprise platform without the cost or disruption of migration. Lee concluded: \"Why do I bother to move? I already have the scalability I need from NetSuite.\"Today, DoorDash’s NetSuite backend provides enterprise-grade security while its familiar front end provides the team flexibility, creating a stable, modern foundation for sustained, high-velocity growth.Expanding the menu without the technical indigestionThat flexibility soon proved invaluable. The ability to add new applications quickly — without long, costly integrations — became a major advantage during hypergrowth.For example, as DoorDash expanded from restaurant delivery into grocery, convenience, and retail, Lee turned to NetSuite’s inventory modules to handle the distinct demands of those new categories.“The flexibility to have and not have, and turn the switch on and off, is easy because it’s all integrated,” he explains.Today, DoorDash’s technology stack spans multiple systems — all integrating seamlessly with NetSuite as the financial hub. “They do it, and you’re done,” Lee says.Embedding expertise to scale smarter, not biggerFor Lee, true partnerships turn vendors into part of the team — and that’s exactly how he describes NetSuite Advanced Customer Support (ACS).\"They are here with us every week. They know all my schematics, they know all my data infrastructure, they know all my database structure within NetSuite. Essentially, they are an extension of my team,\" Lee explains.Close collaboration benefits both parties. DoorDash keeps NetSuite attuned to the realities of hypergrowth and gets instant feedback on technology capability and scalability. In turn, NetSuite stays close to a marquee customer. Interaction is ongoing — and frank, according to Lee. “We work directly with NetSuite ACS and often ask, &#x27;Can NetSuite do this?’ If they can prove it can, we stay with NetSuite.\"Another benefit is the ability to extend DoorDash&#x27;s expertise without expanding headcount. \"If someone says to me, &#x27;Gordon, you&#x27;re just an accountant. How do you know about systems? I say, I don&#x27;t. I have a network guy with us, an expert.’ That&#x27;s the kind of partner I want to surround myself with, so that I can grow beyond what I am.”By embedding expertise within our partnerships, DoorDash scales with precision and control. Lee says the model applies to other companies preparing for IPOs or global expansion. He adds that sustainable growth depends as much on shared understanding as on technology itself. Too often, finance and IT “look at the same requirement but see completely different things,” Lee says, describing what he calls the “blue versus purple” problem. “The accountant doesn’t understand the configuration of the system,” he explains. “The IT guy doesn’t understand what the accountant was trying to tell them.”NetSuite bridges that gap. With a unified data model and built-in best practices across finance, operations, and more, it keeps teams aligned and information consistent. That close collaboration, Lee notes, is what keeps rollouts smooth, data clean, and growth sustainable at any stage.AI strategy: Trust only internal data, get data ducks in a rowLee plans to test the NetSuite AI Connector Service — which supports Model Context Protocol (MCP) and lets customers connect their own AI to NetSuite — to see how faster access to accurate data can drive growth.By implementing an internal instance, Lee is less worried about disruptive errors from LLMs trained on public data sources. \"Think about a generative AI chatbot. When you ask a question, it can reflect many perspectives,” he explains. On the other hand, a chatbot trained on private enterprise systems benefits from “a clean data infrastructure.”Lee is taking a methodical approach: first get data pristine, then train AI on domain-specific terminology, and finally see how internal AI can both find the right information and automate downstream accounting processes to save resources and accelerate growth.Betting long-term on its original financial coreFrom early growth to major acquisitions that helped expand its footprint across the globe, DoorDash has relied on NetSuite as a consistent foundation for innovation and scale.Lee credits NetSuite’s flexible architecture and close partnership with helping enable DoorDash as it continued to scale and cement itself as a leader in local commerce globally.His mantra is simple: “Focus on growth instead of churning through vendors.”* Based on the combined numbers for DoorDash, Wolt, and Deliveroo, measured as of September 2025.Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.",
          "content": "Presented by NetSuiteMost companies racing from startup to an industry leader face a choice: limp along with scrappy early systems or endure a costly platform migration.DoorDash did neither. The local-commerce giant scaled from its 2013 founding through IPO and global expansion — acquiring the Helsiniki-based technology company Wolt in 2022 and UK-based Deliveroo in 2025 — while keeping its original Oracle NetSuite business system. Today, it serves over 50 million consumers in more than 40 countries.*Chief Accounting Officer Gordon Lee says the secret is building a scalable ecosystem that allows teams to use tools that work best for them.Choosing flexibility over uniformityWhen DoorDash selected NetSuite as its corporate financial control center, it wasn&#x27;t looking for a system to enforce uniformity. It sought a scalable platform that could connect all its systems, from ERP, CRM, HR, sourcing, and more. \"Our philosophy has been to create a platform that allows our customers and business partners to use whatever tools work best for them,\" Lee says. \"When we&#x27;re managing growth, the majority of the conversation is about managing expectations — what people expect when you grow from A to B.\"The migration questionTwo years after its founding, DoorDash surpassed one million deliveries and expanded into Canada. As the company scaled, Lee faced growing pressure from vendors insisting that rapid growth required a new enterprise platform.He ran the numbers. The move to another platform could cost millions and consume months of his team&#x27;s focus.Instead, DoorDash stayed with NetSuite, which continued to scale alongside the company’s growth. Built on Oracle Cloud Infrastructure, NetSuite delivers the performance and reliability of an enterprise platform without the cost or disruption of migration. Lee concluded: \"Why do I bother to move? I already have the scalability I need from NetSuite.\"Today, DoorDash’s NetSuite backend provides enterprise-grade security while its familiar front end provides the team flexibility, creating a stable, modern foundation for sustained, high-velocity growth.Expanding the menu without the technical indigestionThat flexibility soon proved invaluable. The ability to add new applications quickly — without long, costly integrations — became a major advantage during hypergrowth.For example, as DoorDash expanded from restaurant delivery into grocery, convenience, and retail, Lee turned to NetSuite’s inventory modules to handle the distinct demands of those new categories.“The flexibility to have and not have, and turn the switch on and off, is easy because it’s all integrated,” he explains.Today, DoorDash’s technology stack spans multiple systems — all integrating seamlessly with NetSuite as the financial hub. “They do it, and you’re done,” Lee says.Embedding expertise to scale smarter, not biggerFor Lee, true partnerships turn vendors into part of the team — and that’s exactly how he describes NetSuite Advanced Customer Support (ACS).\"They are here with us every week. They know all my schematics, they know all my data infrastructure, they know all my database structure within NetSuite. Essentially, they are an extension of my team,\" Lee explains.Close collaboration benefits both parties. DoorDash keeps NetSuite attuned to the realities of hypergrowth and gets instant feedback on technology capability and scalability. In turn, NetSuite stays close to a marquee customer. Interaction is ongoing — and frank, according to Lee. “We work directly with NetSuite ACS and often ask, &#x27;Can NetSuite do this?’ If they can prove it can, we stay with NetSuite.\"Another benefit is the ability to extend DoorDash&#x27;s expertise without expanding headcount. \"If someone says to me, &#x27;Gordon, you&#x27;re just an accountant. How do you know about systems? I say, I don&#x27;t. I have a network guy with us, an expert.’ That&#x27;s the kind of partner I want to surround myself with, so that I can grow beyond what I am.”By embedding expertise within our partnerships, DoorDash scales with precision and control. Lee says the model applies to other companies preparing for IPOs or global expansion. He adds that sustainable growth depends as much on shared understanding as on technology itself. Too often, finance and IT “look at the same requirement but see completely different things,” Lee says, describing what he calls the “blue versus purple” problem. “The accountant doesn’t understand the configuration of the system,” he explains. “The IT guy doesn’t understand what the accountant was trying to tell them.”NetSuite bridges that gap. With a unified data model and built-in best practices across finance, operations, and more, it keeps teams aligned and information consistent. That close collaboration, Lee notes, is what keeps rollouts smooth, data clean, and growth sustainable at any stage.AI strategy: Trust only internal data, get data ducks in a rowLee plans to test the NetSuite AI Connector Service — which supports Model Context Protocol (MCP) and lets customers connect their own AI to NetSuite — to see how faster access to accurate data can drive growth.By implementing an internal instance, Lee is less worried about disruptive errors from LLMs trained on public data sources. \"Think about a generative AI chatbot. When you ask a question, it can reflect many perspectives,” he explains. On the other hand, a chatbot trained on private enterprise systems benefits from “a clean data infrastructure.”Lee is taking a methodical approach: first get data pristine, then train AI on domain-specific terminology, and finally see how internal AI can both find the right information and automate downstream accounting processes to save resources and accelerate growth.Betting long-term on its original financial coreFrom early growth to major acquisitions that helped expand its footprint across the globe, DoorDash has relied on NetSuite as a consistent foundation for innovation and scale.Lee credits NetSuite’s flexible architecture and close partnership with helping enable DoorDash as it continued to scale and cement itself as a leader in local commerce globally.His mantra is simple: “Focus on growth instead of churning through vendors.”* Based on the combined numbers for DoorDash, Wolt, and Deliveroo, measured as of September 2025.Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.",
          "feed_position": 2,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/6ZnGG4zYxTpmnj2txjQwJD/9b3b125e58398fe9ab2b4865aade5e62/AdobeStock_792967914_Editorial_Use_Only.jpeg?w=300&q=30"
        }
      ],
      "featured_image": "https://images.ctfassets.net/jdtwqhzvc2n1/7iyQoeSwdOqqpfcE0PFWgF/48db7d0305019eee107028d9f018d2ac/Semantic_caching.png?w=300&q=30",
      "popularity_score": 2019.655026388889
    },
    {
      "id": "cluster_84",
      "coverage": 2,
      "updated_at": "Sun, 11 Jan 2026 17:56:41 +0000",
      "title": "Google removes AI Overviews for certain medical queries",
      "neutral_headline": "Google removes AI Overviews for certain medical queries",
      "items": [
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2026/01/11/google-removes-ai-overviews-for-certain-medical-queries/",
          "published_at": "Sun, 11 Jan 2026 17:56:41 +0000",
          "title": "Google removes AI Overviews for certain medical queries",
          "standfirst": "This follows an investigation by The Guardian that found Google AI Overviews offering misleading information in response to some health-related queries.",
          "content": "This follows an investigation by The Guardian that found Google AI Overviews offering misleading information in response to some health-related queries.",
          "feed_position": 15
        },
        {
          "source": "Guardian Tech",
          "url": "https://www.theguardian.com/technology/2026/jan/11/google-ai-overviews-health-guardian-investigation",
          "published_at": "Sun, 11 Jan 2026 07:00:19 GMT",
          "title": "‘Dangerous and alarming’: Google removes some of its AI summaries after users’ health put at risk",
          "standfirst": "Exclusive: Guardian investigation finds AI Overviews provided inaccurate and false information when queried over blood testsGoogle has removed some of its artificial intelligence health summaries after a Guardian investigation found people were being put at risk of harm by false and misleading information.The company has said its AI Overviews, which use generative AI to provide snapshots of essential information about a topic or question, are “helpful” and “reliable”. Continue reading...",
          "content": "Exclusive: Guardian investigation finds AI Overviews provided inaccurate and false information when queried over blood testsGoogle has removed some of its artificial intelligence health summaries after a Guardian investigation found people were being put at risk of harm by false and misleading information.The company has said its AI Overviews, which use generative AI to provide snapshots of essential information about a topic or question, are “helpful” and “reliable”. Continue reading...",
          "feed_position": 2
        }
      ],
      "popularity_score": 2000
    },
    {
      "id": "cluster_16",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 17:57:32 +0000",
      "title": "Apple chooses Google’s Gemini over OpenAI’s ChatGPT to power next-gen Siri",
      "neutral_headline": "Apple chooses Google’s Gemini over OpenAI’s ChatGPT to power next-gen Siri",
      "items": [
        {
          "source": "Ars Technica Main",
          "url": "https://arstechnica.com/apple/2026/01/apple-says-its-new-ai-powered-siri-will-use-googles-gemini-language-models/",
          "published_at": "Mon, 12 Jan 2026 17:57:32 +0000",
          "title": "Apple chooses Google’s Gemini over OpenAI’s ChatGPT to power next-gen Siri",
          "standfirst": "Apple goes with Google's tech despite using OpenAI's ChatGPT elsewhere in iOS.",
          "content": "The \"more intelligent\" version of Siri that Apple plans to release later this year will be backed by Google's Gemini language models, the company announced today. CNBC reports that the deal is part of a \"multi-year partnership\" between Apple and Google that will allow Apple to use Google's AI models in its own software. \"After careful evaluation, we determined that Google’s technology provides the most capable foundation for Apple Foundation Models and we’re excited about the innovative new experiences it will unlock for our users,” reads an Apple statement given to CNBC. Today's announcement confirms reporting by Bloomberg's Mark Gurman late last year that Apple and Google were nearing a deal. Apple didn't disclose terms, but Gurman said that Apple would be paying Google \"about $1 billion a year\" for access to its AI models \"following an extensive evaluation period.\"Read full article Comments",
          "feed_position": 0,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/apple_google_hero_3-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/apple_google_hero_3-1152x648.jpg",
      "popularity_score": 378.6139152777778
    },
    {
      "id": "cluster_26",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 17:08:56 +0000",
      "title": "Is this the beginning of the end for GameStop?",
      "neutral_headline": "Is this the beginning of the end for GameStop",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gaming/2026/01/is-this-the-beginning-of-the-end-for-gamestop/",
          "published_at": "Mon, 12 Jan 2026 17:08:56 +0000",
          "title": "Is this the beginning of the end for GameStop?",
          "standfirst": "The sudden closure of hundreds of storefronts isn't exactly a great sign...",
          "content": "Six and a half years ago—after a failed corporate sale attempt, massive financial losses, and the departure/layoff of many key staff—I wrote about what seemed at the time like the \"imminent demise\" of GameStop. Now, after five years of meme stock mania that helped prop up the company's finances a bit, I'll admit the video game and Funko Pop retailer has lasted much longer as a relevant entity than I anticipated. GameStop's surprisingly extended run may be beginning to come to an end though, with Polygon reporting late last week that GameStop has abruptly shut down 400 stores across the US, with even more closures expected before the end of the month. That comes on top of 590 US stores that were shuttered in fiscal 2024 (which ended in January of 2025) and stated plans to end operations for hundreds of remaining international stores across Canada, Australia, and Europe in the coming months, per SEC filings. GameStop still had just over 3,200 stores worldwide as of February 1, 2025, so even hundreds of new and planned store closures don't literally mean the immediate end of the company as a going concern. But when you consider that there were still nearly 6,000 GameStop locations worldwide as of 2019—nearly 4,000 of which were in the US—the long-term trend is clear.Read full article Comments",
          "feed_position": 0,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2019/08/GettyImages-1135950796-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2019/08/GettyImages-1135950796-1152x648.jpg",
      "popularity_score": 347.8039152777778
    },
    {
      "id": "cluster_32",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 16:32:21 +0000",
      "title": "UK probes X over Grok CSAM scandal; Elon Musk cries censorship",
      "neutral_headline": "UK probes X over Grok CSAM scandal; Elon Musk cries censorship",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2026/01/uk-investigating-x-after-grok-undressed-thousands-of-women-and-children/",
          "published_at": "Mon, 12 Jan 2026 16:32:21 +0000",
          "title": "UK probes X over Grok CSAM scandal; Elon Musk cries censorship",
          "standfirst": "Grok tests if UK can penalize platforms for sexualized deepfakes generated by AI.",
          "content": "Elon Musk's X is currently under investigation in the United Kingdom after failing to stop the platform's chatbot, Grok, from generating thousands of sexualized images of women and children. On Monday, UK media regulator Ofcom confirmed that X may have violated the UK's Online Safety Act, which requires platforms to block illegal content. The proliferation of \"undressed images of people\" by X users may amount to intimate image abuse, pornography, and child sexual abuse material (CSAM), the regulator said. And X may also have neglected its duty to stop kids from seeing porn. \"Reports of Grok being used to create and share illegal non-consensual intimate images and child sexual abuse material on X have been deeply concerning,\" an Ofcom spokesperson said. \"Platforms must protect people in the UK from content that’s illegal in the UK, and we won’t hesitate to investigate where we suspect companies are failing in their duties, especially where there’s a risk of harm to children.\"Read full article Comments",
          "feed_position": 1,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2246892016-1024x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2246892016-1024x648.jpg",
      "popularity_score": 340.19419305555556
    },
    {
      "id": "cluster_35",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 15:59:15 +0000",
      "title": "The Chevrolet Bolt is back... but for how long?",
      "neutral_headline": "The Chevrolet Bolt is back... but for how long",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/cars/2026/01/the-chevrolet-bolt-is-back-but-for-how-long/",
          "published_at": "Mon, 12 Jan 2026 15:59:15 +0000",
          "title": "The Chevrolet Bolt is back... but for how long?",
          "standfirst": "The new LFP battery pack has 262 miles of range and fast-charges at 150 kW.",
          "content": "The new Chevrolet Equinox EV is a solid entry into the compact crossover market, and with a (just) sub-$35,000 starting price, it also counts as affordable by the standards of 2026. But if you think that's too rich for your blood, or that the Equinox is still too large for your needs, take heart—the Chevrolet Bolt is back in dealerships now as well. The Bolt was GM's first modern electric vehicle, following on from the hand-built, pre-lithium ion EV1 and the compliance car that was the Spark EV. We're big fans of the Bolt here at Ars Technica. It offered well more than 200 miles of range in a mass-produced EV at a reasonable price well before Tesla's Model 3 started clogging up our roads, it got more efficient over time, and it managed to be fun to drive in the process. General Motors (which owns Chevrolet) probably feels less well-disposed toward the Bolt. It lost thousands of dollars on each car it sold, even before the entire fleet had to be recalled for a costly battery replacement. The issue was due to improperly folded tabs on some cells that could cause a battery fire, giving GM (and its battery partner LG) plenty of bad press in the process. That recall alone cost $1.8 billion.Read full article Comments",
          "feed_position": 2,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/Chevrolet-Bolt-2027-DriverRear-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/Chevrolet-Bolt-2027-DriverRear-1152x648.jpg",
      "popularity_score": 332.6425263888889
    },
    {
      "id": "cluster_48",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 15:21:38 +0000",
      "title": "NASA topples towers used to test Saturn rockets, space shuttle",
      "neutral_headline": "NASA topples towers used to test Saturn rockets, space shuttle",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2026/01/nasa-topples-towers-used-to-test-saturn-rockets-space-shuttle/",
          "published_at": "Mon, 12 Jan 2026 15:21:38 +0000",
          "title": "NASA topples towers used to test Saturn rockets, space shuttle",
          "standfirst": "The Propulsion and Structural Test Facility and Dynamic Test Facility are no more.",
          "content": "Two historic NASA test facilities used in the development of the Saturn V and space shuttle launch vehicles have been demolished after towering over the Marshall Space Flight Center in Alabama since the start of the Space Age. The Propulsion and Structural Test Facility, which was erected in 1957—the same year the first artificial satellite entered Earth orbit—and the Dynamic Test Facility, which has stood since 1964, were brought down by a coordinated series of implosions on Saturday, January 10. Located in Marshall's East Test Area on the US Army's Redstone Arsenal in Huntsville, the two structures were no longer in use and, according to NASA, had a backlog of $25 million in needed repairs. \"This work reflects smart stewardship of taxpayer resources,\" Jared Isaacman, NASA administrator, said in a statement. \"Clearing outdated infrastructure allows NASA to safely modernize, streamline operations and fully leverage the infrastructure investments signed into law by President Trump to keep Marshall positioned at the forefront of aerospace innovation.\"Read full article Comments",
          "feed_position": 4,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/news-010826c-lg-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/news-010826c-lg-1152x648.jpg",
      "popularity_score": 322.0155819444444
    },
    {
      "id": "cluster_37",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 15:55:20 +0000",
      "title": "New research shows how shunning ultraprocessed foods helps with aging",
      "neutral_headline": "New research shows how shunning ultraprocessed foods helps with aging",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/health/2026/01/avoiding-ultraprocessed-foods-supports-healthier-aging/",
          "published_at": "Mon, 12 Jan 2026 15:55:20 +0000",
          "title": "New research shows how shunning ultraprocessed foods helps with aging",
          "standfirst": "Studies have linked ultraprocessed foods to poor health outcomes.",
          "content": "Older adults can dramatically reduce the amount of ultraprocessed foods they eat while keeping a familiar, balanced diet—and this shift leads to improvements across several key markers related to how the body regulates appetite and metabolism. That’s the main finding of a new study my colleagues and I published in the journal Clinical Nutrition. Ultraprocessed foods are made using industrial techniques and ingredients that aren’t typically used in home cooking. They often contain additives such as emulsifiers, flavorings, colors, and preservatives. Common examples include packaged snacks, ready-to-eat meals, and some processed meats. Studies have linked diets high in ultraprocessed foods to poorer health outcomes. My team and I enrolled Americans ages 65 and older in our study, many of whom were overweight or had metabolic risk factors such as insulin resistance or high cholesterol. Participants followed two diets low in ultraprocessed foods for eight weeks each. One included lean red meat (pork); the other was vegetarian with milk and eggs. For two weeks in between, participants returned to their usual diets.Read full article Comments",
          "feed_position": 3,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/ultraprocessed-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/ultraprocessed-1152x648.jpg",
      "popularity_score": 319.5772486111111
    },
    {
      "id": "cluster_65",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 11:30:06 +0000",
      "title": "The most fascinating monitors at CES 2026",
      "neutral_headline": "The most fascinating monitors at CES 2026",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2026/01/the-most-fascinating-monitors-at-ces-2026/",
          "published_at": "Mon, 12 Jan 2026 11:30:06 +0000",
          "title": "The most fascinating monitors at CES 2026",
          "standfirst": "Big sizes, big resolution, and big ideas.",
          "content": "CES 2026 took place in Las Vegas last week, and as usual, we're looking at the most interesting monitors from the show. Not every display is a monitor in the strictest sense, but they all provide a display for computers and have a unique twist that make them worth exploring. Dell’s massive UltraSharp Dell's biggest UltraSharp has a 21:9 aspect ratio. Credit: Dell It was a pretty safe bet that Dell would announce new UltraSharp monitors at CES. The displays are a solid recommendation for reliable USB-C monitors, including for Mac users and people needing something polished for professional or creative work. In recent years, UltraSharp monitors have also boasted more modern features, including integrated web cameras and IPS Black tech. This year, the strategy was clear: Bigger is better.Read full article Comments",
          "feed_position": 5,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/TCXAIO.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/TCXAIO.jpg",
      "popularity_score": 292.15669305555554
    },
    {
      "id": "cluster_80",
      "coverage": 1,
      "updated_at": "Sun, 11 Jan 2026 20:35:33 +0000",
      "title": "That time Will Smith helped discover new species of anaconda",
      "neutral_headline": "That time Will Smith helped discover new species of anaconda",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2026/01/that-time-will-smith-helped-discover-new-species-of-anaconda/",
          "published_at": "Sun, 11 Jan 2026 20:35:33 +0000",
          "title": "That time Will Smith helped discover new species of anaconda",
          "standfirst": "Footage of the 2024 discovery appears in NatGeo's new documentary series Pole to Pole with Will Smith.",
          "content": "In 2024, scientists announced the discovery of a new species of giant anaconda in South America. A National Geographic camera crew was on hand for the 2022 expedition that documented the new species—and so was actor Will Smith, since they were filming for NatGeo's new documentary series, Pole to Pole with Will Smith. Now we can all share in Smith's Amazon experience, courtesy of the three-minute clip above. Along with venom expert Bryan Fry, we follow Smith's journey by boat with a team of indigenous Waorani guides, scouring the river banks for anacondas. And they find one: a female green anaconda about 16 to 17 feet long, \"pure muscle.\" The Waorani secure the giant snake—anacondas aren't venomous but they do bite—so that Fry (with Smith's understandably reluctant help) can collect a scale sample for further analysis. Fry says that this will enable him to determine the accumulation of pollutants in the water. That and other collected samples also enabled scientists to conduct the genetic analysis that resulted in the declaration of a new species: the northern green anaconda (Eunectes akayama, which roughly translates to \"the great snake\"). It is genetically distinct from the southern green anaconda (Eunectes murinus); the two species likely diverged some 10 million years ago. The northern green anaconda's turf includes Venezuela, Colombia, Suriname, French Guyana, and the northern part of Brazil.Read full article Comments",
          "feed_position": 6,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/anaconda3-1152x648-1768069746.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/anaconda3-1152x648-1768069746.jpg",
      "popularity_score": 273
    },
    {
      "id": "cluster_95",
      "coverage": 1,
      "updated_at": "Sun, 11 Jan 2026 12:00:50 +0000",
      "title": "The oceans just keep getting hotter",
      "neutral_headline": "The oceans just keep getting hotter",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2026/01/the-oceans-just-keep-getting-hotter/",
          "published_at": "Sun, 11 Jan 2026 12:00:50 +0000",
          "title": "The oceans just keep getting hotter",
          "standfirst": "For the eighth year in a row, the world’s oceans absorbed a record-breaking amount of heat in 2025.",
          "content": "Since 2018, a group of researchers from around the world has crunched the numbers on how much heat the world’s oceans are absorbing each year. In 2025, their measurements broke records once again, making this the eighth year in a row that the world’s oceans have absorbed more heat than in the years before. The study, which was published Friday in the journal Advances in Atmospheric Science, found that the world’s oceans absorbed an additional 23 zettajoules’ worth of heat in 2025, the most in any year since modern measurements began in the 1960s. That’s significantly higher than the 16 additional zettajoules they absorbed in 2024. The research comes from a team of more than 50 scientists across the United States, Europe, and China. A joule is a common way to measure energy. A single joule is a relatively small unit of measurement—it’s about enough to power a tiny lightbulb for a second, or slightly heat a gram of water. But a zettajoule is one sextillion joules; numerically, the 23 zettajoules the oceans absorbed this year can be written out as 23,000,000,000,000,000,000,000.Read full article Comments",
          "feed_position": 7,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2023/04/sun-over-ocean-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2023/04/sun-over-ocean-1152x648.jpg",
      "popularity_score": 260
    }
  ]
}