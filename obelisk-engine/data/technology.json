{
  "updated_at": "2025-12-17T23:20:19.351Z",
  "clusters": [
    {
      "id": "cluster_17",
      "coverage": 3,
      "updated_at": "2025-12-17T16:24:00-05:00",
      "title": "Meta pauses third-party Horizon VR headsets program",
      "neutral_headline": "Meta pauses third-party Horizon VR headsets program",
      "items": [
        {
          "source": "The Verge",
          "url": "https://www.theverge.com/news/846762/meta-vr-headsets-third-party-program-horizon-os",
          "published_at": "2025-12-17T16:24:00-05:00",
          "title": "Meta pauses third-party Horizon VR headsets program",
          "standfirst": "Meta has \"paused\" its program to license its VR operating system to other hardware companies so they could build their own headsets using the platform, as reported by Road to VR . With the program, announced in April 2024, Meta announced that it would be licensing the Quest OS headset - which, at the same [&#8230;]",
          "content": "Meta has \"paused\" its program to license its VR operating system to other hardware companies so they could build their own headsets using the platform, as reported by Road to VR . With the program, announced in April 2024, Meta announced that it would be licensing the Quest OS headset - which, at the same time, it also renamed to Horizon OS - to hardware makers like Lenovo and Asus. In a blog post at the time, Meta said that the program would give \"more choice to consumers and a larger ecosystem for developers to build for.\" CEO Mark Zuckerberg said in a video that \"our goal is to make it so that the open model defines the next generation o … Read the full story at The Verge.",
          "feed_position": 2
        },
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2025/12/17/meta-is-pausing-its-dream-of-sharing-quests-horizon-os-with-third-party-headset-makers/",
          "published_at": "Wed, 17 Dec 2025 19:32:52 +0000",
          "title": "Meta is pausing its dream of sharing Quest&#8217;s Horizon OS with third-party headset makers",
          "standfirst": "The company said that it would revisit opportunities for third-party partnerships in the future.",
          "content": "The company said that it would revisit opportunities for third-party partnerships in the future.",
          "feed_position": 3
        },
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/251217/p37#a251217p37",
          "published_at": "Wed, 17 Dec 2025 12:40:01 -0500",
          "title": "Meta says it has \"paused\" its third-party Horizon OS headset program, announced in April 2024 with Asus and Lenovo as its first partners to build the headsets (Ben Lang/Road to VR)",
          "standfirst": "Ben Lang / Road to VR: Meta says it has &ldquo;paused&rdquo; its third-party Horizon OS headset program, announced in April 2024 with Asus and Lenovo as its first partners to build the headsets &mdash; Meta has &ldquo;paused&rdquo; its initiative to bring third-party Horizon OS headsets to the market.",
          "content": "Ben Lang / Road to VR: Meta says it has &ldquo;paused&rdquo; its third-party Horizon OS headset program, announced in April 2024 with Asus and Lenovo as its first partners to build the headsets &mdash; Meta has &ldquo;paused&rdquo; its initiative to bring third-party Horizon OS headsets to the market.",
          "feed_position": 9,
          "image_url": "http://www.techmeme.com/251217/i37.jpg"
        }
      ],
      "featured_image": "http://www.techmeme.com/251217/i37.jpg",
      "popularity_score": 3018.061291666667
    },
    {
      "id": "cluster_1",
      "coverage": 2,
      "updated_at": "Wed, 17 Dec 2025 17:55:01 -0500",
      "title": "Meta is testing limiting Facebook professional accounts and Pages to posting just two links per month, unless they subscribe to $14.99+/month Meta Verified (Ivan Mehta/TechCrunch)",
      "neutral_headline": "Facebook is testing a link-posting limit for professional accounts and pages",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/251217/p46#a251217p46",
          "published_at": "Wed, 17 Dec 2025 17:55:01 -0500",
          "title": "Meta is testing limiting Facebook professional accounts and Pages to posting just two links per month, unless they subscribe to $14.99+/month Meta Verified (Ivan Mehta/TechCrunch)",
          "standfirst": "Ivan Mehta / TechCrunch: Meta is testing limiting Facebook professional accounts and Pages to posting just two links per month, unless they subscribe to $14.99+/month Meta Verified &mdash; In a new experiment, Meta is limiting the number of links users can post on Facebook, unless they have a paid Meta Verified subscription.",
          "content": "Ivan Mehta / TechCrunch: Meta is testing limiting Facebook professional accounts and Pages to posting just two links per month, unless they subscribe to $14.99+/month Meta Verified &mdash; In a new experiment, Meta is limiting the number of links users can post on Facebook, unless they have a paid Meta Verified subscription.",
          "feed_position": 0,
          "image_url": "http://www.techmeme.com/251217/i46.jpg"
        },
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2025/12/17/facebook-is-testing-a-link-posting-limit-for-professional-accounts-and-pages/",
          "published_at": "Wed, 17 Dec 2025 18:15:58 +0000",
          "title": "Facebook is testing a link-posting limit for professional accounts and pages",
          "standfirst": "Meta said that the test impacts professional accounts and pages on Facebook.",
          "content": "Meta said that the test impacts professional accounts and pages on Facebook.",
          "feed_position": 5
        }
      ],
      "featured_image": "http://www.techmeme.com/251217/i46.jpg",
      "popularity_score": 2019.5782361111112
    },
    {
      "id": "cluster_14",
      "coverage": 2,
      "updated_at": "Wed, 17 Dec 2025 21:54:54 +0000",
      "title": "The best VPN deals: Up to 88 percent off ProtonVPN, Surfshark, ExpressVPN, NordVPN and more",
      "neutral_headline": "OnePlus 15R review: A 165Hz display and big battery for $700",
      "items": [
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/the-best-vpn-deals-up-to-88-percent-off-protonvpn-surfshark-expressvpn-nordvpn-and-more-120056445.html",
          "published_at": "Wed, 17 Dec 2025 21:54:54 +0000",
          "title": "The best VPN deals: Up to 88 percent off ProtonVPN, Surfshark, ExpressVPN, NordVPN and more",
          "standfirst": "With a good virtual private network (VPN), you can stream TV shows and events from all over the world, protect your information from hackers and thwart those online trackers that watch you sleep and show you weird personalized ads. Although we strongly recommend using a VPN, jumping on the first deal that comes along might get you stuck with a substandard app. Beyond that, even otherwise respectable VPNs sometimes frame their prices in misleading ways, with advertised deals not always as available as they seem to be. Even so, there are some great bargains on the table. For the holiday season, plenty of the best VPNs — including our top pick, Proton VPN — have end-of-year deals live that can save you anywhere from 67 to 88 percent on annual subscriptions. Most of these discounts only apply if you sign up for a year or more, but as long as you're comfortable with a service before you take the plunge, committing actually makes sense. You pay more at the start, but if you divide the cost by the months of subscription, it's much cheaper over time. Best VPN deals ExpressVPN Basic — $97.72 for a two-year subscription with four months free (73 percent off): This is one of the best VPNs, especially for new users, who will find its apps and website headache-free on all platforms. In tests for my ExpressVPN review, it dropped my download speeds by less than 7 percent and successfully changed my virtual location 14 out of 15 times. In short, it's an all-around excellent service that only suffers from being a little overpriced — which is why I'm so excited whenever I find it offering a decent deal. This discount, which gets you 28 months of ExpressVPN service, represents a 73 percent savings. Be aware, though, that it'll renew at the $99.95 per year price. ExpressVPN Advanced — $125.72 for a two-year subscription with four months free (67 percent off): ExpressVPN recently split its pricing into multiple tiers, but they all still come with similar discounts for going long. In addition to top-tier VPN service, advanced users get two additional simultaneous connections (for a total of 12), the ExpressVPN Keys password manager, advanced ad and tracker blocking, ID protection features and a 50 percent discount on an AirCove router. As above, note that it renews at $119.95 annually. NordVPN Basic — $80.73 for a two-year subscription with three months free (74 percent off): NordVPN gets the most important parts of a VPN right. It's fast, it doesn't leak any of your data and it's great at changing your virtual location. I noted in my NordVPN review that it always connects quickly and includes a support page that makes it easy to get live help. NordVPN includes a lot of cool features, like servers that instantly connect you to Tor. This holiday deal gives you 74 percent off the two-year plan, which also comes with three extra months. NordVPN Plus — $105.03 for a two-year subscription with three months free (74 percent off): In another holiday discount, NordVPN has also taken 74 percent off its Plus subscription. For only a little more, you get a powerful ad and tracker blocker that can also catch malware downloads, plus access to the NordPass password manager. A Plus plan also adds a data breach scanner that checks the dark web for your sensitive information. Surfshark Starter — $53.73 for a two-year subscription with three months free (87 percent off): This is the \"basic\" level of Surfshark, but it includes the entire VPN; everything on Surfshark One is an extra perk. With this subscription, you'll get some of the most envelope-pushing features in the VPN world right now. Surfshark can rotate your IP constantly to help you evade detection — it even lets you choose your own entry and exit nodes for a double-hop connection. That all comes with a near-invisible impact on download speeds. With this year-round deal, you can save 87 percent on 27 months of Surfshark. Surfshark One — $61.83 for a two-year subscription with three months free (88 percent off): A VPN is great, but it's not enough to protect your data all on its own. Surfshark One adds several apps that boost your security beyond just VPN service, including Surfshark Antivirus (scans devices and downloads for malware) and Surfshark Alert (alerts you whenever your sensitive information shows up in a data breach), plus Surfshark Search and Alternative ID from the tier below. This extra-low deal gives you 88 percent off all those features. If you bump up to Surfshark One+, you'll also get data removal through Incogni, but the price jumps enough that it's not quite worthwhile in my eyes. CyberGhost — $56.94 for a two-year subscription with two months free (83 percent off): CyberGhost has some of the best automation you'll see on any VPN. With its Smart Rules system, you can determine how its apps respond to different types of Wi-Fi networks, with exceptions for specific networks you know by name. Typically, you can set it to auto-connect, disconnect or send you a message asking what to do. CyberGhost's other best feature is its streaming servers — I've found both better video quality and more consistent unblocking when I use them on streaming sites. Currently, you can get 26 months of CyberGhost for 83 percent off the usual price. hide.me — $69.95 for a two-year subscription with four months free (75 percent off): Hide.me is an excellent free VPN — in fact, it's my favorite on the market, even with EventVPN and the free version of Proton VPN as competition. If you do want to upgrade to its paid plan, though, the two-year subscription offers great savings. Hide.me works well as a no-frills beginner VPN, with apps and a server network it should frankly be charging more for. Private Internet Access — $79 for a three-year subscription with four months free (83 percent off): With this deal, you can get 40 months of Private Internet Access (PIA) for a little bit under $2 per month — an 83 percent discount on its monthly price. Despite being so cheap, PIA has plenty of features, coming with its own DNS servers, a built-in ad blocker and automation powers to rival CyberGhost. However, internet speeds can fluctuate while you're connected. What makes a good VPN deal Practically every VPN heavily discounts its long-term subscriptions year-round, with even sharper discounts around occasions like the holidays. The only noteworthy exception is Mullvad, the Costco hot dog of VPNs (that's a compliment, to be clear). When there's constantly a huge discount going on, it can be hard to tell when you're actually getting a good deal. The best way to squeeze out more savings is to look for seasonal deals, student discounts or exclusive sales like Proton VPN's coupon for Engadget readers. One trick VPNs often use is to add extra months onto an introductory deal, pushing the average monthly price even lower. When it comes time to renew, you usually can't get these extra months again. You often can't even renew for the same basic period of time — for example, you may only be able to renew a two-year subscription for one year. If you're planning to hold onto a VPN indefinitely, check the fine print to see how much it will cost per month after the first renewal, and ensure that fits into your budget. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/the-best-vpn-deals-up-to-88-percent-off-protonvpn-surfshark-expressvpn-nordvpn-and-more-120056445.html?src=rss",
          "content": "With a good virtual private network (VPN), you can stream TV shows and events from all over the world, protect your information from hackers and thwart those online trackers that watch you sleep and show you weird personalized ads. Although we strongly recommend using a VPN, jumping on the first deal that comes along might get you stuck with a substandard app. Beyond that, even otherwise respectable VPNs sometimes frame their prices in misleading ways, with advertised deals not always as available as they seem to be. Even so, there are some great bargains on the table. For the holiday season, plenty of the best VPNs — including our top pick, Proton VPN — have end-of-year deals live that can save you anywhere from 67 to 88 percent on annual subscriptions. Most of these discounts only apply if you sign up for a year or more, but as long as you're comfortable with a service before you take the plunge, committing actually makes sense. You pay more at the start, but if you divide the cost by the months of subscription, it's much cheaper over time. Best VPN deals ExpressVPN Basic — $97.72 for a two-year subscription with four months free (73 percent off): This is one of the best VPNs, especially for new users, who will find its apps and website headache-free on all platforms. In tests for my ExpressVPN review, it dropped my download speeds by less than 7 percent and successfully changed my virtual location 14 out of 15 times. In short, it's an all-around excellent service that only suffers from being a little overpriced — which is why I'm so excited whenever I find it offering a decent deal. This discount, which gets you 28 months of ExpressVPN service, represents a 73 percent savings. Be aware, though, that it'll renew at the $99.95 per year price. ExpressVPN Advanced — $125.72 for a two-year subscription with four months free (67 percent off): ExpressVPN recently split its pricing into multiple tiers, but they all still come with similar discounts for going long. In addition to top-tier VPN service, advanced users get two additional simultaneous connections (for a total of 12), the ExpressVPN Keys password manager, advanced ad and tracker blocking, ID protection features and a 50 percent discount on an AirCove router. As above, note that it renews at $119.95 annually. NordVPN Basic — $80.73 for a two-year subscription with three months free (74 percent off): NordVPN gets the most important parts of a VPN right. It's fast, it doesn't leak any of your data and it's great at changing your virtual location. I noted in my NordVPN review that it always connects quickly and includes a support page that makes it easy to get live help. NordVPN includes a lot of cool features, like servers that instantly connect you to Tor. This holiday deal gives you 74 percent off the two-year plan, which also comes with three extra months. NordVPN Plus — $105.03 for a two-year subscription with three months free (74 percent off): In another holiday discount, NordVPN has also taken 74 percent off its Plus subscription. For only a little more, you get a powerful ad and tracker blocker that can also catch malware downloads, plus access to the NordPass password manager. A Plus plan also adds a data breach scanner that checks the dark web for your sensitive information. Surfshark Starter — $53.73 for a two-year subscription with three months free (87 percent off): This is the \"basic\" level of Surfshark, but it includes the entire VPN; everything on Surfshark One is an extra perk. With this subscription, you'll get some of the most envelope-pushing features in the VPN world right now. Surfshark can rotate your IP constantly to help you evade detection — it even lets you choose your own entry and exit nodes for a double-hop connection. That all comes with a near-invisible impact on download speeds. With this year-round deal, you can save 87 percent on 27 months of Surfshark. Surfshark One — $61.83 for a two-year subscription with three months free (88 percent off): A VPN is great, but it's not enough to protect your data all on its own. Surfshark One adds several apps that boost your security beyond just VPN service, including Surfshark Antivirus (scans devices and downloads for malware) and Surfshark Alert (alerts you whenever your sensitive information shows up in a data breach), plus Surfshark Search and Alternative ID from the tier below. This extra-low deal gives you 88 percent off all those features. If you bump up to Surfshark One+, you'll also get data removal through Incogni, but the price jumps enough that it's not quite worthwhile in my eyes. CyberGhost — $56.94 for a two-year subscription with two months free (83 percent off): CyberGhost has some of the best automation you'll see on any VPN. With its Smart Rules system, you can determine how its apps respond to different types of Wi-Fi networks, with exceptions for specific networks you know by name. Typically, you can set it to auto-connect, disconnect or send you a message asking what to do. CyberGhost's other best feature is its streaming servers — I've found both better video quality and more consistent unblocking when I use them on streaming sites. Currently, you can get 26 months of CyberGhost for 83 percent off the usual price. hide.me — $69.95 for a two-year subscription with four months free (75 percent off): Hide.me is an excellent free VPN — in fact, it's my favorite on the market, even with EventVPN and the free version of Proton VPN as competition. If you do want to upgrade to its paid plan, though, the two-year subscription offers great savings. Hide.me works well as a no-frills beginner VPN, with apps and a server network it should frankly be charging more for. Private Internet Access — $79 for a three-year subscription with four months free (83 percent off): With this deal, you can get 40 months of Private Internet Access (PIA) for a little bit under $2 per month — an 83 percent discount on its monthly price. Despite being so cheap, PIA has plenty of features, coming with its own DNS servers, a built-in ad blocker and automation powers to rival CyberGhost. However, internet speeds can fluctuate while you're connected. What makes a good VPN deal Practically every VPN heavily discounts its long-term subscriptions year-round, with even sharper discounts around occasions like the holidays. The only noteworthy exception is Mullvad, the Costco hot dog of VPNs (that's a compliment, to be clear). When there's constantly a huge discount going on, it can be hard to tell when you're actually getting a good deal. The best way to squeeze out more savings is to look for seasonal deals, student discounts or exclusive sales like Proton VPN's coupon for Engadget readers. One trick VPNs often use is to add extra months onto an introductory deal, pushing the average monthly price even lower. When it comes time to renew, you usually can't get these extra months again. You often can't even renew for the same basic period of time — for example, you may only be able to renew a two-year subscription for one year. If you're planning to hold onto a VPN indefinitely, check the fine print to see how much it will cost per month after the first renewal, and ensure that fits into your budget. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/the-best-vpn-deals-up-to-88-percent-off-protonvpn-surfshark-expressvpn-nordvpn-and-more-120056445.html?src=rss",
          "feed_position": 4
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/big-tech/study-links-amazons-algorithmic-pricing-with-erratic-inflated-costs-for-school-districts-202047988.html",
          "published_at": "Wed, 17 Dec 2025 20:20:47 +0000",
          "title": "Study links Amazon's algorithmic pricing with erratic, inflated costs for school districts",
          "standfirst": "When it comes to convenience, it’s hard to beat Amazon. And that rationale isn’t limited to consumers: Many local districts shopping for supplies with public funds apply the same logic. But the Institute for Local Self-Reliance (ILSR) published a study earlier this month (via The American Prospect) that illustrates the cost of that bargain. It suggests that Amazon’s “dynamic pricing” has led many schools and other localities to overpay for supplies.Public schools and local governments have historically bought supplies by soliciting competitive bids from local suppliers. Those vendors then respond with fixed price lists, delivery timelines and other terms. This competition — all out in the open, part of the public record — encourages low prices and transparency.On the surface, ordering from Amazon appears to offer competition, too. After all, the platform includes third-party vendors fighting for your dollars. But turning taxpayer funds over to Amazon’s algorithms isn’t quite that simple. That’s because the platform’s “dynamic pricing” (algorithmically driven real-time changes) is inherently opaque.According to the report, Amazon’s contracts with public entities don’t include fixed price lists. Instead, they include language built around swings. “This contract has a dynamic pricing structure in which the price for items listed on the online digital marketplace is driven by the market,” Amazon’s contract with Utah reads. “This contract will not need to be amended when prices fluctuate.”Below are some examples of wild price discrepancies for these districts. All of ILSR’s examples are from localities buying supplies from Amazon Business with public funds in 2023.A City of Boulder, CO employee ordered a 12-pack of Sharpie markers from Amazon Business for $8.99. On the same day, a Denver Public Schools worker ordered the same markers for $28.63.Amazon charged Clark County, WA, $146,000 for 610 computer monitors. On another day, that same order would have cost $24,000 less.Pittsburgh Schools bought two cases of Kleenex for $57.99 each. On the same day, Denver Schools paid $36.91 for a single case.On a single August day, Denver Schools placed two separate orders for bulk cases of dry-erase markers. One cost $114.52. The other was $149.07.In March 2023, Denver Schools paid $15.39 for a Swingline stapler (sold by Amazon). A few days later, the same school system paid $61.87 for the same product (sold by a third-party seller).Even in that last example, ILSR says Amazon’s algorithms are the culprit. “It might be tempting to blame the seller for putting a $62 price tag on a stapler or the employee for not noticing the cost,” the nonprofit argues. “But that overlooks Amazon’s pivotal role in the transaction — and the profit it makes. Amazon’s algorithms steer shoppers’ attention, selecting featured products and organizing search results. The platform routinely prompts users to ‘buy it again,’ even when the price has jumped. For busy public school employees, it’s all too easy to simply click the buy button, under the assumption that Amazon is surfacing the best option.”Amazon CEO Andy JassyNoah Berger via Getty ImagesOne portion of the study looked at repeat orders for 2,500 “high-frequency items.” (These included Amazon-brand copy paper, Elmer’s glue, BIC pens, Lysol cleaning wipes and Crayola crayons.) In total, the jurisdictions in the study spent $3 million on those items. But based on the lowest prices Amazon charged during that period, they would have paid only $2.5 million. Across those same items, one school district could have saved 17 percent (about $1 million) if it consistently received Amazon’s lowest prices.What would fair market value have been for those items? Well, it’s hard to say because the algorithms are steering pricing silently in the background. A more thorough study that included the same items, bought exclusively through the traditional procurement method, would tell us much more. And recent history has taught us that trusting Big Tech’s algorithms to serve the public good (rather than its own bottom line) is a fool’s errand.In at least some cases, the practice routes public funds away from local vendors and toward overseas ones — and, of course, Amazon itself. In Berkeley County, WV, the school district spent $1.3 million on Amazon Business in 2023. What portion went to sellers in the state? A measly $142.On top of all of that, the practice has snuffed out many of the smaller vendors that traditionally competed for these contracts. “The disappearance of these small and mid-sized businesses weakens local economies and tax bases,” the report concludes. “And it leaves governments increasingly dependent on Amazon, paving the way for the kind of monopoly control that ensures higher prices, poorer service, and less innovation.”In a statement sent to The Guardian, Amazon disputed the study’s conclusions. “Pricing research is notoriously difficult to conduct accurately and typically lacks reliable methodology, including cherry-picked product selections, mismatched product comparisons and comparing in-stock items with products out-of-stock at competitors,”ILSR’s report drew in spending data from 128 local governments (including cities, counties and school districts) and 122 state agencies. It also gathered contract documents and interviewed public officials, procurement experts and vendors.This article originally appeared on Engadget at https://www.engadget.com/big-tech/study-links-amazons-algorithmic-pricing-with-erratic-inflated-costs-for-school-districts-202047988.html?src=rss",
          "content": "When it comes to convenience, it’s hard to beat Amazon. And that rationale isn’t limited to consumers: Many local districts shopping for supplies with public funds apply the same logic. But the Institute for Local Self-Reliance (ILSR) published a study earlier this month (via The American Prospect) that illustrates the cost of that bargain. It suggests that Amazon’s “dynamic pricing” has led many schools and other localities to overpay for supplies.Public schools and local governments have historically bought supplies by soliciting competitive bids from local suppliers. Those vendors then respond with fixed price lists, delivery timelines and other terms. This competition — all out in the open, part of the public record — encourages low prices and transparency.On the surface, ordering from Amazon appears to offer competition, too. After all, the platform includes third-party vendors fighting for your dollars. But turning taxpayer funds over to Amazon’s algorithms isn’t quite that simple. That’s because the platform’s “dynamic pricing” (algorithmically driven real-time changes) is inherently opaque.According to the report, Amazon’s contracts with public entities don’t include fixed price lists. Instead, they include language built around swings. “This contract has a dynamic pricing structure in which the price for items listed on the online digital marketplace is driven by the market,” Amazon’s contract with Utah reads. “This contract will not need to be amended when prices fluctuate.”Below are some examples of wild price discrepancies for these districts. All of ILSR’s examples are from localities buying supplies from Amazon Business with public funds in 2023.A City of Boulder, CO employee ordered a 12-pack of Sharpie markers from Amazon Business for $8.99. On the same day, a Denver Public Schools worker ordered the same markers for $28.63.Amazon charged Clark County, WA, $146,000 for 610 computer monitors. On another day, that same order would have cost $24,000 less.Pittsburgh Schools bought two cases of Kleenex for $57.99 each. On the same day, Denver Schools paid $36.91 for a single case.On a single August day, Denver Schools placed two separate orders for bulk cases of dry-erase markers. One cost $114.52. The other was $149.07.In March 2023, Denver Schools paid $15.39 for a Swingline stapler (sold by Amazon). A few days later, the same school system paid $61.87 for the same product (sold by a third-party seller).Even in that last example, ILSR says Amazon’s algorithms are the culprit. “It might be tempting to blame the seller for putting a $62 price tag on a stapler or the employee for not noticing the cost,” the nonprofit argues. “But that overlooks Amazon’s pivotal role in the transaction — and the profit it makes. Amazon’s algorithms steer shoppers’ attention, selecting featured products and organizing search results. The platform routinely prompts users to ‘buy it again,’ even when the price has jumped. For busy public school employees, it’s all too easy to simply click the buy button, under the assumption that Amazon is surfacing the best option.”Amazon CEO Andy JassyNoah Berger via Getty ImagesOne portion of the study looked at repeat orders for 2,500 “high-frequency items.” (These included Amazon-brand copy paper, Elmer’s glue, BIC pens, Lysol cleaning wipes and Crayola crayons.) In total, the jurisdictions in the study spent $3 million on those items. But based on the lowest prices Amazon charged during that period, they would have paid only $2.5 million. Across those same items, one school district could have saved 17 percent (about $1 million) if it consistently received Amazon’s lowest prices.What would fair market value have been for those items? Well, it’s hard to say because the algorithms are steering pricing silently in the background. A more thorough study that included the same items, bought exclusively through the traditional procurement method, would tell us much more. And recent history has taught us that trusting Big Tech’s algorithms to serve the public good (rather than its own bottom line) is a fool’s errand.In at least some cases, the practice routes public funds away from local vendors and toward overseas ones — and, of course, Amazon itself. In Berkeley County, WV, the school district spent $1.3 million on Amazon Business in 2023. What portion went to sellers in the state? A measly $142.On top of all of that, the practice has snuffed out many of the smaller vendors that traditionally competed for these contracts. “The disappearance of these small and mid-sized businesses weakens local economies and tax bases,” the report concludes. “And it leaves governments increasingly dependent on Amazon, paving the way for the kind of monopoly control that ensures higher prices, poorer service, and less innovation.”In a statement sent to The Guardian, Amazon disputed the study’s conclusions. “Pricing research is notoriously difficult to conduct accurately and typically lacks reliable methodology, including cherry-picked product selections, mismatched product comparisons and comparing in-stock items with products out-of-stock at competitors,”ILSR’s report drew in spending data from 128 local governments (including cities, counties and school districts) and 122 state agencies. It also gathered contract documents and interviewed public officials, procurement experts and vendors.This article originally appeared on Engadget at https://www.engadget.com/big-tech/study-links-amazons-algorithmic-pricing-with-erratic-inflated-costs-for-school-districts-202047988.html?src=rss",
          "feed_position": 7,
          "image_url": "https://media-mbst-pub-ue1.s3.amazonaws.com/creatr-uploaded-images/2025-02/d51f6860-e4df-11ef-9fbd-65a89c2ab524"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/technology/gemini-3-flash-arrives-with-reduced-costs-and-latency-a-powerful-combo-for",
          "published_at": "Wed, 17 Dec 2025 19:24:00 GMT",
          "title": "Gemini 3 Flash arrives with reduced costs and latency — a powerful combo for enterprises",
          "standfirst": "Enterprises can now harness the power of a large language model that&#x27;s near that of the state-of-the-art Google’s Gemini 3 Pro, but at a fraction of the cost and with increased speed, thanks to the newly released Gemini 3 Flash.The model joins the flagship Gemini 3 Pro, Gemini 3 Deep Think, and Gemini Agent, all of which were announced and released last month.Gemini 3 Flash, now available on Gemini Enterprise, Google Antigravity, Gemini CLI, AI Studio, and on preview in Vertex AI, processes information in near real-time and helps build quick, responsive agentic applications. The company said in a blog post that Gemini 3 Flash “builds on the model series that developers and enterprises already love, optimized for high-frequency workflows that demand speed, without sacrificing quality.The model is also the default for AI Mode on Google Search and the Gemini application. Tulsee Doshi, senior director, product management on the Gemini team, said in a separate blog post that the model “demonstrates that speed and scale don’t have to come at the cost of intelligence.”“Gemini 3 Flash is made for iterative development, offering Gemini 3’s Pro-grade coding performance with low latency — it’s able to reason and solve tasks quickly in high-frequency workflows,” Doshi said. “It strikes an ideal balance for agentic coding, production-ready systems and responsive interactive applications.”Early adoption by specialized firms proves the model&#x27;s reliability in high-stakes fields. Harvey, an AI platform for law firms, reported a 7% jump in reasoning on their internal &#x27;BigLaw Bench,&#x27; while Resemble AI discovered that Gemini 3 Flash could process complex forensic data for deepfake detection 4x faster than Gemini 2.5 Pro. These aren&#x27;t just speed gains; they are enabling &#x27;near real-time&#x27; workflows that were previously impossible.More efficient at a lower costEnterprise AI builders have become more aware of the cost of running AI models, especially as they try to convince stakeholders to put more budget into agentic workflows that run on expensive models. Organizations have turned to smaller or distilled models, focusing on open models or other research and prompting techniques to help manage bloated AI costs.For enterprises, the biggest value proposition for Gemini 3 Flash is that it offers the same level of advanced multimodal capabilities, such as complex video analysis and data extraction, as its larger Gemini counterparts, but is far faster and cheaper. While Google’s internal materials highlight a 3x speed increase over the 2.5 Pro series, data from independent benchmarking firm Artificial Analysis adds a layer of crucial nuance. In the latter organization&#x27;s pre-release testing, Gemini 3 Flash Preview recorded a raw throughput of 218 output tokens per second. This makes it 22% slower than the previous &#x27;non-reasoning&#x27; Gemini 2.5 Flash, but it is still significantly faster than rivals including OpenAI&#x27;s GPT-5.1 high (125 t/s) and DeepSeek V3.2 reasoning (30 t/s).Most notably, Artificial Analysis crowned Gemini 3 Flash as the new leader in their AA-Omniscience knowledge benchmark, where it achieved the highest knowledge accuracy of any model tested to date. However, this intelligence comes with a &#x27;reasoning tax&#x27;: the model more than doubles its token usage compared to the 2.5 Flash series when tackling complex indexes. This high token density is offset by Google&#x27;s aggressive pricing: when accessing through the Gemini API, Gemini 3 Flash costs $0.50 per 1 million input tokens, compared to $1.25/1M input tokens for Gemini 2.5 Pro, and $3/1M output tokens, compared to $ 10/1 M output tokens for Gemini 2.5 Pro. This allows Gemini 3 Flash to claim the title of the most cost-efficient model for its intelligence tier, despite being one of the most &#x27;talkative&#x27; models in terms of raw token volume. Here&#x27;s how it stacks up to rival LLM offerings:ModelInput (/1M)Output (/1M)Total CostSourceQwen 3 Turbo$0.05$0.20$0.25Alibaba CloudGrok 4.1 Fast (reasoning)$0.20$0.50$0.70xAIGrok 4.1 Fast (non-reasoning)$0.20$0.50$0.70xAIdeepseek-chat (V3.2-Exp)$0.28$0.42$0.70DeepSeekdeepseek-reasoner (V3.2-Exp)$0.28$0.42$0.70DeepSeekQwen 3 Plus$0.40$1.20$1.60Alibaba CloudERNIE 5.0$0.85$3.40$4.25QianfanGemini 3 Flash Preview$0.50$3.00$3.50GoogleClaude Haiku 4.5$1.00$5.00$6.00AnthropicQwen-Max$1.60$6.40$8.00Alibaba CloudGemini 3 Pro (≤200K)$2.00$12.00$14.00GoogleGPT-5.2$1.75$14.00$15.75OpenAIClaude Sonnet 4.5$3.00$15.00$18.00AnthropicGemini 3 Pro (>200K)$4.00$18.00$22.00GoogleClaude Opus 4.5$5.00$25.00$30.00AnthropicGPT-5.2 Pro$21.00$168.00$189.00OpenAIMore ways to saveBut enterprise developers and users can cut costs further by eliminating the lag most larger models often have, which racks up token usage. Google said the model “is able to modulate how much it thinks,” so that it uses more thinking and therefore more tokens for more complex tasks than for quick prompts. The company noted Gemini 3 Flash uses 30% fewer tokens than Gemini 2.5 Pro. To balance this new reasoning power with strict corporate latency requirements, Google has introduced a &#x27;Thinking Level&#x27; parameter. Developers can toggle between &#x27;Low&#x27;—to minimize cost and latency for simple chat tasks—and &#x27;High&#x27;—to maximize reasoning depth for complex data extraction. This granular control allows teams to build &#x27;variable-speed&#x27; applications that only consume expensive &#x27;thinking tokens&#x27; when a problem actually demands PhD-level loThe economic story extends beyond simple token prices. With the standard inclusion of Context Caching, enterprises processing massive, static datasets—such as entire legal libraries or codebase repositories—can see a 90% reduction in costs for repeated queries. When combined with the Batch API’s 50% discount, the total cost of ownership for a Gemini-powered agent drops significantly below the threshold of competing frontier models“Gemini 3 Flash delivers exceptional performance on coding and agentic tasks combined with a lower price point, allowing teams to deploy sophisticated reasoning costs across high-volume processes without hitting barriers,” Google said. By offering a model that delivers strong multimodal performance at a more affordable price, Google is making the case that enterprises concerned with controlling their AI spend should choose its models, especially Gemini 3 Flash. Strong benchmark performance But how does Gemini 3 Flash stack up against other models in terms of its performance? Doshi said the model achieved a score of 78% on the SWE-Bench Verified benchmark testing for coding agents, outperforming both the preceding Gemini 2.5 family and the newer Gemini 3 Pro itself!For enterprises, this means high-volume software maintenance and bug-fixing tasks can now be offloaded to a model that is both faster and cheaper than previous flagship models, without a degradation in code quality.The model also performed strongly on other benchmarks, scoring 81.2% on the MMMU Pro benchmark, comparable to Gemini 3 Pro. While most Flash type models are explicitly optimized for short, quick tasks like generating code, Google claims Gemini 3 Flash’s performance “in reasoning, tool use and multimodal capabilities is ideal for developers looking to do more complex video analysis, data extraction and visual Q&A, which means it can enable more intelligent applications — like in-game assistants or A/B test experiments — that demand both quick answers and deep reasoning.”First impressions from early usersSo far, early users have been largely impressed with the model, particularly its benchmark performance. What It Means for Enterprise AI UsageWith Gemini 3 Flash now serving as the default engine across Google Search and the Gemini app, we are witnessing the \"Flash-ification\" of frontier intelligence. By making Pro-level reasoning the new baseline, Google is setting a trap for slower incumbents. The integration into platforms like Google Antigravity suggests that Google isn&#x27;t just selling a model; it&#x27;s selling the infrastructure for the autonomous enterprise. As developers hit the ground running with 3x faster speeds and a 90% discount on context caching, the \"Gemini-first\" strategy becomes a compelling financial argument. In the high-velocity race for AI dominance, Gemini 3 Flash may be the model that finally turns \"vibe coding\" from an experimental hobby into a production-ready reality.",
          "content": "Enterprises can now harness the power of a large language model that&#x27;s near that of the state-of-the-art Google’s Gemini 3 Pro, but at a fraction of the cost and with increased speed, thanks to the newly released Gemini 3 Flash.The model joins the flagship Gemini 3 Pro, Gemini 3 Deep Think, and Gemini Agent, all of which were announced and released last month.Gemini 3 Flash, now available on Gemini Enterprise, Google Antigravity, Gemini CLI, AI Studio, and on preview in Vertex AI, processes information in near real-time and helps build quick, responsive agentic applications. The company said in a blog post that Gemini 3 Flash “builds on the model series that developers and enterprises already love, optimized for high-frequency workflows that demand speed, without sacrificing quality.The model is also the default for AI Mode on Google Search and the Gemini application. Tulsee Doshi, senior director, product management on the Gemini team, said in a separate blog post that the model “demonstrates that speed and scale don’t have to come at the cost of intelligence.”“Gemini 3 Flash is made for iterative development, offering Gemini 3’s Pro-grade coding performance with low latency — it’s able to reason and solve tasks quickly in high-frequency workflows,” Doshi said. “It strikes an ideal balance for agentic coding, production-ready systems and responsive interactive applications.”Early adoption by specialized firms proves the model&#x27;s reliability in high-stakes fields. Harvey, an AI platform for law firms, reported a 7% jump in reasoning on their internal &#x27;BigLaw Bench,&#x27; while Resemble AI discovered that Gemini 3 Flash could process complex forensic data for deepfake detection 4x faster than Gemini 2.5 Pro. These aren&#x27;t just speed gains; they are enabling &#x27;near real-time&#x27; workflows that were previously impossible.More efficient at a lower costEnterprise AI builders have become more aware of the cost of running AI models, especially as they try to convince stakeholders to put more budget into agentic workflows that run on expensive models. Organizations have turned to smaller or distilled models, focusing on open models or other research and prompting techniques to help manage bloated AI costs.For enterprises, the biggest value proposition for Gemini 3 Flash is that it offers the same level of advanced multimodal capabilities, such as complex video analysis and data extraction, as its larger Gemini counterparts, but is far faster and cheaper. While Google’s internal materials highlight a 3x speed increase over the 2.5 Pro series, data from independent benchmarking firm Artificial Analysis adds a layer of crucial nuance. In the latter organization&#x27;s pre-release testing, Gemini 3 Flash Preview recorded a raw throughput of 218 output tokens per second. This makes it 22% slower than the previous &#x27;non-reasoning&#x27; Gemini 2.5 Flash, but it is still significantly faster than rivals including OpenAI&#x27;s GPT-5.1 high (125 t/s) and DeepSeek V3.2 reasoning (30 t/s).Most notably, Artificial Analysis crowned Gemini 3 Flash as the new leader in their AA-Omniscience knowledge benchmark, where it achieved the highest knowledge accuracy of any model tested to date. However, this intelligence comes with a &#x27;reasoning tax&#x27;: the model more than doubles its token usage compared to the 2.5 Flash series when tackling complex indexes. This high token density is offset by Google&#x27;s aggressive pricing: when accessing through the Gemini API, Gemini 3 Flash costs $0.50 per 1 million input tokens, compared to $1.25/1M input tokens for Gemini 2.5 Pro, and $3/1M output tokens, compared to $ 10/1 M output tokens for Gemini 2.5 Pro. This allows Gemini 3 Flash to claim the title of the most cost-efficient model for its intelligence tier, despite being one of the most &#x27;talkative&#x27; models in terms of raw token volume. Here&#x27;s how it stacks up to rival LLM offerings:ModelInput (/1M)Output (/1M)Total CostSourceQwen 3 Turbo$0.05$0.20$0.25Alibaba CloudGrok 4.1 Fast (reasoning)$0.20$0.50$0.70xAIGrok 4.1 Fast (non-reasoning)$0.20$0.50$0.70xAIdeepseek-chat (V3.2-Exp)$0.28$0.42$0.70DeepSeekdeepseek-reasoner (V3.2-Exp)$0.28$0.42$0.70DeepSeekQwen 3 Plus$0.40$1.20$1.60Alibaba CloudERNIE 5.0$0.85$3.40$4.25QianfanGemini 3 Flash Preview$0.50$3.00$3.50GoogleClaude Haiku 4.5$1.00$5.00$6.00AnthropicQwen-Max$1.60$6.40$8.00Alibaba CloudGemini 3 Pro (≤200K)$2.00$12.00$14.00GoogleGPT-5.2$1.75$14.00$15.75OpenAIClaude Sonnet 4.5$3.00$15.00$18.00AnthropicGemini 3 Pro (>200K)$4.00$18.00$22.00GoogleClaude Opus 4.5$5.00$25.00$30.00AnthropicGPT-5.2 Pro$21.00$168.00$189.00OpenAIMore ways to saveBut enterprise developers and users can cut costs further by eliminating the lag most larger models often have, which racks up token usage. Google said the model “is able to modulate how much it thinks,” so that it uses more thinking and therefore more tokens for more complex tasks than for quick prompts. The company noted Gemini 3 Flash uses 30% fewer tokens than Gemini 2.5 Pro. To balance this new reasoning power with strict corporate latency requirements, Google has introduced a &#x27;Thinking Level&#x27; parameter. Developers can toggle between &#x27;Low&#x27;—to minimize cost and latency for simple chat tasks—and &#x27;High&#x27;—to maximize reasoning depth for complex data extraction. This granular control allows teams to build &#x27;variable-speed&#x27; applications that only consume expensive &#x27;thinking tokens&#x27; when a problem actually demands PhD-level loThe economic story extends beyond simple token prices. With the standard inclusion of Context Caching, enterprises processing massive, static datasets—such as entire legal libraries or codebase repositories—can see a 90% reduction in costs for repeated queries. When combined with the Batch API’s 50% discount, the total cost of ownership for a Gemini-powered agent drops significantly below the threshold of competing frontier models“Gemini 3 Flash delivers exceptional performance on coding and agentic tasks combined with a lower price point, allowing teams to deploy sophisticated reasoning costs across high-volume processes without hitting barriers,” Google said. By offering a model that delivers strong multimodal performance at a more affordable price, Google is making the case that enterprises concerned with controlling their AI spend should choose its models, especially Gemini 3 Flash. Strong benchmark performance But how does Gemini 3 Flash stack up against other models in terms of its performance? Doshi said the model achieved a score of 78% on the SWE-Bench Verified benchmark testing for coding agents, outperforming both the preceding Gemini 2.5 family and the newer Gemini 3 Pro itself!For enterprises, this means high-volume software maintenance and bug-fixing tasks can now be offloaded to a model that is both faster and cheaper than previous flagship models, without a degradation in code quality.The model also performed strongly on other benchmarks, scoring 81.2% on the MMMU Pro benchmark, comparable to Gemini 3 Pro. While most Flash type models are explicitly optimized for short, quick tasks like generating code, Google claims Gemini 3 Flash’s performance “in reasoning, tool use and multimodal capabilities is ideal for developers looking to do more complex video analysis, data extraction and visual Q&A, which means it can enable more intelligent applications — like in-game assistants or A/B test experiments — that demand both quick answers and deep reasoning.”First impressions from early usersSo far, early users have been largely impressed with the model, particularly its benchmark performance. What It Means for Enterprise AI UsageWith Gemini 3 Flash now serving as the default engine across Google Search and the Gemini app, we are witnessing the \"Flash-ification\" of frontier intelligence. By making Pro-level reasoning the new baseline, Google is setting a trap for slower incumbents. The integration into platforms like Google Antigravity suggests that Google isn&#x27;t just selling a model; it&#x27;s selling the infrastructure for the autonomous enterprise. As developers hit the ground running with 3x faster speeds and a 90% discount on context caching, the \"Gemini-first\" strategy becomes a compelling financial argument. In the high-velocity race for AI dominance, Gemini 3 Flash may be the model that finally turns \"vibe coding\" from an experimental hobby into a production-ready reality.",
          "feed_position": 0,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/49s3tZpzGRgAER8EKNnJ9f/9b37d007e7f3651a4046466d1159496a/crimedy7_illustration_of_a_robot_running_very_quickly_--ar_16_8c0c48f5-0305-43bc-b49d-4c06e63e9545_0.png?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/orchestration/jp-morgans-ai-adoption-hit-50-of-employees-the-secret-a-connectivity-first",
          "published_at": "Wed, 17 Dec 2025 19:00:00 GMT",
          "title": "JP Morgan’s AI adoption hit 50% of employees. The secret? A connectivity-first architecture",
          "standfirst": "When Derek Waldron and his technical team at JPMorgan Chase first launched an LLM suite with personal assistants two-and-a-half years ago, they weren’t sure what to expect. That wasn’t long after the game-changing emergence of ChatGPT, but in enterprise, skepticism was still high. Surprisingly, employees opted into the internal platform organically — and quickly. Within months, usage jumped from zero to 250,000 employees. Now, more than 60% of employees across sales, finance, technology, operations, and other departments use the continually evolving, continually connected suite.“We were surprised by just how viral it was,” Waldron, JPMorgan’s chief analytics officer, explains in a new VB Beyond the Pilot podcast. Employees weren’t just designing prompts, they were building and customizing assistants with specific personas, instructions, and roles and were sharing their learnings on internal platforms. The financial giant has pulled off what most enterprises still struggle to achieve: large-scale, voluntary employee adoption of AI. It wasn’t the result of mandates; rather, early adopters shared tangible use cases, and workers began feeding off each other’s enthusiasm. This bottom-up usage has ultimately resulted in an innovation flywheel. “It’s this deep rooted innovative population,” Waldron says. “If we can continue to equip them with really easy to use, powerful capabilities, they can turbocharge the next evolution of this journey.” Ubiquitous connectivity plugged into highly sophisticated systems of recordJPMorgan has taken a rare, forward-looking approach to its technical architecture. The company treats AI as a core infrastructure rather than a novelty, operating from the early contrarian stance that the models themselves would become a commodity. Instead, they identified the connectivity around the system as the real challenge and defensible moat.The financial giant invested early in multimodal retrieval-augmented generation (RAG), now in its fourth generation and incorporating multi-modality. Its AI suite is hosted at the center of an enterprise-wide platform equipped with connectors and tools that support analysis and preparation. Employees can plug into an expanding ecosystem of critical business data and interact with “very sophisticated” documents, knowledge and structured data stores, as well as CRM, HR, trading, finance and risk systems. Waldron says his team continues to add more connections by the month. “We built the platform around this type of ubiquitous connectivity,” he explains. Ultimately, AI is a great general-purpose technology that will only grow more powerful, but if people don’t have meaningful access and critical use cases, “you&#x27;re squandering the opportunity.” As Waldron puts it, AI’s capabilities continue to grow impressively — but they simply remain shiny objects for show if they can’t prove real-world use. “Even if super intelligence were to show up tomorrow, there&#x27;s no value that can be optimally extracted if that superintelligence can&#x27;t connect into the systems, the data, the tools, the knowledge, the processes that exist within the enterprise,” he contends. Listen to the full episode to hear about: Waldron’s personal strategy of pausing before asking a human colleague and instead assessing how his AI assistant could answer that question and solve the problem. A \"one platform, many jobs\" approach: No two roles are the same way, so strategy should center on reusable building blocks (RAG, document intelligence, structured data querying) that employees can assemble into role-specific tools.Why RAG maturity matters: JPMorgan evolved through multiple generations of retrieval — from basic vector search to hierarchical, authoritative, multimodal knowledge pipelines.Subscribe to Beyond the Pilot on Apple Podcasts and Spotify.",
          "content": "When Derek Waldron and his technical team at JPMorgan Chase first launched an LLM suite with personal assistants two-and-a-half years ago, they weren’t sure what to expect. That wasn’t long after the game-changing emergence of ChatGPT, but in enterprise, skepticism was still high. Surprisingly, employees opted into the internal platform organically — and quickly. Within months, usage jumped from zero to 250,000 employees. Now, more than 60% of employees across sales, finance, technology, operations, and other departments use the continually evolving, continually connected suite.“We were surprised by just how viral it was,” Waldron, JPMorgan’s chief analytics officer, explains in a new VB Beyond the Pilot podcast. Employees weren’t just designing prompts, they were building and customizing assistants with specific personas, instructions, and roles and were sharing their learnings on internal platforms. The financial giant has pulled off what most enterprises still struggle to achieve: large-scale, voluntary employee adoption of AI. It wasn’t the result of mandates; rather, early adopters shared tangible use cases, and workers began feeding off each other’s enthusiasm. This bottom-up usage has ultimately resulted in an innovation flywheel. “It’s this deep rooted innovative population,” Waldron says. “If we can continue to equip them with really easy to use, powerful capabilities, they can turbocharge the next evolution of this journey.” Ubiquitous connectivity plugged into highly sophisticated systems of recordJPMorgan has taken a rare, forward-looking approach to its technical architecture. The company treats AI as a core infrastructure rather than a novelty, operating from the early contrarian stance that the models themselves would become a commodity. Instead, they identified the connectivity around the system as the real challenge and defensible moat.The financial giant invested early in multimodal retrieval-augmented generation (RAG), now in its fourth generation and incorporating multi-modality. Its AI suite is hosted at the center of an enterprise-wide platform equipped with connectors and tools that support analysis and preparation. Employees can plug into an expanding ecosystem of critical business data and interact with “very sophisticated” documents, knowledge and structured data stores, as well as CRM, HR, trading, finance and risk systems. Waldron says his team continues to add more connections by the month. “We built the platform around this type of ubiquitous connectivity,” he explains. Ultimately, AI is a great general-purpose technology that will only grow more powerful, but if people don’t have meaningful access and critical use cases, “you&#x27;re squandering the opportunity.” As Waldron puts it, AI’s capabilities continue to grow impressively — but they simply remain shiny objects for show if they can’t prove real-world use. “Even if super intelligence were to show up tomorrow, there&#x27;s no value that can be optimally extracted if that superintelligence can&#x27;t connect into the systems, the data, the tools, the knowledge, the processes that exist within the enterprise,” he contends. Listen to the full episode to hear about: Waldron’s personal strategy of pausing before asking a human colleague and instead assessing how his AI assistant could answer that question and solve the problem. A \"one platform, many jobs\" approach: No two roles are the same way, so strategy should center on reusable building blocks (RAG, document intelligence, structured data querying) that employees can assemble into role-specific tools.Why RAG maturity matters: JPMorgan evolved through multiple generations of retrieval — from basic vector search to hierarchical, authoritative, multimodal knowledge pipelines.Subscribe to Beyond the Pilot on Apple Podcasts and Spotify.",
          "feed_position": 1,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/2enHa97PVIbaLzukEpRUth/8362fe4758604496771a13727fada179/JPMorgan.png?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/gaming/io-interactives-007-first-light-reimagines-james-bond-as-a-young-and-reckless-spy-180000758.html",
          "published_at": "Wed, 17 Dec 2025 18:00:00 +0000",
          "title": "IO Interactive's 007 First Light reimagines James Bond as a young and reckless spy",
          "standfirst": "The creators of the Hitman series have honed their style of- open-ended and spectacle-driven sandboxes across several games, and now they're taking their signature immersive gameplay to the world of James Bond. With 007 First Light, developer IO Interactive is crafting an origin story for the globetrotting British spy, showing how he undertook daring missions at the start of his career to eventually become the world's most infamous agent.Before the reveal at The Game Awards of 007 First Light's newest villain, played by Lenny Kravitz, we had an early look at the latest bits of plot for James Bond's origin story as a superspy. We also spoke with cinematic and narrative director Martin Emborg about the main inspirations for the game, how Bond's origin taps into decades of lore, and why a good spy story is timeless.\"What I think is kind of baked into doing an origin story for a character like James Bond, is that everyone knows the character and who he's going to become,\" Emborg said. \"But how does he become this character? I think that's an exciting challenge from a storytelling perspective.\"IO Interactive's James Bond is young and inexperienced.IO InteractiveWhile some Bond films and novels have touched upon the early years of the iconic character, 007 First Light will be the first attempt at an actual, modernized origin story. As a twenty-something new MI6 recruit, this James Bond, portrayed by Patrick Gibson, is inexperienced and brash, which can result in some operations going off the rails. He still possesses a certain cunning, and near-supernatural levels of charisma and resourcefulness. This presents a solid archetype for the game’s open-ended missions where players will explore tightly designed worlds with a multitude of tasks and objectives to handle – however the players see fit.While IO Interactive's Hitman series taps into the spy experience, what really separates James Bond from Agent 47 is that he's a far more social character. The social element will play a big part in how players can find ways to distract, or even outright bluff through charisma to sneak into areas.. Emborg explained that James Bond's resourcefulness also makes him a compelling character for a video game, especially one that is all about player agency.007 First Light will take players to several locations, including Slovakia and what appears to be Vietnam.IO Interactive\"Bond is a competent character,” Emborg said. “We want to give the player the opportunities to have that agency to say, 'Oh, I'm gonna talk to that guy,' or, 'I'm gonna go and punch that guy,’ or, ‘I can probably crawl up into a tight space to sneak in.' Having that sense of agency is pivotal if you want to deliver a full Bond experience. Obviously, the social aspect of that is important; to embed yourself and infiltrate socially is a big part of that Bond experience.\" Emborg said the rhythm of a James Bond game is different from a Hitman title, even though espionage, infiltration and subterfuge are at the core of both. \"We have a lot of experience with that from obviously making Hitman, but there are just many more gears to Bond,” he said. “Of course, the shape of this game is different. We have sandbox locations where you make the open-ended decisions, but we also have other levels where it is a chase or a set-piece encounter, and then it opens up again. So we kind of coined the term, having a game that breathes. It's a very different way of playing this type of game that we usually do, so we've definitely bolstered our toolbox for this game.\"In many ways, 007 First Light is a story that can only be told in an interactive format. The new game will pull from numerous novels and films for its plot, and it also features a large cast of familiar characters seen throughout the franchise – such as MI6 assistant Moneypenny (performed by Kiera Lester), gadget guru and quartermaster Q (played by Alastair Mackenzie), and team leader M (Priyanga Burford). But like any singular Bond story, First Light features its own set of original characters. In addition to James Bond's MI6 mentor John Greenway (portrayed by Lennie James), a new core villain, black-market smuggler and warlord Bawma, will be played by Lenny Kravitz. It's a familiar setup for a Bond experience for sure, but within the context of a video game – one made by IO Interactive – it really taps looks to tap into the Bond fantasy more than other games have. From the gameplay demos and trailers we've seen, IO Interactive looks to be channeling the style and lore from decades of Bond with its modernized reboot of the character. From deep cuts to On Her Majesty's Secret Service – one of the great and underrated Bond movies – and You Only Live Twice, 007 First Light is keeping a keen eye on paying tribute to what came before.\"At the very beginning, it was very much like a maelstrom of [ideas], then suddenly something emerges, and you build up a story around it,\" Emborg said. He continued, “I love doing this stuff and with great reverence for the material. It's a privilege to work in this universe. James Bond is one of the few IPs that still hasn't been overdone, in my view. We have a space to come in with a fresh take, and he's a character who's seen a lot of versions over the years.\"Confirmed MI6 gadgets include the \"Q-Watch.\"IO InteractiveSo far, 007 First Light has the makings of not only the biggest Bond game but also the most true-to-life simulation of being a superspy. While previous Bond games like Goldeneye and Everything or Nothing are well-loved classics, they embody traditional video game shooting spectacle. 007 First Light is looking to lean into the immersive element of the Hitman games in its adaptation of James Bond, finally giving players the opportunities to explore the social aspects of spy work on a grand scale. 007 First Light will arrive on March 27, 2026, for PC, PS5 and Xbox Series X/S.This article originally appeared on Engadget at https://www.engadget.com/gaming/io-interactives-007-first-light-reimagines-james-bond-as-a-young-and-reckless-spy-180000758.html?src=rss",
          "content": "The creators of the Hitman series have honed their style of- open-ended and spectacle-driven sandboxes across several games, and now they're taking their signature immersive gameplay to the world of James Bond. With 007 First Light, developer IO Interactive is crafting an origin story for the globetrotting British spy, showing how he undertook daring missions at the start of his career to eventually become the world's most infamous agent.Before the reveal at The Game Awards of 007 First Light's newest villain, played by Lenny Kravitz, we had an early look at the latest bits of plot for James Bond's origin story as a superspy. We also spoke with cinematic and narrative director Martin Emborg about the main inspirations for the game, how Bond's origin taps into decades of lore, and why a good spy story is timeless.\"What I think is kind of baked into doing an origin story for a character like James Bond, is that everyone knows the character and who he's going to become,\" Emborg said. \"But how does he become this character? I think that's an exciting challenge from a storytelling perspective.\"IO Interactive's James Bond is young and inexperienced.IO InteractiveWhile some Bond films and novels have touched upon the early years of the iconic character, 007 First Light will be the first attempt at an actual, modernized origin story. As a twenty-something new MI6 recruit, this James Bond, portrayed by Patrick Gibson, is inexperienced and brash, which can result in some operations going off the rails. He still possesses a certain cunning, and near-supernatural levels of charisma and resourcefulness. This presents a solid archetype for the game’s open-ended missions where players will explore tightly designed worlds with a multitude of tasks and objectives to handle – however the players see fit.While IO Interactive's Hitman series taps into the spy experience, what really separates James Bond from Agent 47 is that he's a far more social character. The social element will play a big part in how players can find ways to distract, or even outright bluff through charisma to sneak into areas.. Emborg explained that James Bond's resourcefulness also makes him a compelling character for a video game, especially one that is all about player agency.007 First Light will take players to several locations, including Slovakia and what appears to be Vietnam.IO Interactive\"Bond is a competent character,” Emborg said. “We want to give the player the opportunities to have that agency to say, 'Oh, I'm gonna talk to that guy,' or, 'I'm gonna go and punch that guy,’ or, ‘I can probably crawl up into a tight space to sneak in.' Having that sense of agency is pivotal if you want to deliver a full Bond experience. Obviously, the social aspect of that is important; to embed yourself and infiltrate socially is a big part of that Bond experience.\" Emborg said the rhythm of a James Bond game is different from a Hitman title, even though espionage, infiltration and subterfuge are at the core of both. \"We have a lot of experience with that from obviously making Hitman, but there are just many more gears to Bond,” he said. “Of course, the shape of this game is different. We have sandbox locations where you make the open-ended decisions, but we also have other levels where it is a chase or a set-piece encounter, and then it opens up again. So we kind of coined the term, having a game that breathes. It's a very different way of playing this type of game that we usually do, so we've definitely bolstered our toolbox for this game.\"In many ways, 007 First Light is a story that can only be told in an interactive format. The new game will pull from numerous novels and films for its plot, and it also features a large cast of familiar characters seen throughout the franchise – such as MI6 assistant Moneypenny (performed by Kiera Lester), gadget guru and quartermaster Q (played by Alastair Mackenzie), and team leader M (Priyanga Burford). But like any singular Bond story, First Light features its own set of original characters. In addition to James Bond's MI6 mentor John Greenway (portrayed by Lennie James), a new core villain, black-market smuggler and warlord Bawma, will be played by Lenny Kravitz. It's a familiar setup for a Bond experience for sure, but within the context of a video game – one made by IO Interactive – it really taps looks to tap into the Bond fantasy more than other games have. From the gameplay demos and trailers we've seen, IO Interactive looks to be channeling the style and lore from decades of Bond with its modernized reboot of the character. From deep cuts to On Her Majesty's Secret Service – one of the great and underrated Bond movies – and You Only Live Twice, 007 First Light is keeping a keen eye on paying tribute to what came before.\"At the very beginning, it was very much like a maelstrom of [ideas], then suddenly something emerges, and you build up a story around it,\" Emborg said. He continued, “I love doing this stuff and with great reverence for the material. It's a privilege to work in this universe. James Bond is one of the few IPs that still hasn't been overdone, in my view. We have a space to come in with a fresh take, and he's a character who's seen a lot of versions over the years.\"Confirmed MI6 gadgets include the \"Q-Watch.\"IO InteractiveSo far, 007 First Light has the makings of not only the biggest Bond game but also the most true-to-life simulation of being a superspy. While previous Bond games like Goldeneye and Everything or Nothing are well-loved classics, they embody traditional video game shooting spectacle. 007 First Light is looking to lean into the immersive element of the Hitman games in its adaptation of James Bond, finally giving players the opportunities to explore the social aspects of spy work on a grand scale. 007 First Light will arrive on March 27, 2026, for PC, PS5 and Xbox Series X/S.This article originally appeared on Engadget at https://www.engadget.com/gaming/io-interactives-007-first-light-reimagines-james-bond-as-a-young-and-reckless-spy-180000758.html?src=rss",
          "feed_position": 14,
          "image_url": "https://d29szjachogqwa.cloudfront.net/images/user-uploaded/007FirstLight_Screenshots_2.jpg"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/mobile/smartphones/oneplus-15r-review-a-165hz-display-and-big-battery-for-700-150000340.html",
          "published_at": "Wed, 17 Dec 2025 15:00:00 +0000",
          "title": "OnePlus 15R review: A 165Hz display and big battery for $700",
          "standfirst": "I know what you're thinking, didn't OnePlus release a new phone just last month? It did. A little over five weeks after the announcement of the OP15, the company is back with the OnePlus 15R, a more affordable version of its new flagship that starts at $700 (or $200 less than its sibling). Off the top, this will be a shorter review because most of what I said about the OnePlus 15 also applies to the OP15R. It's a great phone that asks you to make one pretty significant compromise. Design and display The OnePlus 15R's screen is slightly cooler than that of the OnePlus 15. Igor Bonifacic for Engadget Like the OnePlus 15, the 15R looks like the OnePlus 13s and 13T, a pair of smaller, 6.32-inch phones the company released in India and China this past spring. I said the design of the OP15 was boring and derivative of the iPhone 16 Pro. The 15R has done nothing to change that opinion. With one fewer camera, the OP15R doesn't look much different from the iPhone 12 I've been hanging on to since 2020. That said, I'm more fond of the 15R's mint breeze color (the phone is also available in charcoal black) than the sand storm shade of my OP15. We're big fans of minty phones here at Engadget, and OnePlus has gone with a particularly pleasing hue of the color with its new phone. With the redesign, OnePlus has also improved the phone's waterproofing, bringing it in line with the OP15. The new handset is IP69K-certified against moisture and dust, meaning it can withstand heated water shot at it at pressure. Like the OP15, the 15R trades OnePlus' old Alert Slider for a new Plus Key. It functions like the iPhone's Action button, allowing you to add a shortcut for a favorite feature. For example, you can configure it to open the camera app or act as a do not disturb toggle, among a few other options. One departure from the OP15 is that the 15R has a larger 6.83-inch display, making it slightly taller than its sibling. OnePlus is marketing this as one reason buyers might pick the 15R over the OP15, but holding the phones side by side, there's not much difference between the two. They're both big, and you'll either like that or won't. On top of being big, the 15R's screen can refresh at a fast 165Hz in games. The two displays are also comparable in terms of resolution and brightness; both can push 1,800 nits of brightness. One difference I noticed is the OnePlus 15 has a warmer panel, even when the two phones are set to the same colorspace. I've reached out to OnePlus to find what might be causing the disparity, but for now it may be due to a quality control issue or oversight in the company's software. One last thing, OnePlus has upgraded the 15R to add an ultrasonic fingerprint sensor beneath the screen. This is placed in a nice spot toward the bottom third of the display, and it's fast and accurate. Performance and battery The OnePlus 15R is also slightly thinner than the OnePlus 15. Igor Bonifacic for Engadget The OnePlus 15R is the first phone in North America to arrive with Qualcomm's latest Snapdragon 8 Gen 5 chipset. Not to be confused with the Snapdragon Gen 5 Elite in the OP15, this new chipset is similar to Qualcomm's flagship system-on-a-chip but has a weaker CPU and GPU. This is reflected in benchmarks like Geekbench 6 where the OP15 handily outperforms the OP15R. It's not even close, either, with the OP15 delivering standout single- and multi-core scores of 3,773 and 11,293, while the 15R put up more modest results of 2,857 and 9,512. From that perspective, you're losing a fair amount of performance, but real-world use tells a different story. Outside of the handful of games such as Call of Duty: Mobile and PUBG that support the OP15 and 15R's 165Hz displays, the Snapdragon 8 Gen 5 offers more than enough muscle for the majority of applications. Even for most games (like the ones I like to play, including Diablo Immortal and League of Legends: Wild Rift), the Snapdragon 8 Gen 5 is a great match. OnePlus also hasn't skimped on the 15R's other internal components. You're still getting 12GB of LPDDR5X Ultra RAM and 256GB of UFS 4.1 storage. That's the same configuration as the base model of the OP15. This translates to a phone that doesn't miss a beat when switching between apps and loading files like images and videos. The 15R has a slightly bigger battery, coming in at 7,400mAh, up from 7,300mAh on the OP15. In practice, the two phones offer the same amount of battery life. Putting them through both Engadget's video rundown test, they both ran for 38 hours before their batteries died (which makes sense given the OP15R has a bigger screen). Like the OP15, the 15R comes with the OnePlus 55W SUPERVOOC charger in the box. The adapter can get the 15R from dead to 100 percent in less than an hour. If you hate charging your phone, the 15R makes that process as painless as possible, with a battery that both lasts long and won't be at the outlet for hours. Cameras A closeup of the OnePlus 15R's camera module. Igor Bonifacic for Engadget By this point you're probably wondering what OnePlus cut from the OP15 to make the 15R more affordable. The answer — quite literally — is an entire camera. The new phone is missing a telephoto camera, something you could find on its predecessor, the OnePlus 13R. And as far as I can tell, the two remaining cameras use the same 50-megapixel and 8MP sensors OnePlus shipped on last year's model. The company also hasn't upgraded the glass on either camera. That leaves the selfie camera as the only area to see some change in the form of a sharper 32MP sensor and the addition of autofocus. Unfortunately, none of the 15R's cameras stand out. As a whole, they suffer from the same set of problems that plague the OnePlus 15's cameras. They're fine out on a sunny day, but as soon as the light becomes a bit challenging, the 15R struggles with shadow details, resulting in muddy pictures. The more I've used both the OP15 and 15R, the more I've come to the conclusion that OnePlus needs to go back to the drawing board with its new Detail Max Engine. It feels like it's holding back what should, at least on paper, be solid hardware. Software Despite it's large size, the OnePlus 15R isn't too heavy. Igor Bonifacic for Engadget There's not much to say here other than the 15R ships with OxygenOS 16, just like the OP15. OnePlus has also promised to support the 15R for the same amount of time as the OP15: four years with software updates and six years with security patches. That's a shorter window than Google and Samsung, both of which promise seven years on all their latest phones. It's hopefully something that OnePlus decides to change starting with the OnePlus 16. The reason I bring that up is that the company’s version of Android is one I like a lot. OxygenOS is slick, with animations that highlight the speed of the 15R's processor and display. The fact the phone comes with the latest version of OxygenOS means you also get access to all of the company's newest AI features, including its Mind Space hub where you can save screenshots and notes for an on-device model to transcribe and summarize. Wrap-up The OnePlus 15R comes in a lovely mint color. Igor Bonifacic for Engadget In short, the OnePlus 15R is the phone for people who don't care about photos and videos. That's the same conclusion I came to with the OP15. If you're a OnePlus fan, the 15R excels in all the areas you would expect the company's devices to make a good showing: performance, battery life and display responsiveness. Given I wasn't too impressed with the OP15's camera, I would actually recommend the 15R over that model. For $200 off the starting price of the OP15, you're getting a device that has almost all of the same strengths of its more expensive sibling. Compared to other phones in its price range, such as the Pixel 10 and Galaxy S25 FE, the 15R is not as well-rounded, and can't compete with those devices in camera quality, but you're getting much better performance, battery life and a display they can't match. This article originally appeared on Engadget at https://www.engadget.com/mobile/smartphones/oneplus-15r-review-a-165hz-display-and-big-battery-for-700-150000340.html?src=rss",
          "content": "I know what you're thinking, didn't OnePlus release a new phone just last month? It did. A little over five weeks after the announcement of the OP15, the company is back with the OnePlus 15R, a more affordable version of its new flagship that starts at $700 (or $200 less than its sibling). Off the top, this will be a shorter review because most of what I said about the OnePlus 15 also applies to the OP15R. It's a great phone that asks you to make one pretty significant compromise. Design and display The OnePlus 15R's screen is slightly cooler than that of the OnePlus 15. Igor Bonifacic for Engadget Like the OnePlus 15, the 15R looks like the OnePlus 13s and 13T, a pair of smaller, 6.32-inch phones the company released in India and China this past spring. I said the design of the OP15 was boring and derivative of the iPhone 16 Pro. The 15R has done nothing to change that opinion. With one fewer camera, the OP15R doesn't look much different from the iPhone 12 I've been hanging on to since 2020. That said, I'm more fond of the 15R's mint breeze color (the phone is also available in charcoal black) than the sand storm shade of my OP15. We're big fans of minty phones here at Engadget, and OnePlus has gone with a particularly pleasing hue of the color with its new phone. With the redesign, OnePlus has also improved the phone's waterproofing, bringing it in line with the OP15. The new handset is IP69K-certified against moisture and dust, meaning it can withstand heated water shot at it at pressure. Like the OP15, the 15R trades OnePlus' old Alert Slider for a new Plus Key. It functions like the iPhone's Action button, allowing you to add a shortcut for a favorite feature. For example, you can configure it to open the camera app or act as a do not disturb toggle, among a few other options. One departure from the OP15 is that the 15R has a larger 6.83-inch display, making it slightly taller than its sibling. OnePlus is marketing this as one reason buyers might pick the 15R over the OP15, but holding the phones side by side, there's not much difference between the two. They're both big, and you'll either like that or won't. On top of being big, the 15R's screen can refresh at a fast 165Hz in games. The two displays are also comparable in terms of resolution and brightness; both can push 1,800 nits of brightness. One difference I noticed is the OnePlus 15 has a warmer panel, even when the two phones are set to the same colorspace. I've reached out to OnePlus to find what might be causing the disparity, but for now it may be due to a quality control issue or oversight in the company's software. One last thing, OnePlus has upgraded the 15R to add an ultrasonic fingerprint sensor beneath the screen. This is placed in a nice spot toward the bottom third of the display, and it's fast and accurate. Performance and battery The OnePlus 15R is also slightly thinner than the OnePlus 15. Igor Bonifacic for Engadget The OnePlus 15R is the first phone in North America to arrive with Qualcomm's latest Snapdragon 8 Gen 5 chipset. Not to be confused with the Snapdragon Gen 5 Elite in the OP15, this new chipset is similar to Qualcomm's flagship system-on-a-chip but has a weaker CPU and GPU. This is reflected in benchmarks like Geekbench 6 where the OP15 handily outperforms the OP15R. It's not even close, either, with the OP15 delivering standout single- and multi-core scores of 3,773 and 11,293, while the 15R put up more modest results of 2,857 and 9,512. From that perspective, you're losing a fair amount of performance, but real-world use tells a different story. Outside of the handful of games such as Call of Duty: Mobile and PUBG that support the OP15 and 15R's 165Hz displays, the Snapdragon 8 Gen 5 offers more than enough muscle for the majority of applications. Even for most games (like the ones I like to play, including Diablo Immortal and League of Legends: Wild Rift), the Snapdragon 8 Gen 5 is a great match. OnePlus also hasn't skimped on the 15R's other internal components. You're still getting 12GB of LPDDR5X Ultra RAM and 256GB of UFS 4.1 storage. That's the same configuration as the base model of the OP15. This translates to a phone that doesn't miss a beat when switching between apps and loading files like images and videos. The 15R has a slightly bigger battery, coming in at 7,400mAh, up from 7,300mAh on the OP15. In practice, the two phones offer the same amount of battery life. Putting them through both Engadget's video rundown test, they both ran for 38 hours before their batteries died (which makes sense given the OP15R has a bigger screen). Like the OP15, the 15R comes with the OnePlus 55W SUPERVOOC charger in the box. The adapter can get the 15R from dead to 100 percent in less than an hour. If you hate charging your phone, the 15R makes that process as painless as possible, with a battery that both lasts long and won't be at the outlet for hours. Cameras A closeup of the OnePlus 15R's camera module. Igor Bonifacic for Engadget By this point you're probably wondering what OnePlus cut from the OP15 to make the 15R more affordable. The answer — quite literally — is an entire camera. The new phone is missing a telephoto camera, something you could find on its predecessor, the OnePlus 13R. And as far as I can tell, the two remaining cameras use the same 50-megapixel and 8MP sensors OnePlus shipped on last year's model. The company also hasn't upgraded the glass on either camera. That leaves the selfie camera as the only area to see some change in the form of a sharper 32MP sensor and the addition of autofocus. Unfortunately, none of the 15R's cameras stand out. As a whole, they suffer from the same set of problems that plague the OnePlus 15's cameras. They're fine out on a sunny day, but as soon as the light becomes a bit challenging, the 15R struggles with shadow details, resulting in muddy pictures. The more I've used both the OP15 and 15R, the more I've come to the conclusion that OnePlus needs to go back to the drawing board with its new Detail Max Engine. It feels like it's holding back what should, at least on paper, be solid hardware. Software Despite it's large size, the OnePlus 15R isn't too heavy. Igor Bonifacic for Engadget There's not much to say here other than the 15R ships with OxygenOS 16, just like the OP15. OnePlus has also promised to support the 15R for the same amount of time as the OP15: four years with software updates and six years with security patches. That's a shorter window than Google and Samsung, both of which promise seven years on all their latest phones. It's hopefully something that OnePlus decides to change starting with the OnePlus 16. The reason I bring that up is that the company’s version of Android is one I like a lot. OxygenOS is slick, with animations that highlight the speed of the 15R's processor and display. The fact the phone comes with the latest version of OxygenOS means you also get access to all of the company's newest AI features, including its Mind Space hub where you can save screenshots and notes for an on-device model to transcribe and summarize. Wrap-up The OnePlus 15R comes in a lovely mint color. Igor Bonifacic for Engadget In short, the OnePlus 15R is the phone for people who don't care about photos and videos. That's the same conclusion I came to with the OP15. If you're a OnePlus fan, the 15R excels in all the areas you would expect the company's devices to make a good showing: performance, battery life and display responsiveness. Given I wasn't too impressed with the OP15's camera, I would actually recommend the 15R over that model. For $200 off the starting price of the OP15, you're getting a device that has almost all of the same strengths of its more expensive sibling. Compared to other phones in its price range, such as the Pixel 10 and Galaxy S25 FE, the 15R is not as well-rounded, and can't compete with those devices in camera quality, but you're getting much better performance, battery life and a display they can't match. This article originally appeared on Engadget at https://www.engadget.com/mobile/smartphones/oneplus-15r-review-a-165hz-display-and-big-battery-for-700-150000340.html?src=rss",
          "feed_position": 18,
          "image_url": "https://d29szjachogqwa.cloudfront.net/images/user-uploaded/oneplus-15r-4.jpg"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/cybersecurity/vpn/how-a-vpn-works-and-why-you-should-care-143000250.html",
          "published_at": "Wed, 17 Dec 2025 14:30:00 +0000",
          "title": "How a VPN works (and why you should care)",
          "standfirst": "The best VPNs can make your online life more private with software that's convenient and cheap — sometimes even free. While keeping your IP address invisible, you can use your VPN to explore streaming content from all over the world or (virtually) sneak into a sports event that's not available in your area.However, while VPNs are widely available, there's a strange dearth of information on what they actually do behind the scenes. You may know that a VPN masks your device with a proxy server to make it look like you're somewhere else, and maybe even that encryption is involved. But finding any more details can mean running a gauntlet of misinformation.That's a shame, because the inner workings of a VPN aren't all that difficult to understand. You may not be able to build one yourself without a degree in computer science, but with a little work, you can understand exactly what it's doing on your computer. That's information you can use to select the right VPN for you, and make the most of it once you've got it.What is a VPN?To make sure nobody gets left behind, I'll start from the beginning. A VPN (virtual private network) is a method of securely accessing a network, either a closed network (like you might have at the office) or the internet as a whole. Initially, organizations set up VPNs so remote workers can work with secure files. While this still happens, the last 15 years have seen VPNs increasingly marketed to individuals, with Proton VPN, ExpressVPN and others seeing massive user growth.Broadly, a VPN consists of two parts: the server, which forwards requests to your chosen destination, and the client, a piece of software that lets you interact with the server. You can find a longer explanation here, but I'll use the two sections below to tell you what you need to know right now.One more note before that — there are multiple kinds of VPNs, including the remote-access VPNs and site-to-site VPNs commonly used by workplaces. However, for this article, I'll be talking mainly about the commercial VPN services sold to individuals for general security needs. Instead of a specific network, these VPNs are designed to handle all of a user's traffic to any point on the internet.What happens when you use a VPN?First, you use the client to connect to a server — either the fastest one available or a particular location you need. Once you've connected, every request you send to the internet goes through the VPN server first. This communication between your device and the web is encrypted so it can't be traced back to you.The VPN server decrypts your requests and sends them on. The destination then communicates with the VPN server, which relays the information back to you — after re-encrypting it so nobody follows it home.Since the VPN does everything on your behalf, it's your \"mask\" online. Your internet service provider (ISP) and third parties can see what's being done, but — so long as you’re not otherwise logged in or identifying yourself — nobody knows that it's you doing it. It's like having a friend order pizza for you so the pizzeria doesn't hear you calling for the third time this week (not that I speak from experience).What's the point of using a VPN?Why add an extra step to the already complex process of getting online? The two biggest reasons are maintaining anonymity and changing your virtual location. I've already explained how a VPN keeps you anonymous. Among other things, this prevents your ISP from selling your browsing history to advertisers and protects activists who face government repercussions for what they do online.Changing your virtual location is part of masking, but it can also be used to see the internet as it's visible in other countries. Streaming services are frequently limited to certain places, and almost all of them change the available content based on their licenses in each nation. You can also use a VPN in a country with a nationwide firewall, like China, to see forbidden outside information sources.How does a VPN work? The full technical explanationMost online explanations stop after defining a VPN as an anonymous agent between you and the internet — but I wrote this article to go a little bit deeper. To understand what a VPN is doing on a technical level, we'll need to cover how the internet works, how the VPN knows where to send encrypted information and just what \"encryption\" actually is.How the internet transmits dataWhen you're not using a VPN, internet traffic goes directly from your modem to your ISP, then on to your chosen destination. The key technologies here are IP, which stands for Internet Protocol, and TCP, which stands for Transmission Control Protocol. They're usually combined as TCP/IP.You may have heard that every online device has an IP address that identifies it to every other device. TCP/IP governs not just those names but how data moves between them. Here's how it works, step-by-step.You click a link or enter a URL into your web browser.Your computer sends a request to your modem, asking to see the page associated with the URL. Your modem forwards the request to your ISP.Your ISP finds a domain name server (DNS) that tells it which IP address is connected to the URL you asked to see. It then sends the request to that IP address along the fastest available route, which will involve being relayed between several nodes.That IP address is connected with a server that holds the content you're looking for. Once it receives the request, it breaks the data down into small packets of about 1 to 1.5 kilobytes.These packets separate to find their own fastest routes back to your ISP, your modem and finally your web browser, which reassembles them.You see a web page, likely no more than a second after you asked for it.The outgoing requests and inbound packets are key to understanding VPN function. A VPN intervenes during step 2 (when your modem contacts your ISP) and step 5 (when your ISP sends the packets back to you). In the next section, I'll explain exactly what it does during those steps.How VPN tunneling protects dataYou might have heard a VPN's activities described as \"tunneling.\" That term refers to a figurative tunnel being created between your device and the VPN. Data enters the tunnel when it's encrypted by the VPN client and exits when it's decrypted by the VPN server. Between those two points, encryption means nobody can see the true data. It's as though it's traveling through an opaque tunnel.While the tunnel is a useful metaphor, it may be better to think of VPN encryption as an encapsulation. Each packet of data sent via VPN is \"wrapped\" in a second packet, which both encrypts the original packet and contains information for reaching the VPN server. However, none of these outer layers have the complete path — each just knows enough to reach the next relay. In this way, the origin point (that's you) remains invisible.The same thing happens when the internet returns content to show you. Your ISP sends the data to the VPN server, because, as far as it knows, that's where the request came from. The VPN then encrypts each packet and sends them back to you for decryption and reassembly. It takes a little longer with the extra steps; that's why VPNs always slightly slow down your browsing speed, though the best ones don't do that by much (Surfshark is currently the fastest).You learned in that last section that two protocols, IP and TCP (usually combined as TCP/IP), are responsible for letting online devices talk to each other, even if they've never connected before. In the same way, a VPN protocol is like a shared language that lets VPNs encrypt, move and decrypt information. See the next section to learn how a VPN protocol works in detail.How VPN protocols encrypt dataVPN protocols are the technology behind VPNs; every other feature of your VPN is just a method of interacting with them. All protocols are designed to encrypt data packets and wrap them in a second layer that includes information on where to send them. The main differences are the shape of that second layer, the types of encryption used and how the client establishes its initial secure connection with the server.It's extremely common for VPNs to advertise protocols with \"bank-grade\" or \"military-grade\" encryption. This is talking about the 256-bit Advanced Encryption Standard (AES-256), a symmetric encryption algorithm, which is used by financial institutions and the US government and military. AES-256 is indeed some of the strongest available encryption, but it's only part of the story. As a symmetric algorithm, it's not fully secure on its own, because the same keys are used to encrypt and decrypt it — and those keys can be stolen.For that reason, most VPN protocols use AES-256 (or a similarly strong cipher like ChaCha20) to encrypt the data packets themselves, then combine it with a larger suite of multiple encryption algorithms. One of the most reliable and popular protocols, OpenVPN, uses the asymmetric TLS protocol to establish a secure relationship between client and server, then transmits packets encrypted with AES-256 across that channel, knowing the keys will be safe.Explaining this could easily reach the length of a book, but the basic principle isn't complicated. In asymmetric encryption, a sender encodes data with a unique key, then a recipient decodes it with a different paired key. The keys are provided by a trusted third party. In a maneuver called a TLS handshake, the server and client send each other encrypted data. If each can decode the other's test data, they know they have a matched pair of keys, which proves that both are the same client and server that got the keys from the trusted authority.Why not just use asymmetric encryption for the data itself, if it's more secure? Mainly, protocols don't do this because it's a lot slower. Asymmetric encryption requires a lot of resource-heavy math that makes connections drag. That's why OpenVPN and others use the asymmetric-to-symmetric two-step instead.To summarize, a VPN protocol is a complex set of instructions and tools that control encryption and routing via VPN servers. Protocols still in use include OpenVPN, WireGuard, IKEv2, SSTP and L2TP. PPTP, one of the oldest protocols, is no longer considered secure. On top of these, VPNs often build their own proprietary protocols, such as ExpressVPN's Lightway.Putting it all togetherNow that we've hit all the relevant information, let's revisit that step-by-step from earlier, this time with a VPN in the mix. Here are the steps, starting with establishing the VPN connection and ending with anonymously viewing a website.You open your VPN client, choose a server location and connect. The VPN client and server authenticate each other with a TLS handshake.The client and server exchange the symmetric keys they'll use to encrypt and decrypt packets for the duration of this session (i.e. until you disconnect). Your VPN client tells you that it's established a secure tunnel.You open your web browser and enter a URL. Your browser sends a request to view the content at that address.The request goes to your VPN client, which encrypts it and adds an outer layer of information with directions to the VPN server.The encrypted request reaches the VPN server, which decrypts it and forwards it to your ISP.As normal, your ISP finds the IP address associated with the URL you entered and forwards your request along.The destination server receives the request and sends all the necessary packets of information back to your ISP, which forwards it to the VPN server.The VPN server encrypts each packet and adds a header directing it to the VPN client.The client decrypts the packets and forwards them to your web browser.You see the web page you opened.Because of the encrypted tunnel, the request arrives at the VPN server without any information on where it came from. Thus, the VPN doesn't actually encrypt your activity on the websites themselves — for the most part, the HTTPS protocol does that. Instead, a VPN gives you a false name to put in the register, with no information that could be traced back to your real identity.How to use this informationNow that you know how a VPN works on a technical level, you're better equipped to choose one for yourself. You can cut through marketing hype statements like:\"Military-grade encryption!\" (It's the same algorithm everybody uses)\"Stay completely anonymous online!\" (Plaintext you post on social media is not encrypted)\"Dodge ISP throttling!\" (If your ISP is throttling you based on your IP address, this works — but if you're being slowed down because of your moment-to-moment activity, your identity doesn't matter)A VPN is just one important part of a complete cybersecurity breakfast. While hiding your IP address, make sure to also use strong passwords, download updates immediately and remain alert for social engineering tactics.This article originally appeared on Engadget at https://www.engadget.com/cybersecurity/vpn/how-a-vpn-works-and-why-you-should-care-143000250.html?src=rss",
          "content": "The best VPNs can make your online life more private with software that's convenient and cheap — sometimes even free. While keeping your IP address invisible, you can use your VPN to explore streaming content from all over the world or (virtually) sneak into a sports event that's not available in your area.However, while VPNs are widely available, there's a strange dearth of information on what they actually do behind the scenes. You may know that a VPN masks your device with a proxy server to make it look like you're somewhere else, and maybe even that encryption is involved. But finding any more details can mean running a gauntlet of misinformation.That's a shame, because the inner workings of a VPN aren't all that difficult to understand. You may not be able to build one yourself without a degree in computer science, but with a little work, you can understand exactly what it's doing on your computer. That's information you can use to select the right VPN for you, and make the most of it once you've got it.What is a VPN?To make sure nobody gets left behind, I'll start from the beginning. A VPN (virtual private network) is a method of securely accessing a network, either a closed network (like you might have at the office) or the internet as a whole. Initially, organizations set up VPNs so remote workers can work with secure files. While this still happens, the last 15 years have seen VPNs increasingly marketed to individuals, with Proton VPN, ExpressVPN and others seeing massive user growth.Broadly, a VPN consists of two parts: the server, which forwards requests to your chosen destination, and the client, a piece of software that lets you interact with the server. You can find a longer explanation here, but I'll use the two sections below to tell you what you need to know right now.One more note before that — there are multiple kinds of VPNs, including the remote-access VPNs and site-to-site VPNs commonly used by workplaces. However, for this article, I'll be talking mainly about the commercial VPN services sold to individuals for general security needs. Instead of a specific network, these VPNs are designed to handle all of a user's traffic to any point on the internet.What happens when you use a VPN?First, you use the client to connect to a server — either the fastest one available or a particular location you need. Once you've connected, every request you send to the internet goes through the VPN server first. This communication between your device and the web is encrypted so it can't be traced back to you.The VPN server decrypts your requests and sends them on. The destination then communicates with the VPN server, which relays the information back to you — after re-encrypting it so nobody follows it home.Since the VPN does everything on your behalf, it's your \"mask\" online. Your internet service provider (ISP) and third parties can see what's being done, but — so long as you’re not otherwise logged in or identifying yourself — nobody knows that it's you doing it. It's like having a friend order pizza for you so the pizzeria doesn't hear you calling for the third time this week (not that I speak from experience).What's the point of using a VPN?Why add an extra step to the already complex process of getting online? The two biggest reasons are maintaining anonymity and changing your virtual location. I've already explained how a VPN keeps you anonymous. Among other things, this prevents your ISP from selling your browsing history to advertisers and protects activists who face government repercussions for what they do online.Changing your virtual location is part of masking, but it can also be used to see the internet as it's visible in other countries. Streaming services are frequently limited to certain places, and almost all of them change the available content based on their licenses in each nation. You can also use a VPN in a country with a nationwide firewall, like China, to see forbidden outside information sources.How does a VPN work? The full technical explanationMost online explanations stop after defining a VPN as an anonymous agent between you and the internet — but I wrote this article to go a little bit deeper. To understand what a VPN is doing on a technical level, we'll need to cover how the internet works, how the VPN knows where to send encrypted information and just what \"encryption\" actually is.How the internet transmits dataWhen you're not using a VPN, internet traffic goes directly from your modem to your ISP, then on to your chosen destination. The key technologies here are IP, which stands for Internet Protocol, and TCP, which stands for Transmission Control Protocol. They're usually combined as TCP/IP.You may have heard that every online device has an IP address that identifies it to every other device. TCP/IP governs not just those names but how data moves between them. Here's how it works, step-by-step.You click a link or enter a URL into your web browser.Your computer sends a request to your modem, asking to see the page associated with the URL. Your modem forwards the request to your ISP.Your ISP finds a domain name server (DNS) that tells it which IP address is connected to the URL you asked to see. It then sends the request to that IP address along the fastest available route, which will involve being relayed between several nodes.That IP address is connected with a server that holds the content you're looking for. Once it receives the request, it breaks the data down into small packets of about 1 to 1.5 kilobytes.These packets separate to find their own fastest routes back to your ISP, your modem and finally your web browser, which reassembles them.You see a web page, likely no more than a second after you asked for it.The outgoing requests and inbound packets are key to understanding VPN function. A VPN intervenes during step 2 (when your modem contacts your ISP) and step 5 (when your ISP sends the packets back to you). In the next section, I'll explain exactly what it does during those steps.How VPN tunneling protects dataYou might have heard a VPN's activities described as \"tunneling.\" That term refers to a figurative tunnel being created between your device and the VPN. Data enters the tunnel when it's encrypted by the VPN client and exits when it's decrypted by the VPN server. Between those two points, encryption means nobody can see the true data. It's as though it's traveling through an opaque tunnel.While the tunnel is a useful metaphor, it may be better to think of VPN encryption as an encapsulation. Each packet of data sent via VPN is \"wrapped\" in a second packet, which both encrypts the original packet and contains information for reaching the VPN server. However, none of these outer layers have the complete path — each just knows enough to reach the next relay. In this way, the origin point (that's you) remains invisible.The same thing happens when the internet returns content to show you. Your ISP sends the data to the VPN server, because, as far as it knows, that's where the request came from. The VPN then encrypts each packet and sends them back to you for decryption and reassembly. It takes a little longer with the extra steps; that's why VPNs always slightly slow down your browsing speed, though the best ones don't do that by much (Surfshark is currently the fastest).You learned in that last section that two protocols, IP and TCP (usually combined as TCP/IP), are responsible for letting online devices talk to each other, even if they've never connected before. In the same way, a VPN protocol is like a shared language that lets VPNs encrypt, move and decrypt information. See the next section to learn how a VPN protocol works in detail.How VPN protocols encrypt dataVPN protocols are the technology behind VPNs; every other feature of your VPN is just a method of interacting with them. All protocols are designed to encrypt data packets and wrap them in a second layer that includes information on where to send them. The main differences are the shape of that second layer, the types of encryption used and how the client establishes its initial secure connection with the server.It's extremely common for VPNs to advertise protocols with \"bank-grade\" or \"military-grade\" encryption. This is talking about the 256-bit Advanced Encryption Standard (AES-256), a symmetric encryption algorithm, which is used by financial institutions and the US government and military. AES-256 is indeed some of the strongest available encryption, but it's only part of the story. As a symmetric algorithm, it's not fully secure on its own, because the same keys are used to encrypt and decrypt it — and those keys can be stolen.For that reason, most VPN protocols use AES-256 (or a similarly strong cipher like ChaCha20) to encrypt the data packets themselves, then combine it with a larger suite of multiple encryption algorithms. One of the most reliable and popular protocols, OpenVPN, uses the asymmetric TLS protocol to establish a secure relationship between client and server, then transmits packets encrypted with AES-256 across that channel, knowing the keys will be safe.Explaining this could easily reach the length of a book, but the basic principle isn't complicated. In asymmetric encryption, a sender encodes data with a unique key, then a recipient decodes it with a different paired key. The keys are provided by a trusted third party. In a maneuver called a TLS handshake, the server and client send each other encrypted data. If each can decode the other's test data, they know they have a matched pair of keys, which proves that both are the same client and server that got the keys from the trusted authority.Why not just use asymmetric encryption for the data itself, if it's more secure? Mainly, protocols don't do this because it's a lot slower. Asymmetric encryption requires a lot of resource-heavy math that makes connections drag. That's why OpenVPN and others use the asymmetric-to-symmetric two-step instead.To summarize, a VPN protocol is a complex set of instructions and tools that control encryption and routing via VPN servers. Protocols still in use include OpenVPN, WireGuard, IKEv2, SSTP and L2TP. PPTP, one of the oldest protocols, is no longer considered secure. On top of these, VPNs often build their own proprietary protocols, such as ExpressVPN's Lightway.Putting it all togetherNow that we've hit all the relevant information, let's revisit that step-by-step from earlier, this time with a VPN in the mix. Here are the steps, starting with establishing the VPN connection and ending with anonymously viewing a website.You open your VPN client, choose a server location and connect. The VPN client and server authenticate each other with a TLS handshake.The client and server exchange the symmetric keys they'll use to encrypt and decrypt packets for the duration of this session (i.e. until you disconnect). Your VPN client tells you that it's established a secure tunnel.You open your web browser and enter a URL. Your browser sends a request to view the content at that address.The request goes to your VPN client, which encrypts it and adds an outer layer of information with directions to the VPN server.The encrypted request reaches the VPN server, which decrypts it and forwards it to your ISP.As normal, your ISP finds the IP address associated with the URL you entered and forwards your request along.The destination server receives the request and sends all the necessary packets of information back to your ISP, which forwards it to the VPN server.The VPN server encrypts each packet and adds a header directing it to the VPN client.The client decrypts the packets and forwards them to your web browser.You see the web page you opened.Because of the encrypted tunnel, the request arrives at the VPN server without any information on where it came from. Thus, the VPN doesn't actually encrypt your activity on the websites themselves — for the most part, the HTTPS protocol does that. Instead, a VPN gives you a false name to put in the register, with no information that could be traced back to your real identity.How to use this informationNow that you know how a VPN works on a technical level, you're better equipped to choose one for yourself. You can cut through marketing hype statements like:\"Military-grade encryption!\" (It's the same algorithm everybody uses)\"Stay completely anonymous online!\" (Plaintext you post on social media is not encrypted)\"Dodge ISP throttling!\" (If your ISP is throttling you based on your IP address, this works — but if you're being slowed down because of your moment-to-moment activity, your identity doesn't matter)A VPN is just one important part of a complete cybersecurity breakfast. While hiding your IP address, make sure to also use strong passwords, download updates immediately and remain alert for social engineering tactics.This article originally appeared on Engadget at https://www.engadget.com/cybersecurity/vpn/how-a-vpn-works-and-why-you-should-care-143000250.html?src=rss",
          "feed_position": 19
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/infrastructure/mistral-launches-ocr-3-to-digitize-enterprise-documents-touts-74-win-rate",
          "published_at": "Wed, 17 Dec 2025 14:00:00 GMT",
          "title": "Mistral launches OCR 3 to digitize enterprise documents, touts 74% win rate and $2-per-1,000-page pricing",
          "standfirst": "Mistral AI, the French artificial intelligence company valued at €11.7 billion, unveiled its third-generation optical character recognition model on Tuesday, positioning document digitization as the critical first step enterprises must take before realizing the full potential of generative AI.The new model, called Mistral OCR 3, claims a 74% win rate against competing products when processing forms, scanned documents, complex tables, and handwritten content. Mistral priced the technology aggressively at $2 per 1,000 pages — with a 50% discount for batch processing — dramatically undercutting many established enterprise document processing solutions.The release arrives at a pivotal moment for the two-year-old startup. Mistral has spent December on an aggressive product offensive, launching its Mistral 3 family of open-weight models, new coding tools called Devstral 2, and now OCR 3. The company faces intensifying pressure from American rivals flush with capital — OpenAI recently sold secondary shares at a reported $500 billion valuation, while Anthropic raised $13 billion in September — and potential regulatory friction as the Trump administration threatens retaliation against European companies over EU technology laws.Why enterprises can&#x27;t adopt AI until they solve their paper problemMarjorie Janiewicz, Mistral&#x27;s Chief Revenue Officer who oversees global revenue including solutions architecture and forward deployment engineering, framed the OCR release as a direct response to patterns the company observed while helping enterprises deploy AI over the past year.\"A lot of very large enterprises are still sitting on a very large volume of critical data that&#x27;s not digitized yet,\" Janiewicz said in an exclusive interview with VentureBeat. \"That data that&#x27;s not digitized represents a massive competitive moat.\"The observation cuts to the heart of a widely documented problem in enterprise AI adoption. Despite billions invested in AI initiatives, most organizations struggle to move beyond proof-of-concept projects into production systems that generate measurable returns. Research consistently shows a significant gap between AI experimentation and real business value.Janiewicz argued that document digitization creates two distinct opportunities. First, it unlocks institutional knowledge accumulated over decades — proprietary data that could power personalized AI systems and agents. Second, it enables the workflow automation that promises to transform day-to-day operations but remains stalled in document-heavy industries.\"When you think about workflow transformation, a lot of enterprises today could benefit from really transformational workflow automation if the data that was core to their business was fully digitized,\" Janiewicz explained.From anti-money laundering to insurance claims, how OCR transforms regulated industriesMistral designed OCR 3 to excel across the regulated, document-intensive industries where AI adoption has proven most challenging — and where the stakes for accuracy are highest.In financial services, Janiewicz pointed to anti-money laundering compliance and know-your-customer processes, where banks process millions of documents annually to meet regulatory requirements. \"When you think about opening a bank account, or a lot of the tasks that are still being done in retail banks, it&#x27;s on paper,\" she said. \"When you start correlating that to anti-money laundering workflow automation processes, or KYC as a customer support process, where governance and being able to inspect things is so essential — a lot of the banks are talking to us about the need to accelerate the pace, the accuracy and the performance of the digitization process.\"The insurance industry presents similar challenges. Claim management workflows require connecting photographs of vehicle damage, handwritten accident reports, and policy documentation to automated processing engines. Healthcare organizations grapple with admission forms, medical histories, prescription records, and consent documentation scattered across paper and digital formats.Manufacturing drew particular enthusiasm from Janiewicz. \"I love manufacturing as an industry,\" she said. \"When you start thinking about the very complex technical documents, many of those documents are either not digitized yet, or they are so complex that extracting valuable information from them to accelerate the manufacturing process, or even innovation, is a challenge.\"Mistral claims major accuracy gains on handwriting, complex tables, and damaged scansAccording to Mistral&#x27;s benchmarks, OCR 3 demonstrates significant improvements over its predecessor across several categories that have historically challenged optical character recognition systems.The model interprets cursive handwriting, mixed-content annotations, and handwritten text layered over printed forms — scenarios that frequently produce errors in traditional OCR systems. It reconstructs complex table structures with headers, merged cells, multi-row blocks, and column hierarchies, outputting HTML table tags that preserve layout for downstream processing.Perhaps most notably for organizations dealing with legacy documents, Mistral claims substantial improvements in handling the artifacts that plague real-world document processing: compression artifacts, skew, distortion, low resolution, and background noise.Tim Law, IDC&#x27;s Director of Research for AI and Automation, underscored the strategic importance of the technology. \"OCR remains foundational for enabling generative AI and agentic AI,\" Law said. \"Those organizations that can efficiently and cost-effectively extract text and embedded images with high fidelity will unlock value and will gain a competitive advantage from their data by providing richer context.\"When asked what prevents well-funded competitors from replicating Mistral&#x27;s approach within months, Janiewicz emphasized the accuracy gap that has frustrated enterprise deployments.\"Enterprises have two and a half years of history with competitive OCR solutions, and the reason we think this is a real advantage for us is accuracy,\" she said. \"Many enterprises are complaining about the accuracy of those systems, which has slowed their ability to digitize their documents.\"How Mistral AI Studio creates a complete document-to-production pipelineBeyond raw model performance, Mistral positioned OCR 3 as part of a vertically integrated stack designed for complex enterprise deployments. The model operates within Document AI, a component of Mistral AI Studio that the company introduced in October as its production platform for enterprise AI development.Mistral AI Studio provides observability, agent runtime capabilities, and an AI registry — infrastructure Janiewicz described as essential for moving AI from experimentation to reliable production systems. OCR 3 feeds directly into this ecosystem, connecting document processing to the company&#x27;s broader model offerings and workflow tools.\"It&#x27;s the vertical integration of OCR, the models, and Studio, coupled with accuracy, that I think is creating a very differentiated play,\" Janiewicz said. \"Most companies today are struggling with off-the-shelf solutions not being good enough to help them transform a complex workflow.\"The release supports deployment across cloud, virtual private cloud, and on-premises environments — flexibility that matters enormously for regulated industries where data sovereignty and security concerns dictate infrastructure decisions.Keeping enterprise data &#x27;home&#x27; in an era of AI security concernsFor financial services, healthcare, and other heavily regulated industries, questions about data handling during AI processing carry significant weight. Janiewicz addressed these concerns directly.\"Many times the models are going to be used on their own GPUs,\" she said, referring to on-premises and VPC deployments. \"That&#x27;s a great way to make sure companies feel that the data is home — it&#x27;s not going to be exposed to anyone else.\"On the sensitive question of training data, Janiewicz was unequivocal: \"For all our training, we never use our customers&#x27; data to train.\"The company announced a partnership with HSBC in recent weeks to build productivity tools for the multinational bank — a significant validation of Mistral&#x27;s enterprise security posture in one of the world&#x27;s most demanding regulatory environments.Mistral&#x27;s December product blitz signals an aggressive push against OpenAI and AnthropicThe OCR 3 release extends Mistral&#x27;s December product blitz, which began when the company launched its Mistral 3 family of open-weight models on December 2. That release included Mistral Large 3, a frontier model with multimodal and multilingual capabilities, alongside nine smaller Ministral 3 models designed for edge deployment on devices with limited connectivity.The company followed up a week later with Devstral 2, a new generation of coding models, and Mistral Vibe, a command-line interface for code automation through natural language — a direct play for the \"vibe coding\" market that has fueled the rise of companies like Cursor.These releases build on substantial infrastructure partnerships. Microsoft distributes Mistral models through Azure Foundry, with OCR 3 expected to become available on the platform. Amazon Web Services added Mistral Large 3 and Ministral 3 models to Amazon Bedrock in early December, providing fully managed access alongside models from Google, OpenAI, and others.Mistral&#x27;s roughly $2 billion (€1.7 billion) Series C round in September, led by Dutch semiconductor equipment maker ASML with participation from NVIDIA, DST Global, and Andreessen Horowitz, gave the company resources to accelerate development. But the funding pales against American competitors — OpenAI sold secondary shares in October at a $500 billion valuation, making it the world&#x27;s most valuable private company, while Anthropic reached a $350 billion valuation in November following investments from Microsoft and Nvidia.Guillaume Lample, Mistral&#x27;s co-founder and chief scientist, has argued that bigger isn&#x27;t always better for enterprise use cases. \"In practice, the huge majority of enterprise use cases are things that can be tackled by small models, especially if you fine-tune them,\" Lample said in a recent interview with TechCrunch.Janiewicz echoed this philosophy. \"The biggest learning over the past 12 months is that off-the-shelf AI is not cutting it in driving real value for the enterprise in production,\" she said. \"Customization of the models, customization of the technology, giving control back to enterprises to build their own AI solutions — that&#x27;s absolutely paramount.\"US-EU technology tensions create new risks for European AI companiesMistral&#x27;s aggressive expansion comes as European technology companies face potential regulatory retaliation from the United States. The Trump administration warned last week that it would use \"every tool at its disposal\" if the European Union continued enforcing its technology laws, putting companies including Mistral, Spotify, Siemens, and Publicis in a precarious position.The European Commission responded that its rules \"apply equally and fairly to all companies operating in the EU,\" but the standoff introduces uncertainty for European AI companies seeking American enterprise customers.Mistral has differentiated itself from Chinese competitors like DeepSeek and Alibaba&#x27;s Qwen by emphasizing its Apache 2.0 licensing and worldwide availability without regional restrictions — a positioning that takes on added significance amid escalating technology tensions between major economic blocs.Aggressive pricing suggests Mistral sees OCR as a gateway to deeper enterprise relationshipsJaniewicz outlined three revenue pillars for Mistral: complex workflow transformation using Mistral Studio and forward deployment engineering; research and development partnerships to co-build specialized models; and productivity tools including the Le Chat assistant and Mistral Code for developers.Document AI and OCR fit into the first pillar while potentially serving as an entry point that leads customers into deeper engagements. \"OCR is a great way to get those enterprises started and being able to start showing some concrete results,\" Janiewicz said.The aggressive pricing — significantly below many enterprise document processing alternatives — suggests Mistral views OCR as a wedge product rather than a primary profit center. Early customers use the technology to process invoices into structured fields, digitize corporate archives, extract clean text from technical and scientific reports, and improve enterprise search.The company also highlighted accessibility applications. AI-powered OCR can transform printed, handwritten, or scanned documents into searchable digital formats compatible with screen readers and assistive technologies — a capability with implications for compliance with disability access requirements in education and government.The unsexy problem that could determine who wins the enterprise AI raceMistral&#x27;s OCR 3 is a calculated wager that the path to enterprise AI dominance runs not through ever-larger language models, but through the unglamorous work of converting paper into data. While competitors race to build more powerful chatbots and autonomous agents, the French startup is betting that enterprises can&#x27;t use any of those tools until they first digitize the institutional knowledge buried in filing cabinets and PDF archives.\"For us, OCR is a great way to get those enterprises started and being able to start showing some concrete results,\" Janiewicz said. \"To us, really, the key message is customization, portability, and control is the secret sauce to ROI.\"The model becomes available Tuesday through Mistral&#x27;s API and the Document AI interface in Mistral AI Studio. Developers can access it using the identifier mistral-ocr-2512.",
          "content": "Mistral AI, the French artificial intelligence company valued at €11.7 billion, unveiled its third-generation optical character recognition model on Tuesday, positioning document digitization as the critical first step enterprises must take before realizing the full potential of generative AI.The new model, called Mistral OCR 3, claims a 74% win rate against competing products when processing forms, scanned documents, complex tables, and handwritten content. Mistral priced the technology aggressively at $2 per 1,000 pages — with a 50% discount for batch processing — dramatically undercutting many established enterprise document processing solutions.The release arrives at a pivotal moment for the two-year-old startup. Mistral has spent December on an aggressive product offensive, launching its Mistral 3 family of open-weight models, new coding tools called Devstral 2, and now OCR 3. The company faces intensifying pressure from American rivals flush with capital — OpenAI recently sold secondary shares at a reported $500 billion valuation, while Anthropic raised $13 billion in September — and potential regulatory friction as the Trump administration threatens retaliation against European companies over EU technology laws.Why enterprises can&#x27;t adopt AI until they solve their paper problemMarjorie Janiewicz, Mistral&#x27;s Chief Revenue Officer who oversees global revenue including solutions architecture and forward deployment engineering, framed the OCR release as a direct response to patterns the company observed while helping enterprises deploy AI over the past year.\"A lot of very large enterprises are still sitting on a very large volume of critical data that&#x27;s not digitized yet,\" Janiewicz said in an exclusive interview with VentureBeat. \"That data that&#x27;s not digitized represents a massive competitive moat.\"The observation cuts to the heart of a widely documented problem in enterprise AI adoption. Despite billions invested in AI initiatives, most organizations struggle to move beyond proof-of-concept projects into production systems that generate measurable returns. Research consistently shows a significant gap between AI experimentation and real business value.Janiewicz argued that document digitization creates two distinct opportunities. First, it unlocks institutional knowledge accumulated over decades — proprietary data that could power personalized AI systems and agents. Second, it enables the workflow automation that promises to transform day-to-day operations but remains stalled in document-heavy industries.\"When you think about workflow transformation, a lot of enterprises today could benefit from really transformational workflow automation if the data that was core to their business was fully digitized,\" Janiewicz explained.From anti-money laundering to insurance claims, how OCR transforms regulated industriesMistral designed OCR 3 to excel across the regulated, document-intensive industries where AI adoption has proven most challenging — and where the stakes for accuracy are highest.In financial services, Janiewicz pointed to anti-money laundering compliance and know-your-customer processes, where banks process millions of documents annually to meet regulatory requirements. \"When you think about opening a bank account, or a lot of the tasks that are still being done in retail banks, it&#x27;s on paper,\" she said. \"When you start correlating that to anti-money laundering workflow automation processes, or KYC as a customer support process, where governance and being able to inspect things is so essential — a lot of the banks are talking to us about the need to accelerate the pace, the accuracy and the performance of the digitization process.\"The insurance industry presents similar challenges. Claim management workflows require connecting photographs of vehicle damage, handwritten accident reports, and policy documentation to automated processing engines. Healthcare organizations grapple with admission forms, medical histories, prescription records, and consent documentation scattered across paper and digital formats.Manufacturing drew particular enthusiasm from Janiewicz. \"I love manufacturing as an industry,\" she said. \"When you start thinking about the very complex technical documents, many of those documents are either not digitized yet, or they are so complex that extracting valuable information from them to accelerate the manufacturing process, or even innovation, is a challenge.\"Mistral claims major accuracy gains on handwriting, complex tables, and damaged scansAccording to Mistral&#x27;s benchmarks, OCR 3 demonstrates significant improvements over its predecessor across several categories that have historically challenged optical character recognition systems.The model interprets cursive handwriting, mixed-content annotations, and handwritten text layered over printed forms — scenarios that frequently produce errors in traditional OCR systems. It reconstructs complex table structures with headers, merged cells, multi-row blocks, and column hierarchies, outputting HTML table tags that preserve layout for downstream processing.Perhaps most notably for organizations dealing with legacy documents, Mistral claims substantial improvements in handling the artifacts that plague real-world document processing: compression artifacts, skew, distortion, low resolution, and background noise.Tim Law, IDC&#x27;s Director of Research for AI and Automation, underscored the strategic importance of the technology. \"OCR remains foundational for enabling generative AI and agentic AI,\" Law said. \"Those organizations that can efficiently and cost-effectively extract text and embedded images with high fidelity will unlock value and will gain a competitive advantage from their data by providing richer context.\"When asked what prevents well-funded competitors from replicating Mistral&#x27;s approach within months, Janiewicz emphasized the accuracy gap that has frustrated enterprise deployments.\"Enterprises have two and a half years of history with competitive OCR solutions, and the reason we think this is a real advantage for us is accuracy,\" she said. \"Many enterprises are complaining about the accuracy of those systems, which has slowed their ability to digitize their documents.\"How Mistral AI Studio creates a complete document-to-production pipelineBeyond raw model performance, Mistral positioned OCR 3 as part of a vertically integrated stack designed for complex enterprise deployments. The model operates within Document AI, a component of Mistral AI Studio that the company introduced in October as its production platform for enterprise AI development.Mistral AI Studio provides observability, agent runtime capabilities, and an AI registry — infrastructure Janiewicz described as essential for moving AI from experimentation to reliable production systems. OCR 3 feeds directly into this ecosystem, connecting document processing to the company&#x27;s broader model offerings and workflow tools.\"It&#x27;s the vertical integration of OCR, the models, and Studio, coupled with accuracy, that I think is creating a very differentiated play,\" Janiewicz said. \"Most companies today are struggling with off-the-shelf solutions not being good enough to help them transform a complex workflow.\"The release supports deployment across cloud, virtual private cloud, and on-premises environments — flexibility that matters enormously for regulated industries where data sovereignty and security concerns dictate infrastructure decisions.Keeping enterprise data &#x27;home&#x27; in an era of AI security concernsFor financial services, healthcare, and other heavily regulated industries, questions about data handling during AI processing carry significant weight. Janiewicz addressed these concerns directly.\"Many times the models are going to be used on their own GPUs,\" she said, referring to on-premises and VPC deployments. \"That&#x27;s a great way to make sure companies feel that the data is home — it&#x27;s not going to be exposed to anyone else.\"On the sensitive question of training data, Janiewicz was unequivocal: \"For all our training, we never use our customers&#x27; data to train.\"The company announced a partnership with HSBC in recent weeks to build productivity tools for the multinational bank — a significant validation of Mistral&#x27;s enterprise security posture in one of the world&#x27;s most demanding regulatory environments.Mistral&#x27;s December product blitz signals an aggressive push against OpenAI and AnthropicThe OCR 3 release extends Mistral&#x27;s December product blitz, which began when the company launched its Mistral 3 family of open-weight models on December 2. That release included Mistral Large 3, a frontier model with multimodal and multilingual capabilities, alongside nine smaller Ministral 3 models designed for edge deployment on devices with limited connectivity.The company followed up a week later with Devstral 2, a new generation of coding models, and Mistral Vibe, a command-line interface for code automation through natural language — a direct play for the \"vibe coding\" market that has fueled the rise of companies like Cursor.These releases build on substantial infrastructure partnerships. Microsoft distributes Mistral models through Azure Foundry, with OCR 3 expected to become available on the platform. Amazon Web Services added Mistral Large 3 and Ministral 3 models to Amazon Bedrock in early December, providing fully managed access alongside models from Google, OpenAI, and others.Mistral&#x27;s roughly $2 billion (€1.7 billion) Series C round in September, led by Dutch semiconductor equipment maker ASML with participation from NVIDIA, DST Global, and Andreessen Horowitz, gave the company resources to accelerate development. But the funding pales against American competitors — OpenAI sold secondary shares in October at a $500 billion valuation, making it the world&#x27;s most valuable private company, while Anthropic reached a $350 billion valuation in November following investments from Microsoft and Nvidia.Guillaume Lample, Mistral&#x27;s co-founder and chief scientist, has argued that bigger isn&#x27;t always better for enterprise use cases. \"In practice, the huge majority of enterprise use cases are things that can be tackled by small models, especially if you fine-tune them,\" Lample said in a recent interview with TechCrunch.Janiewicz echoed this philosophy. \"The biggest learning over the past 12 months is that off-the-shelf AI is not cutting it in driving real value for the enterprise in production,\" she said. \"Customization of the models, customization of the technology, giving control back to enterprises to build their own AI solutions — that&#x27;s absolutely paramount.\"US-EU technology tensions create new risks for European AI companiesMistral&#x27;s aggressive expansion comes as European technology companies face potential regulatory retaliation from the United States. The Trump administration warned last week that it would use \"every tool at its disposal\" if the European Union continued enforcing its technology laws, putting companies including Mistral, Spotify, Siemens, and Publicis in a precarious position.The European Commission responded that its rules \"apply equally and fairly to all companies operating in the EU,\" but the standoff introduces uncertainty for European AI companies seeking American enterprise customers.Mistral has differentiated itself from Chinese competitors like DeepSeek and Alibaba&#x27;s Qwen by emphasizing its Apache 2.0 licensing and worldwide availability without regional restrictions — a positioning that takes on added significance amid escalating technology tensions between major economic blocs.Aggressive pricing suggests Mistral sees OCR as a gateway to deeper enterprise relationshipsJaniewicz outlined three revenue pillars for Mistral: complex workflow transformation using Mistral Studio and forward deployment engineering; research and development partnerships to co-build specialized models; and productivity tools including the Le Chat assistant and Mistral Code for developers.Document AI and OCR fit into the first pillar while potentially serving as an entry point that leads customers into deeper engagements. \"OCR is a great way to get those enterprises started and being able to start showing some concrete results,\" Janiewicz said.The aggressive pricing — significantly below many enterprise document processing alternatives — suggests Mistral views OCR as a wedge product rather than a primary profit center. Early customers use the technology to process invoices into structured fields, digitize corporate archives, extract clean text from technical and scientific reports, and improve enterprise search.The company also highlighted accessibility applications. AI-powered OCR can transform printed, handwritten, or scanned documents into searchable digital formats compatible with screen readers and assistive technologies — a capability with implications for compliance with disability access requirements in education and government.The unsexy problem that could determine who wins the enterprise AI raceMistral&#x27;s OCR 3 is a calculated wager that the path to enterprise AI dominance runs not through ever-larger language models, but through the unglamorous work of converting paper into data. While competitors race to build more powerful chatbots and autonomous agents, the French startup is betting that enterprises can&#x27;t use any of those tools until they first digitize the institutional knowledge buried in filing cabinets and PDF archives.\"For us, OCR is a great way to get those enterprises started and being able to start showing some concrete results,\" Janiewicz said. \"To us, really, the key message is customization, portability, and control is the secret sauce to ROI.\"The model becomes available Tuesday through Mistral&#x27;s API and the Document AI interface in Mistral AI Studio. Developers can access it using the identifier mistral-ocr-2512.",
          "feed_position": 2,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/7lqZ2ovXAsX7AHC4HLbAXA/06b2c7b76be32b638cbaa5272c2d1824/nuneybits_Vector_art_of_a_piece_of_paper_becoming_pixels_2e7a410f-7cb5-4d59-bc42-eede9009d3e5.webp?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/infrastructure/ai-agents-fail-63-of-the-time-on-complex-tasks-patronus-ai-says-its-new",
          "published_at": "Wed, 17 Dec 2025 14:00:00 GMT",
          "title": "AI agents fail 63% of the time on complex tasks. Patronus AI says its new 'living' training worlds can fix that.",
          "standfirst": "Patronus AI, the artificial intelligence evaluation startup backed by $20 million from investors including Lightspeed Venture Partners and Datadog, unveiled a new training architecture Tuesday that it says represents a fundamental shift in how AI agents learn to perform complex tasks.The technology, which the company calls \"Generative Simulators,\" creates adaptive simulation environments that continuously generate new challenges, update rules dynamically, and evaluate an agent&#x27;s performance as it learns — all in real time. The approach marks a departure from the static benchmarks that have long served as the industry standard for measuring AI capabilities but have increasingly come under fire for failing to predict real-world performance.\"Traditional benchmarks measure isolated capabilities, but they miss the interruptions, context switches, and layered decision-making that define real work,\" said Anand Kannappan, chief executive and co-founder of Patronus AI, in an exclusive interview with VentureBeat. \"For agents to perform at human levels, they need to learn the way humans do—through dynamic experience and continuous feedback.\"The announcement arrives at a critical moment for the AI industry. AI agents are reshaping software development, from writing code to carrying out complex instructions. Yet LLM-based agents are prone to errors and often perform poorly on complicated, multi-step tasks. Research published earlier this year found that an agent with just a 1% error rate per step can compound to a 63% chance of failure by the hundredth step — a sobering statistic for enterprises seeking to deploy autonomous AI systems at scale.Why static AI benchmarks are failing — and what comes nextPatronus AI&#x27;s approach addresses what the company describes as a growing mismatch between how AI systems are evaluated and how they actually perform in production. Traditional benchmarks, the company argues, function like standardized tests: they measure specific capabilities at a fixed point in time but struggle to capture the messy, unpredictable nature of real work.The new Generative Simulators architecture flips this model. Rather than presenting agents with a fixed set of questions, the system generates assignments, environmental conditions, and oversight processes on the fly, then adapts based on how the agent behaves.\"Over the past year, we&#x27;ve seen a shift away from traditional static benchmarks toward more interactive learning grounds,\" Rebecca Qian, chief technology officer and co-founder of Patronus AI, told VentureBeat. \"This is partly because of the innovation we&#x27;ve seen from model developers — the shift toward reinforcement learning, post-training, and continual learning, and away from supervised instruction tuning. What that means is there&#x27;s been a collapse in the distinction between training and evaluation. Benchmarks have become environments.\"The technology builds on reinforcement learning — an approach where AI systems learn through trial and error, receiving rewards for correct actions and penalties for mistakes. Reinforcement learning is an approach where AI systems learn to make optimal decisions by receiving rewards or penalties for their actions, improving through trial and error. RL can help agents improve, but it typically requires developers to extensively rewrite their code. This discourages adoption, even though the data these agents generate could significantly boost performance through RL training.Patronus AI also introduced a new concept it calls \"Open Recursive Self-Improvement,\" or ORSI — environments where agents can continuously improve through interaction and feedback without requiring a complete retraining cycle between attempts. The company positions this as critical infrastructure for developing AI systems capable of learning continuously rather than being frozen at a point in time.Inside the &#x27;Goldilocks Zone&#x27;: How adaptive AI training finds the sweet spotAt the heart of Generative Simulators lies what Patronus AI calls a \"curriculum adjuster\" — a component that analyzes agent behavior and dynamically modifies the difficulty and nature of training scenarios. The approach draws inspiration from how effective human teachers adapt their instruction based on student performance.Qian explained the approach using an analogy: \"You can think of this as a teacher-student model, where we&#x27;re training the model and the professor continually adapts the curriculum.\"This adaptive approach addresses a problem that Kannappan described as finding the \"Goldilocks Zone\" in training data — ensuring that examples are neither too easy nor too hard for a given model to learn from effectively.\"What&#x27;s important is not just whether you can train on a data set, but whether you can train on a high-quality data set that&#x27;s tuned to your model—one it can actually learn from,\" Kannappan said. \"We want to make sure the examples aren&#x27;t too hard for the model, nor too easy.\"The company says initial results show meaningful improvements in agent performance. Training on Patronus AI&#x27;s environments has increased task completion rates by 10% to 20% across real-world tasks including software engineering, customer service, and financial analysis, according to the company.The AI cheating problem: How &#x27;moving target&#x27; environments prevent reward hackingOne of the most persistent challenges in training AI agents through reinforcement learning is a phenomenon researchers call \"reward hacking\"—where systems learn to exploit loopholes in their training environment rather than genuinely solving problems. Famous examples include early agents that learned to hide in corners of video games rather than actually play them.Generative Simulators addresses this by making the training environment itself a moving target.\"Reward hacking is fundamentally a problem when systems are static. It&#x27;s like students learning to cheat on a test,\" Qian said. \"But when we&#x27;re continually evolving the environment, we can actually look at parts of the system that need to adapt and evolve. Static benchmarks are fixed targets; generative simulator environments are moving targets.\"Patronus AI reports 15x revenue growth as enterprise demand for agent training surgesPatronus AI positions Generative Simulators as the foundation for a new product line it calls \"RL Environments\" — training grounds designed for foundation model laboratories and enterprises building agents for specific domains. The company says this offering represents a strategic expansion beyond its original focus on evaluation tools.\"We&#x27;ve grown 15x in revenue this year, largely due to the high-quality environments we&#x27;ve developed that have been shown to be extremely learnable by different kinds of frontier models,\" Kannappan said.The CEO declined to specify absolute revenue figures but said the new product has allowed the company to \"move higher up the stack in terms of where we sell and who we sell to.\" The company&#x27;s platform is used by numerous Fortune 500 enterprises and leading AI companies around the world.Why OpenAI, Anthropic, and Google can&#x27;t build everything in-houseA central question facing Patronus AI is why the deep-pocketed laboratories developing frontier models—organizations like OpenAI, Anthropic, and Google DeepMind — would license training infrastructure rather than build it themselves.Kannappan acknowledged that these companies \"are investing significantly in environments\" but argued that the breadth of domains requiring specialized training creates a natural opening for third-party providers.\"They want to improve agents on lots of different domains, whether it&#x27;s coding or tool use or navigating browsers or workflows across finance, healthcare, energy, and education,\" he said. \"Solving all those different operational problems is very difficult for a single company to do.\"The competitive landscape is intensifying. Microsoft recently released Agent Lightning, an open-source framework that makes reinforcement learning work for any AI agent without rewrites. NVIDIA&#x27;s NeMo Gym offers modular RL infrastructure for developing agentic AI systems. Meta researchers released DreamGym in November, a framework that simulates RL environments and dynamically adjusts task difficulty as agents improve.&#x27;Environments are the new oil&#x27;: Patronus AI&#x27;s audacious bet on the future of AI trainingLooking ahead, Patronus AI frames its mission in sweeping terms. The company wants to \"environmentalize all of the world&#x27;s data\" — converting human workflows into structured systems that AI can learn from.\"We think that everything should be an environment—internally, we joke that environments are the new oil,\" Kannappan said. \"Reinforcement learning is just one training method, but the construct of an environment is what really matters.\"Qian described the opportunity in expansive terms: \"This is an entirely new field of research, which doesn&#x27;t happen every day. Generative simulation is inspired by early research in robotics and embodied agents. It&#x27;s been a pipe dream for decades, and we&#x27;re only now able to achieve these ideas because of the capabilities of today&#x27;s models.\"The company launched in September 2023 with a focus on evaluation — helping enterprises identify hallucinations and safety issues in AI outputs. That mission has now expanded upstream into training itself. Patronus AI argues that the traditional separation between evaluation and training is collapsing — and that whoever controls the environments where AI agents learn will shape their capabilities.\"We are really at this critical point, this inflection point, where what we do right now will impact what the world is going to look like for generations to come,\" Qian said.Whether Generative Simulators can deliver on that promise remains to be seen. The company&#x27;s 15x revenue growth suggests enterprise customers are hungry for solutions, but deep-pocketed players from Microsoft to Meta are racing to solve the same fundamental problem. If the last two years have taught the industry anything, it&#x27;s that in AI, the future has a habit of arriving ahead of schedule.",
          "content": "Patronus AI, the artificial intelligence evaluation startup backed by $20 million from investors including Lightspeed Venture Partners and Datadog, unveiled a new training architecture Tuesday that it says represents a fundamental shift in how AI agents learn to perform complex tasks.The technology, which the company calls \"Generative Simulators,\" creates adaptive simulation environments that continuously generate new challenges, update rules dynamically, and evaluate an agent&#x27;s performance as it learns — all in real time. The approach marks a departure from the static benchmarks that have long served as the industry standard for measuring AI capabilities but have increasingly come under fire for failing to predict real-world performance.\"Traditional benchmarks measure isolated capabilities, but they miss the interruptions, context switches, and layered decision-making that define real work,\" said Anand Kannappan, chief executive and co-founder of Patronus AI, in an exclusive interview with VentureBeat. \"For agents to perform at human levels, they need to learn the way humans do—through dynamic experience and continuous feedback.\"The announcement arrives at a critical moment for the AI industry. AI agents are reshaping software development, from writing code to carrying out complex instructions. Yet LLM-based agents are prone to errors and often perform poorly on complicated, multi-step tasks. Research published earlier this year found that an agent with just a 1% error rate per step can compound to a 63% chance of failure by the hundredth step — a sobering statistic for enterprises seeking to deploy autonomous AI systems at scale.Why static AI benchmarks are failing — and what comes nextPatronus AI&#x27;s approach addresses what the company describes as a growing mismatch between how AI systems are evaluated and how they actually perform in production. Traditional benchmarks, the company argues, function like standardized tests: they measure specific capabilities at a fixed point in time but struggle to capture the messy, unpredictable nature of real work.The new Generative Simulators architecture flips this model. Rather than presenting agents with a fixed set of questions, the system generates assignments, environmental conditions, and oversight processes on the fly, then adapts based on how the agent behaves.\"Over the past year, we&#x27;ve seen a shift away from traditional static benchmarks toward more interactive learning grounds,\" Rebecca Qian, chief technology officer and co-founder of Patronus AI, told VentureBeat. \"This is partly because of the innovation we&#x27;ve seen from model developers — the shift toward reinforcement learning, post-training, and continual learning, and away from supervised instruction tuning. What that means is there&#x27;s been a collapse in the distinction between training and evaluation. Benchmarks have become environments.\"The technology builds on reinforcement learning — an approach where AI systems learn through trial and error, receiving rewards for correct actions and penalties for mistakes. Reinforcement learning is an approach where AI systems learn to make optimal decisions by receiving rewards or penalties for their actions, improving through trial and error. RL can help agents improve, but it typically requires developers to extensively rewrite their code. This discourages adoption, even though the data these agents generate could significantly boost performance through RL training.Patronus AI also introduced a new concept it calls \"Open Recursive Self-Improvement,\" or ORSI — environments where agents can continuously improve through interaction and feedback without requiring a complete retraining cycle between attempts. The company positions this as critical infrastructure for developing AI systems capable of learning continuously rather than being frozen at a point in time.Inside the &#x27;Goldilocks Zone&#x27;: How adaptive AI training finds the sweet spotAt the heart of Generative Simulators lies what Patronus AI calls a \"curriculum adjuster\" — a component that analyzes agent behavior and dynamically modifies the difficulty and nature of training scenarios. The approach draws inspiration from how effective human teachers adapt their instruction based on student performance.Qian explained the approach using an analogy: \"You can think of this as a teacher-student model, where we&#x27;re training the model and the professor continually adapts the curriculum.\"This adaptive approach addresses a problem that Kannappan described as finding the \"Goldilocks Zone\" in training data — ensuring that examples are neither too easy nor too hard for a given model to learn from effectively.\"What&#x27;s important is not just whether you can train on a data set, but whether you can train on a high-quality data set that&#x27;s tuned to your model—one it can actually learn from,\" Kannappan said. \"We want to make sure the examples aren&#x27;t too hard for the model, nor too easy.\"The company says initial results show meaningful improvements in agent performance. Training on Patronus AI&#x27;s environments has increased task completion rates by 10% to 20% across real-world tasks including software engineering, customer service, and financial analysis, according to the company.The AI cheating problem: How &#x27;moving target&#x27; environments prevent reward hackingOne of the most persistent challenges in training AI agents through reinforcement learning is a phenomenon researchers call \"reward hacking\"—where systems learn to exploit loopholes in their training environment rather than genuinely solving problems. Famous examples include early agents that learned to hide in corners of video games rather than actually play them.Generative Simulators addresses this by making the training environment itself a moving target.\"Reward hacking is fundamentally a problem when systems are static. It&#x27;s like students learning to cheat on a test,\" Qian said. \"But when we&#x27;re continually evolving the environment, we can actually look at parts of the system that need to adapt and evolve. Static benchmarks are fixed targets; generative simulator environments are moving targets.\"Patronus AI reports 15x revenue growth as enterprise demand for agent training surgesPatronus AI positions Generative Simulators as the foundation for a new product line it calls \"RL Environments\" — training grounds designed for foundation model laboratories and enterprises building agents for specific domains. The company says this offering represents a strategic expansion beyond its original focus on evaluation tools.\"We&#x27;ve grown 15x in revenue this year, largely due to the high-quality environments we&#x27;ve developed that have been shown to be extremely learnable by different kinds of frontier models,\" Kannappan said.The CEO declined to specify absolute revenue figures but said the new product has allowed the company to \"move higher up the stack in terms of where we sell and who we sell to.\" The company&#x27;s platform is used by numerous Fortune 500 enterprises and leading AI companies around the world.Why OpenAI, Anthropic, and Google can&#x27;t build everything in-houseA central question facing Patronus AI is why the deep-pocketed laboratories developing frontier models—organizations like OpenAI, Anthropic, and Google DeepMind — would license training infrastructure rather than build it themselves.Kannappan acknowledged that these companies \"are investing significantly in environments\" but argued that the breadth of domains requiring specialized training creates a natural opening for third-party providers.\"They want to improve agents on lots of different domains, whether it&#x27;s coding or tool use or navigating browsers or workflows across finance, healthcare, energy, and education,\" he said. \"Solving all those different operational problems is very difficult for a single company to do.\"The competitive landscape is intensifying. Microsoft recently released Agent Lightning, an open-source framework that makes reinforcement learning work for any AI agent without rewrites. NVIDIA&#x27;s NeMo Gym offers modular RL infrastructure for developing agentic AI systems. Meta researchers released DreamGym in November, a framework that simulates RL environments and dynamically adjusts task difficulty as agents improve.&#x27;Environments are the new oil&#x27;: Patronus AI&#x27;s audacious bet on the future of AI trainingLooking ahead, Patronus AI frames its mission in sweeping terms. The company wants to \"environmentalize all of the world&#x27;s data\" — converting human workflows into structured systems that AI can learn from.\"We think that everything should be an environment—internally, we joke that environments are the new oil,\" Kannappan said. \"Reinforcement learning is just one training method, but the construct of an environment is what really matters.\"Qian described the opportunity in expansive terms: \"This is an entirely new field of research, which doesn&#x27;t happen every day. Generative simulation is inspired by early research in robotics and embodied agents. It&#x27;s been a pipe dream for decades, and we&#x27;re only now able to achieve these ideas because of the capabilities of today&#x27;s models.\"The company launched in September 2023 with a focus on evaluation — helping enterprises identify hallucinations and safety issues in AI outputs. That mission has now expanded upstream into training itself. Patronus AI argues that the traditional separation between evaluation and training is collapsing — and that whoever controls the environments where AI agents learn will shape their capabilities.\"We are really at this critical point, this inflection point, where what we do right now will impact what the world is going to look like for generations to come,\" Qian said.Whether Generative Simulators can deliver on that promise remains to be seen. The company&#x27;s 15x revenue growth suggests enterprise customers are hungry for solutions, but deep-pocketed players from Microsoft to Meta are racing to solve the same fundamental problem. If the last two years have taught the industry anything, it&#x27;s that in AI, the future has a habit of arriving ahead of schedule.",
          "feed_position": 3,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/6BMqV0oL5HMBZK2BqWxs2t/e4aa9c064a3119876694fa0a019723ec/nuneybits_Vector_art_of_robot_facing_infinite_doors_dad9aaaf-e4d9-471d-b15e-94038ee67004.webp?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/general/big-tech-bent-the-knee-for-trump-in-2025-140000365.html",
          "published_at": "Wed, 17 Dec 2025 14:00:00 +0000",
          "title": "Big Tech bent the knee for Trump in 2025",
          "standfirst": "Sure, we've seen millions poured into lobbying and other means of influence during every presidency, but the last two years set a whole new bar. Business leaders, including those from almost every Big Tech company, stepped over themselves to prove fealty to Donald Trump's second administration. It's easy to see why: Their kowtowing was meant to secure regulatory favors, gain tax and tariff advantages and avoid Trump's ire. Ultimately, it was all in the service of appeasing their shareholders. Why else would Apple CEO Tim Cook, someone who typically cultivates a progressive image, hand deliver a gold plaque to the President of the United States? Before we leave 2025 behind, it's worth documenting the many ways tech companies and leaders debased themselves for political favor with the Trump administration. Alphabet (Google)Google dropped diversity recruitment goals in February, following Trump’s executive orders dismantling DEI programs in the federal government. Google also changed its AI principles to allow AI in weapons and surveillance, a move that is in line with the relaxed artificial intelligence regulation the Trump administration would later adopt for its AI Action Plan. To the chagrin of geographers everywhere, the company also replaced the Gulf of Mexico in Google Maps with \"Gulf of America,\" following Trump's executive order.Additionally, Alphabet agreed to pay $24.5 million to settle Donald Trump's lawsuit against YouTube, following the suspension of his YouTube accounts after the January 6th riot. Trump will receive $22 million, while another $2.5 million of the settlement will be paid out to additional plaintiffs who were part of the class action — which is to say, other rioters involved in the storming of the Capitol.Joining plenty of other tech companies, Google donated $1 million to the Trump inauguration, and it’s also contributing to the cost of Trump’s reported $300 million White House ballroom.AmazonIn August, Amazon Web Services said it would provide up to $1 billion in credits to the Trump administration through 2028. Those credits can be put towards AWS cloud services, training and certification and direct contracts.Amazon founder Jeff Bezos also did his fair share to support Trump: He donated $1 million to Trump's inauguration, and since purchasing the Washington Post in 2013 he pushed the paper to the right. This year, Bezos declared that the Post’s opinion pages would be devoted to the support and defense of “personal liberties” and “free markets.” He added, “We’ll cover other topics too of course, but viewpoints opposing those pillars will be left to be published by others.\" To that end, the Post also hired three new conservative columnists. Bezos reportedly also blocked his paper from endorsing Kamala Harris in the 2024 election.I shared this note with the Washington Post team this morning:I’m writing to let you know about a change coming to our opinion pages. We are going to be writing every day in support and defense of two pillars: personal liberties and free markets. We’ll cover other topics too…— Jeff Bezos (@JeffBezos) February 26, 2025 Amazon, too, is contributing to Trump's $300 million White House ballroom. The Washington Post, unsurprisingly, was one of the first major publications to praise Trump’s ballroom. AppleApple relied on big numbers and flashy trinkets to ingratiate itself to the Trump administration. In February, it said it planned to invest $500 billion into the US economy over the next four years. While that sounds impressive, Apple previously announced another $430 billion multi-year investment for the US in 2021. In a potential bid to avoid the administration’s volatile tariff plans, Apple also said it would invest another $100 billion into the US in August.Tim Cook personally donated $1 million to the Trump inauguration fund, Cook's first political donation since 2017. At that August event, he also gave Donald Trump a now infamous gold statue for being a special little guy. Additionally, Apple followed in Google’s footsteps by replacing the Gulf of Mexico in Apple Maps with the “Gulf of America.” The company is also chipping in for Trump’s $300 million White House ballroom.MetaMeta CEO Mark Zuckerberg wasted no time trying to get into Trump’s good graces, perhaps to erase his previous statement that the President should be “held responsible for his words” for inciting the January 6 Capitol riot. On top of donating $1 million to Trump's inauguration, Meta announced that it would be getting rid of third-party fact-checkers on Facebook and Instagram on January 7. Instead, it's relying on community notes similar to X. Meta also ended its DEI initiatives and changed its hate speech rules to allow for calling LGBTQ people “mentally ill.” \"We do allow allegations of mental illness or abnormality when based on gender or sexual orientation, given political and religious discourse about transgenderism and homosexuality and common non-serious usage of words such as 'weird,'\" reads the company’s updated policy.Even Zuckerberg’s charity, which he runs alongside his wife, bowed to Trump. The Chan Zuckerberg Initiative ended its diversity programs and stopped providing “social advocacy funding,” which supported immigration and racial equity efforts. According to The Guardian, the charity’s website removed every reference to diversity or promoting scientific research from underrepresented groups. Similar to Google, Meta said it will pay Donald Trump $25 million to settle his lawsuit related to his Facebook suspension after the January 6 riot. And yes, Meta is also contributing to Trump's $300 million White House ballroom. MicrosoftMicrosoft contributed $1 million to Trump's inauguration fund. Previously, it donated $500,000 to Biden's fund and the same amount for Trump's first term. It’s also contributing to Trump’s $300 million White House ballroom.Similar to Amazon, Microsoft also offered up to $3.1 billion worth of services to the Trump administration as part of the American-centric “OneGov” strategy. That includes discounts for Microsoft 365, Azure cloud services and cybersecurity tools. Copilot AI will also be discounted to government agencies, and it’ll be completely free for a year for agencies subscribing to Microsoft G5 service.Elon Musk (X, SpaceX, Tesla)Elon Musk was by far the biggest booster for Trump in the business world. He spent a whopping total of $277 million to back Trump and other candidates in 2024, including $239 million to America PAC, his super PAC focused on securing votes for Trump and other Republicans. Musk went so far as to offer $1 million to people who said they would vote for Trump, a move that the Justice Department warned might be illegal. Wisconsin's Attorney General challenged Musk's ploy but the state's Supreme Court declined to hear a case on the matter, thereby giving Musk leeway to award two $1 million checks to voters. Musk's team edited a video of one of the recipients to remove her admission that she was paid \"to vote.\" He also joked that he could be jailed if Kamala Harris won the 2024 election, which could be referencing potential election fraud, his penchant for busting unions, national security concerns from his uncomfortably close relationship with Vladimir Putin or any number of potential crimes.Elon Musk spearheaded DOGE (Department of Government Efficiency), an unelected position from which he was given nearly unprecedented federal oversight. Once installed he hired his techie acolytes to chip away at government budgets and staffs. For the first few weeks of the second Trump administration, it appeared as if Musk had unfettered power to manipulate the government.And let's not forget, while leading DOGE, the world's richest man also destroyed USAID, the world's largest food aid provider, for no apparent reason other than cruelty. Internal docs reveal how govt and aid officials desperately tried to warn Trump advisers at State & USAID about impending disaster and death. But for months nothing changed — despite Rubio’s public promises that food programs would be spared. https://t.co/bIZC5tJKxe— Brett Murphy (@BrettMmurphy) December 17, 2025 After purchasing Twitter in 2022 and renaming it \"X,\" Musk has also transformed Twitter into a Trump-friendly social network focused on “free speech.” He reinstated Trump’s Twitter account, which was banned after the January 6 Capitol riot, and he also paid Trump around $10 million to settle his lawsuit over being kicked off the platform. X is now a platform that amplifies far-right extremists, treats the inclusive term “cisgender” as a slur and doesn’t punish users for deadnaming and misgendering trans people.What about the rest?This is by no means an exhaustive documenting of every single tech tycoon that has bent the knee. NVIDIA CEO Jensen Huang, for instance, told Joe Rogan in an interview that “everything that [Trump] thinks through is very practical and very common sense, and, you know, it's very logical.” According to Axios, Huang added that Trump \"wants to make sure that that the important, critical technology of our nation is built in United States, and that we re-industrialize and get good at manufacturing again, because it's important for jobs.\" It’s also worth remembering that OpenAI CEO Sam Altman and Oracle chairman Larry Ellison joined President Trump onstage in announcing Stargate — “the largest AI infrastructure project by far in history.” The two were joined by Softbank CEO Masayoshi Son, who called the moment the “beginning of a golden age.” The next day, Altman posted on X that he believed Trump “will be incredible for the country in many ways!”Ellison’s son David is the CEO of Skydance, and has infamously been rebuilding Paramount with Bari Weiss since the merger of Paramount Global and Skydance Media was approved this year. The list of major tech players bowing to Trump only grows from here, and putting the bulk of the transactions in one place should serve to remind us how closely tied Big Tech is with American (and global) politics.This article originally appeared on Engadget at https://www.engadget.com/general/big-tech-bent-the-knee-for-trump-in-2025-140000365.html?src=rss",
          "content": "Sure, we've seen millions poured into lobbying and other means of influence during every presidency, but the last two years set a whole new bar. Business leaders, including those from almost every Big Tech company, stepped over themselves to prove fealty to Donald Trump's second administration. It's easy to see why: Their kowtowing was meant to secure regulatory favors, gain tax and tariff advantages and avoid Trump's ire. Ultimately, it was all in the service of appeasing their shareholders. Why else would Apple CEO Tim Cook, someone who typically cultivates a progressive image, hand deliver a gold plaque to the President of the United States? Before we leave 2025 behind, it's worth documenting the many ways tech companies and leaders debased themselves for political favor with the Trump administration. Alphabet (Google)Google dropped diversity recruitment goals in February, following Trump’s executive orders dismantling DEI programs in the federal government. Google also changed its AI principles to allow AI in weapons and surveillance, a move that is in line with the relaxed artificial intelligence regulation the Trump administration would later adopt for its AI Action Plan. To the chagrin of geographers everywhere, the company also replaced the Gulf of Mexico in Google Maps with \"Gulf of America,\" following Trump's executive order.Additionally, Alphabet agreed to pay $24.5 million to settle Donald Trump's lawsuit against YouTube, following the suspension of his YouTube accounts after the January 6th riot. Trump will receive $22 million, while another $2.5 million of the settlement will be paid out to additional plaintiffs who were part of the class action — which is to say, other rioters involved in the storming of the Capitol.Joining plenty of other tech companies, Google donated $1 million to the Trump inauguration, and it’s also contributing to the cost of Trump’s reported $300 million White House ballroom.AmazonIn August, Amazon Web Services said it would provide up to $1 billion in credits to the Trump administration through 2028. Those credits can be put towards AWS cloud services, training and certification and direct contracts.Amazon founder Jeff Bezos also did his fair share to support Trump: He donated $1 million to Trump's inauguration, and since purchasing the Washington Post in 2013 he pushed the paper to the right. This year, Bezos declared that the Post’s opinion pages would be devoted to the support and defense of “personal liberties” and “free markets.” He added, “We’ll cover other topics too of course, but viewpoints opposing those pillars will be left to be published by others.\" To that end, the Post also hired three new conservative columnists. Bezos reportedly also blocked his paper from endorsing Kamala Harris in the 2024 election.I shared this note with the Washington Post team this morning:I’m writing to let you know about a change coming to our opinion pages. We are going to be writing every day in support and defense of two pillars: personal liberties and free markets. We’ll cover other topics too…— Jeff Bezos (@JeffBezos) February 26, 2025 Amazon, too, is contributing to Trump's $300 million White House ballroom. The Washington Post, unsurprisingly, was one of the first major publications to praise Trump’s ballroom. AppleApple relied on big numbers and flashy trinkets to ingratiate itself to the Trump administration. In February, it said it planned to invest $500 billion into the US economy over the next four years. While that sounds impressive, Apple previously announced another $430 billion multi-year investment for the US in 2021. In a potential bid to avoid the administration’s volatile tariff plans, Apple also said it would invest another $100 billion into the US in August.Tim Cook personally donated $1 million to the Trump inauguration fund, Cook's first political donation since 2017. At that August event, he also gave Donald Trump a now infamous gold statue for being a special little guy. Additionally, Apple followed in Google’s footsteps by replacing the Gulf of Mexico in Apple Maps with the “Gulf of America.” The company is also chipping in for Trump’s $300 million White House ballroom.MetaMeta CEO Mark Zuckerberg wasted no time trying to get into Trump’s good graces, perhaps to erase his previous statement that the President should be “held responsible for his words” for inciting the January 6 Capitol riot. On top of donating $1 million to Trump's inauguration, Meta announced that it would be getting rid of third-party fact-checkers on Facebook and Instagram on January 7. Instead, it's relying on community notes similar to X. Meta also ended its DEI initiatives and changed its hate speech rules to allow for calling LGBTQ people “mentally ill.” \"We do allow allegations of mental illness or abnormality when based on gender or sexual orientation, given political and religious discourse about transgenderism and homosexuality and common non-serious usage of words such as 'weird,'\" reads the company’s updated policy.Even Zuckerberg’s charity, which he runs alongside his wife, bowed to Trump. The Chan Zuckerberg Initiative ended its diversity programs and stopped providing “social advocacy funding,” which supported immigration and racial equity efforts. According to The Guardian, the charity’s website removed every reference to diversity or promoting scientific research from underrepresented groups. Similar to Google, Meta said it will pay Donald Trump $25 million to settle his lawsuit related to his Facebook suspension after the January 6 riot. And yes, Meta is also contributing to Trump's $300 million White House ballroom. MicrosoftMicrosoft contributed $1 million to Trump's inauguration fund. Previously, it donated $500,000 to Biden's fund and the same amount for Trump's first term. It’s also contributing to Trump’s $300 million White House ballroom.Similar to Amazon, Microsoft also offered up to $3.1 billion worth of services to the Trump administration as part of the American-centric “OneGov” strategy. That includes discounts for Microsoft 365, Azure cloud services and cybersecurity tools. Copilot AI will also be discounted to government agencies, and it’ll be completely free for a year for agencies subscribing to Microsoft G5 service.Elon Musk (X, SpaceX, Tesla)Elon Musk was by far the biggest booster for Trump in the business world. He spent a whopping total of $277 million to back Trump and other candidates in 2024, including $239 million to America PAC, his super PAC focused on securing votes for Trump and other Republicans. Musk went so far as to offer $1 million to people who said they would vote for Trump, a move that the Justice Department warned might be illegal. Wisconsin's Attorney General challenged Musk's ploy but the state's Supreme Court declined to hear a case on the matter, thereby giving Musk leeway to award two $1 million checks to voters. Musk's team edited a video of one of the recipients to remove her admission that she was paid \"to vote.\" He also joked that he could be jailed if Kamala Harris won the 2024 election, which could be referencing potential election fraud, his penchant for busting unions, national security concerns from his uncomfortably close relationship with Vladimir Putin or any number of potential crimes.Elon Musk spearheaded DOGE (Department of Government Efficiency), an unelected position from which he was given nearly unprecedented federal oversight. Once installed he hired his techie acolytes to chip away at government budgets and staffs. For the first few weeks of the second Trump administration, it appeared as if Musk had unfettered power to manipulate the government.And let's not forget, while leading DOGE, the world's richest man also destroyed USAID, the world's largest food aid provider, for no apparent reason other than cruelty. Internal docs reveal how govt and aid officials desperately tried to warn Trump advisers at State & USAID about impending disaster and death. But for months nothing changed — despite Rubio’s public promises that food programs would be spared. https://t.co/bIZC5tJKxe— Brett Murphy (@BrettMmurphy) December 17, 2025 After purchasing Twitter in 2022 and renaming it \"X,\" Musk has also transformed Twitter into a Trump-friendly social network focused on “free speech.” He reinstated Trump’s Twitter account, which was banned after the January 6 Capitol riot, and he also paid Trump around $10 million to settle his lawsuit over being kicked off the platform. X is now a platform that amplifies far-right extremists, treats the inclusive term “cisgender” as a slur and doesn’t punish users for deadnaming and misgendering trans people.What about the rest?This is by no means an exhaustive documenting of every single tech tycoon that has bent the knee. NVIDIA CEO Jensen Huang, for instance, told Joe Rogan in an interview that “everything that [Trump] thinks through is very practical and very common sense, and, you know, it's very logical.” According to Axios, Huang added that Trump \"wants to make sure that that the important, critical technology of our nation is built in United States, and that we re-industrialize and get good at manufacturing again, because it's important for jobs.\" It’s also worth remembering that OpenAI CEO Sam Altman and Oracle chairman Larry Ellison joined President Trump onstage in announcing Stargate — “the largest AI infrastructure project by far in history.” The two were joined by Softbank CEO Masayoshi Son, who called the moment the “beginning of a golden age.” The next day, Altman posted on X that he believed Trump “will be incredible for the country in many ways!”Ellison’s son David is the CEO of Skydance, and has infamously been rebuilding Paramount with Bari Weiss since the merger of Paramount Global and Skydance Media was approved this year. The list of major tech players bowing to Trump only grows from here, and putting the bulk of the transactions in one place should serve to remind us how closely tied Big Tech is with American (and global) politics.This article originally appeared on Engadget at https://www.engadget.com/general/big-tech-bent-the-knee-for-trump-in-2025-140000365.html?src=rss",
          "feed_position": 22
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/the-best-subscription-gifts-to-send-to-your-loved-ones-this-christmas-disney-bundle-masterclass-field-notes-and-more-141830326.html",
          "published_at": "Wed, 17 Dec 2025 13:00:37 +0000",
          "title": "The best subscription gifts to send to your loved ones this Christmas: Disney+ bundle, MasterClass, Field Notes and more",
          "standfirst": "There are way too many online services and subscriptions to keep track of these days, but the flip side is there’s a tool for just about everything. Time is just about up to get a physical gift shipped in time for the holidays, so below we’ve pulled together some of our favorite digital gifts and subscriptions, including time-tested video, music and gaming services as well as tools to clear your mental space and learn new skills. There are also a few subscriptions that provide ongoing, IRL deliveries, if you think your giftee will appreciate the nostalgic charm of a physical object. Best digital gifts and subscription gifts Gaming subscriptions Game consoles are certainly among the most popular gift ideas this time of year. If you know someone who’s been so good that they’re getting a new Nintendo Switch, PlayStation 5 or Xbox Series X/S, one of these subscriptions will make their shiny toy immediately playable out of the box. There’s no doubt that Microsoft has changed the value proposition for its Game Pass subscription service service. It recently raised the price for its most comprehensive Ultimate tier by 50 percent, to $30 a month. That’s a bitter pill to swallow — but devoted players who have an Xbox as their main (or only) console will still get a ton of value out of Game Pass. The aforementioned Ultimate tier provides access to more than 500 games that’ll work on Xbox, PC and other supported devices. It’s also the only way to get “day one” release games like Hollow Knight: Silksong Ninja Gaiden 4, The Outer World 2, Call of Duty Black Ops 7; in recent years other high-profile day one releases have included Doom: The Dark Ages, Starfield, Clair Obscur: Expedition 33, Forza Motorsport and numerous others. Game Pass Ultimate subscribers also have access to the highest quality and fastest game streaming Xbox offers. For a slightly less hardcore Xbox fan, though, the $15-per-month Premium tier is also worth considering. Nintendo’s Switch Online subscription has gotten some nice upgrades in 2025, the year of the Switch 2. As it has for years, Nintendo offers two different Switch Online plans. The basic $20-per-year plan unlocks online play and enables Switch 2 owners to use the system's GameChat capabilities. It also includes more than 100 Game Boy, NES and Super NES games, cloud backups of your saved games as well as the occasional special offers. It also includes access to the rather silly but enjoyable Nintendo Music app, so you can listen to your favorite Zelda or Mario tracks at any time. The $50 “expansion pack” adds a collection of N64, Game Boy Advance and Sega Genesis games as well as some DLC for games like Mario Kart 8, Animal Crossing: New Horizons and Splatoon 2. But most interesting are the new additions for Switch 2 owners: there’s a small but growing library of GameCube games, including classics like The Legend of Zelda: The Wind Waker and F-Zero GX. Switch 2 owners who have the expansion pack subscription can also upgrade to the Switch 2 versions of The Legend of Zelda: Breath of the Wild and Tears of the Kingdom for free. A PlayStation Plus subscription is a must-have for any PS5 owners. That’s partially because you need one to play multiplayer games online, but there are plenty of other significant benefits. PS Plus comes in three tiers, but the middle “Extra” plan ($15/month or $135/year) is probably best for most gamers. In addition to cloud storage for saves, online multiplayer support and a couple of free games for your library every month, you get access to the PlayStation Plus catalog, which includes more than 400 PS4 and PS5 games. There are a number of heavy hitters here, including The Last of Us Part I and II, Ghost of Tsushima, God of War Ragnarok, Death Stranding, Marvel’s Spider-Man Remastered and Returnal alongside lower-profile hits and indie games such as Citizen Sleeper, Sayonara Wild Hearts, Dave the Diver and Humanity. If you know someone who loves older games though, the “Premium” tier ($18/month or $160/year) adds a bunch of titles from the PS1 through the PS4 as well as perks like game trials and PS5 game streaming from the cloud. Check out the rest of our gift ideas here.This article originally appeared on Engadget at https://www.engadget.com/the-best-subscription-gifts-to-send-to-your-loved-ones-this-christmas-disney-bundle-masterclass-field-notes-and-more-141830326.html?src=rss",
          "content": "There are way too many online services and subscriptions to keep track of these days, but the flip side is there’s a tool for just about everything. Time is just about up to get a physical gift shipped in time for the holidays, so below we’ve pulled together some of our favorite digital gifts and subscriptions, including time-tested video, music and gaming services as well as tools to clear your mental space and learn new skills. There are also a few subscriptions that provide ongoing, IRL deliveries, if you think your giftee will appreciate the nostalgic charm of a physical object. Best digital gifts and subscription gifts Gaming subscriptions Game consoles are certainly among the most popular gift ideas this time of year. If you know someone who’s been so good that they’re getting a new Nintendo Switch, PlayStation 5 or Xbox Series X/S, one of these subscriptions will make their shiny toy immediately playable out of the box. There’s no doubt that Microsoft has changed the value proposition for its Game Pass subscription service service. It recently raised the price for its most comprehensive Ultimate tier by 50 percent, to $30 a month. That’s a bitter pill to swallow — but devoted players who have an Xbox as their main (or only) console will still get a ton of value out of Game Pass. The aforementioned Ultimate tier provides access to more than 500 games that’ll work on Xbox, PC and other supported devices. It’s also the only way to get “day one” release games like Hollow Knight: Silksong Ninja Gaiden 4, The Outer World 2, Call of Duty Black Ops 7; in recent years other high-profile day one releases have included Doom: The Dark Ages, Starfield, Clair Obscur: Expedition 33, Forza Motorsport and numerous others. Game Pass Ultimate subscribers also have access to the highest quality and fastest game streaming Xbox offers. For a slightly less hardcore Xbox fan, though, the $15-per-month Premium tier is also worth considering. Nintendo’s Switch Online subscription has gotten some nice upgrades in 2025, the year of the Switch 2. As it has for years, Nintendo offers two different Switch Online plans. The basic $20-per-year plan unlocks online play and enables Switch 2 owners to use the system's GameChat capabilities. It also includes more than 100 Game Boy, NES and Super NES games, cloud backups of your saved games as well as the occasional special offers. It also includes access to the rather silly but enjoyable Nintendo Music app, so you can listen to your favorite Zelda or Mario tracks at any time. The $50 “expansion pack” adds a collection of N64, Game Boy Advance and Sega Genesis games as well as some DLC for games like Mario Kart 8, Animal Crossing: New Horizons and Splatoon 2. But most interesting are the new additions for Switch 2 owners: there’s a small but growing library of GameCube games, including classics like The Legend of Zelda: The Wind Waker and F-Zero GX. Switch 2 owners who have the expansion pack subscription can also upgrade to the Switch 2 versions of The Legend of Zelda: Breath of the Wild and Tears of the Kingdom for free. A PlayStation Plus subscription is a must-have for any PS5 owners. That’s partially because you need one to play multiplayer games online, but there are plenty of other significant benefits. PS Plus comes in three tiers, but the middle “Extra” plan ($15/month or $135/year) is probably best for most gamers. In addition to cloud storage for saves, online multiplayer support and a couple of free games for your library every month, you get access to the PlayStation Plus catalog, which includes more than 400 PS4 and PS5 games. There are a number of heavy hitters here, including The Last of Us Part I and II, Ghost of Tsushima, God of War Ragnarok, Death Stranding, Marvel’s Spider-Man Remastered and Returnal alongside lower-profile hits and indie games such as Citizen Sleeper, Sayonara Wild Hearts, Dave the Diver and Humanity. If you know someone who loves older games though, the “Premium” tier ($18/month or $160/year) adds a bunch of titles from the PS1 through the PS4 as well as perks like game trials and PS5 game streaming from the cloud. Check out the rest of our gift ideas here.This article originally appeared on Engadget at https://www.engadget.com/the-best-subscription-gifts-to-send-to-your-loved-ones-this-christmas-disney-bundle-masterclass-field-notes-and-more-141830326.html?src=rss",
          "feed_position": 25
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/audio/speakers/alexa-home-theater-everything-you-need-to-know-about-amazons-newest-echo-feature-130000328.html",
          "published_at": "Wed, 17 Dec 2025 13:00:00 +0000",
          "title": "Alexa Home Theater: Everything you need to know about Amazon's newest Echo feature",
          "standfirst": "When Amazon’s latest Echo speakers arrived in October, a feature the company spent time hyping during its fall devices event was missing. Alexa Home Theater arrived in late November, allowing owners of a recent Fire TV device and either of the new Echo speakers the ability to use up to five units and an Echo Sub as a surround sound setup. The main selling point is that the underlying tech allows you to put the speakers wherever you want and Alexa Home Theater will handle the rest. A robust array of speakers is a considerable investment, though, and there are limitations to Amazon’s latest play for your living room.What is Alexa Home Theater?At the most basic level, Alexa Home Theater is an automatic room calibration tool that’s akin to Trueplay from Sonos, SpaceFit Sound from Samsung or Sound Field Optimization from Sony. The feature, no matter which company builds it, is designed to automatically tune the acoustics of a soundbar or other speakers to the acoustics of the room. These tools allow companies to account for things like high ceilings and weird angles, adjusting audio performance so things sound their best no matter how your home is designed. Like similar technologies from other companies, Alexa Home Theater and the latest Echo speakers also support spatial awareness. This allows you to position the speakers however they best fit in your living room and the underlying tech will balance the sound based on their proximity to each other. Gone are the days when you have to put rear speakers in a specific spot in order for them to work. For the latest Echo speakers, Amazon overhauled Alexa Home Theater to accommodate more devices in a single setup. Now you can use up to five Echo Dot Max or second-generation Echo Studio units and a subwoofer at the same time. Of course, this means you can put the compact speakers around the perimeter of your home theater space, providing more immersive surround sound than a single speaker or two can muster. Alexa Home Theater recognizes each Echo speaker and adjusts the system accordingly. What hardware do I need to use it?My test setup was four Echo Studio speakers and a Fire TV Stick 4K Max.Billy Steele for EngadgetIf you want to use more than two Echo speakers and a subwoofer in your Alexa Home Theater setup, you’ll need the latest Amazon devices to make that happen. That list includes the Echo Dot Max and second-gen Echo Studio, along with the Echo Sub if you crave more bass. You’ll also need a Fire TV streaming gadget for the multi-speaker option, but it has to be one of the following:Fire TV Cube (3rd gen)Fire TV Stick 4K Max (2nd gen) Fire TV Stick 4K (2nd gen)Fire TV Stick 4K PlusYou might be wondering why Alexa Home Theater isn’t supported on other (older) Amazon streaming and audio models. Well, it is… but you’ll only be allowed to pair just two Echo speakers of the same model with an optional subwoofer for a total of three devices. That list of supported models is quite long, but it accounts for the two most recent generations of both the Echo and Echo Dot in addition to the Echo Pop, Echo Plus (2nd gen) and the original Echo Studio. This two-speaker method is also supported on more Fire TV devices, including some standalone TVs with Amazon’s streaming platform built in. In that scenario, it means you could pair two Echo speakers with a subwoofer, for example, and have them play the same audio content with Alexa Home Theater.Currently, Amazon says these are Fire TV devices that don’t support Alexa Home Theater: Fire TV Stick 4K SelectAmazon Fire TV 2-SeriesFire TV 4-Series (2025 Release, newest model)Fire TV Omni QLED Series (2025 Release, newest model)Fire TV Omni Mini-LED Series (2024 Release, newest model)How to set up Alexa Home TheaterLike nearly all home theater speakers and soundbars these days, setting up Alexa Home Theater is easy once you have the required hardware in place. Unlike a lot of the competition, this initial configuration isn’t done in a mobile app — it’s handled by the Settings menu on a compatible Fire TV device. Before you initiate the setup process, you’ll want to make sure your Echo speakers and Fire TV device are up to date. You can ask Alexa to check for updates to the speakers, while you’ll need to check the Device & Software section of the Fire TV Settings menu for those gadgets. Once everything is current, here are the steps to follow to get started with Alexa Home Theater: Go to Settings on your Fire TV.Select Display & Sounds. Select Alexa Home Theater.Select Create Alexa Home Theater. Select the compatible Echo speakers. You can also add an Echo Sub during this step. Follow the on-screen instructions to configure the system. After you select either the second-gen Echo Studio or the Echo Dot Max, hit Next and the system will calibrate automatically. If you select other Echo speakers, hit Next and then confirm the placement of the speakers before calibration. When the setup is complete, the Alexa Home Theater system is ready to use. To do so, select either Home or Watch Preview. Using Alexa Home Theater with four Echo Studio speakersThe two Echo Studio speakers that flanked my TV primarily handled dialogue.Billy Steele for EngadgetLike most home theater gear, the Fire TV will play short audio clips through each Echo speaker and your TV speakers to determine their location and to calibrate the group to the room. From start to finish, the setup process takes about five minutes — so long as your Fire TV and all speakers are running up-to-date firmware. It’s a quick and straightforward process, which meant I was watching Fallout soon after putting all of the speakers in place. While streaming that video game-inspired original series, the Echo Studios provided an immersive audio experience with directional sound and crisp, clear dialogue. The front two speakers handle speech, but there’s some nuance in what direction it’s coming from on screen. Rear speakers are tasked with the bulk of the environmental noise, whether that’s nearby water, passing cars, rain or gunfire — just to name a few. I was also pleasantly surprised by how much bassy rumble these speakers produced during movies and shows. I didn’t think a single unit was particularly adept at low-end tone during my review, but I think it would be just fine to use a collection of Echo Studios without having to add the Echo Sub. The overall audio performance here is certainly a boost over any TV’s built-in speakers, but the sense of immersion isn’t as complete as what a soundbar and a pair of rear satellite speakers can provide. Specifically, I felt like the foursome of Echo Studio speakers in my test setup stumbled with the height channels. Despite the angled design of the devices and their multi-driver configurations, the effect of overhead and aerial sounds was quite subdued. And the worst part about this is that there’s no way to try to fix it with settings. That’s because there are minimal settings to adjust once Alexa Home Theater is set up. There are two features — dialogue boost and volume leveler — but those are both Fire TV settings rather than items meant for any connected speakers. In the Alexa Home Theater menu, there’s a lip sync tool to address any audio lag, which is a standard item for any soundbar. I would’ve loved to see some kind of channel adjustment at the very least, so that I could dial in the mix a bit rather than relying on Amazon’s preferences. If you turn to Alexa for help, the assistant can only change bass, treble and midrange, and it can only do so when prompted by voice commands. When I asked for it to turn up the dialogue, it seemed to just increase the volume. If I asked it to adjust the height channels, Alexa responded affirmatively, but it didn’t sound like any tweaks were applied. Amazon later confirmed that those basic EQ changes are Alexa and actually do here. Another gripe I have with Alexa Home Theater is the inability to connect my phone or another device to the entire group for music. You can still sync with individual Echo Studio speakers for music via Bluetooth, but if you want to use the whole shebang, you’ll have to play your tunes from the Fire TV or ask Alexa to help. The Fire TV platform has plenty of audio streaming apps, but Apple Music isn’t there and that’s where my monthly subscription resides. You have to add the Apple Music skill to the Alexa app on your phone first, then you can stream content from the service by asking Amazon’s assistant. For other services with Fire TV apps, you’re using your TV to play music, and most of the time I want the TV off and to just pick up my phone to blast some Spiritbox. I also like to control the queue from my phone, but alas that’s not an option here. Wrap-upAmazon's new Echo Studio is a complete redesign from the 2019 model.Billy Steele for EngadgetThe main issue with using four Echo Studios and a Fire TV is the cost. For the speakers alone, you’re looking at nearly $900 for the setup I tested. When the new Echo speakers were announced in September, Amazon said it would offer Alexa Home Theater bundles with multiple speakers, but I couldn’t find those on the company’s website. For comparison, Sonos currently offers multiple configurations of the Beam soundbar and either Era 100 speakers or a sub for less than $800. For the same price as four Echo Studio speakers, you can get Sony’s Bravia Theater System 6, which comes with a soundbar, rear speakers and a subwoofer. There are other options from other companies as well in the $800 range, and most people will likely be happier with a dedicated soundbar-based setup than four Echo units and a subwoofer. Hello, AirPlay and Google Cast.From an aesthetic perspective, I can see why someone might opt for multiple Echo speakers instead of a mid-sized (or larger) soundbar. Two Echo Studios on either side of my TV with the two rear units tucked away on a shelf certainly creates a clean look. And in terms of sound, using two Echo Studio speakers, let alone four like I tested, will provide a noticeable boost over a TV’s built-in speakers. However, the lack of customization and the inability to control music from my phone for an Alexa Home Theater group makes it hard to recommend Amazon’s latest for shoppers looking for more immersive audio in the living room. Hopefully the company will continue to refine this feature so that it’s more capable in the future.This article originally appeared on Engadget at https://www.engadget.com/audio/speakers/alexa-home-theater-everything-you-need-to-know-about-amazons-newest-echo-feature-130000328.html?src=rss",
          "content": "When Amazon’s latest Echo speakers arrived in October, a feature the company spent time hyping during its fall devices event was missing. Alexa Home Theater arrived in late November, allowing owners of a recent Fire TV device and either of the new Echo speakers the ability to use up to five units and an Echo Sub as a surround sound setup. The main selling point is that the underlying tech allows you to put the speakers wherever you want and Alexa Home Theater will handle the rest. A robust array of speakers is a considerable investment, though, and there are limitations to Amazon’s latest play for your living room.What is Alexa Home Theater?At the most basic level, Alexa Home Theater is an automatic room calibration tool that’s akin to Trueplay from Sonos, SpaceFit Sound from Samsung or Sound Field Optimization from Sony. The feature, no matter which company builds it, is designed to automatically tune the acoustics of a soundbar or other speakers to the acoustics of the room. These tools allow companies to account for things like high ceilings and weird angles, adjusting audio performance so things sound their best no matter how your home is designed. Like similar technologies from other companies, Alexa Home Theater and the latest Echo speakers also support spatial awareness. This allows you to position the speakers however they best fit in your living room and the underlying tech will balance the sound based on their proximity to each other. Gone are the days when you have to put rear speakers in a specific spot in order for them to work. For the latest Echo speakers, Amazon overhauled Alexa Home Theater to accommodate more devices in a single setup. Now you can use up to five Echo Dot Max or second-generation Echo Studio units and a subwoofer at the same time. Of course, this means you can put the compact speakers around the perimeter of your home theater space, providing more immersive surround sound than a single speaker or two can muster. Alexa Home Theater recognizes each Echo speaker and adjusts the system accordingly. What hardware do I need to use it?My test setup was four Echo Studio speakers and a Fire TV Stick 4K Max.Billy Steele for EngadgetIf you want to use more than two Echo speakers and a subwoofer in your Alexa Home Theater setup, you’ll need the latest Amazon devices to make that happen. That list includes the Echo Dot Max and second-gen Echo Studio, along with the Echo Sub if you crave more bass. You’ll also need a Fire TV streaming gadget for the multi-speaker option, but it has to be one of the following:Fire TV Cube (3rd gen)Fire TV Stick 4K Max (2nd gen) Fire TV Stick 4K (2nd gen)Fire TV Stick 4K PlusYou might be wondering why Alexa Home Theater isn’t supported on other (older) Amazon streaming and audio models. Well, it is… but you’ll only be allowed to pair just two Echo speakers of the same model with an optional subwoofer for a total of three devices. That list of supported models is quite long, but it accounts for the two most recent generations of both the Echo and Echo Dot in addition to the Echo Pop, Echo Plus (2nd gen) and the original Echo Studio. This two-speaker method is also supported on more Fire TV devices, including some standalone TVs with Amazon’s streaming platform built in. In that scenario, it means you could pair two Echo speakers with a subwoofer, for example, and have them play the same audio content with Alexa Home Theater.Currently, Amazon says these are Fire TV devices that don’t support Alexa Home Theater: Fire TV Stick 4K SelectAmazon Fire TV 2-SeriesFire TV 4-Series (2025 Release, newest model)Fire TV Omni QLED Series (2025 Release, newest model)Fire TV Omni Mini-LED Series (2024 Release, newest model)How to set up Alexa Home TheaterLike nearly all home theater speakers and soundbars these days, setting up Alexa Home Theater is easy once you have the required hardware in place. Unlike a lot of the competition, this initial configuration isn’t done in a mobile app — it’s handled by the Settings menu on a compatible Fire TV device. Before you initiate the setup process, you’ll want to make sure your Echo speakers and Fire TV device are up to date. You can ask Alexa to check for updates to the speakers, while you’ll need to check the Device & Software section of the Fire TV Settings menu for those gadgets. Once everything is current, here are the steps to follow to get started with Alexa Home Theater: Go to Settings on your Fire TV.Select Display & Sounds. Select Alexa Home Theater.Select Create Alexa Home Theater. Select the compatible Echo speakers. You can also add an Echo Sub during this step. Follow the on-screen instructions to configure the system. After you select either the second-gen Echo Studio or the Echo Dot Max, hit Next and the system will calibrate automatically. If you select other Echo speakers, hit Next and then confirm the placement of the speakers before calibration. When the setup is complete, the Alexa Home Theater system is ready to use. To do so, select either Home or Watch Preview. Using Alexa Home Theater with four Echo Studio speakersThe two Echo Studio speakers that flanked my TV primarily handled dialogue.Billy Steele for EngadgetLike most home theater gear, the Fire TV will play short audio clips through each Echo speaker and your TV speakers to determine their location and to calibrate the group to the room. From start to finish, the setup process takes about five minutes — so long as your Fire TV and all speakers are running up-to-date firmware. It’s a quick and straightforward process, which meant I was watching Fallout soon after putting all of the speakers in place. While streaming that video game-inspired original series, the Echo Studios provided an immersive audio experience with directional sound and crisp, clear dialogue. The front two speakers handle speech, but there’s some nuance in what direction it’s coming from on screen. Rear speakers are tasked with the bulk of the environmental noise, whether that’s nearby water, passing cars, rain or gunfire — just to name a few. I was also pleasantly surprised by how much bassy rumble these speakers produced during movies and shows. I didn’t think a single unit was particularly adept at low-end tone during my review, but I think it would be just fine to use a collection of Echo Studios without having to add the Echo Sub. The overall audio performance here is certainly a boost over any TV’s built-in speakers, but the sense of immersion isn’t as complete as what a soundbar and a pair of rear satellite speakers can provide. Specifically, I felt like the foursome of Echo Studio speakers in my test setup stumbled with the height channels. Despite the angled design of the devices and their multi-driver configurations, the effect of overhead and aerial sounds was quite subdued. And the worst part about this is that there’s no way to try to fix it with settings. That’s because there are minimal settings to adjust once Alexa Home Theater is set up. There are two features — dialogue boost and volume leveler — but those are both Fire TV settings rather than items meant for any connected speakers. In the Alexa Home Theater menu, there’s a lip sync tool to address any audio lag, which is a standard item for any soundbar. I would’ve loved to see some kind of channel adjustment at the very least, so that I could dial in the mix a bit rather than relying on Amazon’s preferences. If you turn to Alexa for help, the assistant can only change bass, treble and midrange, and it can only do so when prompted by voice commands. When I asked for it to turn up the dialogue, it seemed to just increase the volume. If I asked it to adjust the height channels, Alexa responded affirmatively, but it didn’t sound like any tweaks were applied. Amazon later confirmed that those basic EQ changes are Alexa and actually do here. Another gripe I have with Alexa Home Theater is the inability to connect my phone or another device to the entire group for music. You can still sync with individual Echo Studio speakers for music via Bluetooth, but if you want to use the whole shebang, you’ll have to play your tunes from the Fire TV or ask Alexa to help. The Fire TV platform has plenty of audio streaming apps, but Apple Music isn’t there and that’s where my monthly subscription resides. You have to add the Apple Music skill to the Alexa app on your phone first, then you can stream content from the service by asking Amazon’s assistant. For other services with Fire TV apps, you’re using your TV to play music, and most of the time I want the TV off and to just pick up my phone to blast some Spiritbox. I also like to control the queue from my phone, but alas that’s not an option here. Wrap-upAmazon's new Echo Studio is a complete redesign from the 2019 model.Billy Steele for EngadgetThe main issue with using four Echo Studios and a Fire TV is the cost. For the speakers alone, you’re looking at nearly $900 for the setup I tested. When the new Echo speakers were announced in September, Amazon said it would offer Alexa Home Theater bundles with multiple speakers, but I couldn’t find those on the company’s website. For comparison, Sonos currently offers multiple configurations of the Beam soundbar and either Era 100 speakers or a sub for less than $800. For the same price as four Echo Studio speakers, you can get Sony’s Bravia Theater System 6, which comes with a soundbar, rear speakers and a subwoofer. There are other options from other companies as well in the $800 range, and most people will likely be happier with a dedicated soundbar-based setup than four Echo units and a subwoofer. Hello, AirPlay and Google Cast.From an aesthetic perspective, I can see why someone might opt for multiple Echo speakers instead of a mid-sized (or larger) soundbar. Two Echo Studios on either side of my TV with the two rear units tucked away on a shelf certainly creates a clean look. And in terms of sound, using two Echo Studio speakers, let alone four like I tested, will provide a noticeable boost over a TV’s built-in speakers. However, the lack of customization and the inability to control music from my phone for an Alexa Home Theater group makes it hard to recommend Amazon’s latest for shoppers looking for more immersive audio in the living room. Hopefully the company will continue to refine this feature so that it’s more capable in the future.This article originally appeared on Engadget at https://www.engadget.com/audio/speakers/alexa-home-theater-everything-you-need-to-know-about-amazons-newest-echo-feature-130000328.html?src=rss",
          "feed_position": 26,
          "image_url": "https://d29szjachogqwa.cloudfront.net/images/user-uploaded/IMG_7661.jpg"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/computing/laptops/best-chromebooks-160054646.html",
          "published_at": "Wed, 17 Dec 2025 10:01:26 +0000",
          "title": "The best Chromebook you can buy in 2026",
          "standfirst": "Chromebooks have quietly become some of the most useful laptops for everyday tasks. They boot fast, stay secure with automatic updates and often cost far less than traditional Windows or Mac machines. Modern Chromebooks also look and feel better than ever, with brighter screens, stronger processors and designs that range from simple clamshells to flexible 2-in-1s.Whether you need a laptop for school, streaming or a portable option for travel, there is likely a Chromebook that fits your routine. After testing the top models, we picked the best Chromebooks you can buy today to help you find something that balances performance, price and reliability. Table of contents Best Chromebooks in 2026 Acer Chromebook Plus 514 (CB514-6H/T) Acer Chromebook Plus Spin 514 (CP514-5HN) Acer Chromebook Plus 516 GE Lenovo Chromebook Plus 14 Best Chromebooks FAQs Other Chromebooks we tested Best Chromebooks in 2026 Best Chromebooks FAQs What is Chrome OS, and why would I use it over Windows? This is probably the number one question about Chromebooks. There are plenty of inexpensive Windows laptops on the market, so why bother with Chrome's operating system? Glad you asked. For me, the simple and clean nature of Chrome OS is a big selling point. Chrome OS is based on Google’s Chrome browser, which means most of the programs you can run are web based. There’s no bloatware or unwanted apps to uninstall like you often get on Windows laptops, it boots up in seconds, and you can completely reset to factory settings almost as quickly. Of course, simplicity will also be a major drawback for some users. Not being able to install native software can be a dealbreaker if you’re a video editor or software developer. But there are also plenty of people who do the majority of their work in a web browser, using tools like Google Docs and spreadsheets for productivity without needing a full Windows setup. Google and its software partners are getting better every year at supporting more advanced features. For example, Google added video editing tools to the Google Photos app on Chromebooks – it won’t replace Adobe Premiere, but it should be handy for a lot of people. Similarly, Google and Adobe announced Photoshop on the web in 2023, something that brings much of the power of Adobe’s desktop apps to Chromebooks. Chromebooks can also run Android apps, which greatly expands the amount of software available. The quality varies widely, but it means you can do more with a Chromebook beyond just web-based apps. For example, you can install the Netflix app and save videos for offline watching. Other Android apps like Microsoft Office and Adobe Lightroom are surprisingly capable as well. Between Android apps and a general improvement in web apps, Chromebooks are more than just portals to a browser. What do Chromebooks do well? Put simply, web browsing and really anything web based. Online shopping, streaming music and video and using various social media sites are among the most common daily tasks people do on Chromebooks. As you might expect, they also work well with Google services like Photos, Docs, Gmail, Drive, Keep and so on. Yes, any computer that can run Chrome can do that too, but the lightweight nature of Google Chrome OS makes it a responsive and stable platform. As I mentioned before, Chrome OS can run Android apps, so if you’re an Android user you’ll find some nice ties between the platforms. You can get most of the same apps that are on your phone on a Chromebook and keep info in sync between them. You can also use some Android phones as a security key for your Chromebook or instantly tether your 2-in-1 laptop to use mobile data. Google continues to tout security as a major differentiator for Chromebooks, and it’s definitely a factor worth considering. Auto-updates are the first lines of defense: Chrome OS updates download quickly in the background and a fast reboot is all it takes to install the latest version. Google says that each webpage and app on a Chromebook runs in its own sandbox as well, so any security threats are contained to that individual app. Finally, Chrome OS has a self-check called Verified Boot that runs every time a device starts up. Beyond all this, the simple fact that you generally can’t install traditional apps on a Chromebook means there are fewer ways for bad actors to access the system. If you’re interested in Google’s Gemini AI tools, a Chromebook is a good option as well. Every Chromebook in our top picks comes with a full year of Google’s AI Pro plan — this combines the usual Google One perks like 2TB of storage and 10 percent back in purchases from the Google Store with a bunch of AI tools. You’ll get access to Gemini in Chrome, Gmail, Google Docs and other apps, Gemini 2.5 Pro in the Gemini app and more. Given that this plan is $20/month, it’s a pretty solid perk. Chromebook Plus models also include tools like the AI-powered “help me write,” the Google Photos Magic Editor and generative AI backgrounds you can create by filling in a few prompts. As for when to avoid Chromebooks, the answer is simple: If you rely heavily on a specific native application for Windows or a Mac, chances are you won’t find the exact same option on a ChromeOS device. That’s most true in fields like photo and video editing, but it can also be the case in law or finance. Plenty of businesses run on Google’s G suite software, but more still have specific requirements that a Chromebook might not match. If you’re an iPhone user, you’ll also miss out on the way the iPhone easily integrates with an iPad or Mac. For me, the big downside is not being able to access iMessage on a Chromebook. Finally, gaming Chromebooks are not ubiquitous, although they’re becoming a slightly more reasonable option with the rise of cloud gaming. In late 2022, Google and some hardware partners announced a push to make Chromebooks with cloud gaming in mind. From a hardware perspective, that means laptops with bigger screens that have higher refresh rates as well as optimizing those laptops to work with services like NVIDIA GeForce Now, Xbox Game Pass and Amazon Luna. You’ll obviously need an internet connection to use these services, but the good news is that playing modern games on a Chromebook isn’t impossible. You can also install Android games from the Google Play Store, but that’s not what most people are thinking of when they want to game on a laptop. What are the most important specs for a Chromebook? Chrome OS is lightweight and runs well on fairly modest hardware, so the most important thing to look for might not be processor power or storage space. But Google made it easier to get consistent specs and performance late last year when it introduced the Chromebook Plus initiative. Any device with a Chromebook Plus designation meets some minimum requirements, which happen to be very similar to what I’d recommend most people get if they’re looking for the best laptop they can use every day. Chromebook Plus models have at least a 12th-gen Intel Core i3 processor, or an AMD Ryzen 3 7000 series processor, both of which should be more than enough for most people. These laptops also have a minimum of 8GB of RAM and 128GB of SSD storage, which should do the trick unless you’re really pushing your Chromebook. All Chromebook Plus models have to have a 1080p webcam, which is nice in these days of constant video calling, and they also all have to have at least a 1080p FHD IPS screen. Of course, you can get higher specs or better screens if you desire, but I’ve found that basically everything included in the Chromebook Plus target specs makes for a very good experience. Google has an Auto Update policy for Chromebooks as well, and while that’s not exactly a spec, it’s worth checking before you buy. Last year, Google announced that Chromebooks would get software updates and support for an impressive 10 years after their release date. This support page lists the Auto Update expiration date for virtually every Chromebook ever, but a good rule of thumb is to buy the newest machine you can to maximize your support. How much should I spend on a Chromebook? Chromebooks started out notoriously cheap, with list prices often coming in under $300. But as they’ve gone more mainstream, they’ve transitioned from being essentially modern netbooks to some of the best laptops you’ll want to use all day. As such, prices have increased: At this point, you should expect to spend at least $400 if you want a solid daily driver. There are still many Chromebooks out there available at a low price that may be suitable as secondary devices, but a good Chromebook that can be an all-day, every-day laptop will cost more. But, notably, even the best Chromebooks usually cost less than the best Windows laptops, or even the best “regular” laptops out there. There are a handful of premium Chromebooks that approach or even exceed $1,000 that claim to offer better performance and more processing power, but I don’t recommend spending that much. Generally, that’ll get you a better design with more premium materials, as well as more powerful internals and extra storage space, like a higher-capacity SSD. Of course, you also sometimes pay for the brand name. But, the specs I outlined earlier are usually enough, and there are multiple good premium Chromebooks in the $700 to $800 range at this point. See Also: Best Laptops for 2026 Best Gaming Laptops Best 2-in-1 Laptops for 2026 Best Cheap Windows Laptops Best Laptops for College Students Other Chromebooks we tested Lenovo IdeaPad Flex 5i Chromebook Plus This was our pick for best overall Chromebook for years, and it’s still one of the better options you can find for a basic laptop that doesn’t break the bank. It’s a few years older than our current top pick, so its processor isn’t fresh and it only has 128GB of storage. It also won’t get updates from Google as long as newer models. But it still combines a nice screen and keyboard with solid performance. This laptop typically costs $500, which feels high given its a few years old and Acer’s Chromebook Plus 514 is only $350, but if you can find it on sale and can’t find the Acer it’s worth a look. ASUS CX15 This Chromebook is extremely affordable – you can currently pick it up for only $159 at Walmart. That price and its large 15.6-inch screen is mainly what it has going for it, as the Intel Celeron N4500 chip and 4GB of RAM powering it does not provide good performance if you’re doing anything more than browsing with a few tabs open. If you’re shopping for someone with extremely basic needs and have a small budget, the CX15 might fit the bill. But just be aware that you get what you pay for. Samsung Galaxy Chromebook Plus Samsung’s Galaxy Chromebook Plus, released in late 2024, is one of the more unique Chromebooks out there. It’s extremely thin and light, at 0.46 inches and 2.6 pounds, but it manages to include a 15.6-inch display in that frame. That screen is a 1080p panel that’s sharp and bright, but its 16:9 aspect ratio made things feel a bit cramped when scrolling vertically. Performance is very good, and the keyboard is solid, though I’m not a fan of the number pad as it shifts everything to the left. At $700 it’s not cheap, but that feels fair considering its size and capabilities. If you’re looking for a big screen laptop that is also super light, this Chromebook merits consideration, even if it’s not the best option for everyone.This article originally appeared on Engadget at https://www.engadget.com/computing/laptops/best-chromebooks-160054646.html?src=rss",
          "content": "Chromebooks have quietly become some of the most useful laptops for everyday tasks. They boot fast, stay secure with automatic updates and often cost far less than traditional Windows or Mac machines. Modern Chromebooks also look and feel better than ever, with brighter screens, stronger processors and designs that range from simple clamshells to flexible 2-in-1s.Whether you need a laptop for school, streaming or a portable option for travel, there is likely a Chromebook that fits your routine. After testing the top models, we picked the best Chromebooks you can buy today to help you find something that balances performance, price and reliability. Table of contents Best Chromebooks in 2026 Acer Chromebook Plus 514 (CB514-6H/T) Acer Chromebook Plus Spin 514 (CP514-5HN) Acer Chromebook Plus 516 GE Lenovo Chromebook Plus 14 Best Chromebooks FAQs Other Chromebooks we tested Best Chromebooks in 2026 Best Chromebooks FAQs What is Chrome OS, and why would I use it over Windows? This is probably the number one question about Chromebooks. There are plenty of inexpensive Windows laptops on the market, so why bother with Chrome's operating system? Glad you asked. For me, the simple and clean nature of Chrome OS is a big selling point. Chrome OS is based on Google’s Chrome browser, which means most of the programs you can run are web based. There’s no bloatware or unwanted apps to uninstall like you often get on Windows laptops, it boots up in seconds, and you can completely reset to factory settings almost as quickly. Of course, simplicity will also be a major drawback for some users. Not being able to install native software can be a dealbreaker if you’re a video editor or software developer. But there are also plenty of people who do the majority of their work in a web browser, using tools like Google Docs and spreadsheets for productivity without needing a full Windows setup. Google and its software partners are getting better every year at supporting more advanced features. For example, Google added video editing tools to the Google Photos app on Chromebooks – it won’t replace Adobe Premiere, but it should be handy for a lot of people. Similarly, Google and Adobe announced Photoshop on the web in 2023, something that brings much of the power of Adobe’s desktop apps to Chromebooks. Chromebooks can also run Android apps, which greatly expands the amount of software available. The quality varies widely, but it means you can do more with a Chromebook beyond just web-based apps. For example, you can install the Netflix app and save videos for offline watching. Other Android apps like Microsoft Office and Adobe Lightroom are surprisingly capable as well. Between Android apps and a general improvement in web apps, Chromebooks are more than just portals to a browser. What do Chromebooks do well? Put simply, web browsing and really anything web based. Online shopping, streaming music and video and using various social media sites are among the most common daily tasks people do on Chromebooks. As you might expect, they also work well with Google services like Photos, Docs, Gmail, Drive, Keep and so on. Yes, any computer that can run Chrome can do that too, but the lightweight nature of Google Chrome OS makes it a responsive and stable platform. As I mentioned before, Chrome OS can run Android apps, so if you’re an Android user you’ll find some nice ties between the platforms. You can get most of the same apps that are on your phone on a Chromebook and keep info in sync between them. You can also use some Android phones as a security key for your Chromebook or instantly tether your 2-in-1 laptop to use mobile data. Google continues to tout security as a major differentiator for Chromebooks, and it’s definitely a factor worth considering. Auto-updates are the first lines of defense: Chrome OS updates download quickly in the background and a fast reboot is all it takes to install the latest version. Google says that each webpage and app on a Chromebook runs in its own sandbox as well, so any security threats are contained to that individual app. Finally, Chrome OS has a self-check called Verified Boot that runs every time a device starts up. Beyond all this, the simple fact that you generally can’t install traditional apps on a Chromebook means there are fewer ways for bad actors to access the system. If you’re interested in Google’s Gemini AI tools, a Chromebook is a good option as well. Every Chromebook in our top picks comes with a full year of Google’s AI Pro plan — this combines the usual Google One perks like 2TB of storage and 10 percent back in purchases from the Google Store with a bunch of AI tools. You’ll get access to Gemini in Chrome, Gmail, Google Docs and other apps, Gemini 2.5 Pro in the Gemini app and more. Given that this plan is $20/month, it’s a pretty solid perk. Chromebook Plus models also include tools like the AI-powered “help me write,” the Google Photos Magic Editor and generative AI backgrounds you can create by filling in a few prompts. As for when to avoid Chromebooks, the answer is simple: If you rely heavily on a specific native application for Windows or a Mac, chances are you won’t find the exact same option on a ChromeOS device. That’s most true in fields like photo and video editing, but it can also be the case in law or finance. Plenty of businesses run on Google’s G suite software, but more still have specific requirements that a Chromebook might not match. If you’re an iPhone user, you’ll also miss out on the way the iPhone easily integrates with an iPad or Mac. For me, the big downside is not being able to access iMessage on a Chromebook. Finally, gaming Chromebooks are not ubiquitous, although they’re becoming a slightly more reasonable option with the rise of cloud gaming. In late 2022, Google and some hardware partners announced a push to make Chromebooks with cloud gaming in mind. From a hardware perspective, that means laptops with bigger screens that have higher refresh rates as well as optimizing those laptops to work with services like NVIDIA GeForce Now, Xbox Game Pass and Amazon Luna. You’ll obviously need an internet connection to use these services, but the good news is that playing modern games on a Chromebook isn’t impossible. You can also install Android games from the Google Play Store, but that’s not what most people are thinking of when they want to game on a laptop. What are the most important specs for a Chromebook? Chrome OS is lightweight and runs well on fairly modest hardware, so the most important thing to look for might not be processor power or storage space. But Google made it easier to get consistent specs and performance late last year when it introduced the Chromebook Plus initiative. Any device with a Chromebook Plus designation meets some minimum requirements, which happen to be very similar to what I’d recommend most people get if they’re looking for the best laptop they can use every day. Chromebook Plus models have at least a 12th-gen Intel Core i3 processor, or an AMD Ryzen 3 7000 series processor, both of which should be more than enough for most people. These laptops also have a minimum of 8GB of RAM and 128GB of SSD storage, which should do the trick unless you’re really pushing your Chromebook. All Chromebook Plus models have to have a 1080p webcam, which is nice in these days of constant video calling, and they also all have to have at least a 1080p FHD IPS screen. Of course, you can get higher specs or better screens if you desire, but I’ve found that basically everything included in the Chromebook Plus target specs makes for a very good experience. Google has an Auto Update policy for Chromebooks as well, and while that’s not exactly a spec, it’s worth checking before you buy. Last year, Google announced that Chromebooks would get software updates and support for an impressive 10 years after their release date. This support page lists the Auto Update expiration date for virtually every Chromebook ever, but a good rule of thumb is to buy the newest machine you can to maximize your support. How much should I spend on a Chromebook? Chromebooks started out notoriously cheap, with list prices often coming in under $300. But as they’ve gone more mainstream, they’ve transitioned from being essentially modern netbooks to some of the best laptops you’ll want to use all day. As such, prices have increased: At this point, you should expect to spend at least $400 if you want a solid daily driver. There are still many Chromebooks out there available at a low price that may be suitable as secondary devices, but a good Chromebook that can be an all-day, every-day laptop will cost more. But, notably, even the best Chromebooks usually cost less than the best Windows laptops, or even the best “regular” laptops out there. There are a handful of premium Chromebooks that approach or even exceed $1,000 that claim to offer better performance and more processing power, but I don’t recommend spending that much. Generally, that’ll get you a better design with more premium materials, as well as more powerful internals and extra storage space, like a higher-capacity SSD. Of course, you also sometimes pay for the brand name. But, the specs I outlined earlier are usually enough, and there are multiple good premium Chromebooks in the $700 to $800 range at this point. See Also: Best Laptops for 2026 Best Gaming Laptops Best 2-in-1 Laptops for 2026 Best Cheap Windows Laptops Best Laptops for College Students Other Chromebooks we tested Lenovo IdeaPad Flex 5i Chromebook Plus This was our pick for best overall Chromebook for years, and it’s still one of the better options you can find for a basic laptop that doesn’t break the bank. It’s a few years older than our current top pick, so its processor isn’t fresh and it only has 128GB of storage. It also won’t get updates from Google as long as newer models. But it still combines a nice screen and keyboard with solid performance. This laptop typically costs $500, which feels high given its a few years old and Acer’s Chromebook Plus 514 is only $350, but if you can find it on sale and can’t find the Acer it’s worth a look. ASUS CX15 This Chromebook is extremely affordable – you can currently pick it up for only $159 at Walmart. That price and its large 15.6-inch screen is mainly what it has going for it, as the Intel Celeron N4500 chip and 4GB of RAM powering it does not provide good performance if you’re doing anything more than browsing with a few tabs open. If you’re shopping for someone with extremely basic needs and have a small budget, the CX15 might fit the bill. But just be aware that you get what you pay for. Samsung Galaxy Chromebook Plus Samsung’s Galaxy Chromebook Plus, released in late 2024, is one of the more unique Chromebooks out there. It’s extremely thin and light, at 0.46 inches and 2.6 pounds, but it manages to include a 15.6-inch display in that frame. That screen is a 1080p panel that’s sharp and bright, but its 16:9 aspect ratio made things feel a bit cramped when scrolling vertically. Performance is very good, and the keyboard is solid, though I’m not a fan of the number pad as it shifts everything to the left. At $700 it’s not cheap, but that feels fair considering its size and capabilities. If you’re looking for a big screen laptop that is also super light, this Chromebook merits consideration, even if it’s not the best option for everyone.This article originally appeared on Engadget at https://www.engadget.com/computing/laptops/best-chromebooks-160054646.html?src=rss",
          "feed_position": 28,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2024-03/9f90f9e0-e781-11ee-bfff-a29d33be5d18"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/ai-is-moving-to-the-edge-and-network-security-needs-to-catch-up",
          "published_at": "Wed, 17 Dec 2025 08:00:00 GMT",
          "title": "AI is moving to the edge – and network security needs to catch up",
          "standfirst": "Presented by T-Mobile for BusinessSmall and mid-sized businesses are adopting AI at a pace that would have seemed unrealistic even a few years ago. Smart assistants that greet customers, predictive tools that flag inventory shortages before they happen, and on-site analytics that help staff make decisions faster — these used to be features of the enterprise. Now they’re being deployed in retail storefronts, regional medical clinics, branch offices, and remote operations hubs.What’s changed is not just the AI itself, but where it runs. Increasingly, AI workloads are being pushed out of centralized data centers and into the real world — into the places where employees work and customers interact. This shift to the edge promises faster insights and more resilient operations, but it also transforms the demands placed on the network. Edge sites need consistent bandwidth, real-time data pathways, and the ability to process information locally rather than relying on the cloud for every decision.The catch is that as companies race to connect these locations, security often lags behind. A store may adopt AI-enabled cameras or sensors long before it has the policies to manage them. A clinic may roll out mobile diagnostic devices without fully segmenting their traffic. A warehouse may rely on a mix of Wi-Fi, wired, and cellular connections that weren’t designed to support AI-driven operations. When connectivity scales faster than security, it creates cracks — unmonitored devices, inconsistent access controls, and unsegmented data flows that make it hard to see what’s happening, let alone protect it.Edge AI only delivers its full value when connectivity and security evolve together.Why AI is moving to the edge — and what that breaksBusinesses are shifting AI to the edge for three core reasons:Real-time responsiveness: Some decisions can’t wait for a round trip to the cloud. Whether it’s identifying an item on a shelf, detecting an abnormal reading from a medical device, or recognizing a safety risk in a warehouse aisle, the delay introduced by centralized processing can mean missed opportunities or slow reactions.Resilience and privacy: Keeping data and inference local makes operations less vulnerable to outages or latency spikes, and it reduces the flow of sensitive information across networks. This helps SMBs meet data sovereignty and compliance requirements without rewriting their entire infrastructure.Mobility and deployment speed: Many SMBs operate across distributed footprints — remote workers, pop-up locations, seasonal operations, or mobile teams. Wireless-first connectivity, including 5G business lines, lets them deploy AI tools quickly without waiting for fixed circuits or expensive buildouts.Technologies like Edge Control from T-Mobile for Business fit naturally into this model. By routing traffic directly along the paths it needs — keeping latency-sensitive workloads local and bypassing the bottlenecks that traditional VPNs introduce — businesses can adopt edge AI without dragging their network into constant contention.Yet the shift introduces new risk. Every edge site becomes, in effect, its own small data center. A retail store may have cameras, sensors, POS systems, digital signage, and staff devices all sharing the same access point. A clinic may run diagnostic tools, tablets, wearables, and video consult systems side by side. A manufacturing floor might combine robotics, sensors, handheld scanners, and on-site analytics platforms.This diversity increases the attack surface dramatically. Many SMBs roll out connectivity first, then add piecemeal security later — leaving the blind spots attackers rely on.Zero trust becomes essential at the edgeWhen AI is distributed across dozens or hundreds of sites, the old idea of a single secure “inside” network breaks down. Every store, clinic, kiosk, or field location becomes its own micro-environment — and every device within it becomes its own potential entry point.Zero trust offers a framework to make this manageable.At the edge, zero trust means:Verifying identity rather than location — access is granted because a user or device proves who it is, not because it sits behind a corporate firewall.Continuous authentication — trust isn’t permanent; it’s re-evaluated throughout a session.Segmentation that limits movement — if something goes wrong, attackers can’t jump freely from system to system.This approach is especially critical given that many edge devices can’t run traditional security clients. SIM-based identity and secure mobile connectivity — areas where T-Mobile for Business brings significant strength — help verify IoT devices, 5G routers, and sensors that otherwise sit outside the visibility of IT teams.This is why connectivity providers are increasingly combining networking and security into a single approach. T-Mobile for Business embeds segmentation, device visibility, and zero-trust safeguards directly into its wireless-first connectivity offerings, reducing the need for SMBs to stitch together multiple tools.Secure-by-default networks reshape the landscapeA major architectural shift is underway: networks that assume every device, session, and workload must be authenticated, segmented, and monitored from the start. Instead of building security on top of connectivity, the two are fused.T-Mobile for Business solutions shows how this is evolving. Its SASE platform, powered by Palo Alto Networks Prisma SASE 5G, blends secure access with connectivity into one cloud-delivered service. Private Access gives users the least-privileged access they need, nothing more. T-SIMsecure authenticates devices at the SIM layer, allowing IoT sensors and 5G routers to be verified automatically. Security Slice isolates sensitive SASE traffic on a dedicated portion of the 5G network, ensuring consistency even during heavy demand.A unified dashboard like T-Platform brings it together, offering real-time visibility across SASE, IoT, business internet, and edge control — simplifying operations for SMBs with limited staff.The future: AI that runs the edge and protects itAs AI models become more dynamic and autonomous, we’ll see the relationship flip: the edge won’t just support AI; AI will actively run and secure the edge — optimizing traffic paths, adjusting segmentation automatically, and spotting anomalies that matter to one specific store or site.Self-healing networks and adaptive policy engines will move from experimental to expected.For SMBs, this is a pivotal moment. The organizations that modernize their connectivity and security foundations now will be the ones best positioned to scale AI everywhere — safely, confidently, and without unnecessary complexity.Partners like T-Mobile for Business are already moving in this direction, giving SMBs a way to deploy AI at the edge without sacrificing control or visibility.Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.",
          "content": "Presented by T-Mobile for BusinessSmall and mid-sized businesses are adopting AI at a pace that would have seemed unrealistic even a few years ago. Smart assistants that greet customers, predictive tools that flag inventory shortages before they happen, and on-site analytics that help staff make decisions faster — these used to be features of the enterprise. Now they’re being deployed in retail storefronts, regional medical clinics, branch offices, and remote operations hubs.What’s changed is not just the AI itself, but where it runs. Increasingly, AI workloads are being pushed out of centralized data centers and into the real world — into the places where employees work and customers interact. This shift to the edge promises faster insights and more resilient operations, but it also transforms the demands placed on the network. Edge sites need consistent bandwidth, real-time data pathways, and the ability to process information locally rather than relying on the cloud for every decision.The catch is that as companies race to connect these locations, security often lags behind. A store may adopt AI-enabled cameras or sensors long before it has the policies to manage them. A clinic may roll out mobile diagnostic devices without fully segmenting their traffic. A warehouse may rely on a mix of Wi-Fi, wired, and cellular connections that weren’t designed to support AI-driven operations. When connectivity scales faster than security, it creates cracks — unmonitored devices, inconsistent access controls, and unsegmented data flows that make it hard to see what’s happening, let alone protect it.Edge AI only delivers its full value when connectivity and security evolve together.Why AI is moving to the edge — and what that breaksBusinesses are shifting AI to the edge for three core reasons:Real-time responsiveness: Some decisions can’t wait for a round trip to the cloud. Whether it’s identifying an item on a shelf, detecting an abnormal reading from a medical device, or recognizing a safety risk in a warehouse aisle, the delay introduced by centralized processing can mean missed opportunities or slow reactions.Resilience and privacy: Keeping data and inference local makes operations less vulnerable to outages or latency spikes, and it reduces the flow of sensitive information across networks. This helps SMBs meet data sovereignty and compliance requirements without rewriting their entire infrastructure.Mobility and deployment speed: Many SMBs operate across distributed footprints — remote workers, pop-up locations, seasonal operations, or mobile teams. Wireless-first connectivity, including 5G business lines, lets them deploy AI tools quickly without waiting for fixed circuits or expensive buildouts.Technologies like Edge Control from T-Mobile for Business fit naturally into this model. By routing traffic directly along the paths it needs — keeping latency-sensitive workloads local and bypassing the bottlenecks that traditional VPNs introduce — businesses can adopt edge AI without dragging their network into constant contention.Yet the shift introduces new risk. Every edge site becomes, in effect, its own small data center. A retail store may have cameras, sensors, POS systems, digital signage, and staff devices all sharing the same access point. A clinic may run diagnostic tools, tablets, wearables, and video consult systems side by side. A manufacturing floor might combine robotics, sensors, handheld scanners, and on-site analytics platforms.This diversity increases the attack surface dramatically. Many SMBs roll out connectivity first, then add piecemeal security later — leaving the blind spots attackers rely on.Zero trust becomes essential at the edgeWhen AI is distributed across dozens or hundreds of sites, the old idea of a single secure “inside” network breaks down. Every store, clinic, kiosk, or field location becomes its own micro-environment — and every device within it becomes its own potential entry point.Zero trust offers a framework to make this manageable.At the edge, zero trust means:Verifying identity rather than location — access is granted because a user or device proves who it is, not because it sits behind a corporate firewall.Continuous authentication — trust isn’t permanent; it’s re-evaluated throughout a session.Segmentation that limits movement — if something goes wrong, attackers can’t jump freely from system to system.This approach is especially critical given that many edge devices can’t run traditional security clients. SIM-based identity and secure mobile connectivity — areas where T-Mobile for Business brings significant strength — help verify IoT devices, 5G routers, and sensors that otherwise sit outside the visibility of IT teams.This is why connectivity providers are increasingly combining networking and security into a single approach. T-Mobile for Business embeds segmentation, device visibility, and zero-trust safeguards directly into its wireless-first connectivity offerings, reducing the need for SMBs to stitch together multiple tools.Secure-by-default networks reshape the landscapeA major architectural shift is underway: networks that assume every device, session, and workload must be authenticated, segmented, and monitored from the start. Instead of building security on top of connectivity, the two are fused.T-Mobile for Business solutions shows how this is evolving. Its SASE platform, powered by Palo Alto Networks Prisma SASE 5G, blends secure access with connectivity into one cloud-delivered service. Private Access gives users the least-privileged access they need, nothing more. T-SIMsecure authenticates devices at the SIM layer, allowing IoT sensors and 5G routers to be verified automatically. Security Slice isolates sensitive SASE traffic on a dedicated portion of the 5G network, ensuring consistency even during heavy demand.A unified dashboard like T-Platform brings it together, offering real-time visibility across SASE, IoT, business internet, and edge control — simplifying operations for SMBs with limited staff.The future: AI that runs the edge and protects itAs AI models become more dynamic and autonomous, we’ll see the relationship flip: the edge won’t just support AI; AI will actively run and secure the edge — optimizing traffic paths, adjusting segmentation automatically, and spotting anomalies that matter to one specific store or site.Self-healing networks and adaptive policy engines will move from experimental to expected.For SMBs, this is a pivotal moment. The organizations that modernize their connectivity and security foundations now will be the ones best positioned to scale AI everywhere — safely, confidently, and without unnecessary complexity.Partners like T-Mobile for Business are already moving in this direction, giving SMBs a way to deploy AI at the edge without sacrificing control or visibility.Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.",
          "feed_position": 4,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/46j0tsxQUwAmA3wt8aoaBL/7c4104fa0039b7baa791c3d79fcd528c/AdobeStock_1140886327.jpeg?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/infrastructure/why-googles-new-interactions-api-is-such-a-big-deal-for-ai-developers",
          "published_at": "Wed, 17 Dec 2025 03:21:00 GMT",
          "title": "Why Google's new Interactions API is such a big deal for AI developers",
          "standfirst": "For the last two years, the fundamental unit of generative AI development has been the \"completion.\" You send a text prompt to a model, it sends text back, and the transaction ends. If you want to continue the conversation, you have to send the entire history back to the model again. This \"stateless\" architecture—embodied by Google&#x27;s legacy generateContent endpoint—was perfect for simple chatbots. But as developers move toward autonomous agents that use tools, maintain complex states, and \"think\" over long horizons, that stateless model has become a distinct bottleneck.Last week, Google DeepMind finally addressed this infrastructure gap with the public beta launch of the Interactions API (/interactions).While OpenAI began this shift back in March 2025 with its Responses API, Google’s entry signals its own efforts to advance the state-of-the-art. The Interactions API is not just a state management tool; it is a unified interface designed to treat LLMs less like text generators and more like remote operating systems.The &#x27;Remote Compute&#x27; ModelThe core innovation of the Interactions API is the introduction of server-side state as a default behavior.Previously, a developer building a complex agent had to manually manage a growing JSON list of every \"user\" and \"model\" turn, sending megabytes of history back and forth with every request. With the new API, developers simply pass a previous_interaction_id. Google’s infrastructure retains the conversation history, tool outputs, and \"thought\" processes on their end.\"Models are becoming systems and over time, might even become agents themselves,\" wrote DeepMind&#x27;s Ali Çevik and Philipp Schmid, in an official company blog post on the new paradigm. \"Trying to force these capabilities into generateContent would have resulted in an overly complex and fragile API.\"This shift enables Background Execution, a critical feature for the agentic era. Complex workflows—like browsing the web for an hour to synthesize a report—often trigger HTTP timeouts in standard APIs. The Interactions API allows developers to trigger an agent with background=true, disconnect, and poll for the result later. It effectively turns the API into a job queue for intelligence.Native \"Deep Research\" and MCP SupportGoogle is using this new infrastructure to deliver its first built-in agent: Gemini Deep Research.Accessible via the same /interactions endpoint, this agent is capable of executing \"long-horizon research tasks.\" Unlike a standard model that predicts the next token based on your prompt, the Deep Research agent executes a loop of searches, reading, and synthesis.Crucially, Google is also embracing the open ecosystem by adding native support for the Model Context Protocol (MCP). This allows Gemini models to directly call external tools hosted on remote servers—such as a weather service or a database—without the developer having to write custom glue code to parse the tool calls.The Landscape: Google Joins OpenAI in the &#x27;Stateful&#x27; EraGoogle is arguably playing catch-up, but with a distinct philosophical twist. OpenAI moved away from statelessness nine months ago with the launch of the Responses API in March 2025.While both giants are solving the problem of context bloat, their solutions diverge on transparency:OpenAI (The Compression Approach): OpenAI&#x27;s Responses API introduced Compaction—a feature that shrinks conversation history by replacing tool outputs and reasoning chains with opaque \"encrypted compaction items.\" This prioritizes token efficiency but creates a \"black box\" where the model&#x27;s past reasoning is hidden from the developer.Google (The Hosted Approach): Google’s Interactions API keeps the full history available and composable. The data model allows developers to \"debug, manipulate, stream and reason over interleaved messages.\" It prioritizes inspectability over compression.Supported Models & AvailabilityThe Interactions API is currently in Public Beta (documentation here) and is available immediately via Google AI Studio. It supports the full spectrum of Google’s latest generation models, ensuring that developers can match the right model size to their specific agentic task:Gemini 3.0: Gemini 3 Pro Preview.Gemini 2.5: Flash, Flash-lite, and Pro.Agents: Deep Research Preview (deep-research-pro-preview-12-2025).Commercially, the API integrates into Google’s existing pricing structure—you pay standard rates for input and output tokens based on the model you select. However, the value proposition changes with the new data retention policies. Because this API is stateful, Google must store your interaction history to enable features like implicit caching and context retrieval.Access to this storage is determined by your tier. Developers on the Free Tier are limited to a 1-day retention policy, suitable for ephemeral testing but insufficient for long-term agent memory. Developers on the Paid Tier unlock a 55-day retention policy. This extended retention is not just for auditing; it effectively lowers your total cost of ownership by maximizing cache hits. By keeping the history \"hot\" on the server for nearly two months, you avoid paying to re-process massive context windows for recurring users, making the Paid Tier significantly more efficient for production-grade agents.Note: As this is a Beta release, Google has advised that features and schemas are subject to breaking changes.&#x27;You Are Interacting With a System&#x27;Sam Witteveen, a Google Developer Expert in Machine Learning and CEO of Red Dragon AI, sees this release as a necessary evolution of the developer stack.\"If we go back in history... the whole idea was simple text-in, text-out,\" Witteveen noted in a technical breakdown of the release on YouTube. \"But now... you are interacting with a system. A system that can use multiple models, do multiple loops of calls, use tools, and do code execution on the backend.\"Witteveen highlighted the immediate economic benefit of this architecture: Implicit Caching. Because the conversation history lives on Google’s servers, developers aren&#x27;t charged for re-uploading the same context repeatedly. \"You don&#x27;t have to pay as much for the tokens that you are calling,\" he explained.However, the release is not without friction. Witteveen critiqued the current implementation of the Deep Research agent&#x27;s citation system. While the agent provides sources, the URLs returned are often wrapped in internal Google/Vertex AI redirection links rather than raw, usable URLs.\"My biggest gripe is that... these URLs, if I save them and try to use them in a different session, they&#x27;re not going to work,\" Witteveen warned. \"If I want to make a report for someone with citations, I want them to be able to click on the URLs from a PDF file... Having something like medium.com as a citation [without the direct link] is not very good.\"What This Means for Your TeamFor Lead AI Engineers focused on rapid model deployment and fine-tuning, this release offers a direct architectural solution to the persistent \"timeout\" problem: Background Execution. Instead of building complex asynchronous handlers or managing separate job queues for long-running reasoning tasks, you can now offload this complexity directly to Google. However, this convenience introduces a strategic trade-off. While the new Deep Research agent allows for the rapid deployment of sophisticated research capabilities, it operates as a \"black box\" compared to custom-built LangChain or LangGraph flows. Engineers should prototype a \"slow thinking\" feature using the background=true parameter to evaluate if the speed of implementation outweighs the loss of fine-grained control over the research loop.Senior engineers managing AI orchestration and budget will find that the shift to server-side state via previous_interaction_id unlocks Implicit Caching, a major win for both cost and latency metrics. By referencing history stored on Google’s servers, you automatically avoid the token costs associated with re-uploading massive context windows, directly addressing budget constraints while maintaining high performance. The challenge here lies in the supply chain; incorporating Remote MCP (Model Context Protocol) means your agents are connecting directly to external tools, requiring you to rigorously validate that these remote services are secure and authenticated. It is time to audit your current token spend on re-sending conversation history—if it is high, prioritizing a migration to the stateful Interactions API could capture significant savings.For Senior Data Engineers, the Interactions API offers a more robust data model than raw text logs. The structured schema allows for complex histories to be debugged and reasoned over, improving overall Data Integrity across your pipelines. However, you must remain vigilant regarding Data Quality, specifically the issue raised by expert Sam Witteveen regarding citations. The Deep Research agent currently returns \"wrapped\" URLs that may expire or break, rather than raw source links. If your pipelines rely on scraping or archiving these sources, you may need to build a cleaning step to extract the usable URLs. You should also test the structured output capabilities (response_format) to see if they can replace fragile regex parsing in your current ETL pipelines.Finally, for Directors of IT Security, moving state to Google’s centralized servers offers a paradox. It can improve security by keeping API keys and conversation history off client devices, but it introduces a new data residency risk. The critical check here is Google&#x27;s Data Retention Policies: while the Free Tier retains data for only one day, the Paid Tier retains interaction history for 55 days. This stands in contrast to OpenAI’s \"Zero Data Retention\" (ZDR) enterprise options. You must ensure that storing sensitive conversation history for nearly two months complies with your internal governance. If this violates your policy, you must configure calls with store=false, though doing so will disable the stateful features—and the cost benefits—that make this new API valuable.",
          "content": "For the last two years, the fundamental unit of generative AI development has been the \"completion.\" You send a text prompt to a model, it sends text back, and the transaction ends. If you want to continue the conversation, you have to send the entire history back to the model again. This \"stateless\" architecture—embodied by Google&#x27;s legacy generateContent endpoint—was perfect for simple chatbots. But as developers move toward autonomous agents that use tools, maintain complex states, and \"think\" over long horizons, that stateless model has become a distinct bottleneck.Last week, Google DeepMind finally addressed this infrastructure gap with the public beta launch of the Interactions API (/interactions).While OpenAI began this shift back in March 2025 with its Responses API, Google’s entry signals its own efforts to advance the state-of-the-art. The Interactions API is not just a state management tool; it is a unified interface designed to treat LLMs less like text generators and more like remote operating systems.The &#x27;Remote Compute&#x27; ModelThe core innovation of the Interactions API is the introduction of server-side state as a default behavior.Previously, a developer building a complex agent had to manually manage a growing JSON list of every \"user\" and \"model\" turn, sending megabytes of history back and forth with every request. With the new API, developers simply pass a previous_interaction_id. Google’s infrastructure retains the conversation history, tool outputs, and \"thought\" processes on their end.\"Models are becoming systems and over time, might even become agents themselves,\" wrote DeepMind&#x27;s Ali Çevik and Philipp Schmid, in an official company blog post on the new paradigm. \"Trying to force these capabilities into generateContent would have resulted in an overly complex and fragile API.\"This shift enables Background Execution, a critical feature for the agentic era. Complex workflows—like browsing the web for an hour to synthesize a report—often trigger HTTP timeouts in standard APIs. The Interactions API allows developers to trigger an agent with background=true, disconnect, and poll for the result later. It effectively turns the API into a job queue for intelligence.Native \"Deep Research\" and MCP SupportGoogle is using this new infrastructure to deliver its first built-in agent: Gemini Deep Research.Accessible via the same /interactions endpoint, this agent is capable of executing \"long-horizon research tasks.\" Unlike a standard model that predicts the next token based on your prompt, the Deep Research agent executes a loop of searches, reading, and synthesis.Crucially, Google is also embracing the open ecosystem by adding native support for the Model Context Protocol (MCP). This allows Gemini models to directly call external tools hosted on remote servers—such as a weather service or a database—without the developer having to write custom glue code to parse the tool calls.The Landscape: Google Joins OpenAI in the &#x27;Stateful&#x27; EraGoogle is arguably playing catch-up, but with a distinct philosophical twist. OpenAI moved away from statelessness nine months ago with the launch of the Responses API in March 2025.While both giants are solving the problem of context bloat, their solutions diverge on transparency:OpenAI (The Compression Approach): OpenAI&#x27;s Responses API introduced Compaction—a feature that shrinks conversation history by replacing tool outputs and reasoning chains with opaque \"encrypted compaction items.\" This prioritizes token efficiency but creates a \"black box\" where the model&#x27;s past reasoning is hidden from the developer.Google (The Hosted Approach): Google’s Interactions API keeps the full history available and composable. The data model allows developers to \"debug, manipulate, stream and reason over interleaved messages.\" It prioritizes inspectability over compression.Supported Models & AvailabilityThe Interactions API is currently in Public Beta (documentation here) and is available immediately via Google AI Studio. It supports the full spectrum of Google’s latest generation models, ensuring that developers can match the right model size to their specific agentic task:Gemini 3.0: Gemini 3 Pro Preview.Gemini 2.5: Flash, Flash-lite, and Pro.Agents: Deep Research Preview (deep-research-pro-preview-12-2025).Commercially, the API integrates into Google’s existing pricing structure—you pay standard rates for input and output tokens based on the model you select. However, the value proposition changes with the new data retention policies. Because this API is stateful, Google must store your interaction history to enable features like implicit caching and context retrieval.Access to this storage is determined by your tier. Developers on the Free Tier are limited to a 1-day retention policy, suitable for ephemeral testing but insufficient for long-term agent memory. Developers on the Paid Tier unlock a 55-day retention policy. This extended retention is not just for auditing; it effectively lowers your total cost of ownership by maximizing cache hits. By keeping the history \"hot\" on the server for nearly two months, you avoid paying to re-process massive context windows for recurring users, making the Paid Tier significantly more efficient for production-grade agents.Note: As this is a Beta release, Google has advised that features and schemas are subject to breaking changes.&#x27;You Are Interacting With a System&#x27;Sam Witteveen, a Google Developer Expert in Machine Learning and CEO of Red Dragon AI, sees this release as a necessary evolution of the developer stack.\"If we go back in history... the whole idea was simple text-in, text-out,\" Witteveen noted in a technical breakdown of the release on YouTube. \"But now... you are interacting with a system. A system that can use multiple models, do multiple loops of calls, use tools, and do code execution on the backend.\"Witteveen highlighted the immediate economic benefit of this architecture: Implicit Caching. Because the conversation history lives on Google’s servers, developers aren&#x27;t charged for re-uploading the same context repeatedly. \"You don&#x27;t have to pay as much for the tokens that you are calling,\" he explained.However, the release is not without friction. Witteveen critiqued the current implementation of the Deep Research agent&#x27;s citation system. While the agent provides sources, the URLs returned are often wrapped in internal Google/Vertex AI redirection links rather than raw, usable URLs.\"My biggest gripe is that... these URLs, if I save them and try to use them in a different session, they&#x27;re not going to work,\" Witteveen warned. \"If I want to make a report for someone with citations, I want them to be able to click on the URLs from a PDF file... Having something like medium.com as a citation [without the direct link] is not very good.\"What This Means for Your TeamFor Lead AI Engineers focused on rapid model deployment and fine-tuning, this release offers a direct architectural solution to the persistent \"timeout\" problem: Background Execution. Instead of building complex asynchronous handlers or managing separate job queues for long-running reasoning tasks, you can now offload this complexity directly to Google. However, this convenience introduces a strategic trade-off. While the new Deep Research agent allows for the rapid deployment of sophisticated research capabilities, it operates as a \"black box\" compared to custom-built LangChain or LangGraph flows. Engineers should prototype a \"slow thinking\" feature using the background=true parameter to evaluate if the speed of implementation outweighs the loss of fine-grained control over the research loop.Senior engineers managing AI orchestration and budget will find that the shift to server-side state via previous_interaction_id unlocks Implicit Caching, a major win for both cost and latency metrics. By referencing history stored on Google’s servers, you automatically avoid the token costs associated with re-uploading massive context windows, directly addressing budget constraints while maintaining high performance. The challenge here lies in the supply chain; incorporating Remote MCP (Model Context Protocol) means your agents are connecting directly to external tools, requiring you to rigorously validate that these remote services are secure and authenticated. It is time to audit your current token spend on re-sending conversation history—if it is high, prioritizing a migration to the stateful Interactions API could capture significant savings.For Senior Data Engineers, the Interactions API offers a more robust data model than raw text logs. The structured schema allows for complex histories to be debugged and reasoned over, improving overall Data Integrity across your pipelines. However, you must remain vigilant regarding Data Quality, specifically the issue raised by expert Sam Witteveen regarding citations. The Deep Research agent currently returns \"wrapped\" URLs that may expire or break, rather than raw source links. If your pipelines rely on scraping or archiving these sources, you may need to build a cleaning step to extract the usable URLs. You should also test the structured output capabilities (response_format) to see if they can replace fragile regex parsing in your current ETL pipelines.Finally, for Directors of IT Security, moving state to Google’s centralized servers offers a paradox. It can improve security by keeping API keys and conversation history off client devices, but it introduces a new data residency risk. The critical check here is Google&#x27;s Data Retention Policies: while the Free Tier retains data for only one day, the Paid Tier retains interaction history for 55 days. This stands in contrast to OpenAI’s \"Zero Data Retention\" (ZDR) enterprise options. You must ensure that storing sensitive conversation history for nearly two months complies with your internal governance. If this violates your policy, you must configure calls with store=false, though doing so will disable the stateful features—and the cost benefits—that make this new API valuable.",
          "feed_position": 5,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/5OKuwmihZkysC3G6qE6Lvr/ffb6afcf05b65493ada4348587cec2d4/yFbr_v7c7mxthUkaoWhwD.png?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/infrastructure/agentic-design-patterns-the-missing-link-between-ai-demos-and-enterprise",
          "published_at": "Wed, 17 Dec 2025 00:00:00 GMT",
          "title": "Agentic design patterns: The missing link between AI demos and enterprise value",
          "standfirst": "The enterprise AI market is currently nursing a massive hangover. For the past two years, decision-makers have been inundated with demos of autonomous agents booking flights, writing code, and analyzing data. Yet, the reality on the ground is starkly different. While experimentation is at an all-time high, deployment of reliable, autonomous agents in production remains challenging. A recent study by MIT’s Project NANDA highlighted a sobering statistic: Roughly 95% of AI projects fail to deliver bottom-line value. They hit walls when moved from the sandbox to the real world, often breaking under the weight of edge cases, hallucinations, or integration failures.According to Antonio Gulli, a senior engineer at Google and the Director of the Engineering Office of the CTO, the industry is suffering from a fundamental misunderstanding of what agents actually are. We have treated them as magic boxes rather than complex software systems. \"AI engineering, especially with large models and agents, is really no different from any form of engineering, like software or civil engineering,\" Gulli said in an exclusive interview with VentureBeat. \"To build something lasting, you cannot just chase the latest model or framework.\"Gulli argues that the solution to the \"trough of disillusionment\" is not a smarter model, but better architecture. His recent book, \"Agentic Design Patterns,\" provides repeatable, rigorous architectural standards that turn \"toy\" agents into reliable enterprise tools. The book pays homage to the original \"Design Patterns\" (one of my favorite books on software engineering), which brought order to object-oriented programming in the 1990s.Gulli introduces 21 fundamental patterns that serve as the building blocks for reliable agentic systems. These are practical engineering structures that dictate how an agent thinks, remembers, and acts. \"Of course, it&#x27;s important to have the state-of-the-art, but you need to step back and reflect on the fundamental principles driving AI systems,\" Gulli said. \"These patterns are the engineering foundation that improves the solution quality.\"The enterprise survival kitFor enterprise leaders looking to stabilize their AI stack, Gulli identifies five \"low-hanging fruit\" patterns that offer the highest immediate impact: Reflection, Routing, Communication, Guardrails, and Memory. The most critical shift in agent design is the move from simple \"stimulus-response\" bots to systems capable of Reflection. A standard LLM tries to answer a query immediately, which often leads to hallucination. A reflective agent, however, mimics human reasoning by creating a plan, executing it, and then critiquing its own output before presenting it to the user. This internal feedback loop is often the difference between a wrong answer and a correct one.Once an agent can think, it needs to be efficient. This is where Routing becomes essential for cost control. Instead of sending every query to a massive, expensive \"God model,\" a routing layer analyzes the complexity of the request. Simple tasks are directed to faster, cheaper models, while complex reasoning is reserved for the heavy hitters. This architecture allows enterprises to scale without blowing up their inference budgets. “A model can act as a router to other models, or even the same model with different system prompts and functions,” Gulli said.Connecting these agents to the outside world requires standardized Communication by giving models access to tools such as search, queries, and code execution. In the past, connecting an LLM to a database meant writing custom, brittle code. Gulli points to the rise of the Model Context Protocol (MCP) as a pivotal moment. MCP acts like a USB port for AI, providing a standardized way for agents to plug into data sources and tools. This standardization extends to \"Agent-to-Agent\" (A2A) communication, allowing specialized agents to collaborate on complex tasks without custom integration overhead.However, even a smart, efficient agent is useless if it cannot retain information. Memory patterns solve the \"goldfish\" problem, where agents forget instructions over long conversations. By structuring how an agent stores and retrieves past interactions and experiences, developers can create persistent, context-aware assistants. “The way you create memory is fundamental for the quality of the agents,” Gulli said.Finally, none of this matters if the agent is a liability. Guardrails provide the necessary constraints to ensure an agent operates within safety and compliance boundaries. This goes beyond a simple system prompt asking the model to \"be nice\"; it involves architectural checks and escalation policies that prevent data leakage or unauthorized actions. Gulli emphasizes that defining these \"hard\" boundaries is \"extremely important\" for security, ensuring that an agent trying to be helpful doesn&#x27;t accidentally expose private data or execute irreversible commands outside its authorized scope.Fixing reliability with transactional safetyFor many CIOs, the hesitation to deploy agents stems from fear. An autonomous agent that can read emails or modify files poses a significant risk if it goes off the rails. Gulli addresses this by borrowing a concept from database management: transactional safety. \"If an agent takes an action, we must implement checkpoints and rollbacks, just as we do for transactional safety in databases,\" Gulli said.In this model, an agent’s actions are tentative until validated. If the system detects an anomaly or an error, it can \"rollback\" to a previous safe state, undoing the agent’s actions. This safety net allows enterprises to trust agents with write-access to systems, knowing there is an undo button. Testing these systems requires a new approach as well. Traditional unit tests check if a function returns the right value, but an agent might arrive at the right answer via a flawed, dangerous process. Gulli advocates for evaluating Agent Trajectories, metrics that evaluate how agents behave over time.“[Agent Trajectories] involves analyzing the entire sequence of decisions and tools used to reach a conclusion, ensuring the full process is sound, not just the final answer,” he said.This is often augmented by the Critique pattern, where a separate, specialized agent is tasked with judging the performance of the primary agent. This mutual check is fundamental to preventing the propagation of errors, essentially creating an automated peer-review system for AI decisions.Future-proofing: From prompt engineering to context engineeringLooking toward 2026, the era of the single, general-purpose model is likely ending. Gulli predicts a shift toward a landscape dominated by fleets of specialized agents. \"I strongly believe we will see a specialization of agents,\" he said. \"The model will still be the brain... but the agents will become truly multi-agent systems with specialized tasks—agents focusing on retrieval, image generation, video creation — communicating with each other.\"In this future, the primary skill for developers will not be to coax a model into working with clever phrasing and prompt engineering. Instead, they will need to focus on context engineering, the discipline that focuses on designing the information flow, managing the state, and curating the context that the model \"sees.\" It is a move from linguistic trickery to systems engineering. By adopting these patterns and focusing on the \"plumbing\" of AI rather than just the models, enterprises can finally bridge the gap between the hype and the bottom line. \"We should not use AI just for the sake of AI,\" Gulli warns. \"We must start with a clear definition of the business problem and how to best leverage the technology to solve it.\"",
          "content": "The enterprise AI market is currently nursing a massive hangover. For the past two years, decision-makers have been inundated with demos of autonomous agents booking flights, writing code, and analyzing data. Yet, the reality on the ground is starkly different. While experimentation is at an all-time high, deployment of reliable, autonomous agents in production remains challenging. A recent study by MIT’s Project NANDA highlighted a sobering statistic: Roughly 95% of AI projects fail to deliver bottom-line value. They hit walls when moved from the sandbox to the real world, often breaking under the weight of edge cases, hallucinations, or integration failures.According to Antonio Gulli, a senior engineer at Google and the Director of the Engineering Office of the CTO, the industry is suffering from a fundamental misunderstanding of what agents actually are. We have treated them as magic boxes rather than complex software systems. \"AI engineering, especially with large models and agents, is really no different from any form of engineering, like software or civil engineering,\" Gulli said in an exclusive interview with VentureBeat. \"To build something lasting, you cannot just chase the latest model or framework.\"Gulli argues that the solution to the \"trough of disillusionment\" is not a smarter model, but better architecture. His recent book, \"Agentic Design Patterns,\" provides repeatable, rigorous architectural standards that turn \"toy\" agents into reliable enterprise tools. The book pays homage to the original \"Design Patterns\" (one of my favorite books on software engineering), which brought order to object-oriented programming in the 1990s.Gulli introduces 21 fundamental patterns that serve as the building blocks for reliable agentic systems. These are practical engineering structures that dictate how an agent thinks, remembers, and acts. \"Of course, it&#x27;s important to have the state-of-the-art, but you need to step back and reflect on the fundamental principles driving AI systems,\" Gulli said. \"These patterns are the engineering foundation that improves the solution quality.\"The enterprise survival kitFor enterprise leaders looking to stabilize their AI stack, Gulli identifies five \"low-hanging fruit\" patterns that offer the highest immediate impact: Reflection, Routing, Communication, Guardrails, and Memory. The most critical shift in agent design is the move from simple \"stimulus-response\" bots to systems capable of Reflection. A standard LLM tries to answer a query immediately, which often leads to hallucination. A reflective agent, however, mimics human reasoning by creating a plan, executing it, and then critiquing its own output before presenting it to the user. This internal feedback loop is often the difference between a wrong answer and a correct one.Once an agent can think, it needs to be efficient. This is where Routing becomes essential for cost control. Instead of sending every query to a massive, expensive \"God model,\" a routing layer analyzes the complexity of the request. Simple tasks are directed to faster, cheaper models, while complex reasoning is reserved for the heavy hitters. This architecture allows enterprises to scale without blowing up their inference budgets. “A model can act as a router to other models, or even the same model with different system prompts and functions,” Gulli said.Connecting these agents to the outside world requires standardized Communication by giving models access to tools such as search, queries, and code execution. In the past, connecting an LLM to a database meant writing custom, brittle code. Gulli points to the rise of the Model Context Protocol (MCP) as a pivotal moment. MCP acts like a USB port for AI, providing a standardized way for agents to plug into data sources and tools. This standardization extends to \"Agent-to-Agent\" (A2A) communication, allowing specialized agents to collaborate on complex tasks without custom integration overhead.However, even a smart, efficient agent is useless if it cannot retain information. Memory patterns solve the \"goldfish\" problem, where agents forget instructions over long conversations. By structuring how an agent stores and retrieves past interactions and experiences, developers can create persistent, context-aware assistants. “The way you create memory is fundamental for the quality of the agents,” Gulli said.Finally, none of this matters if the agent is a liability. Guardrails provide the necessary constraints to ensure an agent operates within safety and compliance boundaries. This goes beyond a simple system prompt asking the model to \"be nice\"; it involves architectural checks and escalation policies that prevent data leakage or unauthorized actions. Gulli emphasizes that defining these \"hard\" boundaries is \"extremely important\" for security, ensuring that an agent trying to be helpful doesn&#x27;t accidentally expose private data or execute irreversible commands outside its authorized scope.Fixing reliability with transactional safetyFor many CIOs, the hesitation to deploy agents stems from fear. An autonomous agent that can read emails or modify files poses a significant risk if it goes off the rails. Gulli addresses this by borrowing a concept from database management: transactional safety. \"If an agent takes an action, we must implement checkpoints and rollbacks, just as we do for transactional safety in databases,\" Gulli said.In this model, an agent’s actions are tentative until validated. If the system detects an anomaly or an error, it can \"rollback\" to a previous safe state, undoing the agent’s actions. This safety net allows enterprises to trust agents with write-access to systems, knowing there is an undo button. Testing these systems requires a new approach as well. Traditional unit tests check if a function returns the right value, but an agent might arrive at the right answer via a flawed, dangerous process. Gulli advocates for evaluating Agent Trajectories, metrics that evaluate how agents behave over time.“[Agent Trajectories] involves analyzing the entire sequence of decisions and tools used to reach a conclusion, ensuring the full process is sound, not just the final answer,” he said.This is often augmented by the Critique pattern, where a separate, specialized agent is tasked with judging the performance of the primary agent. This mutual check is fundamental to preventing the propagation of errors, essentially creating an automated peer-review system for AI decisions.Future-proofing: From prompt engineering to context engineeringLooking toward 2026, the era of the single, general-purpose model is likely ending. Gulli predicts a shift toward a landscape dominated by fleets of specialized agents. \"I strongly believe we will see a specialization of agents,\" he said. \"The model will still be the brain... but the agents will become truly multi-agent systems with specialized tasks—agents focusing on retrieval, image generation, video creation — communicating with each other.\"In this future, the primary skill for developers will not be to coax a model into working with clever phrasing and prompt engineering. Instead, they will need to focus on context engineering, the discipline that focuses on designing the information flow, managing the state, and curating the context that the model \"sees.\" It is a move from linguistic trickery to systems engineering. By adopting these patterns and focusing on the \"plumbing\" of AI rather than just the models, enterprises can finally bridge the gap between the hype and the bottom line. \"We should not use AI just for the sake of AI,\" Gulli warns. \"We must start with a clear definition of the business problem and how to best leverage the technology to solve it.\"",
          "feed_position": 6,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/1yOe7kspaeOj7DzSNo65Xo/cef0e42d94b03ff85f2f99a2a9478c8e/Agentic_design_patterns.jpg?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/big-tech/ces-2026-preview-what-were-expecting-from-techs-biggest-conference-in-january-120000768.html",
          "published_at": "Tue, 16 Dec 2025 18:36:35 +0000",
          "title": "CES 2026 preview: What we're expecting from tech's biggest conference in January",
          "standfirst": "CES doesn't start until January, but whispers of the products and announcements that could be in store for tech's biggest annual conference have already started to take shape. The CES 2026 show floor is officially open from January 6 through 9, although the show kicks off with events on Sunday January 4 and a host of press conferences on Monday. As always, product demos, announcements and networking will be happening at the Las Vegas Convention Center and surrounding hotels all over the city. As usual, Engadget will be covering the event in-person and remotely, bringing you news and hands-ons straight from the show floor.More specific details and pre-announcements should trickle out as CES approaches, but in the meantime, we do know what companies will be hosting press conferences and what tech trends could rear their heads at the show.What we already know aboutPress conferences and show floor booths are the bread and butter of CES. The Consumer Technology Association has already published a searchable directory of who will have a presence at the show, along with a schedule of every official panel and presentation.On Sunday, January 4, Samsung will kick-off CES with \"The First Look,\" a presentation hosted by TM Roh, the CEO of Samsung's DX Division, on the company's \"vision for the DX (Device eXperience) Division in 2026, along with new AI-driven customer experiences.\" That'll be followed by multiple press conferences throughout Monday, January 5. LG is hosting its \"Innovation in Tune with You\" presentation to share \"its vision for elevating daily life through Affectionate Intelligence\" at the start of the day, Intel is launching its new Core Ultra Series 3 processors in the afternoon, Sony Honda Mobility is holding a press conference on its first car and AMD CEO Lisa Su will cover AMD's upcoming chip announcements at a keynote address that closes out the day.On the week of December 15, the CTA added a keynote by NVIDIA CEO Jensen Huang to its schedule. The event will take place on January 5 at 1PM PT and, according to the website, will last about 90 minutes. Based on the description on the listing, the presentation will “showcase the latest NVIDIA solutions driving innovation and productivity across industries.”Finally, on Tuesday, January 6, Lenovo CEO Yuanqing Yang will host Lenovo's Tech World Conference at Sphere, using the large and decidedly curved screen to share the company's \"commitment to delivering smarter AI for all by constantly redefining how technology can engage, inspire, and empower.\" It’s worth noting that Lenovo is the parent company of Motorola, which still makes phones and foldables that feature AI tools, so it’s possible those devices feature in the presentation as well. Outside of the formal introduction of new products and initiatives, reading the tea leaves of what was announced last year and what companies are reportedly working on, we can make some educated guesses at what we could see at CES 2026.New chips from AMD, Intel and QualcommCES is frequently the start of a cascade of new chip announcements for a given year, and one of the first places new silicon appears in real consumer products. AMD will likely use its keynote to introduce new versions of its Ryzen chips, including the recently spotted Ryzen 7 9850X3D, which is expected to offer better single-threaded performance, and the Ryzen 9000G series, which could be built with AMD's Zen 5 architecture. The company might also use its CES stage to go over its new FSR Redstone AI upscaling tech.Intel has already publicly announced that it'll launch its Panther Lake chips at CES 2026. The officially titled Intel Core Ultra Series 3 chips fit into Intel's overall \"AI PC\" push, but are specifically meant for premium laptops. Based on a preview from October 2025, Intel says the first chip made with its 2-nanometer 18A process will offer 50 percent more processing performance than previous generations and for the chip's Arc GPU, a 50 percent performance bump from last generation.Qualcomm is also rumored to be targeting laptops at the show, building on the work it's done moving its Snapdragon chips out of phones and tablets and into other types of computers. The company's Snapdragon X2 Elite and X2 Elite Premium chips should start appearing in laptops at CES 2026, offering a look at the improved speed and AI performance the company promised in 2025.Brighter, \"truer\" screensSony announced a collection of new Bravia TVs in April 2025, replacing the company's flagship, filling in its midrange options and adding a new budget model to the mix. The star of this updated Bravia lineup is the Bravia 9, which features a QD-OLED panel, but Sony appears to be prepping entirely new display tech for 2026. In March 2025, Sony introduced a new RGB LED panel that uses individual Mini LED backlights colored in red, green and blue to produce even brighter, more accurate colors. In contrast to a QD-OLED, which filters a layer of blue organic light emitting diodes through quantum dots that change color, Sony's \"General RGB LED Backlight Technology\" can get as bright as a Mini LED panel without needing an extra filter layer or worrying about OLED's problems with burn-in. The company has already trademarked the name \"True RGB,\" which could end up being what Sony calls this new flavor of display if it decides to show them off at CES. It seems entirely likely, because CES is nothing if not a TV show — it’s a sure bet that we’ll see new TVs from the likes of LG and Samsung in addition to Sony. If the company doesn't introduce new display tech for its TVs, it does have a new 240Hz PlayStation monitor coming in 2026 that it could show off at CES instead.Sony isn't the only company hyped on bright screens. Samsung is reportedly pushing an updated version of the HDR10 and HDR10+ standards that could be ready to demo at CES 2026. The new HDR10+ Advanced standard would be Samsung's answer to Dolby Vision 2, which includes support for things bi-directional tone mapping and intelligent features that automatically adapt sports and gaming content. Samsung's take will reportedly offer improved brightness, genre-based tone mapping and intelligent motion smoothing options, among other improvements.Ballie Watch 2026The ball-shaped yellow robot lovingly known as \"Ballie\" has been announced twice, first in 2020 and then again in 2024 with a projector in tow. Samsung said Ballie would go on sale in 2025 at CES last year and then shared in April 2025 that Ballie would ship this summer with Google's Gemini onboard. But it's nearly 2026, and Ballie is nowhere to be seen. It's possible Samsung could make a third attempt at announcing its robot at CES 2026, but whether or not it does, robotics will still be a big part of the show.Robot vacuums and mops were a major highlight of CES 2025, and it's safe to expect notable improvements from the new models that are announced at CES 2026. Not every company will adopt the retractable arm of the Roborock Saros Z70, but robot vacuums with legs for rising over small ledges like the Dreame X50 seem like they could become the norm. Roborock could also show off its new Roborock Qrevo Curv 2 Flow, the first of its robot vacuums to feature a retractable roller mop.Beyond just traversing spaces more efficiently, improving robots' navigation could also be a major concern at the show. Prominent members of the AI industry are turning their attention from large language models to world models, which aim to give AI a deep understanding of physical space. Those world models could be the key to making robots, bipedal or otherwise, competent at navigating homes and workplaces, and will likely be a significant talking point at CES 2026.We’ll be updating this article throughout the month as more rumors surface and new products are confirmed — stay tuned for future updates!Update, December 11 2025, 11:03AM ET: This story has been updated to include detail on Lenovo being Motorola’s parent company and how the latter might have a part in the Tuesday presentation.Update, December 16 2025, 1:33PM ET: This story has been updated to include the NVIDIA press conference, which was added to the CTA schedule within the last two days.This article originally appeared on Engadget at https://www.engadget.com/big-tech/ces-2026-preview-what-were-expecting-from-techs-biggest-conference-in-january-120000768.html?src=rss",
          "content": "CES doesn't start until January, but whispers of the products and announcements that could be in store for tech's biggest annual conference have already started to take shape. The CES 2026 show floor is officially open from January 6 through 9, although the show kicks off with events on Sunday January 4 and a host of press conferences on Monday. As always, product demos, announcements and networking will be happening at the Las Vegas Convention Center and surrounding hotels all over the city. As usual, Engadget will be covering the event in-person and remotely, bringing you news and hands-ons straight from the show floor.More specific details and pre-announcements should trickle out as CES approaches, but in the meantime, we do know what companies will be hosting press conferences and what tech trends could rear their heads at the show.What we already know aboutPress conferences and show floor booths are the bread and butter of CES. The Consumer Technology Association has already published a searchable directory of who will have a presence at the show, along with a schedule of every official panel and presentation.On Sunday, January 4, Samsung will kick-off CES with \"The First Look,\" a presentation hosted by TM Roh, the CEO of Samsung's DX Division, on the company's \"vision for the DX (Device eXperience) Division in 2026, along with new AI-driven customer experiences.\" That'll be followed by multiple press conferences throughout Monday, January 5. LG is hosting its \"Innovation in Tune with You\" presentation to share \"its vision for elevating daily life through Affectionate Intelligence\" at the start of the day, Intel is launching its new Core Ultra Series 3 processors in the afternoon, Sony Honda Mobility is holding a press conference on its first car and AMD CEO Lisa Su will cover AMD's upcoming chip announcements at a keynote address that closes out the day.On the week of December 15, the CTA added a keynote by NVIDIA CEO Jensen Huang to its schedule. The event will take place on January 5 at 1PM PT and, according to the website, will last about 90 minutes. Based on the description on the listing, the presentation will “showcase the latest NVIDIA solutions driving innovation and productivity across industries.”Finally, on Tuesday, January 6, Lenovo CEO Yuanqing Yang will host Lenovo's Tech World Conference at Sphere, using the large and decidedly curved screen to share the company's \"commitment to delivering smarter AI for all by constantly redefining how technology can engage, inspire, and empower.\" It’s worth noting that Lenovo is the parent company of Motorola, which still makes phones and foldables that feature AI tools, so it’s possible those devices feature in the presentation as well. Outside of the formal introduction of new products and initiatives, reading the tea leaves of what was announced last year and what companies are reportedly working on, we can make some educated guesses at what we could see at CES 2026.New chips from AMD, Intel and QualcommCES is frequently the start of a cascade of new chip announcements for a given year, and one of the first places new silicon appears in real consumer products. AMD will likely use its keynote to introduce new versions of its Ryzen chips, including the recently spotted Ryzen 7 9850X3D, which is expected to offer better single-threaded performance, and the Ryzen 9000G series, which could be built with AMD's Zen 5 architecture. The company might also use its CES stage to go over its new FSR Redstone AI upscaling tech.Intel has already publicly announced that it'll launch its Panther Lake chips at CES 2026. The officially titled Intel Core Ultra Series 3 chips fit into Intel's overall \"AI PC\" push, but are specifically meant for premium laptops. Based on a preview from October 2025, Intel says the first chip made with its 2-nanometer 18A process will offer 50 percent more processing performance than previous generations and for the chip's Arc GPU, a 50 percent performance bump from last generation.Qualcomm is also rumored to be targeting laptops at the show, building on the work it's done moving its Snapdragon chips out of phones and tablets and into other types of computers. The company's Snapdragon X2 Elite and X2 Elite Premium chips should start appearing in laptops at CES 2026, offering a look at the improved speed and AI performance the company promised in 2025.Brighter, \"truer\" screensSony announced a collection of new Bravia TVs in April 2025, replacing the company's flagship, filling in its midrange options and adding a new budget model to the mix. The star of this updated Bravia lineup is the Bravia 9, which features a QD-OLED panel, but Sony appears to be prepping entirely new display tech for 2026. In March 2025, Sony introduced a new RGB LED panel that uses individual Mini LED backlights colored in red, green and blue to produce even brighter, more accurate colors. In contrast to a QD-OLED, which filters a layer of blue organic light emitting diodes through quantum dots that change color, Sony's \"General RGB LED Backlight Technology\" can get as bright as a Mini LED panel without needing an extra filter layer or worrying about OLED's problems with burn-in. The company has already trademarked the name \"True RGB,\" which could end up being what Sony calls this new flavor of display if it decides to show them off at CES. It seems entirely likely, because CES is nothing if not a TV show — it’s a sure bet that we’ll see new TVs from the likes of LG and Samsung in addition to Sony. If the company doesn't introduce new display tech for its TVs, it does have a new 240Hz PlayStation monitor coming in 2026 that it could show off at CES instead.Sony isn't the only company hyped on bright screens. Samsung is reportedly pushing an updated version of the HDR10 and HDR10+ standards that could be ready to demo at CES 2026. The new HDR10+ Advanced standard would be Samsung's answer to Dolby Vision 2, which includes support for things bi-directional tone mapping and intelligent features that automatically adapt sports and gaming content. Samsung's take will reportedly offer improved brightness, genre-based tone mapping and intelligent motion smoothing options, among other improvements.Ballie Watch 2026The ball-shaped yellow robot lovingly known as \"Ballie\" has been announced twice, first in 2020 and then again in 2024 with a projector in tow. Samsung said Ballie would go on sale in 2025 at CES last year and then shared in April 2025 that Ballie would ship this summer with Google's Gemini onboard. But it's nearly 2026, and Ballie is nowhere to be seen. It's possible Samsung could make a third attempt at announcing its robot at CES 2026, but whether or not it does, robotics will still be a big part of the show.Robot vacuums and mops were a major highlight of CES 2025, and it's safe to expect notable improvements from the new models that are announced at CES 2026. Not every company will adopt the retractable arm of the Roborock Saros Z70, but robot vacuums with legs for rising over small ledges like the Dreame X50 seem like they could become the norm. Roborock could also show off its new Roborock Qrevo Curv 2 Flow, the first of its robot vacuums to feature a retractable roller mop.Beyond just traversing spaces more efficiently, improving robots' navigation could also be a major concern at the show. Prominent members of the AI industry are turning their attention from large language models to world models, which aim to give AI a deep understanding of physical space. Those world models could be the key to making robots, bipedal or otherwise, competent at navigating homes and workplaces, and will likely be a significant talking point at CES 2026.We’ll be updating this article throughout the month as more rumors surface and new products are confirmed — stay tuned for future updates!Update, December 11 2025, 11:03AM ET: This story has been updated to include detail on Lenovo being Motorola’s parent company and how the latter might have a part in the Tuesday presentation.Update, December 16 2025, 1:33PM ET: This story has been updated to include the NVIDIA press conference, which was added to the CTA schedule within the last two days.This article originally appeared on Engadget at https://www.engadget.com/big-tech/ces-2026-preview-what-were-expecting-from-techs-biggest-conference-in-january-120000768.html?src=rss",
          "feed_position": 39
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/home/what-happened-to-irobot-can-happen-to-anyone-164500625.html",
          "published_at": "Tue, 16 Dec 2025 16:45:00 +0000",
          "title": "What happened to iRobot can happen to anyone",
          "standfirst": "The company which popularized robot vacuum cleaners around the world has filed for Chapter 11 bankruptcy. iRobot, makers of the Roomba, has been synonymous with the category since its inception, but its star had dulled in recent years. The company plans to sell its assets to its primary supplier, China’s Picea Robotics, in the hope of maintaining its business.Everyone’s got a strident opinion as to why iRobot fell from grace. The rugged individualists blame limp regulators on both sides of the pond (and their hatred for big tech) for blocking Amazon’s attempted purchase in 2023. Those on the hardware side of the fence say iRobot’s refusal to embrace LiDAR for navigation until this year left it behind rivals.Then there’s the geopolitical experts, who can point at China’s industrial policy, subsidies and favorable regulatory environment compared to the US approach. After all, iRobot’s US gear is made in Vietnam, which is now subject to a 46 percent import levy. As BBC News reported, that added around $23 million to iRobot’s costs and increased the price of its hardware.The real answer is that iRobot’s demise was caused by a perfect storm of all these factors piling on to the company. More importantly, iRobot’s situation isn’t any way unique, and should serve as a warning to every major American technology brand. It’s also a lesson in why companies need to deal with existential threats when they have the time and cash to do so. For instance, once iRobot perfected the concept for the Roomba, it wasn’t long before the first copies burst onto the scene. iRobot had the brand and the know-how, but that only goes so far against well-motivated copycats. Think about the first Samsung Android handsets, and how quickly they went from iPhone imitations to class-defining devices of their own — and how hard Apple fought in court to prevent it.Even before this year’s tariffs, iRobot struggled to compete on price in a manner we’ve seen in other fields. Remember Fitbit before Google purchased it, happily selling $80 fitness trackers for years until Xiaomi swiped the low-end part of its business for itself. Even if the early MiBands weren’t very good, you could buy three for the price of a single Fitbit Charge. Yes, the argument around quality and reliability is important, but it’s often not as loud or compelling as a competing product sold for a fraction of the price.iRobot should have either made more of an effort to offer a dirt-cheap model to undercut its rivals, or more likely pull out of the low end altogether. Earlier today, I checked out local retail listings for Roombas and its nearest competitors. Next to one another were the Roomba 405 Combo with Dock and the Roborock Q7 L5+ — both capable of vacuuming and mopping your floor. The former is currently on sale for $400 direct from iRobot, while the latter is currently selling for $220. I’m sure plenty of buyers would have seen the price difference and opted for the cheaper model.I’m not going to throw too many Told You So’s over iRobot’s fence for not embracing LiDAR sooner. Its omission was a mistake, but you could see why it was shy about abandoning its existing setup. But the company had forgotten one key mantra about the tech world, Andy Grove’s maxim that “only the paranoid survive.” Even the fanciest, highest-end Roombas of the last five years felt a generation behind rival products.And, at the risk of sounding like a marketing guru, it was never clear what iRobot, or Roomba, stood for. When companies flooded the market with cheaper models, iRobot needed to make it clear what it meant when you bought a Roomba over a generic model. What did, and could, it offer beyond the name and history that made it stand out against cheaper competitors? Companies like Apple and Dyson command a premium, but you almost always know what you’re getting for your money.All I can say is that it’s good that there isn’t another American company presently in a similar position. I certainly can’t think of a controversial US company that builds things with wheels that has historically rejected LiDAR for its autonomous services. One that has a brand that doesn’t stand for much, or has its identity tied too closely to the identity of its CEO. One that is staring down the barrel at a raft of better equipped and often cheaper Chinese alternatives. Because that company could surely be looking at a similar fate a decade or so down the road.This article originally appeared on Engadget at https://www.engadget.com/home/what-happened-to-irobot-can-happen-to-anyone-164500625.html?src=rss",
          "content": "The company which popularized robot vacuum cleaners around the world has filed for Chapter 11 bankruptcy. iRobot, makers of the Roomba, has been synonymous with the category since its inception, but its star had dulled in recent years. The company plans to sell its assets to its primary supplier, China’s Picea Robotics, in the hope of maintaining its business.Everyone’s got a strident opinion as to why iRobot fell from grace. The rugged individualists blame limp regulators on both sides of the pond (and their hatred for big tech) for blocking Amazon’s attempted purchase in 2023. Those on the hardware side of the fence say iRobot’s refusal to embrace LiDAR for navigation until this year left it behind rivals.Then there’s the geopolitical experts, who can point at China’s industrial policy, subsidies and favorable regulatory environment compared to the US approach. After all, iRobot’s US gear is made in Vietnam, which is now subject to a 46 percent import levy. As BBC News reported, that added around $23 million to iRobot’s costs and increased the price of its hardware.The real answer is that iRobot’s demise was caused by a perfect storm of all these factors piling on to the company. More importantly, iRobot’s situation isn’t any way unique, and should serve as a warning to every major American technology brand. It’s also a lesson in why companies need to deal with existential threats when they have the time and cash to do so. For instance, once iRobot perfected the concept for the Roomba, it wasn’t long before the first copies burst onto the scene. iRobot had the brand and the know-how, but that only goes so far against well-motivated copycats. Think about the first Samsung Android handsets, and how quickly they went from iPhone imitations to class-defining devices of their own — and how hard Apple fought in court to prevent it.Even before this year’s tariffs, iRobot struggled to compete on price in a manner we’ve seen in other fields. Remember Fitbit before Google purchased it, happily selling $80 fitness trackers for years until Xiaomi swiped the low-end part of its business for itself. Even if the early MiBands weren’t very good, you could buy three for the price of a single Fitbit Charge. Yes, the argument around quality and reliability is important, but it’s often not as loud or compelling as a competing product sold for a fraction of the price.iRobot should have either made more of an effort to offer a dirt-cheap model to undercut its rivals, or more likely pull out of the low end altogether. Earlier today, I checked out local retail listings for Roombas and its nearest competitors. Next to one another were the Roomba 405 Combo with Dock and the Roborock Q7 L5+ — both capable of vacuuming and mopping your floor. The former is currently on sale for $400 direct from iRobot, while the latter is currently selling for $220. I’m sure plenty of buyers would have seen the price difference and opted for the cheaper model.I’m not going to throw too many Told You So’s over iRobot’s fence for not embracing LiDAR sooner. Its omission was a mistake, but you could see why it was shy about abandoning its existing setup. But the company had forgotten one key mantra about the tech world, Andy Grove’s maxim that “only the paranoid survive.” Even the fanciest, highest-end Roombas of the last five years felt a generation behind rival products.And, at the risk of sounding like a marketing guru, it was never clear what iRobot, or Roomba, stood for. When companies flooded the market with cheaper models, iRobot needed to make it clear what it meant when you bought a Roomba over a generic model. What did, and could, it offer beyond the name and history that made it stand out against cheaper competitors? Companies like Apple and Dyson command a premium, but you almost always know what you’re getting for your money.All I can say is that it’s good that there isn’t another American company presently in a similar position. I certainly can’t think of a controversial US company that builds things with wheels that has historically rejected LiDAR for its autonomous services. One that has a brand that doesn’t stand for much, or has its identity tied too closely to the identity of its CEO. One that is staring down the barrel at a raft of better equipped and often cheaper Chinese alternatives. Because that company could surely be looking at a similar fate a decade or so down the road.This article originally appeared on Engadget at https://www.engadget.com/home/what-happened-to-irobot-can-happen-to-anyone-164500625.html?src=rss",
          "feed_position": 42
        }
      ],
      "featured_image": "https://media-mbst-pub-ue1.s3.amazonaws.com/creatr-uploaded-images/2025-02/d51f6860-e4df-11ef-9fbd-65a89c2ab524",
      "popularity_score": 2018.5762916666667
    },
    {
      "id": "cluster_48",
      "coverage": 2,
      "updated_at": "2025-12-17T13:30:53-05:00",
      "title": "The Oscars will stream on YouTube in 2029",
      "neutral_headline": "The Oscars will stream on YouTube in 2029",
      "items": [
        {
          "source": "The Verge",
          "url": "https://www.theverge.com/news/846560/youtube-the-oscars-streaming-2029",
          "published_at": "2025-12-17T13:30:53-05:00",
          "title": "The Oscars will stream on YouTube in 2029",
          "standfirst": "The Oscars are headed to YouTube - in a few years. \"Beginning in 2029, the Oscars will broadcast exclusively on YouTube for free globally and on YouTube TV in the U.S.,\" YouTube announced. YouTube's coverage will include the red carpet, behind the scenes content, and Governors Ball access. The deal runs through 2033, according to [&#8230;]",
          "content": "The Oscars are headed to YouTube - in a few years. \"Beginning in 2029, the Oscars will broadcast exclusively on YouTube for free globally and on YouTube TV in the U.S.,\" YouTube announced. YouTube's coverage will include the red carpet, behind the scenes content, and Governors Ball access. The deal runs through 2033, according to an Academy of Motion Picture Arts and Sciences press release shared by The Hollywood Reporter. ABC will continue to broadcast the Oscars through the 100th edition of the show in 2028. Earlier this year, YouTube was rumored to be looking into acquiring the rights to the Oscars. \"This collaboration will leverage Y … Read the full story at The Verge.",
          "feed_position": 9
        },
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/251217/p38#a251217p38",
          "published_at": "Wed, 17 Dec 2025 12:54:20 -0500",
          "title": "The Academy signs a multi-year deal that will give YouTube the exclusive global rights to the Oscars, beginning in 2029 and running through 2033 (Scott Feinberg/The Hollywood Reporter)",
          "standfirst": "Scott Feinberg / The Hollywood Reporter: The Academy signs a multi-year deal that will give YouTube the exclusive global rights to the Oscars, beginning in 2029 and running through 2033 &mdash; The Academy announced on Wednesday: &mdash; LOS ANGELES, CA - The Academy of Motion Picture Arts and Sciences and YouTube signed a multi-year deal &hellip;",
          "content": "Scott Feinberg / The Hollywood Reporter: The Academy signs a multi-year deal that will give YouTube the exclusive global rights to the Oscars, beginning in 2029 and running through 2033 &mdash; The Academy announced on Wednesday: &mdash; LOS ANGELES, CA - The Academy of Motion Picture Arts and Sciences and YouTube signed a multi-year deal &hellip;",
          "feed_position": 8,
          "image_url": "http://www.techmeme.com/251217/i38.jpg"
        }
      ],
      "featured_image": "http://www.techmeme.com/251217/i38.jpg",
      "popularity_score": 2015.176013888889
    },
    {
      "id": "cluster_59",
      "coverage": 2,
      "updated_at": "Wed, 17 Dec 2025 11:15:02 -0500",
      "title": "Google makes Gemini 3 Flash the default model in the Gemini app and Search's AI mode; it scored 33.7% without tool use on Humanity's Last Exam vs. 3 Pro's 37.5% (Ivan Mehta/TechCrunch)",
      "neutral_headline": "Google launches Gemini 3 Flash, makes it the default model in the Gemini app",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/251217/p35#a251217p35",
          "published_at": "Wed, 17 Dec 2025 11:15:02 -0500",
          "title": "Google makes Gemini 3 Flash the default model in the Gemini app and Search's AI mode; it scored 33.7% without tool use on Humanity's Last Exam vs. 3 Pro's 37.5% (Ivan Mehta/TechCrunch)",
          "standfirst": "Ivan Mehta / TechCrunch: Google makes Gemini 3 Flash the default model in the Gemini app and Search's AI mode; it scored 33.7% without tool use on Humanity's Last Exam vs. 3 Pro's 37.5% &mdash; Google today released its fast and cheap Gemini 3 Flash model, based on the Gemini 3 released last month, looking to steal OpenAI's thunder.",
          "content": "Ivan Mehta / TechCrunch: Google makes Gemini 3 Flash the default model in the Gemini app and Search's AI mode; it scored 33.7% without tool use on Humanity's Last Exam vs. 3 Pro's 37.5% &mdash; Google today released its fast and cheap Gemini 3 Flash model, based on the Gemini 3 released last month, looking to steal OpenAI's thunder.",
          "feed_position": 11,
          "image_url": "http://www.techmeme.com/251217/i35.jpg"
        },
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2025/12/17/google-launches-gemini-3-flash-makes-it-the-default-model-in-the-gemini-app/",
          "published_at": "Wed, 17 Dec 2025 16:00:00 +0000",
          "title": "Google launches Gemini 3 Flash, makes it the default model in the Gemini app",
          "standfirst": "Google is making Gemini 3 Flash the default model in the Gemini app and oAI model for Search",
          "content": "Google is making Gemini 3 Flash the default model in the Gemini app and oAI model for Search",
          "feed_position": 9
        }
      ],
      "featured_image": "http://www.techmeme.com/251217/i35.jpg",
      "popularity_score": 2012.9118469444445
    },
    {
      "id": "cluster_73",
      "coverage": 2,
      "updated_at": "Wed, 17 Dec 2025 10:30:00 -0500",
      "title": "Google brings its vibe coding tool Opal to Gemini on the web, letting users create their own custom AI-powered mini apps, called Gems, which can be reused later (Sarah Perez/TechCrunch)",
      "neutral_headline": "Google&#8217;s vibe-coding tool Opal comes to Gemini",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/251217/p32#a251217p32",
          "published_at": "Wed, 17 Dec 2025 10:30:00 -0500",
          "title": "Google brings its vibe coding tool Opal to Gemini on the web, letting users create their own custom AI-powered mini apps, called Gems, which can be reused later (Sarah Perez/TechCrunch)",
          "standfirst": "Sarah Perez / TechCrunch: Google brings its vibe coding tool Opal to Gemini on the web, letting users create their own custom AI-powered mini apps, called Gems, which can be reused later &mdash; Google's vibe-coding tool, Opal, is making its way to Gemini. The company on Wednesday said it is integrating the tool &hellip;",
          "content": "Sarah Perez / TechCrunch: Google brings its vibe coding tool Opal to Gemini on the web, letting users create their own custom AI-powered mini apps, called Gems, which can be reused later &mdash; Google's vibe-coding tool, Opal, is making its way to Gemini. The company on Wednesday said it is integrating the tool &hellip;",
          "feed_position": 14,
          "image_url": "http://www.techmeme.com/251217/i32.jpg"
        },
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2025/12/17/googles-vibe-coding-tool-opal-comes-to-gemini/",
          "published_at": "Wed, 17 Dec 2025 15:16:42 +0000",
          "title": "Google&#8217;s vibe-coding tool Opal comes to Gemini",
          "standfirst": "The company on Wednesday said it is integrating the tool, which lets you build AI-powered mini apps, inside the Gemini web app, allowing users to create their own custom apps.",
          "content": "The company on Wednesday said it is integrating the tool, which lets you build AI-powered mini apps, inside the Gemini web app, allowing users to create their own custom apps.",
          "feed_position": 14
        }
      ],
      "featured_image": "http://www.techmeme.com/251217/i32.jpg",
      "popularity_score": 2012.161291388889
    },
    {
      "id": "cluster_25",
      "coverage": 1,
      "updated_at": "Wed, 17 Dec 2025 20:56:32 +0000",
      "title": "Man sues cops who jailed him for 37 days for trolling a Charlie Kirk vigil",
      "neutral_headline": "Man sues cops who jailed him for 37 days for trolling a Charlie Kirk vigil",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/12/man-sues-cops-who-jailed-him-for-37-days-for-trolling-a-charlie-kirk-vigil/",
          "published_at": "Wed, 17 Dec 2025 20:56:32 +0000",
          "title": "Man sues cops who jailed him for 37 days for trolling a Charlie Kirk vigil",
          "standfirst": "Cops may be fined for jailing a man over his Facebook posts.",
          "content": "Larry Bushart, a man who was jailed for 37 days for reposting a Trump meme, has now sued the cops who allegedly schemed to keep him imprisoned for as long as possible simply because they disagreed with his point of view. Bushart is a former cop who lost his post-retirement job after a seemingly vengeful sheriff jailed him for trolling a Charlie Kirk vigil post in a Facebook group. Upset that Kirk’s death commanded more attention than other victims of gun violence, Bushart posted a string of memes, among which was an image of Trump with an actual quote saying “We have to get over it” about a 2024 school shooting. Perry County sheriff Nick Weems has since acknowledged that he “knew” that the meme referenced a prior school shooting. However, the entire time that Bushart was detained, Weems maintained that Bushart’s post incited “mass hysteria” from parents concerned that he was threatening violence at a local high school. Painting Bushart as indifferent to the supposed hysteria, Weems justified his arrest, as well as the $2 million bond ensuring Bushart couldn’t afford bail and remained behind bars.Read full article Comments",
          "feed_position": 0,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/Larry-Bushart-via-FIRE.jpg.jpeg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/Larry-Bushart-via-FIRE.jpg.jpeg",
      "popularity_score": 365.6035136111111
    },
    {
      "id": "cluster_8",
      "coverage": 1,
      "updated_at": "Wed, 17 Dec 2025 22:22:33 +0000",
      "title": "OpenAI’s new ChatGPT image generator makes faking photos easy",
      "neutral_headline": "OpenAI’s new ChatGPT image generator makes faking photos easy",
      "items": [
        {
          "source": "Ars Technica Main",
          "url": "https://arstechnica.com/ai/2025/12/openais-new-chatgpt-image-generator-makes-faking-photos-easy/",
          "published_at": "Wed, 17 Dec 2025 22:22:33 +0000",
          "title": "OpenAI’s new ChatGPT image generator makes faking photos easy",
          "standfirst": "New GPT Image 1.5 allows more detailed conversational image editing, for better or worse.",
          "content": "For most of photography’s roughly 200-year history, altering a photo convincingly required either a darkroom, some Photoshop expertise, or, at minimum, a steady hand with scissors and glue. On Tuesday, OpenAI released a tool that reduces the process to typing a sentence. It’s not the first company to do so. While OpenAI had a conversational image-editing model in the works since GPT-4o in 2024, Google beat OpenAI to market in March with a public prototype, then refined it to a popular model called Nano Banana image model (and Nano Banana Pro). The enthusiastic response to Google’s image-editing model in the AI community got OpenAI’s attention. OpenAI’s new GPT Image 1.5 is an AI image synthesis model that reportedly generates images up to four times faster than its predecessor and costs about 20 percent less through the API. The model rolled out to all ChatGPT users on Tuesday and represents another step toward making photorealistic image manipulation a casual process that requires no particular visual skills.Read full article Comments",
          "feed_position": 1,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/barbarian_crt_1-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/barbarian_crt_1-1152x648.jpg",
      "popularity_score": 357.03712472222225
    },
    {
      "id": "cluster_26",
      "coverage": 1,
      "updated_at": "Wed, 17 Dec 2025 20:46:39 +0000",
      "title": "FCC chair scrubs website after learning it called FCC an “independent agency”",
      "neutral_headline": "FCC chair scrubs website after learning it called FCC an “independent agency”",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/12/fcc-deletes-independent-agency-from-website-as-carr-defends-allegiance-to-trump/",
          "published_at": "Wed, 17 Dec 2025 20:46:39 +0000",
          "title": "FCC chair scrubs website after learning it called FCC an “independent agency”",
          "standfirst": "Meanwhile, Ted Cruz wants to restrict FCC's power to intimidate broadcasters.",
          "content": "Federal Communications Commission Chairman Brendan Carr today faced blistering criticism in a Senate hearing for his September threats to revoke ABC station licenses over comments made by Jimmy Kimmel. While Democrats provided nearly all the criticism, Sen. Ted Cruz (R-Texas) said that Congress should act to restrict the FCC’s power to intimidate news broadcasters. As an immediate result of today’s hearing, the FCC removed a statement from its website that said it is an independent agency. Carr, who has embraced President Trump’s declaration that independent agencies may no longer operate independently from the White House, apparently didn’t realize that the website still called the FCC an independent agency. “Yes or no, is the FCC an independent agency?” Sen. Ben Ray Luján (D-N.M.) asked. Carr answered that the FCC is not independent, prompting Luján to point to a statement on the FCC website calling the FCC “an independent US government agency overseen by Congress.”Read full article Comments",
          "feed_position": 1,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/brendan-carr-hearing-1152x648-1766003281.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/brendan-carr-hearing-1152x648-1766003281.jpg",
      "popularity_score": 355.4387913888889
    },
    {
      "id": "cluster_3",
      "coverage": 1,
      "updated_at": "Wed, 17 Dec 2025 22:46:21 +0000",
      "title": "Physicists 3D-printed a Christmas tree of ice",
      "neutral_headline": "Physicists 3D-printed a Christmas tree of ice",
      "items": [
        {
          "source": "Ars Technica Main",
          "url": "https://arstechnica.com/science/2025/12/physicists-3d-printed-a-christmas-tree-of-ice/",
          "published_at": "Wed, 17 Dec 2025 22:46:21 +0000",
          "title": "Physicists 3D-printed a Christmas tree of ice",
          "standfirst": "New method uses no freezing technology or refrigeration equipment—just water and a vacuum.",
          "content": "Physicists at the University of Amsterdam came up with a really cool bit of Christmas decor: a miniature 3D-printed Christmas tree, a mere 8 centimeters tall, made of ice, without any refrigeration equipment or other freezing technology, and at minimal cost. The secret is evaporative cooling, according to a preprint posted to the physics arXiv. Evaporative cooling is a well-known phenomenon; mammals use it to regulate body temperature. You can see it in your morning cup of hot coffee: the hotter atoms rise to the top of the magnetic trap and “jump out” as steam. It also plays a role (along with shock wave dynamics and various other factors) in the formation of “wine tears.” It’s a key step in creating Bose-Einstein condensates. And evaporative cooling is also the main culprit behind the infamous “stall” that so frequently plagues aspiring BBQ pit masters eager to make a successful pork butt. The meat sweats as it cooks, releasing the moisture within, and that moisture evaporates and cools the meat, effectively canceling out the heat from the BBQ. That’s why a growing number of competitive pit masters wrap their meat in tinfoil after the first few hours (usually when the internal temperature hits 170° F).Read full article Comments",
          "feed_position": 0,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/icytree1-1152x648-1765997543.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/icytree1-1152x648-1765997543.jpg",
      "popularity_score": 349.4337913888889
    },
    {
      "id": "cluster_37",
      "coverage": 1,
      "updated_at": "Wed, 17 Dec 2025 19:18:29 +0000",
      "title": "Donut Lab’s hub motor meets WATT’s battery to create new EV skateboard",
      "neutral_headline": "Donut Lab’s hub motor meets WATT’s battery to create new EV skateboard",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/cars/2025/12/donut-labs-hub-motor-meets-watts-battery-to-create-new-ev-skateboard/",
          "published_at": "Wed, 17 Dec 2025 19:18:29 +0000",
          "title": "Donut Lab’s hub motor meets WATT’s battery to create new EV skateboard",
          "standfirst": "A prototype of the EV platform will be shown off at CES next month.",
          "content": "One of the big stories of last year’s Consumer Electronics Show in Las Vegas was the debut of Donut Lab’s diminutive but powerful electric motors. When I spoke with Donut earlier this year, the company told me that it was looking at applications ranging from drones to large automotive motors, but also things like wind turbines and even washing machines. Now, almost a year later, we have our first look at an electric vehicle that uses the technology, thanks to a new collaboration between Donut Lab and WATT Electric Vehicle Company. WATT had previously developed the Passenger and Commercial EV Skateboard (PACES), a lightweight aluminum platform for low-volume EVs. Now, it’s integrating Donut’s motors, first with one for each rear wheel, although there will eventually be an all-wheel-drive variant, too. The small EV chassis is rather intriguing—with the motors in the rear wheel hubs, the layout is even more space-efficient than a more conventional EV, which still needs to find a few cubic feet to package its drive unit(s). The two companies see plenty of potential for the platform, which they say could give rise to “multiple vehicle configurations from beach buggies to high-performance sports cars to commercial delivery vehicles.”Read full article Comments",
          "feed_position": 2,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/Heroshot-Angle-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/Heroshot-Angle-1152x648.jpg",
      "popularity_score": 334.96934694444445
    },
    {
      "id": "cluster_46",
      "coverage": 1,
      "updated_at": "Wed, 17 Dec 2025 18:41:25 +0000",
      "title": "Bursting AI bubble may be EU’s “secret weapon” in clash with Trump, expert says",
      "neutral_headline": "Bursting AI bubble may be EU’s “secret weapon” in clash with Trump, expert says",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/12/us-threatens-crackdown-on-eu-firms-as-clash-over-tech-regulations-intensifies/",
          "published_at": "Wed, 17 Dec 2025 18:41:25 +0000",
          "title": "Bursting AI bubble may be EU’s “secret weapon” in clash with Trump, expert says",
          "standfirst": "Spotify and Accenture caught in crossfire as Trump attacks EU tech regulations.",
          "content": "The US threatened to restrict some of the largest service providers in the European Union as retaliation for EU tech regulations and investigations are increasingly drawing Donald Trump’s ire. On Tuesday, the Office of the US Trade Representative (USTR) issued a warning on X, naming Spotify, Accenture, Amadeus, Mistral, Publicis, and DHL among nine firms suddenly yanked into the middle of the US-EU tech fight. “The European Union and certain EU Member States have persisted in a continuing course of discriminatory and harassing lawsuits, taxes, fines, and directives against US service providers,” USTR’s post said.Read full article Comments",
          "feed_position": 3,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2233373547-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2233373547-1152x648.jpg",
      "popularity_score": 333.35156916666665
    },
    {
      "id": "cluster_60",
      "coverage": 1,
      "updated_at": "Wed, 17 Dec 2025 16:10:43 +0000",
      "title": "Trump admin threatens to break up major climate research center",
      "neutral_headline": "Trump admin threatens to break up major climate research center",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2025/12/trump-admin-threatens-to-break-up-major-climate-research-center/",
          "published_at": "Wed, 17 Dec 2025 16:10:43 +0000",
          "title": "Trump admin threatens to break up major climate research center",
          "standfirst": "Major research institution dismissed as a source of \"climate alarmism.\"",
          "content": "On Tuesday, the head of the Office of Management and Budget, Russell Vought, announced that a major climate research center will be “broken up.” The National Center for Atmospheric Research, or NCAR, is a significant contributor to research on the weather, climate, and other atmospheric phenomena. The move will be a crippling blow to climate research in the US and is being widely decried by scientists. Vought initially gave a statement regarding NCAR to USA Today and later confirmed the outlet’s reporting on social media. Calling it “one of the largest sources of climate alarmism in the country,” Vought also decried what he termed “woke” activities at NCAR. These appear to be fairly typical efforts made to attract underrepresented groups to the sciences—efforts that were uncontroversial prior to the current administration. NCAR is primarily based in a complex on the outskirts of Boulder, Colorado, and maintains a supercomputing center in Wyoming. Much of its funding comes from the National Science Foundation, but the day-to-day management is handled by the University Corporation for Atmospheric Research (UCAR), a nonprofit that represents 130 individual educational institutions. In addition to climate science, researchers based there study Earth and space weather, atmospheric chemistry, and their impacts on the environment and humans. NCAR hosts a series of webpages that explain its research and all the ways it helps society.Read full article Comments",
          "feed_position": 4,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-1086618384-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-1086618384-1152x648.jpg",
      "popularity_score": 313.8399025
    },
    {
      "id": "cluster_63",
      "coverage": 1,
      "updated_at": "Wed, 17 Dec 2025 16:00:25 +0000",
      "title": "Google releases Gemini 3 Flash, promising improved intelligence and efficiency",
      "neutral_headline": "Google releases Gemini 3 Flash, promising improved intelligence and efficiency",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/google/2025/12/google-releases-gemini-3-flash-promising-improved-intelligence-and-efficiency/",
          "published_at": "Wed, 17 Dec 2025 16:00:25 +0000",
          "title": "Google releases Gemini 3 Flash, promising improved intelligence and efficiency",
          "standfirst": "Google's Gemini 3 family is now complete with release of Gemini 3 Flash.",
          "content": "Google began its transition to Gemini 3 a few weeks ago with the launch of the Pro model, and the arrival of Gemini 3 Flash kicks it into high gear. The new, faster Gemini 3 model is coming to the Gemini app and search, and developers will be able to access it immediately via the Gemini API, Vertex AI, AI Studio, and Antigravity. Google’s bigger gen AI model is also picking up steam, with both Gemini 3 Pro and its image component (Nano Banana Pro) expanding in search. This may come as a shock, but Google says Gemini 3 Flash is faster and more capable than its previous base model. As usual, Google has a raft of benchmark numbers that show modest improvements for the new model. It bests the old 2.5 Flash in basic academic and reasoning tests like GPQA Diamond and MMMU Pro (where it even beats 3 Pro). It gets a larger boost in Humanity’s Last Exam (HLE), which tests advanced domain-specific knowledge. Gemini 3 Flash has tripled the old models’ score in HLE, landing at 33.7 percent without tool use. That’s just a few points behind the Gemini 3 Pro model. Credit: Google Read full article Comments",
          "feed_position": 5,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/gemini-3_flash_model_blog_header_dark_bleed_2096x1182-1152x648.png"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/gemini-3_flash_model_blog_header_dark_bleed_2096x1182-1152x648.png",
      "popularity_score": 307.6682358333333
    },
    {
      "id": "cluster_74",
      "coverage": 1,
      "updated_at": "Wed, 17 Dec 2025 15:25:25 +0000",
      "title": "Browser extensions with 8 million users collect extended AI conversations",
      "neutral_headline": "Browser extensions with 8 million users collect extended AI conversations",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/security/2025/12/browser-extensions-with-8-million-users-collect-extended-ai-conversations/",
          "published_at": "Wed, 17 Dec 2025 15:25:25 +0000",
          "title": "Browser extensions with 8 million users collect extended AI conversations",
          "standfirst": "The extensions, available for Chromium browsers, harvest full AI conversations over months.",
          "content": "Browser extensions with more than 8 million installs are harvesting complete and extended conversations from users’ AI conversations and selling them for marketing purposes, according to data collected from the Google and Microsoft pages hosting them. Security firm Koi discovered the eight extensions, which as of late Tuesday night remained available in both Google’s and Microsoft’s extension stores. Seven of them carry “Featured” badges, which are endorsements meant to signal that the companies have determined the extensions meet their quality standards. The free extensions provide functions such as VPN routing to safeguard online privacy and ad blocking for ad-free browsing. All provide assurances that user data remains anonymous and isn’t shared for purposes other than their described use. A gold mine for marketers and data brokers An examination of the extensions’ underlying code tells a much more complicated story. Each contains eight of what Koi calls “executor” scripts, with each being unique for ChatGPT, Claude, Gemini, and five other leading AI chat platforms. The scripts are injected into webpages anytime the user visits one of these platforms. From there, the scripts override browsers’ built-in functions for making network requests and receiving responses.Read full article Comments",
          "feed_position": 6,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/electronic-privacy-invasion-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/electronic-privacy-invasion-1152x648.jpg",
      "popularity_score": 300.0849025
    },
    {
      "id": "cluster_77",
      "coverage": 1,
      "updated_at": "Wed, 17 Dec 2025 15:09:03 +0000",
      "title": "Electric vehicles cause tension in the automotive aftermarket",
      "neutral_headline": "Electric vehicles cause tension in the automotive aftermarket",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/cars/2025/12/how-is-ev-technology-being-accepted-by-the-aftermarket/",
          "published_at": "Wed, 17 Dec 2025 15:09:03 +0000",
          "title": "Electric vehicles cause tension in the automotive aftermarket",
          "standfirst": "At SEMA, we heard Trump-like talking points but saw skateboard chassis, too.",
          "content": "After federal clean vehicle tax credits ended in September, the electric vehicle industry reached a crossroads. Well, technically, it has been there since Trump took office. This is a weird period in automotive history; A chunk of the industry is full-steam ahead with EV development, another is cutting back, and the consumer is left wondering what the electrification landscape will look like next year, let alone in three, during the next administration. But what about the automotive aftermarket? Typically, this corner benefits from whatever progress is made on the OEM front—have Trump’s policies expanded or contracted its EV technological development? I recently spent some time chatting with personnel of the Specialty Equipment Market Association (SEMA) at its yearly tradeshow in Las Vegas to find out. I also hit the bricks (or, rather, bright carpeting) of the massive show itself, seeking out some new, unique developments in the space that behoove EV tech’s inherent benefits. Above one of the show’s several sprawling halls, I met with Mike Spagnola, SEMA’s CEO, and Karen Bailey-Chapman, senior vice president, public and government affairs, to learn what the organization’s official stance is. First and foremost: It doesn’t want to be told what to do.Read full article Comments",
          "feed_position": 7,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/SEMAPhotos20251-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/SEMAPhotos20251-1152x648.jpg",
      "popularity_score": 280.8121247222222
    },
    {
      "id": "cluster_80",
      "coverage": 1,
      "updated_at": "Wed, 17 Dec 2025 14:41:36 +0000",
      "title": "Twitter comeback bid draws lawsuit from Elon Musk’s X Corp",
      "neutral_headline": "Twitter comeback bid draws lawsuit from Elon Musk’s X Corp",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/12/twitter-comeback-bid-draws-lawsuit-from-elon-musks-x-corp/",
          "published_at": "Wed, 17 Dec 2025 14:41:36 +0000",
          "title": "Twitter comeback bid draws lawsuit from Elon Musk’s X Corp",
          "standfirst": "“Huge uphill battle” to obtain and use Twitter name.",
          "content": "On Tuesday, X Corporation, formerly known as Twitter, sued “Operation Bluebird,” the new startup that is seeking to reclaim the allegedly abandoned Twitter trademark and relaunch a new social media network under that name. In its 43-page lawsuit, which was filed in federal court in Delaware, X Corporation alleges trademark infringement, adding that despite Bluebird’s “purported plan, it cannot bring Twitter ‘back’—Twitter never left and continues to be exclusively owned by X Corp.” One of Bluebird’s leaders, Michael Peroff, told Ars in an email that Operation Bluebird was “fully expecting” a lawsuit from X Corporation and that “we planned for it.”Read full article Comments",
          "feed_position": 8,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2023/03/getty-twitter-broken-phone-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2023/03/getty-twitter-broken-phone-1152x648.jpg",
      "popularity_score": 264.35462472222224
    },
    {
      "id": "cluster_92",
      "coverage": 1,
      "updated_at": "Wed, 17 Dec 2025 12:30:15 +0000",
      "title": "US oil industry doesn’t see profit in Trump’s “pro-petroleum” moves",
      "neutral_headline": "US oil industry doesn’t see profit in Trump’s “pro-petroleum” moves",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2025/12/us-oil-industry-doesnt-see-profit-in-trumps-pro-petroleum-moves/",
          "published_at": "Wed, 17 Dec 2025 12:30:15 +0000",
          "title": "US oil industry doesn’t see profit in Trump’s “pro-petroleum” moves",
          "standfirst": "The financial picture around drilling is moving against the Trump administration’s hopes",
          "content": "As the Trump administration makes announcement after announcement about its efforts to promote the US fossil fuel industry, the industry isn’t exactly jumping at new opportunities. Some high-profile oil and gas industry leaders and organizations have objected to changes to long-standing government policy positions that give companies firm ground on which to make their plans. And the financial picture around oil and gas drilling is moving against the Trump administration’s hopes. Though politicians may tout new opportunities to drill offshore or in Arctic Alaska, the commercial payoff is not clear and even unlikely.Read full article Comments",
          "feed_position": 9,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2245935569-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2245935569-1152x648.jpg",
      "popularity_score": 252.16545805555555
    },
    {
      "id": "cluster_96",
      "coverage": 1,
      "updated_at": "Wed, 17 Dec 2025 12:00:13 +0000",
      "title": "“A Band-Aid on a giant gash”: Trump’s attacks on science may ruin his AI moonshot",
      "neutral_headline": "“A Band-Aid on a giant gash”: Trump’s attacks on science may ruin his AI moonshot",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/12/trump-spent-2025-attacking-science-that-could-set-back-his-genesis-mission/",
          "published_at": "Wed, 17 Dec 2025 12:00:13 +0000",
          "title": "“A Band-Aid on a giant gash”: Trump’s attacks on science may ruin his AI moonshot",
          "standfirst": "Trump’s AI “Manhattan Project” will fail if DOGE cuts are kept, critics say.",
          "content": "By executive order last month, Donald Trump launched his so-called “Genesis Mission.” Described as a “historic national effort” to “invest in AI-enabled science to accelerate scientific advancement,” Trump claimed his mission would address key challenges to American energy dominance, innovation, and national security. This mission, Trump boasted, would be a game-changer to science akin to putting a man on the moon or firing the first nuclear weapons. By building “an integrated AI platform” trained on “the world’s largest collection” of federal scientific data sets, he promised, the government could set off cascades of scientific breakthroughs.Read full article Comments",
          "feed_position": 10,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/trump-project-genesis-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/trump-project-genesis-1152x648.jpg",
      "popularity_score": 156.6649025
    },
    {
      "id": "cluster_127",
      "coverage": 1,
      "updated_at": "Tue, 16 Dec 2025 18:52:43 +0000",
      "title": "Software leaks point to the first Apple Silicon “iMac Pro,” among other devices",
      "neutral_headline": "Software leaks point to the first Apple Silicon “iMac Pro,” among other devices",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2025/12/leaked-debug-kit-suggests-apple-is-testing-a-new-imac-pro-among-many-other-macs/",
          "published_at": "Tue, 16 Dec 2025 18:52:43 +0000",
          "title": "Software leaks point to the first Apple Silicon “iMac Pro,” among other devices",
          "standfirst": "Resurrected high-end all-in-one could be a worthy successor to 2017's iMac Pro.",
          "content": "Apple doesn’t like to talk about its upcoming products before it’s ready, but sometimes the company’s software does the talking for it. So far this week we’ve had a couple of software-related leaks that have outed products Apple is currently testing—one a pre-release build of iOS 26, and the other some leaked files from a kernel debug kit (both via MacRumors). Most of the new devices referenced in these leaks are straightforward updates to products that already exist: a new Apple TV, a HomePod mini 2, new AirTags and AirPods, an M4 iPad Air, a 12th-generation iPad to replace the current A16 version, next-generation iPhones (including the 17e, 18, and the rumored foldable model), a new Studio Display model, some new smart home products we’ve already heard about elsewhere, and M5 updates for the MacBook Air, Mac mini, Mac Studio, and the other MacBook Pros. There’s also yet another reference to the lower-cost MacBook that Apple is apparently planning to replace the M1 MacBook Air it still sells via Walmart for $599. For power users, though, the most interesting revelation might be that Apple is working on a higher-end Apple Silicon iMac powered by an M5 Max chip. The kernel debug kit references an iMac with the internal identifier J833c, based on a platform identified as H17C—and H17C is apparently based on the M5 Max, rather than a lower-end M5 chip. (For those who don’t have Apple’s branding memorized, “Max” is associated with Apple’s second-fastest chips; the M5 Max would be faster than the M5 or M5 Pro, but slower than the rumored M5 Ultra.)Read full article Comments",
          "feed_position": 14,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/iMac-Pro-front-scaled-1-1152x648.jpeg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/iMac-Pro-front-scaled-1-1152x648.jpeg",
      "popularity_score": 151
    },
    {
      "id": "cluster_137",
      "coverage": 1,
      "updated_at": "Tue, 16 Dec 2025 15:09:45 +0000",
      "title": "Ars Live Today: 3 former CDC leaders detail impacts of RFK Jr.’s anti-science agenda",
      "neutral_headline": "Ars Live Today: 3 former CDC leaders detail impacts of RFK Jr.’s anti-science agenda",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/health/2025/12/ars-live-3-former-cdc-leaders-detail-impacts-of-rfk-jr-s-anti-science-agenda/",
          "published_at": "Tue, 16 Dec 2025 15:09:45 +0000",
          "title": "Ars Live Today: 3 former CDC leaders detail impacts of RFK Jr.’s anti-science agenda",
          "standfirst": "Join us today, December 16, at 2 pm ET to hear from leaders who resigned in protest.",
          "content": "The Centers for Disease Control and Prevention is in critical condition. This year, the premier public health agency had its funding brutally cut and staff gutted, its mission sabotaged, and its headquarters riddled with literal bullets. The over 500 rounds fired were meant for its scientists and public health experts, who endured only to be sidelined, ignored, and overruled by Health Secretary Robert F. Kennedy Jr., an anti-vaccine activist hellbent on warping the agency to fit his anti-science agenda. Then, on August 27, Kennedy fired CDC Director Susan Monarez just weeks after she was confirmed by the Senate. She had refused to blindly approve vaccine recommendations from a panel of vaccine skeptics and contrarians that he had hand-selected. The agency descended into chaos, and Monarez wasn’t the only one to leave the agency that day. Three top leaders had reached their breaking point and coordinated their resignations upon the dramatic ouster: Drs. Demetre Daskalakis, Debra Houry, and Daniel Jernigan walked out of the agency as their colleagues rallied around them.Read full article Comments",
          "feed_position": 18,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2231820155-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2231820155-1152x648.jpg",
      "popularity_score": 148
    },
    {
      "id": "cluster_117",
      "coverage": 1,
      "updated_at": "Tue, 16 Dec 2025 21:25:05 +0000",
      "title": "The $4.3 billion space telescope Trump tried to cancel is now complete",
      "neutral_headline": "The $4.3 billion space telescope Trump tried to cancel is now complete",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2025/12/the-4-3-billion-space-telescope-trump-tried-to-cancel-is-now-complete/",
          "published_at": "Tue, 16 Dec 2025 21:25:05 +0000",
          "title": "The $4.3 billion space telescope Trump tried to cancel is now complete",
          "standfirst": "\"We're going to be making 3D movies of what is going on in the Milky Way galaxy.\"",
          "content": "A few weeks ago, technicians inside a cavernous clean room in Maryland made the final connection to complete assembly of NASA’s Nancy Grace Roman Space Telescope. Parts of this new observatory, named for NASA’s first chief astronomer, recently completed a spate of tests to ensure it can survive the shaking and intense sound of a rocket launch. Engineers placed the core of the telescope inside a thermal vacuum chamber, where it withstood the airless conditions and extreme temperature swings it will see in space. Then, on November 25, teams at NASA’s Goddard Space Flight Center in Greenbelt, Maryland, joined the inner and outer portions of the Roman Space Telescope. With this milestone, NASA declared the observatory complete and on track for launch as soon as fall 2026.Read full article Comments",
          "feed_position": 12,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/Trailer_still_1-1-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/Trailer_still_1-1-1152x648.jpg",
      "popularity_score": 141
    },
    {
      "id": "cluster_132",
      "coverage": 1,
      "updated_at": "Tue, 16 Dec 2025 17:00:57 +0000",
      "title": "Utah leaders hinder efforts to develop solar energy supply",
      "neutral_headline": "Utah leaders hinder efforts to develop solar energy supply",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2025/12/utah-leaders-hinder-efforts-to-develop-solar-energy-supply/",
          "published_at": "Tue, 16 Dec 2025 17:00:57 +0000",
          "title": "Utah leaders hinder efforts to develop solar energy supply",
          "standfirst": "Solar power accounts for two-thirds of the new projects waiting to connect to the state’s power grid.",
          "content": "Utah Gov. Spencer Cox believes his state needs more power—a lot more. By some estimates, Utah will require as much electricity in the next five years as it generated all last century to meet the demands of a growing population as well as chase data centers and AI developers to fuel its economy. To that end, Cox announced Operation Gigawatt last year, declaring the state would double energy production in the next decade. Although the announcement was short on details, Cox, a Republican, promised his administration would take an “any of the above” approach, which aims to expand all sources of energy production. Despite that goal, the Utah Legislature’s Republican supermajority, with Cox’s acquiescence, has taken a hard turn against solar power—which has been coming online faster than any other source in Utah and accounts for two-thirds of the new projects waiting to connect to the state’s power grid.Read full article Comments",
          "feed_position": 16,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2241255023-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2241255023-1152x648.jpg",
      "popularity_score": 139
    },
    {
      "id": "cluster_136",
      "coverage": 1,
      "updated_at": "Tue, 16 Dec 2025 15:38:57 +0000",
      "title": "2026 Mercedes CLA first drive: Entry level doesn’t mean basic",
      "neutral_headline": "2026 Mercedes CLA first drive: Entry level doesn’t mean basic",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/cars/2025/12/2026-mercedes-cla-first-drive-entry-level-doesnt-mean-basic/",
          "published_at": "Tue, 16 Dec 2025 15:38:57 +0000",
          "title": "2026 Mercedes CLA first drive: Entry level doesn’t mean basic",
          "standfirst": "Starting at $47,250, Mercedes' new electric sedan is quite compelling.",
          "content": "Mercedes-Benz provided flights from Washington to San Francisco and accommodation so Ars could drive the CLA. Ars does not accept paid editorial content. SAN FRANCISCO—Automakers are starting to follow somewhat familiar paths as they continue their journeys to electrification. Electric vehicles are, at first, strange new tech, and usually look like it. Mercedes-Benz’s EQS and EQE are good examples—with bodies that look like bars of soap worn down in the shower, they stood out. For early adopters and trailblazers that might be fine, but you need to sell cars to normal people if you want to survive, and that means making EVs more normal. Which is what Mercedes did with its newest one, the all-electric CLA. The normal looks belie the amount of new technology that Mercedes has packed into the CLA, though. The car sticks to the four-door coupe look that the company pioneered a couple of decades ago, but there’s a thoroughly modern electric powertrain connected to the wheels, run by four powerful networked computers. And yes, there’s AI. (For the pedants, “coupe” means cut down, not two-door, so the name is accurate.) The CLA is the first of a new series of Mercedes that will use the same modular architecture, and interestingly, it’s powertrain agnostic—a hybrid CLA is coming in time, too. But first the battery EV, which makes good use of some technology Mercedes developed for the EQXX concept car.Read full article Comments",
          "feed_position": 17,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/2026-Mercedes-Benz-CLA-4-1152x648-1765897346.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/2026-Mercedes-Benz-CLA-4-1152x648-1765897346.jpg",
      "popularity_score": 139
    },
    {
      "id": "cluster_116",
      "coverage": 1,
      "updated_at": "Tue, 16 Dec 2025 21:57:55 +0000",
      "title": "Texas sues biggest TV makers, alleging smart TVs spy on users without consent",
      "neutral_headline": "Texas sues biggest TV makers, alleging smart TVs spy on users without consent",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/12/texas-sues-biggest-tv-makers-alleging-smart-tvs-spy-on-users-without-consent/",
          "published_at": "Tue, 16 Dec 2025 21:57:55 +0000",
          "title": "Texas sues biggest TV makers, alleging smart TVs spy on users without consent",
          "standfirst": "Automated Content Recognition brings \"mass surveillance\" to homes, lawsuits say.",
          "content": "Texas Attorney General Ken Paxton sued five large TV manufacturers yesterday, alleging that their smart TVs spy on viewers without consent. Paxton sued Samsung, the longtime TV market share leader, along with LG, Sony, Hisense, and TCL. “These companies have been unlawfully collecting personal data through Automated Content Recognition (‘ACR’) technology,” Paxton’s office alleged in a press release that contains links to all five lawsuits. “ACR in its simplest terms is an uninvited, invisible digital invader. This software can capture screenshots of a user’s television display every 500 milliseconds, monitor viewing activity in real time, and transmit that information back to the company without the user’s knowledge or consent. The companies then sell that consumer information to target ads across platforms for a profit. This technology puts users’ privacy and sensitive information, such as passwords, bank information, and other personal information at risk.” The lawsuits allege violations of the Texas Deceptive Trade Practices Act, seeking damages of up to $10,000 for each violation and up to $250,000 for each violation affecting people 65 years or older. Texas also wants restraining orders prohibiting the collection, sharing, and selling of ACR data while the lawsuits are pending.Read full article Comments",
          "feed_position": 11,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/smart-tv-and-phone-1152x648-1765917384.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/smart-tv-and-phone-1152x648-1765917384.jpg",
      "popularity_score": 133
    },
    {
      "id": "cluster_120",
      "coverage": 1,
      "updated_at": "Tue, 16 Dec 2025 20:25:57 +0000",
      "title": "Senators count the shady ways data centers pass energy costs on to Americans",
      "neutral_headline": "Senators count the shady ways data centers pass energy costs on to Americans",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/12/shady-data-center-deals-doom-americans-to-higher-energy-bills-senators-say/",
          "published_at": "Tue, 16 Dec 2025 20:25:57 +0000",
          "title": "Senators count the shady ways data centers pass energy costs on to Americans",
          "standfirst": "Senators demand Big Tech pay upfront for data center spikes in electricity bills.",
          "content": "Senators launched a probe Tuesday demanding that tech companies explain exactly how they plan to prevent data center projects from increasing electricity bills in communities where prices are already skyrocketing. In letters to seven AI firms, Senators Elizabeth Warren (D-Mass.), Chris Van Hollen (D-Md.), and Richard Blumenthal (D-Conn.) cited a study estimating that “electricity prices have increased by as much as 267 percent in the past five years” in “areas located near significant data center activity.” Prices increase, senators noted, when utility companies build out extra infrastructure to meet data centers’ energy demands—which can amount to one customer suddenly consuming as much power as an entire city. They also increase when demand for local power outweighs supply. In some cases, residents are blindsided by higher bills, not even realizing a data center project was approved, because tech companies seem intent on dodging backlash and frequently do not allow terms of deals to be publicly disclosed.Read full article Comments",
          "feed_position": 13,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2249021916-1024x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2249021916-1024x648.jpg",
      "popularity_score": 133
    },
    {
      "id": "cluster_130",
      "coverage": 1,
      "updated_at": "Tue, 16 Dec 2025 17:28:32 +0000",
      "title": "Reporter suggests Half-Life 3 will be a Steam Machine launch title",
      "neutral_headline": "Reporter suggests Half-Life 3 will be a Steam Machine launch title",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gaming/2025/12/journalist-insists-half-life-3-will-launch-with-steam-machine-in-spring-2026/",
          "published_at": "Tue, 16 Dec 2025 17:28:32 +0000",
          "title": "Reporter suggests Half-Life 3 will be a Steam Machine launch title",
          "standfirst": "\"Everybody I've talked to are still adamant,\" even after previous rumors didn't pan out.",
          "content": "If you can take your mind way back to the beginning of 2025, you might remember a fresh wave of rumors suggesting that Half-Life 3 was finally reaching the final stages of production, and could be announced and/or released at any moment. Now, though, 2025 seems set to come to a close without any official news of a game fans have been waiting literal decades for. That doesn’t necessarily mean a Half-Life 3 announcement and/or release isn’t imminent, though. On the contrary, veteran journalist Mike Straw insisted on a recent Insider Gaming podcast that “everybody I’ve talked to are still adamant [Half-Life 3] is a game that will be a launch title with the Steam Machine.” Straw—who has a long history of reporting gaming rumors from anonymous sources—said this Half-Life 3 information is “not [from] these run-of-the-mill sources that haven’t gotten me information before. … These aren’t like random, one-off people.” And those sources are “still adamant that the game is coming in the spring,” Straw added, noting that he was “specifically told [that] spring 2026 [is the window] for the Steam Machine, for the Frame, for the Controller, [and] for Half-Life 3.”Read full article Comments",
          "feed_position": 15,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2017/08/024-1152x648-1740609184.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2017/08/024-1152x648-1740609184.jpg",
      "popularity_score": 133
    }
  ]
}