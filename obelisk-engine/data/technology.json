{
  "updated_at": "2025-11-13T19:16:18.011Z",
  "clusters": [
    {
      "id": "cluster_2",
      "coverage": 2,
      "updated_at": "Thu, 13 Nov 2025 19:07:49 +0000",
      "title": "Vampire Survivors VR asks what if the bullet hell was on your face?",
      "neutral_headline": "Vampire Survivors VR asks what if the bullet hell was on your face?",
      "items": [
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/ar-vr/vampire-survivors-vr-asks-what-if-the-bullet-hell-was-on-your-face-190749256.html",
          "published_at": "Thu, 13 Nov 2025 19:07:49 +0000",
          "title": "Vampire Survivors VR asks what if the bullet hell was on your face?",
          "standfirst": "During my many hours playing Vampire Survivors I often wondered what it would feel like to have the absolutely insane bullet hell action extremely close to my eyeballs. Now my dream is being fulfilled, as there's a VR port for Meta Quest headsets. The appropriately-named Vampire Survivors VR costs $10 and is available for the Meta Quest 2, 3, 3S and Pro. This isn't a game that puts you in the middle of the action, as you won't have to duck and weave around the living room to avoid incoming hordes. Rather, it gives you a top-down view of the stage and you use a controller to navigate as you would on any other platform. This kind of tactic has worked in the past with VR titles like Demeo. It can be played seated or standing, which isn't true of all VR games. Also, this is a standalone version and there's no cross-buy with other platforms. That means that save files don't carry over. That's a bummer, but I guess it makes sense given the uniqueness of VR. Developer Poncle has not revealed if this version will feature online co-op, which was recently announced as coming soon to the PC and console builds. As for DLC, Vampire Survivors VR includes the base game and the Legacy of the Moonspell and Tides of the Foscari expansions. We don't have any information regarding the status of other expansions like the cool Castlevania one or the utterly bizarre Balatro tie-in. Vampire Survivors VR is available right now, for those willing to risk a massive headache and perhaps a spot of nausea. Poncle currently has no plans to develop it for other VR platforms, like Steam VR or Pico.This article originally appeared on Engadget at https://www.engadget.com/ar-vr/vampire-survivors-vr-asks-what-if-the-bullet-hell-was-on-your-face-190749256.html?src=rss",
          "content": "During my many hours playing Vampire Survivors I often wondered what it would feel like to have the absolutely insane bullet hell action extremely close to my eyeballs. Now my dream is being fulfilled, as there's a VR port for Meta Quest headsets. The appropriately-named Vampire Survivors VR costs $10 and is available for the Meta Quest 2, 3, 3S and Pro. This isn't a game that puts you in the middle of the action, as you won't have to duck and weave around the living room to avoid incoming hordes. Rather, it gives you a top-down view of the stage and you use a controller to navigate as you would on any other platform. This kind of tactic has worked in the past with VR titles like Demeo. It can be played seated or standing, which isn't true of all VR games. Also, this is a standalone version and there's no cross-buy with other platforms. That means that save files don't carry over. That's a bummer, but I guess it makes sense given the uniqueness of VR. Developer Poncle has not revealed if this version will feature online co-op, which was recently announced as coming soon to the PC and console builds. As for DLC, Vampire Survivors VR includes the base game and the Legacy of the Moonspell and Tides of the Foscari expansions. We don't have any information regarding the status of other expansions like the cool Castlevania one or the utterly bizarre Balatro tie-in. Vampire Survivors VR is available right now, for those willing to risk a massive headache and perhaps a spot of nausea. Poncle currently has no plans to develop it for other VR platforms, like Steam VR or Pico.This article originally appeared on Engadget at https://www.engadget.com/ar-vr/vampire-survivors-vr-asks-what-if-the-bullet-hell-was-on-your-face-190749256.html?src=rss",
          "feed_position": 1
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/walmart-subscriptions-are-only-49-for-black-friday-and-it-includes-access-to-peacock-192739301.html",
          "published_at": "Thu, 13 Nov 2025 18:38:34 +0000",
          "title": "Walmart+ subscriptions are only $49 for Black Friday, and it includes access to Peacock",
          "standfirst": "Walmart is offering its Walmart+ subscription at half off for new sign-ups, and it includes a choice of either Peacock Premium or Paramount+ Essential. The deal for new subscribers is just $49 for the first year, marked down from $98. The real value is in selecting Peacock Premium, which would normally run you $110 per year on its own. With the current discount on a Walmart+ subscription you are essentially getting half off on your streaming subscription for that year. Just about every major streaming service has raised its prices in the last year, including HBO Max, Disney+, Netflix, Apple TV and YouTube TV, so saving some money on one of them just might be worth the effort. Cord cutting is not nearly as affordable as it used to be, so finding a deal like this is pretty helpful. Walmart+ itself offers myriad additional benefits like early access to Black Friday deals, free shipping on orders over $35, discounts on gas, free online veterinary care and more. Earlier this year, Walmart+ subscribers got first dibs on the Nintendo Switch 2 at the retailer. You can also use that free shipping to take advantage of Walmart's drone delivery program in a handful of select cities.This article originally appeared on Engadget at https://www.engadget.com/deals/walmart-subscriptions-are-only-49-for-black-friday-and-it-includes-access-to-peacock-192739301.html?src=rss",
          "content": "Walmart is offering its Walmart+ subscription at half off for new sign-ups, and it includes a choice of either Peacock Premium or Paramount+ Essential. The deal for new subscribers is just $49 for the first year, marked down from $98. The real value is in selecting Peacock Premium, which would normally run you $110 per year on its own. With the current discount on a Walmart+ subscription you are essentially getting half off on your streaming subscription for that year. Just about every major streaming service has raised its prices in the last year, including HBO Max, Disney+, Netflix, Apple TV and YouTube TV, so saving some money on one of them just might be worth the effort. Cord cutting is not nearly as affordable as it used to be, so finding a deal like this is pretty helpful. Walmart+ itself offers myriad additional benefits like early access to Black Friday deals, free shipping on orders over $35, discounts on gas, free online veterinary care and more. Earlier this year, Walmart+ subscribers got first dibs on the Nintendo Switch 2 at the retailer. You can also use that free shipping to take advantage of Walmart's drone delivery program in a handful of select cities.This article originally appeared on Engadget at https://www.engadget.com/deals/walmart-subscriptions-are-only-49-for-black-friday-and-it-includes-access-to-peacock-192739301.html?src=rss",
          "feed_position": 5
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/upwork-study-shows-ai-agents-excel-with-human-partners-but-fail",
          "published_at": "Thu, 13 Nov 2025 18:30:00 GMT",
          "title": "Upwork study shows AI agents excel with human partners but fail independently",
          "standfirst": "Artificial intelligence agents powered by the world&#x27;s most advanced language models routinely fail to complete even straightforward professional tasks on their own, according to groundbreaking research released Thursday by Upwork, the largest online work marketplace.But the same study reveals a more promising path forward: When AI agents collaborate with human experts, project completion rates surge by up to 70%, suggesting the future of work may not pit humans against machines but rather pair them together in powerful new ways.The findings, drawn from more than 300 real client projects posted to Upwork&#x27;s platform, marking the first systematic evaluation of how human expertise amplifies AI agent performance in actual professional work — not synthetic tests or academic simulations. The research challenges both the hype around fully autonomous AI agents and fears that such technology will imminently replace knowledge workers.\"AI agents aren&#x27;t that agentic, meaning they aren&#x27;t that good,\" Andrew Rabinovich, Upwork&#x27;s chief technology officer and head of AI and machine learning, said in an exclusive interview with VentureBeat. \"However, when paired with expert human professionals, project completion rates improve dramatically, supporting our firm belief that the future of work will be defined by humans and AI collaborating to get more work done, with human intuition and domain expertise playing a critical role.\"How AI agents performed on 300+ real freelance jobs—and why they struggledUpwork&#x27;s Human+Agent Productivity Index (HAPI) evaluated how three leading AI systems — Gemini 2.5 Pro, OpenAI&#x27;s GPT-5, and Claude Sonnet 4 — performed on actual jobs posted by paying clients across categories including writing, data science, web development, engineering, sales, and translation.Critically, Upwork deliberately selected simple, well-defined projects where AI agents stood a reasonable chance of success. These jobs, priced under $500, represent less than 6% of Upwork&#x27;s total gross services volume — a tiny fraction of the platform&#x27;s overall business and an acknowledgment of current AI limitations.\"The reality is that although we study AI, and I&#x27;ve been doing this for 25 years, and we see significant breakthroughs, the reality is that these agents aren&#x27;t that agentic,\" Rabinovich told VentureBeat. \"So if we go up the value chain, the problems become so much more difficult, then we don&#x27;t think they can solve them at all, even to scratch the surface. So we specifically chose simpler tasks that would give an agent some kind of traction.\"Even on these deliberately simplified tasks, AI agents working independently struggled. But when expert freelancers provided feedback — spending an average of just 20 minutes per review cycle — the agents&#x27; performance improved substantially with each iteration.20 minutes of human feedback boosted AI completion rates up to 70%The research reveals stark differences in how AI agents perform with and without human guidance across different types of work. For data science and analytics projects, Claude Sonnet 4 achieved a 64% completion rate working alone but jumped to 93% after receiving feedback from a human expert. In sales and marketing work, Gemini 2.5 Pro&#x27;s completion rate rose from 17% independently to 31% with human input. OpenAI&#x27;s GPT-5 showed similarly dramatic improvements in engineering and architecture tasks, climbing from 30% to 50% completion.The pattern held across virtually all categories, with agents responding particularly well to human feedback on qualitative, creative work requiring editorial judgment — areas like writing, translation, and marketing — where completion rates increased by up to 17 percentage points per feedback cycle.The finding challenges a fundamental assumption in the AI industry: that agent benchmarks conducted in isolation accurately predict real-world performance.\"While we show that in the tasks that we have selected for agents to perform in isolation, they perform similarly to the previous results that we&#x27;ve seen published openly, what we&#x27;ve shown is that in collaboration with humans, the performance of these agents improves surprisingly well,\" Rabinovich said. \"It&#x27;s not just a one-turn back and forth, but the more feedback the human provides, the better the agent gets at performing.\"Why ChatGPT can ace the SAT but can&#x27;t count the R&#x27;s in &#x27;strawberry&#x27;The research arrives as the AI industry grapples with a measurement crisis. Traditional benchmarks — standardized tests that AI models can master, sometimes scoring perfectly on SAT exams or mathematics olympiads — have proven poor predictors of real-world capability.\"With advances of large language models, what we&#x27;re now seeing is that these static, academic datasets are completely saturated,\" Rabinovich said. \"So you could get a perfect score in the SAT test or LSAT or any of the math olympiads, and then you would ask ChatGPT how many R&#x27;s there are in the word strawberry, and it would get it wrong.\"This phenomenon — where AI systems ace formal tests but stumble on trivial real-world questions — has led to growing skepticism about AI capabilities, even as companies race to deploy autonomous agents. Several recent benchmarks from other firms have tested AI agents on Upwork jobs, but those evaluations measured only isolated performance, not the collaborative potential that Upwork&#x27;s research reveals.\"We wanted to evaluate the quality of these agents on actual real work with economic value associated with it, and not only see how well these agents do, but also see how these agents do in collaboration with humans, because we sort of knew already that in isolation, they&#x27;re not that advanced,\" Rabinovich explained.For Upwork, which connects roughly 800,000 active clients posting more than 3 million jobs annually to a global pool of freelancers, the research serves a strategic business purpose: establishing quality standards for AI agents before allowing them to compete or collaborate with human workers on its platform.The economics of human-AI teamwork: Why paying for expert feedback still saves moneyDespite requiring multiple rounds of human feedback — each lasting about 20 minutes — the time investment remains \"orders of magnitude different between a human doing the work alone, versus a human doing the work with an AI agent,\" Rabinovich said. Where a project might take a freelancer days to complete independently, the agent-plus-human approach can deliver results in hours through iterative cycles of automated work and expert refinement.The economic implications extend beyond simple time savings. Upwork recently reported that gross services volume from AI-related work grew 53% year-over-year in the third quarter of 2025, one of the strongest growth drivers for the company. But executives have been careful to frame AI not as a replacement for freelancers but as an enhancement to their capabilities.\"AI was a huge overhang for our valuation,\" Erica Gessert, Upwork&#x27;s CFO, told CFO Brew in October. \"There was this belief that all work was going to go away. AI was going to take it, and especially work that&#x27;s done by people like freelancers, because they are impermanent. Actually, the opposite is true.\"The company&#x27;s strategy centers on enabling freelancers to handle more complex, higher-value work by offloading routine tasks to AI. \"Freelancers actually prefer to have tools that automate the manual labor and repetitive part of their work, and really focus on the creative and conceptual part of the process,\" Rabinovich said.Rather than replacing jobs, he argues, AI will transform them: \"Simpler tasks will be automated by agents, but the jobs will become much more complex in the number of tasks, so the amount of work and therefore earnings for freelancers will actually only go up.\"AI coding agents excel, but creative writing and translation still need humansThe research reveals a clear pattern in agent capabilities. AI systems perform best on \"deterministic and verifiable\" tasks with objectively correct answers, like solving math problems or writing basic code. \"Most coding tasks are very similar to each other,\" Rabinovich noted. \"That&#x27;s why coding agents are becoming so good.\"In Upwork&#x27;s tests, web development, mobile app development, and data science projects — especially those involving structured, computational work — saw the highest standalone agent completion rates. Claude Sonnet 4 completed 68% of web development jobs and 64% of data science projects without human help, while Gemini 2.5 Pro achieved 74% on certain technical tasks.But qualitative work proved far more challenging. When asked to create website layouts, write marketing copy, or translate content with appropriate cultural nuance, agents floundered without expert guidance. \"When you ask it to write you a poem, the quality of the poem is extremely subjective,\" Rabinovich said. \"Since the rubrics for evaluation were provided by humans, there&#x27;s some level of variability in representation.\"Writing, translation, and sales and marketing projects showed the most dramatic improvements from human feedback. For writing work, completion rates increased by up to 17 percentage points after expert review. Engineering and architecture projects requiring creative problem-solving — like civil engineering or architectural design — improved by as much as 23 percentage points with human oversight.This pattern suggests AI agents excel at pattern matching and replication but struggle with creativity, judgment, and context — precisely the skills that define higher-value professional work.Inside the research: How Upwork tested AI agents with peer-reviewed scientific methodsUpwork partnered with elite freelancers on its platform to evaluate every deliverable produced by AI agents, both independently and after each cycle of human feedback. These evaluators created detailed rubrics defining whether projects met core requirements specified in job descriptions, then scored outputs across multiple iterations.Importantly, evaluators focused only on objective completion criteria, excluding subjective factors like stylistic preferences or quality judgments that might emerge in actual client relationships. \"Rubric-based completion rates should not be viewed as a measure of whether an agent would be paid in a real marketplace setting,\" the research notes, \"but as an indicator of its ability to fulfill explicitly defined requests.\"This distinction matters: An AI agent might technically complete all specified requirements yet still produce work a client rejects as inadequate. Conversely, subjective client satisfaction — the true measure of marketplace success — remains beyond current measurement capabilities.The research underwent double-blind peer review and was accepted to NeurIPS, the premier academic conference for AI research, where Upwork will present full results in early December. The company plans to publish a complete methodology and make the benchmark available to the research community, updating the task pool regularly to prevent overfitting as agents improve.\"The idea is for this benchmark to be a living and breathing platform where agents can come in and evaluate themselves on all categories of work, and the tasks that will be offered on the platform will always update, so that these agents don&#x27;t overfit and basically memorize the tasks at hand,\" Rabinovich said.Upwork&#x27;s AI strategy: Building Uma, a &#x27;meta-agent&#x27; that manages human and AI workersThe research directly informs Upwork&#x27;s product roadmap as the company positions itself for what executives call \"the age of AI and beyond.\" Rather than building its own AI agents to complete specific tasks, Upwork is developing Uma, a \"meta orchestration agent\" that coordinates between human workers, AI systems, and clients.\"Today, Upwork is a marketplace where clients look for freelancers to get work done, and then talent comes to Upwork to find work,\" Rabinovich explained. \"This is getting expanded into a domain where clients come to Upwork, communicate with Uma, this meta-orchestration agent, and then Uma identifies the necessary talent to get the job done, gets the tasks outcomes completed, and then delivers that to the client.\"In this vision, clients would interact primarily with Uma rather than directly hiring freelancers. The AI system would analyze project requirements, determine which tasks require human expertise versus AI execution, coordinate the workflow, and ensure quality — acting as an intelligent project manager rather than a replacement worker.\"We don&#x27;t want to build agents that actually complete the tasks, but we are building this meta orchestration agent that figures out what human and agent talent is necessary in order to complete the tasks,\" Rabinovich said. \"Uma evaluates the work to be delivered to the client, orchestrates the interaction between humans and agents, and is able to learn from all the interactions that happen on the platform how to break jobs into tasks so that they get completed in a timely and effective manner.\"The company recently announced plans to open its first international office in Lisbon, Portugal, by the fourth quarter of 2026, with a focus on AI infrastructure development and technical hiring. The expansion follows Upwork&#x27;s record-breaking third quarter, driven partly by AI-powered product innovation and strong demand for workers with AI skills.OpenAI, Anthropic, and Google race to build autonomous agents—but reality lags hypeUpwork&#x27;s findings arrive amid escalating competition in the AI agent space. OpenAI, Anthropic, Google, and numerous startups are racing to develop autonomous agents capable of complex multi-step tasks, from booking travel to analyzing financial data to writing software.But recent high-profile stumbles have tempered initial enthusiasm. AI agents frequently misunderstand instructions, make logical errors, or produce confidently wrong results — a phenomenon researchers call \"hallucination.\" The gap between controlled demonstration videos and reliable real-world performance remains vast.\"There have been some evaluations that came from OpenAI and other platforms where real Upwork tasks were considered for completion by agents, and across the board, the reported results were not very optimistic, in the sense that they showed that agents—even the best ones, meaning powered by most advanced LLMs — can&#x27;t really compete with humans that well, because the completion rates are pretty low,\" Rabinovich said.Rather than waiting for AI to fully mature — a timeline that remains uncertain—Upwork is betting on a hybrid approach that leverages AI&#x27;s strengths (speed, scalability, pattern recognition) while retaining human strengths (judgment, creativity, contextual understanding).This philosophy extends to learning and improvement. Current AI models train primarily on static datasets scraped from the internet, supplemented by human preference feedback. But most professional work is qualitative, making it difficult for AI systems to know whether their outputs are actually good without expert evaluation.\"Unless you have this collaboration between the human and the machine, where the human is kind of the teacher and the machine is the student trying to discover new solutions, none of this will be possible,\" Rabinovich said. \"Upwork is very uniquely positioned to create such an environment because if you try to do this with, say, self-driving cars, and you tell Waymo cars to explore new ways of getting to the airport, like avoiding traffic signs, then a bunch of bad things will happen. In doing work on Upwork, if it creates a wrong website, it doesn&#x27;t cost very much, and there&#x27;s no negative side effects. But the opportunity to learn is absolutely tremendous.\"Will AI take your job? The evidence suggests a more complicated answerWhile much public discourse around AI focuses on job displacement, Rabinovich argues the historical pattern suggests otherwise — though the transition may prove disruptive.\"The narrative in the public is that AI is eliminating jobs, whether it&#x27;s writing, translation, coding or other digital work, but no one really talks about the exponential amount of new types of work that it will create,\" he said. \"When we invented electricity and steam engines and things like that, they certainly replaced certain jobs, but the amount of new jobs that were introduced is exponentially more, and we think the same is going to happen here.\"The research identifies emerging job categories focused on AI oversight: designing effective human-machine workflows, providing high-quality feedback to improve agent performance, and verifying that AI-generated work meets quality standards. These skills—prompt engineering, agent supervision, output verification—barely existed two years ago but now command premium rates on platforms like Upwork.\"New types of skills from humans are becoming necessary in the form of how to design the interaction between humans and machines, how to guide agents to make them better, and ultimately, how to verify that whatever agentic proposals are being made are actually correct, because that&#x27;s what&#x27;s necessary in order to advance the state of AI,\" Rabinovich said.The question remains whether this transition— from doing tasks to overseeing them — will create opportunities as quickly as it disrupts existing roles. For freelancers on Upwork, the answer may already be emerging in their bank accounts: The platform saw AI-related work grow 53% year-over-year, even as fears of AI-driven unemployment dominated headlines.",
          "content": "Artificial intelligence agents powered by the world&#x27;s most advanced language models routinely fail to complete even straightforward professional tasks on their own, according to groundbreaking research released Thursday by Upwork, the largest online work marketplace.But the same study reveals a more promising path forward: When AI agents collaborate with human experts, project completion rates surge by up to 70%, suggesting the future of work may not pit humans against machines but rather pair them together in powerful new ways.The findings, drawn from more than 300 real client projects posted to Upwork&#x27;s platform, marking the first systematic evaluation of how human expertise amplifies AI agent performance in actual professional work — not synthetic tests or academic simulations. The research challenges both the hype around fully autonomous AI agents and fears that such technology will imminently replace knowledge workers.\"AI agents aren&#x27;t that agentic, meaning they aren&#x27;t that good,\" Andrew Rabinovich, Upwork&#x27;s chief technology officer and head of AI and machine learning, said in an exclusive interview with VentureBeat. \"However, when paired with expert human professionals, project completion rates improve dramatically, supporting our firm belief that the future of work will be defined by humans and AI collaborating to get more work done, with human intuition and domain expertise playing a critical role.\"How AI agents performed on 300+ real freelance jobs—and why they struggledUpwork&#x27;s Human+Agent Productivity Index (HAPI) evaluated how three leading AI systems — Gemini 2.5 Pro, OpenAI&#x27;s GPT-5, and Claude Sonnet 4 — performed on actual jobs posted by paying clients across categories including writing, data science, web development, engineering, sales, and translation.Critically, Upwork deliberately selected simple, well-defined projects where AI agents stood a reasonable chance of success. These jobs, priced under $500, represent less than 6% of Upwork&#x27;s total gross services volume — a tiny fraction of the platform&#x27;s overall business and an acknowledgment of current AI limitations.\"The reality is that although we study AI, and I&#x27;ve been doing this for 25 years, and we see significant breakthroughs, the reality is that these agents aren&#x27;t that agentic,\" Rabinovich told VentureBeat. \"So if we go up the value chain, the problems become so much more difficult, then we don&#x27;t think they can solve them at all, even to scratch the surface. So we specifically chose simpler tasks that would give an agent some kind of traction.\"Even on these deliberately simplified tasks, AI agents working independently struggled. But when expert freelancers provided feedback — spending an average of just 20 minutes per review cycle — the agents&#x27; performance improved substantially with each iteration.20 minutes of human feedback boosted AI completion rates up to 70%The research reveals stark differences in how AI agents perform with and without human guidance across different types of work. For data science and analytics projects, Claude Sonnet 4 achieved a 64% completion rate working alone but jumped to 93% after receiving feedback from a human expert. In sales and marketing work, Gemini 2.5 Pro&#x27;s completion rate rose from 17% independently to 31% with human input. OpenAI&#x27;s GPT-5 showed similarly dramatic improvements in engineering and architecture tasks, climbing from 30% to 50% completion.The pattern held across virtually all categories, with agents responding particularly well to human feedback on qualitative, creative work requiring editorial judgment — areas like writing, translation, and marketing — where completion rates increased by up to 17 percentage points per feedback cycle.The finding challenges a fundamental assumption in the AI industry: that agent benchmarks conducted in isolation accurately predict real-world performance.\"While we show that in the tasks that we have selected for agents to perform in isolation, they perform similarly to the previous results that we&#x27;ve seen published openly, what we&#x27;ve shown is that in collaboration with humans, the performance of these agents improves surprisingly well,\" Rabinovich said. \"It&#x27;s not just a one-turn back and forth, but the more feedback the human provides, the better the agent gets at performing.\"Why ChatGPT can ace the SAT but can&#x27;t count the R&#x27;s in &#x27;strawberry&#x27;The research arrives as the AI industry grapples with a measurement crisis. Traditional benchmarks — standardized tests that AI models can master, sometimes scoring perfectly on SAT exams or mathematics olympiads — have proven poor predictors of real-world capability.\"With advances of large language models, what we&#x27;re now seeing is that these static, academic datasets are completely saturated,\" Rabinovich said. \"So you could get a perfect score in the SAT test or LSAT or any of the math olympiads, and then you would ask ChatGPT how many R&#x27;s there are in the word strawberry, and it would get it wrong.\"This phenomenon — where AI systems ace formal tests but stumble on trivial real-world questions — has led to growing skepticism about AI capabilities, even as companies race to deploy autonomous agents. Several recent benchmarks from other firms have tested AI agents on Upwork jobs, but those evaluations measured only isolated performance, not the collaborative potential that Upwork&#x27;s research reveals.\"We wanted to evaluate the quality of these agents on actual real work with economic value associated with it, and not only see how well these agents do, but also see how these agents do in collaboration with humans, because we sort of knew already that in isolation, they&#x27;re not that advanced,\" Rabinovich explained.For Upwork, which connects roughly 800,000 active clients posting more than 3 million jobs annually to a global pool of freelancers, the research serves a strategic business purpose: establishing quality standards for AI agents before allowing them to compete or collaborate with human workers on its platform.The economics of human-AI teamwork: Why paying for expert feedback still saves moneyDespite requiring multiple rounds of human feedback — each lasting about 20 minutes — the time investment remains \"orders of magnitude different between a human doing the work alone, versus a human doing the work with an AI agent,\" Rabinovich said. Where a project might take a freelancer days to complete independently, the agent-plus-human approach can deliver results in hours through iterative cycles of automated work and expert refinement.The economic implications extend beyond simple time savings. Upwork recently reported that gross services volume from AI-related work grew 53% year-over-year in the third quarter of 2025, one of the strongest growth drivers for the company. But executives have been careful to frame AI not as a replacement for freelancers but as an enhancement to their capabilities.\"AI was a huge overhang for our valuation,\" Erica Gessert, Upwork&#x27;s CFO, told CFO Brew in October. \"There was this belief that all work was going to go away. AI was going to take it, and especially work that&#x27;s done by people like freelancers, because they are impermanent. Actually, the opposite is true.\"The company&#x27;s strategy centers on enabling freelancers to handle more complex, higher-value work by offloading routine tasks to AI. \"Freelancers actually prefer to have tools that automate the manual labor and repetitive part of their work, and really focus on the creative and conceptual part of the process,\" Rabinovich said.Rather than replacing jobs, he argues, AI will transform them: \"Simpler tasks will be automated by agents, but the jobs will become much more complex in the number of tasks, so the amount of work and therefore earnings for freelancers will actually only go up.\"AI coding agents excel, but creative writing and translation still need humansThe research reveals a clear pattern in agent capabilities. AI systems perform best on \"deterministic and verifiable\" tasks with objectively correct answers, like solving math problems or writing basic code. \"Most coding tasks are very similar to each other,\" Rabinovich noted. \"That&#x27;s why coding agents are becoming so good.\"In Upwork&#x27;s tests, web development, mobile app development, and data science projects — especially those involving structured, computational work — saw the highest standalone agent completion rates. Claude Sonnet 4 completed 68% of web development jobs and 64% of data science projects without human help, while Gemini 2.5 Pro achieved 74% on certain technical tasks.But qualitative work proved far more challenging. When asked to create website layouts, write marketing copy, or translate content with appropriate cultural nuance, agents floundered without expert guidance. \"When you ask it to write you a poem, the quality of the poem is extremely subjective,\" Rabinovich said. \"Since the rubrics for evaluation were provided by humans, there&#x27;s some level of variability in representation.\"Writing, translation, and sales and marketing projects showed the most dramatic improvements from human feedback. For writing work, completion rates increased by up to 17 percentage points after expert review. Engineering and architecture projects requiring creative problem-solving — like civil engineering or architectural design — improved by as much as 23 percentage points with human oversight.This pattern suggests AI agents excel at pattern matching and replication but struggle with creativity, judgment, and context — precisely the skills that define higher-value professional work.Inside the research: How Upwork tested AI agents with peer-reviewed scientific methodsUpwork partnered with elite freelancers on its platform to evaluate every deliverable produced by AI agents, both independently and after each cycle of human feedback. These evaluators created detailed rubrics defining whether projects met core requirements specified in job descriptions, then scored outputs across multiple iterations.Importantly, evaluators focused only on objective completion criteria, excluding subjective factors like stylistic preferences or quality judgments that might emerge in actual client relationships. \"Rubric-based completion rates should not be viewed as a measure of whether an agent would be paid in a real marketplace setting,\" the research notes, \"but as an indicator of its ability to fulfill explicitly defined requests.\"This distinction matters: An AI agent might technically complete all specified requirements yet still produce work a client rejects as inadequate. Conversely, subjective client satisfaction — the true measure of marketplace success — remains beyond current measurement capabilities.The research underwent double-blind peer review and was accepted to NeurIPS, the premier academic conference for AI research, where Upwork will present full results in early December. The company plans to publish a complete methodology and make the benchmark available to the research community, updating the task pool regularly to prevent overfitting as agents improve.\"The idea is for this benchmark to be a living and breathing platform where agents can come in and evaluate themselves on all categories of work, and the tasks that will be offered on the platform will always update, so that these agents don&#x27;t overfit and basically memorize the tasks at hand,\" Rabinovich said.Upwork&#x27;s AI strategy: Building Uma, a &#x27;meta-agent&#x27; that manages human and AI workersThe research directly informs Upwork&#x27;s product roadmap as the company positions itself for what executives call \"the age of AI and beyond.\" Rather than building its own AI agents to complete specific tasks, Upwork is developing Uma, a \"meta orchestration agent\" that coordinates between human workers, AI systems, and clients.\"Today, Upwork is a marketplace where clients look for freelancers to get work done, and then talent comes to Upwork to find work,\" Rabinovich explained. \"This is getting expanded into a domain where clients come to Upwork, communicate with Uma, this meta-orchestration agent, and then Uma identifies the necessary talent to get the job done, gets the tasks outcomes completed, and then delivers that to the client.\"In this vision, clients would interact primarily with Uma rather than directly hiring freelancers. The AI system would analyze project requirements, determine which tasks require human expertise versus AI execution, coordinate the workflow, and ensure quality — acting as an intelligent project manager rather than a replacement worker.\"We don&#x27;t want to build agents that actually complete the tasks, but we are building this meta orchestration agent that figures out what human and agent talent is necessary in order to complete the tasks,\" Rabinovich said. \"Uma evaluates the work to be delivered to the client, orchestrates the interaction between humans and agents, and is able to learn from all the interactions that happen on the platform how to break jobs into tasks so that they get completed in a timely and effective manner.\"The company recently announced plans to open its first international office in Lisbon, Portugal, by the fourth quarter of 2026, with a focus on AI infrastructure development and technical hiring. The expansion follows Upwork&#x27;s record-breaking third quarter, driven partly by AI-powered product innovation and strong demand for workers with AI skills.OpenAI, Anthropic, and Google race to build autonomous agents—but reality lags hypeUpwork&#x27;s findings arrive amid escalating competition in the AI agent space. OpenAI, Anthropic, Google, and numerous startups are racing to develop autonomous agents capable of complex multi-step tasks, from booking travel to analyzing financial data to writing software.But recent high-profile stumbles have tempered initial enthusiasm. AI agents frequently misunderstand instructions, make logical errors, or produce confidently wrong results — a phenomenon researchers call \"hallucination.\" The gap between controlled demonstration videos and reliable real-world performance remains vast.\"There have been some evaluations that came from OpenAI and other platforms where real Upwork tasks were considered for completion by agents, and across the board, the reported results were not very optimistic, in the sense that they showed that agents—even the best ones, meaning powered by most advanced LLMs — can&#x27;t really compete with humans that well, because the completion rates are pretty low,\" Rabinovich said.Rather than waiting for AI to fully mature — a timeline that remains uncertain—Upwork is betting on a hybrid approach that leverages AI&#x27;s strengths (speed, scalability, pattern recognition) while retaining human strengths (judgment, creativity, contextual understanding).This philosophy extends to learning and improvement. Current AI models train primarily on static datasets scraped from the internet, supplemented by human preference feedback. But most professional work is qualitative, making it difficult for AI systems to know whether their outputs are actually good without expert evaluation.\"Unless you have this collaboration between the human and the machine, where the human is kind of the teacher and the machine is the student trying to discover new solutions, none of this will be possible,\" Rabinovich said. \"Upwork is very uniquely positioned to create such an environment because if you try to do this with, say, self-driving cars, and you tell Waymo cars to explore new ways of getting to the airport, like avoiding traffic signs, then a bunch of bad things will happen. In doing work on Upwork, if it creates a wrong website, it doesn&#x27;t cost very much, and there&#x27;s no negative side effects. But the opportunity to learn is absolutely tremendous.\"Will AI take your job? The evidence suggests a more complicated answerWhile much public discourse around AI focuses on job displacement, Rabinovich argues the historical pattern suggests otherwise — though the transition may prove disruptive.\"The narrative in the public is that AI is eliminating jobs, whether it&#x27;s writing, translation, coding or other digital work, but no one really talks about the exponential amount of new types of work that it will create,\" he said. \"When we invented electricity and steam engines and things like that, they certainly replaced certain jobs, but the amount of new jobs that were introduced is exponentially more, and we think the same is going to happen here.\"The research identifies emerging job categories focused on AI oversight: designing effective human-machine workflows, providing high-quality feedback to improve agent performance, and verifying that AI-generated work meets quality standards. These skills—prompt engineering, agent supervision, output verification—barely existed two years ago but now command premium rates on platforms like Upwork.\"New types of skills from humans are becoming necessary in the form of how to design the interaction between humans and machines, how to guide agents to make them better, and ultimately, how to verify that whatever agentic proposals are being made are actually correct, because that&#x27;s what&#x27;s necessary in order to advance the state of AI,\" Rabinovich said.The question remains whether this transition— from doing tasks to overseeing them — will create opportunities as quickly as it disrupts existing roles. For freelancers on Upwork, the answer may already be emerging in their bank accounts: The platform saw AI-related work grow 53% year-over-year, even as fears of AI-driven unemployment dominated headlines.",
          "feed_position": 0,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/1v1ddqAyA62qgnuh9O3ek5/6c2c87800376ecfd23a29d2c01c593e4/nuneybits_Robot_and_human_working_side_by_side_in_a_modern_offi_8b67c654-4d49-42c2-a291-5a9bb86c99db.webp?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/quicken-simplifi-subscriptions-are-half-off-for-black-friday-160025840.html",
          "published_at": "Thu, 13 Nov 2025 18:01:27 +0000",
          "title": "Quicken Simplifi subscriptions are half off for Black Friday",
          "standfirst": "Budgeting can be a stressful, challenging and uncomfortable experience. While it's completely possible to do it on your own, we've become fans of a few great budgeting apps. Take our favorite budgeting app Quicken, which is having a 50 percent off sale for Black Friday. The Quicken Simplifi app is down to $3 monthly from $6 monthly, adding up to $36 for the year. Quicken Classic, the company's \"original desktop software\" for \"experienced investors\" is also half off at $6 monthly, down from $12 monthly. The sale starts today and is available until Wednesday, December 3. One of the many things that sets Quicken Simplifi apart from its competitors is its sleek, easy to use interface. The setup is pretty straightforward and it allows for your spouse or financial advisor to act as co-manager of the account. It also clearly shows figures like net worth, recent spending, upcoming recurring payments and more. Plus, there's an option to say if you're expecting a refund. Quicken Simplifi unfortunately doesn't offer a free trial so testing it out with a discount means less money invested if it's not for you. This article originally appeared on Engadget at https://www.engadget.com/deals/quicken-simplifi-subscriptions-are-half-off-for-black-friday-160025840.html?src=rss",
          "content": "Budgeting can be a stressful, challenging and uncomfortable experience. While it's completely possible to do it on your own, we've become fans of a few great budgeting apps. Take our favorite budgeting app Quicken, which is having a 50 percent off sale for Black Friday. The Quicken Simplifi app is down to $3 monthly from $6 monthly, adding up to $36 for the year. Quicken Classic, the company's \"original desktop software\" for \"experienced investors\" is also half off at $6 monthly, down from $12 monthly. The sale starts today and is available until Wednesday, December 3. One of the many things that sets Quicken Simplifi apart from its competitors is its sleek, easy to use interface. The setup is pretty straightforward and it allows for your spouse or financial advisor to act as co-manager of the account. It also clearly shows figures like net worth, recent spending, upcoming recurring payments and more. Plus, there's an option to say if you're expecting a refund. Quicken Simplifi unfortunately doesn't offer a free trial so testing it out with a discount means less money invested if it's not for you. This article originally appeared on Engadget at https://www.engadget.com/deals/quicken-simplifi-subscriptions-are-half-off-for-black-friday-160025840.html?src=rss",
          "feed_position": 7
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/lego-black-friday-deals-on-star-wars-and-disney-sets-are-up-to-37-percent-off-ahead-of-the-big-sale-155007410.html",
          "published_at": "Thu, 13 Nov 2025 17:47:35 +0000",
          "title": "Lego Black Friday deals on Star Wars and Disney sets are up to 37 percent off ahead of the big sale",
          "standfirst": "Lego is cutting prices for Black Friday and Cyber Monday. I'm a lifelong Lego lover, and my younger self would have squealed at some of the deals I'm seeing — as I'm betting is true for many of you. With so many to choose from, you'll be able to find one that makes a great gift for anyone who you know loves these little building bricks. Throughout the month, I'll be checking all the biggest online shopping centers in search of the biggest discounts on the best Lego sets available. We've already seen low prices on Disney and Harry Potter advent calendars your kids will love this December, plus beautiful botanicals and art pieces for yourself or the adults on your list. It's only likely to get better, so keep checking regularly. In general, we always recommend using a price tracker when determining if a Lego deal is in fact a good one. You'll find Lego deals across the board this holiday season at retailers like Amazon and Walmart, but don't overlook Lego's own site. If you join the free Lego Insiders program, you'll build up points with each purchase that you can redeem in the future, get special discounts and sometimes get exclusive gifts when you buy. While not a deal, arguably the hottest Lego for Black Friday will be the brand new Star Trek USS Enterprise set, which was announced recently. It has a whopping 3,600 pieces and will be a must-have for any Star Trek fans. The set will be available starting November 28 for $400. Best Lego Black Friday deals LEGO Disney Frozen Advent Calendar 2025 43273 for $31 (32 percent off) Lego Harry Potter Advent Calendar 2025 76456 for $37 (17 percent off) LEGO Star Wars Brick-Built Star Wars Logo 75407 for $48 (19 percent off) LEGO Star Wars Grogu with Hover Pram Building Toy Set 75403 for $50 (50 percent off) LEGO Star Wars Millennium Falcon A New Hope 25th Anniversary Collectable Model 75375 for $68 (20 percent off) LEGO Star Wars R2-D2 Building Toy Set 75379 for $80 (20 percent off) LEGO Harry Potter Hogwarts Castle and Grounds 76419 for $136 (20 percent off) LEGO Creator 3 in 1 Magical Unicorn Toy 31140 for $7 (32 percent off) LEGO City Donut Truck Toy 60452 for $16 (20 percent off) LEGO Speed Champions 2 Fast 2 Furious Nissan Skyline GT-R (R34) Race Car 76917 for $18 (28 percent off) LEGO Botanicals Happy Plants Building Toys 10349 for $20 (13 percent off) LEGO Botanicals Mini Orchid Building Set 10343 for $24 (20 percent off) LEGO Art Hokusai The Great Wave Framed Japanese Wall Art Building Set 31208 for $85 (15 percent off) LEGO Ideas Tuxedo Cat 21349 for $80 (20 percent off) LEGO Icons Fountain Garden Building Set 10359 for $70 (30 percent off) LEGO Dreamzzz Izzie's Dream Animals Toys 71481 for $32 (20 percent off) LEGO NINJAGO Dragon Stone Shrine 71819 for $79 (34 percent off) LEGO Harry Potter Mandrake Figure & Pot Plant Toy 76433 for $49 (30 percent off) This article originally appeared on Engadget at https://www.engadget.com/deals/lego-black-friday-deals-on-star-wars-and-disney-sets-are-up-to-37-percent-off-ahead-of-the-big-sale-155007410.html?src=rss",
          "content": "Lego is cutting prices for Black Friday and Cyber Monday. I'm a lifelong Lego lover, and my younger self would have squealed at some of the deals I'm seeing — as I'm betting is true for many of you. With so many to choose from, you'll be able to find one that makes a great gift for anyone who you know loves these little building bricks. Throughout the month, I'll be checking all the biggest online shopping centers in search of the biggest discounts on the best Lego sets available. We've already seen low prices on Disney and Harry Potter advent calendars your kids will love this December, plus beautiful botanicals and art pieces for yourself or the adults on your list. It's only likely to get better, so keep checking regularly. In general, we always recommend using a price tracker when determining if a Lego deal is in fact a good one. You'll find Lego deals across the board this holiday season at retailers like Amazon and Walmart, but don't overlook Lego's own site. If you join the free Lego Insiders program, you'll build up points with each purchase that you can redeem in the future, get special discounts and sometimes get exclusive gifts when you buy. While not a deal, arguably the hottest Lego for Black Friday will be the brand new Star Trek USS Enterprise set, which was announced recently. It has a whopping 3,600 pieces and will be a must-have for any Star Trek fans. The set will be available starting November 28 for $400. Best Lego Black Friday deals LEGO Disney Frozen Advent Calendar 2025 43273 for $31 (32 percent off) Lego Harry Potter Advent Calendar 2025 76456 for $37 (17 percent off) LEGO Star Wars Brick-Built Star Wars Logo 75407 for $48 (19 percent off) LEGO Star Wars Grogu with Hover Pram Building Toy Set 75403 for $50 (50 percent off) LEGO Star Wars Millennium Falcon A New Hope 25th Anniversary Collectable Model 75375 for $68 (20 percent off) LEGO Star Wars R2-D2 Building Toy Set 75379 for $80 (20 percent off) LEGO Harry Potter Hogwarts Castle and Grounds 76419 for $136 (20 percent off) LEGO Creator 3 in 1 Magical Unicorn Toy 31140 for $7 (32 percent off) LEGO City Donut Truck Toy 60452 for $16 (20 percent off) LEGO Speed Champions 2 Fast 2 Furious Nissan Skyline GT-R (R34) Race Car 76917 for $18 (28 percent off) LEGO Botanicals Happy Plants Building Toys 10349 for $20 (13 percent off) LEGO Botanicals Mini Orchid Building Set 10343 for $24 (20 percent off) LEGO Art Hokusai The Great Wave Framed Japanese Wall Art Building Set 31208 for $85 (15 percent off) LEGO Ideas Tuxedo Cat 21349 for $80 (20 percent off) LEGO Icons Fountain Garden Building Set 10359 for $70 (30 percent off) LEGO Dreamzzz Izzie's Dream Animals Toys 71481 for $32 (20 percent off) LEGO NINJAGO Dragon Stone Shrine 71819 for $79 (34 percent off) LEGO Harry Potter Mandrake Figure & Pot Plant Toy 76433 for $49 (30 percent off) This article originally appeared on Engadget at https://www.engadget.com/deals/lego-black-friday-deals-on-star-wars-and-disney-sets-are-up-to-37-percent-off-ahead-of-the-big-sale-155007410.html?src=rss",
          "feed_position": 8
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/entertainment/streaming/how-youtube-tv-subscribers-can-get-their-20-credit-while-disneys-espn-and-abc-remain-off-the-air-172824463.html",
          "published_at": "Thu, 13 Nov 2025 17:28:24 +0000",
          "title": "How YouTube TV subscribers can get their $20 credit while Disney’s ESPN and ABC remain off the air",
          "standfirst": "Don't forget to claim your $20 credit from YouTube. (Getty Images)NurPhoto via Getty Images If you're a YouTube TV subscriber who relies on ESPN for football games, or maybe you never miss an episode of Dancing with the Stars (which airs on ABC), you're probably getting frustrated with the ongoing Disney feud. The two channels, along with other Disney-owned channels like Freeform and FX, have been off air since October 30, which is just over two weeks. While no resolution has been reached yet, the two large streaming companies are still negotiating the new pricing terms. One analyst has estimated that Disney is losing upwards of $4 million per day while the holdout continues. \"We’re ready to go as long as they want to,” said Disney's CFO Hugh Johnston on CNBC while discussing the company's quarterly earnings. Disney, of course, is trying to maximize the fees it gets for its channels, while YouTube claims Disney is \"proposing costly economic terms,\" which could result in higher subscription prices. But there's good news: YouTube has made good on its promise to offer a $20 credit to subscribers — you should've received an email from YouTube about it. However, it's not automatic so you'll need to claim it first — here's how to do it. How to get your $20 credit from YouTube TV YouTube TV already costs you at least $83 a month, so take advantage of the $20 credit while you can. Here's what you need to do. Open YouTube TV in a web browser and go to your Membership Settings in the upper-right corner. Go to the bottom of the menu and click Updates. Click \"Claim credit\" on the Updates screen. You should see a confirmation screen showing you've claimed the credit. It will be applied to your next bill, likely in December. Here's what your screen should look like when claiming the $20 credit. (Screenshots by Engadget) How to pause or cancel YouTube TV If the two can't reach an agreement, you may be looking to cancel your YouTube TV subscription (or at least pause it until there's a resolution). Here's how to do it. Open YouTube TV in a web browser and go to your Membership Settings. Click Membership. Click Manage. Click Cancel Membership, and then click Cancel to confirm. In the same settings, you can also pause your subscription until YouTube cuts a deal with Disney. Go to Settings > Membership > select the number of weeks you want to pause your subscription, and then click Pause. That'll at least give you more time to decide whether it's worth keeping your account or closing it out. What are my alternatives to YouTube TV? Your $20 credit just goes to your next YouTube TV bill. But if you view that as \"$20 more to spend on streaming this month,\" you can put that budget towards some fairly straightforward workarounds, including ESPN's new standalone service, Fubo (a YouTube TV competitor) and several more. Check out the best ways to watch ESPN and ABC during the YouTube TV blackout for more.This article originally appeared on Engadget at https://www.engadget.com/entertainment/streaming/how-youtube-tv-subscribers-can-get-their-20-credit-while-disneys-espn-and-abc-remain-off-the-air-172824463.html?src=rss",
          "content": "Don't forget to claim your $20 credit from YouTube. (Getty Images)NurPhoto via Getty Images If you're a YouTube TV subscriber who relies on ESPN for football games, or maybe you never miss an episode of Dancing with the Stars (which airs on ABC), you're probably getting frustrated with the ongoing Disney feud. The two channels, along with other Disney-owned channels like Freeform and FX, have been off air since October 30, which is just over two weeks. While no resolution has been reached yet, the two large streaming companies are still negotiating the new pricing terms. One analyst has estimated that Disney is losing upwards of $4 million per day while the holdout continues. \"We’re ready to go as long as they want to,” said Disney's CFO Hugh Johnston on CNBC while discussing the company's quarterly earnings. Disney, of course, is trying to maximize the fees it gets for its channels, while YouTube claims Disney is \"proposing costly economic terms,\" which could result in higher subscription prices. But there's good news: YouTube has made good on its promise to offer a $20 credit to subscribers — you should've received an email from YouTube about it. However, it's not automatic so you'll need to claim it first — here's how to do it. How to get your $20 credit from YouTube TV YouTube TV already costs you at least $83 a month, so take advantage of the $20 credit while you can. Here's what you need to do. Open YouTube TV in a web browser and go to your Membership Settings in the upper-right corner. Go to the bottom of the menu and click Updates. Click \"Claim credit\" on the Updates screen. You should see a confirmation screen showing you've claimed the credit. It will be applied to your next bill, likely in December. Here's what your screen should look like when claiming the $20 credit. (Screenshots by Engadget) How to pause or cancel YouTube TV If the two can't reach an agreement, you may be looking to cancel your YouTube TV subscription (or at least pause it until there's a resolution). Here's how to do it. Open YouTube TV in a web browser and go to your Membership Settings. Click Membership. Click Manage. Click Cancel Membership, and then click Cancel to confirm. In the same settings, you can also pause your subscription until YouTube cuts a deal with Disney. Go to Settings > Membership > select the number of weeks you want to pause your subscription, and then click Pause. That'll at least give you more time to decide whether it's worth keeping your account or closing it out. What are my alternatives to YouTube TV? Your $20 credit just goes to your next YouTube TV bill. But if you view that as \"$20 more to spend on streaming this month,\" you can put that budget towards some fairly straightforward workarounds, including ESPN's new standalone service, Fubo (a YouTube TV competitor) and several more. Check out the best ways to watch ESPN and ABC during the YouTube TV blackout for more.This article originally appeared on Engadget at https://www.engadget.com/entertainment/streaming/how-youtube-tv-subscribers-can-get-their-20-credit-while-disneys-espn-and-abc-remain-off-the-air-172824463.html?src=rss",
          "feed_position": 9,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2025-11/16f53210-bc0d-11f0-bfde-967b9d41c528"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/dyson-black-friday-deals-on-cordless-and-robot-vacuums-already-get-you-up-to-500-off-173533407.html",
          "published_at": "Thu, 13 Nov 2025 17:05:38 +0000",
          "title": "Dyson Black Friday deals on cordless and robot vacuums already get you up to $500 off",
          "standfirst": "Early Black Friday deals are starting to pop up across the web, and a great one to check out is at Dyson. While we still think you have the best shot to get the steepest discounts the closer to Black Friday we get, some of the discounts on Dyson's site right now are some of the best we've seen. One of those is 50 percent off the Dyson 360 Vis Nav robot vacuum, which is down to $500. Dyson was pretty late to the robot-vacuum party, but its entry was (and remains) one of the strongest in the category. It doesn't have a lot of bells and whistles like a self-emptying base or mopping capabilities, but it makes up for that by having probably the best suction power of any robovac we've tested. All kinds of debris will fall in its path: dirt, dust, food crumbs, pet hair and more. It also has excellent obstacle avoidance, so you'll rarely — if ever — have to dislodge it from getting stuck on the edge of a carpet or wedged in between furniture. Dyson's mobile app is easy to use as well, so if you're looking for a robot vacuum that does its main job incredibly well and you don't mind skipping on some extras, the 360 Vis Nav is a great option. Cordless vacuums are also a part of the sale. Take the Dyson V9 Motorbar cordless vacuum on sale for just $270 at both Dyson and Amazon, which is a discount of $330. That's more than half off. Dyson devices are all over our list of the best cordless vacuums, and for good reason. The company makes effective products. The V9 Motorbar has been designed to clean all floor types, in addition to upholstery. It's also been engineered to squeeze into tight spots, which is great for hitting those oft-neglected parts of the home. The suction power is on point and the battery lasts for 40 minutes before requiring a charge. That's just enough time to vacuum a standard-sized home if you don't stop for too many breaks. The V9 is getting a bit long-in-the-tooth. If you want a newer model, the V11 Extra is on sale for $400, which is a discount of $260. This one boosts the suction power and increases the battery life to 60 minutes. This article originally appeared on Engadget at https://www.engadget.com/deals/dyson-black-friday-deals-on-cordless-and-robot-vacuums-already-get-you-up-to-500-off-173533407.html?src=rss",
          "content": "Early Black Friday deals are starting to pop up across the web, and a great one to check out is at Dyson. While we still think you have the best shot to get the steepest discounts the closer to Black Friday we get, some of the discounts on Dyson's site right now are some of the best we've seen. One of those is 50 percent off the Dyson 360 Vis Nav robot vacuum, which is down to $500. Dyson was pretty late to the robot-vacuum party, but its entry was (and remains) one of the strongest in the category. It doesn't have a lot of bells and whistles like a self-emptying base or mopping capabilities, but it makes up for that by having probably the best suction power of any robovac we've tested. All kinds of debris will fall in its path: dirt, dust, food crumbs, pet hair and more. It also has excellent obstacle avoidance, so you'll rarely — if ever — have to dislodge it from getting stuck on the edge of a carpet or wedged in between furniture. Dyson's mobile app is easy to use as well, so if you're looking for a robot vacuum that does its main job incredibly well and you don't mind skipping on some extras, the 360 Vis Nav is a great option. Cordless vacuums are also a part of the sale. Take the Dyson V9 Motorbar cordless vacuum on sale for just $270 at both Dyson and Amazon, which is a discount of $330. That's more than half off. Dyson devices are all over our list of the best cordless vacuums, and for good reason. The company makes effective products. The V9 Motorbar has been designed to clean all floor types, in addition to upholstery. It's also been engineered to squeeze into tight spots, which is great for hitting those oft-neglected parts of the home. The suction power is on point and the battery lasts for 40 minutes before requiring a charge. That's just enough time to vacuum a standard-sized home if you don't stop for too many breaks. The V9 is getting a bit long-in-the-tooth. If you want a newer model, the V11 Extra is on sale for $400, which is a discount of $260. This one boosts the suction power and increases the battery life to 60 minutes. This article originally appeared on Engadget at https://www.engadget.com/deals/dyson-black-friday-deals-on-cordless-and-robot-vacuums-already-get-you-up-to-500-off-173533407.html?src=rss",
          "feed_position": 10
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/cameras/best-drone-120046775.html",
          "published_at": "Thu, 13 Nov 2025 17:00:36 +0000",
          "title": "The best drone for 2025",
          "standfirst": "Drones have evolved from hobbyist gadgets into everyday tools for creators and explorers. They’re smaller, easier to fly and packed with smart features that make aerial photography more accessible than ever. Whether you want to film sweeping landscapes or just learn to pilot your first quadcopter, there’s a drone built to match your needs.Beginner models now include GPS, collision sensors and automated flight modes that simplify controls. Move up a tier and you’ll find foldable designs that fit easily in a backpack while capturing stable 4K footage that rivals handheld cameras. For experienced flyers, the best camera drones combine larger sensors with precise stabilization and automated tracking for professional-quality results.We’ve tested drones across price ranges and categories to help you find one that fits how you want to fly, whether that means filming cinematic shots, traveling light or discovering a new perspective from above. Table of contents Best drones for 2025 What to look for in a drone Best drone FAQs Best drones for 2025 What to look for in a drone Camera features For this guide, we're looking only at drones that are basically flying cameras, so you want the best video and photo features possible. Bigger devices like DJI’s Mavic 3 Pro or Air 3S carry relatively large sensors, offering superior camera quality for nighttime cityscapes or other low-light scenes. Smaller models like the Mini 4 Pro and HoverAir X1 Max use smaller camera sensors, so they aren’t as good in dim light. Field of view and minimum aperture are also important, with most drones typically having a wide-angle focal length, though a few others like the HoverAir X1 Max carry an ultrawide lens. Some models have multiple cameras including a wide and a zoom. As for aperture, lower numbers are better and allow for shooting in dim light. Most DJI models are solid in this regard, while the HoverAir models don’t perform as well. Video resolution and slow-mo are also essential camera capabilities. Most drones these days can shoot at 4K with a frame rate of at least 30 fps, though some offer 6K or even 8K at up to 30 fps. Higher-end models can shoot 4K at up to 120 fps, allowing you to slow down the action dramatically to create a cinematic look. Other noteworthy features include log or HDR video that supports higher dynamic range, particularly in bright and sunny conditions. Finally, the camera’s gimbal and stabilization are important factors to keep your footage looking as smooth as possible. Some drones have gimbals that can rotate the camera 90 degrees to give social media creators the maximum resolution for vertical formats. Drone features: Speed, range, safety, battery life and obstacle detection By and large, there are two types of camera drones to consider. The first are standard drones (usually with open propellers but not always) designed to fly outside and take scenic shots. Often there’s nothing to stop the props from striking skin or objects, so they can’t really be used indoors or around people. Some models like the DJI Neo and Flip have prop guards that better protect bystanders and property, as well as the drone itself. Then there's first-person-view (FPV) camera drones, which often have propeller guards and are meant to be used both indoors or outside to capture exciting footage. Standard models don’t need to go particularly fast as they’re mainly used to shoot fun videos for social media, but FPV drones need to move at high speeds to create excitement. Because of that speed, they’re also better in breezy conditions thanks to stronger wind resistance, and they can fight gusts and return home more quickly. Acrobatic abilities (often promoted by the manufacturer in ads or packaging) are also important for FPV drones, as it allows the user to perform tricks and zip around obstacles. Battery life is another important factor. The best drones boast a battery endurance of up to 45 minutes, while FPV drones like the Avata 2 can only fly for about half that time as they tend to be heavier and carry smaller batteries to reduce weight. As a general rule, a single battery isn’t enough for any serious shooting so you’d do well to buy your drone in a kit with a few batteries and a charger. As for range, DJI tends to dominate in this area, with its latest models able to maintain a video signal at a distance up to 20km (12.4 miles). HoverAir’s models are weaker with the top-end X1 Max model limited to just 1km (0.6 miles) when using the optional beacon system. DJI also offers multiple ways to control its drones including headsets, joystick-type controllers, motion detection controllers and smartphones. The best drones have sensors to detect obstacles in all directions. Others are limited to only avoiding obstructions coming at them from the front and some only rely on the main camera to prevent crashes. Finally, if you want to have your drone follow you around automatically, you’ll need it to be able to track you around when you’re vlogging, riding a bike or skiing, while also avoiding obstacles. Smooth takeoff and return-to-home features are especially valuable here for both beginners and experienced drone pilots as well. Best drone FAQs What are the rules for owning a drone? Anyone can buy any drone, but once purchased, all drones between 250g and 25 kg must be registered with the FAA and marked with the FAA registration number. Recreational pilots with drones over 249g must pass the recreational UAS safety \"TRUST\" exam and carry proof of TRUST completion when flying a drone. Commercial pilots must obtain a Remote Pilot Certificate from the FAA. You must be aware of and avoid any areas with airspace restrictions, particularly around airports. Are drones safe to fly in the city? In general, it is not legal to fly a drone within city limits over populations, as a crash from a high altitude could injure or kill someone. However, they can be flown over adjacent, non-populated areas in many cases. Here is a guide to where: https://uavcoach.com/where-to-fly-drone/ What is the average flight time of a drone? Most drones can fly for around 20-30 minutes, though some advanced models like DJI's Mavic 4 can fly up to 40 minutes or more.This article originally appeared on Engadget at https://www.engadget.com/cameras/best-drone-120046775.html?src=rss",
          "content": "Drones have evolved from hobbyist gadgets into everyday tools for creators and explorers. They’re smaller, easier to fly and packed with smart features that make aerial photography more accessible than ever. Whether you want to film sweeping landscapes or just learn to pilot your first quadcopter, there’s a drone built to match your needs.Beginner models now include GPS, collision sensors and automated flight modes that simplify controls. Move up a tier and you’ll find foldable designs that fit easily in a backpack while capturing stable 4K footage that rivals handheld cameras. For experienced flyers, the best camera drones combine larger sensors with precise stabilization and automated tracking for professional-quality results.We’ve tested drones across price ranges and categories to help you find one that fits how you want to fly, whether that means filming cinematic shots, traveling light or discovering a new perspective from above. Table of contents Best drones for 2025 What to look for in a drone Best drone FAQs Best drones for 2025 What to look for in a drone Camera features For this guide, we're looking only at drones that are basically flying cameras, so you want the best video and photo features possible. Bigger devices like DJI’s Mavic 3 Pro or Air 3S carry relatively large sensors, offering superior camera quality for nighttime cityscapes or other low-light scenes. Smaller models like the Mini 4 Pro and HoverAir X1 Max use smaller camera sensors, so they aren’t as good in dim light. Field of view and minimum aperture are also important, with most drones typically having a wide-angle focal length, though a few others like the HoverAir X1 Max carry an ultrawide lens. Some models have multiple cameras including a wide and a zoom. As for aperture, lower numbers are better and allow for shooting in dim light. Most DJI models are solid in this regard, while the HoverAir models don’t perform as well. Video resolution and slow-mo are also essential camera capabilities. Most drones these days can shoot at 4K with a frame rate of at least 30 fps, though some offer 6K or even 8K at up to 30 fps. Higher-end models can shoot 4K at up to 120 fps, allowing you to slow down the action dramatically to create a cinematic look. Other noteworthy features include log or HDR video that supports higher dynamic range, particularly in bright and sunny conditions. Finally, the camera’s gimbal and stabilization are important factors to keep your footage looking as smooth as possible. Some drones have gimbals that can rotate the camera 90 degrees to give social media creators the maximum resolution for vertical formats. Drone features: Speed, range, safety, battery life and obstacle detection By and large, there are two types of camera drones to consider. The first are standard drones (usually with open propellers but not always) designed to fly outside and take scenic shots. Often there’s nothing to stop the props from striking skin or objects, so they can’t really be used indoors or around people. Some models like the DJI Neo and Flip have prop guards that better protect bystanders and property, as well as the drone itself. Then there's first-person-view (FPV) camera drones, which often have propeller guards and are meant to be used both indoors or outside to capture exciting footage. Standard models don’t need to go particularly fast as they’re mainly used to shoot fun videos for social media, but FPV drones need to move at high speeds to create excitement. Because of that speed, they’re also better in breezy conditions thanks to stronger wind resistance, and they can fight gusts and return home more quickly. Acrobatic abilities (often promoted by the manufacturer in ads or packaging) are also important for FPV drones, as it allows the user to perform tricks and zip around obstacles. Battery life is another important factor. The best drones boast a battery endurance of up to 45 minutes, while FPV drones like the Avata 2 can only fly for about half that time as they tend to be heavier and carry smaller batteries to reduce weight. As a general rule, a single battery isn’t enough for any serious shooting so you’d do well to buy your drone in a kit with a few batteries and a charger. As for range, DJI tends to dominate in this area, with its latest models able to maintain a video signal at a distance up to 20km (12.4 miles). HoverAir’s models are weaker with the top-end X1 Max model limited to just 1km (0.6 miles) when using the optional beacon system. DJI also offers multiple ways to control its drones including headsets, joystick-type controllers, motion detection controllers and smartphones. The best drones have sensors to detect obstacles in all directions. Others are limited to only avoiding obstructions coming at them from the front and some only rely on the main camera to prevent crashes. Finally, if you want to have your drone follow you around automatically, you’ll need it to be able to track you around when you’re vlogging, riding a bike or skiing, while also avoiding obstacles. Smooth takeoff and return-to-home features are especially valuable here for both beginners and experienced drone pilots as well. Best drone FAQs What are the rules for owning a drone? Anyone can buy any drone, but once purchased, all drones between 250g and 25 kg must be registered with the FAA and marked with the FAA registration number. Recreational pilots with drones over 249g must pass the recreational UAS safety \"TRUST\" exam and carry proof of TRUST completion when flying a drone. Commercial pilots must obtain a Remote Pilot Certificate from the FAA. You must be aware of and avoid any areas with airspace restrictions, particularly around airports. Are drones safe to fly in the city? In general, it is not legal to fly a drone within city limits over populations, as a crash from a high altitude could injure or kill someone. However, they can be flown over adjacent, non-populated areas in many cases. Here is a guide to where: https://uavcoach.com/where-to-fly-drone/ What is the average flight time of a drone? Most drones can fly for around 20-30 minutes, though some advanced models like DJI's Mavic 4 can fly up to 40 minutes or more.This article originally appeared on Engadget at https://www.engadget.com/cameras/best-drone-120046775.html?src=rss",
          "feed_position": 11
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/gaming/pc/valves-trio-of-hardware-announcements-revived-my-half-life-3-fever-dream-170000561.html",
          "published_at": "Thu, 13 Nov 2025 17:00:00 +0000",
          "title": "Valve's trio of hardware announcements revived my Half-Life 3 fever dream",
          "standfirst": "Yesterday, Valve announced three (3) hardware products: a Steam Machine console, an accompanying Steam Controller and the long-rumored Steam Frame VR headset. This hardware, along with the excellent Steam Deck, gives Valve a pretty comprehensive way to get people playing games wherever they want, on any sort of screen. And, of course, the games are what this is all about. Steam’s catalog is impossibly vast, encompassing every genre you can imagine — but there’s still one crucial title missing from the thousands of games available.I am, of course, talking about Half-Life 3.I swore back in 2017 that I’d stop beating this dead horse, but Valve sucked me back in with the utterly unexpected, excellent, VR-only Half-Life: Alyx prequel. At the very least, it was a sign that the Half-Life universe wasn’t dead and buried in Valve’s mind, despite the fact that it had lain dormant with an unresolved cliffhanger for more than a dozen years.At the time, Valve indicated it was interested in moving forward with more games in the series, though I wouldn’t have been surprised if the company just dropped things again. But, a big push into hardware that is significantly more powerful than the Steam Deck feels like another perfect opportunity to make Half-Life 3 happen.And there have actually been a few more concrete bread crumbs to follow over the last year or so indicating Valve might finally be returning to the Half-Life story. It started with the 20th anniversary of Half-Life 2, when Valve dropped a major update for the game. “Every map in Half-Life 2 has been looked over by Valve level designers to fix longstanding bugs, restore content and features lost to time, and improve the quality of a few things like lightmap resolution and fog,” the developer wrote. Along with some developer commentary, a documentary and the inclusion of the two episodic follow-up games, this was a pretty substantial update for such an old game.At the end of last year, YouTuber Gabe Follower dropped some details on a potential Half-Life 3 coming soon. Follower had previously called the release of Counter-Strike 2, lending some credibility to his findings. To make a long story short, Follower claimed a Valve project internally titled “HLX” had reached the play-testing stage. That didn’t necessarily mean a launch was imminent, but at the very least the game was advancing in development.Another less consequential but fun tidbit dropped around the same time: actor Michael Shapiro (who voiced the infamous G-Man in the Half-Life series) posted a New Years’ message where he spoke in the G-Man’s strange accent and said he’d see viewers in the year to come. Not coincidentally, he also did this in 2020 prior to the Half-Life: Alyx launch. The game had already been announced when he posted that message, but it’s still an intriguing tease. The timing couldn’t be better, either. The Game Awards are less than a month away, and that extravaganza is about the biggest platform you could ask for if you’re announcing a big new title. Not that Valve really needs the stage — they could just drop a trailer on YouTube and the gaming world would take care of the rest.But as a companion piece to the company’s renewed hardware ambitions? The synergy would be too good to pass up. After all, the Valve Index VR headset launched just a short time before Half-Life: Alyx was announced, and anyone who had purchased it got the game for free. A theoretical Half-Life 3 isn’t quite the same, as there’s no chance the game will require the official Steam Machine. But it would still make a heck of a launch title to help drive interest in the company’s new devices. As for me, I’m not letting myself get too excited here. I remember in 2013, when Valve introduced the first Steam Machines initiative and its first attempt at a controller, I assumed it would be a perfect time to announce Half-Life 3. That clearly did not happen. But I’d be lying if I said I wasn’t a little bit hopeful this time around. There’s enough smoke to make me think that the fire is real; it’s hopefully time to wake up and smell the ashes. This article originally appeared on Engadget at https://www.engadget.com/gaming/pc/valves-trio-of-hardware-announcements-revived-my-half-life-3-fever-dream-170000561.html?src=rss",
          "content": "Yesterday, Valve announced three (3) hardware products: a Steam Machine console, an accompanying Steam Controller and the long-rumored Steam Frame VR headset. This hardware, along with the excellent Steam Deck, gives Valve a pretty comprehensive way to get people playing games wherever they want, on any sort of screen. And, of course, the games are what this is all about. Steam’s catalog is impossibly vast, encompassing every genre you can imagine — but there’s still one crucial title missing from the thousands of games available.I am, of course, talking about Half-Life 3.I swore back in 2017 that I’d stop beating this dead horse, but Valve sucked me back in with the utterly unexpected, excellent, VR-only Half-Life: Alyx prequel. At the very least, it was a sign that the Half-Life universe wasn’t dead and buried in Valve’s mind, despite the fact that it had lain dormant with an unresolved cliffhanger for more than a dozen years.At the time, Valve indicated it was interested in moving forward with more games in the series, though I wouldn’t have been surprised if the company just dropped things again. But, a big push into hardware that is significantly more powerful than the Steam Deck feels like another perfect opportunity to make Half-Life 3 happen.And there have actually been a few more concrete bread crumbs to follow over the last year or so indicating Valve might finally be returning to the Half-Life story. It started with the 20th anniversary of Half-Life 2, when Valve dropped a major update for the game. “Every map in Half-Life 2 has been looked over by Valve level designers to fix longstanding bugs, restore content and features lost to time, and improve the quality of a few things like lightmap resolution and fog,” the developer wrote. Along with some developer commentary, a documentary and the inclusion of the two episodic follow-up games, this was a pretty substantial update for such an old game.At the end of last year, YouTuber Gabe Follower dropped some details on a potential Half-Life 3 coming soon. Follower had previously called the release of Counter-Strike 2, lending some credibility to his findings. To make a long story short, Follower claimed a Valve project internally titled “HLX” had reached the play-testing stage. That didn’t necessarily mean a launch was imminent, but at the very least the game was advancing in development.Another less consequential but fun tidbit dropped around the same time: actor Michael Shapiro (who voiced the infamous G-Man in the Half-Life series) posted a New Years’ message where he spoke in the G-Man’s strange accent and said he’d see viewers in the year to come. Not coincidentally, he also did this in 2020 prior to the Half-Life: Alyx launch. The game had already been announced when he posted that message, but it’s still an intriguing tease. The timing couldn’t be better, either. The Game Awards are less than a month away, and that extravaganza is about the biggest platform you could ask for if you’re announcing a big new title. Not that Valve really needs the stage — they could just drop a trailer on YouTube and the gaming world would take care of the rest.But as a companion piece to the company’s renewed hardware ambitions? The synergy would be too good to pass up. After all, the Valve Index VR headset launched just a short time before Half-Life: Alyx was announced, and anyone who had purchased it got the game for free. A theoretical Half-Life 3 isn’t quite the same, as there’s no chance the game will require the official Steam Machine. But it would still make a heck of a launch title to help drive interest in the company’s new devices. As for me, I’m not letting myself get too excited here. I remember in 2013, when Valve introduced the first Steam Machines initiative and its first attempt at a controller, I assumed it would be a perfect time to announce Half-Life 3. That clearly did not happen. But I’d be lying if I said I wasn’t a little bit hopeful this time around. There’s enough smoke to make me think that the fire is real; it’s hopefully time to wake up and smell the ashes. This article originally appeared on Engadget at https://www.engadget.com/gaming/pc/valves-trio-of-hardware-announcements-revived-my-half-life-3-fever-dream-170000561.html?src=rss",
          "feed_position": 12
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/black-friday-deals-for-2025-have-arrived-the-best-tech-sales-from-amazon-apple-lego-anker-and-others-so-far-100052657.html",
          "published_at": "Thu, 13 Nov 2025 16:36:23 +0000",
          "title": "Black Friday deals for 2025 have arrived: The best tech sales from Amazon, Apple, Lego, Anker and others so far",
          "standfirst": "Black Friday has become the time to buy the hottest tech of the year. Whether you're shopping for yourself or stocking up on gifts for the holidays, Black Friday deals are sure to bring the best prices of the year to things like headphones, game consoles, robot vacuums, phone accessories and everything in between. You don't even have to wait until Black Friday proper to save a ton of money. Over the past few years, we've seen Black Friday tech deals start earlier and earlier — to the point where the entire month of November is packed with discounts.If you're on the hunt for solid tech deals, Engadget has you covered. We've collected the best Black Friday deals on tech you can get right now, and we'll continue to update this post as we get closer to the big day at the end of November. Note that you probably have the best chance of snagging record-low prices when we get to about one week before Thanksgiving, but these deals available now are worth considering. Black Friday deals to shop now Apple AirTags (four pack) for $65 (34 percent off): iPhone users who frequently misplace things should invest in a few AirTags. Slip them into your wallet, bag, jacket and other belongings to keep track of their locations in the Find My app. Just make sure that, if you're going to attach one to your keys, you also pick up an AirTag holder to go along with it. Amazon Fire TV Stick 4K Max for $35 (42 percent off): One of our favorite streaming devices, the 4K Max dongle supports Dolby Vision, Dolby Atmos, 4K and HDR10+ and it has Wi-Fi 6E for a speedier, more stable internet connection. It also supports the Fire TV Ambient Experience, which can display art on your TV when you're not actively using it. Ninja Dual-Zone air fryer (10-quart) for $180 (22 percent off): If you cook for large crowds on Thanksgiving and other occasions, this is the air fryer to get. Not only is it a large, 10-quart capacity model, but it also has two separate cooking areas. You can crisp up potatoes on one side and brussel sprouts on the other with no issues. Use the Smart Finish feature to cook two separate foods in different ways and have them both be done at the same time, or Match Cook to copy the cooking method in both chambers. LEGO Star Wars Millennium Falcon A New Hope 25th Anniversary Collectable 75375 for $68 (20 percent off): This is a set that any Star Wars fan will love to build and then love to display once it's complete. The 921-piece set features a fully-detailed Millennium Falcone, buildable stand and nameplate. It's one of many Lego Black Friday deals you can get right now. Nintendo Switch 2 + Mario Kart World bundle for $499: Black Friday Nintendo sales were announced recently and, unsurprisingly, there won't be many true deals out there this year. There are no straight discounts on the Switch 2 console, so your best bet is to pick up a bundle that saves you some cash on a Switch 2 game. One of the best is the Mario Kart Wold bundle, but Pokémon fans should consider the Pokémon Legends: Z-A bundle, too. Apple MacBook Air (13-inch, M4) for $749 ($250 off): Our top pick for the best laptop you can get, the M4 MacBook Air will be plenty of power for most people in a convenient, premium package. It's thin and light as ever, with an excellent keyboard and trackpad, plus enough battery life to get you through a whole day of work, video calls and more. Apple Watch SE 3 for $200 ($50 off): The SE has been our top pick for the best Apple Watch for those on a budget, and the latest model only solidifies that further. It has the same chipset found in the latest flagship Apple Watches, fast-charging capabilities, an always-on display and most of the same activity-tracking features you'll find in more expensive model. Apple Mac Mini M4 for $499 ($100 off): Desktop users looking for an upgrade should consider the latest Mac Mini, which runs on the M4 chip and 16GB of RAM as standard in the base configuration. This version has a smaller design that takes up less space, front-facing USB-C ports and a headphone jack, plus Thunderbolt 5 support. Jisulife Life 7 handheld fan for $25 (14 percent off): This handy little fan is a must-have if you life in a warm climate or have a tropical vacation planned anytime soon. It can be used as a table or handheld fan and even be worn around the neck so you don't have to hold it at all. Its 5,000 mAh battery allows it to last hours on a single charge, and the small display in the middle of the fan's blades show its remaining battery level. Leebin 2025 electric spin scrubber for $40 (43 percent off, Prime exclusive): This weird little scrubber makes cleaning my bathroom and shower much less of a pain. Just choose the brush head you need for your job and the rotating head takes care of most of the hard work. I love the adjustable handle, which extends from 12 to 50 inches so you can get into hard-to-reach places without breaking a sweat. Monarch Money budgeting app (one year) for $50 (50 percent off with code MONARCHVIP): One of our favorite budgeting apps, Monarch Money gives you a lot of control over the organization of your funds. There's a helpful goals feature for when you're planning out big purchases or financial milestones you want to hit, and we found the month-in-review recap it provides to be more thorough than other budgeting apps we tried. There's even Zillow integration for folks looking to buy a home. SanDisk microSD Express card (256GB) for $60 (12 percent off): If you have a Switch 2, no regular microSD card will do if you want to expand the console's storage. You need a newer microSD Express card, and currently there are only a handful on the market. We did some testing to find the best microSD Express card for the Switch 2 and found that performance was, in general, very similar amongst all the readily available cards. We recommend getting whichever fits within your budget at the capacity you want. Google TV Streamer 4K for $75 ($25 off): Our top pick for the best streaming device right now, the latest version of Google's streamer supports 4K video and an excellent, easy-to-use interface that will feel familiar to anyone who's seen a set with the Google TV technology built in. It provides access to all of the major streaming services including Netflix, Disney+, HBO Max, YouTube and more, plus it has a handy on-screen pop up that lets you control all compatible smart home devices right from your TV. Also available at Walmart. Dyson 360 Vis Nav robot vacuum for $400 ($600 off): This is one of the best robot vacuums you can get, period. It doesn't have a self-emptying base, but its superior suction power almost makes up for that. It's one of the strongest robot vacuums I've ever tested, and it has excellent obstacle avoidance. The latter means you will rarely, if ever, have to attend to it getting caught on the edge of a carpet or getting stuck under a piece of furniture. If a cordless stick vacuum is what you're looking for, don't forget to check out all of the other Dyson Black Friday deals. EcoFlow Black Friday deals — get up to 80 percent off: Portable power stations are an investment, but they can be crucial pieces of tech during emergencies. The top pick from our friends at Yahoo Tech has been heavily discounted in this early Black Friday sale. You can pick up the EcoFlow Delta Pro 3 for $1,400 off, down to $2,299, or the power station with an extra battery bundled in for $2,699 off, down to $3,599. Black Friday FAQs When is Black Friday 2025? Black Friday 2025 lands on November 28. Which stores have Black Friday deals? Many physical retail stores have Black Friday deals including Walmart, Target, Best Buy and others. Even more retailers have online Black Friday deals, including Amazon, GameStop, Costco and others. When do Black Friday sales start? Gone are the times when Black Friday sales were one-day-only affairs. Now, Black Friday deals are often available starting on Thanksgiving, or even earlier. Last year, we saw Black Friday deals online begin the week before Black Friday proper. When do Black Friday sales end? Black Friday and Cyber Monday have blended a lot over the past few years. Now, you can expect to see a good portion of Black Friday deals extend through the weekend and into Cyber Monday. It's not uncommon for Black Friday deals to expire at the end of Cyber Monday. Which retailers have the best Black Friday tech deals? The best Black Friday tech deals are typically available online at retailers like Amazon, Walmart, Best Buy and Target. It's also a good idea to check the store websites of the companies that make the products you want — for example, if you're looking for a Sonos speaker, check the Sonos website on Black Friday. Most of the time, you'll find the best Black Friday tech deals are matched at multiple retailers. Does Apple have Black Friday sales? No, you will usually not find Black Friday sales at Apple stores or on Apple's website. However, you can find Black Friday deals on Apple devices elsewhere; we recommend checking Amazon, Best Buy and other big retailers for discounts on iPads, Apple Watches and more on Black Friday. Does Amazon have Black Friday sales? Yes, Amazon has Black Friday sales. The online retailer's site will look similar to Prime Day on Black Friday, with discounts on all sorts of items from household essentials to fashion to tech.This article originally appeared on Engadget at https://www.engadget.com/deals/black-friday-deals-for-2025-have-arrived-the-best-tech-sales-from-amazon-apple-lego-anker-and-others-so-far-100052657.html?src=rss",
          "content": "Black Friday has become the time to buy the hottest tech of the year. Whether you're shopping for yourself or stocking up on gifts for the holidays, Black Friday deals are sure to bring the best prices of the year to things like headphones, game consoles, robot vacuums, phone accessories and everything in between. You don't even have to wait until Black Friday proper to save a ton of money. Over the past few years, we've seen Black Friday tech deals start earlier and earlier — to the point where the entire month of November is packed with discounts.If you're on the hunt for solid tech deals, Engadget has you covered. We've collected the best Black Friday deals on tech you can get right now, and we'll continue to update this post as we get closer to the big day at the end of November. Note that you probably have the best chance of snagging record-low prices when we get to about one week before Thanksgiving, but these deals available now are worth considering. Black Friday deals to shop now Apple AirTags (four pack) for $65 (34 percent off): iPhone users who frequently misplace things should invest in a few AirTags. Slip them into your wallet, bag, jacket and other belongings to keep track of their locations in the Find My app. Just make sure that, if you're going to attach one to your keys, you also pick up an AirTag holder to go along with it. Amazon Fire TV Stick 4K Max for $35 (42 percent off): One of our favorite streaming devices, the 4K Max dongle supports Dolby Vision, Dolby Atmos, 4K and HDR10+ and it has Wi-Fi 6E for a speedier, more stable internet connection. It also supports the Fire TV Ambient Experience, which can display art on your TV when you're not actively using it. Ninja Dual-Zone air fryer (10-quart) for $180 (22 percent off): If you cook for large crowds on Thanksgiving and other occasions, this is the air fryer to get. Not only is it a large, 10-quart capacity model, but it also has two separate cooking areas. You can crisp up potatoes on one side and brussel sprouts on the other with no issues. Use the Smart Finish feature to cook two separate foods in different ways and have them both be done at the same time, or Match Cook to copy the cooking method in both chambers. LEGO Star Wars Millennium Falcon A New Hope 25th Anniversary Collectable 75375 for $68 (20 percent off): This is a set that any Star Wars fan will love to build and then love to display once it's complete. The 921-piece set features a fully-detailed Millennium Falcone, buildable stand and nameplate. It's one of many Lego Black Friday deals you can get right now. Nintendo Switch 2 + Mario Kart World bundle for $499: Black Friday Nintendo sales were announced recently and, unsurprisingly, there won't be many true deals out there this year. There are no straight discounts on the Switch 2 console, so your best bet is to pick up a bundle that saves you some cash on a Switch 2 game. One of the best is the Mario Kart Wold bundle, but Pokémon fans should consider the Pokémon Legends: Z-A bundle, too. Apple MacBook Air (13-inch, M4) for $749 ($250 off): Our top pick for the best laptop you can get, the M4 MacBook Air will be plenty of power for most people in a convenient, premium package. It's thin and light as ever, with an excellent keyboard and trackpad, plus enough battery life to get you through a whole day of work, video calls and more. Apple Watch SE 3 for $200 ($50 off): The SE has been our top pick for the best Apple Watch for those on a budget, and the latest model only solidifies that further. It has the same chipset found in the latest flagship Apple Watches, fast-charging capabilities, an always-on display and most of the same activity-tracking features you'll find in more expensive model. Apple Mac Mini M4 for $499 ($100 off): Desktop users looking for an upgrade should consider the latest Mac Mini, which runs on the M4 chip and 16GB of RAM as standard in the base configuration. This version has a smaller design that takes up less space, front-facing USB-C ports and a headphone jack, plus Thunderbolt 5 support. Jisulife Life 7 handheld fan for $25 (14 percent off): This handy little fan is a must-have if you life in a warm climate or have a tropical vacation planned anytime soon. It can be used as a table or handheld fan and even be worn around the neck so you don't have to hold it at all. Its 5,000 mAh battery allows it to last hours on a single charge, and the small display in the middle of the fan's blades show its remaining battery level. Leebin 2025 electric spin scrubber for $40 (43 percent off, Prime exclusive): This weird little scrubber makes cleaning my bathroom and shower much less of a pain. Just choose the brush head you need for your job and the rotating head takes care of most of the hard work. I love the adjustable handle, which extends from 12 to 50 inches so you can get into hard-to-reach places without breaking a sweat. Monarch Money budgeting app (one year) for $50 (50 percent off with code MONARCHVIP): One of our favorite budgeting apps, Monarch Money gives you a lot of control over the organization of your funds. There's a helpful goals feature for when you're planning out big purchases or financial milestones you want to hit, and we found the month-in-review recap it provides to be more thorough than other budgeting apps we tried. There's even Zillow integration for folks looking to buy a home. SanDisk microSD Express card (256GB) for $60 (12 percent off): If you have a Switch 2, no regular microSD card will do if you want to expand the console's storage. You need a newer microSD Express card, and currently there are only a handful on the market. We did some testing to find the best microSD Express card for the Switch 2 and found that performance was, in general, very similar amongst all the readily available cards. We recommend getting whichever fits within your budget at the capacity you want. Google TV Streamer 4K for $75 ($25 off): Our top pick for the best streaming device right now, the latest version of Google's streamer supports 4K video and an excellent, easy-to-use interface that will feel familiar to anyone who's seen a set with the Google TV technology built in. It provides access to all of the major streaming services including Netflix, Disney+, HBO Max, YouTube and more, plus it has a handy on-screen pop up that lets you control all compatible smart home devices right from your TV. Also available at Walmart. Dyson 360 Vis Nav robot vacuum for $400 ($600 off): This is one of the best robot vacuums you can get, period. It doesn't have a self-emptying base, but its superior suction power almost makes up for that. It's one of the strongest robot vacuums I've ever tested, and it has excellent obstacle avoidance. The latter means you will rarely, if ever, have to attend to it getting caught on the edge of a carpet or getting stuck under a piece of furniture. If a cordless stick vacuum is what you're looking for, don't forget to check out all of the other Dyson Black Friday deals. EcoFlow Black Friday deals — get up to 80 percent off: Portable power stations are an investment, but they can be crucial pieces of tech during emergencies. The top pick from our friends at Yahoo Tech has been heavily discounted in this early Black Friday sale. You can pick up the EcoFlow Delta Pro 3 for $1,400 off, down to $2,299, or the power station with an extra battery bundled in for $2,699 off, down to $3,599. Black Friday FAQs When is Black Friday 2025? Black Friday 2025 lands on November 28. Which stores have Black Friday deals? Many physical retail stores have Black Friday deals including Walmart, Target, Best Buy and others. Even more retailers have online Black Friday deals, including Amazon, GameStop, Costco and others. When do Black Friday sales start? Gone are the times when Black Friday sales were one-day-only affairs. Now, Black Friday deals are often available starting on Thanksgiving, or even earlier. Last year, we saw Black Friday deals online begin the week before Black Friday proper. When do Black Friday sales end? Black Friday and Cyber Monday have blended a lot over the past few years. Now, you can expect to see a good portion of Black Friday deals extend through the weekend and into Cyber Monday. It's not uncommon for Black Friday deals to expire at the end of Cyber Monday. Which retailers have the best Black Friday tech deals? The best Black Friday tech deals are typically available online at retailers like Amazon, Walmart, Best Buy and Target. It's also a good idea to check the store websites of the companies that make the products you want — for example, if you're looking for a Sonos speaker, check the Sonos website on Black Friday. Most of the time, you'll find the best Black Friday tech deals are matched at multiple retailers. Does Apple have Black Friday sales? No, you will usually not find Black Friday sales at Apple stores or on Apple's website. However, you can find Black Friday deals on Apple devices elsewhere; we recommend checking Amazon, Best Buy and other big retailers for discounts on iPads, Apple Watches and more on Black Friday. Does Amazon have Black Friday sales? Yes, Amazon has Black Friday sales. The online retailer's site will look similar to Prime Day on Black Friday, with discounts on all sorts of items from household essentials to fashion to tech.This article originally appeared on Engadget at https://www.engadget.com/deals/black-friday-deals-for-2025-have-arrived-the-best-tech-sales-from-amazon-apple-lego-anker-and-others-so-far-100052657.html?src=rss",
          "feed_position": 15
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/nintendo-announces-its-black-friday-and-cyber-monday-2025-sale-switch-2-bundles-switch-game-deals-accessories-and-more-155223057.html",
          "published_at": "Thu, 13 Nov 2025 16:33:24 +0000",
          "title": "Nintendo announces its Black Friday and Cyber Monday 2025 sale: Switch 2 bundles, Switch game deals, accessories and more",
          "standfirst": "When it comes to holiday video game gifts, Nintendo gear is often at the top of the list for kids and adults like. This year likely more so than ever thanks to the launch of the Switch 2 back in the spring. But fi you were hoping to save money on the console with Black Friday deals, you may be disappointed. The Nintendo Black Friday sale was just announced, and unsurprisingly, there are a scant few real \"deals\" to be had. This is typical of Nintendo, though — actual Nintendo Black Friday deals are few and far between. However, there are ways to at least get the best value for your money if you're going to pick up a Switch 2 before this year is over. As has been the case for many years, the marquee Nintendo deals for the holidays come in the form of console bundles. When the Switch 2 launched earlier this year, it was available as just the console only for $449 or bundled with Mario Kart World for $499. Both options are still available now, but there's a new bundle to consider as well — the console with the new Pokémon Legends: Z-A game, which also costs $499. Considering the games by themselves cost $70 each, you do save a bit by picking up a console bundle. you can pick up the console and its bundles at most retailers including Amazon, Walmart, Best Buy and others. When it comes to deals on Nintendo Switch 2 games, the Nintendo eShop will have Cyber Deals starting on November 20, running through December 3. The shop will feature \"holiday offers on select games,\" so it appears we'll all just have to go to the online store on November 20 to see the games on offer. Starting on November 23, select retailers will have discounts on some physical Switch games including Princess Peach: Showtime!, The Legend of Zelda: Echoes of Wisdom, Luigi’s Mansion 3 and Kirby’s Return to Dream Land Deluxe. Those will each be $40, while other games like Super Mario Odyssey, Nintendo Switch Sports, Paper Mario: The Thousand-Year Door and Splatoon 3 will be $30. Even if you can't get huge discounts on Nintendo consoles or new games this year, that doesn't mean you can't find decent deals on other Nintendo gear. There are plenty of great ideas for gifts for the Nintendo fan in your life, and Engadget's Sam Rutherford got to see a bunch of them in person when he attended Nintendo's holiday showcase. From collectibles to clothing to plushies and holiday decor, there's really a ton to choose from — but you may want to pace yourself if you're also a Nintendo fan finding things that you want to pick up for yourself in the process of looking for good gifts. Here are just some of the best Nintendo gift ideas that you can look out for during Black Friday and Cyber Monday. This article originally appeared on Engadget at https://www.engadget.com/deals/nintendo-announces-its-black-friday-and-cyber-monday-2025-sale-switch-2-bundles-switch-game-deals-accessories-and-more-155223057.html?src=rss",
          "content": "When it comes to holiday video game gifts, Nintendo gear is often at the top of the list for kids and adults like. This year likely more so than ever thanks to the launch of the Switch 2 back in the spring. But fi you were hoping to save money on the console with Black Friday deals, you may be disappointed. The Nintendo Black Friday sale was just announced, and unsurprisingly, there are a scant few real \"deals\" to be had. This is typical of Nintendo, though — actual Nintendo Black Friday deals are few and far between. However, there are ways to at least get the best value for your money if you're going to pick up a Switch 2 before this year is over. As has been the case for many years, the marquee Nintendo deals for the holidays come in the form of console bundles. When the Switch 2 launched earlier this year, it was available as just the console only for $449 or bundled with Mario Kart World for $499. Both options are still available now, but there's a new bundle to consider as well — the console with the new Pokémon Legends: Z-A game, which also costs $499. Considering the games by themselves cost $70 each, you do save a bit by picking up a console bundle. you can pick up the console and its bundles at most retailers including Amazon, Walmart, Best Buy and others. When it comes to deals on Nintendo Switch 2 games, the Nintendo eShop will have Cyber Deals starting on November 20, running through December 3. The shop will feature \"holiday offers on select games,\" so it appears we'll all just have to go to the online store on November 20 to see the games on offer. Starting on November 23, select retailers will have discounts on some physical Switch games including Princess Peach: Showtime!, The Legend of Zelda: Echoes of Wisdom, Luigi’s Mansion 3 and Kirby’s Return to Dream Land Deluxe. Those will each be $40, while other games like Super Mario Odyssey, Nintendo Switch Sports, Paper Mario: The Thousand-Year Door and Splatoon 3 will be $30. Even if you can't get huge discounts on Nintendo consoles or new games this year, that doesn't mean you can't find decent deals on other Nintendo gear. There are plenty of great ideas for gifts for the Nintendo fan in your life, and Engadget's Sam Rutherford got to see a bunch of them in person when he attended Nintendo's holiday showcase. From collectibles to clothing to plushies and holiday decor, there's really a ton to choose from — but you may want to pace yourself if you're also a Nintendo fan finding things that you want to pick up for yourself in the process of looking for good gifts. Here are just some of the best Nintendo gift ideas that you can look out for during Black Friday and Cyber Monday. This article originally appeared on Engadget at https://www.engadget.com/deals/nintendo-announces-its-black-friday-and-cyber-monday-2025-sale-switch-2-bundles-switch-game-deals-accessories-and-more-155223057.html?src=rss",
          "feed_position": 16
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/gaming/pc/dbrands-companion-cube-is-the-perfect-partner-for-your-future-steam-machine-161634417.html",
          "published_at": "Thu, 13 Nov 2025 16:16:35 +0000",
          "title": "Dbrand’s Companion Cube is the perfect partner for your future Steam Machine",
          "standfirst": "Valve’s second attempt at a console-like gaming PC for your living room is hopefully going to be a triumph worthy of plenty of cake, with or without fun accessories. But if you really want to make the Steam Machine feel extra special when it arrives in early 2026, you’re going to want to dress it up in Dbrand’s Companion Cube skin. A tribute to the iconic Weighted Companion Cube from Valve’s beloved Portal series, it’s such a fitting design that I sort of wonder if the company is kicking itself for not making something similar for its cube-shaped compact PC. Of course, putting your Steam Machine inside this will ensure it stands out from the other consoles occupying your TV unit, which is the opposite effect that the standard design is going to have. While it does have a customizable front plate and an LED light strip, the default all-black colorway could hardly be less attention-grabbing. In case you missed yesterday's announcement, the Steam Machine is a Linux-based mini PC that runs SteamOS and is designed to be plugged into your TV, like a console. Valve says it’s roughly six times more powerful than a Steam Deck and is capable of supporting 4K/60fps gaming with FSR. You can also use it to stream VR games to the new Stream Frame headset, while the Steam Controller, with its distinctive trackpads, allows you to play your Steam games wirelessly. Given that we don’t yet have a release date or price for the Steam Machine, Dbrand’s accessory doesn't have either of those yet either, but it is coming in 2026. And I’m fairly confident that isn’t a lie…This article originally appeared on Engadget at https://www.engadget.com/gaming/pc/dbrands-companion-cube-is-the-perfect-partner-for-your-future-steam-machine-161634417.html?src=rss",
          "content": "Valve’s second attempt at a console-like gaming PC for your living room is hopefully going to be a triumph worthy of plenty of cake, with or without fun accessories. But if you really want to make the Steam Machine feel extra special when it arrives in early 2026, you’re going to want to dress it up in Dbrand’s Companion Cube skin. A tribute to the iconic Weighted Companion Cube from Valve’s beloved Portal series, it’s such a fitting design that I sort of wonder if the company is kicking itself for not making something similar for its cube-shaped compact PC. Of course, putting your Steam Machine inside this will ensure it stands out from the other consoles occupying your TV unit, which is the opposite effect that the standard design is going to have. While it does have a customizable front plate and an LED light strip, the default all-black colorway could hardly be less attention-grabbing. In case you missed yesterday's announcement, the Steam Machine is a Linux-based mini PC that runs SteamOS and is designed to be plugged into your TV, like a console. Valve says it’s roughly six times more powerful than a Steam Deck and is capable of supporting 4K/60fps gaming with FSR. You can also use it to stream VR games to the new Stream Frame headset, while the Steam Controller, with its distinctive trackpads, allows you to play your Steam games wirelessly. Given that we don’t yet have a release date or price for the Steam Machine, Dbrand’s accessory doesn't have either of those yet either, but it is coming in 2026. And I’m fairly confident that isn’t a lie…This article originally appeared on Engadget at https://www.engadget.com/gaming/pc/dbrands-companion-cube-is-the-perfect-partner-for-your-future-steam-machine-161634417.html?src=rss",
          "feed_position": 17
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/inside-linkedins-generative-ai-cookbook-how-it-scaled-people-search-to-1-3",
          "published_at": "Thu, 13 Nov 2025 16:00:00 GMT",
          "title": "Inside LinkedIn’s generative AI cookbook: How it scaled people search to 1.3 billion users",
          "standfirst": "LinkedIn is launching its new AI-powered people search this week, after what seems like a very long wait for what should have been a natural offering for generative AI.It comes a full three years after the launch of ChatGPT and six months after LinkedIn launched its AI job search offering. For technical leaders, this timeline illustrates a key enterprise lesson: Deploying generative AI in real enterprise settings is challenging, especially at a scale of 1.3 billion users. It’s a slow, brutal process of pragmatic optimization.The following account is based on several exclusive interviews with the LinkedIn product and engineering team behind the launch.First, here’s how the product works: A user can now type a natural language query like, \"Who is knowledgeable about curing cancer?\" into LinkedIn’s search bar.LinkedIn&#x27;s old search, based on keywords, would have been stumped. It would have looked only for references to \"cancer\". If a user wanted to get sophisticated, they would have had to run separate, rigid keyword searches for \"cancer\" and then \"oncology\" and manually try to piece the results together.The new AI-powered system, however, understands the intent of the search because the LLM under the hood grasps semantic meaning. It recognizes, for example, that \"cancer\" is conceptually related to \"oncology\" and even less directly, to \"genomics research.\" As a result, it surfaces a far more relevant list of people, including oncology leaders and researchers, even if their profiles don&#x27;t use the exact word \"cancer.\"The system also balances this relevance with usefulness. Instead of just showing the world&#x27;s top oncologist (who might be an unreachable third-degree connection), it will also weigh who in your immediate network — like a first-degree connection — is \"pretty relevant\" and can serve as a crucial bridge to that expert.See the video below for an example.Arguably, though, the more important lesson for enterprise practitioners is the \"cookbook\" LinkedIn has developed: a replicable, multi-stage pipeline of distillation, co-design, and relentless optimization. LinkedIn had to perfect this on one product before attempting it on another.\"Don&#x27;t try to do too much all at once,\" writes Wenjing Zhang, LinkedIn&#x27;s VP of Engineering, in a post about the product launch, and who also spoke with VentureBeat last week in an interview. She notes that an earlier \"sprawling ambition\" to build a unified system for all of LinkedIn&#x27;s products \"stalled progress.\"Instead, LinkedIn focused on winning one vertical first. The success of its previously launched AI Job Search — which led to job seekers without a four-year degree being 10% more likely to get hired, according to VP of Product Engineering Erran Berger — provided the blueprint.Now, the company is applying that blueprint to a far larger challenge. \"It&#x27;s one thing to be able to do this across tens of millions of jobs,\" Berger told VentureBeat. \"It&#x27;s another thing to do this across north of a billion members.\"For enterprise AI builders, LinkedIn&#x27;s journey provides a technical playbook for what it actually takes to move from a successful pilot to a billion-user-scale product.The new challenge: a 1.3 billion-member graphThe job search product created a robust recipe that the new people search product could build upon, Berger explained. The recipe started with with a \"golden data set\" of just a few hundred to a thousand real query-profile pairs, meticulously scored against a detailed 20- to 30-page \"product policy\" document. To scale this for training, LinkedIn used this small golden set to prompt a large foundation model to generate a massive volume of synthetic training data. This synthetic data was used to train a 7-billion-parameter \"Product Policy\" model — a high-fidelity judge of relevance that was too slow for live production but perfect for teaching smaller models.However, the team hit a wall early on. For six to nine months, they struggled to train a single model that could balance strict policy adherence (relevance) against user engagement signals. The \"aha moment\" came when they realized they needed to break the problem down. They distilled the 7B policy model into a 1.7B teacher model focused solely on relevance. They then paired it with separate teacher models trained to predict specific member actions, such as job applications for the jobs product, or connecting and following for people search. This \"multi-teacher\" ensemble produced soft probability scores that the final student model learned to mimic via KL divergence loss.The resulting architecture operates as a two-stage pipeline. First, a larger 8B parameter model handles broad retrieval, casting a wide net to pull candidates from the graph. Then, the highly distilled student model takes over for fine-grained ranking. While the job search product successfully deployed a 0.6B (600-million) parameter student, the new people search product required even more aggressive compression. As Zhang notes, the team pruned their new student model from 440M down to just 220M parameters, achieving the necessary speed for 1.3 billion users with less than 1% relevance loss.But applying this to people search broke the old architecture. The new problem included not just ranking but also retrieval.“A billion records,\" Berger said, is a \"different beast.\"The team’s prior retrieval stack was built on CPUs. To handle the new scale and the latency demands of a \"snappy\" search experience, the team had to move its indexing to GPU-based infrastructure. This was a foundational architectural shift that the job search product did not require.Organizationally, LinkedIn benefited from multiple approaches. For a time, LinkedIn had two separate teams — job search and people search — attempting to solve the problem in parallel. But once the job search team achieved its breakthrough using the policy-driven distillation method, Berger and his leadership team intervened. They brought over the architects of the job search win — product lead Rohan Rajiv and engineering lead Wenjing Zhang — to transplant their &#x27;cookbook&#x27; directly to the new domain.Distilling for a 10x throughput gainWith the retrieval problem solved, the team faced the ranking and efficiency challenge. This is where the cookbook was adapted with new, aggressive optimization techniques.Zhang’s technical post (I’ll insert the link once it goes live) provides the specific details our audience of AI engineers will appreciate. One of the more significant optimizations was input size.To feed the model, the team trained another LLM with reinforcement learning (RL) for a single purpose: to summarize the input context. This \"summarizer\" model was able to reduce the model&#x27;s input size by 20-fold with minimal information loss.The combined result of the 220M-parameter model and the 20x input reduction? A 10x increase in ranking throughput, allowing the team to serve the model efficiently to its massive user base.Pragmatism over hype: building tools, not agentsThroughout our discussions, Berger was adamant about something else that might catch peoples’ attention: The real value for enterprises today lies in perfecting recommender systems, not in chasing \"agentic hype.\" He also refused to talk about the specific models that the company used for the searches, suggesting it almost doesn&#x27;t matter. The company selects models based on which one it finds the most efficient for the task.The new AI-powered people search is a manifestation of Berger’s philosophy that it’s best to optimize the recommender system first. The architecture includes a new \"intelligent query routing layer,\" as Berger explained, that itself is LLM-powered. This router pragmatically decides if a user&#x27;s query — like \"trust expert\" — should go to the new semantic, natural-language stack or to the old, reliable lexical search.This entire, complex system is designed to be a \"tool\" that a future agent will use, not the agent itself.\"Agentic products are only as good as the tools that they use to accomplish tasks for people,\" Berger said. \"You can have the world&#x27;s best reasoning model, and if you&#x27;re trying to use an agent to do people search but the people search engine is not very good, you&#x27;re not going to be able to deliver.\" Now that the people search is available, Berger suggested that one day the company will be offering agents to use it. But he didn’t provide details on timing. He also said the recipe used for job and people search will be spread across the company’s other products.For enterprises building their own AI roadmaps, LinkedIn&#x27;s playbook is clear:Be pragmatic: Don&#x27;t try to boil the ocean. Win one vertical, even if it takes 18 months.Codify the \"cookbook\": Turn that win into a repeatable process (policy docs, distillation pipelines, co-design).Optimize relentlessly: The real 10x gains come after the initial model, in pruning, distillation, and creative optimizations like an RL-trained summarizer.LinkedIn&#x27;s journey shows that for real-world enterprise AI, emphasis on specific models or cool agentic systems should take a back seat. The durable, strategic advantage comes from mastering the pipeline — the &#x27;AI-native&#x27; cookbook of co-design, distillation, and ruthless optimization.(Editor&#x27;s note: We will be publishing a full-length podcast with LinkedIn&#x27;s Erran Berger, which will dive deeper into these technical details, on the VentureBeat podcast feed soon.)",
          "content": "LinkedIn is launching its new AI-powered people search this week, after what seems like a very long wait for what should have been a natural offering for generative AI.It comes a full three years after the launch of ChatGPT and six months after LinkedIn launched its AI job search offering. For technical leaders, this timeline illustrates a key enterprise lesson: Deploying generative AI in real enterprise settings is challenging, especially at a scale of 1.3 billion users. It’s a slow, brutal process of pragmatic optimization.The following account is based on several exclusive interviews with the LinkedIn product and engineering team behind the launch.First, here’s how the product works: A user can now type a natural language query like, \"Who is knowledgeable about curing cancer?\" into LinkedIn’s search bar.LinkedIn&#x27;s old search, based on keywords, would have been stumped. It would have looked only for references to \"cancer\". If a user wanted to get sophisticated, they would have had to run separate, rigid keyword searches for \"cancer\" and then \"oncology\" and manually try to piece the results together.The new AI-powered system, however, understands the intent of the search because the LLM under the hood grasps semantic meaning. It recognizes, for example, that \"cancer\" is conceptually related to \"oncology\" and even less directly, to \"genomics research.\" As a result, it surfaces a far more relevant list of people, including oncology leaders and researchers, even if their profiles don&#x27;t use the exact word \"cancer.\"The system also balances this relevance with usefulness. Instead of just showing the world&#x27;s top oncologist (who might be an unreachable third-degree connection), it will also weigh who in your immediate network — like a first-degree connection — is \"pretty relevant\" and can serve as a crucial bridge to that expert.See the video below for an example.Arguably, though, the more important lesson for enterprise practitioners is the \"cookbook\" LinkedIn has developed: a replicable, multi-stage pipeline of distillation, co-design, and relentless optimization. LinkedIn had to perfect this on one product before attempting it on another.\"Don&#x27;t try to do too much all at once,\" writes Wenjing Zhang, LinkedIn&#x27;s VP of Engineering, in a post about the product launch, and who also spoke with VentureBeat last week in an interview. She notes that an earlier \"sprawling ambition\" to build a unified system for all of LinkedIn&#x27;s products \"stalled progress.\"Instead, LinkedIn focused on winning one vertical first. The success of its previously launched AI Job Search — which led to job seekers without a four-year degree being 10% more likely to get hired, according to VP of Product Engineering Erran Berger — provided the blueprint.Now, the company is applying that blueprint to a far larger challenge. \"It&#x27;s one thing to be able to do this across tens of millions of jobs,\" Berger told VentureBeat. \"It&#x27;s another thing to do this across north of a billion members.\"For enterprise AI builders, LinkedIn&#x27;s journey provides a technical playbook for what it actually takes to move from a successful pilot to a billion-user-scale product.The new challenge: a 1.3 billion-member graphThe job search product created a robust recipe that the new people search product could build upon, Berger explained. The recipe started with with a \"golden data set\" of just a few hundred to a thousand real query-profile pairs, meticulously scored against a detailed 20- to 30-page \"product policy\" document. To scale this for training, LinkedIn used this small golden set to prompt a large foundation model to generate a massive volume of synthetic training data. This synthetic data was used to train a 7-billion-parameter \"Product Policy\" model — a high-fidelity judge of relevance that was too slow for live production but perfect for teaching smaller models.However, the team hit a wall early on. For six to nine months, they struggled to train a single model that could balance strict policy adherence (relevance) against user engagement signals. The \"aha moment\" came when they realized they needed to break the problem down. They distilled the 7B policy model into a 1.7B teacher model focused solely on relevance. They then paired it with separate teacher models trained to predict specific member actions, such as job applications for the jobs product, or connecting and following for people search. This \"multi-teacher\" ensemble produced soft probability scores that the final student model learned to mimic via KL divergence loss.The resulting architecture operates as a two-stage pipeline. First, a larger 8B parameter model handles broad retrieval, casting a wide net to pull candidates from the graph. Then, the highly distilled student model takes over for fine-grained ranking. While the job search product successfully deployed a 0.6B (600-million) parameter student, the new people search product required even more aggressive compression. As Zhang notes, the team pruned their new student model from 440M down to just 220M parameters, achieving the necessary speed for 1.3 billion users with less than 1% relevance loss.But applying this to people search broke the old architecture. The new problem included not just ranking but also retrieval.“A billion records,\" Berger said, is a \"different beast.\"The team’s prior retrieval stack was built on CPUs. To handle the new scale and the latency demands of a \"snappy\" search experience, the team had to move its indexing to GPU-based infrastructure. This was a foundational architectural shift that the job search product did not require.Organizationally, LinkedIn benefited from multiple approaches. For a time, LinkedIn had two separate teams — job search and people search — attempting to solve the problem in parallel. But once the job search team achieved its breakthrough using the policy-driven distillation method, Berger and his leadership team intervened. They brought over the architects of the job search win — product lead Rohan Rajiv and engineering lead Wenjing Zhang — to transplant their &#x27;cookbook&#x27; directly to the new domain.Distilling for a 10x throughput gainWith the retrieval problem solved, the team faced the ranking and efficiency challenge. This is where the cookbook was adapted with new, aggressive optimization techniques.Zhang’s technical post (I’ll insert the link once it goes live) provides the specific details our audience of AI engineers will appreciate. One of the more significant optimizations was input size.To feed the model, the team trained another LLM with reinforcement learning (RL) for a single purpose: to summarize the input context. This \"summarizer\" model was able to reduce the model&#x27;s input size by 20-fold with minimal information loss.The combined result of the 220M-parameter model and the 20x input reduction? A 10x increase in ranking throughput, allowing the team to serve the model efficiently to its massive user base.Pragmatism over hype: building tools, not agentsThroughout our discussions, Berger was adamant about something else that might catch peoples’ attention: The real value for enterprises today lies in perfecting recommender systems, not in chasing \"agentic hype.\" He also refused to talk about the specific models that the company used for the searches, suggesting it almost doesn&#x27;t matter. The company selects models based on which one it finds the most efficient for the task.The new AI-powered people search is a manifestation of Berger’s philosophy that it’s best to optimize the recommender system first. The architecture includes a new \"intelligent query routing layer,\" as Berger explained, that itself is LLM-powered. This router pragmatically decides if a user&#x27;s query — like \"trust expert\" — should go to the new semantic, natural-language stack or to the old, reliable lexical search.This entire, complex system is designed to be a \"tool\" that a future agent will use, not the agent itself.\"Agentic products are only as good as the tools that they use to accomplish tasks for people,\" Berger said. \"You can have the world&#x27;s best reasoning model, and if you&#x27;re trying to use an agent to do people search but the people search engine is not very good, you&#x27;re not going to be able to deliver.\" Now that the people search is available, Berger suggested that one day the company will be offering agents to use it. But he didn’t provide details on timing. He also said the recipe used for job and people search will be spread across the company’s other products.For enterprises building their own AI roadmaps, LinkedIn&#x27;s playbook is clear:Be pragmatic: Don&#x27;t try to boil the ocean. Win one vertical, even if it takes 18 months.Codify the \"cookbook\": Turn that win into a repeatable process (policy docs, distillation pipelines, co-design).Optimize relentlessly: The real 10x gains come after the initial model, in pruning, distillation, and creative optimizations like an RL-trained summarizer.LinkedIn&#x27;s journey shows that for real-world enterprise AI, emphasis on specific models or cool agentic systems should take a back seat. The durable, strategic advantage comes from mastering the pipeline — the &#x27;AI-native&#x27; cookbook of co-design, distillation, and ruthless optimization.(Editor&#x27;s note: We will be publishing a full-length podcast with LinkedIn&#x27;s Erran Berger, which will dive deeper into these technical details, on the VentureBeat podcast feed soon.)",
          "feed_position": 1,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/lgGMZPiCCaW1DNedEGAbm/a05d0e3b39eea0fae4e58895fad8d198/Screenshot_2025-11-12_at_4.53.46%C3%A2__PM.png?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/gaming/playstation/sonys-latest-horizon-spin-off-is-an-mmorpg-for-pc-and-mobile-but-not-ps5-153532860.html",
          "published_at": "Thu, 13 Nov 2025 15:35:32 +0000",
          "title": "Sony’s latest Horizon spin-off is an MMORPG for PC and mobile, but not PS5",
          "standfirst": "An MMO based on Sony's Horizon series is on the way. However, Horizon Steel Frontiers is not coming to PS5, at least not initially. It's a mobile-first game that's also coming to PC, in another example of Sony Interactive Entertainment expanding beyond its core PlayStation console business.NCSoft, the MMO developer and publisher behind the likes of the Guild Wars series and Throne and Liberty, is taking the lead on Horizon Steel Frontiers. It's working on the game alongside original Horizon developer Guerrilla Games and Sony.Horizon Steel Frontiers is said to build on the fun robot dinosaur hunting action of Horizon Zero Dawn and Horizon Forbidden West with greater player freedom, \"deeply customizable combat\" and other advanced MMORPG systems. You'll be able to undertake \"large-scale raids\" with other players. You'll likely end up competing with other players for resources too.You'll create your own character, who belongs to a tribe of your choosing. Just like in the main games, status effects are a key component of combat. You'll also be able to grapple onto giant machines, chip parts of them off and use weapons that robot enemies drop against them. In a neat touch, you can carry these weapons on your mount and use them in your next fight. Horizon Steel Frontiers has Tallnecks too, so I'm happy about that.The action is set in a region called the Deadlands, which is inspired by New Mexico and Arizona, and you'll share this part of the Horizon world with \"thousands of other players,\" according to Guerrilla studio director Jan-Bart Van Beek. As with the series' core games, the story here concerns finding a balance between humanity, technology and nature.Guerrilla said back in 2022 that it was working on more “epic solo adventures for Aloy” (the protagonist of the mainline entries) and it's said to be making its own Horizon multiplayer game. This MMO isn't the first Horizon spin-off either. Lego Horizon Adventures, from Guerilla Games and Studio Gobo, arrived a year ago. Co-op is a key aspect of that game, so that's a multiplayer title too.Handing development of Horizon Steel Frontiers to a studio with vast experience in the MMO genre is a smart move on Sony's part. The company pivoted a few years back to focus heavily on live-service games, but that strategy hasn't panned out so well. Sure, Helldivers 2 has been a major hit, but Concord was an utter disaster. An attempt to make a multiplayer game in the world of The Last of Us didn't work out. Sony's acquisition of Bungie hasn't gone as smoothly as expected either. The company said this week it wrote down the value of Bungie's assets by $204 million amid Destiny 2's struggles, putting even more pressure on the upcoming Marathon to succeed.Sony and NCSoft have not yet revealed a release date for Horizon Steel Frontiers. In any case, Horizon is one of Sony's most popular franchises and it should make for strong fodder for an MMO. The action in the gameplay trailer looks as slick as you'd expect from this series and taking down robot dinos with friends in Monster Hunter-style action could be a lot of fun. It's probably a good thing that you're not playing as Aloy here, given how annoyingly reluctant she is to accept help from would-be allies in her own games.This article originally appeared on Engadget at https://www.engadget.com/gaming/playstation/sonys-latest-horizon-spin-off-is-an-mmorpg-for-pc-and-mobile-but-not-ps5-153532860.html?src=rss",
          "content": "An MMO based on Sony's Horizon series is on the way. However, Horizon Steel Frontiers is not coming to PS5, at least not initially. It's a mobile-first game that's also coming to PC, in another example of Sony Interactive Entertainment expanding beyond its core PlayStation console business.NCSoft, the MMO developer and publisher behind the likes of the Guild Wars series and Throne and Liberty, is taking the lead on Horizon Steel Frontiers. It's working on the game alongside original Horizon developer Guerrilla Games and Sony.Horizon Steel Frontiers is said to build on the fun robot dinosaur hunting action of Horizon Zero Dawn and Horizon Forbidden West with greater player freedom, \"deeply customizable combat\" and other advanced MMORPG systems. You'll be able to undertake \"large-scale raids\" with other players. You'll likely end up competing with other players for resources too.You'll create your own character, who belongs to a tribe of your choosing. Just like in the main games, status effects are a key component of combat. You'll also be able to grapple onto giant machines, chip parts of them off and use weapons that robot enemies drop against them. In a neat touch, you can carry these weapons on your mount and use them in your next fight. Horizon Steel Frontiers has Tallnecks too, so I'm happy about that.The action is set in a region called the Deadlands, which is inspired by New Mexico and Arizona, and you'll share this part of the Horizon world with \"thousands of other players,\" according to Guerrilla studio director Jan-Bart Van Beek. As with the series' core games, the story here concerns finding a balance between humanity, technology and nature.Guerrilla said back in 2022 that it was working on more “epic solo adventures for Aloy” (the protagonist of the mainline entries) and it's said to be making its own Horizon multiplayer game. This MMO isn't the first Horizon spin-off either. Lego Horizon Adventures, from Guerilla Games and Studio Gobo, arrived a year ago. Co-op is a key aspect of that game, so that's a multiplayer title too.Handing development of Horizon Steel Frontiers to a studio with vast experience in the MMO genre is a smart move on Sony's part. The company pivoted a few years back to focus heavily on live-service games, but that strategy hasn't panned out so well. Sure, Helldivers 2 has been a major hit, but Concord was an utter disaster. An attempt to make a multiplayer game in the world of The Last of Us didn't work out. Sony's acquisition of Bungie hasn't gone as smoothly as expected either. The company said this week it wrote down the value of Bungie's assets by $204 million amid Destiny 2's struggles, putting even more pressure on the upcoming Marathon to succeed.Sony and NCSoft have not yet revealed a release date for Horizon Steel Frontiers. In any case, Horizon is one of Sony's most popular franchises and it should make for strong fodder for an MMO. The action in the gameplay trailer looks as slick as you'd expect from this series and taking down robot dinos with friends in Monster Hunter-style action could be a lot of fun. It's probably a good thing that you're not playing as Aloy here, given how annoyingly reluctant she is to accept help from would-be allies in her own games.This article originally appeared on Engadget at https://www.engadget.com/gaming/playstation/sonys-latest-horizon-spin-off-is-an-mmorpg-for-pc-and-mobile-but-not-ps5-153532860.html?src=rss",
          "feed_position": 19
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/ar-vr/valve-confirms-that-it-has-stopped-making-the-index-vr-headset-150324456.html",
          "published_at": "Thu, 13 Nov 2025 15:03:24 +0000",
          "title": "Valve confirms that it has stopped making the Index VR headset",
          "standfirst": "In case you missed it, Valve announced a load of new hardware this week, including a second stab at the Steam Machine, a Steam controller, and a long-rumored new VR headset called the Steam Frame. But in with the new often means out with the old, and perhaps inevitably, the company has confirmed that its previous headset, the Valve Index, is no more. Valve’s Lawrence Yang told The Verge that it’s \"no longer manufacturing\" the Index, which we called \"the best desktop VR yet\" when it launched in 2019. The Index arrived around the same time as the Oculus Quest and its promise of an affordable all-in-one future for VR. By contrast, the Valve Index was very much still a high-end tethered device for the hardcore enthusiasts, with a price tag that reflected that. We don’t know how much the Steam Frame will cost yet, but it definitely sounds like Valve is making a play for the more casual VR crowd here too, as well as those who want to play demanding 3D titles. You can stream flatscreen and VR games from your PC or Steam Machine using a wireless adapter, but the Steam Frame is also a standalone device like the Meta Quest 3, backed by a built-in Snapdragon 8 Gen 3 chipset and 16GB of RAM. Valve is supporting Android games too, seemingly a move to entice VR developers to bring their Quest games over to Steam. In order to track your movements in virtual space, the Valve Index relied on external lighthouse base stations, which meant you had to go through a more than a little laborious setup process to play roomscale VR games. Consumer VR has moved towards built-in sensors since then, and it sounds like Valve wants to leave its lighthouses in the past too, with the company confirming to The Verge that they won’t be supported on the Steam Frame. The new headset instead has four high-res monochrome cameras for inside-out tracking, as well as infrared LEDs on the outside that help with tracking in darker environments.This article originally appeared on Engadget at https://www.engadget.com/ar-vr/valve-confirms-that-it-has-stopped-making-the-index-vr-headset-150324456.html?src=rss",
          "content": "In case you missed it, Valve announced a load of new hardware this week, including a second stab at the Steam Machine, a Steam controller, and a long-rumored new VR headset called the Steam Frame. But in with the new often means out with the old, and perhaps inevitably, the company has confirmed that its previous headset, the Valve Index, is no more. Valve’s Lawrence Yang told The Verge that it’s \"no longer manufacturing\" the Index, which we called \"the best desktop VR yet\" when it launched in 2019. The Index arrived around the same time as the Oculus Quest and its promise of an affordable all-in-one future for VR. By contrast, the Valve Index was very much still a high-end tethered device for the hardcore enthusiasts, with a price tag that reflected that. We don’t know how much the Steam Frame will cost yet, but it definitely sounds like Valve is making a play for the more casual VR crowd here too, as well as those who want to play demanding 3D titles. You can stream flatscreen and VR games from your PC or Steam Machine using a wireless adapter, but the Steam Frame is also a standalone device like the Meta Quest 3, backed by a built-in Snapdragon 8 Gen 3 chipset and 16GB of RAM. Valve is supporting Android games too, seemingly a move to entice VR developers to bring their Quest games over to Steam. In order to track your movements in virtual space, the Valve Index relied on external lighthouse base stations, which meant you had to go through a more than a little laborious setup process to play roomscale VR games. Consumer VR has moved towards built-in sensors since then, and it sounds like Valve wants to leave its lighthouses in the past too, with the company confirming to The Verge that they won’t be supported on the Steam Frame. The new headset instead has four high-res monochrome cameras for inside-out tracking, as well as infrared LEDs on the outside that help with tracking in darker environments.This article originally appeared on Engadget at https://www.engadget.com/ar-vr/valve-confirms-that-it-has-stopped-making-the-index-vr-headset-150324456.html?src=rss",
          "feed_position": 20
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/sandisks-switch-2-compatible-microsd-express-card-is-cheaper-than-ever-ahead-of-black-friday-151331109.html",
          "published_at": "Thu, 13 Nov 2025 15:01:25 +0000",
          "title": "SanDisk's Switch 2-compatible microSD Express card is cheaper than ever ahead of Black Friday",
          "standfirst": "If you already picked up a Switch 2, you're probably looking for deals that can help you kit out your new console without spending too much money. While maybe not the most exciting thing, a microSD Express card will be a key component for long-term gaming bliss. SanDisk's 256GB microSD Express Card for the Switch 2 is on sale for the first time ahead of Black Friday, down to $60 right now. This model is also available in storage sizes of 128GB and 512GB, but at the moment, only the 256GB model has a discount. This particular model easily made our list of the best microSD cards for the Nintendo Switch 2. It even made our list of the best Switch 2 accessories. It just gets the job done. We loved the speed on offer here. It was the fastest of all the cards we tested when transferring games and loading games. We also found that it performed admirably at just about every test we threw at it. The card was always consistently right near the top, thanks to outstanding sequential read and write performance. This was backed up by benchmark testing with PC tools like CrystalDiskMark. The Switch 2 only works with SD Express cards, so this covers that. Luckily, this card isn't just for Nintendo's latest console. It'll work with just about everything, if you ever find it outstays its usefulness as a storage container for Mario and friends. Elsewhere when it comes to microSD Express cards on sale: PNY's 128GB card is down to $40. This article originally appeared on Engadget at https://www.engadget.com/deals/sandisks-switch-2-compatible-microsd-express-card-is-cheaper-than-ever-ahead-of-black-friday-151331109.html?src=rss",
          "content": "If you already picked up a Switch 2, you're probably looking for deals that can help you kit out your new console without spending too much money. While maybe not the most exciting thing, a microSD Express card will be a key component for long-term gaming bliss. SanDisk's 256GB microSD Express Card for the Switch 2 is on sale for the first time ahead of Black Friday, down to $60 right now. This model is also available in storage sizes of 128GB and 512GB, but at the moment, only the 256GB model has a discount. This particular model easily made our list of the best microSD cards for the Nintendo Switch 2. It even made our list of the best Switch 2 accessories. It just gets the job done. We loved the speed on offer here. It was the fastest of all the cards we tested when transferring games and loading games. We also found that it performed admirably at just about every test we threw at it. The card was always consistently right near the top, thanks to outstanding sequential read and write performance. This was backed up by benchmark testing with PC tools like CrystalDiskMark. The Switch 2 only works with SD Express cards, so this covers that. Luckily, this card isn't just for Nintendo's latest console. It'll work with just about everything, if you ever find it outstays its usefulness as a storage container for Mario and friends. Elsewhere when it comes to microSD Express cards on sale: PNY's 128GB card is down to $40. This article originally appeared on Engadget at https://www.engadget.com/deals/sandisks-switch-2-compatible-microsd-express-card-is-cheaper-than-ever-ahead-of-black-friday-151331109.html?src=rss",
          "feed_position": 21
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/8bitdos-ultimate-controller-is-on-sale-for-only-43-in-this-black-friday-deal-133023111.html",
          "published_at": "Thu, 13 Nov 2025 14:46:26 +0000",
          "title": "8Bitdo's Ultimate Controller is on sale for only $43 in this Black Friday deal",
          "standfirst": "Early Black Friday deals are starting to flood the internet. If you're starting to think about the gifts you have to pick up for everyone in your life, you may be able to save already on some of them — including tech. One of the standout deals we've found so far is on the 8Bitdo Ultimate Bluetooth Controller and Charging Dock. It's 39 percent off right now and down to $43 — a new record-low price. The controller comes with perks such as the charging dock, 22 hours of battery per charge and compatibility with everything from Steam Deck to Switch. The deal is only for the white model. Earlier this year, 8Bitdo released a new version of this $70 controller, aptly called the Ultimate 2 Bluetooth. We rated it as one of the best controllers for the Nintendo Switch 2. While the upgraded model brings you features like more precise and sensitive joysticks, this sale brings the original Ultimate Controller back into view — and our shopping carts. Plus, it also works well with the Nintendo Switch 2. This article originally appeared on Engadget at https://www.engadget.com/deals/8bitdos-ultimate-controller-is-on-sale-for-only-43-in-this-black-friday-deal-133023111.html?src=rss",
          "content": "Early Black Friday deals are starting to flood the internet. If you're starting to think about the gifts you have to pick up for everyone in your life, you may be able to save already on some of them — including tech. One of the standout deals we've found so far is on the 8Bitdo Ultimate Bluetooth Controller and Charging Dock. It's 39 percent off right now and down to $43 — a new record-low price. The controller comes with perks such as the charging dock, 22 hours of battery per charge and compatibility with everything from Steam Deck to Switch. The deal is only for the white model. Earlier this year, 8Bitdo released a new version of this $70 controller, aptly called the Ultimate 2 Bluetooth. We rated it as one of the best controllers for the Nintendo Switch 2. While the upgraded model brings you features like more precise and sensitive joysticks, this sale brings the original Ultimate Controller back into view — and our shopping carts. Plus, it also works well with the Nintendo Switch 2. This article originally appeared on Engadget at https://www.engadget.com/deals/8bitdos-ultimate-controller-is-on-sale-for-only-43-in-this-black-friday-deal-133023111.html?src=rss",
          "feed_position": 22
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/mobile/smartphones/oneplus-15-review-a-great-phone-if-photography-isnt-a-priority-143000489.html",
          "published_at": "Thu, 13 Nov 2025 14:30:00 +0000",
          "title": "OnePlus 15 review: A great phone, if photography isn't a priority",
          "standfirst": "If you’re reading this review, there’s a chance you’ve come to it with some confusion. Didn’t OnePlus already release a new flagship phone this year, and wasn’t it called the OnePlus 13? The answer is yes to both those questions. So, what gives? Well, over the last few years, OnePlus has been working to shorten the gap between when its phones debut in China and when they’re available in the rest of the world. This year, the OnePlus 15 arrives in North America just a few short weeks after its initial October 27 release. And like it did with the jump between the OP3 and OP5, OnePlus is skipping the OP14 because of the number four and its unlucky association in Chinese culture.With that cleared up, you might think the OnePlus 15 suffers from following too soon after its predecessor. I’m happy to report it offers some clear upgrades over the OP13, including a faster processor and an absurdly big battery. What it doesn’t do, however, is change the OnePlus formula — for better and worse.Pricing and availabilityDue to the federal government shutdown, the OnePlus 15 does not have a US release date yet. The company had hoped to begin selling the phone starting today, November 13, but the phone has yet to obtain the necessary clearance. \"As is the case with every smartphone manufacturer, the United States’ Federal Communications Commission certifies OnePlus devices before they are sold in the US. As a result of the government shutdown, device certifications have been delayed,\" Spenser Blank, OnePlus North America’s head of marketing and communications, told Engadget.\"Subsequently, US sales for the OnePlus 15 will be postponed until they have been secured. The OnePlus 15 has already finished all the required tests from the FCC’s recognized labs and the certification application has been formally submitted. We are hopeful that approvals can be generated quickly and as a result, we can bring the OnePlus 15 to our customers in the U.S. expeditiously.\" There's also some uncertainty around pricing. On release, OnePlus plans to offer two versions of the OnePlus 15. The base model, with 12GB of RAM and 256GB of storage, will cost $900. The company will also sell a 16GB/512GB variant that will cost $1,000. However, OnePlus warns the price of both models is \"subject to change due to current market conditions.\"Hardware and displayThe OnePlus 15 offers a big, vibrant AMOLED display. Igor Bonifacic for EngadgetThe OnePlus 15 is a great phone with a boring, derivative design. It borrows its visual identity from the OnePlus 13s and 13T, a pair of smaller, 6.32-inch phones OnePlus released in India and China this past spring. There’s no two ways about it, in making its new flagship look more like those devices, OnePlus has at the same time made it look more like last year’s iPhone 16 Pro.It’s a shame. The design of the OnePlus 13 was one of the things my co-worker Sam Rutherford praised that phone for, and it’s something I liked about it too. The 15 just looks generic, even if there are a few nice touches. For example, the sand storm color has a nice, ceramic-like feel to it. One other impressive aspect of the design is how thin OnePlus has managed to keep the 15 while adding a massive 7,300mAh battery. At 0.31 inches thick, the sand storm variant is only slightly fatter than the 0.29-inch thick Galaxy S25 FE I recently reviewed, which has a more modest 4,900mAh battery (the two other OP15 colorways, infinite black and ultra violet, are listed as slightly thicker at 0.32 inches).Now, if you’re a longtime OnePlus fan, I need to mention the 15 doesn’t have the signature Alert Slider found on the company’s previous phones. I know what you’re thinking, did OnePlus do away with a fan favorite feature to add an AI button? The answer is both yes and no. The first time you tap the Plus Key, OxygenOS will prompt you to make it a shortcut for one of eight functions. Naturally, the default option is to use the button in conjunction with the company’s newish Mind Space AI hub, but you also can bind other functions to the Plus Key, including the old Alert Slider functionality. Even if it’s another thing that’s derivative of the iPhone, I like the flexibility the Plus Key gives.Separately, OnePlus has gone with a different screen this time around. The OP15 offers a 6.78-inch AMOLED display with a 1,272 x 2,772 resolution. The new screen is both smaller and less dense than the one found on the OnePlus 13, which was 6.82 inches big and had a 1,440 x 3,168 resolution. It’s also faster, offering a 165Hz refresh rate in select games, up from 120Hz on the earlier model. OnePlus told me it went with the lower resolution display because there’s no OLED manufacturer making QHD panels that fast yet. I’ll have more to say about the display in the performance section, but for now I’ll say the OP15’s screen is one of the best parts of using the phone. It’s dense enough to make text and images look sharp, and with up to 1,800 nits of brightness available, it’s easy to see and use even in harsh sunlight.CamerasThe OnePlus 15's cameras are a half step behind the competition. Igor Bonifacic for EngadgetOnePlus and Hasselblad ended their partnership in September, and the OP15 marks the debut of the company’s new DetailMax imaging engine. I’ll get to the software in a moment. First, OnePlus has once again gone with a main camera system built around three 50-megapixel sensors. What’s different this time around are the lenses. Two of them are slightly slower than their counterparts on the OnePlus 13. The main camera now has an f/1.8 aperture lens, down from the f/1.6 glass found on last year’s model. At the same time, OnePlus has gone with an f/2.8 lens for the telephoto camera. For comparison, the OP13 had an f/2.6 lens for distant shots. The good news is both cameras still come with optical image stabilization (OIS).Broadly, the OP15 suffers from the same problem its predecessor did. Most photos look good — sometimes great even — but they don’t come out as nice as what you might snap with the latest phones from Apple, Google or Samsung. The difference is most noticeable in nighttime and low-light photos where the OP15 can sometimes struggle to eke out shadow detail. It will also completely miss a shot because it used too slow of a shutter speed. I sometimes saw similar results in daytime photos in situations where the company’s high dynamic range algorithm would get tripped up by harsh lighting. Specifically, the shots had overly lifted shadows and unnatural highlights, resulting in photos that look flat with poor contrast.I also wish the OP15 had a better telephoto camera. It’s not bad by any means, but after seeing what the Pixel 10 Pro can do with its 5x zoom, any phone with a 3x telephoto doesn’t feel special. The limitations of the OP15’s hardware is especially noticeable when you try to push the camera beyond its maximum optical zoom. OnePlus says the camera offers 7x lossless zoom, but in my testing, I found there was a subtle drop in picture quality above 6x. At 10x and beyond, there's very noticeable pixel smearing. There are two areas where the OP15’s cameras impress. Across both stills and video, it does a great job of measuring color temperature and ensuring images come out true to life. Even more impressive is the phone’s ability to capture a burst of photos at 10 fps, up from 6 fps on the OP13. If you’ve read one of my reviews before, you’ve probably noticed I enjoy photographing the cats in my neighborhood, and no phone has made that task as easy as the OP15.As it’s already wintery in Toronto, I wasn’t able to test the OP15 new underwater camera mode. But if you live in a warmer climate, the feature is designed to make it easier to snap photos in the water by temporarily turning the OP15’s physical buttons into camera controls. This is also as good as any time to mention that OnePlus has shored up the OP15’s waterproofing. The phone is now rated IP68 against water and dust, and carries IP69 and IP69K protection against pressurized water at up to 176 degrees Fahrenheit.Performance and battery lifeIn North America, the OP15's SUPERVOOC adapter can charge the phone at 80W. Igor Bonifacic for EngadgetThe OP15 is the first phone in North America to ship with the Snapdragon 8 Elite Gen 5, and as you can probably guess, it’s a performance powerhouse. In Geekbench’s processor suite, the OP15 put up a single-core score of 3,696 and a multi-core mark of 11,187. That puts it in select company with the iPhone Air and its A19 Pro chipset, which in our testing had a slight edge in single-core performance but didn’t perform nearly as well in the multi-core suite (likely due to thermal limitations). In real-world use, I tried my best to find a game or application that could trip up the OP15, but between its new Snapdragon chipset and the 16GB of RAM that came in my unit, the phone handled everything with ease.It’s hard to describe how smooth the OnePlus 15 feels relative to other phones I’ve used recently. Every swipe and scroll feels nearly instantaneous. I suspect that’s a byproduct of the OP15’s dedicated touch response chip, which samples the screen at 3,200Hz. Speaking of the display, that 165Hz refresh rate I mentioned at the top is more of a forward-looking feature right now. As things stand, there are only seven games that can render at 165 fps. One of those, PUBG, does so through frame interpolation. So unless you’re an avid Call of Duty Mobile or Clash of Clans player, you won’t notice that benefit of the OP15’s screen, yet. Things could change in time, especially as more OEMs bring 165Hz displays to their phones. For now, the OP15 is still a great gaming phone, but it has yet to live up to its true potential.As much performance as the OnePlus 15 offers, what’s more impressive is its 7,300mAh battery. It’s the result of a new technology the company calls Silicon NanoStack. It allowed OnePlus to make a denser battery and one it claims will age more gracefully over time, thanks to a design that retains more than 80 percent of its health after four years. Obviously, I haven’t had the OP15 long enough to test that claim, but what I can say is that it offers tremendous battery life out of the box. On our local video rundown test, it posted a time of 38 hours and 30 minutes, which is eight hours longer than the OnePlus 13, the previous record holder. The Aramid case is one of three cases OnePlus offers alongside the OP15. Igor Bonifacic for EngadgetJust as impressive is how quickly the OP15 can charge. With the included 80W SUPERVOOC adapter, the battery can go from dead to full in about 40 minutes. The one downside of how OnePlus has approached charging is that the phone doesn't support the Qi2 standard. It can wirelessly charge at 50W, but you'll need to buy the company's proprietary AIRVOOC magnetic puck.SoftwareTwo screenshots showing of the OnePlus Mind SpaceIgor Bonifacic for EngadgetWhen he reviewed the OnePlus 13 in February, Engadget’s Sam Rutherford praised OnePlus for its restrained AI approach. Unfortunately, no company appears immune to the technology’s pull at this point, and in the months since, OnePlus has begun integrating more AI features into OxygenOS. Thankfully, many of those are either easy to ignore or situationally useful.First, there’s Mind Space, which is functionally similar to Nothing’s Essential Space. You can either tap the Plus Key (if it’s configured for use with Mind Space) or swipe up on the touchscreen with three fingers to save a screenshot to the hub. From there, the OP15’s built-in LLM will summarize the image, and you can ask the model questions about it. It’s also possible to save voice memos to the hub, and OnePlus offers a few other AI tools there, including one for scanning documents. All of these work well, and like I said, if they’re not your thing, they’re easy to ignore.The OnePlus 15's volume rocker and power button are located on the right side of the phone. Igor Bonifacic for EngadgetIt’s been a few years since I’ve used OxygenOS, so it was a pleasant surprise to learn it remains one of the more attractive and tasteful Android skins on the market. A standout is some of the custom animations OnePlus has baked into the OS to accentuate the speed of the phone. I also find OnePlus has one of the best organized quick settings menus. There are just enough customization options there to make it feel powerful, but not enough to overwhelm.One area where OnePlus could do better is software support. The company has pledged to provide the OP15 with four years of software updates and six years of security patches. That’s worse than both Google and Samsung, which have committed to supporting their latest phones for seven years.Wrap-upThe OnePlus 15 sits on a set of icy concrete steps. Igor Bonifacic for EngadgetIn using the OnePlus 15, I was frequently reminded of the last OnePlus phone I reviewed, the OnePlus 7 Pro. At the time, it was the company’s most expensive device ever, coming in at the same $750 price as the iPhone XR and Galaxy S10e. The appeal of that phone was its speedy Snapdragon 855 processor and the fact it was one of the first smartphones with a 90Hz AMOLED screen. It was also the first OnePlus phone with a camera that was more than just serviceable.All these years later, the appeal of the OP15 feels similar. It’s a phone for those who value speed over everything else. The tricky thing about this phone is judging its value when its price could change tomorrow. As I mentioned earlier, in the US the OP15 will start at $900, with OnePlus warning pricing for both models could “change due to current market conditions.” At $1,000, the 16GB model is a compelling alternative to the Pixel 10 Pro XL, offering a newer processor, more storage and a significantly bigger battery.Here’s the thing: Google has already aggressively discounted the entire Pixel 10 lineup, and until Black Friday, you can get the Pro XL for $899 — $100 less than the 16GB OnePlus 15. For most people, I think that’s the play, given both the Pro and Pro XL have the better telephoto camera and Google has promised to support all of its latest phones for seven years. That said, if you’re okay with a worse camera overall, the OP15 has a lot going for it, and provided OnePlus can successfully navigate an uncertain tariff regime, it will end up not just one of the best phones of 2025 but much of 2026 too.This article originally appeared on Engadget at https://www.engadget.com/mobile/smartphones/oneplus-15-review-a-great-phone-if-photography-isnt-a-priority-143000489.html?src=rss",
          "content": "If you’re reading this review, there’s a chance you’ve come to it with some confusion. Didn’t OnePlus already release a new flagship phone this year, and wasn’t it called the OnePlus 13? The answer is yes to both those questions. So, what gives? Well, over the last few years, OnePlus has been working to shorten the gap between when its phones debut in China and when they’re available in the rest of the world. This year, the OnePlus 15 arrives in North America just a few short weeks after its initial October 27 release. And like it did with the jump between the OP3 and OP5, OnePlus is skipping the OP14 because of the number four and its unlucky association in Chinese culture.With that cleared up, you might think the OnePlus 15 suffers from following too soon after its predecessor. I’m happy to report it offers some clear upgrades over the OP13, including a faster processor and an absurdly big battery. What it doesn’t do, however, is change the OnePlus formula — for better and worse.Pricing and availabilityDue to the federal government shutdown, the OnePlus 15 does not have a US release date yet. The company had hoped to begin selling the phone starting today, November 13, but the phone has yet to obtain the necessary clearance. \"As is the case with every smartphone manufacturer, the United States’ Federal Communications Commission certifies OnePlus devices before they are sold in the US. As a result of the government shutdown, device certifications have been delayed,\" Spenser Blank, OnePlus North America’s head of marketing and communications, told Engadget.\"Subsequently, US sales for the OnePlus 15 will be postponed until they have been secured. The OnePlus 15 has already finished all the required tests from the FCC’s recognized labs and the certification application has been formally submitted. We are hopeful that approvals can be generated quickly and as a result, we can bring the OnePlus 15 to our customers in the U.S. expeditiously.\" There's also some uncertainty around pricing. On release, OnePlus plans to offer two versions of the OnePlus 15. The base model, with 12GB of RAM and 256GB of storage, will cost $900. The company will also sell a 16GB/512GB variant that will cost $1,000. However, OnePlus warns the price of both models is \"subject to change due to current market conditions.\"Hardware and displayThe OnePlus 15 offers a big, vibrant AMOLED display. Igor Bonifacic for EngadgetThe OnePlus 15 is a great phone with a boring, derivative design. It borrows its visual identity from the OnePlus 13s and 13T, a pair of smaller, 6.32-inch phones OnePlus released in India and China this past spring. There’s no two ways about it, in making its new flagship look more like those devices, OnePlus has at the same time made it look more like last year’s iPhone 16 Pro.It’s a shame. The design of the OnePlus 13 was one of the things my co-worker Sam Rutherford praised that phone for, and it’s something I liked about it too. The 15 just looks generic, even if there are a few nice touches. For example, the sand storm color has a nice, ceramic-like feel to it. One other impressive aspect of the design is how thin OnePlus has managed to keep the 15 while adding a massive 7,300mAh battery. At 0.31 inches thick, the sand storm variant is only slightly fatter than the 0.29-inch thick Galaxy S25 FE I recently reviewed, which has a more modest 4,900mAh battery (the two other OP15 colorways, infinite black and ultra violet, are listed as slightly thicker at 0.32 inches).Now, if you’re a longtime OnePlus fan, I need to mention the 15 doesn’t have the signature Alert Slider found on the company’s previous phones. I know what you’re thinking, did OnePlus do away with a fan favorite feature to add an AI button? The answer is both yes and no. The first time you tap the Plus Key, OxygenOS will prompt you to make it a shortcut for one of eight functions. Naturally, the default option is to use the button in conjunction with the company’s newish Mind Space AI hub, but you also can bind other functions to the Plus Key, including the old Alert Slider functionality. Even if it’s another thing that’s derivative of the iPhone, I like the flexibility the Plus Key gives.Separately, OnePlus has gone with a different screen this time around. The OP15 offers a 6.78-inch AMOLED display with a 1,272 x 2,772 resolution. The new screen is both smaller and less dense than the one found on the OnePlus 13, which was 6.82 inches big and had a 1,440 x 3,168 resolution. It’s also faster, offering a 165Hz refresh rate in select games, up from 120Hz on the earlier model. OnePlus told me it went with the lower resolution display because there’s no OLED manufacturer making QHD panels that fast yet. I’ll have more to say about the display in the performance section, but for now I’ll say the OP15’s screen is one of the best parts of using the phone. It’s dense enough to make text and images look sharp, and with up to 1,800 nits of brightness available, it’s easy to see and use even in harsh sunlight.CamerasThe OnePlus 15's cameras are a half step behind the competition. Igor Bonifacic for EngadgetOnePlus and Hasselblad ended their partnership in September, and the OP15 marks the debut of the company’s new DetailMax imaging engine. I’ll get to the software in a moment. First, OnePlus has once again gone with a main camera system built around three 50-megapixel sensors. What’s different this time around are the lenses. Two of them are slightly slower than their counterparts on the OnePlus 13. The main camera now has an f/1.8 aperture lens, down from the f/1.6 glass found on last year’s model. At the same time, OnePlus has gone with an f/2.8 lens for the telephoto camera. For comparison, the OP13 had an f/2.6 lens for distant shots. The good news is both cameras still come with optical image stabilization (OIS).Broadly, the OP15 suffers from the same problem its predecessor did. Most photos look good — sometimes great even — but they don’t come out as nice as what you might snap with the latest phones from Apple, Google or Samsung. The difference is most noticeable in nighttime and low-light photos where the OP15 can sometimes struggle to eke out shadow detail. It will also completely miss a shot because it used too slow of a shutter speed. I sometimes saw similar results in daytime photos in situations where the company’s high dynamic range algorithm would get tripped up by harsh lighting. Specifically, the shots had overly lifted shadows and unnatural highlights, resulting in photos that look flat with poor contrast.I also wish the OP15 had a better telephoto camera. It’s not bad by any means, but after seeing what the Pixel 10 Pro can do with its 5x zoom, any phone with a 3x telephoto doesn’t feel special. The limitations of the OP15’s hardware is especially noticeable when you try to push the camera beyond its maximum optical zoom. OnePlus says the camera offers 7x lossless zoom, but in my testing, I found there was a subtle drop in picture quality above 6x. At 10x and beyond, there's very noticeable pixel smearing. There are two areas where the OP15’s cameras impress. Across both stills and video, it does a great job of measuring color temperature and ensuring images come out true to life. Even more impressive is the phone’s ability to capture a burst of photos at 10 fps, up from 6 fps on the OP13. If you’ve read one of my reviews before, you’ve probably noticed I enjoy photographing the cats in my neighborhood, and no phone has made that task as easy as the OP15.As it’s already wintery in Toronto, I wasn’t able to test the OP15 new underwater camera mode. But if you live in a warmer climate, the feature is designed to make it easier to snap photos in the water by temporarily turning the OP15’s physical buttons into camera controls. This is also as good as any time to mention that OnePlus has shored up the OP15’s waterproofing. The phone is now rated IP68 against water and dust, and carries IP69 and IP69K protection against pressurized water at up to 176 degrees Fahrenheit.Performance and battery lifeIn North America, the OP15's SUPERVOOC adapter can charge the phone at 80W. Igor Bonifacic for EngadgetThe OP15 is the first phone in North America to ship with the Snapdragon 8 Elite Gen 5, and as you can probably guess, it’s a performance powerhouse. In Geekbench’s processor suite, the OP15 put up a single-core score of 3,696 and a multi-core mark of 11,187. That puts it in select company with the iPhone Air and its A19 Pro chipset, which in our testing had a slight edge in single-core performance but didn’t perform nearly as well in the multi-core suite (likely due to thermal limitations). In real-world use, I tried my best to find a game or application that could trip up the OP15, but between its new Snapdragon chipset and the 16GB of RAM that came in my unit, the phone handled everything with ease.It’s hard to describe how smooth the OnePlus 15 feels relative to other phones I’ve used recently. Every swipe and scroll feels nearly instantaneous. I suspect that’s a byproduct of the OP15’s dedicated touch response chip, which samples the screen at 3,200Hz. Speaking of the display, that 165Hz refresh rate I mentioned at the top is more of a forward-looking feature right now. As things stand, there are only seven games that can render at 165 fps. One of those, PUBG, does so through frame interpolation. So unless you’re an avid Call of Duty Mobile or Clash of Clans player, you won’t notice that benefit of the OP15’s screen, yet. Things could change in time, especially as more OEMs bring 165Hz displays to their phones. For now, the OP15 is still a great gaming phone, but it has yet to live up to its true potential.As much performance as the OnePlus 15 offers, what’s more impressive is its 7,300mAh battery. It’s the result of a new technology the company calls Silicon NanoStack. It allowed OnePlus to make a denser battery and one it claims will age more gracefully over time, thanks to a design that retains more than 80 percent of its health after four years. Obviously, I haven’t had the OP15 long enough to test that claim, but what I can say is that it offers tremendous battery life out of the box. On our local video rundown test, it posted a time of 38 hours and 30 minutes, which is eight hours longer than the OnePlus 13, the previous record holder. The Aramid case is one of three cases OnePlus offers alongside the OP15. Igor Bonifacic for EngadgetJust as impressive is how quickly the OP15 can charge. With the included 80W SUPERVOOC adapter, the battery can go from dead to full in about 40 minutes. The one downside of how OnePlus has approached charging is that the phone doesn't support the Qi2 standard. It can wirelessly charge at 50W, but you'll need to buy the company's proprietary AIRVOOC magnetic puck.SoftwareTwo screenshots showing of the OnePlus Mind SpaceIgor Bonifacic for EngadgetWhen he reviewed the OnePlus 13 in February, Engadget’s Sam Rutherford praised OnePlus for its restrained AI approach. Unfortunately, no company appears immune to the technology’s pull at this point, and in the months since, OnePlus has begun integrating more AI features into OxygenOS. Thankfully, many of those are either easy to ignore or situationally useful.First, there’s Mind Space, which is functionally similar to Nothing’s Essential Space. You can either tap the Plus Key (if it’s configured for use with Mind Space) or swipe up on the touchscreen with three fingers to save a screenshot to the hub. From there, the OP15’s built-in LLM will summarize the image, and you can ask the model questions about it. It’s also possible to save voice memos to the hub, and OnePlus offers a few other AI tools there, including one for scanning documents. All of these work well, and like I said, if they’re not your thing, they’re easy to ignore.The OnePlus 15's volume rocker and power button are located on the right side of the phone. Igor Bonifacic for EngadgetIt’s been a few years since I’ve used OxygenOS, so it was a pleasant surprise to learn it remains one of the more attractive and tasteful Android skins on the market. A standout is some of the custom animations OnePlus has baked into the OS to accentuate the speed of the phone. I also find OnePlus has one of the best organized quick settings menus. There are just enough customization options there to make it feel powerful, but not enough to overwhelm.One area where OnePlus could do better is software support. The company has pledged to provide the OP15 with four years of software updates and six years of security patches. That’s worse than both Google and Samsung, which have committed to supporting their latest phones for seven years.Wrap-upThe OnePlus 15 sits on a set of icy concrete steps. Igor Bonifacic for EngadgetIn using the OnePlus 15, I was frequently reminded of the last OnePlus phone I reviewed, the OnePlus 7 Pro. At the time, it was the company’s most expensive device ever, coming in at the same $750 price as the iPhone XR and Galaxy S10e. The appeal of that phone was its speedy Snapdragon 855 processor and the fact it was one of the first smartphones with a 90Hz AMOLED screen. It was also the first OnePlus phone with a camera that was more than just serviceable.All these years later, the appeal of the OP15 feels similar. It’s a phone for those who value speed over everything else. The tricky thing about this phone is judging its value when its price could change tomorrow. As I mentioned earlier, in the US the OP15 will start at $900, with OnePlus warning pricing for both models could “change due to current market conditions.” At $1,000, the 16GB model is a compelling alternative to the Pixel 10 Pro XL, offering a newer processor, more storage and a significantly bigger battery.Here’s the thing: Google has already aggressively discounted the entire Pixel 10 lineup, and until Black Friday, you can get the Pro XL for $899 — $100 less than the 16GB OnePlus 15. For most people, I think that’s the play, given both the Pro and Pro XL have the better telephoto camera and Google has promised to support all of its latest phones for seven years. That said, if you’re okay with a worse camera overall, the OP15 has a lot going for it, and provided OnePlus can successfully navigate an uncertain tariff regime, it will end up not just one of the best phones of 2025 but much of 2026 too.This article originally appeared on Engadget at https://www.engadget.com/mobile/smartphones/oneplus-15-review-a-great-phone-if-photography-isnt-a-priority-143000489.html?src=rss",
          "feed_position": 24,
          "image_url": "https://d29szjachogqwa.cloudfront.net/images/user-uploaded/oneplus-15-2.jpg"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/apples-macbook-air-m4-drops-to-a-record-low-price-before-black-friday-183808088.html",
          "published_at": "Thu, 13 Nov 2025 14:16:28 +0000",
          "title": "Apple's MacBook Air M4 drops to a record-low price before Black Friday",
          "standfirst": "Now's a great time to pick up a new MacBook Air if you've been thinking about taking the plunge. Amazon has the M4-powered, 13-inch MacBook Air for a record-low price of $749 right now. The 25 percent discount applies to multiple colors, too. We ranked this as our favorite Apple laptop in our list of the best MacBook computers. Heck, it's even our very favorite laptop. Full stop. The performance is exceptionally snappy, thanks to the M4 chip. We appreciated the upgraded battery life, which now lasts for around 18 hours per charge. That's well beyond a full day of work. The design is lightweight, but sturdy. This has become a hallmark for modern MacBook Air computers. The screen is both gorgeous and roomy, even though it's technically just a 13-inch panel. There's support for the P3 wide color gamut and it can reach up to 500 nits of brightness. This is a near-perfect laptop, but there are a couple of nitpicks. There's no USB-C port on the right side, limiting how users can arrange accessories on a desk. Also, the screen is capped with a 60Hz refresh rate. Another potential complication is the looming specter of the M5 chip. The company has already released the MacBook Pro M5, so a new MacBook Air is likely coming in the nearish future. (Read: sometime in early 2026). If you need more screen space, you'll find a similar discount on the 15-inch MacBook Air on Amazon, too. Most color options are $250 off and down to $949 for the base model (you guessed it — another all-time low). This article originally appeared on Engadget at https://www.engadget.com/deals/apples-macbook-air-m4-drops-to-a-record-low-price-before-black-friday-183808088.html?src=rss",
          "content": "Now's a great time to pick up a new MacBook Air if you've been thinking about taking the plunge. Amazon has the M4-powered, 13-inch MacBook Air for a record-low price of $749 right now. The 25 percent discount applies to multiple colors, too. We ranked this as our favorite Apple laptop in our list of the best MacBook computers. Heck, it's even our very favorite laptop. Full stop. The performance is exceptionally snappy, thanks to the M4 chip. We appreciated the upgraded battery life, which now lasts for around 18 hours per charge. That's well beyond a full day of work. The design is lightweight, but sturdy. This has become a hallmark for modern MacBook Air computers. The screen is both gorgeous and roomy, even though it's technically just a 13-inch panel. There's support for the P3 wide color gamut and it can reach up to 500 nits of brightness. This is a near-perfect laptop, but there are a couple of nitpicks. There's no USB-C port on the right side, limiting how users can arrange accessories on a desk. Also, the screen is capped with a 60Hz refresh rate. Another potential complication is the looming specter of the M5 chip. The company has already released the MacBook Pro M5, so a new MacBook Air is likely coming in the nearish future. (Read: sometime in early 2026). If you need more screen space, you'll find a similar discount on the 15-inch MacBook Air on Amazon, too. Most color options are $250 off and down to $949 for the base model (you guessed it — another all-time low). This article originally appeared on Engadget at https://www.engadget.com/deals/apples-macbook-air-m4-drops-to-a-record-low-price-before-black-friday-183808088.html?src=rss",
          "feed_position": 25
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/black-friday-apple-deals-bring-the-apple-watch-se-3-down-to-a-record-low-price-133057344.html",
          "published_at": "Thu, 13 Nov 2025 13:46:26 +0000",
          "title": "Black Friday Apple deals bring the Apple Watch SE 3 down to a record-low price",
          "standfirst": "The era of holiday shopping is coming and there are a lot of great tech gift options for loved ones or as a treat for yourself. That includes the new Apple Watch SE 3, which you can snag for only $200 right now. We consider this to be the best budget Apple Watch, and arguably the best smartwatch for folks who have never owned one before. The latest version runs on the same chipset found in the new flagship models, and it has most of the same fitness and workout tracking features you'll find in those more expensive devices as well. The SE 3 also now has an always-on display, making it easier to glance down throughout the day to check the time or see activity stats without moving your wrist, and fast-charging support makes it a more viable sleep tracker. Just plop it down on its charger for a bit at the end of the day and put it back on to monitor your sleep overnight. Also discounted is the high-end Apple Watch Ultra 3, which you can snag for $100 off. The sale model comes with 64GB of storage, a 49mm screen and GPS and cellular service. Notably, it's also only available with the one size, adjustable band and in two colors: a Black titanium case with Black Ocean band and a natural titanium Case with Anchor Blue Ocean band. The Apple Watch Ultra 3 came out in early September and is one of the first smartwatches to support satellite communications. This feature means you can call, send messages or share your location with emergency services through the watch — even if you don't have a connection. The new Ultra 3 also has a larger screen thanks to thinner bezels and a battery that can last for up to 42 hours. This article originally appeared on Engadget at https://www.engadget.com/deals/black-friday-apple-deals-bring-the-apple-watch-se-3-down-to-a-record-low-price-133057344.html?src=rss",
          "content": "The era of holiday shopping is coming and there are a lot of great tech gift options for loved ones or as a treat for yourself. That includes the new Apple Watch SE 3, which you can snag for only $200 right now. We consider this to be the best budget Apple Watch, and arguably the best smartwatch for folks who have never owned one before. The latest version runs on the same chipset found in the new flagship models, and it has most of the same fitness and workout tracking features you'll find in those more expensive devices as well. The SE 3 also now has an always-on display, making it easier to glance down throughout the day to check the time or see activity stats without moving your wrist, and fast-charging support makes it a more viable sleep tracker. Just plop it down on its charger for a bit at the end of the day and put it back on to monitor your sleep overnight. Also discounted is the high-end Apple Watch Ultra 3, which you can snag for $100 off. The sale model comes with 64GB of storage, a 49mm screen and GPS and cellular service. Notably, it's also only available with the one size, adjustable band and in two colors: a Black titanium case with Black Ocean band and a natural titanium Case with Anchor Blue Ocean band. The Apple Watch Ultra 3 came out in early September and is one of the first smartwatches to support satellite communications. This feature means you can call, send messages or share your location with emergency services through the watch — even if you don't have a connection. The new Ultra 3 also has a larger screen thanks to thinner bezels and a battery that can last for up to 42 hours. This article originally appeared on Engadget at https://www.engadget.com/deals/black-friday-apple-deals-bring-the-apple-watch-se-3-down-to-a-record-low-price-133057344.html?src=rss",
          "feed_position": 28
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/ecoflow-black-friday-deals-get-up-to-42-percent-off-portable-power-stations-130041144.html",
          "published_at": "Thu, 13 Nov 2025 13:30:37 +0000",
          "title": "EcoFlow Black Friday deals: Get up to 42 percent off portable power stations",
          "standfirst": "The EcoFlow Black Friday sale is in full swing, knocking thousands of dollars off portable power stations and their accessories. One of the best discounts at the moment is on the Delta Pro 3, which is 37 percent off and down to $2,299. That's the lowest we've seen it and, considering it typically costs $3,699, it's a great deal. Amazon's matching the sale price as well. The Delta Pro 3 topped Yahoo's list of the best portable power stations, and for very good reason. This thing is a beast. It boasts a 4,096Wh capacity, so it can power an average 500-watt refrigerator for over 24 hours. That's with continuous use. It can be stretched out to two or three days by only running the appliance during daylight hours. There's even a discounted bundle that includes an extra battery for $3,599. It includes four standard 120V AC outlets and a single 240V outlet. It could potentially be a temporary hub of a whole-home battery backup. There are numerous charging options here, including a standard AC outlet, solar panels and, interestingly, a cigarette lighter. The only potential downside here is the Delta Pro 3 really pushes the boundaries of what can be considered portable. It weighs 113 pounds, though it does have wheels and a telescoping handle. The Delta Pro 3 is just one of the products on sale right now. The Delta Pro Ultra, which is intended as a whole-home backup, is down to $3,999. This represents a savings of more than $2,000. Another Yahoo top pick, the Delta 2 Max, is $1,000 off and down to $899. This article originally appeared on Engadget at https://www.engadget.com/deals/ecoflow-black-friday-deals-get-up-to-42-percent-off-portable-power-stations-130041144.html?src=rss",
          "content": "The EcoFlow Black Friday sale is in full swing, knocking thousands of dollars off portable power stations and their accessories. One of the best discounts at the moment is on the Delta Pro 3, which is 37 percent off and down to $2,299. That's the lowest we've seen it and, considering it typically costs $3,699, it's a great deal. Amazon's matching the sale price as well. The Delta Pro 3 topped Yahoo's list of the best portable power stations, and for very good reason. This thing is a beast. It boasts a 4,096Wh capacity, so it can power an average 500-watt refrigerator for over 24 hours. That's with continuous use. It can be stretched out to two or three days by only running the appliance during daylight hours. There's even a discounted bundle that includes an extra battery for $3,599. It includes four standard 120V AC outlets and a single 240V outlet. It could potentially be a temporary hub of a whole-home battery backup. There are numerous charging options here, including a standard AC outlet, solar panels and, interestingly, a cigarette lighter. The only potential downside here is the Delta Pro 3 really pushes the boundaries of what can be considered portable. It weighs 113 pounds, though it does have wheels and a telescoping handle. The Delta Pro 3 is just one of the products on sale right now. The Delta Pro Ultra, which is intended as a whole-home backup, is down to $3,999. This represents a savings of more than $2,000. Another Yahoo top pick, the Delta 2 Max, is $1,000 off and down to $899. This article originally appeared on Engadget at https://www.engadget.com/deals/ecoflow-black-friday-deals-get-up-to-42-percent-off-portable-power-stations-130041144.html?src=rss",
          "feed_position": 29
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/amazon-black-friday-deals-include-the-fire-tv-stick-4k-max-for-only-35-132100009.html",
          "published_at": "Thu, 13 Nov 2025 13:21:00 +0000",
          "title": "Amazon Black Friday deals include the Fire TV Stick 4K Max for only $35",
          "standfirst": "Amazon has early Black Friday savings on its most potent streaming stick. The Fire TV Stick 4K Max is 41 percent off, bringing it close to its record low. You can get it now for $35. The Fire TV Stick 4K Max is one of Engadget's favorite streaming devices. The stick supports a solid mix of advanced technologies for its price: Dolby Vision, Dolby Atmos, 4K and HDR10+. As long as the rest of your entertainment setup can handle it, you'll get a high-quality picture and sound. It also supports Wi-Fi 6E, enabling better, faster connectivity with compatible routers. The 4K Max has the fastest processor of any Amazon Fire TV Stick, so expect zippy navigation. It also supports Amazon's Ambient Experience. This mode displays art (like on Samsung's The Frame) while the device is in standby. It's also a solid choice for gaming: It supports Xbox cloud streaming and works well as a retro game emulator, too. The UI is where Amazon appears to be subsidizing the device's low cost. Expect to see loads of Prime Video content promos, along with other ads. But for $35 (compared to its MSRP of $60), you may find it easier to justify that tradeoff. Also on sale is Amazon's Fire TV Stick HD, our pick for the best budget streaming stick. This model doesn't support 4K; instead, it limits you to 1080p at 60 fps. At $18 for Black Friday, it's certainly cheap. But if you have a 4K TV (or plan to soon), you may want to consider the slightly more expensive model. This article originally appeared on Engadget at https://www.engadget.com/deals/amazon-black-friday-deals-include-the-fire-tv-stick-4k-max-for-only-35-132100009.html?src=rss",
          "content": "Amazon has early Black Friday savings on its most potent streaming stick. The Fire TV Stick 4K Max is 41 percent off, bringing it close to its record low. You can get it now for $35. The Fire TV Stick 4K Max is one of Engadget's favorite streaming devices. The stick supports a solid mix of advanced technologies for its price: Dolby Vision, Dolby Atmos, 4K and HDR10+. As long as the rest of your entertainment setup can handle it, you'll get a high-quality picture and sound. It also supports Wi-Fi 6E, enabling better, faster connectivity with compatible routers. The 4K Max has the fastest processor of any Amazon Fire TV Stick, so expect zippy navigation. It also supports Amazon's Ambient Experience. This mode displays art (like on Samsung's The Frame) while the device is in standby. It's also a solid choice for gaming: It supports Xbox cloud streaming and works well as a retro game emulator, too. The UI is where Amazon appears to be subsidizing the device's low cost. Expect to see loads of Prime Video content promos, along with other ads. But for $35 (compared to its MSRP of $60), you may find it easier to justify that tradeoff. Also on sale is Amazon's Fire TV Stick HD, our pick for the best budget streaming stick. This model doesn't support 4K; instead, it limits you to 1080p at 60 fps. At $18 for Black Friday, it's certainly cheap. But if you have a 4K TV (or plan to soon), you may want to consider the slightly more expensive model. This article originally appeared on Engadget at https://www.engadget.com/deals/amazon-black-friday-deals-include-the-fire-tv-stick-4k-max-for-only-35-132100009.html?src=rss",
          "feed_position": 32
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/cameras/dji-neo-2-review-the-best-budget-drone-is-now-even-better-120026045.html",
          "published_at": "Thu, 13 Nov 2025 12:00:26 +0000",
          "title": "DJI Neo 2 review: The best budget drone is now even better",
          "standfirst": "Even with its US future in limbo, DJI keeps releasing impressive drones. Its latest is the Neo 2, an inexpensive, lightweight model aimed at creators and hobbyists. It’s an upgraded version of the Neo, an immensely popular drone both in the US and elsewhere. The new model is equally safe and easy to fly thanks to the shrouded props, palm takeoff and voice control. DJI didn’t just tack on a number and call it a day, though. The Neo 2 is loaded with new features like LiDAR obstacle detection, improved video quality and longer battery life — all at only a slightly higher price. In fact, a couple of the new features can’t even be found on high-end DJI drones. The original Neo wasn’t perfect; the propellers on that model emitted a banshee-like scream. The lack of obstacle detection and short battery life also wasn’t ideal. Has the company solved these problems on the new model? Mostly, yes — and it’s now a nearly perfect personal drone. Design Though the Neo 2 has the same 6.5-inch square size and friendly appearance as before, there are some important differences. A new LiDAR sensor beside the camera is designed to detect forward obstacles, and is a surprising inclusion for such an affordable drone. On the left front is a small display — a first for a DJI drone — that shows the current shot mode (follow, dronie, etc.). The gimbal has also been upgraded and can now rotate instead of just tilt. That means the camera will stay level when the drone banks, with no ugly cropping or reduced resolution that I occasionally saw when doing extreme maneuvers with the Neo. The Neo 2 has three buttons on the side (rather than just one on top as before), along with two new omnidirectional sensors. On top of helping the Neo 2 avoid obstacles, these improve stability in featureless or non-GPS environments like indoors and over water. Finally, if you buy a Neo 2 with the optional RC-2 or RC-N3 controller, it includes a removable Digital Transceiver with a pair of antennae. Those components give it a butterfly-like appearance and could affect long term durability. However, making them an add-on gave DJI a way to make the basic (non-controller) version as inexpensive as possible. With all those changes, the Neo 2 is a bit heavier at 5.6 ounces (160 grams) with the Digital Transceiver, compared to 4.8 ounces (135 grams) for the Neo. That’s still easily small enough to slide in a bag and light enough (under 250 grams) that you won’t need to register your drone with the authorities. Features Gesture control is a major new Neo 2 feature not found on other DJI drones. It lets you fly with surprising precision, if you don’t mind looking a little silly. To do so, wave a hand up, down or sideways to climb and bank, move two hands apart or together to make it approach or move away (think pinch-to-zoom in the air), clench a fist to stop it and hold out your palm to land. Gestures can be used in combination with other fly modes so you get the exact shot you want. Steve Dent for Engadget Along with the ability to wave your hands, the Neo 2 offers other ways to fly like palm takeoff. After selecting the flight mode (shown on the new display and audibly announced), you can launch the Neo 2 from your hand by pressing the takeoff button or using voice control. It will then execute the chosen mode, fly back to you and land on your outstretched palm. Smartphone control (not seen on other DJI drones) is available for manual flight. Wi-Fi range has been extended by ten times to 546 yards, compared to 55 yards for the Neo. However, for that range you need a clear signal path with no obstacles between you and the drone. For more precise flight and longer range (up to 6 miles), the Neo 2 supports RC-N2, RC-N3 or RC-2 controllers. And FPV (first person) flying is still an option when using DJI’s Goggles N3 or Goggles 3, paired with the RC Motion 3 or FPV Remote Controller 3. The Neo 3’s Quickshots repertoire has also been updated. A fun new mode is the “Hitchcock zoom.” When activated, the drone backs away while zooming in at the same time, creating a trippy perspective effect you might’ve seen in Vertigo and Jaws. Again, this feature is unique to the Neo 2 in DJI’s lineup. Other key functions include compatibility with DJI’s microphones, including the Mic 2, Mic 3 and Mic Mini. Since the Neo 2 is quieter than the Neo (more on that shortly), noise cancelling requirements are reduced so audio quality is improved. Onboard storage has also been upgraded to 49GB (up from 22GB), letting you record up to 175 minutes of 4K video. At the same time, DJI more than doubled the Wi-Fi transfer speed to 80MB/s. Performance The Neo 2 now has omni sensors and LiDAR for obstacle protectoin Steve Dent for Engadget The Neo 2 retains the Neo’s toughness and protection, but it can now fly around obstacles instead of just plowing through them thanks to the LiDAR and sensors. To test that, I used it to follow subjects around trees, buildings and other obstacles. The aim was to see not only how well it avoided crashes in all directions, but to observe how smoothly it tracked while doing so. To start, I engaged the ActiveTrack follow feature and used the Trace mode “steering wheel’ to follow subjects from the front, sides and back. When flying forward, the Neo 2 avoided most obstacles using its LiDAR, while swooping cinematically around trees and branches. It occasionally failed to detect small twigs and leaves, but unlike open-prop DJI drones, it usually flew through them without incident. With just two omni sensors, the Neo 2 is less adept at avoiding obstacles to the sides and rear, however. So if you’re using it to vlog while walking or biking, it’s best to make sure there’s nothing in the way. With a top speed of just 18 mph, the original Neo was so slow that it could barely track a bike. Luckily, the Neo 2 is much faster with speeds up to 27 mph in follow mode and even has a special “Bicycle Tracking” mode. I tested that on an e-bike and the drone easily kept up with me while easily avoiding obstacles in the forward direction. Again, you need to be extra careful when it’s flying backwards or sideways. The Neo 2 is as agile as ever, so the extra velocity makes it a better FPV drone as well. While wearing the Goggles N3, I was able to enjoy that sensation of extra speed as it swooped around obstacles. That, plus the low price, makes it a good first drone for anyone looking to get into FPV flying. I likened the Neo’s prop noise to a banshee howl, but the Neo 2 is maybe a… banshee purr. Both the decibel level and pitch are now tolerable, even indoors. I flew it around at a fairly loud party and hardly anyone noticed; it also didn’t draw much attention in a public park. The only negative is it’s a high-pitched sound, a noise that carries even across high altitudes. Video quality DJI's Neo 2 now offers 4K at up to 100 fps Steve Dent for Engadget With a new 12MP, 1/2-inch sensor camera, video quality is excellent for this price range. The Neo 2 now supports 4K at up to 60 fps or 100 fps in slo-mo mode. On the previous model, it was just 30 fps. The Neo 2 also has an f/2.0 iris to let in more light compared to the previous f/2.8. With those updates, the drone offers sharper and more color-accurate video and photos than the Neo. There are of course some sacrifices at this price. The small sensor means the Neo 2 has mediocre low-light capability, with pronounced grain at the maximum ISO 12,800 rating that’s even noticeable at ISO 3,200. Unlike the $400 Flip, the Neo 2 has no 10-bit D-LogM capability, so over- or underexposed video is hard to correct. With those issues, video and photos from the Neo 2 aren't quite good enough for professional work. However, it’s excellent for social media users, hobbyists and content creators, delivering smartphone-quality aerial shots. Wrap-up DJI has yet to reveal availability or pricing of the Neo 2 in the US (or whether it will come here at all) due to a looming December 23 ban. If it does arrive, it will effectively have no competition at its price point, which I expect to be around $250. The HoverAir X1 is the only name brand alternative, but costs twice as much. That model offers solid follow-me capabilities for activities like biking and hiking. Feature-wise, though, it pales in comparison to the Neo 2, with inferior video quality, battery life, range and obstacle detection. DJI’s Neo 2 is not just the best personal drone; it's the best tech product I’ve seen in a while, period. It retains everything I liked about the Neo, especially the ease of use and safety features. On top of that, it adds a host of useful functions like obstacle protection and, thank goodness, lower noise levels. With all that, the Neo 2 performs that rare trick of doing much more than I expected — for a lot less money.This article originally appeared on Engadget at https://www.engadget.com/cameras/dji-neo-2-review-the-best-budget-drone-is-now-even-better-120026045.html?src=rss",
          "content": "Even with its US future in limbo, DJI keeps releasing impressive drones. Its latest is the Neo 2, an inexpensive, lightweight model aimed at creators and hobbyists. It’s an upgraded version of the Neo, an immensely popular drone both in the US and elsewhere. The new model is equally safe and easy to fly thanks to the shrouded props, palm takeoff and voice control. DJI didn’t just tack on a number and call it a day, though. The Neo 2 is loaded with new features like LiDAR obstacle detection, improved video quality and longer battery life — all at only a slightly higher price. In fact, a couple of the new features can’t even be found on high-end DJI drones. The original Neo wasn’t perfect; the propellers on that model emitted a banshee-like scream. The lack of obstacle detection and short battery life also wasn’t ideal. Has the company solved these problems on the new model? Mostly, yes — and it’s now a nearly perfect personal drone. Design Though the Neo 2 has the same 6.5-inch square size and friendly appearance as before, there are some important differences. A new LiDAR sensor beside the camera is designed to detect forward obstacles, and is a surprising inclusion for such an affordable drone. On the left front is a small display — a first for a DJI drone — that shows the current shot mode (follow, dronie, etc.). The gimbal has also been upgraded and can now rotate instead of just tilt. That means the camera will stay level when the drone banks, with no ugly cropping or reduced resolution that I occasionally saw when doing extreme maneuvers with the Neo. The Neo 2 has three buttons on the side (rather than just one on top as before), along with two new omnidirectional sensors. On top of helping the Neo 2 avoid obstacles, these improve stability in featureless or non-GPS environments like indoors and over water. Finally, if you buy a Neo 2 with the optional RC-2 or RC-N3 controller, it includes a removable Digital Transceiver with a pair of antennae. Those components give it a butterfly-like appearance and could affect long term durability. However, making them an add-on gave DJI a way to make the basic (non-controller) version as inexpensive as possible. With all those changes, the Neo 2 is a bit heavier at 5.6 ounces (160 grams) with the Digital Transceiver, compared to 4.8 ounces (135 grams) for the Neo. That’s still easily small enough to slide in a bag and light enough (under 250 grams) that you won’t need to register your drone with the authorities. Features Gesture control is a major new Neo 2 feature not found on other DJI drones. It lets you fly with surprising precision, if you don’t mind looking a little silly. To do so, wave a hand up, down or sideways to climb and bank, move two hands apart or together to make it approach or move away (think pinch-to-zoom in the air), clench a fist to stop it and hold out your palm to land. Gestures can be used in combination with other fly modes so you get the exact shot you want. Steve Dent for Engadget Along with the ability to wave your hands, the Neo 2 offers other ways to fly like palm takeoff. After selecting the flight mode (shown on the new display and audibly announced), you can launch the Neo 2 from your hand by pressing the takeoff button or using voice control. It will then execute the chosen mode, fly back to you and land on your outstretched palm. Smartphone control (not seen on other DJI drones) is available for manual flight. Wi-Fi range has been extended by ten times to 546 yards, compared to 55 yards for the Neo. However, for that range you need a clear signal path with no obstacles between you and the drone. For more precise flight and longer range (up to 6 miles), the Neo 2 supports RC-N2, RC-N3 or RC-2 controllers. And FPV (first person) flying is still an option when using DJI’s Goggles N3 or Goggles 3, paired with the RC Motion 3 or FPV Remote Controller 3. The Neo 3’s Quickshots repertoire has also been updated. A fun new mode is the “Hitchcock zoom.” When activated, the drone backs away while zooming in at the same time, creating a trippy perspective effect you might’ve seen in Vertigo and Jaws. Again, this feature is unique to the Neo 2 in DJI’s lineup. Other key functions include compatibility with DJI’s microphones, including the Mic 2, Mic 3 and Mic Mini. Since the Neo 2 is quieter than the Neo (more on that shortly), noise cancelling requirements are reduced so audio quality is improved. Onboard storage has also been upgraded to 49GB (up from 22GB), letting you record up to 175 minutes of 4K video. At the same time, DJI more than doubled the Wi-Fi transfer speed to 80MB/s. Performance The Neo 2 now has omni sensors and LiDAR for obstacle protectoin Steve Dent for Engadget The Neo 2 retains the Neo’s toughness and protection, but it can now fly around obstacles instead of just plowing through them thanks to the LiDAR and sensors. To test that, I used it to follow subjects around trees, buildings and other obstacles. The aim was to see not only how well it avoided crashes in all directions, but to observe how smoothly it tracked while doing so. To start, I engaged the ActiveTrack follow feature and used the Trace mode “steering wheel’ to follow subjects from the front, sides and back. When flying forward, the Neo 2 avoided most obstacles using its LiDAR, while swooping cinematically around trees and branches. It occasionally failed to detect small twigs and leaves, but unlike open-prop DJI drones, it usually flew through them without incident. With just two omni sensors, the Neo 2 is less adept at avoiding obstacles to the sides and rear, however. So if you’re using it to vlog while walking or biking, it’s best to make sure there’s nothing in the way. With a top speed of just 18 mph, the original Neo was so slow that it could barely track a bike. Luckily, the Neo 2 is much faster with speeds up to 27 mph in follow mode and even has a special “Bicycle Tracking” mode. I tested that on an e-bike and the drone easily kept up with me while easily avoiding obstacles in the forward direction. Again, you need to be extra careful when it’s flying backwards or sideways. The Neo 2 is as agile as ever, so the extra velocity makes it a better FPV drone as well. While wearing the Goggles N3, I was able to enjoy that sensation of extra speed as it swooped around obstacles. That, plus the low price, makes it a good first drone for anyone looking to get into FPV flying. I likened the Neo’s prop noise to a banshee howl, but the Neo 2 is maybe a… banshee purr. Both the decibel level and pitch are now tolerable, even indoors. I flew it around at a fairly loud party and hardly anyone noticed; it also didn’t draw much attention in a public park. The only negative is it’s a high-pitched sound, a noise that carries even across high altitudes. Video quality DJI's Neo 2 now offers 4K at up to 100 fps Steve Dent for Engadget With a new 12MP, 1/2-inch sensor camera, video quality is excellent for this price range. The Neo 2 now supports 4K at up to 60 fps or 100 fps in slo-mo mode. On the previous model, it was just 30 fps. The Neo 2 also has an f/2.0 iris to let in more light compared to the previous f/2.8. With those updates, the drone offers sharper and more color-accurate video and photos than the Neo. There are of course some sacrifices at this price. The small sensor means the Neo 2 has mediocre low-light capability, with pronounced grain at the maximum ISO 12,800 rating that’s even noticeable at ISO 3,200. Unlike the $400 Flip, the Neo 2 has no 10-bit D-LogM capability, so over- or underexposed video is hard to correct. With those issues, video and photos from the Neo 2 aren't quite good enough for professional work. However, it’s excellent for social media users, hobbyists and content creators, delivering smartphone-quality aerial shots. Wrap-up DJI has yet to reveal availability or pricing of the Neo 2 in the US (or whether it will come here at all) due to a looming December 23 ban. If it does arrive, it will effectively have no competition at its price point, which I expect to be around $250. The HoverAir X1 is the only name brand alternative, but costs twice as much. That model offers solid follow-me capabilities for activities like biking and hiking. Feature-wise, though, it pales in comparison to the Neo 2, with inferior video quality, battery life, range and obstacle detection. DJI’s Neo 2 is not just the best personal drone; it's the best tech product I’ve seen in a while, period. It retains everything I liked about the Neo, especially the ease of use and safety features. On top of that, it adds a host of useful functions like obstacle protection and, thank goodness, lower noise levels. With all that, the Neo 2 performs that rare trick of doing much more than I expected — for a lot less money.This article originally appeared on Engadget at https://www.engadget.com/cameras/dji-neo-2-review-the-best-budget-drone-is-now-even-better-120026045.html?src=rss",
          "feed_position": 36,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2025-11/6c85f620-bfe5-11f0-b7df-a85585c76318"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/best-streaming-service-deals-133028980.html",
          "published_at": "Thu, 13 Nov 2025 11:01:26 +0000",
          "title": "The best streaming deals: Save on DirecTV, Audible, Starz and more",
          "standfirst": "If you’ve been shocked by how much you spend on streaming services lately, you’re not alone. Companies like Netflix, Disney, HBO Max and others have been consistently raising prices to the point where you may question if streaming is even worth it anymore. We at Engadget still think it is, but we also think you should be smart with your money — and that’s where streaming deals come in. Yes, it is possible to get discounts on services like Peacock and Paramount+, even if those deals aren’t as common as a sale on AirPods. If you’re looking to save money and still stream all of the content you want, Engadget can help by laying out the best streaming deals you can get right now, how you can save with bundles and everything you should know before paying for yet another streaming service. Best streaming deals True streaming deals can be hard to come by. Most often, they’ll pop up during the Black Friday shopping period. On occasion, we’ll see them sparingly throughout the year and they usually take the form of a discounted monthly or annual rate for a limited period of time. Also, true streaming deals are typically on the ad-supported versions of a service, but once in a while you’ll find a unicorn of a deal on a tier that has ad-free viewing. If you’re able to wait for a deal before subscribing to a streaming service, we recommend doing so. You’ll save money upfront and in the long run, and you also have the option to cancel your subscription before the price goes back up to the normal rate. Audible subscription (three months) for $3 ($42 off): From now through mid-December, you can get Amazon’s audiobook subscription for just a dollar a month for three months. Note that it will auto-renew at $15 per month after that, but you can cancel at any point. Starz (one year) for $30 ($40 off): Pay upfront for one year and you can get $40 off a Stars annual subscription. There's a month-to-month option too, which costs $5 per month for the first three months if you don't want to commit to the full year. Either option gives you access to the entire Starz TV and movie library with offline viewing and no ads. Hulu + Live TV — $64.99/mo for 3 months ($25/mo off): New and eligible returning subscribers can get three months of Hulu + Live TV at a rate of $65 per month, which is much cheaper than the current $83-per-month rate and a whopping 27 percent off the new $90-per-month rate that kicks in on October 21. In addition to live TV content, unlimited DVR and access to more than 95 live TV channels, this service also includes Disney+ and ESPN Select, so you're essentially getting three separate streaming services under this umbrella. Just remember that your subscription will be billed at the new standard $90/month rate after the first three months. Fubo Pro for $55/month for the first month ($30 off): Fubo has introductory discounts on most of its packages, and the Pro package is the least expensive plan currently listed. It offers access to 224 channels, unlimited cloud DVR and up to 10 simultaneous streams. It even includes regional sports content from the NHL, MLB and NBA. Spotify Premium Individual (one month) for $0 ($12 off): This is our favorite music streaming service for podcasts and social features. The Premium Individual plan lets you listen ad-free and skip songs at will. You can also organize your listening queue and download content for offline listening. Just be aware, your subscription will auto-renew at the end of the trial period. So if you don't want to be on the hook for the $12 monthly fee, set a reminder to cancel and go back to the free version. Streaming bundle discounts There’s more consolidation happening now than ever before in the streaming space, and that means there are more streaming bundle options. These bundles offer you access to more content with one subscription price, but those prices are typically higher than paying for a single service by itself (obviously). It may be tempting to just get the bundle, but if only one of those services in the bundle speaks to you, you’ll spend less overall by just paying for the single service. Speaking of a deep love for a single streaming service: if all of your favorite shows are on Peacock or the latest releases on HBO Max consistently bring you joy, consider paying for one year upfront. Subscribing with an annual plan usually saves you money in the long term over paying on a monthly basis. Unfortunately, not all streaming services (looking at you, Netflix) have an annual subscription option. Disney+ If you feel like Charlie Kelly trying to figure out who Pepe Silvia is when you look at Disney's streaming prices chart, you're not alone. The confusion comes from the fact that Disney owns, or has a hand in, many streaming services including Hulu and ESPN. Throw in a partnership with HBO Max and you have a ton of options to consider and, probably, whiplash to match. Here's a quick overview of popular Disney+ bundle pricing. Disney+ and Hulu bundle (with ads) — $13/month Disney+ and Hulu bundle (without ads) — $20/month Disney+, Hulu and ESPN Select (with ads) — $20/month Disney+, Hulu and ESPN Select (without ads on Disney+ and Hulu only) — $30/month Disney+, Hulu and HBO Max (with ads) — $20/month Disney+, Hulu and HBO Max (without ads) — $33/month Peacock TV Peacock doesn't have any streaming bundles available all year round, but you can save if you pay for one year upfront. Peacock Select (with ads) — $8/month or $80/year Peacock Premium (with ads) — $11/month or $110/year Peacock Premium Plus (without ads) — $17/month or $170/year Paramount+ Paramount+ used to bill its tier with Showtime as a sort of bundle, but it has since renamed its plans and focused the Showtime inclusion in its premium tier as just another bonus of paying for the higher priced plan. Paramount+ Essential (with ads) —$8/month or $60/year Paramount Premium (without ads) — $13/month or $120/year Student discounts on streaming services It pays to be a student — sometimes, at least. A number of streaming services have student discounts you can take advantage of as long as you're actively studying. What that translates to most of the time is being able to verify your student status and signing up with your .edu email address. HBO Max student discount — subscribe for $5/month (50 percent off): HBO Max offers their ad-supported tier to students for half off the usual rate. You’ll just have to verify that you’re a student through Unidays, and make note that this offer is only good for up to 12 months of service. Hulu student discount — subscribe for $2/month (75 percent off): Those with a valid student ID can get Hulu’s ad-supported tier for 75 percent off the typical rate. They’ll keep the same sale price for as long as they’re a student as well. Spotify student discount — Premium + Hulu with ads for $6/month (72 percent off): Spotify’s student offer continues to be one of the best around, giving you access to the Premium tier of the music streamer and Hulu’s ad-supported plan for only $6 monthly. Purchased separately, you’d pay $22 per month for both of the services. Plus, the first month is free when you sign up. NBA League Pass student discount — one year for $120 (40 percent off): Students can get one year of League Pass for only $10 per month, which includes access to NBA TV and the ability to watch classic and archive games on-demand. On the NBA League Pass website, look for the student discount banner at the top and follow the instructions to verify your student status. Read more streaming coverage The best live TV streaming services to cut cable The best streaming services: Netflix, Hulu, HBO Max and more The best streaming devices Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/best-streaming-service-deals-133028980.html?src=rss",
          "content": "If you’ve been shocked by how much you spend on streaming services lately, you’re not alone. Companies like Netflix, Disney, HBO Max and others have been consistently raising prices to the point where you may question if streaming is even worth it anymore. We at Engadget still think it is, but we also think you should be smart with your money — and that’s where streaming deals come in. Yes, it is possible to get discounts on services like Peacock and Paramount+, even if those deals aren’t as common as a sale on AirPods. If you’re looking to save money and still stream all of the content you want, Engadget can help by laying out the best streaming deals you can get right now, how you can save with bundles and everything you should know before paying for yet another streaming service. Best streaming deals True streaming deals can be hard to come by. Most often, they’ll pop up during the Black Friday shopping period. On occasion, we’ll see them sparingly throughout the year and they usually take the form of a discounted monthly or annual rate for a limited period of time. Also, true streaming deals are typically on the ad-supported versions of a service, but once in a while you’ll find a unicorn of a deal on a tier that has ad-free viewing. If you’re able to wait for a deal before subscribing to a streaming service, we recommend doing so. You’ll save money upfront and in the long run, and you also have the option to cancel your subscription before the price goes back up to the normal rate. Audible subscription (three months) for $3 ($42 off): From now through mid-December, you can get Amazon’s audiobook subscription for just a dollar a month for three months. Note that it will auto-renew at $15 per month after that, but you can cancel at any point. Starz (one year) for $30 ($40 off): Pay upfront for one year and you can get $40 off a Stars annual subscription. There's a month-to-month option too, which costs $5 per month for the first three months if you don't want to commit to the full year. Either option gives you access to the entire Starz TV and movie library with offline viewing and no ads. Hulu + Live TV — $64.99/mo for 3 months ($25/mo off): New and eligible returning subscribers can get three months of Hulu + Live TV at a rate of $65 per month, which is much cheaper than the current $83-per-month rate and a whopping 27 percent off the new $90-per-month rate that kicks in on October 21. In addition to live TV content, unlimited DVR and access to more than 95 live TV channels, this service also includes Disney+ and ESPN Select, so you're essentially getting three separate streaming services under this umbrella. Just remember that your subscription will be billed at the new standard $90/month rate after the first three months. Fubo Pro for $55/month for the first month ($30 off): Fubo has introductory discounts on most of its packages, and the Pro package is the least expensive plan currently listed. It offers access to 224 channels, unlimited cloud DVR and up to 10 simultaneous streams. It even includes regional sports content from the NHL, MLB and NBA. Spotify Premium Individual (one month) for $0 ($12 off): This is our favorite music streaming service for podcasts and social features. The Premium Individual plan lets you listen ad-free and skip songs at will. You can also organize your listening queue and download content for offline listening. Just be aware, your subscription will auto-renew at the end of the trial period. So if you don't want to be on the hook for the $12 monthly fee, set a reminder to cancel and go back to the free version. Streaming bundle discounts There’s more consolidation happening now than ever before in the streaming space, and that means there are more streaming bundle options. These bundles offer you access to more content with one subscription price, but those prices are typically higher than paying for a single service by itself (obviously). It may be tempting to just get the bundle, but if only one of those services in the bundle speaks to you, you’ll spend less overall by just paying for the single service. Speaking of a deep love for a single streaming service: if all of your favorite shows are on Peacock or the latest releases on HBO Max consistently bring you joy, consider paying for one year upfront. Subscribing with an annual plan usually saves you money in the long term over paying on a monthly basis. Unfortunately, not all streaming services (looking at you, Netflix) have an annual subscription option. Disney+ If you feel like Charlie Kelly trying to figure out who Pepe Silvia is when you look at Disney's streaming prices chart, you're not alone. The confusion comes from the fact that Disney owns, or has a hand in, many streaming services including Hulu and ESPN. Throw in a partnership with HBO Max and you have a ton of options to consider and, probably, whiplash to match. Here's a quick overview of popular Disney+ bundle pricing. Disney+ and Hulu bundle (with ads) — $13/month Disney+ and Hulu bundle (without ads) — $20/month Disney+, Hulu and ESPN Select (with ads) — $20/month Disney+, Hulu and ESPN Select (without ads on Disney+ and Hulu only) — $30/month Disney+, Hulu and HBO Max (with ads) — $20/month Disney+, Hulu and HBO Max (without ads) — $33/month Peacock TV Peacock doesn't have any streaming bundles available all year round, but you can save if you pay for one year upfront. Peacock Select (with ads) — $8/month or $80/year Peacock Premium (with ads) — $11/month or $110/year Peacock Premium Plus (without ads) — $17/month or $170/year Paramount+ Paramount+ used to bill its tier with Showtime as a sort of bundle, but it has since renamed its plans and focused the Showtime inclusion in its premium tier as just another bonus of paying for the higher priced plan. Paramount+ Essential (with ads) —$8/month or $60/year Paramount Premium (without ads) — $13/month or $120/year Student discounts on streaming services It pays to be a student — sometimes, at least. A number of streaming services have student discounts you can take advantage of as long as you're actively studying. What that translates to most of the time is being able to verify your student status and signing up with your .edu email address. HBO Max student discount — subscribe for $5/month (50 percent off): HBO Max offers their ad-supported tier to students for half off the usual rate. You’ll just have to verify that you’re a student through Unidays, and make note that this offer is only good for up to 12 months of service. Hulu student discount — subscribe for $2/month (75 percent off): Those with a valid student ID can get Hulu’s ad-supported tier for 75 percent off the typical rate. They’ll keep the same sale price for as long as they’re a student as well. Spotify student discount — Premium + Hulu with ads for $6/month (72 percent off): Spotify’s student offer continues to be one of the best around, giving you access to the Premium tier of the music streamer and Hulu’s ad-supported plan for only $6 monthly. Purchased separately, you’d pay $22 per month for both of the services. Plus, the first month is free when you sign up. NBA League Pass student discount — one year for $120 (40 percent off): Students can get one year of League Pass for only $10 per month, which includes access to NBA TV and the ability to watch classic and archive games on-demand. On the NBA League Pass website, look for the student discount banner at the top and follow the instructions to verify your student status. Read more streaming coverage The best live TV streaming services to cut cable The best streaming services: Netflix, Hulu, HBO Max and more The best streaming devices Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/best-streaming-service-deals-133028980.html?src=rss",
          "feed_position": 37
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/audio/headphones/best-earbuds-for-android-devices-120015765.html",
          "published_at": "Thu, 13 Nov 2025 10:01:26 +0000",
          "title": "The best earbuds for Android devices in 2025",
          "standfirst": "Justified or not, Apple’s AirPods have become the default choice for most iPhone owners in the market for new wireless earbuds. If you’re shopping for an Android phone, however, the top picks aren’t so obvious. That’s where we can help: We’ve tested dozens of wireless earbuds over the years, so we’ve broken down our top recommendations for Android users below. Whether you want powerful noise cancellation, a pair that’ll survive in the gym or just something that works particularly well with a Galaxy or Pixel phone, here are our favorite AirPods alternatives for Android. Table of contents The best Android earbuds for 2025 What to look for in wireless earbuds for Android devices How we test Android earbuds Recent updates Best Android earbuds for 2025 What to look for in wireless earbuds for Android devices Photo by Jeff Dunn / Engadget For the most part, the features you want from a set of “Android earbuds” are the same as what you want from any headphones. Great sound quality, a comfortable fit and sufficient battery life are still the foundations. Adequate water resistance is good for workouts, and nobody wants a crummy mic for making calls. Once you approach the $100 range, features like active noise cancellation (ANC), wireless charging, an ambient sound mode (which lets you better hear outside noise without turning off your music) and multipoint connectivity (the ability to pair with multiple devices simultaneously) should be expected. For Android devices specifically, there are a few extras to consider. A dedicated app that makes it easy to switch sound modes, customize the audio profile, locate your earbuds if they ever get misplaced or adjust other settings is strongly preferred. Features like Google Fast Pair or NFC-based pairing, which can help you avoid having to dig through your Bluetooth menu to connect your earbuds for the first time, are also nice perks. Some Android devices can also utilize higher-quality Bluetooth codecs such as aptX Adaptive or Sony’s LDAC — these aren’t nearly as important to audio quality as the actual architecture of your earbuds, but they can help wring out a little more detail if the buds are capable enough and you’re streaming lossless files. AptX Adaptive can also help reduce latency, which is good for streaming video or gaming. Diversity is Android’s greatest strength, but it also means that some wireless earbuds play nicer with certain devices, typically those made by the same company. Recent Samsung earbuds, for instance, come with a few perks that are only available if you use a Galaxy phone. We have a couple of recommendations related to this idea above. How we test Android earbuds Photo by Billy Steele/Engadget The best way to test earphones is simply to wear them as much as possible, so that’s what we do. We typically do this over a one- to two-week period, though embargo times occasionally force us to finish our review process a bit faster. We listen to a test playlist that includes several musical genres and podcasts, paying close attention to how each pair approaches the bass, mid and treble frequencies to get an accurate sense of its sound profile. We also test at high and low volumes to check for consistency in the tuning. We do not have access to a dummy head to take more objective measurements, but we’ll sometimes look to sites like Rtings, SoundGuys and others that do just to ensure our impressions are not wildly off-base. If a model supports custom EQ, we’ll tinker with that and use the available EQ presets to see if one sounds dramatically better than the others — though in general we base most of our impressions on the stock tuning each pair uses by default. To assess microphone quality, we record our own audio samples and take multiple calls with a partner both indoors and outside. For battery life, we play our test playlist on a loop with the volume around 75 percent and measure how long it takes for each set to drain. Where applicable, we do a thorough review of a pair’s companion app and test each available feature. While comfort is ultimately subjective, we take note of how secure each pair feels while we’re on the move. We also use certain pairs in especially crowded public spaces to get a better sense of their passive and active noise cancellation, as well as their ability to maintain a consistent Bluetooth connection. Recent updates November 2025: The lightly updated Beats Powerbeats Fit replace the older Beats Fit Pro as our top pick for working out. We’ve also noted the new Google Pixel Buds 2a as a cheaper alternative to the Pixel Buds Pro 2, which remain our recommendation for Pixel phone users. August 2025: We’ve taken another sweep to ensure our advice is still up-to-date. May 2025: We’ve checked this guide to ensure our top picks still stand and noted a couple alternatives to the Noble Fokus Rex5, since that pair has had stock issues of late. We’re also keeping an eye on how the Trump administration’s tariff policy affects the pricing and stock of our recommendations (and the consumer tech industry as a whole). All of our picks are still available in their normal price ranges today, but we’ll update this guide if that changes. February 2025: The Noble FoKus Rex5 is our new \"best for sound quality\" pick, replacing the Sennheiser Momentum True Wireless 4. Our other recommendations remain unchanged. December 2024: We’ve lightly edited this guide for clarity and ensured that our current picks are still accurate.This article originally appeared on Engadget at https://www.engadget.com/audio/headphones/best-earbuds-for-android-devices-120015765.html?src=rss",
          "content": "Justified or not, Apple’s AirPods have become the default choice for most iPhone owners in the market for new wireless earbuds. If you’re shopping for an Android phone, however, the top picks aren’t so obvious. That’s where we can help: We’ve tested dozens of wireless earbuds over the years, so we’ve broken down our top recommendations for Android users below. Whether you want powerful noise cancellation, a pair that’ll survive in the gym or just something that works particularly well with a Galaxy or Pixel phone, here are our favorite AirPods alternatives for Android. Table of contents The best Android earbuds for 2025 What to look for in wireless earbuds for Android devices How we test Android earbuds Recent updates Best Android earbuds for 2025 What to look for in wireless earbuds for Android devices Photo by Jeff Dunn / Engadget For the most part, the features you want from a set of “Android earbuds” are the same as what you want from any headphones. Great sound quality, a comfortable fit and sufficient battery life are still the foundations. Adequate water resistance is good for workouts, and nobody wants a crummy mic for making calls. Once you approach the $100 range, features like active noise cancellation (ANC), wireless charging, an ambient sound mode (which lets you better hear outside noise without turning off your music) and multipoint connectivity (the ability to pair with multiple devices simultaneously) should be expected. For Android devices specifically, there are a few extras to consider. A dedicated app that makes it easy to switch sound modes, customize the audio profile, locate your earbuds if they ever get misplaced or adjust other settings is strongly preferred. Features like Google Fast Pair or NFC-based pairing, which can help you avoid having to dig through your Bluetooth menu to connect your earbuds for the first time, are also nice perks. Some Android devices can also utilize higher-quality Bluetooth codecs such as aptX Adaptive or Sony’s LDAC — these aren’t nearly as important to audio quality as the actual architecture of your earbuds, but they can help wring out a little more detail if the buds are capable enough and you’re streaming lossless files. AptX Adaptive can also help reduce latency, which is good for streaming video or gaming. Diversity is Android’s greatest strength, but it also means that some wireless earbuds play nicer with certain devices, typically those made by the same company. Recent Samsung earbuds, for instance, come with a few perks that are only available if you use a Galaxy phone. We have a couple of recommendations related to this idea above. How we test Android earbuds Photo by Billy Steele/Engadget The best way to test earphones is simply to wear them as much as possible, so that’s what we do. We typically do this over a one- to two-week period, though embargo times occasionally force us to finish our review process a bit faster. We listen to a test playlist that includes several musical genres and podcasts, paying close attention to how each pair approaches the bass, mid and treble frequencies to get an accurate sense of its sound profile. We also test at high and low volumes to check for consistency in the tuning. We do not have access to a dummy head to take more objective measurements, but we’ll sometimes look to sites like Rtings, SoundGuys and others that do just to ensure our impressions are not wildly off-base. If a model supports custom EQ, we’ll tinker with that and use the available EQ presets to see if one sounds dramatically better than the others — though in general we base most of our impressions on the stock tuning each pair uses by default. To assess microphone quality, we record our own audio samples and take multiple calls with a partner both indoors and outside. For battery life, we play our test playlist on a loop with the volume around 75 percent and measure how long it takes for each set to drain. Where applicable, we do a thorough review of a pair’s companion app and test each available feature. While comfort is ultimately subjective, we take note of how secure each pair feels while we’re on the move. We also use certain pairs in especially crowded public spaces to get a better sense of their passive and active noise cancellation, as well as their ability to maintain a consistent Bluetooth connection. Recent updates November 2025: The lightly updated Beats Powerbeats Fit replace the older Beats Fit Pro as our top pick for working out. We’ve also noted the new Google Pixel Buds 2a as a cheaper alternative to the Pixel Buds Pro 2, which remain our recommendation for Pixel phone users. August 2025: We’ve taken another sweep to ensure our advice is still up-to-date. May 2025: We’ve checked this guide to ensure our top picks still stand and noted a couple alternatives to the Noble Fokus Rex5, since that pair has had stock issues of late. We’re also keeping an eye on how the Trump administration’s tariff policy affects the pricing and stock of our recommendations (and the consumer tech industry as a whole). All of our picks are still available in their normal price ranges today, but we’ll update this guide if that changes. February 2025: The Noble FoKus Rex5 is our new \"best for sound quality\" pick, replacing the Sennheiser Momentum True Wireless 4. Our other recommendations remain unchanged. December 2024: We’ve lightly edited this guide for clarity and ensured that our current picks are still accurate.This article originally appeared on Engadget at https://www.engadget.com/audio/headphones/best-earbuds-for-android-devices-120015765.html?src=rss",
          "feed_position": 39,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2023-06/f9e8ef90-0c55-11ee-97db-1ddd4c19f474"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/one-of-our-favorite-budgeting-apps-is-half-off-before-black-friday-174011108.html",
          "published_at": "Wed, 12 Nov 2025 20:30:38 +0000",
          "title": "One of our favorite budgeting apps is half off before Black Friday",
          "standfirst": "Monarch Money is one of our favorite budgeting apps and, fittingly enough, there's a way for newcomers to save money on a subscription right now. If you use the code MONARCHVIP at checkout, you can get an annual plan for 50 percent off. It typically costs $100, but you can get 12 months of access for $50 with this code. There are some key caveats here. The discount is only for new users, and it can't be combined with other offers. The code only works when you sign up through the web. You can't redeem it through the Monarch mobile app. We feel that Monarch has a steeper learning curve than some other budget trackers and that certain aspects of the app are slightly more complex than they probably need to be. But it offers a great deal of customization and granularity, which outweighs our misgivings. On the main dashboard, you'll see your net worth along with your latest transactions, spending versus the previous month, your income so far for the month and details about upcoming bills, your investments and goals you've set. There's also a link to a month-in-review page, which offers an in-depth overview of what's been happening with your money that month. You'll also be able to take a peek at how your net worth has changed over time. Monarch can connect to your bank and track Apple Card, Apple Cash and Savings accounts. It can pull in your transactions and balance history automatically and detect your recurring expenses and income. The app can even keep your car valuation up to date. While it might take a little work to set up Monarch (and you might have to tweak things here and there), it's a detailed budgeting app that can help you keep better track of your income, expenditure and net worth. If you're a former Mint user (RIP), Monarch Money is a great alternative if you haven't yet found a Mint replacement. But it's worth mentioning that our favorite Mint replacement service, Quicken Simplifi, also has a sale going on right now. It's offering 50 percent off when you sign up for an annual subscription, billed at $3 per month with the discount. That comes out to $36 for the first year. This article originally appeared on Engadget at https://www.engadget.com/deals/one-of-our-favorite-budgeting-apps-is-half-off-before-black-friday-174011108.html?src=rss",
          "content": "Monarch Money is one of our favorite budgeting apps and, fittingly enough, there's a way for newcomers to save money on a subscription right now. If you use the code MONARCHVIP at checkout, you can get an annual plan for 50 percent off. It typically costs $100, but you can get 12 months of access for $50 with this code. There are some key caveats here. The discount is only for new users, and it can't be combined with other offers. The code only works when you sign up through the web. You can't redeem it through the Monarch mobile app. We feel that Monarch has a steeper learning curve than some other budget trackers and that certain aspects of the app are slightly more complex than they probably need to be. But it offers a great deal of customization and granularity, which outweighs our misgivings. On the main dashboard, you'll see your net worth along with your latest transactions, spending versus the previous month, your income so far for the month and details about upcoming bills, your investments and goals you've set. There's also a link to a month-in-review page, which offers an in-depth overview of what's been happening with your money that month. You'll also be able to take a peek at how your net worth has changed over time. Monarch can connect to your bank and track Apple Card, Apple Cash and Savings accounts. It can pull in your transactions and balance history automatically and detect your recurring expenses and income. The app can even keep your car valuation up to date. While it might take a little work to set up Monarch (and you might have to tweak things here and there), it's a detailed budgeting app that can help you keep better track of your income, expenditure and net worth. If you're a former Mint user (RIP), Monarch Money is a great alternative if you haven't yet found a Mint replacement. But it's worth mentioning that our favorite Mint replacement service, Quicken Simplifi, also has a sale going on right now. It's offering 50 percent off when you sign up for an annual subscription, billed at $3 per month with the discount. That comes out to $36 for the first year. This article originally appeared on Engadget at https://www.engadget.com/deals/one-of-our-favorite-budgeting-apps-is-half-off-before-black-friday-174011108.html?src=rss",
          "feed_position": 43
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/gaming/pc/valve-announces-new-steam-machine-and-steam-controller-182836847.html",
          "published_at": "Wed, 12 Nov 2025 19:31:01 +0000",
          "title": "Valve announces new Steam Machine and Steam Controller",
          "standfirst": "Valve is making another run at offering a console-style experience in your living room. The company has announced a new Steam Machine and Steam Controller that let you play PC games on your TV in the same way the Steam Deck lets you play them on the go. Better yet, it’s planning to release them both in early 2026.The Steam Machine works like a console, but is technically a compact PC running Linux-based SteamOS. The boxy device features a customizable front plate and LED light strip, with a USB-A port and a microSD card slot available up front, and DisplayPort 1.4, HDMI 2.0 and ethernet ports on the back. Inside, the Steam Machine is powered by what Valve describes as a “semi-custom AMD Zen 4” CPU and a “semi-Custom AMD RDNA3 ” GPU with “16GB DDR5 + 8GB GDDR6 VRAM” and either 512GB or 2TB of SSD storage.Valve says the Steam Machine has “roughly six times the horsepower” of the Steam Deck, and is capable of supporting 4K gaming at 60 FPS with FSR. Interestingly, Valve is also pitching the device as a way to stream more demanding games to your Steam Deck, the Steam Frame VR headset the company also announced today or any device running Steam Link.Someone holding the new Steam Controller, with trackpads visible.ValveWhile you could use the Steam Machine with a traditional Bluetooth controller, Valve has created its own solution. The new Steam Controller puts all of the various control methods of the Steam Deck into a wireless controller. That includes sticks, face buttons, grip buttons, triggers and bumpers, but also trackpads for mouse controls and gyro controls, too. The Steam Controller works over both Bluetooth or a wired connection, and Valve is also including a charging dongle that doubles as a wireless transmitter for the fastest possible connection.Like the original Steam Controller, your input method can be individually customized for each game, and profiles can be shared. Valve also says the new controller will work with any device that runs Steam, including the Steam Deck, Steam Machine and Steam Frame.Missing from Valve’s announcement is any kind of official price. Early hands-ons with both the Steam Machine and Steam Controller suggest Valve wants the devices to be competitively priced with equivalent PCs and game controllers. Given the extra power and features, though, it seems like they might not be as much of a deal as the $400 Steam Deck was at launch.This article originally appeared on Engadget at https://www.engadget.com/gaming/pc/valve-announces-new-steam-machine-and-steam-controller-182836847.html?src=rss",
          "content": "Valve is making another run at offering a console-style experience in your living room. The company has announced a new Steam Machine and Steam Controller that let you play PC games on your TV in the same way the Steam Deck lets you play them on the go. Better yet, it’s planning to release them both in early 2026.The Steam Machine works like a console, but is technically a compact PC running Linux-based SteamOS. The boxy device features a customizable front plate and LED light strip, with a USB-A port and a microSD card slot available up front, and DisplayPort 1.4, HDMI 2.0 and ethernet ports on the back. Inside, the Steam Machine is powered by what Valve describes as a “semi-custom AMD Zen 4” CPU and a “semi-Custom AMD RDNA3 ” GPU with “16GB DDR5 + 8GB GDDR6 VRAM” and either 512GB or 2TB of SSD storage.Valve says the Steam Machine has “roughly six times the horsepower” of the Steam Deck, and is capable of supporting 4K gaming at 60 FPS with FSR. Interestingly, Valve is also pitching the device as a way to stream more demanding games to your Steam Deck, the Steam Frame VR headset the company also announced today or any device running Steam Link.Someone holding the new Steam Controller, with trackpads visible.ValveWhile you could use the Steam Machine with a traditional Bluetooth controller, Valve has created its own solution. The new Steam Controller puts all of the various control methods of the Steam Deck into a wireless controller. That includes sticks, face buttons, grip buttons, triggers and bumpers, but also trackpads for mouse controls and gyro controls, too. The Steam Controller works over both Bluetooth or a wired connection, and Valve is also including a charging dongle that doubles as a wireless transmitter for the fastest possible connection.Like the original Steam Controller, your input method can be individually customized for each game, and profiles can be shared. Valve also says the new controller will work with any device that runs Steam, including the Steam Deck, Steam Machine and Steam Frame.Missing from Valve’s announcement is any kind of official price. Early hands-ons with both the Steam Machine and Steam Controller suggest Valve wants the devices to be competitively priced with equivalent PCs and game controllers. Given the extra power and features, though, it seems like they might not be as much of a deal as the $400 Steam Deck was at launch.This article originally appeared on Engadget at https://www.engadget.com/gaming/pc/valve-announces-new-steam-machine-and-steam-controller-182836847.html?src=rss",
          "feed_position": 47,
          "image_url": "https://d29szjachogqwa.cloudfront.net/images/user-uploaded/Valve-Steam-Controller.jpg"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/weibos-new-open-source-ai-model-vibethinker-1-5b-outperforms-deepseek-r1-on",
          "published_at": "Wed, 12 Nov 2025 19:31:00 GMT",
          "title": "Weibo's new open source AI model VibeThinker-1.5B outperforms DeepSeek-R1 on $7,800 post-training budget",
          "standfirst": "Another day in late 2025, another impressive result from a Chinese company in open source artificial intelligence.Chinese social networking company Weibo&#x27;s AI division recently released its open source VibeThinker-1.5B—a 1.5 billion parameter large language model (LLM) that is a fine-tuned variant of rival Chinese tech firm Alibaba&#x27;s Qwen2.5-Math-1.5B. It&#x27;s available now for free download and usage by researchers and enterprise developers—even for commercial purposes—under a permissive MIT License on Hugging Face, GitHub and ModelScope, with a technical report on open access science publishing site arxiv.org.And yet, despite its compact size, VibeThinker-1.5B achieves benchmark-topping reasoning performance on math and code tasks, rivaling or surpassing models hundreds of times its size, even outperforming Chinese rival DeepSeek&#x27;s famed R1 that went viral at the start of this year—a 671-billion parameter model—on formal reasoning benchmark.It further eclipses Mistral AI&#x27;s Magistral Medium and holds its own against Anthropic&#x27;s Claude Opus 4 and OpenAI&#x27;s gpt-oss-20B Medium, all while requiring a fraction of the infrastructure and investment.It also does so having been post-trained on a budget of merely $7800 USD for compute resources (3900 GPU hours on Nvidia H800s) — far less than the tens, or even hundreds, of thousands of dollars typically required to fine-tune models of similar or larger scale.Recall this is not the total cost of the model&#x27;s development, however: LLMs are trained in stages. First comes pre-training, when the model learns basic language structure and general knowledge by predicting the next word across enormous amounts of text from the internet, books, and articles. This gives it fluency but not much sense of how to follow instructions or hold a conversationPost-training comes next, using much smaller, higher-quality datasets—typically collections of example questions, prompts, and expert-written answers—to teach the model how to respond helpfully, reason through problems, and align with human expectations. Still, Weibo&#x27;s post-training cost effectiveness on VibeThinker-1.5B is noteworthy and should be commended.The open-source release upends assumptions about parameter scale, compute intensity, and the minimum viable size for high-performance LLMs.A Different Training Approach: Spectrum-to-SignalVibeThinker-1.5B owes its performance not to scale, but to the training framework behind it: the Spectrum-to-Signal Principle (SSP).Instead of optimizing a model purely for single-answer correctness (Pass@1), the SSP framework decouples supervised fine-tuning (SFT) and reinforcement learning (RL) into two distinct phases with different goals:SFT (“Spectrum Phase”): The model is trained to maximize diversity across potential correct answers, improving its Pass@K score. This builds a wide range of plausible solution paths.RL (“Signal Phase”): A second-stage reinforcement learning system (called MaxEnt-Guided Policy Optimization, or MGPO) is used to identify and amplify the most correct paths from this diverse solution pool. MGPO prioritizes problems where the model is most uncertain, using entropy-based weighting to focus learning.The authors argue this separation allows small models to explore reasoning space more effectively—achieving signal amplification without relying on massive parameter counts.VibeThinker-1.5B makes a compelling case that the industry’s reliance on parameter scaling as the only route to better reasoning performance may be outdated. By adopting a diversity-first training pipeline, WeiboAI has shown that smaller, more accessible models can match and even outperform billion-dollar systems in logic-heavy tasks.The low resource footprint is among the most significant aspects of VibeThinker-1.5B. At under $8,000, the post-training cost is 30–60x lower than models like DeepSeek R1 and MiniMax-M1, which cost between $294K and $535K to train.Performance Across DomainsDespite its small size, VibeThinker-1.5B delivers cross-domain reasoning that outpaces many larger open-source and commercial models:ModelAIME25LiveCodeBench v6GPQA-DiamondVibeThinker-1.5B74.451.146.7GPT-OSS-20B-Medium72.154.966.0Claude Opus 469.256.679.6MiniMax M1 (456B)74.662.369.2DeepSeek R1 (671B)70.065.971.5Kimi K2 (1.09T)49.553.775.1VibeThinker was benchmarked against both reasoning-centric models (Magistral, Claude, OpenAI o3-mini) and non-reasoning LLMs (GPT-4.1, Kimi K2, DeepSeek V3). Across structured reasoning benchmarks, the model consistently outperformed non-reasoning models, regardless of size:On AIME24 (math), it beat Kimi K2 (1.09T) by over 10 points (80.3 vs. 69.6).On LiveCodeBench v6, it surpassed Claude Opus 4 (51.1 vs. 47.4).On GPQA, it scored below GPT-4.1 and Claude, but still doubled its base model (from 16.4 to 46.7).This supports the authors’ claim that size is not the only path to reasoning capability—with proper training design, smaller models can reach or even exceed the performance of far larger systems in targeted tasks.Notably, it achieves parity with models hundreds of times larger on math and code, though it lags behind in general knowledge reasoning (GPQA), where larger models maintain an edge.This suggests a potential specialization trade-off: while VibeThinker excels at structured logical tasks, it has less capacity for wide-ranging encyclopedic recall, a known limitation of smaller architectures.Guidance for Enterprise AdoptionThe release includes recommended inference settings (temperature = 0.6, top_p = 0.95, max tokens = 40960).The model is small enough to be deployed on edge devices, including mobile phones and vehicle-embedded systems, while inference costs are estimated to be 20–70x cheaper than with large models.This positions VibeThinker-1.5B not just as a research achievement, but as a potential foundation for cost-efficient, locally deployable reasoning systems.Weibo’s Strategy and Market PositionWeibo, launched by Sina Corporation in 2009, remains a cornerstone of China’s social media ecosystem. Often described as China’s version of X (formerly Twitter), the platform blends microblogging, multimedia content, and trending-topic features with a regulatory environment shaped by tight government oversight. Despite counting 600 million monthly active users (more than twice that of X), investors are not optimistic about its advertising revenue growth potential in the near term, and Weibo is navigating intensifying competition from video-first platforms like Douyin, which are drawing younger users and increasing time-spent elsewhere. In response, Weibo has leaned into creator-economy monetization, live-streaming, and vertical video—adding tools for influencer engagement, e-commerce integration, and richer analytics for brands.The platform’s role as a digital public square also makes it a focus of regulatory scrutiny. Chinese authorities continue to apply pressure on issues ranging from content governance to data security. In September 2025, Weibo was among the platforms cited in official warnings, highlighting its ongoing exposure to policy risks.Weibo’s push into AI R&D—exemplified by the release of VibeThinker-1.5B—signals a shift in ambition. Beyond being a media platform, Weibo is positioning itself as a player in the next phase of Chinese AI development, using its capital reserves, user behavior data, and in-house research capacity to pursue adjacent technical domains.What It Means for Enterprise Technical Decision MakersFor engineering leaders and enterprise AI teams, VibeThinker’s release has practical implications for everything from orchestration pipelines to cost modeling. A 1.5B-parameter model that outperforms 100x larger models on math and programming tasks doesn’t just save compute—it shifts the architectural balance. It enables LLM inference on constrained infrastructure, reduces latency at the edge, and lowers the barrier to entry for applications that otherwise would have required API access to closed, frontier-scale models.That matters for enterprise ML leads trying to deploy reasoning-capable agents within existing systems, or for platform owners tasked with integrating LLMs into automated workflows. It also speaks to those running reinforcement learning from human feedback (RLHF) pipelines or managing inference optimization across hybrid cloud environments. The model’s post-training methodology—particularly its entropy-targeted reinforcement learning approach—offers a roadmap for teams looking to refine smaller checkpoints instead of relying on large-scale pretraining.VibeThinker’s benchmark transparency and data decontamination steps also address another emerging priority in enterprise AI: auditability. While its performance on general-knowledge tests still trails large frontier models, its task-specific reliability makes it an attractive candidate for controlled environments where correctness matters more than coverage.In short, VibeThinker-1.5B isn’t just a research milestone—it’s a strong candidate for practical enterprise use, deployment and learnings. It suggests that a new class of compact, reasoning-optimized models is viable for enterprise use cases that were previously the domain of far larger systems. For organizations trying to balance cost, latency, interpretability, and control, it’s a good new option to the long, growing list of Chinese open source offerings.",
          "content": "Another day in late 2025, another impressive result from a Chinese company in open source artificial intelligence.Chinese social networking company Weibo&#x27;s AI division recently released its open source VibeThinker-1.5B—a 1.5 billion parameter large language model (LLM) that is a fine-tuned variant of rival Chinese tech firm Alibaba&#x27;s Qwen2.5-Math-1.5B. It&#x27;s available now for free download and usage by researchers and enterprise developers—even for commercial purposes—under a permissive MIT License on Hugging Face, GitHub and ModelScope, with a technical report on open access science publishing site arxiv.org.And yet, despite its compact size, VibeThinker-1.5B achieves benchmark-topping reasoning performance on math and code tasks, rivaling or surpassing models hundreds of times its size, even outperforming Chinese rival DeepSeek&#x27;s famed R1 that went viral at the start of this year—a 671-billion parameter model—on formal reasoning benchmark.It further eclipses Mistral AI&#x27;s Magistral Medium and holds its own against Anthropic&#x27;s Claude Opus 4 and OpenAI&#x27;s gpt-oss-20B Medium, all while requiring a fraction of the infrastructure and investment.It also does so having been post-trained on a budget of merely $7800 USD for compute resources (3900 GPU hours on Nvidia H800s) — far less than the tens, or even hundreds, of thousands of dollars typically required to fine-tune models of similar or larger scale.Recall this is not the total cost of the model&#x27;s development, however: LLMs are trained in stages. First comes pre-training, when the model learns basic language structure and general knowledge by predicting the next word across enormous amounts of text from the internet, books, and articles. This gives it fluency but not much sense of how to follow instructions or hold a conversationPost-training comes next, using much smaller, higher-quality datasets—typically collections of example questions, prompts, and expert-written answers—to teach the model how to respond helpfully, reason through problems, and align with human expectations. Still, Weibo&#x27;s post-training cost effectiveness on VibeThinker-1.5B is noteworthy and should be commended.The open-source release upends assumptions about parameter scale, compute intensity, and the minimum viable size for high-performance LLMs.A Different Training Approach: Spectrum-to-SignalVibeThinker-1.5B owes its performance not to scale, but to the training framework behind it: the Spectrum-to-Signal Principle (SSP).Instead of optimizing a model purely for single-answer correctness (Pass@1), the SSP framework decouples supervised fine-tuning (SFT) and reinforcement learning (RL) into two distinct phases with different goals:SFT (“Spectrum Phase”): The model is trained to maximize diversity across potential correct answers, improving its Pass@K score. This builds a wide range of plausible solution paths.RL (“Signal Phase”): A second-stage reinforcement learning system (called MaxEnt-Guided Policy Optimization, or MGPO) is used to identify and amplify the most correct paths from this diverse solution pool. MGPO prioritizes problems where the model is most uncertain, using entropy-based weighting to focus learning.The authors argue this separation allows small models to explore reasoning space more effectively—achieving signal amplification without relying on massive parameter counts.VibeThinker-1.5B makes a compelling case that the industry’s reliance on parameter scaling as the only route to better reasoning performance may be outdated. By adopting a diversity-first training pipeline, WeiboAI has shown that smaller, more accessible models can match and even outperform billion-dollar systems in logic-heavy tasks.The low resource footprint is among the most significant aspects of VibeThinker-1.5B. At under $8,000, the post-training cost is 30–60x lower than models like DeepSeek R1 and MiniMax-M1, which cost between $294K and $535K to train.Performance Across DomainsDespite its small size, VibeThinker-1.5B delivers cross-domain reasoning that outpaces many larger open-source and commercial models:ModelAIME25LiveCodeBench v6GPQA-DiamondVibeThinker-1.5B74.451.146.7GPT-OSS-20B-Medium72.154.966.0Claude Opus 469.256.679.6MiniMax M1 (456B)74.662.369.2DeepSeek R1 (671B)70.065.971.5Kimi K2 (1.09T)49.553.775.1VibeThinker was benchmarked against both reasoning-centric models (Magistral, Claude, OpenAI o3-mini) and non-reasoning LLMs (GPT-4.1, Kimi K2, DeepSeek V3). Across structured reasoning benchmarks, the model consistently outperformed non-reasoning models, regardless of size:On AIME24 (math), it beat Kimi K2 (1.09T) by over 10 points (80.3 vs. 69.6).On LiveCodeBench v6, it surpassed Claude Opus 4 (51.1 vs. 47.4).On GPQA, it scored below GPT-4.1 and Claude, but still doubled its base model (from 16.4 to 46.7).This supports the authors’ claim that size is not the only path to reasoning capability—with proper training design, smaller models can reach or even exceed the performance of far larger systems in targeted tasks.Notably, it achieves parity with models hundreds of times larger on math and code, though it lags behind in general knowledge reasoning (GPQA), where larger models maintain an edge.This suggests a potential specialization trade-off: while VibeThinker excels at structured logical tasks, it has less capacity for wide-ranging encyclopedic recall, a known limitation of smaller architectures.Guidance for Enterprise AdoptionThe release includes recommended inference settings (temperature = 0.6, top_p = 0.95, max tokens = 40960).The model is small enough to be deployed on edge devices, including mobile phones and vehicle-embedded systems, while inference costs are estimated to be 20–70x cheaper than with large models.This positions VibeThinker-1.5B not just as a research achievement, but as a potential foundation for cost-efficient, locally deployable reasoning systems.Weibo’s Strategy and Market PositionWeibo, launched by Sina Corporation in 2009, remains a cornerstone of China’s social media ecosystem. Often described as China’s version of X (formerly Twitter), the platform blends microblogging, multimedia content, and trending-topic features with a regulatory environment shaped by tight government oversight. Despite counting 600 million monthly active users (more than twice that of X), investors are not optimistic about its advertising revenue growth potential in the near term, and Weibo is navigating intensifying competition from video-first platforms like Douyin, which are drawing younger users and increasing time-spent elsewhere. In response, Weibo has leaned into creator-economy monetization, live-streaming, and vertical video—adding tools for influencer engagement, e-commerce integration, and richer analytics for brands.The platform’s role as a digital public square also makes it a focus of regulatory scrutiny. Chinese authorities continue to apply pressure on issues ranging from content governance to data security. In September 2025, Weibo was among the platforms cited in official warnings, highlighting its ongoing exposure to policy risks.Weibo’s push into AI R&D—exemplified by the release of VibeThinker-1.5B—signals a shift in ambition. Beyond being a media platform, Weibo is positioning itself as a player in the next phase of Chinese AI development, using its capital reserves, user behavior data, and in-house research capacity to pursue adjacent technical domains.What It Means for Enterprise Technical Decision MakersFor engineering leaders and enterprise AI teams, VibeThinker’s release has practical implications for everything from orchestration pipelines to cost modeling. A 1.5B-parameter model that outperforms 100x larger models on math and programming tasks doesn’t just save compute—it shifts the architectural balance. It enables LLM inference on constrained infrastructure, reduces latency at the edge, and lowers the barrier to entry for applications that otherwise would have required API access to closed, frontier-scale models.That matters for enterprise ML leads trying to deploy reasoning-capable agents within existing systems, or for platform owners tasked with integrating LLMs into automated workflows. It also speaks to those running reinforcement learning from human feedback (RLHF) pipelines or managing inference optimization across hybrid cloud environments. The model’s post-training methodology—particularly its entropy-targeted reinforcement learning approach—offers a roadmap for teams looking to refine smaller checkpoints instead of relying on large-scale pretraining.VibeThinker’s benchmark transparency and data decontamination steps also address another emerging priority in enterprise AI: auditability. While its performance on general-knowledge tests still trails large frontier models, its task-specific reliability makes it an attractive candidate for controlled environments where correctness matters more than coverage.In short, VibeThinker-1.5B isn’t just a research milestone—it’s a strong candidate for practical enterprise use, deployment and learnings. It suggests that a new class of compact, reasoning-optimized models is viable for enterprise use cases that were previously the domain of far larger systems. For organizations trying to balance cost, latency, interpretability, and control, it’s a good new option to the long, growing list of Chinese open source offerings.",
          "feed_position": 2,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/4s7atIbhZpkjUNIE9NqvrE/de645440ccc36273944e9ba58f78fea7/ChatGPT_Image_Nov_12__2025__02_29_18_PM.png?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/ar-vr/valves-steam-frame-vr-headset-is-finally-official-and-its-coming-in-2026-181909387.html",
          "published_at": "Wed, 12 Nov 2025 19:26:01 +0000",
          "title": "Valve’s Steam Frame VR headset is finally official and it's coming in 2026",
          "standfirst": "Valve made a triumphant return to the hardware market with the Steam Deck and its OLED-toting counterpart, and now it’s having another crack at virtual reality with the Steam Frame. The Steam Frame is the long-rumored headset from Valve that had previously been codenamed \"Deckard.\" The company also announced a new Steam controller and PC called the Steam Machine. All three devices are coming in early 2026. Valve is holding off on announcing pricing and exact availability of the new hardware. There are Steam Frame dev kits available for developers. Valve says the Steam Frame is a wireless, \"streaming-first\" headset and you can hop into your games as soon as you pop it on. It supports both VR and flatscreen games. The company made a plug-and-play 6GHz wireless adapter that you slot into your PC (or Steam Machine). It has a dual-radio setup to help minimize interference, with one radio dedicated to streaming audio and visuals to the headset, and the other for Wi-Fi. A standalone VR headset But you don't need a PC to play games on the Steam Frame. As with Meta Quest headsets, it can run games as a standalone device. The headset has a Snapdragon 8 Gen 3 chipset, 16GB of RAM and up to 1TB of built-in UFS storage. There's a microSD card slot, along with support for Wi-Fi 7 and Bluetooth 5.3. Of course, the headset is powered by SteamOS. As with the Steam Deck and Steam Machine, there'll be a Steam Frame verified program, so you can see at a glance which games will run on the Steam Frame in standalone mode. What's more, the Steam Frame will support Android games. It seems Valve is hoping that developers who made games and VR experiences for Android-based headsets (such as the Meta Quest lineup) will bring them to Steam. The Steam Frame runs on a rechargeable 21.6Wh Li-ion battery. There's one USB-C 2.0 port at the back that you'll use for both charging and data transfers. You can recharge the battery at a rate of up to 45W. It's unclear how long the Steam Frame's battery will run on a charge. The battery is positioned on the rear of the headstrap. So you won't necessarily need to have an external battery pack that's attached to the system by an annoying cable. It'll be possible to swap the standard headstrap (into which the audio drivers are integrated) for a different option, perhaps one with a larger battery. Even with the battery built into the headstrap, Valve says the Steam Frame weighs just under a pound at 440 grams. The core module — the front part — is 185 grams (6.5oz) and the headstrap weighs 245 grams (8.6 ounces). Image optimization tech The Steam Frame has an optimization feature called Foveated Streaming. Valve says this uses low-latency eye-tracking (powered by two internal cameras) to optimize the detail in the image wherever your eyes are looking. The company claims it can offer a \"10x improvement in image quality and effective bandwidth.\" Foveated Streaming is said to work for every game in your Steam library. The headset has dual 2160 x 2160 LCD panels with refresh rates of up to 144Hz, a field of view of up to 110 degrees and an IPD target range of 60mm to 70mm. Valve added that \"thin and light custom pancake lenses provide edge-to-edge sharpness and a large eye box.\" The company says the maximum width for eye glasses is 140mm. As for audio, the Steam Frame has dual stereo speakers on each side with support for high-fidelity audio. Valve says the speakers on each side are \"oriented in opposite directions to cancel out vibrations,\" which can impact the tracking system. Speaking of which, the headset has four high-res monochrome cameras for controller and headset tracking — the Steam Frame uses inside-out tracking. Valve says there are infrared LEDs on the outside of the device that can help support tracking in dark environments. There's monochrome passthrough support too. Valve Steam Frame controllers Naturally, you'll need a way to play all of the games, so the headset comes with a pair of Steam Frame controllers. The headset tracks the positions of the controllers for VR games, with full 6-DOF tracking and IMU support. They have a split gamepad format with a D-pad, thumbsticks, ABXY buttons, triggers and bumpers. They're designed to work with your entire Steam library, and they certainly look a bit more intuitive than the PlayStation VR2 controllers. Rather than going down the Hall effect route, Valve opted for magnetic thumbsticks, which support capacitive finger tracking. Each controller is said to run for around 40 hours before you'll have to swap out the AA battery that powers it. If you'd rather play games on the Steam Frame with the new Steam Controller, you'll absolutely be able to do that. The Steam Frame is far from Valve's first VR headset. It released the Valve Index in 2019, and previously worked with HTC on its Vive headsets, which were initially consumer VR products before HTC shifted its focus to business and enterprise. While none of Valve’s previous PC-focused headsets had the mainstream impact of Meta’s Quest lineup or arguably even PlayStation VR (which by all accounts is still an active platform, not that Sony’s release calendar backs it up), the company is responsible for what is probably the medium’s greatest-ever game in Half-Life: Alyx. And with SteamOS on the Steam Deck being such a hit that other companies are practically begging Valve to let them put it in their own rival handhelds, it’s easy to imagine the Steam Frame becoming a serious rival to the Meta Quest.This article originally appeared on Engadget at https://www.engadget.com/ar-vr/valves-steam-frame-vr-headset-is-finally-official-and-its-coming-in-2026-181909387.html?src=rss",
          "content": "Valve made a triumphant return to the hardware market with the Steam Deck and its OLED-toting counterpart, and now it’s having another crack at virtual reality with the Steam Frame. The Steam Frame is the long-rumored headset from Valve that had previously been codenamed \"Deckard.\" The company also announced a new Steam controller and PC called the Steam Machine. All three devices are coming in early 2026. Valve is holding off on announcing pricing and exact availability of the new hardware. There are Steam Frame dev kits available for developers. Valve says the Steam Frame is a wireless, \"streaming-first\" headset and you can hop into your games as soon as you pop it on. It supports both VR and flatscreen games. The company made a plug-and-play 6GHz wireless adapter that you slot into your PC (or Steam Machine). It has a dual-radio setup to help minimize interference, with one radio dedicated to streaming audio and visuals to the headset, and the other for Wi-Fi. A standalone VR headset But you don't need a PC to play games on the Steam Frame. As with Meta Quest headsets, it can run games as a standalone device. The headset has a Snapdragon 8 Gen 3 chipset, 16GB of RAM and up to 1TB of built-in UFS storage. There's a microSD card slot, along with support for Wi-Fi 7 and Bluetooth 5.3. Of course, the headset is powered by SteamOS. As with the Steam Deck and Steam Machine, there'll be a Steam Frame verified program, so you can see at a glance which games will run on the Steam Frame in standalone mode. What's more, the Steam Frame will support Android games. It seems Valve is hoping that developers who made games and VR experiences for Android-based headsets (such as the Meta Quest lineup) will bring them to Steam. The Steam Frame runs on a rechargeable 21.6Wh Li-ion battery. There's one USB-C 2.0 port at the back that you'll use for both charging and data transfers. You can recharge the battery at a rate of up to 45W. It's unclear how long the Steam Frame's battery will run on a charge. The battery is positioned on the rear of the headstrap. So you won't necessarily need to have an external battery pack that's attached to the system by an annoying cable. It'll be possible to swap the standard headstrap (into which the audio drivers are integrated) for a different option, perhaps one with a larger battery. Even with the battery built into the headstrap, Valve says the Steam Frame weighs just under a pound at 440 grams. The core module — the front part — is 185 grams (6.5oz) and the headstrap weighs 245 grams (8.6 ounces). Image optimization tech The Steam Frame has an optimization feature called Foveated Streaming. Valve says this uses low-latency eye-tracking (powered by two internal cameras) to optimize the detail in the image wherever your eyes are looking. The company claims it can offer a \"10x improvement in image quality and effective bandwidth.\" Foveated Streaming is said to work for every game in your Steam library. The headset has dual 2160 x 2160 LCD panels with refresh rates of up to 144Hz, a field of view of up to 110 degrees and an IPD target range of 60mm to 70mm. Valve added that \"thin and light custom pancake lenses provide edge-to-edge sharpness and a large eye box.\" The company says the maximum width for eye glasses is 140mm. As for audio, the Steam Frame has dual stereo speakers on each side with support for high-fidelity audio. Valve says the speakers on each side are \"oriented in opposite directions to cancel out vibrations,\" which can impact the tracking system. Speaking of which, the headset has four high-res monochrome cameras for controller and headset tracking — the Steam Frame uses inside-out tracking. Valve says there are infrared LEDs on the outside of the device that can help support tracking in dark environments. There's monochrome passthrough support too. Valve Steam Frame controllers Naturally, you'll need a way to play all of the games, so the headset comes with a pair of Steam Frame controllers. The headset tracks the positions of the controllers for VR games, with full 6-DOF tracking and IMU support. They have a split gamepad format with a D-pad, thumbsticks, ABXY buttons, triggers and bumpers. They're designed to work with your entire Steam library, and they certainly look a bit more intuitive than the PlayStation VR2 controllers. Rather than going down the Hall effect route, Valve opted for magnetic thumbsticks, which support capacitive finger tracking. Each controller is said to run for around 40 hours before you'll have to swap out the AA battery that powers it. If you'd rather play games on the Steam Frame with the new Steam Controller, you'll absolutely be able to do that. The Steam Frame is far from Valve's first VR headset. It released the Valve Index in 2019, and previously worked with HTC on its Vive headsets, which were initially consumer VR products before HTC shifted its focus to business and enterprise. While none of Valve’s previous PC-focused headsets had the mainstream impact of Meta’s Quest lineup or arguably even PlayStation VR (which by all accounts is still an active platform, not that Sony’s release calendar backs it up), the company is responsible for what is probably the medium’s greatest-ever game in Half-Life: Alyx. And with SteamOS on the Steam Deck being such a hit that other companies are practically begging Valve to let them put it in their own rival handhelds, it’s easy to imagine the Steam Frame becoming a serious rival to the Meta Quest.This article originally appeared on Engadget at https://www.engadget.com/ar-vr/valves-steam-frame-vr-headset-is-finally-official-and-its-coming-in-2026-181909387.html?src=rss",
          "feed_position": 48,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2025-11/5cfcae20-bffd-11f0-9bdf-3763c02a40b3"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/how-deductive-ai-saved-doordash-1-000-engineering-hours-by-automating",
          "published_at": "Wed, 12 Nov 2025 14:00:00 GMT",
          "title": "How Deductive AI saved DoorDash 1,000 engineering hours by automating software debugging",
          "standfirst": "As software systems grow more complex and AI tools generate code faster than ever, a fundamental problem is getting worse: Engineers are drowning in debugging work, spending up to half their time hunting down the causes of software failures instead of building new products. The challenge has become so acute that it&#x27;s creating a new category of tooling — AI agents that can diagnose production failures in minutes instead of hours.Deductive AI, a startup emerging from stealth mode Wednesday, believes it has found a solution by applying reinforcement learning — the same technology that powers game-playing AI systems — to the messy, high-stakes world of production software incidents. The company announced it has raised $7.5 million in seed funding led by CRV, with participation from Databricks Ventures, Thomvest Ventures, and PrimeSet, to commercialize what it calls \"AI SRE agents\" that can diagnose and help fix software failures at machine speed.The pitch resonates with a growing frustration inside engineering organizations: Modern observability tools can show that something broke, but they rarely explain why. When a production system fails at 3 a.m., engineers still face hours of manual detective work, cross-referencing logs, metrics, deployment histories, and code changes across dozens of interconnected services to identify the root cause.\"The complexities and inter-dependencies of modern infrastructure means that investigating the root cause of an outage or incident can feel like searching for a needle in a haystack, except the haystack is the size of a football field, it&#x27;s made of a million other needles, it&#x27;s constantly reshuffling itself, and is on fire — and every second you don&#x27;t find it equals lost revenue,\" said Sameer Agarwal, Deductive&#x27;s co-founder and chief technology officer, in an exclusive interview with VentureBeat.Deductive&#x27;s system builds what the company calls a \"knowledge graph\" that maps relationships across codebases, telemetry data, engineering discussions, and internal documentation. When an incident occurs, multiple AI agents work together to form hypotheses, test them against live system evidence, and converge on a root cause — mimicking the investigative workflow of experienced site reliability engineers, but completing the process in minutes rather than hours.The technology has already shown measurable impact at some of the world&#x27;s most demanding production environments. DoorDash&#x27;s advertising platform, which runs real-time auctions that must complete in under 100 milliseconds, has integrated Deductive into its incident response workflow. The company has set an ambitious 2026 goal of resolving production incidents within 10 minutes.\"Our Ads Platform operates at a pace where manual, slow-moving investigations are no longer viable. Every minute of downtime directly affects company revenue,\" said Shahrooz Ansari, Senior Director of Engineering at DoorDash, in an interview with VentureBeat. \"Deductive has become a critical extension of our team, rapidly synthesizing signals across dozens of services and surfacing the insights that matter—within minutes.\"DoorDash estimates that Deductive has root-caused approximately 100 production incidents over the past few months, translating to more than 1,000 hours of annual engineering productivity and a revenue impact \"in millions of dollars,\" according to Ansari. At location intelligence company Foursquare, Deductive reduced the time to diagnose Apache Spark job failures by 90% —t urning a process that previously took hours or days into one that completes in under 10 minutes — while generating over $275,000 in annual savings.Why AI-generated code is creating a debugging crisisThe timing of Deductive&#x27;s launch reflects a brewing tension in software development: AI coding assistants are enabling engineers to generate code faster than ever, but the resulting software is often harder to understand and maintain.\"Vibe coding,\" a term popularized by AI researcher Andrej Karpathy, refers to using natural-language prompts to generate code through AI assistants. While these tools accelerate development, they can introduce what Agarwal describes as \"redundancies, breaks in architectural boundaries, assumptions, or ignored design patterns\" that accumulate over time.\"Most AI-generated code still introduces redundancies, breaks architectural boundaries, makes assumptions, or ignores established design patterns,\" Agarwal told Venturebeat. \"In many ways, we now need AI to help clean up the mess that AI itself is creating.\"The claim that engineers spend roughly half their time on debugging isn&#x27;t hyperbole. The Association for Computing Machinery reports that developers spend 35% to 50% of their time validating and debugging software. More recently, Harness&#x27;s State of Software Delivery 2025 report found that 67% of developers are spending more time debugging AI-generated code.\"We&#x27;ve seen world-class engineers spending half of their time debugging instead of building,\" said Rakesh Kothari, Deductive&#x27;s co-founder and CEO. \"And as vibe coding generates new code at a rate we&#x27;ve never seen, this problem is only going to get worse.\"How Deductive&#x27;s AI agents actually investigate production failuresDeductive&#x27;s technical approach differs substantially from the AI features being added to existing observability platforms like Datadog or New Relic. Most of those systems use large language models to summarize data or identify correlations, but they lack what Agarwal calls \"code-aware reasoning\"—the ability to understand not just that something broke, but why the code behaves the way it does.\"Most enterprises use multiple observability tools across different teams and services, so no vendor has a single holistic view of how their systems behave, fail, and recover—nor are they able to pair that with an understanding of the code that defines system behavior,\" Agarwal explained. \"These are key ingredients to resolving software incidents and it is exactly the gap Deductive fills.\"The system connects to existing infrastructure using read-only API access to observability platforms, code repositories, incident management tools, and chat systems. It then continuously builds and updates its knowledge graph, mapping dependencies between services and tracking deployment histories.When an alert fires, Deductive launches what the company describes as a multi-agent investigation. Different agents specialize in different aspects of the problem: one might analyze recent code changes, another examines trace data, while a third correlates the timing of the incident with recent deployments. The agents share findings and iteratively refine their hypotheses.The critical difference from rule-based automation is Deductive&#x27;s use of reinforcement learning. The system learns from every incident which investigative steps led to correct diagnoses and which were dead ends. When engineers provide feedback, the system incorporates that signal into its learning model.\"Each time it observes an investigation, it learns which steps, data sources, and decisions led to the right outcome,\" Agarwal said. \"It learns how to think through problems, not just point them out.\"At DoorDash, a recent latency spike in an API initially appeared to be an isolated service issue. Deductive&#x27;s investigation revealed that the root cause was actually timeout errors from a downstream machine learning platform undergoing a deployment. The system connected these dots by analyzing log volumes, traces, and deployment metadata across multiple services.\"Without Deductive, our team would have had to manually correlate the latency spike across all logs, traces, and deployment histories,\" Ansari said. \"Deductive was able to explain not just what changed, but how and why it impacted production behavior.\"The company keeps humans in the loop—for nowWhile Deductive&#x27;s technology could theoretically push fixes directly to production systems, the company has deliberately chosen to keep humans in the loop—at least for now.\"While our system is capable of deeper automation and could push fixes to production, currently, we recommend precise fixes and mitigations that engineers can review, validate, and apply,\" Agarwal said. \"We believe maintaining a human in the loop is essential for trust, transparency and operational safety.\"However, he acknowledged that \"over time, we do think that deeper automation will come and how humans operate in the loop will evolve.\"Databricks and ThoughtSpot veterans bet on reasoning over observabilityThe founding team brings deep expertise from building some of Silicon Valley&#x27;s most successful data infrastructure platforms. Agarwal earned his Ph.D. at UC Berkeley, where he created BlinkDB, an influential system for approximate query processing. He was among the first engineers at Databricks, where he helped build Apache Spark. Kothari was an early engineer at ThoughtSpot, where he led teams focused on distributed query processing and large-scale system optimization.The investor syndicate reflects both the technical credibility and market opportunity. Beyond CRV&#x27;s Max Gazor, the round included participation from Ion Stoica, founder of Databricks and Anyscale; Ajeet Singh, founder of Nutanix and ThoughtSpot; and Ben Sigelman, founder of Lightstep.Rather than competing with platforms like Datadog or PagerDuty, Deductive positions itself as a complementary layer that sits on top of existing tools. The pricing model reflects this: Instead of charging based on data volume, Deductive charges based on the number of incidents investigated, plus a base platform fee.The company offers both cloud-hosted and self-hosted deployment options and emphasizes that it doesn&#x27;t store customer data on its servers or use it to train models for other customers — a critical assurance given the proprietary nature of both code and production system behavior.With fresh capital and early customer traction at companies like DoorDash, Foursquare, and Kumo AI, Deductive plans to expand its team and deepen the system&#x27;s reasoning capabilities from reactive incident analysis to proactive prevention. The near-term vision: helping teams predict problems before they occur.DoorDash&#x27;s Ansari offers a pragmatic endorsement of where the technology stands today: \"Investigations that were previously manual and time-consuming are now automated, allowing engineers to shift their energy toward prevention, business impact, and innovation.\"In an industry where every second of downtime translates to lost revenue, that shift from firefighting to building increasingly looks less like a luxury and more like table stakes.",
          "content": "As software systems grow more complex and AI tools generate code faster than ever, a fundamental problem is getting worse: Engineers are drowning in debugging work, spending up to half their time hunting down the causes of software failures instead of building new products. The challenge has become so acute that it&#x27;s creating a new category of tooling — AI agents that can diagnose production failures in minutes instead of hours.Deductive AI, a startup emerging from stealth mode Wednesday, believes it has found a solution by applying reinforcement learning — the same technology that powers game-playing AI systems — to the messy, high-stakes world of production software incidents. The company announced it has raised $7.5 million in seed funding led by CRV, with participation from Databricks Ventures, Thomvest Ventures, and PrimeSet, to commercialize what it calls \"AI SRE agents\" that can diagnose and help fix software failures at machine speed.The pitch resonates with a growing frustration inside engineering organizations: Modern observability tools can show that something broke, but they rarely explain why. When a production system fails at 3 a.m., engineers still face hours of manual detective work, cross-referencing logs, metrics, deployment histories, and code changes across dozens of interconnected services to identify the root cause.\"The complexities and inter-dependencies of modern infrastructure means that investigating the root cause of an outage or incident can feel like searching for a needle in a haystack, except the haystack is the size of a football field, it&#x27;s made of a million other needles, it&#x27;s constantly reshuffling itself, and is on fire — and every second you don&#x27;t find it equals lost revenue,\" said Sameer Agarwal, Deductive&#x27;s co-founder and chief technology officer, in an exclusive interview with VentureBeat.Deductive&#x27;s system builds what the company calls a \"knowledge graph\" that maps relationships across codebases, telemetry data, engineering discussions, and internal documentation. When an incident occurs, multiple AI agents work together to form hypotheses, test them against live system evidence, and converge on a root cause — mimicking the investigative workflow of experienced site reliability engineers, but completing the process in minutes rather than hours.The technology has already shown measurable impact at some of the world&#x27;s most demanding production environments. DoorDash&#x27;s advertising platform, which runs real-time auctions that must complete in under 100 milliseconds, has integrated Deductive into its incident response workflow. The company has set an ambitious 2026 goal of resolving production incidents within 10 minutes.\"Our Ads Platform operates at a pace where manual, slow-moving investigations are no longer viable. Every minute of downtime directly affects company revenue,\" said Shahrooz Ansari, Senior Director of Engineering at DoorDash, in an interview with VentureBeat. \"Deductive has become a critical extension of our team, rapidly synthesizing signals across dozens of services and surfacing the insights that matter—within minutes.\"DoorDash estimates that Deductive has root-caused approximately 100 production incidents over the past few months, translating to more than 1,000 hours of annual engineering productivity and a revenue impact \"in millions of dollars,\" according to Ansari. At location intelligence company Foursquare, Deductive reduced the time to diagnose Apache Spark job failures by 90% —t urning a process that previously took hours or days into one that completes in under 10 minutes — while generating over $275,000 in annual savings.Why AI-generated code is creating a debugging crisisThe timing of Deductive&#x27;s launch reflects a brewing tension in software development: AI coding assistants are enabling engineers to generate code faster than ever, but the resulting software is often harder to understand and maintain.\"Vibe coding,\" a term popularized by AI researcher Andrej Karpathy, refers to using natural-language prompts to generate code through AI assistants. While these tools accelerate development, they can introduce what Agarwal describes as \"redundancies, breaks in architectural boundaries, assumptions, or ignored design patterns\" that accumulate over time.\"Most AI-generated code still introduces redundancies, breaks architectural boundaries, makes assumptions, or ignores established design patterns,\" Agarwal told Venturebeat. \"In many ways, we now need AI to help clean up the mess that AI itself is creating.\"The claim that engineers spend roughly half their time on debugging isn&#x27;t hyperbole. The Association for Computing Machinery reports that developers spend 35% to 50% of their time validating and debugging software. More recently, Harness&#x27;s State of Software Delivery 2025 report found that 67% of developers are spending more time debugging AI-generated code.\"We&#x27;ve seen world-class engineers spending half of their time debugging instead of building,\" said Rakesh Kothari, Deductive&#x27;s co-founder and CEO. \"And as vibe coding generates new code at a rate we&#x27;ve never seen, this problem is only going to get worse.\"How Deductive&#x27;s AI agents actually investigate production failuresDeductive&#x27;s technical approach differs substantially from the AI features being added to existing observability platforms like Datadog or New Relic. Most of those systems use large language models to summarize data or identify correlations, but they lack what Agarwal calls \"code-aware reasoning\"—the ability to understand not just that something broke, but why the code behaves the way it does.\"Most enterprises use multiple observability tools across different teams and services, so no vendor has a single holistic view of how their systems behave, fail, and recover—nor are they able to pair that with an understanding of the code that defines system behavior,\" Agarwal explained. \"These are key ingredients to resolving software incidents and it is exactly the gap Deductive fills.\"The system connects to existing infrastructure using read-only API access to observability platforms, code repositories, incident management tools, and chat systems. It then continuously builds and updates its knowledge graph, mapping dependencies between services and tracking deployment histories.When an alert fires, Deductive launches what the company describes as a multi-agent investigation. Different agents specialize in different aspects of the problem: one might analyze recent code changes, another examines trace data, while a third correlates the timing of the incident with recent deployments. The agents share findings and iteratively refine their hypotheses.The critical difference from rule-based automation is Deductive&#x27;s use of reinforcement learning. The system learns from every incident which investigative steps led to correct diagnoses and which were dead ends. When engineers provide feedback, the system incorporates that signal into its learning model.\"Each time it observes an investigation, it learns which steps, data sources, and decisions led to the right outcome,\" Agarwal said. \"It learns how to think through problems, not just point them out.\"At DoorDash, a recent latency spike in an API initially appeared to be an isolated service issue. Deductive&#x27;s investigation revealed that the root cause was actually timeout errors from a downstream machine learning platform undergoing a deployment. The system connected these dots by analyzing log volumes, traces, and deployment metadata across multiple services.\"Without Deductive, our team would have had to manually correlate the latency spike across all logs, traces, and deployment histories,\" Ansari said. \"Deductive was able to explain not just what changed, but how and why it impacted production behavior.\"The company keeps humans in the loop—for nowWhile Deductive&#x27;s technology could theoretically push fixes directly to production systems, the company has deliberately chosen to keep humans in the loop—at least for now.\"While our system is capable of deeper automation and could push fixes to production, currently, we recommend precise fixes and mitigations that engineers can review, validate, and apply,\" Agarwal said. \"We believe maintaining a human in the loop is essential for trust, transparency and operational safety.\"However, he acknowledged that \"over time, we do think that deeper automation will come and how humans operate in the loop will evolve.\"Databricks and ThoughtSpot veterans bet on reasoning over observabilityThe founding team brings deep expertise from building some of Silicon Valley&#x27;s most successful data infrastructure platforms. Agarwal earned his Ph.D. at UC Berkeley, where he created BlinkDB, an influential system for approximate query processing. He was among the first engineers at Databricks, where he helped build Apache Spark. Kothari was an early engineer at ThoughtSpot, where he led teams focused on distributed query processing and large-scale system optimization.The investor syndicate reflects both the technical credibility and market opportunity. Beyond CRV&#x27;s Max Gazor, the round included participation from Ion Stoica, founder of Databricks and Anyscale; Ajeet Singh, founder of Nutanix and ThoughtSpot; and Ben Sigelman, founder of Lightstep.Rather than competing with platforms like Datadog or PagerDuty, Deductive positions itself as a complementary layer that sits on top of existing tools. The pricing model reflects this: Instead of charging based on data volume, Deductive charges based on the number of incidents investigated, plus a base platform fee.The company offers both cloud-hosted and self-hosted deployment options and emphasizes that it doesn&#x27;t store customer data on its servers or use it to train models for other customers — a critical assurance given the proprietary nature of both code and production system behavior.With fresh capital and early customer traction at companies like DoorDash, Foursquare, and Kumo AI, Deductive plans to expand its team and deepen the system&#x27;s reasoning capabilities from reactive incident analysis to proactive prevention. The near-term vision: helping teams predict problems before they occur.DoorDash&#x27;s Ansari offers a pragmatic endorsement of where the technology stands today: \"Investigations that were previously manual and time-consuming are now automated, allowing engineers to shift their energy toward prevention, business impact, and innovation.\"In an industry where every second of downtime translates to lost revenue, that shift from firefighting to building increasingly looks less like a luxury and more like table stakes.",
          "feed_position": 3,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/7mfhEiM01EDWrgDZYpDbte/23713914379b94e43303f9965ccc40ae/nuneybits_Vector_art_of_robot_holding_blueprint_193c9fc5-bbb5-46ea-9ff6-1a08bb03716e.webp?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/openai-reboots-chatgpt-experience-with-gpt-5-1-after-mixed-reviews-of-gpt-5",
          "published_at": "Wed, 12 Nov 2025 05:00:00 GMT",
          "title": "OpenAI reboots ChatGPT experience with GPT-5.1 after mixed reviews of GPT-5",
          "standfirst": "ChatGPT is about to become faster and more conversational as OpenAI upgrades its flagship model GPT-5 to GPT-5.1.OpenAI announced two updates to the GPT-5 series: GPT-5.1 Instant and GPT-5.1 Thinking. Both models are now accessible on ChatGPT. GPT-5.1 Instant, essentially the default and most-used model, is now “warmer, more intelligent, and better at following your instructions,” according to OpenAI. Meanwhile, GPT-5.1 Thinking is an advanced reasoning model that responds faster for simple tasks and more persistently on complex ones.“We heard clearly from users that great AI should not only be smart, but also enjoyable to talk to,” OpenAI said in a blog post. “GPT-5.1 improves meaningfully on both intelligence and communication style.” The company added that both models offer a way for users to “shape ChatGPT’s tone,” allowing people to control how the chat platform responds depending on the conversation they are having. Both models were rolled out to ChatGPT Pro, Plus, Go and Business users, as well as the free tier. Those on the Enterprise and Edu plans will get a seven-day early-access toggle for the models before GPT-5.1 becomes the default model. OpenAI said the models can also be accessible through the API, both with adapted reasoning. OpenAI has noted that it will soon update GPT-5 Pro to version 5.1. Instant and Thinking models The 5.1 tag reflects improvements to the base model, and OpenAI considers these part of the GPT-5 family, trained on the same stack and data as its reasoning models. The biggest difference between 5.1 and 5 is its more natural and conversational tone, OpenAI CEO for Applications Fidji Simo said in a Substack post. “Based on early testing, it often surprises people with its playfulness while remaining clear and useful,” OpenAI said in its post. Instant can use adaptive reasoning to help it decide when it needs to think about its answers, especially when it comes to more complicated questions. OpenAI noted that it has improved the model&#x27;s instruction following, so that while it continues to respond quickly, it also directly addresses the user’s query. Recent model releases, such as Baidu’s ERNIE-4.5-VL-28B-A3B-Thinking, have been outperforming GPT-5 in benchmarks like instruction-following. GPT-5.1 Thinking can figure out on its own how much reasoning power it should devote to a prompt. It adapts to the type and complexity of a query, so it will take longer to answer a fuller, complex question than a simple summary request. OpenAI said evaluations showed that GPT-5.1 Thinking spends less time and therefore uses fewer tokens on simple tasks compared to GPT-5, outperforming the base model in speed of response. One thing enterprises should note is that GPT-5.1 Thinking answers “with less jargon and fewer undefined terms.” OpenAI said removing jargony responses makes Thinking more approachable when it comes to explaining technical concepts.More personalizationAnother big update to ChatGPT is increased personalization. This allows users to toggle between a friendly and authoritative chat platform experience in their conversations. ChatGPT already allows users to choose preset options for model tone, but the new update expands these options “to better reflect the most common ways people use ChatGPT.”Options include \"default,\" \"friendly\" (formerly \"listener\"), \"efficient\" (previously \"robot\"), \"professional,\" \"candid\" and \"quirky.\" Two other personalities, \"cynical\" and \"nerdy,\" remain unchanged. “We think many people will find that GPT-5.1 does a better job of bringing IQ and EQ together, but one default clearly can’t meet everyone’s needs,\" Simo said. \"That’s why we’re also making it easier to customize ChatGPT with a range of presets to choose from. The model has the same capabilities whether you select default or one of these options, but the style of its responses will differ — more formal or familiar, more playful or direct, more or less jargon or slang. Of course, eight personalities still don&#x27;t cover the full range of human diversity, but we know from our research that many people prefer simple, guided control over too many settings or open-ended options.\"People can also adjust how much ChatGPT uses emojis. OpenAI offers granular controls for responses and is experimenting with the ability to make the models more concise, warm or scannable.Saving a rolloutOpenAI’s GPT-5 rollout was…less than perfect. While company executives, including CEO Sam Altman, touted the new model’s capabilities, a decision to initially sunset older and beloved models on ChatGPT was met with dissatisfaction. Worse yet, many early adopters found that GPT-5 didn’t perform better than older options in domains such as math, science and writing. This led Altman to walk back some of his statements around model removal, blaming performance issues on GPT-5’s router. The router, which automatically directs queries to the most suited models, is not going away, as GPT-5.1 Auto will route prompts to the model type that can best answer queries. OpenAI is careful to note that GPT-5 models Instant, Thinking and Pro are still available in ChatGPT’s model dropdown, although paid subscribers only have three months to compare these older versions with the 5.1 update. The sunset period for GPT-5, however, will not impact models like GPT-4o.“Going forward, when we introduce new ChatGPT models, our approach is to give people ample space to evaluate what’s changed and share feedback, allowing us to continue innovating our frontier models while transitioning smoothly,” the company said. “Sunset periods will be communicated clearly and with plenty of advance notice.”",
          "content": "ChatGPT is about to become faster and more conversational as OpenAI upgrades its flagship model GPT-5 to GPT-5.1.OpenAI announced two updates to the GPT-5 series: GPT-5.1 Instant and GPT-5.1 Thinking. Both models are now accessible on ChatGPT. GPT-5.1 Instant, essentially the default and most-used model, is now “warmer, more intelligent, and better at following your instructions,” according to OpenAI. Meanwhile, GPT-5.1 Thinking is an advanced reasoning model that responds faster for simple tasks and more persistently on complex ones.“We heard clearly from users that great AI should not only be smart, but also enjoyable to talk to,” OpenAI said in a blog post. “GPT-5.1 improves meaningfully on both intelligence and communication style.” The company added that both models offer a way for users to “shape ChatGPT’s tone,” allowing people to control how the chat platform responds depending on the conversation they are having. Both models were rolled out to ChatGPT Pro, Plus, Go and Business users, as well as the free tier. Those on the Enterprise and Edu plans will get a seven-day early-access toggle for the models before GPT-5.1 becomes the default model. OpenAI said the models can also be accessible through the API, both with adapted reasoning. OpenAI has noted that it will soon update GPT-5 Pro to version 5.1. Instant and Thinking models The 5.1 tag reflects improvements to the base model, and OpenAI considers these part of the GPT-5 family, trained on the same stack and data as its reasoning models. The biggest difference between 5.1 and 5 is its more natural and conversational tone, OpenAI CEO for Applications Fidji Simo said in a Substack post. “Based on early testing, it often surprises people with its playfulness while remaining clear and useful,” OpenAI said in its post. Instant can use adaptive reasoning to help it decide when it needs to think about its answers, especially when it comes to more complicated questions. OpenAI noted that it has improved the model&#x27;s instruction following, so that while it continues to respond quickly, it also directly addresses the user’s query. Recent model releases, such as Baidu’s ERNIE-4.5-VL-28B-A3B-Thinking, have been outperforming GPT-5 in benchmarks like instruction-following. GPT-5.1 Thinking can figure out on its own how much reasoning power it should devote to a prompt. It adapts to the type and complexity of a query, so it will take longer to answer a fuller, complex question than a simple summary request. OpenAI said evaluations showed that GPT-5.1 Thinking spends less time and therefore uses fewer tokens on simple tasks compared to GPT-5, outperforming the base model in speed of response. One thing enterprises should note is that GPT-5.1 Thinking answers “with less jargon and fewer undefined terms.” OpenAI said removing jargony responses makes Thinking more approachable when it comes to explaining technical concepts.More personalizationAnother big update to ChatGPT is increased personalization. This allows users to toggle between a friendly and authoritative chat platform experience in their conversations. ChatGPT already allows users to choose preset options for model tone, but the new update expands these options “to better reflect the most common ways people use ChatGPT.”Options include \"default,\" \"friendly\" (formerly \"listener\"), \"efficient\" (previously \"robot\"), \"professional,\" \"candid\" and \"quirky.\" Two other personalities, \"cynical\" and \"nerdy,\" remain unchanged. “We think many people will find that GPT-5.1 does a better job of bringing IQ and EQ together, but one default clearly can’t meet everyone’s needs,\" Simo said. \"That’s why we’re also making it easier to customize ChatGPT with a range of presets to choose from. The model has the same capabilities whether you select default or one of these options, but the style of its responses will differ — more formal or familiar, more playful or direct, more or less jargon or slang. Of course, eight personalities still don&#x27;t cover the full range of human diversity, but we know from our research that many people prefer simple, guided control over too many settings or open-ended options.\"People can also adjust how much ChatGPT uses emojis. OpenAI offers granular controls for responses and is experimenting with the ability to make the models more concise, warm or scannable.Saving a rolloutOpenAI’s GPT-5 rollout was…less than perfect. While company executives, including CEO Sam Altman, touted the new model’s capabilities, a decision to initially sunset older and beloved models on ChatGPT was met with dissatisfaction. Worse yet, many early adopters found that GPT-5 didn’t perform better than older options in domains such as math, science and writing. This led Altman to walk back some of his statements around model removal, blaming performance issues on GPT-5’s router. The router, which automatically directs queries to the most suited models, is not going away, as GPT-5.1 Auto will route prompts to the model type that can best answer queries. OpenAI is careful to note that GPT-5 models Instant, Thinking and Pro are still available in ChatGPT’s model dropdown, although paid subscribers only have three months to compare these older versions with the 5.1 update. The sunset period for GPT-5, however, will not impact models like GPT-4o.“Going forward, when we introduce new ChatGPT models, our approach is to give people ample space to evaluate what’s changed and share feedback, allowing us to continue innovating our frontier models while transitioning smoothly,” the company said. “Sunset periods will be communicated clearly and with plenty of advance notice.”",
          "feed_position": 4,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/3CpLfvpU0TfycKYEA3DBLw/f7dc8b8d4db1e3eef3d0b619d662879e/crimedy7_illustration_of_a_conversation_abstract_--ar_169_--v_5a880096-9873-4985-85ae-e8c247d831fc_0.png?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/baidu-just-dropped-an-open-source-multimodal-ai-that-it-claims-beats-gpt-5",
          "published_at": "Wed, 12 Nov 2025 00:00:00 GMT",
          "title": "Baidu just dropped an open-source multimodal AI that it claims beats GPT-5 and Gemini",
          "standfirst": "Baidu Inc., China&#x27;s largest search engine company, released a new artificial intelligence model on Monday that its developers claim outperforms competitors from Google and OpenAI on several vision-related benchmarks despite using a fraction of the computing resources typically required for such systems.The model, dubbed ERNIE-4.5-VL-28B-A3B-Thinking, is the latest salvo in an escalating competition among technology companies to build AI systems that can understand and reason about images, videos, and documents alongside traditional text — capabilities increasingly critical for enterprise applications ranging from automated document processing to industrial quality control.What sets Baidu&#x27;s release apart is its efficiency: the model activates just 3 billion parameters during operation while maintaining 28 billion total parameters through a sophisticated routing architecture. According to documentation released with the model, this design allows it to match or exceed the performance of much larger competing systems on tasks involving document understanding, chart analysis, and visual reasoning while consuming significantly less computational power and memory.\"Built upon the powerful ERNIE-4.5-VL-28B-A3B architecture, the newly upgraded ERNIE-4.5-VL-28B-A3B-Thinking achieves a remarkable leap forward in multimodal reasoning capabilities,\" Baidu wrote in the model&#x27;s technical documentation on Hugging Face, the AI model repository where the system was released.The company said the model underwent \"an extensive mid-training phase\" that incorporated \"a vast and highly diverse corpus of premium visual-language reasoning data,\" dramatically boosting its ability to align visual and textual information semantically.How the model mimics human visual problem-solving through dynamic image analysisPerhaps the model&#x27;s most distinctive feature is what Baidu calls \"Thinking with Images\" — a capability that allows the AI to dynamically zoom in and out of images to examine fine-grained details, mimicking how humans approach visual problem-solving tasks.\"The model thinks like a human, capable of freely zooming in and out of images to grasp every detail and uncover all information,\" according to the model card. When paired with tools like image search, Baidu claims this feature \"dramatically elevates the model&#x27;s ability to process fine-grained details and handle long-tail visual knowledge.\"This approach marks a departure from traditional vision-language models, which typically process images at a fixed resolution. By allowing dynamic image examination, the system can theoretically handle scenarios requiring both broad context and granular detail—such as analyzing complex technical diagrams or detecting subtle defects in manufacturing quality control.The model also supports what Baidu describes as enhanced \"visual grounding\" capabilities with \"more precise grounding and flexible instruction execution, easily triggering grounding functions in complex industrial scenarios,\" suggesting potential applications in robotics, warehouse automation, and other settings where AI systems must identify and locate specific objects in visual scenes.Baidu&#x27;s performance claims draw scrutiny as independent testing remains pendingBaidu&#x27;s assertion that the model outperforms Google&#x27;s Gemini 2.5 Pro and OpenAI&#x27;s GPT-5-High on various document and chart understanding benchmarks has drawn attention across social media, though independent verification of these claims remains pending.The company released the model under the permissive Apache 2.0 license, allowing unrestricted commercial use—a strategic decision that contrasts with the more restrictive licensing approaches of some competitors and could accelerate enterprise adoption.\"Apache 2.0 is smart,\" wrote one X user responding to Baidu&#x27;s announcement, highlighting the competitive advantage of open licensing in the enterprise market.According to Baidu&#x27;s documentation, the model demonstrates six core capabilities beyond traditional text processing. In visual reasoning, the system can perform what Baidu describes as \"multi-step reasoning, chart analysis, and causal reasoning capabilities in complex visual tasks,\" aided by what the company characterizes as \"large-scale reinforcement learning.\" For STEM problem solving, Baidu claims that \"leveraging its powerful visual abilities, the model achieves a leap in performance on STEM tasks like solving problems from photos.\" The visual grounding capability allows the model to identify and locate objects within images with what Baidu characterizes as industrial-grade precision. Through tool integration, the system can invoke external functions including image search capabilities to access information beyond its training data.For video understanding, Baidu claims the model possesses \"outstanding temporal awareness and event localization abilities, accurately identifying content changes across different time segments in a video.\" Finally, the thinking with images feature enables the dynamic zoom functionality that distinguishes this model from competitors.Inside the mixture-of-experts architecture that powers efficient multimodal processingUnder the hood, ERNIE-4.5-VL-28B-A3B-Thinking employs a Mixture-of-Experts (MoE) architecture — a design pattern that has become increasingly popular for building efficient large-scale AI systems. Rather than activating all 28 billion parameters for every task, the model uses a routing mechanism to selectively activate only the 3 billion parameters most relevant to each specific input.This approach offers substantial practical advantages for enterprise deployments. According to Baidu&#x27;s documentation, the model can run on a single 80GB GPU — hardware readily available in many corporate data centers — making it significantly more accessible than competing systems that may require multiple high-end accelerators.The technical documentation reveals that Baidu employed several advanced training techniques to achieve the model&#x27;s capabilities. The company used \"cutting-edge multimodal reinforcement learning techniques on verifiable tasks, integrating GSPO and IcePop strategies to stabilize MoE training combined with dynamic difficulty sampling for exceptional learning efficiency.\"Baidu also notes that in response to \"strong community demand,\" the company \"significantly strengthened the model&#x27;s grounding performance with improved instruction-following capabilities.\"The new model fits into Baidu&#x27;s ambitious multimodal AI ecosystemThe new release is one component of Baidu&#x27;s broader ERNIE 4.5 model family, which the company unveiled in June 2025. That family comprises 10 distinct variants, including Mixture-of-Experts models ranging from the flagship ERNIE-4.5-VL-424B-A47B with 424 billion total parameters down to a compact 0.3 billion parameter dense model.According to Baidu&#x27;s technical report on the ERNIE 4.5 family, the models incorporate \"a novel heterogeneous modality structure, which supports parameter sharing across modalities while also allowing dedicated parameters for each individual modality.\"This architectural choice addresses a longstanding challenge in multimodal AI development: training systems on both visual and textual data without one modality degrading the performance of the other. Baidu claims this design \"has the advantage to enhance multimodal understanding without compromising, and even improving, performance on text-related tasks.\"The company reported achieving 47% Model FLOPs Utilization (MFU) — a measure of training efficiency — during pre-training of its largest ERNIE 4.5 language model, using the PaddlePaddle deep learning framework developed in-house.Comprehensive developer tools aim to simplify enterprise deployment and integrationFor organizations looking to deploy the model, Baidu has released a comprehensive suite of development tools through ERNIEKit, what the company describes as an \"industrial-grade training and compression development toolkit.\"The model offers full compatibility with popular open-source frameworks including Hugging Face Transformers, vLLM (a high-performance inference engine), and Baidu&#x27;s own FastDeploy toolkit. This multi-platform support could prove critical for enterprise adoption, allowing organizations to integrate the model into existing AI infrastructure without wholesale platform changes.Sample code released by Baidu shows a relatively straightforward implementation path. Using the Transformers library, developers can load and run the model with approximately 30 lines of Python code, according to the documentation on Hugging Face.For production deployments requiring higher throughput, Baidu provides vLLM integration with specialized support for the model&#x27;s \"reasoning-parser\" and \"tool-call-parser\" capabilities — features that enable the dynamic image examination and external tool integration that distinguish this model from earlier systems.The company also offers FastDeploy, a proprietary inference toolkit that Baidu claims delivers \"production-ready, easy-to-use multi-hardware deployment solutions\" with support for various quantization schemes that can reduce memory requirements and increase inference speed.Why this release matters for the enterprise AI market at a critical inflection pointThe release comes at a pivotal moment in the enterprise AI market. As organizations move beyond experimental chatbot deployments toward production systems that process documents, analyze visual data, and automate complex workflows, demand for capable and cost-effective vision-language models has intensified.Several enterprise use cases appear particularly well-suited to the model&#x27;s capabilities. Document processing — extracting information from invoices, contracts, and forms — represents a massive market where accurate chart and table understanding directly translates to cost savings through automation. Manufacturing quality control, where AI systems must detect visual defects, could benefit from the model&#x27;s grounding capabilities. Customer service applications that handle images from users could leverage the multi-step visual reasoning.The model&#x27;s efficiency profile may prove especially attractive to mid-market organizations and startups that lack the computing budgets of large technology companies. By fitting on a single 80GB GPU — hardware costing roughly $10,000 to $30,000 depending on the specific model — the system becomes economically viable for a much broader range of organizations than models requiring multi-GPU setups costing hundreds of thousands of dollars.\"With all these new models, where&#x27;s the best place to actually build and scale? Access to compute is everything,\" wrote one X user in response to Baidu&#x27;s announcement, highlighting the persistent infrastructure challenges facing organizations attempting to deploy advanced AI systems.The Apache 2.0 licensing further lowers barriers to adoption. Unlike models released under more restrictive licenses that may limit commercial use or require revenue sharing, organizations can deploy ERNIE-4.5-VL-28B-A3B-Thinking in production applications without ongoing licensing fees or usage restrictions.Competition intensifies as Chinese tech giant takes aim at Google and OpenAIBaidu&#x27;s release intensifies competition in the vision-language model space, where Google, OpenAI, Anthropic, and Chinese companies including Alibaba and ByteDance have all released capable systems in recent months.The company&#x27;s performance claims — if validated by independent testing — would represent a significant achievement. Google&#x27;s Gemini 2.5 Pro and OpenAI&#x27;s GPT-5-High are substantially larger models backed by the deep resources of two of the world&#x27;s most valuable technology companies. That a more compact, openly available model could match or exceed their performance on specific tasks would suggest the field is advancing more rapidly than some analysts anticipated.\"Impressive that ERNIE is outperforming Gemini 2.5 Pro,\" wrote one social media commenter, expressing surprise at the claimed results.However, some observers counseled caution about benchmark comparisons. \"It&#x27;s fascinating to see how multimodal models are evolving, especially with features like &#x27;Thinking with Images,&#x27;\" wrote one X user. \"That said, I&#x27;m curious if ERNIE-4.5&#x27;s edge over competitors like Gemini-2.5-Pro and GPT-5-High primarily lies in specific use cases like document and chart\" understanding rather than general-purpose vision tasks.Industry analysts note that benchmark performance often fails to capture real-world behavior across the diverse scenarios enterprises encounter. A model that excels at document understanding may struggle with creative visual tasks or real-time video analysis. Organizations evaluating these systems typically conduct extensive internal testing on representative workloads before committing to production deployments.Technical limitations and infrastructure requirements that enterprises must considerDespite its capabilities, the model faces several technical challenges common to large vision-language systems. The minimum requirement of 80GB of GPU memory, while more accessible than some competitors, still represents a significant infrastructure investment. Organizations without existing GPU infrastructure would need to procure specialized hardware or rely on cloud computing services, introducing ongoing operational costs.The model&#x27;s context window — the amount of text and visual information it can process simultaneously — is listed as 128K tokens in Baidu&#x27;s documentation. While substantial, this may prove limiting for some document processing scenarios involving very long technical manuals or extensive video content.Questions also remain about the model&#x27;s behavior on adversarial inputs, out-of-distribution data, and edge cases. Baidu&#x27;s documentation does not provide detailed information about safety testing, bias mitigation, or failure modes — considerations increasingly important for enterprise deployments where errors could have financial or safety implications.What technical decision-makers need to evaluate beyond the benchmark numbersFor technical decision-makers evaluating the model, several implementation factors warrant consideration beyond raw performance metrics.The model&#x27;s MoE architecture, while efficient during inference, adds complexity to deployment and optimization. Organizations must ensure their infrastructure can properly route inputs to the appropriate expert subnetworks — a capability not universally supported across all deployment platforms.The \"Thinking with Images\" feature, while innovative, requires integration with image manipulation tools to achieve its full potential. Baidu&#x27;s documentation suggests this capability works best \"when paired with tools like image zooming and image search,\" implying that organizations may need to build additional infrastructure to fully leverage this functionality.The model&#x27;s video understanding capabilities, while highlighted in marketing materials, come with practical constraints. Processing video requires substantially more computational resources than static images, and the documentation does not specify maximum video length or optimal frame rates.Organizations considering deployment should also evaluate Baidu&#x27;s ongoing commitment to the model. Open-source AI models require continuing maintenance, security updates, and potential retraining as data distributions shift over time. While the Apache 2.0 license ensures the model remains available, future improvements and support depend on Baidu&#x27;s strategic priorities.Developer community responds with enthusiasm tempered by practical requestsEarly response from the AI research and development community has been cautiously optimistic. Developers have requested versions of the model in additional formats including GGUF (a quantization format popular for local deployment) and MNN (a mobile neural network framework), suggesting interest in running the system on resource-constrained devices.\"Release MNN and GGUF so I can run it on my phone,\" wrote one developer, highlighting demand for mobile deployment options.Other developers praised Baidu&#x27;s technical choices while requesting additional resources. \"Fantastic model! Did you use discoveries from PaddleOCR?\" asked one user, referencing Baidu&#x27;s open-source optical character recognition toolkit.The model&#x27;s lengthy name—ERNIE-4.5-VL-28B-A3B-Thinking—drew lighthearted commentary. \"ERNIE-4.5-VL-28B-A3B-Thinking might be the longest model name in history,\" joked one observer. \"But hey, if you&#x27;re outperforming Gemini-2.5-Pro with only 3B active params, you&#x27;ve earned the right to a dramatic name!\"Baidu plans to showcase the ERNIE lineup during its Baidu World 2025 conference on November 13, where the company is expected to provide additional details about the model&#x27;s development, performance validation, and future roadmap.The release marks a strategic move by Baidu to establish itself as a major player in the global AI infrastructure market. While Chinese AI companies have historically focused primarily on domestic markets, the open-source release under a permissive license signals ambitions to compete internationally with Western AI giants.For enterprises, the release adds another capable option to a rapidly expanding menu of AI models. Organizations no longer face a binary choice between building proprietary systems or licensing closed-source models from a handful of vendors. The proliferation of capable open-source alternatives like ERNIE-4.5-VL-28B-A3B-Thinking is reshaping the economics of AI deployment and accelerating adoption across industries.Whether the model delivers on its performance promises in real-world deployments remains to be seen. But for organizations seeking powerful, cost-effective tools for visual understanding and reasoning, one thing is certain. As one developer succinctly summarized: \"Open source plus commercial use equals chef&#x27;s kiss. Baidu not playing around.\"",
          "content": "Baidu Inc., China&#x27;s largest search engine company, released a new artificial intelligence model on Monday that its developers claim outperforms competitors from Google and OpenAI on several vision-related benchmarks despite using a fraction of the computing resources typically required for such systems.The model, dubbed ERNIE-4.5-VL-28B-A3B-Thinking, is the latest salvo in an escalating competition among technology companies to build AI systems that can understand and reason about images, videos, and documents alongside traditional text — capabilities increasingly critical for enterprise applications ranging from automated document processing to industrial quality control.What sets Baidu&#x27;s release apart is its efficiency: the model activates just 3 billion parameters during operation while maintaining 28 billion total parameters through a sophisticated routing architecture. According to documentation released with the model, this design allows it to match or exceed the performance of much larger competing systems on tasks involving document understanding, chart analysis, and visual reasoning while consuming significantly less computational power and memory.\"Built upon the powerful ERNIE-4.5-VL-28B-A3B architecture, the newly upgraded ERNIE-4.5-VL-28B-A3B-Thinking achieves a remarkable leap forward in multimodal reasoning capabilities,\" Baidu wrote in the model&#x27;s technical documentation on Hugging Face, the AI model repository where the system was released.The company said the model underwent \"an extensive mid-training phase\" that incorporated \"a vast and highly diverse corpus of premium visual-language reasoning data,\" dramatically boosting its ability to align visual and textual information semantically.How the model mimics human visual problem-solving through dynamic image analysisPerhaps the model&#x27;s most distinctive feature is what Baidu calls \"Thinking with Images\" — a capability that allows the AI to dynamically zoom in and out of images to examine fine-grained details, mimicking how humans approach visual problem-solving tasks.\"The model thinks like a human, capable of freely zooming in and out of images to grasp every detail and uncover all information,\" according to the model card. When paired with tools like image search, Baidu claims this feature \"dramatically elevates the model&#x27;s ability to process fine-grained details and handle long-tail visual knowledge.\"This approach marks a departure from traditional vision-language models, which typically process images at a fixed resolution. By allowing dynamic image examination, the system can theoretically handle scenarios requiring both broad context and granular detail—such as analyzing complex technical diagrams or detecting subtle defects in manufacturing quality control.The model also supports what Baidu describes as enhanced \"visual grounding\" capabilities with \"more precise grounding and flexible instruction execution, easily triggering grounding functions in complex industrial scenarios,\" suggesting potential applications in robotics, warehouse automation, and other settings where AI systems must identify and locate specific objects in visual scenes.Baidu&#x27;s performance claims draw scrutiny as independent testing remains pendingBaidu&#x27;s assertion that the model outperforms Google&#x27;s Gemini 2.5 Pro and OpenAI&#x27;s GPT-5-High on various document and chart understanding benchmarks has drawn attention across social media, though independent verification of these claims remains pending.The company released the model under the permissive Apache 2.0 license, allowing unrestricted commercial use—a strategic decision that contrasts with the more restrictive licensing approaches of some competitors and could accelerate enterprise adoption.\"Apache 2.0 is smart,\" wrote one X user responding to Baidu&#x27;s announcement, highlighting the competitive advantage of open licensing in the enterprise market.According to Baidu&#x27;s documentation, the model demonstrates six core capabilities beyond traditional text processing. In visual reasoning, the system can perform what Baidu describes as \"multi-step reasoning, chart analysis, and causal reasoning capabilities in complex visual tasks,\" aided by what the company characterizes as \"large-scale reinforcement learning.\" For STEM problem solving, Baidu claims that \"leveraging its powerful visual abilities, the model achieves a leap in performance on STEM tasks like solving problems from photos.\" The visual grounding capability allows the model to identify and locate objects within images with what Baidu characterizes as industrial-grade precision. Through tool integration, the system can invoke external functions including image search capabilities to access information beyond its training data.For video understanding, Baidu claims the model possesses \"outstanding temporal awareness and event localization abilities, accurately identifying content changes across different time segments in a video.\" Finally, the thinking with images feature enables the dynamic zoom functionality that distinguishes this model from competitors.Inside the mixture-of-experts architecture that powers efficient multimodal processingUnder the hood, ERNIE-4.5-VL-28B-A3B-Thinking employs a Mixture-of-Experts (MoE) architecture — a design pattern that has become increasingly popular for building efficient large-scale AI systems. Rather than activating all 28 billion parameters for every task, the model uses a routing mechanism to selectively activate only the 3 billion parameters most relevant to each specific input.This approach offers substantial practical advantages for enterprise deployments. According to Baidu&#x27;s documentation, the model can run on a single 80GB GPU — hardware readily available in many corporate data centers — making it significantly more accessible than competing systems that may require multiple high-end accelerators.The technical documentation reveals that Baidu employed several advanced training techniques to achieve the model&#x27;s capabilities. The company used \"cutting-edge multimodal reinforcement learning techniques on verifiable tasks, integrating GSPO and IcePop strategies to stabilize MoE training combined with dynamic difficulty sampling for exceptional learning efficiency.\"Baidu also notes that in response to \"strong community demand,\" the company \"significantly strengthened the model&#x27;s grounding performance with improved instruction-following capabilities.\"The new model fits into Baidu&#x27;s ambitious multimodal AI ecosystemThe new release is one component of Baidu&#x27;s broader ERNIE 4.5 model family, which the company unveiled in June 2025. That family comprises 10 distinct variants, including Mixture-of-Experts models ranging from the flagship ERNIE-4.5-VL-424B-A47B with 424 billion total parameters down to a compact 0.3 billion parameter dense model.According to Baidu&#x27;s technical report on the ERNIE 4.5 family, the models incorporate \"a novel heterogeneous modality structure, which supports parameter sharing across modalities while also allowing dedicated parameters for each individual modality.\"This architectural choice addresses a longstanding challenge in multimodal AI development: training systems on both visual and textual data without one modality degrading the performance of the other. Baidu claims this design \"has the advantage to enhance multimodal understanding without compromising, and even improving, performance on text-related tasks.\"The company reported achieving 47% Model FLOPs Utilization (MFU) — a measure of training efficiency — during pre-training of its largest ERNIE 4.5 language model, using the PaddlePaddle deep learning framework developed in-house.Comprehensive developer tools aim to simplify enterprise deployment and integrationFor organizations looking to deploy the model, Baidu has released a comprehensive suite of development tools through ERNIEKit, what the company describes as an \"industrial-grade training and compression development toolkit.\"The model offers full compatibility with popular open-source frameworks including Hugging Face Transformers, vLLM (a high-performance inference engine), and Baidu&#x27;s own FastDeploy toolkit. This multi-platform support could prove critical for enterprise adoption, allowing organizations to integrate the model into existing AI infrastructure without wholesale platform changes.Sample code released by Baidu shows a relatively straightforward implementation path. Using the Transformers library, developers can load and run the model with approximately 30 lines of Python code, according to the documentation on Hugging Face.For production deployments requiring higher throughput, Baidu provides vLLM integration with specialized support for the model&#x27;s \"reasoning-parser\" and \"tool-call-parser\" capabilities — features that enable the dynamic image examination and external tool integration that distinguish this model from earlier systems.The company also offers FastDeploy, a proprietary inference toolkit that Baidu claims delivers \"production-ready, easy-to-use multi-hardware deployment solutions\" with support for various quantization schemes that can reduce memory requirements and increase inference speed.Why this release matters for the enterprise AI market at a critical inflection pointThe release comes at a pivotal moment in the enterprise AI market. As organizations move beyond experimental chatbot deployments toward production systems that process documents, analyze visual data, and automate complex workflows, demand for capable and cost-effective vision-language models has intensified.Several enterprise use cases appear particularly well-suited to the model&#x27;s capabilities. Document processing — extracting information from invoices, contracts, and forms — represents a massive market where accurate chart and table understanding directly translates to cost savings through automation. Manufacturing quality control, where AI systems must detect visual defects, could benefit from the model&#x27;s grounding capabilities. Customer service applications that handle images from users could leverage the multi-step visual reasoning.The model&#x27;s efficiency profile may prove especially attractive to mid-market organizations and startups that lack the computing budgets of large technology companies. By fitting on a single 80GB GPU — hardware costing roughly $10,000 to $30,000 depending on the specific model — the system becomes economically viable for a much broader range of organizations than models requiring multi-GPU setups costing hundreds of thousands of dollars.\"With all these new models, where&#x27;s the best place to actually build and scale? Access to compute is everything,\" wrote one X user in response to Baidu&#x27;s announcement, highlighting the persistent infrastructure challenges facing organizations attempting to deploy advanced AI systems.The Apache 2.0 licensing further lowers barriers to adoption. Unlike models released under more restrictive licenses that may limit commercial use or require revenue sharing, organizations can deploy ERNIE-4.5-VL-28B-A3B-Thinking in production applications without ongoing licensing fees or usage restrictions.Competition intensifies as Chinese tech giant takes aim at Google and OpenAIBaidu&#x27;s release intensifies competition in the vision-language model space, where Google, OpenAI, Anthropic, and Chinese companies including Alibaba and ByteDance have all released capable systems in recent months.The company&#x27;s performance claims — if validated by independent testing — would represent a significant achievement. Google&#x27;s Gemini 2.5 Pro and OpenAI&#x27;s GPT-5-High are substantially larger models backed by the deep resources of two of the world&#x27;s most valuable technology companies. That a more compact, openly available model could match or exceed their performance on specific tasks would suggest the field is advancing more rapidly than some analysts anticipated.\"Impressive that ERNIE is outperforming Gemini 2.5 Pro,\" wrote one social media commenter, expressing surprise at the claimed results.However, some observers counseled caution about benchmark comparisons. \"It&#x27;s fascinating to see how multimodal models are evolving, especially with features like &#x27;Thinking with Images,&#x27;\" wrote one X user. \"That said, I&#x27;m curious if ERNIE-4.5&#x27;s edge over competitors like Gemini-2.5-Pro and GPT-5-High primarily lies in specific use cases like document and chart\" understanding rather than general-purpose vision tasks.Industry analysts note that benchmark performance often fails to capture real-world behavior across the diverse scenarios enterprises encounter. A model that excels at document understanding may struggle with creative visual tasks or real-time video analysis. Organizations evaluating these systems typically conduct extensive internal testing on representative workloads before committing to production deployments.Technical limitations and infrastructure requirements that enterprises must considerDespite its capabilities, the model faces several technical challenges common to large vision-language systems. The minimum requirement of 80GB of GPU memory, while more accessible than some competitors, still represents a significant infrastructure investment. Organizations without existing GPU infrastructure would need to procure specialized hardware or rely on cloud computing services, introducing ongoing operational costs.The model&#x27;s context window — the amount of text and visual information it can process simultaneously — is listed as 128K tokens in Baidu&#x27;s documentation. While substantial, this may prove limiting for some document processing scenarios involving very long technical manuals or extensive video content.Questions also remain about the model&#x27;s behavior on adversarial inputs, out-of-distribution data, and edge cases. Baidu&#x27;s documentation does not provide detailed information about safety testing, bias mitigation, or failure modes — considerations increasingly important for enterprise deployments where errors could have financial or safety implications.What technical decision-makers need to evaluate beyond the benchmark numbersFor technical decision-makers evaluating the model, several implementation factors warrant consideration beyond raw performance metrics.The model&#x27;s MoE architecture, while efficient during inference, adds complexity to deployment and optimization. Organizations must ensure their infrastructure can properly route inputs to the appropriate expert subnetworks — a capability not universally supported across all deployment platforms.The \"Thinking with Images\" feature, while innovative, requires integration with image manipulation tools to achieve its full potential. Baidu&#x27;s documentation suggests this capability works best \"when paired with tools like image zooming and image search,\" implying that organizations may need to build additional infrastructure to fully leverage this functionality.The model&#x27;s video understanding capabilities, while highlighted in marketing materials, come with practical constraints. Processing video requires substantially more computational resources than static images, and the documentation does not specify maximum video length or optimal frame rates.Organizations considering deployment should also evaluate Baidu&#x27;s ongoing commitment to the model. Open-source AI models require continuing maintenance, security updates, and potential retraining as data distributions shift over time. While the Apache 2.0 license ensures the model remains available, future improvements and support depend on Baidu&#x27;s strategic priorities.Developer community responds with enthusiasm tempered by practical requestsEarly response from the AI research and development community has been cautiously optimistic. Developers have requested versions of the model in additional formats including GGUF (a quantization format popular for local deployment) and MNN (a mobile neural network framework), suggesting interest in running the system on resource-constrained devices.\"Release MNN and GGUF so I can run it on my phone,\" wrote one developer, highlighting demand for mobile deployment options.Other developers praised Baidu&#x27;s technical choices while requesting additional resources. \"Fantastic model! Did you use discoveries from PaddleOCR?\" asked one user, referencing Baidu&#x27;s open-source optical character recognition toolkit.The model&#x27;s lengthy name—ERNIE-4.5-VL-28B-A3B-Thinking—drew lighthearted commentary. \"ERNIE-4.5-VL-28B-A3B-Thinking might be the longest model name in history,\" joked one observer. \"But hey, if you&#x27;re outperforming Gemini-2.5-Pro with only 3B active params, you&#x27;ve earned the right to a dramatic name!\"Baidu plans to showcase the ERNIE lineup during its Baidu World 2025 conference on November 13, where the company is expected to provide additional details about the model&#x27;s development, performance validation, and future roadmap.The release marks a strategic move by Baidu to establish itself as a major player in the global AI infrastructure market. While Chinese AI companies have historically focused primarily on domestic markets, the open-source release under a permissive license signals ambitions to compete internationally with Western AI giants.For enterprises, the release adds another capable option to a rapidly expanding menu of AI models. Organizations no longer face a binary choice between building proprietary systems or licensing closed-source models from a handful of vendors. The proliferation of capable open-source alternatives like ERNIE-4.5-VL-28B-A3B-Thinking is reshaping the economics of AI deployment and accelerating adoption across industries.Whether the model delivers on its performance promises in real-world deployments remains to be seen. But for organizations seeking powerful, cost-effective tools for visual understanding and reasoning, one thing is certain. As one developer succinctly summarized: \"Open source plus commercial use equals chef&#x27;s kiss. Baidu not playing around.\"",
          "feed_position": 5,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/6PAfmxLhH2Yv7BpN8vczIt/f33baa764e279c49d01c5a10da8eef61/nuneybits_Vector_art_of_a_GPU_made_out_of_computer_code_and_the_59f97a50-f492-452f-bd5d-1d6e6e904c4a.webp?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/metas-spice-framework-lets-ai-systems-teach-themselves-to-reason",
          "published_at": "Tue, 11 Nov 2025 22:21:00 GMT",
          "title": "Meta’s SPICE framework lets AI systems teach themselves to reason",
          "standfirst": "Researchers at Meta FAIR and the National University of Singapore have developed a new reinforcement learning framework for self-improving AI systems. Called Self-Play In Corpus Environments (SPICE), the framework pits two AI agents against each other, creating its own challenges and gradually improving without human supervision.While currently a proof-of-concept, this self-play mechanism could provide a basis for future AI systems that can dynamically adapt to their environments, making them more robust against the unpredictability of real-world applications.The challenge of self-improving AIThe goal of self-improving AI is to create systems that can enhance their capabilities by interacting with their environment. A common approach is reinforcement learning with verifiable rewards (RLVR), where models are rewarded for providing the correct answers to problems. This is often limited by its reliance on human-curated problem sets and domain-specific reward engineering, which makes it difficult to scale.Self-play, where a model improves by competing against itself, is another promising paradigm. But existing self-play methods for language models are often limited by two critical factors. Factual errors in generated questions and answers compound, leading to a feedback loop of hallucinations. When the problem generator and solver have information symmetry (i.e., share the same knowledge base) they fail to generate genuinely new challenges and fall into repetitive patterns. As the researchers note in their paper, “These systematic empirical failures indicate that self-improvement requires interaction with an external source providing diverse, verifiable feedback, rather than closed-loop pure introspection.”How SPICE worksSPICE is a self-play framework where a single model acts in two distinct roles. A \"Challenger\" constructs a curriculum of challenging problems from a large corpus of documents. A \"Reasoner\" then attempts to solve these problems without access to the source documents. This setup breaks the information symmetry that limits other self-play methods, as the Reasoner does not have access to the documents and knowledge that the Challenger uses to generate the problems.Grounding the tasks in a vast and diverse corpus of documents prevents hallucination by anchoring questions and answers in real-world content. This is important because for AI systems to reliably self-improve, they need external grounding sources. Therefore, LLM agents should learn from interactions with humans and the real world, not just their own outputs, to avoid compounding errors.The adversarial dynamic between the two roles creates an automatic curriculum. The Challenger is rewarded for generating problems that are both diverse and at the frontier of the Reasoner&#x27;s capability (not too easy and also not impossible). The Reasoner is rewarded for answering correctly. This symbiotic interaction pushes both agents to continuously discover and overcome new challenges. Because the system uses raw documents instead of pre-defined question-answer pairs, it can generate diverse task formats, such as multiple-choice and free-form questions. This flexibility allows SPICE to be applied to any domain, breaking the bottleneck that has confined previous methods to narrow fields like math and code. It also reduces dependence on expensive human-curated datasets for specialized domains like legal or medical analysis.SPICE in actionThe researchers evaluated SPICE on several base models, including Qwen3-4B-Base and OctoThinker-3B-Hybrid-Base. They compared its performance against baselines such as the base model with no training, a Reasoner model trained with a fixed \"Strong Challenger\" (Qwen3-32B-Instruct), and pure self-play methods like R-Zero and Absolute Zero. The evaluation covered a wide range of mathematical and general reasoning benchmarks.Across all models, SPICE consistently outperformed the baselines, delivering significant improvements in both mathematical and general reasoning tasks. The results show that the reasoning capabilities developed through corpus-grounded self-play transfer broadly across different models, thanks to the diverse external knowledge corpus they used.A key finding is that the adversarial dynamic creates an effective automatic curriculum. As training progresses, the Challenger learns to generate increasingly difficult problems. In one experiment, the Reasoner&#x27;s pass rate on a fixed set of problems increased from 55% to 85% over time, showing its improved capabilities. Meanwhile, later versions of the Challenger were able to generate questions that dropped the pass rate of an early-stage Reasoner from 55% to 35%, confirming that both roles co-evolve successfully.The researchers conclude that this approach presents a paradigm shift in self-improving reasoning methods from “closed-loop self-play that often stagnates due to hallucination drift, to open-ended improvement through interaction with the vast, verifiable knowledge embedded in web document corpora.”Currently, the corpus used for SPICE represents human experience captured in text. The ultimate goal is for self-improving systems to generate questions based on interactions with reality, including the physical world, the internet, and human interactions across multiple modalities like video, audio, and sensor data.",
          "content": "Researchers at Meta FAIR and the National University of Singapore have developed a new reinforcement learning framework for self-improving AI systems. Called Self-Play In Corpus Environments (SPICE), the framework pits two AI agents against each other, creating its own challenges and gradually improving without human supervision.While currently a proof-of-concept, this self-play mechanism could provide a basis for future AI systems that can dynamically adapt to their environments, making them more robust against the unpredictability of real-world applications.The challenge of self-improving AIThe goal of self-improving AI is to create systems that can enhance their capabilities by interacting with their environment. A common approach is reinforcement learning with verifiable rewards (RLVR), where models are rewarded for providing the correct answers to problems. This is often limited by its reliance on human-curated problem sets and domain-specific reward engineering, which makes it difficult to scale.Self-play, where a model improves by competing against itself, is another promising paradigm. But existing self-play methods for language models are often limited by two critical factors. Factual errors in generated questions and answers compound, leading to a feedback loop of hallucinations. When the problem generator and solver have information symmetry (i.e., share the same knowledge base) they fail to generate genuinely new challenges and fall into repetitive patterns. As the researchers note in their paper, “These systematic empirical failures indicate that self-improvement requires interaction with an external source providing diverse, verifiable feedback, rather than closed-loop pure introspection.”How SPICE worksSPICE is a self-play framework where a single model acts in two distinct roles. A \"Challenger\" constructs a curriculum of challenging problems from a large corpus of documents. A \"Reasoner\" then attempts to solve these problems without access to the source documents. This setup breaks the information symmetry that limits other self-play methods, as the Reasoner does not have access to the documents and knowledge that the Challenger uses to generate the problems.Grounding the tasks in a vast and diverse corpus of documents prevents hallucination by anchoring questions and answers in real-world content. This is important because for AI systems to reliably self-improve, they need external grounding sources. Therefore, LLM agents should learn from interactions with humans and the real world, not just their own outputs, to avoid compounding errors.The adversarial dynamic between the two roles creates an automatic curriculum. The Challenger is rewarded for generating problems that are both diverse and at the frontier of the Reasoner&#x27;s capability (not too easy and also not impossible). The Reasoner is rewarded for answering correctly. This symbiotic interaction pushes both agents to continuously discover and overcome new challenges. Because the system uses raw documents instead of pre-defined question-answer pairs, it can generate diverse task formats, such as multiple-choice and free-form questions. This flexibility allows SPICE to be applied to any domain, breaking the bottleneck that has confined previous methods to narrow fields like math and code. It also reduces dependence on expensive human-curated datasets for specialized domains like legal or medical analysis.SPICE in actionThe researchers evaluated SPICE on several base models, including Qwen3-4B-Base and OctoThinker-3B-Hybrid-Base. They compared its performance against baselines such as the base model with no training, a Reasoner model trained with a fixed \"Strong Challenger\" (Qwen3-32B-Instruct), and pure self-play methods like R-Zero and Absolute Zero. The evaluation covered a wide range of mathematical and general reasoning benchmarks.Across all models, SPICE consistently outperformed the baselines, delivering significant improvements in both mathematical and general reasoning tasks. The results show that the reasoning capabilities developed through corpus-grounded self-play transfer broadly across different models, thanks to the diverse external knowledge corpus they used.A key finding is that the adversarial dynamic creates an effective automatic curriculum. As training progresses, the Challenger learns to generate increasingly difficult problems. In one experiment, the Reasoner&#x27;s pass rate on a fixed set of problems increased from 55% to 85% over time, showing its improved capabilities. Meanwhile, later versions of the Challenger were able to generate questions that dropped the pass rate of an early-stage Reasoner from 55% to 35%, confirming that both roles co-evolve successfully.The researchers conclude that this approach presents a paradigm shift in self-improving reasoning methods from “closed-loop self-play that often stagnates due to hallucination drift, to open-ended improvement through interaction with the vast, verifiable knowledge embedded in web document corpora.”Currently, the corpus used for SPICE represents human experience captured in text. The ultimate goal is for self-improving systems to generate questions based on interactions with reality, including the physical world, the internet, and human interactions across multiple modalities like video, audio, and sensor data.",
          "feed_position": 6,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/UAYtncQWQ2a2g1rzWOCHR/19d3e60c8a6eef26b99a5a492491bbec/Adversarial_AI_training.jpg?w=300&q=30"
        }
      ],
      "featured_image": "https://images.ctfassets.net/jdtwqhzvc2n1/1v1ddqAyA62qgnuh9O3ek5/6c2c87800376ecfd23a29d2c01c593e4/nuneybits_Robot_and_human_working_side_by_side_in_a_modern_offi_8b67c654-4d49-42c2-a291-5a9bb86c99db.webp?w=300&q=30",
      "popularity_score": 2019.8586083333332,
      "ai_summary": [
        "Vampire Survivors VR is a new VR port for Meta Quest headsets.",
        "The game costs $10 and is available for Meta Quest 2, 3, 3S, and Pro.",
        "It offers a top-down view, controlled with a controller, not room-scale.",
        "The standalone version does not support cross-buy or save file transfers.",
        "The game includes base game and two expansions, but not all DLC."
      ]
    },
    {
      "id": "cluster_23",
      "coverage": 2,
      "updated_at": "Thu, 13 Nov 2025 13:10:05 -0500",
      "title": "Apple unveils Mini Apps Partner Program, offering a reduced 15% commission on IAPs for mini apps, or \"self-contained\" experiences built with web tech like HTML5 (Sarah Perez/TechCrunch)",
      "neutral_headline": "Apple unveils Mini Apps Partner Program, offering a reduced 15% commission on IAPs for mini apps, or \"self-contained\" experiences built with web tech like HTML5 (Sarah Perez/TechCrunch)",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/251113/p41#a251113p41",
          "published_at": "Thu, 13 Nov 2025 13:10:05 -0500",
          "title": "Apple unveils Mini Apps Partner Program, offering a reduced 15% commission on IAPs for mini apps, or \"self-contained\" experiences built with web tech like HTML5 (Sarah Perez/TechCrunch)",
          "standfirst": "Sarah Perez / TechCrunch: Apple unveils Mini Apps Partner Program, offering a reduced 15% commission on IAPs for mini apps, or &ldquo;self-contained&rdquo; experiences built with web tech like HTML5 &mdash; Apple announced on Thursday the launch of a new developer program, the Mini Apps Partner Program &hellip;",
          "content": "Sarah Perez / TechCrunch: Apple unveils Mini Apps Partner Program, offering a reduced 15% commission on IAPs for mini apps, or &ldquo;self-contained&rdquo; experiences built with web tech like HTML5 &mdash; Apple announced on Thursday the launch of a new developer program, the Mini Apps Partner Program &hellip;",
          "feed_position": 2,
          "image_url": "http://www.techmeme.com/251113/i41.jpg"
        },
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2025/11/13/apple-halves-commissions-for-mini-app-makers/",
          "published_at": "Thu, 13 Nov 2025 17:32:39 +0000",
          "title": "Apple halves commissions for mini app makers",
          "standfirst": "Apple introduces a new program for app developers offering a reduced 15% commission on qualifying mini app transactions.",
          "content": "Apple introduces a new program for app developers offering a reduced 15% commission on qualifying mini app transactions.",
          "feed_position": 4
        }
      ],
      "featured_image": "http://www.techmeme.com/251113/i41.jpg",
      "popularity_score": 2018.8963858333334,
      "ai_summary": [
        "Apple launched the Mini Apps Partner Program for developers.",
        "The program offers a reduced 15% commission on mini app transactions.",
        "Mini apps are \"self-contained\" experiences built with web tech.",
        "The program aims to encourage development of mini apps.",
        "The reduced commission applies to qualifying transactions."
      ]
    },
    {
      "id": "cluster_35",
      "coverage": 2,
      "updated_at": "Thu, 13 Nov 2025 12:25:01 -0500",
      "title": "Meta overhauls Facebook Marketplace, adding new collaborative tools, expanded social features, an improved checkout experience, and Meta AI integrations (Aisha Malik/TechCrunch)",
      "neutral_headline": "Meta overhauls Facebook Marketplace, adding new collaborative tools, expanded social features, an improved checkout experience, and Meta AI integrations (Aisha Malik/TechCrunch)",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/251113/p37#a251113p37",
          "published_at": "Thu, 13 Nov 2025 12:25:01 -0500",
          "title": "Meta overhauls Facebook Marketplace, adding new collaborative tools, expanded social features, an improved checkout experience, and Meta AI integrations (Aisha Malik/TechCrunch)",
          "standfirst": "Aisha Malik / TechCrunch: Meta overhauls Facebook Marketplace, adding new collaborative tools, expanded social features, an improved checkout experience, and Meta AI integrations &mdash; Meta is overhauling Facebook Marketplace with new collaborative tools, social features, an improved checkout experience, and Meta AI integrations, the company announced Thursday.",
          "content": "Aisha Malik / TechCrunch: Meta overhauls Facebook Marketplace, adding new collaborative tools, expanded social features, an improved checkout experience, and Meta AI integrations &mdash; Meta is overhauling Facebook Marketplace with new collaborative tools, social features, an improved checkout experience, and Meta AI integrations, the company announced Thursday.",
          "feed_position": 6,
          "image_url": "http://www.techmeme.com/251113/i37.jpg"
        },
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2025/11/13/facebook-marketplace-gets-new-collaborative-and-social-features-meta-ai-integrations/",
          "published_at": "Thu, 13 Nov 2025 17:00:00 +0000",
          "title": "Facebook Marketplace gets new collaborative and social features, Meta AI integrations",
          "standfirst": "Meta is testing collaborative buying to let users invite a friend to join their chat with a seller.",
          "content": "Meta is testing collaborative buying to let users invite a friend to join their chat with a seller.",
          "feed_position": 8
        }
      ],
      "featured_image": "http://www.techmeme.com/251113/i37.jpg",
      "popularity_score": 2018.1452747222222,
      "ai_summary": [
        "Meta is overhauling Facebook Marketplace with new features.",
        "New features include collaborative tools and social features.",
        "An improved checkout experience is being implemented.",
        "Meta AI integrations are being added to the platform.",
        "Collaborative buying allows users to invite friends to chats."
      ]
    },
    {
      "id": "cluster_40",
      "coverage": 2,
      "updated_at": "2025-11-13T12:15:39-05:00",
      "title": "Tesla is recalling over 10,000 Powerwall 2 batteries due to burn risks",
      "neutral_headline": "Tesla is recalling over 10,000 Powerwall 2 batteries due to burn risks",
      "items": [
        {
          "source": "The Verge",
          "url": "https://www.theverge.com/news/820123/tesla-recall-uscpsc-powerwall-2-batteries-overheat-fire-burn-hazard",
          "published_at": "2025-11-13T12:15:39-05:00",
          "title": "Tesla is recalling over 10,000 Powerwall 2 batteries due to burn risks",
          "standfirst": "Tesla is expanding a Powerwall 2 battery recall that started in Australia last September to the US. A ”third-party battery cell defect” can cause units to “stop functioning normally, resulting in overheating, smoking and in some cases smoke or flame causing minor property damage.” The company has received 22 reports of overheating, six reports of [&#8230;]",
          "content": "The company is proactively discharging the units remotely to make them safe until they’re replaced. | Image: Tesla Tesla is expanding a Powerwall 2 battery recall that started in Australia last September to the US. A ”third-party battery cell defect” can cause units to “stop functioning normally, resulting in overheating, smoking and in some cases smoke or flame causing minor property damage.” The company has received 22 reports of overheating, six reports of smoke, and five reports of minor property damage due to fire but no injuries, according to the US Consumer Product Safety Commission (USCPSC). The recall includes about 10,500 batteries that were sold through Tesla’s website and by certified Tesla installers across the country between November 2020 and December 2022. The affected units can be identified by the Powerwall 2 branding printed on the side, but Tesla says nearly all of them have already been remotely discharged so they no longer “pose an operational risk.” The rest will be discharged by its technicians. Tesla is removing and replacing all the affected Powerwall 2 units at no additional cost and is notifying customers through its mobile app. The company is prioritizing the replacements and says certified installers “will reach out directly by email or phone” to schedule an installation appointment. Customers with questions are encouraged to contact their original certified installers first, but can also contact Tesla’s support teams directly at 1-877-961-7652 or by email at powerwallsupportna@tesla.com for a faster response.",
          "feed_position": 8
        },
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2025/11/13/tesla-powerwall-2-recall-expands-to-us-after-reports-of-fires/",
          "published_at": "Thu, 13 Nov 2025 16:26:12 +0000",
          "title": "Tesla Powerwall 2 recall expands to US after reports of fires",
          "standfirst": "The U.S. Consumer Product Safety Commission has received 22 reports of overheating, smoking, or burning Tesla Powerwall 2 home batteries. The company will replace defective units.",
          "content": "The U.S. Consumer Product Safety Commission has received 22 reports of overheating, smoking, or burning Tesla Powerwall 2 home batteries. The company will replace defective units.",
          "feed_position": 14
        }
      ],
      "popularity_score": 2017.989163611111,
      "ai_summary": [
        "Tesla is expanding a Powerwall 2 battery recall to the US.",
        "The recall is due to a \"third-party battery cell defect.\"",
        "The defect can cause overheating, smoking, or fire.",
        "The company has received 22 reports of overheating.",
        "Tesla will replace the defective Powerwall 2 units."
      ]
    },
    {
      "id": "cluster_44",
      "coverage": 2,
      "updated_at": "Thu, 13 Nov 2025 12:10:00 -0500",
      "title": "Exowatt, which aims to use concentrated solar power to generate 1¢ per kWh electricity for data centers, raised a $120M Series A from a16z, Sam Altman, and more (Tim De Chant/TechCrunch)",
      "neutral_headline": "Exowatt, which aims to use concentrated solar power to generate 1¢ per kWh electricity for data centers, raised a $120M Series A from a16z, Sam Altman, and more (Tim De Chant/TechCrunch)",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/251113/p35#a251113p35",
          "published_at": "Thu, 13 Nov 2025 12:10:00 -0500",
          "title": "Exowatt, which aims to use concentrated solar power to generate 1¢ per kWh electricity for data centers, raised a $120M Series A from a16z, Sam Altman, and more (Tim De Chant/TechCrunch)",
          "standfirst": "Tim De Chant / TechCrunch: Exowatt, which aims to use concentrated solar power to generate 1&cent; per kWh electricity for data centers, raised a $120M Series A from a16z, Sam Altman, and more &mdash; When Hannan Happi started thinking about how to solve the AI power crisis, he kept one figure in mind: one cent per kilowatt-hour.",
          "content": "Tim De Chant / TechCrunch: Exowatt, which aims to use concentrated solar power to generate 1&cent; per kWh electricity for data centers, raised a $120M Series A from a16z, Sam Altman, and more &mdash; When Hannan Happi started thinking about how to solve the AI power crisis, he kept one figure in mind: one cent per kilowatt-hour.",
          "feed_position": 8,
          "image_url": "http://www.techmeme.com/251113/i35.jpg"
        },
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2025/11/13/sam-atlman-backed-exowatt-wants-to-power-ai-data-centers-with-billions-of-hot-rocks/",
          "published_at": "Thu, 13 Nov 2025 15:30:00 +0000",
          "title": "Sam Altman-backed Exowatt wants to power AI data centers with billions of hot rocks",
          "standfirst": "The solar-thermal startup wants to deliver electricity for as little as one cent per kWh. But first it has to scale production to 1 million units per year.",
          "content": "The solar-thermal startup wants to deliver electricity for as little as one cent per kWh. But first it has to scale production to 1 million units per year.",
          "feed_position": 17
        }
      ],
      "featured_image": "http://www.techmeme.com/251113/i35.jpg",
      "popularity_score": 2017.8949969444445,
      "ai_summary": [
        "Exowatt raised $120M in Series A funding.",
        "The company aims to generate electricity for data centers.",
        "Their goal is to produce electricity at 1 cent per kWh.",
        "Investors include a16z and Sam Altman.",
        "The company uses concentrated solar power technology."
      ]
    },
    {
      "id": "cluster_52",
      "coverage": 2,
      "updated_at": "Thu, 13 Nov 2025 11:50:00 -0500",
      "title": "TikTok launches Bulletin board, which lets brands and creators share public, one-to-many messages to their followers, similar to Instagram's broadcast channels (Aisha Malik/TechCrunch)",
      "neutral_headline": "TikTok launches Bulletin board, which lets brands and creators share public, one-to-many messages to their followers, similar to Instagram's broadcast channels (Aisha Malik/TechCrunch)",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/251113/p34#a251113p34",
          "published_at": "Thu, 13 Nov 2025 11:50:00 -0500",
          "title": "TikTok launches Bulletin board, which lets brands and creators share public, one-to-many messages to their followers, similar to Instagram's broadcast channels (Aisha Malik/TechCrunch)",
          "standfirst": "Aisha Malik / TechCrunch: TikTok launches Bulletin board, which lets brands and creators share public, one-to-many messages to their followers, similar to Instagram's broadcast channels &mdash; TikTok announced on Thursday that it's launching &ldquo;bulletin board,&rdquo; a new feature that lets brands and creators share public, one-to-many messages to their followers.",
          "content": "Aisha Malik / TechCrunch: TikTok launches Bulletin board, which lets brands and creators share public, one-to-many messages to their followers, similar to Instagram's broadcast channels &mdash; TikTok announced on Thursday that it's launching &ldquo;bulletin board,&rdquo; a new feature that lets brands and creators share public, one-to-many messages to their followers.",
          "feed_position": 9,
          "image_url": "http://www.techmeme.com/251113/i34.jpg"
        },
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2025/11/13/tiktok-launches-its-own-version-of-instagrams-broadcast-channels/",
          "published_at": "Thu, 13 Nov 2025 16:36:02 +0000",
          "title": "TikTok launches its own version of Instagram&#8217;s broadcast channels",
          "standfirst": "TikTok is rolling out “bulletin board,” a new one-to-many messaging feature that mirrors Instagram’s broadcast channels. The tool lets creators and brands share text, images, and videos directly with followers.",
          "content": "TikTok is rolling out “bulletin board,” a new one-to-many messaging feature that mirrors Instagram’s broadcast channels. The tool lets creators and brands share text, images, and videos directly with followers.",
          "feed_position": 11
        }
      ],
      "featured_image": "http://www.techmeme.com/251113/i34.jpg",
      "popularity_score": 2017.561663611111,
      "ai_summary": [
        "TikTok launched \"bulletin board\" for brands and creators.",
        "The feature allows one-to-many messaging to followers.",
        "It is similar to Instagram's broadcast channels.",
        "Users can share text, images, and videos.",
        "The feature aims to enhance creator-follower communication."
      ]
    },
    {
      "id": "cluster_18",
      "coverage": 1,
      "updated_at": "Thu, 13 Nov 2025 18:24:53 +0000",
      "title": "Civil war is brewing in the wasteland in Fallout S2 trailer",
      "neutral_headline": "Civil war is brewing in the wasteland in Fallout S2 trailer",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/culture/2025/11/civil-war-is-brewing-in-the-wasteland-in-fallout-s2-trailer/",
          "published_at": "Thu, 13 Nov 2025 18:24:53 +0000",
          "title": "Civil war is brewing in the wasteland in Fallout S2 trailer",
          "standfirst": "Ghoulish Elvis impersonators are the least of Lucy's problems as the wasteland prepares for war.",
          "content": "We got our first glimpse of the much-anticipated second season of Fallout (adapted from the popular video game franchise) in August when Prime Video released an extended teaser. We now have the official trailer, with all the deadpan humor, explosions, and mutant atrocities one could hope for—including ghoulish Elvis impersonators in New Vegas. (Spoilers for S1 below.) As previously reported, in S1, we met Lucy MacLean (Ella Purnell), a young woman whose vault is raided by surface dwellers. The raiders kill many vault residents and kidnap her father, Hank (Kyle MacLachlan), so the sheltered Lucy sets out on a quest to find him. Life on the surface is pretty brutal, but Lucy learns fast. Along the way, she finds an ally (and love interest) in Maximus (Aaron Moten), a squire masquerading as a knight of the Brotherhood of Steel. And she runs afoul of a gunslinger and bounty hunter known as the Ghoul (Walton Goggins), a former Hollywood actor named Cooper Howard who survived the original nuclear blast, but radiation exposure turned him into, well, a ghoul.Read full article Comments",
          "feed_position": 0,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/fallout3-1152x648-1763055623.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/fallout3-1152x648-1763055623.jpg",
      "popularity_score": 367.1430525,
      "ai_summary": [
        "A trailer for Fallout Season 2 has been released.",
        "The trailer hints at a brewing civil war.",
        "Ghoulish Elvis impersonators are featured in the trailer.",
        "The trailer shows the wasteland preparing for conflict.",
        "The trailer teases the plot of the upcoming season."
      ]
    },
    {
      "id": "cluster_22",
      "coverage": 1,
      "updated_at": "Thu, 13 Nov 2025 18:11:17 +0000",
      "title": "After years of saying no, Tesla reportedly adding Apple CarPlay to its cars",
      "neutral_headline": "After years of saying no, Tesla reportedly adding Apple CarPlay to its cars",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/cars/2025/11/after-years-of-saying-no-tesla-reportedly-adding-apple-carplay-to-its-cars/",
          "published_at": "Thu, 13 Nov 2025 18:11:17 +0000",
          "title": "After years of saying no, Tesla reportedly adding Apple CarPlay to its cars",
          "standfirst": "Almost half of US car buyers won't consider a car without Apple CarPlay.",
          "content": "Apple CarPlay, the interface that lets you cast your phone to your car’s infotainment screen, may finally be coming to Tesla’s electric vehicles. CarPlay is nearly a decade old at this point, and it has become so popular that almost half of car buyers have said they won’t consider a car without the feature, and the overwhelming majority of automakers have included CarPlay in their vehicles. Until now, that hasn’t included Tesla. CEO Elon Musk doesn’t appear to have opined on the omission, though he has frequently criticized Apple. In the past, Musk has said the goal of Tesla infotainment is to be “the most amount of fun you can have in a car.” Tesla has regularly added purile features like fart noises to the system, and it has also integrated video games that drivers can play while they charge. For customers who want to stream music, Tesla has instead offered Spotify, Tidal, and even Apple Music apps.Read full article Comments",
          "feed_position": 1,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-998425604-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-998425604-1152x648.jpg",
      "popularity_score": 359.9163858333333,
      "ai_summary": [
        "Tesla is reportedly planning to add Apple CarPlay to its cars.",
        "Almost half of US car buyers want Apple CarPlay.",
        "Tesla had previously resisted integrating CarPlay.",
        "The change would address consumer demand.",
        "No official announcement has been made yet."
      ]
    },
    {
      "id": "cluster_31",
      "coverage": 1,
      "updated_at": "Thu, 13 Nov 2025 17:30:50 +0000",
      "title": "Google will let Android power users bypass upcoming sideloading restrictions",
      "neutral_headline": "Google will let Android power users bypass upcoming sideloading restrictions",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2025/11/google-will-let-android-power-users-bypass-upcoming-sideloading-restrictions/",
          "published_at": "Thu, 13 Nov 2025 17:30:50 +0000",
          "title": "Google will let Android power users bypass upcoming sideloading restrictions",
          "standfirst": "Google will preserve sideloading for power users, but it hasn't decided how that will work yet.",
          "content": "Google recently decided that the freedom afforded by Android was a bit too much and announced developer verification, a system that will require developers outside the Google Play platform to register with Google. Users and developers didn’t accept Google’s rationale and have been complaining loudly. As Google begins early access testing, it has conceded that “experienced users” should have an escape hatch. According to Google, online scam and malware campaigns are getting more aggressive, and there’s real harm being done in spite of the platform’s sideloading scare screens. Google says it’s common for scammers to use social engineering to create a false sense of urgency, prompting users to bypass Android’s built-in protections to install malicious apps. Google’s solution to this problem, as announced several months ago, is to force everyone making apps to verify their identities. Unverified apps won’t install on any Google-certified device once verification rolls out. Without this, the company claims malware creators can endlessly create new apps to scam people. However, the centralized nature of verification threatened to introduce numerous headaches into a process that used to be straightforward for power users.Read full article Comments",
          "feed_position": 2,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/07/Android-statue-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/07/Android-statue-1152x648.jpg",
      "popularity_score": 343.24221916666664,
      "ai_summary": [
        "Google will allow power users to bypass sideloading restrictions.",
        "The exact method for bypassing restrictions is not yet decided.",
        "Google is preserving sideloading for advanced users.",
        "The restrictions are upcoming, but details are not yet public.",
        "The move aims to balance security and user freedom."
      ]
    },
    {
      "id": "cluster_58",
      "coverage": 1,
      "updated_at": "Thu, 13 Nov 2025 16:30:28 +0000",
      "title": "Valve says it’s still waiting for better chips to power Steam Deck 2",
      "neutral_headline": "Valve says it’s still waiting for better chips to power Steam Deck 2",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gaming/2025/11/valve-says-its-still-waiting-for-better-chips-to-power-steam-deck-2/",
          "published_at": "Thu, 13 Nov 2025 16:30:28 +0000",
          "title": "Valve says it’s still waiting for better chips to power Steam Deck 2",
          "standfirst": "Even a 50 percent performance-per-watt improvement wouldn't be enough, engineer says.",
          "content": "Yesterday’s announcement of new living room and VR hardware from Valve obviously has many gamers clamoring for any news of a more powerful version of the nearly 4-year-old Steam Deck. In a new interview with IGN, though, Valve Software Engineer Pierre-Loup Griffais says that portable gaming silicon still hasn’t advanced enough to justify brand-new benchmark hardware. “The thing we’re making sure of is that it’s a worthwhile enough performance upgrade [for a Steam Deck 2] to make sense as a standalone product,” Griffais told IGN. “We’re not interested in getting to a point where it’s 20 or 30 or even 50 percent more performance at the same battery life. We want something a little bit more demarcated than that.” “So we’ve been working back from silicon advancements and architectural improvements, and I think we have a pretty good idea of what the next version of Steam Deck is going to be, but right now there’s no offerings in that landscape, in the SoC [System on a Chip] landscape, that we think would truly be a next-gen performance Steam Deck,” Griffais continued.Read full article Comments",
          "feed_position": 3,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2023/11/IMG_8144-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2023/11/IMG_8144-1152x648.jpg",
      "popularity_score": 335.2361080555556,
      "ai_summary": [
        "Valve is waiting for better chips for Steam Deck 2.",
        "Even a 50% performance improvement is insufficient.",
        "The engineer cited performance-per-watt as a key factor.",
        "The current chips do not meet Valve's requirements.",
        "The Steam Deck 2 release is dependent on chip advancements."
      ]
    },
    {
      "id": "cluster_65",
      "coverage": 1,
      "updated_at": "Thu, 13 Nov 2025 16:11:18 +0000",
      "title": "Tracking the winds that have turned Mars into a planet of dust",
      "neutral_headline": "Mars Dust Storms and Wind Patterns Under Investigation",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2025/11/tracking-the-winds-that-have-turned-mars-into-a-planet-of-dust/",
          "published_at": "Thu, 13 Nov 2025 16:11:18 +0000",
          "title": "Tracking the winds that have turned Mars into a planet of dust",
          "standfirst": "Tracking winds across Mars' surface and their association with dust storms.",
          "content": "Mars is cold, parched, and extremely dusty. Powerful gusts of wind kick up literal tons of reddish dust that often takes the form of whorls known as dust devils. These winds also shroud the planet in dust by lifting material from the surface and blowing it into the atmosphere (what little Mars has left of an atmosphere), sometimes creating dust storms that rage for days. Researcher Valentin Bickel wanted to know just how intense winds can be on the red planet. Using data obtained by the Mars camera CaSSIS (Color and Stereo Surface Imaging System), the ExoMars Trace Gas Orbiter, and stereo camera HRSC (High Resolution Stereo Camera) on board ESA orbiter Mars Express, he and his team used deep learning to analyze stereo images that were taken seconds apart at the same location. These images can track the motion of dust devils, and the researchers use them to infer how the winds behind the dust devils move and lift dust from the surface. That dust goes on to have a big influence on the Martian weather. Bickel, of the Center for Space and Habitability at the University of Bern, noticed that the tumultuous Martian winds are even faster than previous observations had made them out to be. They carry more dust than was previously thought. “Our observations show that strong near-surface winds are abundant on Mars and play an important role in atmospheric dust sourcing, directly informing more accurate models of Mars’ atmosphere, weather, and climate,” the researchers said in a study recently published in Science Advances.Read full article Comments",
          "feed_position": 4,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/image-1152x648-1762728374.jpeg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/image-1152x648-1762728374.jpeg",
      "popularity_score": 317.9166636111111,
      "ai_summary": [
        "Scientists are studying Martian winds and their relationship to dust storms on the planet.",
        "Researchers are tracking wind patterns across the surface of Mars.",
        "The goal is to understand how these winds contribute to the formation of dust storms.",
        "This research could help predict and mitigate the effects of these storms.",
        "Understanding these processes is crucial for future Mars exploration missions."
      ]
    },
    {
      "id": "cluster_80",
      "coverage": 1,
      "updated_at": "Thu, 13 Nov 2025 14:00:57 +0000",
      "title": "Google is rolling out conversational shopping—and ads—in AI Mode search",
      "neutral_headline": "Google Introduces Conversational Shopping in AI Mode Search",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/google/2025/11/google-rolling-out-conversational-shopping-and-ads-in-ai-mode-search/",
          "published_at": "Thu, 13 Nov 2025 14:00:57 +0000",
          "title": "Google is rolling out conversational shopping—and ads—in AI Mode search",
          "standfirst": "Conversational shopping is Google's first big swing at monetizing AI Mode search.",
          "content": "In recent months, Google has promised to inject generative AI into the online shopping experience, and now it’s following through. The previously announced shopping features of AI Mode search are rolling out, and Gemini will also worm its way into Google’s forgotten Duplex automated phone call tech. It’s all coming in time for the holidays to allegedly make your gifting more convenient and also conveniently ensure that Google gets a piece of the action. At Google I/O in May, the company announced its intention to bring conversational shopping to AI Mode. According to Google, its enormous “Shopping Graph” or retailer data means its AI is uniquely positioned to deliver useful suggestions. In the coming weeks, users in the US will be able to ask AI Mode complex questions about what to buy, and it will deliver suggestions, guides, tables, and other generated content to help you decide. And since this is gen AI, it comes with the usual disclaimers about possible mistakes. AI Mode shopping features. AI Mode shopping features. You’re probably wondering where you’ll see sponsored shopping content in these experiences. Google says some of the content that appears in AI Mode will be ads, just like if you look up shopping results in a traditional search. Shopping features are also coming to the Gemini app, but Google says it won’t have sponsored content in the results for the time being.Read full article Comments",
          "feed_position": 7,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/AI-Mode-1152x648.png"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/AI-Mode-1152x648.png",
      "popularity_score": 304.7441636111111,
      "ai_summary": [
        "Google is implementing conversational shopping features within its AI Mode search function.",
        "This is Google's initial attempt to monetize its AI Mode search capabilities.",
        "The new feature allows users to interact with the search engine to make purchases.",
        "Ads will be integrated into the conversational shopping experience.",
        "This move reflects Google's strategy to capitalize on AI-driven search."
      ]
    },
    {
      "id": "cluster_70",
      "coverage": 1,
      "updated_at": "Thu, 13 Nov 2025 15:30:49 +0000",
      "title": "Waymo to roll out driverless taxis on highways in three US cities",
      "neutral_headline": "Waymo Expands Driverless Taxi Service to Highways",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/cars/2025/11/waymo-to-roll-out-driverless-taxis-on-highways-in-three-us-cities/",
          "published_at": "Thu, 13 Nov 2025 15:30:49 +0000",
          "title": "Waymo to roll out driverless taxis on highways in three US cities",
          "standfirst": "Alphabet will add routes in LA, Phoenix, and San Francisco for its driverless cars.",
          "content": "Waymo is set to expand its self-driving taxi service onto highways in three US cities on Wednesday, an advance for the venture owned by Google’s parent company that raises the stakes on safety. The company said it would add the high-speed roads to its routes in Los Angeles, Phoenix, and San Francisco, allowing Waymo cars to ferry passengers to more destinations including San Jose International Airport. Waymo’s rollout on highways marks a significant step for the robotaxi operator as it aims to encourage the mass adoption of driverless vehicles. It is the first time a company will carry out paid driverless services on the highway without a driver behind the wheel.Read full article Comments",
          "feed_position": 5,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-1608040724-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-1608040724-1152x648.jpg",
      "popularity_score": 299.2419413888889,
      "ai_summary": [
        "Waymo will introduce driverless taxi services on highways in three US cities.",
        "The expansion includes routes in Los Angeles, Phoenix, and San Francisco.",
        "This marks a significant step in the deployment of autonomous vehicle technology.",
        "Waymo is owned by Alphabet, Google's parent company.",
        "The service aims to provide a convenient and safe transportation option."
      ]
    },
    {
      "id": "cluster_74",
      "coverage": 1,
      "updated_at": "Thu, 13 Nov 2025 15:09:52 +0000",
      "title": "What would a “simplified” Starship plan for the Moon actually look like?",
      "neutral_headline": "Simplified Starship Plan for Moon Exploration Examined",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2025/11/what-would-a-simplified-starship-plan-for-the-moon-actually-look-like/",
          "published_at": "Thu, 13 Nov 2025 15:09:52 +0000",
          "title": "What would a “simplified” Starship plan for the Moon actually look like?",
          "standfirst": "The problem is that it may be difficult to find options that both NASA and SpaceX like.",
          "content": "In what will likely be his most consequential act as NASA’s interim leader, Sean Duffy said last month that the space agency was “opening up” its competition to develop a lunar lander that will put humans on the surface of the Moon. As part of this move, Duffy asked NASA’s current lunar lander contractors, SpaceX and Blue Origin, for more nimble plans. Neither has specified those plans publicly, but a recent update from SpaceX referenced a “simplified” version of the Starship system it’s building to help NASA return humans to the Moon. “Since the contract was awarded, we have been consistently responsive to NASA as requirements for Artemis III have changed and have shared ideas on how to simplify the mission to align with national priorities,” the company said. “In response to the latest calls, we’ve shared and are formally assessing a simplified mission architecture and concept of operations that we believe will result in a faster return to the Moon while simultaneously improving crew safety.”Read full article Comments",
          "feed_position": 6,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/07_24_25_HLS_on_surface_elevator_down_4d48994673-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/07_24_25_HLS_on_surface_elevator_down_4d48994673-1152x648.jpg",
      "popularity_score": 288.8927747222222,
      "ai_summary": [
        "The article explores potential simplified plans for SpaceX's Starship lunar missions.",
        "It discusses the challenges in finding solutions acceptable to both NASA and SpaceX.",
        "The focus is on streamlining the design for lunar landing and return.",
        "NASA's requirements and SpaceX's capabilities are key considerations.",
        "The goal is to optimize the Starship for efficient Moon exploration."
      ]
    },
    {
      "id": "cluster_104",
      "coverage": 1,
      "updated_at": "Wed, 12 Nov 2025 22:54:47 +0000",
      "title": "OpenAI walks a tricky tightrope with GPT-5.1’s eight new personalities",
      "neutral_headline": "OpenAI Navigates GPT-5.1 with New Personality Controls",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/ai/2025/11/openai-walks-a-tricky-tightrope-with-gpt-5-1s-eight-new-personalities/",
          "published_at": "Wed, 12 Nov 2025 22:54:47 +0000",
          "title": "OpenAI walks a tricky tightrope with GPT-5.1’s eight new personalities",
          "standfirst": "New controls attempt to please critics on both sides with a balance between bland and habit-forming.",
          "content": "On Wednesday, OpenAI released GPT-5.1 Instant and GPT-5.1 Thinking, two updated versions of its flagship AI models now available in ChatGPT. The company is wrapping the models in the language of anthropomorphism, claiming that they’re warmer, more conversational, and better at following instructions. The release follows complaints earlier this year that its previous models were excessively cheerful and sycophantic, along with an opposing controversy among users over how OpenAI modified the default GPT-5 output style after several suicide lawsuits. The company now faces intense scrutiny from lawyers and regulators that could threaten its future operations. In that kind of environment, it’s difficult to just release a new AI model, throw out a few stats, and move on like the company could even a year ago. But here are the basics: The new GPT-5.1 Instant model will serve as ChatGPT’s faster default option for most tasks, while GPT-5.1 Thinking is a simulated reasoning model that attempts to handle more complex problem-solving tasks.Read full article Comments",
          "feed_position": 8,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/09/aaGettyImages-862457080-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/09/aaGettyImages-862457080-1152x648.jpg",
      "popularity_score": 268,
      "ai_summary": [
        "OpenAI is introducing eight new personalities for its GPT-5.1 model.",
        "The new controls aim to balance user preferences and address criticism.",
        "The goal is to avoid both blandness and overly habit-forming interactions.",
        "OpenAI is attempting to satisfy diverse user expectations.",
        "The changes reflect ongoing efforts to refine AI model behavior."
      ]
    },
    {
      "id": "cluster_107",
      "coverage": 1,
      "updated_at": "Wed, 12 Nov 2025 22:12:36 +0000",
      "title": "With another record broken, the world’s busiest spaceport keeps getting busier",
      "neutral_headline": "Spaceport Activity Increases with Record-Breaking Launches",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2025/11/with-another-record-broken-the-worlds-busiest-spaceport-keeps-getting-busier/",
          "published_at": "Wed, 12 Nov 2025 22:12:36 +0000",
          "title": "With another record broken, the world’s busiest spaceport keeps getting busier",
          "standfirst": "It's not just the number of rocket launches, but how much stuff they're carrying into orbit.",
          "content": "CAPE CANAVERAL, Florida—Another Falcon 9 rocket fired off its launch pad here on Monday night, taking with it another 29 Starlink Internet satellites to orbit. This was the 94th orbital launch from Florida’s Space Coast so far in 2025, breaking the previous record for the most satellite launches in a calendar year from the world’s busiest spaceport. Monday night’s launch came two days after a Chinese Long March 11 rocket lifted off from an oceangoing platform on the opposite side of the world, marking humanity’s 255th mission to reach orbit this year, a new annual record for global launch activity. As of Wednesday, a handful of additional missions have pushed the global figure this year to 259, putting the world on pace for around 300 orbital launches by the end of 2025. This will more than double the global tally of 135 orbital launches in 2021.Read full article Comments",
          "feed_position": 9,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/IMG_9791-1-1152x648-1762927742.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/IMG_9791-1-1152x648-1762927742.jpg",
      "popularity_score": 251,
      "ai_summary": [
        "The world's busiest spaceport continues to break records for launches.",
        "The increase is not only in the number of launches but also payload mass.",
        "More items are being sent into orbit than ever before.",
        "This reflects the growing space industry and its capabilities.",
        "The trend indicates a significant expansion of space-based activities."
      ]
    },
    {
      "id": "cluster_123",
      "coverage": 1,
      "updated_at": "Wed, 12 Nov 2025 18:37:26 +0000",
      "title": "Super Mario Galaxy Movie trailer introduces Princess Rosalina",
      "neutral_headline": "Super Mario Galaxy Movie Trailer Features Princess Rosalina",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/culture/2025/11/nintendo-drops-official-trailer-for-super-mario-galaxy-movie/",
          "published_at": "Wed, 12 Nov 2025 18:37:26 +0000",
          "title": "Super Mario Galaxy Movie trailer introduces Princess Rosalina",
          "standfirst": "It's a sequel to 2023's Super Mario Bros. Movie, which racked up $1.36 billion at the box office.",
          "content": "The Super Mario Bros. Movie dominated the box office in 2023, racking up $1.36 billion and snagging several Oscar nominations for good measure. So naturally there’s a sequel, and Nintendo just dropped the official trailer for The Super Mario Galaxy Movie, due out next spring. (Spoilers for the 2023 film below.) The first attempt at a Super Mario movie adaptation in 1993 was notoriously a dismal failure, although it still has its ’90s-nostalgic fans. But 2023’s Super Mario Bros. Movie won over gaming fans who were skeptical about another adaption—including Ars Senior Gaming Editor Kyle Orland. “This film version captures all the fun and vibrancy of the Mario games, with enough references to familiar characters, items, and locations to make even a die-hard Mario fan’s head spin,” he wrote in his 2023 review, adding that, despite a few flaws, the film was “everything that a 10-year-old version of me could ever have dreamed a Mario movie could be.”Read full article Comments",
          "feed_position": 15,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/galaxy6-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/galaxy6-1152x648.jpg",
      "popularity_score": 148,
      "ai_summary": [
        "A trailer for the Super Mario Galaxy movie has been released.",
        "The film is a sequel to the 2023 Super Mario Bros. Movie.",
        "The original film earned $1.36 billion at the global box office.",
        "Princess Rosalina is a featured character in the new movie.",
        "The sequel aims to build on the success of the original film."
      ]
    },
    {
      "id": "cluster_124",
      "coverage": 1,
      "updated_at": "Wed, 12 Nov 2025 18:27:27 +0000",
      "title": "OpenAI slams court order that lets NYT read 20 million complete user chats",
      "neutral_headline": "OpenAI slams court order that lets NYT read 20 million complete user chats",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/11/openai-fights-order-to-hand-over-20-million-private-chatgpt-conversations/",
          "published_at": "Wed, 12 Nov 2025 18:27:27 +0000",
          "title": "OpenAI slams court order that lets NYT read 20 million complete user chats",
          "standfirst": "OpenAI: NYT wants evidence of ChatGPT users trying to get around news paywall.",
          "content": "OpenAI wants a court to reverse a ruling forcing the ChatGPT maker to give 20 million user chats to The New York Times and other news plaintiffs that sued it over alleged copyright infringement. Although OpenAI previously offered 20 million user chats as a counter to the NYT’s demand for 120 million, the AI company says a court order requiring production of the chats is too broad. “The logs at issue here are complete conversations: each log in the 20 million sample represents a complete exchange of multiple prompt-output pairs between a user and ChatGPT,” OpenAI said today in a filing in US District Court for the Southern District of New York. “Disclosure of those logs is thus much more likely to expose private information [than individual prompt-output pairs], in the same way that eavesdropping on an entire conversation reveals more private information than a 5-second conversation fragment.” OpenAI’s filing said that “more than 99.99%” of the chats “have nothing to do with this case.” It asked the district court to “vacate the order and order News Plaintiffs to respond to OpenAI’s proposal for identifying relevant logs.” OpenAI could also seek review in a federal court of appeals.Read full article Comments",
          "feed_position": 16,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/chatgpt-app-icon-1152x648-1762971088.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/chatgpt-app-icon-1152x648-1762971088.jpg",
      "popularity_score": 148,
      "ai_summary": [
        "OpenAI is criticizing a court order allowing The New York Times access to user chats.",
        "The company claims the NYT seeks evidence of users bypassing paywalls.",
        "The legal dispute centers on the use of ChatGPT and its data.",
        "OpenAI is contesting the court's decision.",
        "The case highlights issues of user privacy and data access."
      ]
    },
    {
      "id": "cluster_113",
      "coverage": 1,
      "updated_at": "Wed, 12 Nov 2025 20:38:01 +0000",
      "title": "Microsoft releases update-fixing update for update-eligible Windows 10 PCs",
      "neutral_headline": "Microsoft Releases Update to Fix Update Issues for Windows 10",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2025/11/microsoft-releases-update-fixing-update-for-update-eligible-windows-10-pcs/",
          "published_at": "Wed, 12 Nov 2025 20:38:01 +0000",
          "title": "Microsoft releases update-fixing update for update-eligible Windows 10 PCs",
          "standfirst": "A bug was keeping Windows 10 PCs from enrolling in Microsoft's ESU program.",
          "content": "Officially, Windows 10 died last month, a little over a decade after its initial release. But the old operating system’s enduring popularity has prompted Microsoft to promise between one and three years of Extended Security Updates (ESUs) for many Windows 10 PCs. For individuals with Windows 10 PCs, it’s relatively easy to get an additional year of updates at no cost. Or at least, it’s supposed to be. Bugs initially identified by Windows Latest were keeping some Windows 10 PCs from successfully enrolling in the ESU program, preventing those PCs from signing up to grab the free updates. And because each Windows 10 PC has to be manually enrolled in the program, a broken enrollment process also meant broken security updates. To fix the problems, Microsoft released an update for Windows 10 22H2 (KB5071959) this week that both acknowledges and fixes an issue “where the enrollment wizard may fail during enrollment.” It’s being offered to all Windows 10 PCs regardless of whether they’re enrolled in the ESU program “as it resolves an issue that was preventing affected customers from receiving essential security updates.”Read full article Comments",
          "feed_position": 10,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2023/12/win10-new-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2023/12/win10-new-1152x648.jpg",
      "popularity_score": 145,
      "ai_summary": [
        "Microsoft released an update to resolve update problems for Windows 10 PCs.",
        "The bug prevented some PCs from enrolling in the ESU program.",
        "The update addresses issues related to the Extended Security Updates program.",
        "The fix ensures that eligible Windows 10 PCs can receive updates.",
        "This update aims to maintain security for Windows 10 users."
      ]
    },
    {
      "id": "cluster_127",
      "coverage": 1,
      "updated_at": "Wed, 12 Nov 2025 17:35:36 +0000",
      "title": "Corals survived past climate changes by retreating to the deeps",
      "neutral_headline": "Corals Survived Past Climate Changes by Retreating Deep",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2025/11/corals-survived-past-climate-changes-by-retreating-to-the-deeps/",
          "published_at": "Wed, 12 Nov 2025 17:35:36 +0000",
          "title": "Corals survived past climate changes by retreating to the deeps",
          "standfirst": "A recent die-off in Florida puts the spotlight on corals' survival strategies.",
          "content": "Scientists have found that the 2023 marine heat wave caused “functional extinction” of two Acropora reef-building coral species living in the Florida Reef, which stretches from the Dry Tortugas National Park to Miami. “At this point, we do not think there’s much of a chance for natural recovery—their numbers are so low that successful reproduction is incredibly unlikely,” said Ross Cunning, a coral biologist at the John G. Shedd Aquarium. This isn’t the first time corals have faced the borderline of extinction over the last 460 million years, and they have always managed to bounce back and recolonize habitats lost during severe climate changes. The problem is that we won’t live long enough to see them doing that again.Read full article Comments",
          "feed_position": 19,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-1708146111-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-1708146111-1152x648.jpg",
      "popularity_score": 141,
      "ai_summary": [
        "Corals have survived past climate changes by retreating to deeper waters.",
        "A recent die-off in Florida highlights coral survival strategies.",
        "The article examines how corals adapt to changing environmental conditions.",
        "Deep-water refuges have been crucial for coral survival.",
        "The study provides insights into coral resilience and conservation."
      ]
    },
    {
      "id": "cluster_119",
      "coverage": 1,
      "updated_at": "Wed, 12 Nov 2025 19:17:52 +0000",
      "title": "Audi goes full minimalism for its first-ever Formula 1 livery",
      "neutral_headline": "Audi Unveils Minimalist Livery for Formula 1 Entry",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/cars/2025/11/audi-goes-full-minimalism-for-its-first-ever-formula-1-livery/",
          "published_at": "Wed, 12 Nov 2025 19:17:52 +0000",
          "title": "Audi goes full minimalism for its first-ever Formula 1 livery",
          "standfirst": "Audi says it wants to be an F1 title contender by 2030.",
          "content": "Audi provided flights from Washington, DC, to Munich and accommodation so Ars could visit its motorsports facility and see its F1 car. Ars does not accept paid editorial content. MUNICH, Germany—Audi’s long-awaited Formula 1 team gave the world its first look at what the Audi R26 will look like when it takes to the track next year. Well, sort of—the car you see here is a generic show car for the 2026 aero regulations, but the livery you see, plus the sponsors’ logos, will race next year. “By entering the pinnacle of motorsport, Audi is making a clear, ambitious statement. It is the next chapter in the company’s renewal. Formula 1 will be a catalyst for the change towards a leaner, faster, and more innovative Audi,” said Gernot Döllner, Audi’s CEO. “We are not entering Formula 1 just to be there. We want to win. At the same time, we know that you don’t become a top team in Formula 1 overnight. It takes time, perseverance, and tireless questioning of the status quo. By 2030, we want to fight for the World Championship title,” Döllner said. After the complicated liveries of cars like the R18 or Audi's Formula E program, the R26 is refreshingly simple. Credit: Jonathan Gitlin None of the sponsors have been announced yet, so the car is bare for now. Credit: Jonathan Gitlin The view Audi hopes its rivals get next year. Credit: Jonathan Gitlin Read full article Comments",
          "feed_position": 13,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/Audi-F1-livery-reveal-8-of-9-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/Audi-F1-livery-reveal-8-of-9-1152x648.jpg",
      "popularity_score": 139,
      "ai_summary": [
        "Audi has revealed a minimalist livery for its Formula 1 debut.",
        "The company aims to become an F1 title contender by 2030.",
        "The design reflects Audi's branding and racing ambitions.",
        "The livery represents Audi's entry into Formula 1 racing.",
        "Audi is investing in its F1 program to achieve success."
      ]
    },
    {
      "id": "cluster_114",
      "coverage": 1,
      "updated_at": "Wed, 12 Nov 2025 20:28:52 +0000",
      "title": "An explosion 92 million miles away just grounded Jeff Bezos’ New Glenn rocket",
      "neutral_headline": "An explosion 92 million miles away just grounded Jeff Bezos’ New Glenn rocket",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2025/11/an-explosion-92-million-miles-away-just-grounded-jeff-bezos-new-glenn-rocket/",
          "published_at": "Wed, 12 Nov 2025 20:28:52 +0000",
          "title": "An explosion 92 million miles away just grounded Jeff Bezos’ New Glenn rocket",
          "standfirst": "\"NASA is postponing launch until space weather conditions improve.\"",
          "content": "CAPE CANAVERAL, Florida—The second flight of Blue Origin’s New Glenn rocket was postponed again Wednesday as a supercharged wave of magnetized plasma from the Sun enveloped the Earth, triggering colorful auroral displays and concerns over possible impacts to communications, navigation, and power grids. Solar storms like the one this week can also affect satellite operations. That is the worry that caused NASA to hold off on launching a pair of science probes from Cape Canaveral Space Force Station, Florida, on Wednesday aboard Blue Origin’s New Glenn rocket. In a statement, Blue Origin said NASA, its customer on the upcoming launch, decided to postpone the mission to send the agency’s two ESCAPADE spacecraft on a journey to Mars.Read full article Comments",
          "feed_position": 11,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/Oct_2_M7pt3_Flare_171-304-131_Crop-1152x648-1762966677.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/Oct_2_M7pt3_Flare_171-304-131_Crop-1152x648-1762966677.jpg",
      "popularity_score": 133,
      "ai_summary": [
        "An explosion on the sun has caused a delay in the New Glenn rocket launch.",
        "NASA is postponing the launch due to space weather conditions.",
        "The solar event has impacted launch readiness.",
        "The launch is delayed until conditions improve.",
        "The postponement is a safety measure related to space weather."
      ]
    },
    {
      "id": "cluster_116",
      "coverage": 1,
      "updated_at": "Wed, 12 Nov 2025 19:53:07 +0000",
      "title": "Well-received big-budget Alien Earth TV series gets a second season",
      "neutral_headline": "Well-received big-budget Alien Earth TV series gets a second season",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/culture/2025/11/alien-earth-and-series-creator-noah-hawley-will-return-for-season-2/",
          "published_at": "Wed, 12 Nov 2025 19:53:07 +0000",
          "title": "Well-received big-budget Alien Earth TV series gets a second season",
          "standfirst": "Production will move from Thailand to London, suggesting a new setting.",
          "content": "Alien Earth will return to FX (and Disney+ and Hulu) for a second season, thanks to a new deal between Disney and series creator Noah Hawley. The new season has no air date yet, but we do know one thing about it: It will be shot in London. The first season was shot in Thailand, and most of the story took place in Southeast Asia, so the change in shooting location suggests a new setting for much of the next season. Production on season two will reportedly begin next year. For those who watched season one to its conclusion, season two probably seemed like a sure thing; the finale resolved many of the core conflicts of that first batch of episodes, but also was clearly intended to be the launching point for a new storyline in season two.Read full article Comments",
          "feed_position": 12,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/alien-earth-facehugger-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/alien-earth-facehugger-1152x648.jpg",
      "popularity_score": 133,
      "ai_summary": [
        "A big-budget Alien Earth TV series has been renewed for a second season.",
        "Production will move from Thailand to London for the new season.",
        "The change in location suggests a shift in the setting.",
        "The series has received positive reviews and audience reception.",
        "The second season will continue the story with new elements."
      ]
    },
    {
      "id": "cluster_125",
      "coverage": 1,
      "updated_at": "Wed, 12 Nov 2025 18:00:43 +0000",
      "title": "Valve rejoins the VR hardware wars with standalone Steam Frame",
      "neutral_headline": "Valve rejoins the VR hardware wars with standalone Steam Frame",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gaming/2025/11/valve-rejoins-the-vr-hardware-wars-with-standalone-steam-frame/",
          "published_at": "Wed, 12 Nov 2025 18:00:43 +0000",
          "title": "Valve rejoins the VR hardware wars with standalone Steam Frame",
          "standfirst": "SteamOS-powered headset sports semi-modular design, wireless \"low-latency\" PC streaming.",
          "content": "Six years ago, Valve made its second big virtual reality push, launching the Valve Index headset alongside VR blockbuster Half-Life Alyx. Since then, the company seems to have lost interest in virtual reality gaming, letting competitors like Meta release regular standalone hardware updates as the PC-tethered Index continued to age. Now, after years of rumors, Valve is finally ready to officially rejoin the VR hardware race. The Steam Frame, set to launch in early 2026, will run both VR and traditional Steam games locally through SteamOS or stream them wirelessly from a local PC. Powered by a Snapdragon 8 Gen 3 processor with 16 GB of RAM, the Steam Frame sports a 2160 x 2160 resolution display per eye at an “up to 110 degrees” field-of-view and up to 144 Hz. That’s all roughly in line with 2023’s Meta Quest 3, which runs on the slightly less performant Snapdragon XR2 Gen 2 processor. Valve’s new headset will be available in models sporting 256GB and 1TB or internal storage, both with the option for expansion via a microSD card slot. Pricing details have not yet been revealed publicly.Read full article Comments",
          "feed_position": 17,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/SF_headsetControllers_3Q-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/SF_headsetControllers_3Q-1152x648.jpg",
      "popularity_score": 133,
      "ai_summary": [
        "Valve is re-entering the VR hardware market with the Steam Frame headset.",
        "The headset features a standalone design and SteamOS.",
        "It offers wireless, low-latency PC streaming capabilities.",
        "The design is semi-modular for user customization.",
        "Valve aims to provide an immersive VR experience."
      ]
    },
    {
      "id": "cluster_126",
      "coverage": 1,
      "updated_at": "Wed, 12 Nov 2025 18:00:37 +0000",
      "title": "Steam Deck minus the screen: Valve announces new Steam Machine, Controller hardware",
      "neutral_headline": "Steam Deck minus the screen: Valve announces new Steam Machine, Controller hardware",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gaming/2025/11/steam-deck-minus-the-screen-valve-announces-new-steam-machine-controller-hardware/",
          "published_at": "Wed, 12 Nov 2025 18:00:37 +0000",
          "title": "Steam Deck minus the screen: Valve announces new Steam Machine, Controller hardware",
          "standfirst": "SteamOS-powered cube for your TV targets early 2026 launch, no pricing details.",
          "content": "Nearly four years after the Steam Deck changed the world of portable gaming, Valve is getting ready to release SteamOS-powered hardware designed for the living room TV, or even as a desktop PC gaming replacement. The simply named Steam Machine and Steam Controller, both planned to ship in early 2026, are “optimized for gaming on Steam and designed for players to get even more out of their Steam Library,” Valve said in a press release. A Steam Machine spec sheet shared by Valve lists a “semi-custom” six-core AMD Zen 4 CPU clocked at up to 4.8 Ghz alongside an AMD RDNA3 GPU with 28 compute units. The motherboard will include 16GB of DDR5 RAM and an additional 8GB of dedicated DDR6 VRAM for the GPU. The new hardware will come in two configurations with 512GB or 2TB of unspecified “SSD storage,” though Valve isn’t sharing pricing for either just yet. If you squint, you can make out a few ports on this unmarked black square. Credit: Valve A strip of LEDs adds a touch of color to the front face of the Steam Machine. I'm a fan of the big fan. Credit: Valve Those chips and numbers suggest the Steam Machine will have roughly the same horsepower as a mid-range desktop gaming PC from a few years back. But Valve says its “Machine”—which it ranks as “over 6x more powerful than the Steam Deck”—is powerful enough to support ray-tracing and/or 4K, 60 fps gaming using FSR upscaling.Read full article Comments",
          "feed_position": 18,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/SM_3Q-1152x648-1762899545.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/SM_3Q-1152x648-1762899545.jpg",
      "popularity_score": 133,
      "ai_summary": [
        "Valve announced new Steam Machine and Controller hardware.",
        "The SteamOS-powered cube is designed for TV use.",
        "The target launch date is early 2026.",
        "Pricing details for the new hardware are not yet available.",
        "The new products expand Valve's hardware offerings."
      ]
    },
    {
      "id": "cluster_122",
      "coverage": 1,
      "updated_at": "Wed, 12 Nov 2025 18:59:38 +0000",
      "title": "Quantum computing tech keeps edging forward",
      "neutral_headline": "Quantum computing tech keeps edging forward",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2025/11/quantum-roundup-lots-of-companies-announcing-new-tech/",
          "published_at": "Wed, 12 Nov 2025 18:59:38 +0000",
          "title": "Quantum computing tech keeps edging forward",
          "standfirst": "IBM follows through on its June promises, plus more trapped ion news.",
          "content": "The end of the year is usually a busy time in the quantum computing arena, as companies often try to announce that they’ve reached major milestones before the year wraps up. This year has been no exception. And while not all of these announcements involve interesting new architectures like the one we looked at recently, they’re a good way to mark progress in the field, and they often involve the sort of smaller, incremental steps needed to push the field forward. What follows is a quick look at a handful of announcements from the past few weeks that struck us as potentially interesting. IBM follows through IBM is one of the companies announcing a brand-new architecture this year. That’s not at all a surprise, given that the company promised to do so back in June; this week sees the company confirming that it has built the two processors it said it would earlier in the year. These include one called Loon, which is focused on the architecture that IBM will use to host error-corrected logical qubits. Loon represents two major changes for the company: a shift to nearest-neighbor connections and the addition of long-distance connections.Read full article Comments",
          "feed_position": 14,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/IBM-Quantum-Loon-Wafer_2-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/IBM-Quantum-Loon-Wafer_2-1152x648.jpg",
      "popularity_score": 130
    }
  ]
}