{
  "updated_at": "2026-01-13T23:20:10.253Z",
  "clusters": [
    {
      "id": "cluster_5",
      "coverage": 2,
      "updated_at": "Tue, 13 Jan 2026 17:25:00 -0500",
      "title": "As part of its Reality Labs cuts, Meta is closing three VR gaming studios and will stop developing new content and features for its VR fitness app Supernatural (Jay Peters/The Verge)",
      "neutral_headline": "Meta is closing down three VR studios as part of its metaverse cuts",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/260113/p47#a260113p47",
          "published_at": "Tue, 13 Jan 2026 17:25:00 -0500",
          "title": "As part of its Reality Labs cuts, Meta is closing three VR gaming studios and will stop developing new content and features for its VR fitness app Supernatural (Jay Peters/The Verge)",
          "standfirst": "Jay Peters / The Verge: As part of its Reality Labs cuts, Meta is closing three VR gaming studios and will stop developing new content and features for its VR fitness app Supernatural &mdash; &#65279;Developers from Twisted Pixel Games and Sanzaru Games have posted about their studios being closed down.",
          "content": "Jay Peters / The Verge: As part of its Reality Labs cuts, Meta is closing three VR gaming studios and will stop developing new content and features for its VR fitness app Supernatural &mdash; &#65279;Developers from Twisted Pixel Games and Sanzaru Games have posted about their studios being closed down.",
          "feed_position": 2,
          "image_url": "http://www.techmeme.com/260113/i47.jpg"
        },
        {
          "source": "The Verge",
          "url": "https://www.theverge.com/news/861420/meta-reality-labs-layoffs-vr-studios-twisted-pixel-sanzaru-armature",
          "published_at": "2026-01-13T13:51:34-05:00",
          "title": "Meta is closing down three VR studios as part of its metaverse cuts",
          "standfirst": "Meta is laying off about 10 percent of its Reality Labs metaverse division, and the cuts include closing down some of its VR gaming studios. Twisted Pixel Games, the developer of Marvel's Deadpool VR, Sanzaru Games, the developer of the Asgard's Wrath franchise, and Armature Studio, which worked on the Resident Evil 4 VR port, [&#8230;]",
          "content": "Marvel’s Deadpool VR. Meta is laying off about 10 percent of its Reality Labs metaverse division, and the cuts include closing down some of its VR gaming studios. Twisted Pixel Games, the developer of Marvel's Deadpool VR, Sanzaru Games, the developer of the Asgard's Wrath franchise, and Armature Studio, which worked on the Resident Evil 4 VR port, are all being closed down, according to an internal memo viewed by Bloomberg. The team behind the VR fitness app Supernatural will no longer develop new content or features for it, though the \"existing product\" will still be supported, Bloomberg says. Meta spokesperson Tracy Clayton confirmed to The Verge that Bloombe … Read the full story at The Verge.",
          "feed_position": 8
        }
      ],
      "featured_image": "http://www.techmeme.com/260113/i47.jpg",
      "popularity_score": 2019.0804852777778
    },
    {
      "id": "cluster_11",
      "coverage": 2,
      "updated_at": "Tue, 13 Jan 2026 22:03:20 +0000",
      "title": "Roblox's age verification system is reportedly a trainwreck",
      "neutral_headline": "Roblox's age verification system is reportedly a trainwreck",
      "items": [
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/gaming/robloxs-age-verification-system-is-reportedly-a-trainwreck-220320016.html",
          "published_at": "Tue, 13 Jan 2026 22:03:20 +0000",
          "title": "Roblox's age verification system is reportedly a trainwreck",
          "standfirst": "Roblox's age-verification system was designed as a response to allegations it has a child predator problem. Less than a week in, how's it going? Well, Wired reported on Tuesday that, in some cases, it's classifying children as adults and adults as children. So, not so great!Last week, Roblox made age verification mandatory for anyone using the platform's chat feature. That process involves either submitting a facial age estimate via selfie or (optionally for anyone 13 or older) uploading a government ID check. After verifying, you can only chat with groups of players around your age.The move came after reports grew of predators using the platform to groom young children. That, in turn, led to lawsuits from Louisiana, Texas and Kentucky. Meanwhile, Florida's attorney general has issued criminal subpoenas.So, it might not be hyperbole to say Roblox's survival could depend on how it handles this problem. It isn't exactly off to a hot start. There are reports of a 23-year-old being misidentified as a 16- to 17-year-old. (\"I don't want to be chatting with fucking children,\" they said.) Another report claimed an 18-year-old was placed in the 13 to 15 range.But the problem is happening in reverse, too. Online videos show children spoofing the system into believing they were adults by using avatar images. One clever kid drew wrinkles and stubble on his face and was instantly deemed 21+. Another flashed a photo of Kurt Cobain and got an adult classification.The feature isn't working as planned, to say the least.RobloxIn addition, Roblox posted last week that some parents were providing age checks on behalf of their children, leading to their children being placed in the 21+ category. The company said it's \"working on solutions to address\" that particular problem and will share more soon.Developers with games on Roblox are upset. The platform's dev forum includes thousands of negative comments about the updates, with many of them wanting the entire update reversed. One shared a graph showing that the percentage using the chat feature dropped from around 90 percent to 36.5 percent.Where does this leave Roblox? Well, with some developers describing games on the platform as feeling \"lifeless\" or like \"a total ghost town,\" the company has its hands full. It will have to figure out how to balance its priorities of keeping predators out without breaking things for everyone else. The full report from Wired is worth a read.This article originally appeared on Engadget at https://www.engadget.com/gaming/robloxs-age-verification-system-is-reportedly-a-trainwreck-220320016.html?src=rss",
          "content": "Roblox's age-verification system was designed as a response to allegations it has a child predator problem. Less than a week in, how's it going? Well, Wired reported on Tuesday that, in some cases, it's classifying children as adults and adults as children. So, not so great!Last week, Roblox made age verification mandatory for anyone using the platform's chat feature. That process involves either submitting a facial age estimate via selfie or (optionally for anyone 13 or older) uploading a government ID check. After verifying, you can only chat with groups of players around your age.The move came after reports grew of predators using the platform to groom young children. That, in turn, led to lawsuits from Louisiana, Texas and Kentucky. Meanwhile, Florida's attorney general has issued criminal subpoenas.So, it might not be hyperbole to say Roblox's survival could depend on how it handles this problem. It isn't exactly off to a hot start. There are reports of a 23-year-old being misidentified as a 16- to 17-year-old. (\"I don't want to be chatting with fucking children,\" they said.) Another report claimed an 18-year-old was placed in the 13 to 15 range.But the problem is happening in reverse, too. Online videos show children spoofing the system into believing they were adults by using avatar images. One clever kid drew wrinkles and stubble on his face and was instantly deemed 21+. Another flashed a photo of Kurt Cobain and got an adult classification.The feature isn't working as planned, to say the least.RobloxIn addition, Roblox posted last week that some parents were providing age checks on behalf of their children, leading to their children being placed in the 21+ category. The company said it's \"working on solutions to address\" that particular problem and will share more soon.Developers with games on Roblox are upset. The platform's dev forum includes thousands of negative comments about the updates, with many of them wanting the entire update reversed. One shared a graph showing that the percentage using the chat feature dropped from around 90 percent to 36.5 percent.Where does this leave Roblox? Well, with some developers describing games on the platform as feeling \"lifeless\" or like \"a total ghost town,\" the company has its hands full. It will have to figure out how to balance its priorities of keeping predators out without breaking things for everyone else. The full report from Wired is worth a read.This article originally appeared on Engadget at https://www.engadget.com/gaming/robloxs-age-verification-system-is-reportedly-a-trainwreck-220320016.html?src=rss",
          "feed_position": 1,
          "image_url": "https://d29szjachogqwa.cloudfront.net/images/user-uploaded/EN_Age_Check_Flow_1_9823.png"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/ai/senate-passes-defiance-act-for-a-second-time-to-address-grok-deepfakes-212151712.html",
          "published_at": "Tue, 13 Jan 2026 21:21:51 +0000",
          "title": "Senate passes Defiance Act for a second time to address Grok deepfakes",
          "standfirst": "The Senate has passed the Disrupt Explicit Forged Images and Non-Consensual Edits (DEFIANCE ) Act with unanimous consent, according to the bill’s co-sponsor Senator Dick Durbin (D-IL). The bill lets the subjects of nonconsensual, sexually explicit deepfakes take civil action against the people who create and host them.Deepfakes are a known issue online, but without the proper protections, easy access to AI-powered image and video generation tools has made it possible for anyone to create compromising content using another person's likeness. This has become a particular problem on X, where the integration of Grok, the AI assistant created by X's parent company xAI, makes it possible for anyone to turn the content of another person's post into an image-generating prompt. Over the last month, that's allowed users to create sexually explicit images of children, just by replying to a post with @grok and a request. In response, Ofcom, the UK's media regulator, has already opened an investigation into X for potentially violating the Online Safety Act. The chatbot has also been outright blocked in Malaysia and Indonesia. The DEFIANCE Act won't prevent Grok or other AI tools from generating nonconsensual deepfakes, but it would make creating or hosting that content potentially very expensive for anyone on the receiving end of a lawsuit.The Senate passed an earlier version of the DEFIANCE Act in 2024, but it stalled in the House. Given the urgency of Grok's deepfake problem, the hope is this new version of the bill won't see the same resistance. Congress passed an earlier piece of deepfake regulation last year, the Take It Down Act, with bipartisan support. That bill was focused on the companies who host nonconsensual, sexually explicit content, rather than the people exploited by it.This article originally appeared on Engadget at https://www.engadget.com/ai/senate-passes-defiance-act-for-a-second-time-to-address-grok-deepfakes-212151712.html?src=rss",
          "content": "The Senate has passed the Disrupt Explicit Forged Images and Non-Consensual Edits (DEFIANCE ) Act with unanimous consent, according to the bill’s co-sponsor Senator Dick Durbin (D-IL). The bill lets the subjects of nonconsensual, sexually explicit deepfakes take civil action against the people who create and host them.Deepfakes are a known issue online, but without the proper protections, easy access to AI-powered image and video generation tools has made it possible for anyone to create compromising content using another person's likeness. This has become a particular problem on X, where the integration of Grok, the AI assistant created by X's parent company xAI, makes it possible for anyone to turn the content of another person's post into an image-generating prompt. Over the last month, that's allowed users to create sexually explicit images of children, just by replying to a post with @grok and a request. In response, Ofcom, the UK's media regulator, has already opened an investigation into X for potentially violating the Online Safety Act. The chatbot has also been outright blocked in Malaysia and Indonesia. The DEFIANCE Act won't prevent Grok or other AI tools from generating nonconsensual deepfakes, but it would make creating or hosting that content potentially very expensive for anyone on the receiving end of a lawsuit.The Senate passed an earlier version of the DEFIANCE Act in 2024, but it stalled in the House. Given the urgency of Grok's deepfake problem, the hope is this new version of the bill won't see the same resistance. Congress passed an earlier piece of deepfake regulation last year, the Take It Down Act, with bipartisan support. That bill was focused on the companies who host nonconsensual, sexually explicit content, rather than the people exploited by it.This article originally appeared on Engadget at https://www.engadget.com/ai/senate-passes-defiance-act-for-a-second-time-to-address-grok-deepfakes-212151712.html?src=rss",
          "feed_position": 3
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/orchestration/why-egnyte-keeps-hiring-junior-engineers-despite-the-rise-of-ai-coding-tools",
          "published_at": "Tue, 13 Jan 2026 21:00:00 GMT",
          "title": "Why Egnyte keeps hiring junior engineers despite the rise of AI coding tools",
          "standfirst": "Egnyte, the $1.5 billion cloud content governance company, has embedded AI coding tools across its global team of more than 350 developers — but not to reduce headcount. Instead, the company continues to hire junior engineers, using AI to accelerate onboarding, deepen codebase understanding, and shorten the path from junior to senior contributor. The approach challenges a dominant 2025 narrative that automation will replace developers, showing instead how enterprises are using AI to scale engineering capacity while keeping humans firmly in the loop.“To have engineers disappear or us not hiring junior engineers doesn&#x27;t look like the likely outcome,” Amrit Jassal, Egnyte CTO and co-founder, told VentureBeat. “You&#x27;ve got to have people, you&#x27;re training and doing all types of succession planning. The junior engineer of today is the senior engineer of tomorrow.”How Egnyte coders are using AI — without ceding controlEgnyte — which has more than 22,000 users including NASDAQ, Red Bull, and BuzzFeed — has rolled out Claude Code, Cursor, Augment, and Gemini CLI coding tools across its developer base to support its core business strategies and expand its newer AI offerings like customer-facing copilots and customizable AI agents. Devs use these tools across a variety of tasks, the simplest of which include data retrieval, code comprehension, smart search, and code lookup. Egnyte’s code base has lots of Java code, which uses numerous libraries, each with different versions, Jassal explained. AI tools are great for peer-to-peer programming, helping new users get a lay of the land, or existing users probe into different code repositories. “We have a pretty big code base, right?” Jassal said. “Let&#x27;s say you&#x27;re looking at an iOS application, but you&#x27;re not well versed; you will fire up Google CLI or an Augment, and ask it to discover the code base.”Some Egnyte devs are moving into automatic pull request summaries, which provide simple overviews of code changes that essentially explain the “what,” “how,” and “why” of proposed modifications. “But obviously, any change that&#x27;s made, we don&#x27;t want to hear that AI made the change; it has to be that developer made the change,” Jassal pointed out. “I would not trust AI to commit to the production code base.” Commits still pass through human review and security validation, and anything red-flagged is escalated to senior engineers. Devs are warned of the dangers of settling into autopilot mode or blindly trusting code. A model may not have been exposed to, or given enough samples of, certain coding components and infrastructure in its training. Another growing, and closely monitored, use case for AI is unit testing, where code components are run in isolation to ensure they work as intended. “At the end of the day, it is a productivity improvement tool,” he said. “It is really a continuation, it&#x27;s like any other tool, it&#x27;s not some magic.”Beyond core engineering, AI is helping other teams collaborate with programmers. Product management, for instance, is using tools like Vercel to bring “demo-worthy” prototypes, rather than just ideas, to devs, who can then move ahead with mock-ups. Or, if UX teams are looking to change certain elements on a dashboard, AI can quickly spin up a handful of options, like different widgets or buttons. “Then you come to engineering with that, and the engineer immediately knows what you really intend to do with it,” Jassal said. Setting expectations, meeting devs where they areHowever, day-to-day activities for all Egnyte engineers, including junior developers, extend beyond just coding. Junior developers are given hands-on tasks across the full development lifecycle to accelerate their growth and experience, Jassal said. For instance, they assist with requirement analysis in early software engineering phases, as well as deployment, productization and post-deployment maintenance.In turn, these activities require “Egnyte-specific tacit knowledge and experience” offered by senior engineers. One clear example of work that sits firmly with senior engineers is authoring architecture notes, as these cut across the platform and require a more holistic, system-level view, Jassal said. “Many of the traditional roadblocks are navigated faster these days with AI; for example, understanding the codebase, dissecting requirements, auto-testing,” he said. “This faster track allows our talented junior hires to progress more quickly and provide higher value to the company sooner.”The company expects a much faster learning curve from junior to mid-level engineers, Jassal said. “It&#x27;s always the case that people coming straight into the workforce are much more excited about trying new things,” Jassal said. But that has to be colored with reality to temper expectations, he added. On the other hand, some senior engineers may need to be ramped up in their adoption because they’re hesitant or had ho-hum or bad experiences with earlier generation tools. This requires incremental introduction. “The senior people, having been burnt multiple times, bring that perspective,” he said. \"So both [types of engineers] play an important role.”Hiring will continue for scale and fresh perspective“In general, I would say it has been really hyped by folks who want to sell you tokens,” Jassal said referring to people who talk about human coders becoming obsolete. \"Vibe coding\" could be construed in a similar vein: Like others in software development, he prefers the term “AI assisted coding,” wherein programmers have a self-driven loop, generating code, analyzing exceptions, then correcting and scaling. At least in Egnyte’s case, hiring will continue, even if at a slower clip as people become more productive thanks to AI, Jassal said. “We are not just hiring for scale, but to develop the next generation of senior developers and inject fresh perspectives into our development practices,” he said. The takeaway for technical decision-makers is not that AI will eliminate engineering jobs — but that it will reshape how talent is developed. At Egnyte, AI-assisted coding is compressing learning curves and raising expectations, not removing humans from the process. Enterprises that treat AI as a replacement risk hollowing out their future senior talent pipeline; those that treat it as infrastructure can move faster without losing the judgment, creativity, and accountability that only engineers provide.",
          "content": "Egnyte, the $1.5 billion cloud content governance company, has embedded AI coding tools across its global team of more than 350 developers — but not to reduce headcount. Instead, the company continues to hire junior engineers, using AI to accelerate onboarding, deepen codebase understanding, and shorten the path from junior to senior contributor. The approach challenges a dominant 2025 narrative that automation will replace developers, showing instead how enterprises are using AI to scale engineering capacity while keeping humans firmly in the loop.“To have engineers disappear or us not hiring junior engineers doesn&#x27;t look like the likely outcome,” Amrit Jassal, Egnyte CTO and co-founder, told VentureBeat. “You&#x27;ve got to have people, you&#x27;re training and doing all types of succession planning. The junior engineer of today is the senior engineer of tomorrow.”How Egnyte coders are using AI — without ceding controlEgnyte — which has more than 22,000 users including NASDAQ, Red Bull, and BuzzFeed — has rolled out Claude Code, Cursor, Augment, and Gemini CLI coding tools across its developer base to support its core business strategies and expand its newer AI offerings like customer-facing copilots and customizable AI agents. Devs use these tools across a variety of tasks, the simplest of which include data retrieval, code comprehension, smart search, and code lookup. Egnyte’s code base has lots of Java code, which uses numerous libraries, each with different versions, Jassal explained. AI tools are great for peer-to-peer programming, helping new users get a lay of the land, or existing users probe into different code repositories. “We have a pretty big code base, right?” Jassal said. “Let&#x27;s say you&#x27;re looking at an iOS application, but you&#x27;re not well versed; you will fire up Google CLI or an Augment, and ask it to discover the code base.”Some Egnyte devs are moving into automatic pull request summaries, which provide simple overviews of code changes that essentially explain the “what,” “how,” and “why” of proposed modifications. “But obviously, any change that&#x27;s made, we don&#x27;t want to hear that AI made the change; it has to be that developer made the change,” Jassal pointed out. “I would not trust AI to commit to the production code base.” Commits still pass through human review and security validation, and anything red-flagged is escalated to senior engineers. Devs are warned of the dangers of settling into autopilot mode or blindly trusting code. A model may not have been exposed to, or given enough samples of, certain coding components and infrastructure in its training. Another growing, and closely monitored, use case for AI is unit testing, where code components are run in isolation to ensure they work as intended. “At the end of the day, it is a productivity improvement tool,” he said. “It is really a continuation, it&#x27;s like any other tool, it&#x27;s not some magic.”Beyond core engineering, AI is helping other teams collaborate with programmers. Product management, for instance, is using tools like Vercel to bring “demo-worthy” prototypes, rather than just ideas, to devs, who can then move ahead with mock-ups. Or, if UX teams are looking to change certain elements on a dashboard, AI can quickly spin up a handful of options, like different widgets or buttons. “Then you come to engineering with that, and the engineer immediately knows what you really intend to do with it,” Jassal said. Setting expectations, meeting devs where they areHowever, day-to-day activities for all Egnyte engineers, including junior developers, extend beyond just coding. Junior developers are given hands-on tasks across the full development lifecycle to accelerate their growth and experience, Jassal said. For instance, they assist with requirement analysis in early software engineering phases, as well as deployment, productization and post-deployment maintenance.In turn, these activities require “Egnyte-specific tacit knowledge and experience” offered by senior engineers. One clear example of work that sits firmly with senior engineers is authoring architecture notes, as these cut across the platform and require a more holistic, system-level view, Jassal said. “Many of the traditional roadblocks are navigated faster these days with AI; for example, understanding the codebase, dissecting requirements, auto-testing,” he said. “This faster track allows our talented junior hires to progress more quickly and provide higher value to the company sooner.”The company expects a much faster learning curve from junior to mid-level engineers, Jassal said. “It&#x27;s always the case that people coming straight into the workforce are much more excited about trying new things,” Jassal said. But that has to be colored with reality to temper expectations, he added. On the other hand, some senior engineers may need to be ramped up in their adoption because they’re hesitant or had ho-hum or bad experiences with earlier generation tools. This requires incremental introduction. “The senior people, having been burnt multiple times, bring that perspective,” he said. \"So both [types of engineers] play an important role.”Hiring will continue for scale and fresh perspective“In general, I would say it has been really hyped by folks who want to sell you tokens,” Jassal said referring to people who talk about human coders becoming obsolete. \"Vibe coding\" could be construed in a similar vein: Like others in software development, he prefers the term “AI assisted coding,” wherein programmers have a self-driven loop, generating code, analyzing exceptions, then correcting and scaling. At least in Egnyte’s case, hiring will continue, even if at a slower clip as people become more productive thanks to AI, Jassal said. “We are not just hiring for scale, but to develop the next generation of senior developers and inject fresh perspectives into our development practices,” he said. The takeaway for technical decision-makers is not that AI will eliminate engineering jobs — but that it will reshape how talent is developed. At Egnyte, AI-assisted coding is compressing learning curves and raising expectations, not removing humans from the process. Enterprises that treat AI as a replacement risk hollowing out their future senior talent pipeline; those that treat it as infrastructure can move faster without losing the judgment, creativity, and accountability that only engineers provide.",
          "feed_position": 0,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/7DUx86jAglIoinWLIrfpO8/abd01f833379598313f3bdeb2c35a96e/Egnyte.png?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/orchestration/this-new-dead-simple-prompt-technique-boosts-accuracy-on-llms-by-up-to-76-on",
          "published_at": "Tue, 13 Jan 2026 19:57:00 GMT",
          "title": "This new, dead simple prompt technique boosts accuracy on LLMs by up to 76% on non-reasoning tasks",
          "standfirst": "In the chaotic world of Large Language Model (LLM) optimization, engineers have spent the last few years developing increasingly esoteric rituals to get better answers. We’ve seen \"Chain of Thought\" (asking the model to think step-by-step and often, show those \"reasoning traces\" to the user), \"Emotional Blackmail\" (telling the model its career depends on the answer, or that it is being accused of sexual misconduct), and complex multi-shot prompting frameworks.But a new paper released by Google Research suggests that we may have been overthinking it. The researchers found that simply repeating the input query—literally copying and pasting the prompt so it appears twice—consistently improves performance across major models including Gemini, GPT-4o, Claude, and DeepSeek.The paper, titled \"Prompt Repetition Improves Non-Reasoning LLMs,\" released last month just before the holidays, presents a finding that is almost suspiciously simple: for tasks that don’t require complex reasoning steps, stating the prompt twice yields significantly better results than stating it once. Even better, because of how transformer architecture works, this \"one weird trick\" comes with virtually zero penalty in terms of generation speed.The Causal Blind SpotTo understand why repeating a question makes a supercomputer smarter, you have to look at the architectural limitations of the standard Transformer model.Most modern LLMs are trained as \"causal\" language models. This means they process text strictly from left to right. When the model is processing the 5th token in your sentence, it can \"attend\" (pay attention) to tokens 1 through 4, but it has zero knowledge of token 6, because it hasn&#x27;t happened yet.This creates a fundamental constraint in how models understand user queries. As the authors note, the order of information matters immensely. A query formatted as <CONTEXT> <QUESTION> often yields different results than <QUESTION> <CONTEXT> because, in the latter case, the model reads the question before it knows the context it’s supposed to apply it to.Prompt repetition hacks this limitation by transforming an input of <QUERY> into <QUERY><QUERY>.By the time the model begins processing the second iteration of the query, it has already \"read\" the first iteration. This allows the tokens in the second copy to attend to every single token in the first copy. Effectively, the second repetition enjoys a form of bidirectional attention—it can \"look back\" at the entire query to resolve ambiguities or retrieve specific details that might have been missed in a single pass.The Benchmarks: 47 Wins, 0 LossesThe researchers, Yaniv Leviathan, Matan Kalman, and Yossi Matias, tested this hypothesis across a suite of seven popular benchmarks, including ARC, OpenBookOA, GSM8K, and MMLU-Pro. They evaluated seven different models, ranging from lightweight models like Gemini 2.0 Flash Lite and GPT-4o-mini to heavyweights like Claude 3.7 Sonnet and DeepSeek V3.The results were statistically stark. When asking models not to use explicit reasoning (i.e., just giving a direct answer), prompt repetition won 47 out of 70 head-to-head tests against the baseline, with zero losses.The gains were particularly dramatic in tasks requiring precise retrieval from a prompt. The team designed a custom \"NameIndex\" benchmark, where the model is given a list of 50 names and asked to identify the 25th one.Baseline Performance: Gemini 2.0 Flash-Lite scored a dismal 21.33% accuracy.With Repetition: Accuracy skyrocketed to 97.33%.This massive jump illustrates the \"causal blind spot\" perfectly. In a single pass, the model might lose track of the count by the time it reaches the 25th name. In the repeated pass, the model effectively has the entire list in its \"working memory\" before it attempts to solve the retrieval task.The \"Free Lunch\" of LatencyUsually, adding text to a prompt increases costs and latency. If you double the input, surely you double the wait time?Surprisingly, no. The paper demonstrates that prompt repetition is essentially \"free\" regarding user-perceived latency.LLM processing is divided into two stages:Prefill: The model processes the input prompt. This is highly parallelizable; the GPU can crunch the entire prompt matrix simultaneously.Generation (Decoding): The model generates the answer one token at a time. This is serial and slow.Prompt repetition only increases the work in the prefill stage. Because modern hardware handles prefill so efficiently, the user barely notices the difference. The researchers found that repeating the prompt did not increase the length of the generated answer, nor did it increase the \"time to first token\" latency for most models.The only exceptions were Anthropic’s models (Claude Haiku and Sonnet) on extremely long requests, where the prefill stage eventually hit a bottleneck. But for the vast majority of use cases, the technique improves accuracy without slowing down the chat experience.Reasoning vs. RepetitionThere is a caveat: this technique is primarily for \"non-reasoning\" tasks—scenarios where you want a direct answer rather than a step-by-step derivation.When the researchers tested prompt repetition combined with \"Chain of Thought\" (asking the model to \"think step by step\"), the gains largely vanished, showing neutral to slightly positive results (5 wins, 1 loss, 22 ties).The authors posit that reasoning models naturally perform a version of repetition themselves. When a model \"thinks,\" it often restates the premise of the question in its generated output before solving it. Therefore, explicitly repeating the prompt in the input becomes redundant. However, for applications where you need a fast, direct answer without the verbosity (and cost) of a long reasoning trace, prompt repetition offers a powerful alternative.Strategic Implementation for the EnterpriseFor enterprise leadership, this research represents that rarest of things in AI development: a \"free\" optimization. But capitalization requires nuance; this isn&#x27;t a setting to toggle blindly across an entire organization, but rather a tactical adjustment that ripples across engineering, orchestration, and security.For technical leads balancing the eternal triangle of speed, quality, and cost, prompt repetition offers a way to punch above your weight class. The data shows that smaller, faster models—like Gemini 2.0 Flash Lite—can achieve near-perfect retrieval accuracy (jumping from 21.33% to 97.33%) simply by processing the input twice. This changes the calculus for model selection: before upgrading to a larger, more expensive model to solve an accuracy bottleneck, engineers should first test whether simple repetition allows their current \"Lite\" models to close the gap. It is a potential strategy for retaining the speed and cost benefits of lightweight infrastructure without sacrificing performance on extraction and retrieval tasks.This logic naturally shifts the burden to the orchestration layer. For those managing the middleware and API gateways that glue AI applications together, prompt repetition should likely become a standard, invisible component of the pipeline logic rather than a user behavior. However, because the technique is neutral for reasoning-heavy tasks but highly effective for direct answers, it requires conditional application. A smart orchestration harness would automatically identify requests routed to non-reasoning endpoints—such as entity extraction, classification, or simple Q&A—and double the prompt before passing it to the model. This optimizes performance at the infrastructure level, delivering better results without requiring action from end-users or increasing the generation budget.Finally, this heightened attentiveness introduces a new variable for security teams. If repeating a prompt clarifies a user&#x27;s intent to the model, it stands to reason that malicious intents might be clarified as well. Security directors will need to update their red-teaming protocols to test \"repeated injection\" attacks—verifying whether repeating a jailbreak command (e.g., \"Ignore previous instructions\") makes the model \"attend\" to the breach more effectively. Conversely, this mechanism offers a new defensive tool: repeating System Prompts. Stating safety guardrails twice at the start of the context window could force the model to attend to safety constraints more rigorously, acting as a low-cost reinforcement for robust security operations.Why This MattersThis research highlights a crucial insight for developers building on top of LLMs: our current models are still deeply constrained by their unidirectional nature. While we wait for new architectures that might solve causal blindness, crude but effective workarounds like prompt repetition offer immediate value.The authors suggest this could become a default behavior for future systems. We might soon see inference engines that silently double our prompts in the background before sending them to the model, or \"Reasoning\" models trained to internalize this repetition strategy to be more efficient.For now, if you are struggling to get a model to follow complex instructions or retrieve specific details from a long document, the solution might not be a better prompt. You might just need to say it again.",
          "content": "In the chaotic world of Large Language Model (LLM) optimization, engineers have spent the last few years developing increasingly esoteric rituals to get better answers. We’ve seen \"Chain of Thought\" (asking the model to think step-by-step and often, show those \"reasoning traces\" to the user), \"Emotional Blackmail\" (telling the model its career depends on the answer, or that it is being accused of sexual misconduct), and complex multi-shot prompting frameworks.But a new paper released by Google Research suggests that we may have been overthinking it. The researchers found that simply repeating the input query—literally copying and pasting the prompt so it appears twice—consistently improves performance across major models including Gemini, GPT-4o, Claude, and DeepSeek.The paper, titled \"Prompt Repetition Improves Non-Reasoning LLMs,\" released last month just before the holidays, presents a finding that is almost suspiciously simple: for tasks that don’t require complex reasoning steps, stating the prompt twice yields significantly better results than stating it once. Even better, because of how transformer architecture works, this \"one weird trick\" comes with virtually zero penalty in terms of generation speed.The Causal Blind SpotTo understand why repeating a question makes a supercomputer smarter, you have to look at the architectural limitations of the standard Transformer model.Most modern LLMs are trained as \"causal\" language models. This means they process text strictly from left to right. When the model is processing the 5th token in your sentence, it can \"attend\" (pay attention) to tokens 1 through 4, but it has zero knowledge of token 6, because it hasn&#x27;t happened yet.This creates a fundamental constraint in how models understand user queries. As the authors note, the order of information matters immensely. A query formatted as <CONTEXT> <QUESTION> often yields different results than <QUESTION> <CONTEXT> because, in the latter case, the model reads the question before it knows the context it’s supposed to apply it to.Prompt repetition hacks this limitation by transforming an input of <QUERY> into <QUERY><QUERY>.By the time the model begins processing the second iteration of the query, it has already \"read\" the first iteration. This allows the tokens in the second copy to attend to every single token in the first copy. Effectively, the second repetition enjoys a form of bidirectional attention—it can \"look back\" at the entire query to resolve ambiguities or retrieve specific details that might have been missed in a single pass.The Benchmarks: 47 Wins, 0 LossesThe researchers, Yaniv Leviathan, Matan Kalman, and Yossi Matias, tested this hypothesis across a suite of seven popular benchmarks, including ARC, OpenBookOA, GSM8K, and MMLU-Pro. They evaluated seven different models, ranging from lightweight models like Gemini 2.0 Flash Lite and GPT-4o-mini to heavyweights like Claude 3.7 Sonnet and DeepSeek V3.The results were statistically stark. When asking models not to use explicit reasoning (i.e., just giving a direct answer), prompt repetition won 47 out of 70 head-to-head tests against the baseline, with zero losses.The gains were particularly dramatic in tasks requiring precise retrieval from a prompt. The team designed a custom \"NameIndex\" benchmark, where the model is given a list of 50 names and asked to identify the 25th one.Baseline Performance: Gemini 2.0 Flash-Lite scored a dismal 21.33% accuracy.With Repetition: Accuracy skyrocketed to 97.33%.This massive jump illustrates the \"causal blind spot\" perfectly. In a single pass, the model might lose track of the count by the time it reaches the 25th name. In the repeated pass, the model effectively has the entire list in its \"working memory\" before it attempts to solve the retrieval task.The \"Free Lunch\" of LatencyUsually, adding text to a prompt increases costs and latency. If you double the input, surely you double the wait time?Surprisingly, no. The paper demonstrates that prompt repetition is essentially \"free\" regarding user-perceived latency.LLM processing is divided into two stages:Prefill: The model processes the input prompt. This is highly parallelizable; the GPU can crunch the entire prompt matrix simultaneously.Generation (Decoding): The model generates the answer one token at a time. This is serial and slow.Prompt repetition only increases the work in the prefill stage. Because modern hardware handles prefill so efficiently, the user barely notices the difference. The researchers found that repeating the prompt did not increase the length of the generated answer, nor did it increase the \"time to first token\" latency for most models.The only exceptions were Anthropic’s models (Claude Haiku and Sonnet) on extremely long requests, where the prefill stage eventually hit a bottleneck. But for the vast majority of use cases, the technique improves accuracy without slowing down the chat experience.Reasoning vs. RepetitionThere is a caveat: this technique is primarily for \"non-reasoning\" tasks—scenarios where you want a direct answer rather than a step-by-step derivation.When the researchers tested prompt repetition combined with \"Chain of Thought\" (asking the model to \"think step by step\"), the gains largely vanished, showing neutral to slightly positive results (5 wins, 1 loss, 22 ties).The authors posit that reasoning models naturally perform a version of repetition themselves. When a model \"thinks,\" it often restates the premise of the question in its generated output before solving it. Therefore, explicitly repeating the prompt in the input becomes redundant. However, for applications where you need a fast, direct answer without the verbosity (and cost) of a long reasoning trace, prompt repetition offers a powerful alternative.Strategic Implementation for the EnterpriseFor enterprise leadership, this research represents that rarest of things in AI development: a \"free\" optimization. But capitalization requires nuance; this isn&#x27;t a setting to toggle blindly across an entire organization, but rather a tactical adjustment that ripples across engineering, orchestration, and security.For technical leads balancing the eternal triangle of speed, quality, and cost, prompt repetition offers a way to punch above your weight class. The data shows that smaller, faster models—like Gemini 2.0 Flash Lite—can achieve near-perfect retrieval accuracy (jumping from 21.33% to 97.33%) simply by processing the input twice. This changes the calculus for model selection: before upgrading to a larger, more expensive model to solve an accuracy bottleneck, engineers should first test whether simple repetition allows their current \"Lite\" models to close the gap. It is a potential strategy for retaining the speed and cost benefits of lightweight infrastructure without sacrificing performance on extraction and retrieval tasks.This logic naturally shifts the burden to the orchestration layer. For those managing the middleware and API gateways that glue AI applications together, prompt repetition should likely become a standard, invisible component of the pipeline logic rather than a user behavior. However, because the technique is neutral for reasoning-heavy tasks but highly effective for direct answers, it requires conditional application. A smart orchestration harness would automatically identify requests routed to non-reasoning endpoints—such as entity extraction, classification, or simple Q&A—and double the prompt before passing it to the model. This optimizes performance at the infrastructure level, delivering better results without requiring action from end-users or increasing the generation budget.Finally, this heightened attentiveness introduces a new variable for security teams. If repeating a prompt clarifies a user&#x27;s intent to the model, it stands to reason that malicious intents might be clarified as well. Security directors will need to update their red-teaming protocols to test \"repeated injection\" attacks—verifying whether repeating a jailbreak command (e.g., \"Ignore previous instructions\") makes the model \"attend\" to the breach more effectively. Conversely, this mechanism offers a new defensive tool: repeating System Prompts. Stating safety guardrails twice at the start of the context window could force the model to attend to safety constraints more rigorously, acting as a low-cost reinforcement for robust security operations.Why This MattersThis research highlights a crucial insight for developers building on top of LLMs: our current models are still deeply constrained by their unidirectional nature. While we wait for new architectures that might solve causal blindness, crude but effective workarounds like prompt repetition offer immediate value.The authors suggest this could become a default behavior for future systems. We might soon see inference engines that silently double our prompts in the background before sending them to the model, or \"Reasoning\" models trained to internalize this repetition strategy to be more efficient.For now, if you are struggling to get a model to follow complex instructions or retrieve specific details from a long document, the solution might not be a better prompt. You might just need to say it again.",
          "feed_position": 1,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/6PTJl3Wssuvl5m3nljR103/21ab6a44da755c1cf2dca3b8fdd4ad08/cool-guys.png?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/entertainment/streaming/nba-league-pass-is-up-to-55-percent-off-right-now-163421218.html",
          "published_at": "Tue, 13 Jan 2026 16:34:21 +0000",
          "title": "NBA League Pass is up to 55 percent off right now",
          "standfirst": "NBA League Pass, the streaming service that lets you catch hundreds of out-of-market NBA games, is on sale right now for up to 55 percent off. The League Pass Premium subscription is on sale for $75, down from $160, while League Pass Standard is marked down to $50 from $110. We're almost halfway through the season at this point, so it makes sense for a service like League Pass to start offering some discounts. The Standard plan includes commercials and support for only one device at a time, while the Premium tier offers no commercials, in-arena streams during breaks in the game, offline viewing of full games and concurrent streams on up to three devices at once. Last year, League Pass added multiview, which allows you to view up to four games at once on a single screen. This is included across both subscription tiers. The service also added a smart rewind tool that automatically selects key highlights and plays from each game. Outside the US and Canada, League Pass carries every single NBA game live, but within these countries a bevy of restrictions apply. In the US, any games being shown on your regional sports network will be blacked out as the service is meant for out-of-market games only. Also, any nationally broadcast games will not be available live, but instead will be available for on-demand viewing at 6AM ET the following day. The service is only for regular-season games. If you're an avid NBA fan that follows multiple teams then the League Pass almost certainly carries dozens of games you can watch even with the restrictions in the US. Subscribers can get a list of applicable blackouts by entering their ZIP code before signing up. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/entertainment/streaming/nba-league-pass-is-up-to-55-percent-off-right-now-163421218.html?src=rss",
          "content": "NBA League Pass, the streaming service that lets you catch hundreds of out-of-market NBA games, is on sale right now for up to 55 percent off. The League Pass Premium subscription is on sale for $75, down from $160, while League Pass Standard is marked down to $50 from $110. We're almost halfway through the season at this point, so it makes sense for a service like League Pass to start offering some discounts. The Standard plan includes commercials and support for only one device at a time, while the Premium tier offers no commercials, in-arena streams during breaks in the game, offline viewing of full games and concurrent streams on up to three devices at once. Last year, League Pass added multiview, which allows you to view up to four games at once on a single screen. This is included across both subscription tiers. The service also added a smart rewind tool that automatically selects key highlights and plays from each game. Outside the US and Canada, League Pass carries every single NBA game live, but within these countries a bevy of restrictions apply. In the US, any games being shown on your regional sports network will be blacked out as the service is meant for out-of-market games only. Also, any nationally broadcast games will not be available live, but instead will be available for on-demand viewing at 6AM ET the following day. The service is only for regular-season games. If you're an avid NBA fan that follows multiple teams then the League Pass almost certainly carries dozens of games you can watch even with the restrictions in the US. Subscribers can get a list of applicable blackouts by entering their ZIP code before signing up. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/entertainment/streaming/nba-league-pass-is-up-to-55-percent-off-right-now-163421218.html?src=rss",
          "feed_position": 7
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/data/deepseeks-conditional-memory-fixes-silent-llm-waste-gpu-cycles-lost-to",
          "published_at": "Tue, 13 Jan 2026 16:00:00 GMT",
          "title": "DeepSeek’s conditional memory fixes silent LLM waste: GPU cycles lost to static lookups",
          "standfirst": "When an enterprise LLM retrieves a product name, technical specification, or standard contract clause, it&#x27;s using expensive GPU computation designed for complex reasoning — just to access static information. This happens millions of times per day. Each lookup wastes cycles and inflates infrastructure costs. DeepSeek&#x27;s newly released research on \"conditional memory\" addresses this architectural limitation directly. The work introduces Engram, a module that separates static pattern retrieval from dynamic reasoning. It delivers results that challenge assumptions about what memory is actually for in neural networks. The paper was co-authored by DeepSeek founder Liang Wenfeng.Through systematic experiments DeepSeek found the optimal balance between computation and memory with 75% of sparse model capacity allocated to dynamic reasoning and 25% to static lookups. This memory system improved reasoning more than knowledge retrieval. Complex reasoning benchmarks jumped from 70% to 74% accuracy, while knowledge-focused tests improved from 57% to 61%. These improvements came from tests including Big-Bench Hard, ARC-Challenge, and MMLU.The research arrives as enterprises face mounting pressure to deploy more capable AI systems while navigating GPU memory constraints and infrastructure costs. DeepSeek&#x27;s approach offers a potential path forward by fundamentally rethinking how models should be structured.How conditional memory solves a different issue than agentic memory and RAGAgentic memory systems, sometimes referred to as contextual memory — like Hindsight, MemOS, or Memp — focus on episodic memory. They store records of past conversations, user preferences, and interaction history. These systems help agents maintain context across sessions and learn from experience. But they&#x27;re external to the model&#x27;s forward pass and don&#x27;t optimize how the model internally processes static linguistic patterns.For Chris Latimer, founder and CEO of Vectorize, which developed Hindsight, the conditional memory approach used in Engram solves a different problem than agentic AI memory.\"It&#x27;s not solving the problem of connecting agents to external memory like conversation histories and knowledge stores,\" Latimer told VentureBeat. \"It&#x27;s more geared towards squeezing performance out of smaller models and getting more mileage out of scarce GPU resources.\"Conditional memory tackles a fundamental issue: Transformers lack a native knowledge lookup primitive. When processing text, they must simulate retrieval of static patterns through expensive neural computation across multiple layers. These patterns include named entities, technical terminology, and common phrases.The DeepSeek paper illustrates this with a concrete example. Recognizing \"Diana, Princess of Wales\" requires consuming multiple layers of attention and feed-forward networks to progressively compose features. The model essentially uses deep, dynamic logic circuits to perform what should be a simple hash table lookup. It&#x27;s like using a calculator to remember your phone number rather than just looking it up.\"The problem is that Transformer lacks a &#x27;native knowledge lookup&#x27; ability,\" the researchers write. \"Many tasks that should be solved in O(1) time like retrieval have to be &#x27;simulated for retrieval&#x27; through a large amount of computation, which is very inefficient.\"How conditional memory worksEngram introduces \"conditional memory\" to work alongside MoE&#x27;s conditional computation. The mechanism is straightforward. The module takes sequences of two to three tokens and uses hash functions to look them up in a massive embedding table. Retrieval happens in constant time, regardless of table size.But retrieved patterns need filtering. A hash lookup for \"Apple\" might collide with unrelated content, or the word might mean the fruit rather than the company. Engram solves this with a gating mechanism. The model&#x27;s current understanding of context (accumulated through earlier attention layers) acts as a filter. If retrieved memory contradicts the current context, the gate suppresses it. If it fits, the gate lets it through.The module isn&#x27;t applied at every layer. Strategic placement balances performance gains against system latency.This dual-system design raises a critical question: How much capacity should each get? DeepSeek&#x27;s key finding: the optimal split is 75-80% for computation and 20-25% for memory. Testing found pure MoE (100% computation) proved suboptimal. Too much computation wastes depth reconstructing static patterns; too much memory loses reasoning capacity.Infrastructure efficiency: the GPU memory bypassPerhaps Engram&#x27;s most pragmatic contribution is its infrastructure-aware design. Unlike MoE&#x27;s dynamic routing, which depends on runtime hidden states, Engram&#x27;s retrieval indices depend solely on input token sequences. This deterministic nature enables a prefetch-and-overlap strategy.\"The challenge is that GPU memory is limited and expensive, so using bigger models gets costly and harder to deploy,\" Latimer said. \"The clever idea behind Engram is to keep the main model on the GPU, but offload a big chunk of the model&#x27;s stored information into a separate memory on regular RAM, which the model can use on a just-in-time basis.\"During inference, the system can asynchronously retrieve embeddings from host CPU memory via PCIe. This happens while GPU computes preceding transformer blocks. Strategic layer placement leverages computation of early layers as a buffer to mask communication latency.The researchers demonstrated this with a 100B-parameter embedding table entirely offloaded to host DRAM. They achieved throughput penalties below 3%. This decoupling of storage from compute addresses a critical enterprise constraint as GPU high-bandwidth memory remains expensive and scarce.What this means for enterprise AI deploymentFor enterprises evaluating AI infrastructure strategies, DeepSeek&#x27;s findings suggest several actionable insights:1. Hybrid architectures outperform pure approaches. The 75/25 allocation law indicates that optimal models should split sparse capacity between computation and memory. 2. Infrastructure costs may shift from GPU to memory. If Engram-style architectures prove viable in production, infrastructure investment patterns could change. The ability to store 100B+ parameters in CPU memory with minimal overhead suggests that memory-rich, compute-moderate configurations may offer better performance-per-dollar than pure GPU scaling.3. Reasoning improvements exceed knowledge gains. The surprising finding that reasoning benefits more than knowledge retrieval suggests that memory&#x27;s value extends beyond obvious use cases. For enterprises leading AI adoption, Engram demonstrates that the next frontier may not be simply bigger models. It&#x27;s smarter architectural choices that respect the fundamental distinction between static knowledge and dynamic reasoning. The research suggests that optimal AI systems will increasingly resemble hybrid architectures. Organizations waiting to adopt AI later in the cycle should monitor whether major model providers incorporate conditional memory principles into their architectures. If the 75/25 allocation law holds across scales and domains, the next generation of foundation models may deliver substantially better reasoning performance at lower infrastructure costs.",
          "content": "When an enterprise LLM retrieves a product name, technical specification, or standard contract clause, it&#x27;s using expensive GPU computation designed for complex reasoning — just to access static information. This happens millions of times per day. Each lookup wastes cycles and inflates infrastructure costs. DeepSeek&#x27;s newly released research on \"conditional memory\" addresses this architectural limitation directly. The work introduces Engram, a module that separates static pattern retrieval from dynamic reasoning. It delivers results that challenge assumptions about what memory is actually for in neural networks. The paper was co-authored by DeepSeek founder Liang Wenfeng.Through systematic experiments DeepSeek found the optimal balance between computation and memory with 75% of sparse model capacity allocated to dynamic reasoning and 25% to static lookups. This memory system improved reasoning more than knowledge retrieval. Complex reasoning benchmarks jumped from 70% to 74% accuracy, while knowledge-focused tests improved from 57% to 61%. These improvements came from tests including Big-Bench Hard, ARC-Challenge, and MMLU.The research arrives as enterprises face mounting pressure to deploy more capable AI systems while navigating GPU memory constraints and infrastructure costs. DeepSeek&#x27;s approach offers a potential path forward by fundamentally rethinking how models should be structured.How conditional memory solves a different issue than agentic memory and RAGAgentic memory systems, sometimes referred to as contextual memory — like Hindsight, MemOS, or Memp — focus on episodic memory. They store records of past conversations, user preferences, and interaction history. These systems help agents maintain context across sessions and learn from experience. But they&#x27;re external to the model&#x27;s forward pass and don&#x27;t optimize how the model internally processes static linguistic patterns.For Chris Latimer, founder and CEO of Vectorize, which developed Hindsight, the conditional memory approach used in Engram solves a different problem than agentic AI memory.\"It&#x27;s not solving the problem of connecting agents to external memory like conversation histories and knowledge stores,\" Latimer told VentureBeat. \"It&#x27;s more geared towards squeezing performance out of smaller models and getting more mileage out of scarce GPU resources.\"Conditional memory tackles a fundamental issue: Transformers lack a native knowledge lookup primitive. When processing text, they must simulate retrieval of static patterns through expensive neural computation across multiple layers. These patterns include named entities, technical terminology, and common phrases.The DeepSeek paper illustrates this with a concrete example. Recognizing \"Diana, Princess of Wales\" requires consuming multiple layers of attention and feed-forward networks to progressively compose features. The model essentially uses deep, dynamic logic circuits to perform what should be a simple hash table lookup. It&#x27;s like using a calculator to remember your phone number rather than just looking it up.\"The problem is that Transformer lacks a &#x27;native knowledge lookup&#x27; ability,\" the researchers write. \"Many tasks that should be solved in O(1) time like retrieval have to be &#x27;simulated for retrieval&#x27; through a large amount of computation, which is very inefficient.\"How conditional memory worksEngram introduces \"conditional memory\" to work alongside MoE&#x27;s conditional computation. The mechanism is straightforward. The module takes sequences of two to three tokens and uses hash functions to look them up in a massive embedding table. Retrieval happens in constant time, regardless of table size.But retrieved patterns need filtering. A hash lookup for \"Apple\" might collide with unrelated content, or the word might mean the fruit rather than the company. Engram solves this with a gating mechanism. The model&#x27;s current understanding of context (accumulated through earlier attention layers) acts as a filter. If retrieved memory contradicts the current context, the gate suppresses it. If it fits, the gate lets it through.The module isn&#x27;t applied at every layer. Strategic placement balances performance gains against system latency.This dual-system design raises a critical question: How much capacity should each get? DeepSeek&#x27;s key finding: the optimal split is 75-80% for computation and 20-25% for memory. Testing found pure MoE (100% computation) proved suboptimal. Too much computation wastes depth reconstructing static patterns; too much memory loses reasoning capacity.Infrastructure efficiency: the GPU memory bypassPerhaps Engram&#x27;s most pragmatic contribution is its infrastructure-aware design. Unlike MoE&#x27;s dynamic routing, which depends on runtime hidden states, Engram&#x27;s retrieval indices depend solely on input token sequences. This deterministic nature enables a prefetch-and-overlap strategy.\"The challenge is that GPU memory is limited and expensive, so using bigger models gets costly and harder to deploy,\" Latimer said. \"The clever idea behind Engram is to keep the main model on the GPU, but offload a big chunk of the model&#x27;s stored information into a separate memory on regular RAM, which the model can use on a just-in-time basis.\"During inference, the system can asynchronously retrieve embeddings from host CPU memory via PCIe. This happens while GPU computes preceding transformer blocks. Strategic layer placement leverages computation of early layers as a buffer to mask communication latency.The researchers demonstrated this with a 100B-parameter embedding table entirely offloaded to host DRAM. They achieved throughput penalties below 3%. This decoupling of storage from compute addresses a critical enterprise constraint as GPU high-bandwidth memory remains expensive and scarce.What this means for enterprise AI deploymentFor enterprises evaluating AI infrastructure strategies, DeepSeek&#x27;s findings suggest several actionable insights:1. Hybrid architectures outperform pure approaches. The 75/25 allocation law indicates that optimal models should split sparse capacity between computation and memory. 2. Infrastructure costs may shift from GPU to memory. If Engram-style architectures prove viable in production, infrastructure investment patterns could change. The ability to store 100B+ parameters in CPU memory with minimal overhead suggests that memory-rich, compute-moderate configurations may offer better performance-per-dollar than pure GPU scaling.3. Reasoning improvements exceed knowledge gains. The surprising finding that reasoning benefits more than knowledge retrieval suggests that memory&#x27;s value extends beyond obvious use cases. For enterprises leading AI adoption, Engram demonstrates that the next frontier may not be simply bigger models. It&#x27;s smarter architectural choices that respect the fundamental distinction between static knowledge and dynamic reasoning. The research suggests that optimal AI systems will increasingly resemble hybrid architectures. Organizations waiting to adopt AI later in the cycle should monitor whether major model providers incorporate conditional memory principles into their architectures. If the 75/25 allocation law holds across scales and domains, the next generation of foundation models may deliver substantially better reasoning performance at lower infrastructure costs.",
          "feed_position": 2,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/5QbXis6MzFunSR0Q7iejyX/2cd47fca23ad6f0fd1e3094fc096252e/conditional-memory-smk.jpg?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/ai/anthropic-launches-claude-cowork-a-version-of-its-coding-ai-for-regular-people-193000849.html",
          "published_at": "Tue, 13 Jan 2026 15:27:21 +0000",
          "title": "Anthropic launches Claude Cowork, a version of its coding AI for regular people",
          "standfirst": "If you follow Anthropic, you're probably familiar with Claude Code. Since the fall of 2024, the company has been training its AI models to use and navigate computers like a human would, and the coding agent has been the most practical expression of that work, giving developers a way to automate rote programming tasks. Starting today, Anthropic is giving regular people a way to take advantage of those capabilities, with the release of a new preview feature called Claude Cowork.The company is billing Cowork as \"a simpler way for anyone — not just developers — to work with Claude.\" After you give the system access to a folder on your computer, it can read, edit or create new files in that folder on your behalf. Anthropic gives a few different example use cases for Cowork. For instance, you could ask Claude to organize your downloads folder, telling it to rename the files contained within to something that's easier to parse at a glance. Another example: you could use Claude to turn screenshots of receipts and invoices into a spreadsheet for tracking expenses. Cowork can also navigate websites — provided you install Claude’s Chrome plugin — and make can use Anthropic's Connectors framework to access third-party apps like Canva. \"Cowork is designed to make using Claude for new work as simple as possible. You don’t need to keep manually providing context or converting Claude’s outputs into the right format,\" the company said. \"Nor do you have to wait for Claude to finish before offering further ideas or feedback: you can queue up tasks and let Claude work through them in parallel.\" If the idea of granting Claude access to your computer sounds ill-advised, Anthropic says Claude \"can’t read or edit anything you don’t give it explicit access to.\" However, the company does note the system can \"take potentially destructive actions,\" such as deleting a file that is important to you or misinterpreting your instructions. For that reason, Anthropic suggests it's best to give \"very clear\" guidance to Claude. Anthropic isn’t the first to offer a computer agent. Microsoft, for example, has been pushing Copilot hard for nearly three years, despite seemingly limited adoption. For Anthropic, the challenge will be convincing people these tools are useful where others have failed. The fact Claude Code has been universally loved by programmers may make that task easier. For now, Anthropic is giving users of its pricey Claude Max subscription first access to the preview. If you want to try Cowork for yourself, you'll also need a Mac with the Claude macOS app installed. For everyone else, you’ll need to join a wait list. This article originally appeared on Engadget at https://www.engadget.com/ai/anthropic-launches-claude-cowork-a-version-of-its-coding-ai-for-regular-people-193000849.html?src=rss",
          "content": "If you follow Anthropic, you're probably familiar with Claude Code. Since the fall of 2024, the company has been training its AI models to use and navigate computers like a human would, and the coding agent has been the most practical expression of that work, giving developers a way to automate rote programming tasks. Starting today, Anthropic is giving regular people a way to take advantage of those capabilities, with the release of a new preview feature called Claude Cowork.The company is billing Cowork as \"a simpler way for anyone — not just developers — to work with Claude.\" After you give the system access to a folder on your computer, it can read, edit or create new files in that folder on your behalf. Anthropic gives a few different example use cases for Cowork. For instance, you could ask Claude to organize your downloads folder, telling it to rename the files contained within to something that's easier to parse at a glance. Another example: you could use Claude to turn screenshots of receipts and invoices into a spreadsheet for tracking expenses. Cowork can also navigate websites — provided you install Claude’s Chrome plugin — and make can use Anthropic's Connectors framework to access third-party apps like Canva. \"Cowork is designed to make using Claude for new work as simple as possible. You don’t need to keep manually providing context or converting Claude’s outputs into the right format,\" the company said. \"Nor do you have to wait for Claude to finish before offering further ideas or feedback: you can queue up tasks and let Claude work through them in parallel.\" If the idea of granting Claude access to your computer sounds ill-advised, Anthropic says Claude \"can’t read or edit anything you don’t give it explicit access to.\" However, the company does note the system can \"take potentially destructive actions,\" such as deleting a file that is important to you or misinterpreting your instructions. For that reason, Anthropic suggests it's best to give \"very clear\" guidance to Claude. Anthropic isn’t the first to offer a computer agent. Microsoft, for example, has been pushing Copilot hard for nearly three years, despite seemingly limited adoption. For Anthropic, the challenge will be convincing people these tools are useful where others have failed. The fact Claude Code has been universally loved by programmers may make that task easier. For now, Anthropic is giving users of its pricey Claude Max subscription first access to the preview. If you want to try Cowork for yourself, you'll also need a Mac with the Claude macOS app installed. For everyone else, you’ll need to join a wait list. This article originally appeared on Engadget at https://www.engadget.com/ai/anthropic-launches-claude-cowork-a-version-of-its-coding-ai-for-regular-people-193000849.html?src=rss",
          "feed_position": 8
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/mobile/tablets/remarkable-e-ink-tablet-bundles-are-up-to-90-off-right-now-150242312.html",
          "published_at": "Tue, 13 Jan 2026 15:02:42 +0000",
          "title": "reMarkable E Ink tablet bundles are up to $90 off right now",
          "standfirst": "E Ink tablet maker reMarkable is running a bundle deal right now that can save you between $80 and $90 when buying a reMarkable 2 along with a Marker stylus and a folio case. The savings vary depending on the bundle you configure, but this can bring your out-the-door cost down to $449 from $529 for the tablet, Marker stylus and polymer weave book folio. The company also sells a newer stylus called Marker Plus that lets you erase by flipping it around just like a real pencil, but that will cost you an extra $50. If you’ve been eyeing a dedicated writing tablet for work, school or just jotting down notes without the distraction of endless apps, this bundle deal is an ideal opportunity to pick one up. The reMarkable 2 earned our top pick for best e-ink tablet. In our review, we said the tablet was prettier than ever with a 10.3-inch display and a handsome aluminum frame. The tablet is only 4.7mm thick and weighs less than a pound, helping it feel lean and portable. The display can detect over 4,000 different levels of pressure with the Marker stylus, allowing for precise shading when sketching and the latency between the stylus and the screen is just 21ms. reMarkable fitted the display with a resin layer on top of the glass to make writing on it feel more realistic. We didn't think this passed muster, but we found writing on it was a joy nonetheless. The tablet supports PDFs and ePUBs, which can be added via the companion mobile app or a desktop computer. You can also pair the reMarkable 2 with Google Drive, Microsoft OneDrive or Dropbox to access files. The battery is rated for an impressive two weeks between charges. The reMarkable Paper Pro, a higher-end model with a richer feature set like a full color display and a built-in reading light, is our pick for best premium e-ink tablet. The pricier tablet also has bundle deals right now with savings up to $80 depending on configuration. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/mobile/tablets/remarkable-e-ink-tablet-bundles-are-up-to-90-off-right-now-150242312.html?src=rss",
          "content": "E Ink tablet maker reMarkable is running a bundle deal right now that can save you between $80 and $90 when buying a reMarkable 2 along with a Marker stylus and a folio case. The savings vary depending on the bundle you configure, but this can bring your out-the-door cost down to $449 from $529 for the tablet, Marker stylus and polymer weave book folio. The company also sells a newer stylus called Marker Plus that lets you erase by flipping it around just like a real pencil, but that will cost you an extra $50. If you’ve been eyeing a dedicated writing tablet for work, school or just jotting down notes without the distraction of endless apps, this bundle deal is an ideal opportunity to pick one up. The reMarkable 2 earned our top pick for best e-ink tablet. In our review, we said the tablet was prettier than ever with a 10.3-inch display and a handsome aluminum frame. The tablet is only 4.7mm thick and weighs less than a pound, helping it feel lean and portable. The display can detect over 4,000 different levels of pressure with the Marker stylus, allowing for precise shading when sketching and the latency between the stylus and the screen is just 21ms. reMarkable fitted the display with a resin layer on top of the glass to make writing on it feel more realistic. We didn't think this passed muster, but we found writing on it was a joy nonetheless. The tablet supports PDFs and ePUBs, which can be added via the companion mobile app or a desktop computer. You can also pair the reMarkable 2 with Google Drive, Microsoft OneDrive or Dropbox to access files. The battery is rated for an impressive two weeks between charges. The reMarkable Paper Pro, a higher-end model with a richer feature set like a full color display and a built-in reading light, is our pick for best premium e-ink tablet. The pricier tablet also has bundle deals right now with savings up to $80 depending on configuration. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/mobile/tablets/remarkable-e-ink-tablet-bundles-are-up-to-90-off-right-now-150242312.html?src=rss",
          "feed_position": 9
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/apps/apple-bundles-creative-apps-such-as-final-cut-pro-and-logic-pro-into-a-single-subscription-145210038.html",
          "published_at": "Tue, 13 Jan 2026 14:52:10 +0000",
          "title": "Apple bundles creative apps such as Final Cut Pro and Logic Pro into a single subscription",
          "standfirst": "Apple has been putting more onus on its services for the past several years — the company makes tens of billions of dollars in revenue from that side of the business, which it claimed had a record year in 2025. Apple is nudging a little more in that direction with a new subscription bundle called Apple Creator Studio.This allows creators to pay a single fee ($13 per month or $129 per year) to use Final Cut Pro, Logic Pro, Pixelmator Pro, Motion, Compressor and MainStage. Subscribers will get access to “premium content” in Pages, Keynote and Numbers (as well as in Freeform later this year). Of course, there are AI features too. Apple Creator Studio will be available starting on January 28 and you can try it out at no cost through a one-month free trial. College students and educators can subscribe to Apple Creator Studio for $3 per month or $30 per year. Up to six people can access all of the plan’s features if one person in a Family Sharing group subscribes. Apple noted that Final Cut Pro, Pixelmator Pro, Logic Pro, Motion, Compressor and MainStage will still be available as one-time purchases for Mac through the Mac App Store. Given that those can be pretty pricy (going up to $300 for Final Cut Pro), the subscription could be enticing to many burgeoning creators.This seems like Apple’s attempt to muscle in on Adobe’s territory, especially now that it’s bringing AI features to many of these apps. Adding new features to productivity apps like Numbers and Keynote means Apple’s taking a shot at the likes of Microsoft 365 Copilot (yeeeeah, that’s what Office is called now) and Google Workspace as well.On Mac and iPad, Final Cut Pro has a new feature called Beat Detection. Apple suggests this makes “editing video to the rhythm of music fast and fun.” It uses an AI model from Logic Pro to analyze music tracks and display a Beat Grid. The idea here is to visualize song parts, beats and bars to help editors align their cuts with the music. The Montage Maker tool in Final Cut Pro on an iPad.AppleAn AI-powered Montage Maker tool can stitch together “a dynamic video based on the best visual moments within the footage.” You’ll be able to tweak these montages and use an Auto Crop tool to reframe the clip into a vertical format to make it a better fit for social media. Final Cut Pro has transcript and visual search functions too.Logic Pro, MainStage, Pixelmator Pro (which is coming to iPad with Apple Pencil support) and Motion will all have AI-powered features as well. As you might expect, you’ll need an Apple Intelligence-capable device to use some of these.Apple is also introducing something called the Content Hub. This media library includes “curated, high-quality photos, graphics and illustrations.” As for Keynote, Pages, and Numbers, you’ll be able to access premium templates and themes in those otherwise-free apps with a Apple Creator Studio plan. Subscribers will be able to try beta versions of new features, such as a way to generate a draft of a Keynote presentation text based on an outline, and a Magic Fill tool to generate formulas and fill in tables in Numbers.This article originally appeared on Engadget at https://www.engadget.com/apps/apple-bundles-creative-apps-such-as-final-cut-pro-and-logic-pro-into-a-single-subscription-145210038.html?src=rss",
          "content": "Apple has been putting more onus on its services for the past several years — the company makes tens of billions of dollars in revenue from that side of the business, which it claimed had a record year in 2025. Apple is nudging a little more in that direction with a new subscription bundle called Apple Creator Studio.This allows creators to pay a single fee ($13 per month or $129 per year) to use Final Cut Pro, Logic Pro, Pixelmator Pro, Motion, Compressor and MainStage. Subscribers will get access to “premium content” in Pages, Keynote and Numbers (as well as in Freeform later this year). Of course, there are AI features too. Apple Creator Studio will be available starting on January 28 and you can try it out at no cost through a one-month free trial. College students and educators can subscribe to Apple Creator Studio for $3 per month or $30 per year. Up to six people can access all of the plan’s features if one person in a Family Sharing group subscribes. Apple noted that Final Cut Pro, Pixelmator Pro, Logic Pro, Motion, Compressor and MainStage will still be available as one-time purchases for Mac through the Mac App Store. Given that those can be pretty pricy (going up to $300 for Final Cut Pro), the subscription could be enticing to many burgeoning creators.This seems like Apple’s attempt to muscle in on Adobe’s territory, especially now that it’s bringing AI features to many of these apps. Adding new features to productivity apps like Numbers and Keynote means Apple’s taking a shot at the likes of Microsoft 365 Copilot (yeeeeah, that’s what Office is called now) and Google Workspace as well.On Mac and iPad, Final Cut Pro has a new feature called Beat Detection. Apple suggests this makes “editing video to the rhythm of music fast and fun.” It uses an AI model from Logic Pro to analyze music tracks and display a Beat Grid. The idea here is to visualize song parts, beats and bars to help editors align their cuts with the music. The Montage Maker tool in Final Cut Pro on an iPad.AppleAn AI-powered Montage Maker tool can stitch together “a dynamic video based on the best visual moments within the footage.” You’ll be able to tweak these montages and use an Auto Crop tool to reframe the clip into a vertical format to make it a better fit for social media. Final Cut Pro has transcript and visual search functions too.Logic Pro, MainStage, Pixelmator Pro (which is coming to iPad with Apple Pencil support) and Motion will all have AI-powered features as well. As you might expect, you’ll need an Apple Intelligence-capable device to use some of these.Apple is also introducing something called the Content Hub. This media library includes “curated, high-quality photos, graphics and illustrations.” As for Keynote, Pages, and Numbers, you’ll be able to access premium templates and themes in those otherwise-free apps with a Apple Creator Studio plan. Subscribers will be able to try beta versions of new features, such as a way to generate a draft of a Keynote presentation text based on an outline, and a Magic Fill tool to generate formulas and fill in tables in Numbers.This article originally appeared on Engadget at https://www.engadget.com/apps/apple-bundles-creative-apps-such-as-final-cut-pro-and-logic-pro-into-a-single-subscription-145210038.html?src=rss",
          "feed_position": 11,
          "image_url": "https://d29szjachogqwa.cloudfront.net/images/user-uploaded/final_cut.jpg"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/the-disney-hulu-bundle-is-on-sale-for-10-for-one-month-right-now-192814784.html",
          "published_at": "Tue, 13 Jan 2026 14:06:25 +0000",
          "title": "The Disney+ Hulu bundle is on sale for $10 for one month right now",
          "standfirst": "You have the best chance to save on streaming services during the holiday shopping season, but throughout the year, the occasional deal pops up that's worth considering. Case in point: this new Disney+ deal. New and eligible returning subscribers can sign up for the Disney+ Hulu bundle (with ads) for $10 for one month of access. That's $3 off the usual price of the bundle for one month, and more than 58 percent off if you consider the cost of each service individually (Disney+ at $12 per month and, separately, Hulu also at $12 per month). We'd be remiss if we didn't mention that this isn't quite as good as the Black Friday deal we saw last year, which offered the same bundle for $5 per month for one year. However, if you missed that offer or just want to try out Disney+ and Hulu for a brief period of time, this is a good way to do so. Disney+ and Hulu make one of the most balanced streaming pairs available, blending family-friendly favorites with acclaimed originals and network TV staples. Disney+ brings a vast library of animated classics, blockbuster franchises and exclusive content from Marvel, Pixar, Star Wars and National Geographic. It’s the place to stream nearly every Star Wars film and series, plus the full Marvel Cinematic Universe lineup and Disney’s most recent theatrical releases. Hulu balances things out with a more adult-oriented lineup of current TV shows, next-day network episodes and a growing roster of award-winning originals. The platform hosts series like The Bear, The Handmaid’s Tale and Only Murders in the Building, alongside comedies, thrillers and documentaries that regularly feature in awards conversations. It’s also the home for next-day streaming of ABC and FX shows, making it especially useful if you’ve already cut the cable cord but still want to keep up with primetime TV. The Duo Basic bundle ties these two services together under a single subscription, offering a simple way to expand your library without juggling multiple accounts. This tier includes ads on both platforms, but the trade-off is significant savings compared with paying for each service separately. For many households, that’s an acceptable compromise when it means access to such a wide range of content. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/the-disney-hulu-bundle-is-on-sale-for-10-for-one-month-right-now-192814784.html?src=rss",
          "content": "You have the best chance to save on streaming services during the holiday shopping season, but throughout the year, the occasional deal pops up that's worth considering. Case in point: this new Disney+ deal. New and eligible returning subscribers can sign up for the Disney+ Hulu bundle (with ads) for $10 for one month of access. That's $3 off the usual price of the bundle for one month, and more than 58 percent off if you consider the cost of each service individually (Disney+ at $12 per month and, separately, Hulu also at $12 per month). We'd be remiss if we didn't mention that this isn't quite as good as the Black Friday deal we saw last year, which offered the same bundle for $5 per month for one year. However, if you missed that offer or just want to try out Disney+ and Hulu for a brief period of time, this is a good way to do so. Disney+ and Hulu make one of the most balanced streaming pairs available, blending family-friendly favorites with acclaimed originals and network TV staples. Disney+ brings a vast library of animated classics, blockbuster franchises and exclusive content from Marvel, Pixar, Star Wars and National Geographic. It’s the place to stream nearly every Star Wars film and series, plus the full Marvel Cinematic Universe lineup and Disney’s most recent theatrical releases. Hulu balances things out with a more adult-oriented lineup of current TV shows, next-day network episodes and a growing roster of award-winning originals. The platform hosts series like The Bear, The Handmaid’s Tale and Only Murders in the Building, alongside comedies, thrillers and documentaries that regularly feature in awards conversations. It’s also the home for next-day streaming of ABC and FX shows, making it especially useful if you’ve already cut the cable cord but still want to keep up with primetime TV. The Duo Basic bundle ties these two services together under a single subscription, offering a simple way to expand your library without juggling multiple accounts. This tier includes ads on both platforms, but the trade-off is significant savings compared with paying for each service separately. For many households, that’s an acceptable compromise when it means access to such a wide range of content. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/the-disney-hulu-bundle-is-on-sale-for-10-for-one-month-right-now-192814784.html?src=rss",
          "feed_position": 13
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/airtags-are-back-on-sale-with-a-four-pack-going-for-65-202333994.html",
          "published_at": "Tue, 13 Jan 2026 13:14:02 +0000",
          "title": "AirTags are back on sale, with a four-pack going for $65",
          "standfirst": "Most Apple products are pretty expensive, but some of the most affordable (and useful) ones are AirTags. The Bluetooth trackers are priced pretty reasonably even when not on sale, but they can be a steal if you can get them on a discount — like right now. A four pack of AirTags is on sale for $65 at Amazon, which is only a few dollars more than the record-low price we saw during Black Friday this year. AirTags can be useful for people who travel frequently, helping you to keep track of essentials like your passport as well as a way to keep tabs on luggage while you're on the go. If you do purchase some AirTags, we have some recommendations for useful accessories to go along with them, such as different styles of cases to best attach the trackers to different types of items. These are worth looking over and adding to your shopping cart in order to make the most of the product. AirTags have an IP67 rating for water and dust resistance and their replaceable batteries should last for about a year. They can also support Precision Finding, which gives more exact directions to a lost item, when paired with most models after the iPhone 11. Up to five people can share an AirTag's location, which is helpful for families or large travel groups. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/airtags-are-back-on-sale-with-a-four-pack-going-for-65-202333994.html?src=rss",
          "content": "Most Apple products are pretty expensive, but some of the most affordable (and useful) ones are AirTags. The Bluetooth trackers are priced pretty reasonably even when not on sale, but they can be a steal if you can get them on a discount — like right now. A four pack of AirTags is on sale for $65 at Amazon, which is only a few dollars more than the record-low price we saw during Black Friday this year. AirTags can be useful for people who travel frequently, helping you to keep track of essentials like your passport as well as a way to keep tabs on luggage while you're on the go. If you do purchase some AirTags, we have some recommendations for useful accessories to go along with them, such as different styles of cases to best attach the trackers to different types of items. These are worth looking over and adding to your shopping cart in order to make the most of the product. AirTags have an IP67 rating for water and dust resistance and their replaceable batteries should last for about a year. They can also support Precision Finding, which gives more exact directions to a lost item, when paired with most models after the iPhone 11. Up to five people can share an AirTag's location, which is helpful for families or large travel groups. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/airtags-are-back-on-sale-with-a-four-pack-going-for-65-202333994.html?src=rss",
          "feed_position": 17
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/cameras/insta360-releases-ai-powered-follow-up-to-its-link-webcams-130003572.html",
          "published_at": "Tue, 13 Jan 2026 13:00:03 +0000",
          "title": "Insta360 releases AI-powered follow-up to its Link webcams",
          "standfirst": "Insta360, a company most known for its action cameras, has released two new AI-powered 4K webcams, the Link 2 Pro and Link 2C Pro, aimed at creators, educators and remote professionals. The company's goal with these models is \"a webcam experience that looks and sounds remarkably close to a professional camera and microphone setup.\" Both models use a larger 1/1.3-inch sensor with dual native ISO for improved low-light performance over the previous generation, and both support HDR. Insta360 says the audio on both models leverages beamforming technology as well as AI noise canceling to help voices sound clearer in noisy environments. Users can choose from four pickup modes designed for different sound sources like \"Focus\" that isolates a single voice or \"Wide\" if there are multiple speakers. Video resolution on both models tops out 4K at 30 fps, and Insta360 says its updated True Focus system uses phase-detection autofocus to lock onto subjects, keeping them in focus while they move. There's also a \"Natural Bokeh\" mode meant to mimic the shallow depth-of-field look of a traditional DSLR camera, for users who enjoy that look. As for what sets them apart, the Link 2 Pro sports a 2-axis gimbal for AI-assisted tracking, which offers single or group-mode framing, while the Link 2C Pro is static and designed for fixed-position setups. Both models offer gesture control features, allowing users to control certain functions hands-free. These include starting or stopping tracking and zooming in or out. Both models also include a magnetic mount for easy placement on metal surfaces. Several different modes are offered that aid in teaching and presenting. Among them are Smart Whiteboard mode, which will automatically detect a user's whiteboard and keep it clearly in frame, and DeskView mode, which captures an overhead view of a user's desk. There's also a green screen mode, a portrait mode and support for virtual backgrounds. The new webcams also support Insta360 InSight, the company's subscription AI-powered meeting assistant. InSight can record meetings, generate transcripts, create summaries and more. The Link 2 Pro will retail for $250 while the Link 2C Pro will go for $200. Both models are available for purchase now.This article originally appeared on Engadget at https://www.engadget.com/cameras/insta360-releases-ai-powered-follow-up-to-its-link-webcams-130003572.html?src=rss",
          "content": "Insta360, a company most known for its action cameras, has released two new AI-powered 4K webcams, the Link 2 Pro and Link 2C Pro, aimed at creators, educators and remote professionals. The company's goal with these models is \"a webcam experience that looks and sounds remarkably close to a professional camera and microphone setup.\" Both models use a larger 1/1.3-inch sensor with dual native ISO for improved low-light performance over the previous generation, and both support HDR. Insta360 says the audio on both models leverages beamforming technology as well as AI noise canceling to help voices sound clearer in noisy environments. Users can choose from four pickup modes designed for different sound sources like \"Focus\" that isolates a single voice or \"Wide\" if there are multiple speakers. Video resolution on both models tops out 4K at 30 fps, and Insta360 says its updated True Focus system uses phase-detection autofocus to lock onto subjects, keeping them in focus while they move. There's also a \"Natural Bokeh\" mode meant to mimic the shallow depth-of-field look of a traditional DSLR camera, for users who enjoy that look. As for what sets them apart, the Link 2 Pro sports a 2-axis gimbal for AI-assisted tracking, which offers single or group-mode framing, while the Link 2C Pro is static and designed for fixed-position setups. Both models offer gesture control features, allowing users to control certain functions hands-free. These include starting or stopping tracking and zooming in or out. Both models also include a magnetic mount for easy placement on metal surfaces. Several different modes are offered that aid in teaching and presenting. Among them are Smart Whiteboard mode, which will automatically detect a user's whiteboard and keep it clearly in frame, and DeskView mode, which captures an overhead view of a user's desk. There's also a green screen mode, a portrait mode and support for virtual backgrounds. The new webcams also support Insta360 InSight, the company's subscription AI-powered meeting assistant. InSight can record meetings, generate transcripts, create summaries and more. The Link 2 Pro will retail for $250 while the Link 2C Pro will go for $200. Both models are available for purchase now.This article originally appeared on Engadget at https://www.engadget.com/cameras/insta360-releases-ai-powered-follow-up-to-its-link-webcams-130003572.html?src=rss",
          "feed_position": 18
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/technology/salesforce-rolls-out-new-slackbot-ai-agent-as-it-battles-microsoft-and",
          "published_at": "Tue, 13 Jan 2026 13:00:00 GMT",
          "title": "Salesforce rolls out new Slackbot AI agent as it battles Microsoft and Google in workplace AI",
          "standfirst": "Salesforce on Tuesday launched an entirely rebuilt version of Slackbot, the company&#x27;s workplace assistant, transforming it from a simple notification tool into what executives describe as a fully powered AI agent capable of searching enterprise data, drafting documents, and taking action on behalf of employees.The new Slackbot, now generally available to Business+ and Enterprise+ customers, is Salesforce&#x27;s most aggressive move yet to position Slack at the center of the emerging \"agentic AI\" movement — where software agents work alongside humans to complete complex tasks. The launch comes as Salesforce attempts to convince investors that artificial intelligence will bolster its products rather than render them obsolete.\"Slackbot isn&#x27;t just another copilot or AI assistant,\" said Parker Harris, Salesforce co-founder and Slack&#x27;s chief technology officer, in an exclusive interview with Salesforce. \"It&#x27;s the front door to the agentic enterprise, powered by Salesforce.\"From tricycle to Porsche: Salesforce rebuilt Slackbot from the ground upHarris was blunt about what distinguishes the new Slackbot from its predecessor: \"The old Slackbot was, you know, a little tricycle, and the new Slackbot is like, you know, a Porsche.\"The original Slackbot, which has existed since Slack&#x27;s early days, performed basic algorithmic tasks — reminding users to add colleagues to documents, suggesting channel archives, and delivering simple notifications. The new version runs on an entirely different architecture built around a large language model and sophisticated search capabilities that can access Salesforce records, Google Drive files, calendar data, and years of Slack conversations.\"It&#x27;s two different things,\" Harris explained. \"The old Slackbot was algorithmic and fairly simple. The new Slackbot is brand new — it&#x27;s based around an LLM and a very robust search engine, and connections to third-party search engines, third-party enterprise data.\"Salesforce chose to retain the Slackbot brand despite the fundamental technical overhaul. \"People know what Slackbot is, and so we wanted to carry that forward,\" Harris said.Why Anthropic&#x27;s Claude powers the new Slackbot — and which AI models could come nextThe new Slackbot runs on Claude, Anthropic&#x27;s large language model, a choice driven partly by compliance requirements. Slack&#x27;s commercial service operates under FedRAMP Moderate certification to serve U.S. federal government customers, and Harris said Anthropic was \"the only provider that could give us a compliant LLM\" when Slack began building the new system.But that exclusivity won&#x27;t last. \"We are, this year, going to support additional providers,\" Harris said. \"We have a great relationship with Google. Gemini is incredible — performance is great, cost is great. So we&#x27;re going to use Gemini for some things.\" He added that OpenAI remains a possibility as well.Harris echoed Salesforce CEO Marc Benioff&#x27;s view that large language models are becoming commoditized: \"You&#x27;ve heard Marc talk about LLMs are commodities, that they&#x27;re democratized. I call them CPUs.\"On the sensitive question of training data, Harris was unequivocal: Salesforce does not train any models on customer data. \"Models don&#x27;t have any sort of security,\" he explained. \"If we trained it on some confidential conversation that you and I have, I don&#x27;t want Carolyn to know — if I train it into the LLM, there is no way for me to say you get to see the answer, but Carolyn doesn&#x27;t.\"Inside Salesforce&#x27;s internal experiment: 80,000 employees tested Slackbot with striking resultsSalesforce has been testing the new Slackbot internally for months, rolling it out to all 80,000 employees. According to Ryan Gavin, Slack&#x27;s chief marketing officer, the results have been striking: \"It&#x27;s the fastest adopted product in Salesforce history.\"Internal data shows that two-thirds of Salesforce employees have tried the new Slackbot, with 80% of those users continuing to use it regularly. Internal satisfaction rates reached 96% — the highest for any AI feature Slack has shipped. Employees report saving between two and 20 hours per week.The adoption happened largely organically. \"I think it was about five days, and a Canvas was developed by our employees called &#x27;The Most Stealable Slackbot Prompts,&#x27;\" Gavin said. \"People just started adding to it organically. I think it&#x27;s up to 250-plus prompts that are in this Canvas right now.\"Kate Crotty, a principal UX researcher at Salesforce, found that 73% of internal adoption was driven by social sharing rather than top-down mandates. \"Everybody is there to help each other learn and communicate hacks,\" she said.How Slackbot transforms scattered enterprise data into executive-ready insightsDuring a product demonstration, Amy Bauer, Slack&#x27;s product experience designer, showed how Slackbot can synthesize information across multiple sources. In one example, she asked Slackbot to analyze customer feedback from a pilot program, upload an image of a usage dashboard, and have Slackbot correlate the qualitative and quantitative data.\"This is where Slackbot really earns its keep for me,\" Bauer explained. \"What it&#x27;s doing is not just simply reading the image — it&#x27;s actually looking at the image and comparing it to the insight it just generated for me.\"Slackbot can then query Salesforce to find enterprise accounts with open deals that might be good candidates for early access, creating what Bauer called \"a really great justification and plan to move forward.\" Finally, it can synthesize all that information into a Canvas — Slack&#x27;s collaborative document format — and find calendar availability among stakeholders to schedule a review meeting.\"Up until this point, we have been working in a one-to-one capacity with Slackbot,\" Bauer said. \"But one of the benefits that I can do now is take this insight and have it generate this into a Canvas, a shared workspace where I can iterate on it, refine it with Slackbot, or share it out with my team.\"Rob Seaman, Slack&#x27;s chief product officer, said the Canvas creation demonstrates where the product is heading: \"This is making a tool call internally to Slack Canvas to actually write, effectively, a shared document. But it signals where we&#x27;re going with Slackbot — we&#x27;re eventually going to be adding in additional third-party tool calls.\"MrBeast&#x27;s company became a Slackbot guinea pig—and employees say they&#x27;re saving 90 minutes a dayAmong Salesforce&#x27;s pilot customers is Beast Industries, the parent company of YouTube star MrBeast. Luis Madrigal, the company&#x27;s chief information officer, joined the launch announcement to describe his experience.\"As somebody who has rolled out enterprise technologies for over two decades now, this was practically one of the easiest,\" Madrigal said. \"The plumbing is there. Slack as an implementation, Enterprise Tools — being able to turn on the Slackbot and the Slack AI functionality was as simple as having my team go in, review, do a quick security review.\"Madrigal said his security team signed off \"rather quickly\" — unusual for enterprise AI deployments — because Slackbot accesses only the information each individual user already has permission to view. \"Given all the guardrails you guys have put into place for Slackbot to be unique and customized to only the information that each individual user has, only the conversations and the Slack rooms and Slack channels that they&#x27;re part of—that made my security team sign off rather quickly.\"One Beast Industries employee, Sinan, the head of Beast Games marketing, reported saving \"at bare minimum, 90 minutes a day.\" Another employee, Spencer, a creative supervisor, described it as \"an assistant who&#x27;s paying attention when I&#x27;m not.\"Other pilot customers include Slalom, reMarkable, Xero, Mercari, and Engine. Mollie Bodensteiner, SVP of Operations at Engine, called Slackbot \"an absolute &#x27;chaos tamer&#x27; for our team,\" estimating it saves her about 30 minutes daily \"just by eliminating context switching.\"Slackbot vs. Microsoft Copilot vs. Google Gemini: The fight for enterprise AI dominanceThe launch puts Salesforce in direct competition with Microsoft&#x27;s Copilot, which is integrated into Teams and the broader Microsoft 365 suite, as well as Google&#x27;s Gemini integrations across Workspace. When asked what distinguishes Slackbot from these alternatives, Seaman pointed to context and convenience.\"The thing that makes it most powerful for our customers and users is the proximity — it&#x27;s just right there in your Slack,\" Seaman said. \"There&#x27;s a tremendous convenience affordance that&#x27;s naturally built into it.\"The deeper advantage, executives argue, is that Slackbot already understands users&#x27; work without requiring setup or training. \"Most AI tools sound the same no matter who is using them,\" the company&#x27;s announcement stated. \"They lack context, miss nuance, and force you to jump between tools to get anything done.\"Harris put it more directly: \"If you&#x27;ve ever had that magic experience with AI — I think ChatGPT is a great example, it&#x27;s a great experience from a consumer perspective — Slackbot is really what we&#x27;re doing in the enterprise, to be this employee super agent that is loved, just like people love using Slack.\"Amy Bauer emphasized the frictionless nature of the experience. \"Slackbot is inherently grounded in the context, in the data that you have in Slack,\" she said. \"So as you continue working in Slack, Slackbot gets better because it&#x27;s grounded in the work that you&#x27;re doing there. There is no setup. There is no configuration for those end users.\"Salesforce&#x27;s ambitious plan to make Slackbot the one &#x27;super agent&#x27; that controls all the othersSalesforce positions Slackbot as what Harris calls a \"super agent\" — a central hub that can eventually coordinate with other AI agents across an organization.\"Every corporation is going to have an employee super agent,\" Harris said. \"Slackbot is essentially taking the magic of what Slack does. We think that Slackbot, and we&#x27;re really excited about it, is going to be that.\"The vision extends to third-party agents already launching in Slack. Last month, Anthropic released a preview of Claude Code for Slack, allowing developers to interact with Claude&#x27;s coding capabilities directly in chat threads. OpenAI, Google, Vercel, and others have also built agents for the platform.\"Most of the net-new apps that are being deployed to Slack are agents,\" Seaman noted during the press conference. \"This is proof of the promise of humans and agents coexisting and working together in Slack to solve problems.\"Harris described a future where Slackbot becomes an MCP (Model Context Protocol) client, able to leverage tools from across the software ecosystem — similar to how the developer tool Cursor works. \"Slack can be an MCP client, and Slackbot will be the hub of that, leveraging all these tools out in the world, some of which will be these amazing agents,\" he said.But Harris also cautioned against over-promising on multi-agent coordination. \"I still think we&#x27;re in the single agent world,\" he said. \"FY26 is going to be the year where we started to see more coordination. But we&#x27;re going to do it with customer success in mind, and not demonstrate and talk about, like, &#x27;I&#x27;ve got 1,000 agents working together,&#x27; because I think that&#x27;s unrealistic.\"Slackbot costs nothing extra, but Salesforce&#x27;s data access fees could squeeze some customersSlackbot is included at no additional cost for customers on Business+ and Enterprise+ plans. \"There&#x27;s no additional fees customers have to do,\" Gavin confirmed. \"If they&#x27;re on one of those plans, they&#x27;re going to get Slackbot.\"However, some enterprise customers may face other cost pressures related to Salesforce&#x27;s broader data strategy. CIOs may see price increases for third-party applications that work with Salesforce data, as effects of higher charges for API access ripple through the software supply chain.Fivetran CEO George Fraser has warned that Salesforce&#x27;s shift in pricing policy for API access could have tangible consequences for enterprises relying on Salesforce as a system of record. \"They might not be able to use Fivetran to replicate their data to Snowflake and instead have to use Salesforce Data Cloud. Or they might find that they are not able to interact with their data via ChatGPT, and instead have to use Agentforce,\" Fraser said in a recent CIO report.Salesforce has framed the pricing change as standard industry practice.What Slackbot can do today, what&#x27;s coming in weeks, and what&#x27;s still on the roadmapThe new Slackbot begins rolling out today and will reach all eligible customers by the end of February. Mobile availability will complete by March 3, Bauer confirmed during her interview with VentureBeat.Some capabilities remain works in progress. Calendar reading and availability checking are available at launch, but the ability to actually book meetings is \"coming a few weeks after,\" according to Seaman. Image generation is not currently supported, though Bauer said it&#x27;s \"something that we are looking at in the future.\"When asked about integration with competing CRM systems like HubSpot and Microsoft Dynamics, Salesforce representatives declined to provide specifics during the interview, though they acknowledged the question touched on key competitive differentiators.Salesforce is betting the future of work looks like a chat window—and it&#x27;s not aloneThe Slackbot launch is Salesforce&#x27;s bet that the future of enterprise work is conversational — that employees will increasingly prefer to interact with AI through natural language rather than navigating traditional software interfaces.Harris described Slack&#x27;s product philosophy using principles like \"don&#x27;t make me think\" and \"be a great host.\" The goal, he said, is for Slackbot to surface information proactively rather than requiring users to hunt for it.\"One of the revelations for me is LLMs applied to unstructured information are incredible,\" Harris said. \"And the amount of value you have if you&#x27;re a Slack user, if your corporation uses Slack — the amount of value in Slack is unbelievable. Because you&#x27;re talking about work, you&#x27;re sharing documents, you&#x27;re making decisions, but you can&#x27;t as a human go through that and really get the same value that an LLM can do.\"Looking ahead, Harris expects the interfaces themselves to evolve beyond pure conversation. \"We&#x27;re kind of saturating what we can do with purely conversational UIs,\" he said. \"I think we&#x27;ll start to see agents building an interface that best suits your intent, as opposed to trying to surface something within a conversational interface that matches your intent.\"Microsoft, Google, and a growing roster of AI startups are placing similar bets — that the winning enterprise AI will be the one embedded in the tools workers already use, not another application to learn. The race to become that invisible layer of workplace intelligence is now fully underway.For Salesforce, the stakes extend beyond a single product launch. After a bruising year on Wall Street and persistent questions about whether AI threatens its core business, the company is wagering that Slackbot can prove the opposite — that the tens of millions of people already chatting in Slack every day is not a vulnerability, but an unassailable advantage.Haley Gault, the Salesforce account executive in Pittsburgh who stumbled upon the new Slackbot on a snowy morning, captured the shift in a single sentence: \"I honestly can&#x27;t imagine working for another company not having access to these types of tools. This is just how I work now.\"That&#x27;s precisely what Salesforce is counting on.",
          "content": "Salesforce on Tuesday launched an entirely rebuilt version of Slackbot, the company&#x27;s workplace assistant, transforming it from a simple notification tool into what executives describe as a fully powered AI agent capable of searching enterprise data, drafting documents, and taking action on behalf of employees.The new Slackbot, now generally available to Business+ and Enterprise+ customers, is Salesforce&#x27;s most aggressive move yet to position Slack at the center of the emerging \"agentic AI\" movement — where software agents work alongside humans to complete complex tasks. The launch comes as Salesforce attempts to convince investors that artificial intelligence will bolster its products rather than render them obsolete.\"Slackbot isn&#x27;t just another copilot or AI assistant,\" said Parker Harris, Salesforce co-founder and Slack&#x27;s chief technology officer, in an exclusive interview with Salesforce. \"It&#x27;s the front door to the agentic enterprise, powered by Salesforce.\"From tricycle to Porsche: Salesforce rebuilt Slackbot from the ground upHarris was blunt about what distinguishes the new Slackbot from its predecessor: \"The old Slackbot was, you know, a little tricycle, and the new Slackbot is like, you know, a Porsche.\"The original Slackbot, which has existed since Slack&#x27;s early days, performed basic algorithmic tasks — reminding users to add colleagues to documents, suggesting channel archives, and delivering simple notifications. The new version runs on an entirely different architecture built around a large language model and sophisticated search capabilities that can access Salesforce records, Google Drive files, calendar data, and years of Slack conversations.\"It&#x27;s two different things,\" Harris explained. \"The old Slackbot was algorithmic and fairly simple. The new Slackbot is brand new — it&#x27;s based around an LLM and a very robust search engine, and connections to third-party search engines, third-party enterprise data.\"Salesforce chose to retain the Slackbot brand despite the fundamental technical overhaul. \"People know what Slackbot is, and so we wanted to carry that forward,\" Harris said.Why Anthropic&#x27;s Claude powers the new Slackbot — and which AI models could come nextThe new Slackbot runs on Claude, Anthropic&#x27;s large language model, a choice driven partly by compliance requirements. Slack&#x27;s commercial service operates under FedRAMP Moderate certification to serve U.S. federal government customers, and Harris said Anthropic was \"the only provider that could give us a compliant LLM\" when Slack began building the new system.But that exclusivity won&#x27;t last. \"We are, this year, going to support additional providers,\" Harris said. \"We have a great relationship with Google. Gemini is incredible — performance is great, cost is great. So we&#x27;re going to use Gemini for some things.\" He added that OpenAI remains a possibility as well.Harris echoed Salesforce CEO Marc Benioff&#x27;s view that large language models are becoming commoditized: \"You&#x27;ve heard Marc talk about LLMs are commodities, that they&#x27;re democratized. I call them CPUs.\"On the sensitive question of training data, Harris was unequivocal: Salesforce does not train any models on customer data. \"Models don&#x27;t have any sort of security,\" he explained. \"If we trained it on some confidential conversation that you and I have, I don&#x27;t want Carolyn to know — if I train it into the LLM, there is no way for me to say you get to see the answer, but Carolyn doesn&#x27;t.\"Inside Salesforce&#x27;s internal experiment: 80,000 employees tested Slackbot with striking resultsSalesforce has been testing the new Slackbot internally for months, rolling it out to all 80,000 employees. According to Ryan Gavin, Slack&#x27;s chief marketing officer, the results have been striking: \"It&#x27;s the fastest adopted product in Salesforce history.\"Internal data shows that two-thirds of Salesforce employees have tried the new Slackbot, with 80% of those users continuing to use it regularly. Internal satisfaction rates reached 96% — the highest for any AI feature Slack has shipped. Employees report saving between two and 20 hours per week.The adoption happened largely organically. \"I think it was about five days, and a Canvas was developed by our employees called &#x27;The Most Stealable Slackbot Prompts,&#x27;\" Gavin said. \"People just started adding to it organically. I think it&#x27;s up to 250-plus prompts that are in this Canvas right now.\"Kate Crotty, a principal UX researcher at Salesforce, found that 73% of internal adoption was driven by social sharing rather than top-down mandates. \"Everybody is there to help each other learn and communicate hacks,\" she said.How Slackbot transforms scattered enterprise data into executive-ready insightsDuring a product demonstration, Amy Bauer, Slack&#x27;s product experience designer, showed how Slackbot can synthesize information across multiple sources. In one example, she asked Slackbot to analyze customer feedback from a pilot program, upload an image of a usage dashboard, and have Slackbot correlate the qualitative and quantitative data.\"This is where Slackbot really earns its keep for me,\" Bauer explained. \"What it&#x27;s doing is not just simply reading the image — it&#x27;s actually looking at the image and comparing it to the insight it just generated for me.\"Slackbot can then query Salesforce to find enterprise accounts with open deals that might be good candidates for early access, creating what Bauer called \"a really great justification and plan to move forward.\" Finally, it can synthesize all that information into a Canvas — Slack&#x27;s collaborative document format — and find calendar availability among stakeholders to schedule a review meeting.\"Up until this point, we have been working in a one-to-one capacity with Slackbot,\" Bauer said. \"But one of the benefits that I can do now is take this insight and have it generate this into a Canvas, a shared workspace where I can iterate on it, refine it with Slackbot, or share it out with my team.\"Rob Seaman, Slack&#x27;s chief product officer, said the Canvas creation demonstrates where the product is heading: \"This is making a tool call internally to Slack Canvas to actually write, effectively, a shared document. But it signals where we&#x27;re going with Slackbot — we&#x27;re eventually going to be adding in additional third-party tool calls.\"MrBeast&#x27;s company became a Slackbot guinea pig—and employees say they&#x27;re saving 90 minutes a dayAmong Salesforce&#x27;s pilot customers is Beast Industries, the parent company of YouTube star MrBeast. Luis Madrigal, the company&#x27;s chief information officer, joined the launch announcement to describe his experience.\"As somebody who has rolled out enterprise technologies for over two decades now, this was practically one of the easiest,\" Madrigal said. \"The plumbing is there. Slack as an implementation, Enterprise Tools — being able to turn on the Slackbot and the Slack AI functionality was as simple as having my team go in, review, do a quick security review.\"Madrigal said his security team signed off \"rather quickly\" — unusual for enterprise AI deployments — because Slackbot accesses only the information each individual user already has permission to view. \"Given all the guardrails you guys have put into place for Slackbot to be unique and customized to only the information that each individual user has, only the conversations and the Slack rooms and Slack channels that they&#x27;re part of—that made my security team sign off rather quickly.\"One Beast Industries employee, Sinan, the head of Beast Games marketing, reported saving \"at bare minimum, 90 minutes a day.\" Another employee, Spencer, a creative supervisor, described it as \"an assistant who&#x27;s paying attention when I&#x27;m not.\"Other pilot customers include Slalom, reMarkable, Xero, Mercari, and Engine. Mollie Bodensteiner, SVP of Operations at Engine, called Slackbot \"an absolute &#x27;chaos tamer&#x27; for our team,\" estimating it saves her about 30 minutes daily \"just by eliminating context switching.\"Slackbot vs. Microsoft Copilot vs. Google Gemini: The fight for enterprise AI dominanceThe launch puts Salesforce in direct competition with Microsoft&#x27;s Copilot, which is integrated into Teams and the broader Microsoft 365 suite, as well as Google&#x27;s Gemini integrations across Workspace. When asked what distinguishes Slackbot from these alternatives, Seaman pointed to context and convenience.\"The thing that makes it most powerful for our customers and users is the proximity — it&#x27;s just right there in your Slack,\" Seaman said. \"There&#x27;s a tremendous convenience affordance that&#x27;s naturally built into it.\"The deeper advantage, executives argue, is that Slackbot already understands users&#x27; work without requiring setup or training. \"Most AI tools sound the same no matter who is using them,\" the company&#x27;s announcement stated. \"They lack context, miss nuance, and force you to jump between tools to get anything done.\"Harris put it more directly: \"If you&#x27;ve ever had that magic experience with AI — I think ChatGPT is a great example, it&#x27;s a great experience from a consumer perspective — Slackbot is really what we&#x27;re doing in the enterprise, to be this employee super agent that is loved, just like people love using Slack.\"Amy Bauer emphasized the frictionless nature of the experience. \"Slackbot is inherently grounded in the context, in the data that you have in Slack,\" she said. \"So as you continue working in Slack, Slackbot gets better because it&#x27;s grounded in the work that you&#x27;re doing there. There is no setup. There is no configuration for those end users.\"Salesforce&#x27;s ambitious plan to make Slackbot the one &#x27;super agent&#x27; that controls all the othersSalesforce positions Slackbot as what Harris calls a \"super agent\" — a central hub that can eventually coordinate with other AI agents across an organization.\"Every corporation is going to have an employee super agent,\" Harris said. \"Slackbot is essentially taking the magic of what Slack does. We think that Slackbot, and we&#x27;re really excited about it, is going to be that.\"The vision extends to third-party agents already launching in Slack. Last month, Anthropic released a preview of Claude Code for Slack, allowing developers to interact with Claude&#x27;s coding capabilities directly in chat threads. OpenAI, Google, Vercel, and others have also built agents for the platform.\"Most of the net-new apps that are being deployed to Slack are agents,\" Seaman noted during the press conference. \"This is proof of the promise of humans and agents coexisting and working together in Slack to solve problems.\"Harris described a future where Slackbot becomes an MCP (Model Context Protocol) client, able to leverage tools from across the software ecosystem — similar to how the developer tool Cursor works. \"Slack can be an MCP client, and Slackbot will be the hub of that, leveraging all these tools out in the world, some of which will be these amazing agents,\" he said.But Harris also cautioned against over-promising on multi-agent coordination. \"I still think we&#x27;re in the single agent world,\" he said. \"FY26 is going to be the year where we started to see more coordination. But we&#x27;re going to do it with customer success in mind, and not demonstrate and talk about, like, &#x27;I&#x27;ve got 1,000 agents working together,&#x27; because I think that&#x27;s unrealistic.\"Slackbot costs nothing extra, but Salesforce&#x27;s data access fees could squeeze some customersSlackbot is included at no additional cost for customers on Business+ and Enterprise+ plans. \"There&#x27;s no additional fees customers have to do,\" Gavin confirmed. \"If they&#x27;re on one of those plans, they&#x27;re going to get Slackbot.\"However, some enterprise customers may face other cost pressures related to Salesforce&#x27;s broader data strategy. CIOs may see price increases for third-party applications that work with Salesforce data, as effects of higher charges for API access ripple through the software supply chain.Fivetran CEO George Fraser has warned that Salesforce&#x27;s shift in pricing policy for API access could have tangible consequences for enterprises relying on Salesforce as a system of record. \"They might not be able to use Fivetran to replicate their data to Snowflake and instead have to use Salesforce Data Cloud. Or they might find that they are not able to interact with their data via ChatGPT, and instead have to use Agentforce,\" Fraser said in a recent CIO report.Salesforce has framed the pricing change as standard industry practice.What Slackbot can do today, what&#x27;s coming in weeks, and what&#x27;s still on the roadmapThe new Slackbot begins rolling out today and will reach all eligible customers by the end of February. Mobile availability will complete by March 3, Bauer confirmed during her interview with VentureBeat.Some capabilities remain works in progress. Calendar reading and availability checking are available at launch, but the ability to actually book meetings is \"coming a few weeks after,\" according to Seaman. Image generation is not currently supported, though Bauer said it&#x27;s \"something that we are looking at in the future.\"When asked about integration with competing CRM systems like HubSpot and Microsoft Dynamics, Salesforce representatives declined to provide specifics during the interview, though they acknowledged the question touched on key competitive differentiators.Salesforce is betting the future of work looks like a chat window—and it&#x27;s not aloneThe Slackbot launch is Salesforce&#x27;s bet that the future of enterprise work is conversational — that employees will increasingly prefer to interact with AI through natural language rather than navigating traditional software interfaces.Harris described Slack&#x27;s product philosophy using principles like \"don&#x27;t make me think\" and \"be a great host.\" The goal, he said, is for Slackbot to surface information proactively rather than requiring users to hunt for it.\"One of the revelations for me is LLMs applied to unstructured information are incredible,\" Harris said. \"And the amount of value you have if you&#x27;re a Slack user, if your corporation uses Slack — the amount of value in Slack is unbelievable. Because you&#x27;re talking about work, you&#x27;re sharing documents, you&#x27;re making decisions, but you can&#x27;t as a human go through that and really get the same value that an LLM can do.\"Looking ahead, Harris expects the interfaces themselves to evolve beyond pure conversation. \"We&#x27;re kind of saturating what we can do with purely conversational UIs,\" he said. \"I think we&#x27;ll start to see agents building an interface that best suits your intent, as opposed to trying to surface something within a conversational interface that matches your intent.\"Microsoft, Google, and a growing roster of AI startups are placing similar bets — that the winning enterprise AI will be the one embedded in the tools workers already use, not another application to learn. The race to become that invisible layer of workplace intelligence is now fully underway.For Salesforce, the stakes extend beyond a single product launch. After a bruising year on Wall Street and persistent questions about whether AI threatens its core business, the company is wagering that Slackbot can prove the opposite — that the tens of millions of people already chatting in Slack every day is not a vulnerability, but an unassailable advantage.Haley Gault, the Salesforce account executive in Pittsburgh who stumbled upon the new Slackbot on a snowy morning, captured the shift in a single sentence: \"I honestly can&#x27;t imagine working for another company not having access to these types of tools. This is just how I work now.\"That&#x27;s precisely what Salesforce is counting on.",
          "feed_position": 3,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/4Xrcg14GLKFlwSEnuEzxyS/21c85d29d03c4c974076475c009e3b38/nuneybits_Vector_art_of_chat_bubbles_on_a_computer_screen_in_th_5018a7ea-3496-4103-8453-7ba1b129189a.webp?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/technology/why-sakana-ais-big-win-is-a-big-deal-for-the-future-of-enterprise-agents",
          "published_at": "Tue, 13 Jan 2026 12:59:00 GMT",
          "title": "Why Sakana AI’s big win is a big deal for the future of enterprise agents",
          "standfirst": "In an impressive feat, Japanese startup Sakana AI’s coding agent ALE-Agent recently secured first place in the AtCoder Heuristic Contest (AHC058), a complex coding competition that involves complicated optimization problems — and a more difficult and perhaps telling challenge than benchmarks like HumanEval, which mostly test the ability to write isolated functions, and which many AI models and agents now regularly pass with ease (\"benchmark saturation\"). Sakana&#x27;s accomplishment with ALE-Agent hints at a shift toward agents capable of autonomously optimizing themselves to navigate and perform well in complex, dynamic systems such as enterprise software stacks, workflows, and operational environments. In four hours, the agent used inference-time scaling to generate, test, and iterate over hundreds of solutions, solving a problem that typically requires deep intuition and time-consuming trial and error from human experts. It outperformed over 800 human participants, including top-tier competitive programmers.How ALE-Agent worksThe challenge in AHC058 was a classic combinatorial optimization problem. Participants were tasked with managing a set of machines with hierarchical relationships, such as machines that produce apples, and other machines that build those apple-producing machines. The goal was to maximize output over a fixed number of turns.In the enterprise world, this workflow usually follows a strict pattern: a domain expert works with a client to define an \"objective function\" (aka the Scorer), and then engineers build a software system to optimize it. These problems are notoriously difficult because they cannot be solved in a single stage. They require exploration, strategy, and the ability to pivot when a plan isn&#x27;t working.Human experts typically approach this using a two-stage strategy. First, they use a \"Greedy\" method (a lightweight solver that makes the best immediate choice at each step) to generate a decent baseline solution. Then, they apply \"simulated annealing,\" a technique that takes the existing plan and makes tiny, random adjustments to see if the score improves. However, this standard approach is rigid. If the initial Greedy plan heads in the wrong direction, simulated annealing can rarely fix it because it only looks for local improvements in a faulty area of the solution space.ALE-Agent’s innovation was transforming this static initialization tool into a dynamic reconstruction engine. Instead of relying on immediate value, the agent independently derived a concept it called \"Virtual Power.\" It assigned values to components that were not yet operational, treating them as if they already possessed value. By valuing potential future assets rather than just current ones, the agent capitalized on the \"compound interest effect,\" a concept it explicitly identified in its internal logs. Basically, it could look a few steps ahead and reason about the future instead of looking at the immediate feedback it was receiving from its environment.Crucially, the agent needed to maintain this strategy over a four-hour window without losing focus, a common failure mode known as “context drift.” In comments provided to VentureBeat, the Sakana AI team explained that the agent generates textual \"insights\" by reflecting on each trial. It gathers this knowledge to prevent cycling back to previously failed strategies and creates a working memory that allows it to look a few steps ahead rather than just reacting to immediate feedback.Furthermore, the agent integrated Greedy methods directly into the simulated annealing phase to avoid getting stuck in local optima, using high-speed reconstruction to delete and rebuild large sections of the solution on the fly.From coding to enterprise optimizationThis breakthrough fits directly into existing enterprise workflows where a scoring function is already available. Currently, companies rely on scarce engineering talent to write optimization algorithms. ALE-Agent demonstrates a future where humans define the \"Scorer\" (i.e., the business logic and goals) and the agent handles the technical implementation.This shifts the operational bottleneck from engineering capacity to metric clarity. If an enterprise can measure a goal, the agent can optimize it. This has direct applications in logistics, such as vehicle routing, as well as server load balancing and resource allocation.According to the Sakana AI team, this could democratize optimization. \"It enables a future where non-technical clients can interact directly with the agent, tweaking business constraints in real-time until they get the output they desire,\" they said.The Sakana AI team told VentureBeat that ALE-Agent is currently proprietary and not available for public use, and the company is currently focused on internal development and proof-of-concept collaborations with enterprises.At the same time, the team is already looking ahead to \"self-rewriting\" agents. These future agents could define their own scorers, making them feasible for ill-defined problems where human experts struggle to formulate clear initial metrics.The cost of intelligenceRunning ALE-Agent was not cheap. The four-hour operation incurred approximately $1,300 in compute costs involving over 4,000 reasoning calls to models like GPT-5.2 and Gemini 3 Pro. While this price point might seem high for a single coding task, the return on investment for optimization problems is often asymmetric. In a resource-management setting, a one-time cost of a few thousand dollars can result in millions of dollars in annual efficiency savings.However, enterprises expecting costs to simply drop might be missing the strategic picture. While the cost of tokens is falling, total spend may actually rise as companies compete for better answers, a concept known as the Jevons paradox.\"While smarter algorithms will drive efficiency, the primary value of AI is its ability to explore vast solution spaces,\" the Sakana AI team said. \"As inference costs fall, rather than simply banking the savings, enterprises will likely choose to leverage that affordability to conduct even deeper, broader searches to find superior solutions.\"The experiment highlights the immense value still to be unlocked through inference-time scaling techniques. As AI systems gain the ability to handle complex reasoning tasks across longer contexts, building better scaffolding and allocating larger budgets for \"thinking time\" allows agents to rival top human experts.",
          "content": "In an impressive feat, Japanese startup Sakana AI’s coding agent ALE-Agent recently secured first place in the AtCoder Heuristic Contest (AHC058), a complex coding competition that involves complicated optimization problems — and a more difficult and perhaps telling challenge than benchmarks like HumanEval, which mostly test the ability to write isolated functions, and which many AI models and agents now regularly pass with ease (\"benchmark saturation\"). Sakana&#x27;s accomplishment with ALE-Agent hints at a shift toward agents capable of autonomously optimizing themselves to navigate and perform well in complex, dynamic systems such as enterprise software stacks, workflows, and operational environments. In four hours, the agent used inference-time scaling to generate, test, and iterate over hundreds of solutions, solving a problem that typically requires deep intuition and time-consuming trial and error from human experts. It outperformed over 800 human participants, including top-tier competitive programmers.How ALE-Agent worksThe challenge in AHC058 was a classic combinatorial optimization problem. Participants were tasked with managing a set of machines with hierarchical relationships, such as machines that produce apples, and other machines that build those apple-producing machines. The goal was to maximize output over a fixed number of turns.In the enterprise world, this workflow usually follows a strict pattern: a domain expert works with a client to define an \"objective function\" (aka the Scorer), and then engineers build a software system to optimize it. These problems are notoriously difficult because they cannot be solved in a single stage. They require exploration, strategy, and the ability to pivot when a plan isn&#x27;t working.Human experts typically approach this using a two-stage strategy. First, they use a \"Greedy\" method (a lightweight solver that makes the best immediate choice at each step) to generate a decent baseline solution. Then, they apply \"simulated annealing,\" a technique that takes the existing plan and makes tiny, random adjustments to see if the score improves. However, this standard approach is rigid. If the initial Greedy plan heads in the wrong direction, simulated annealing can rarely fix it because it only looks for local improvements in a faulty area of the solution space.ALE-Agent’s innovation was transforming this static initialization tool into a dynamic reconstruction engine. Instead of relying on immediate value, the agent independently derived a concept it called \"Virtual Power.\" It assigned values to components that were not yet operational, treating them as if they already possessed value. By valuing potential future assets rather than just current ones, the agent capitalized on the \"compound interest effect,\" a concept it explicitly identified in its internal logs. Basically, it could look a few steps ahead and reason about the future instead of looking at the immediate feedback it was receiving from its environment.Crucially, the agent needed to maintain this strategy over a four-hour window without losing focus, a common failure mode known as “context drift.” In comments provided to VentureBeat, the Sakana AI team explained that the agent generates textual \"insights\" by reflecting on each trial. It gathers this knowledge to prevent cycling back to previously failed strategies and creates a working memory that allows it to look a few steps ahead rather than just reacting to immediate feedback.Furthermore, the agent integrated Greedy methods directly into the simulated annealing phase to avoid getting stuck in local optima, using high-speed reconstruction to delete and rebuild large sections of the solution on the fly.From coding to enterprise optimizationThis breakthrough fits directly into existing enterprise workflows where a scoring function is already available. Currently, companies rely on scarce engineering talent to write optimization algorithms. ALE-Agent demonstrates a future where humans define the \"Scorer\" (i.e., the business logic and goals) and the agent handles the technical implementation.This shifts the operational bottleneck from engineering capacity to metric clarity. If an enterprise can measure a goal, the agent can optimize it. This has direct applications in logistics, such as vehicle routing, as well as server load balancing and resource allocation.According to the Sakana AI team, this could democratize optimization. \"It enables a future where non-technical clients can interact directly with the agent, tweaking business constraints in real-time until they get the output they desire,\" they said.The Sakana AI team told VentureBeat that ALE-Agent is currently proprietary and not available for public use, and the company is currently focused on internal development and proof-of-concept collaborations with enterprises.At the same time, the team is already looking ahead to \"self-rewriting\" agents. These future agents could define their own scorers, making them feasible for ill-defined problems where human experts struggle to formulate clear initial metrics.The cost of intelligenceRunning ALE-Agent was not cheap. The four-hour operation incurred approximately $1,300 in compute costs involving over 4,000 reasoning calls to models like GPT-5.2 and Gemini 3 Pro. While this price point might seem high for a single coding task, the return on investment for optimization problems is often asymmetric. In a resource-management setting, a one-time cost of a few thousand dollars can result in millions of dollars in annual efficiency savings.However, enterprises expecting costs to simply drop might be missing the strategic picture. While the cost of tokens is falling, total spend may actually rise as companies compete for better answers, a concept known as the Jevons paradox.\"While smarter algorithms will drive efficiency, the primary value of AI is its ability to explore vast solution spaces,\" the Sakana AI team said. \"As inference costs fall, rather than simply banking the savings, enterprises will likely choose to leverage that affordability to conduct even deeper, broader searches to find superior solutions.\"The experiment highlights the immense value still to be unlocked through inference-time scaling techniques. As AI systems gain the ability to handle complex reasoning tasks across longer contexts, building better scaffolding and allocating larger budgets for \"thinking time\" allows agents to rival top human experts.",
          "feed_position": 4,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/3BUB5AM3ylCZKK1ID2lgQV/b563d16484c9a96c1bc412c1a4f404e2/AI_optimization_algorithm.jpg?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/general/the-morning-after-engadget-newsletter-151521620.html",
          "published_at": "Tue, 13 Jan 2026 12:15:21 +0000",
          "title": "The Morning After: Apple will use Gemini to power Siri AI",
          "standfirst": "Apple and Google have confirmed that Gemini’s models power the new version of Siri and other generative AI features. CNBC broke the news, but Apple and Google soon followed up with a lengthy joint statement. Here’s part of it: “Apple determined that Google’s Al technology provides the most capable foundation for Apple Foundation Models… Apple Intelligence will continue to run on Apple devices and Private Cloud Compute, while maintaining Apple’s industry-leading privacy standards.” In June, it was reported that Apple was considering partnerships with OpenAI and Anthropic for Siri (the voice assistant can currently tap ChatGPT for certain queries as part of Apple Intelligence). Two months later, Google emerged as a contender. Another report suggested Apple might build the new Siri using a custom version of Gemini — and that it would pay Google around $1 billion a year for the privilege. However, no official deal numbers were shared. It’s also notable that current iPhones have direct access to OpenAI’s ChatGPT. But how long for? — Mat Smith The other big stories this morning Engadget Podcast: Best of CES 2026 and a chat with Pebble’s founder Meta closes 550,000 accounts to comply with Australia’s kids social media ban The best winter tech to get you through the coldest months Meta appoints ex-Trump and Bush official as its new president and vice chair Netflix wins 7 awards at the Golden Globes Adolescence and KPop Demon Hunters picked up several each. Netflix Netflix’s hit show Adolescence received four awards, including best limited or anthology series. It also won best actor (Stephen Graham), supporting actor (Owen Cooper) and supporting actress (Erin Doherty) in a miniseries or television film. KPop Demon Hunters — which my nieces refuse to stop talking about — won best animated feature and best original song. “I just want to say this award goes to people who have had doors closed on them, and I can confidently say rejection is redirection. So never give up.” It’s not all good news. Netflix also won best performance in stand-up comedy on television for Ricky Gervais: Mortality. Apple and HBO Max each won three, and Hulu got one award on the night. Continue reading. NASA makes final preparations for its first crewed moon mission in over 50 years The agency is targeting a February launch date for Artemis 2. A few years ago, NASA announced it was pushing the Artemis 2 mission back to April 2026. The agency now says it could launch as early as February. NASA is finalizing preparations for the mission and will soon roll out the Space Launch System (SLS) rocket and the Orion spacecraft to the launch pad at the Kennedy Space Center in Florida. Artemis 2 is the first crewed mission to the moon since the Apollo program’s final flight in 1972. The 10-day mission will have four astronauts, who’ll test whether Orion’s critical life-support systems can sustain human passengers on future longer-duration missions. They will first orbit the Earth twice before making their way 4,700 miles beyond the far side of the moon. Continue reading. Lego’s first Pokémon sets are now available for pre-order Pikachu, Eevee, Venusaur, Charizard and Blastoise will ship February 27. Lego Pre-orders for the first three Lego-Pokémon kits are open now. One of the debut pocket monsters is, of course, Pikachu. You can build the 2,050-piece kit to show Pikachu either at rest or leaping out of an open Poké Ball into battle. It costs $200. There’s also a 587-piece model of Eevee, for $60. On the bigger side is a set featuring a trio of Pokémon: Venusaur, Charizard and Blastoise. This kit has 6,838 pieces and can show the group together in battle formation or separately in their own mini environments. It retails for $650 and is a bit much. Continue reading.This article originally appeared on Engadget at https://www.engadget.com/general/the-morning-after-engadget-newsletter-151521620.html?src=rss",
          "content": "Apple and Google have confirmed that Gemini’s models power the new version of Siri and other generative AI features. CNBC broke the news, but Apple and Google soon followed up with a lengthy joint statement. Here’s part of it: “Apple determined that Google’s Al technology provides the most capable foundation for Apple Foundation Models… Apple Intelligence will continue to run on Apple devices and Private Cloud Compute, while maintaining Apple’s industry-leading privacy standards.” In June, it was reported that Apple was considering partnerships with OpenAI and Anthropic for Siri (the voice assistant can currently tap ChatGPT for certain queries as part of Apple Intelligence). Two months later, Google emerged as a contender. Another report suggested Apple might build the new Siri using a custom version of Gemini — and that it would pay Google around $1 billion a year for the privilege. However, no official deal numbers were shared. It’s also notable that current iPhones have direct access to OpenAI’s ChatGPT. But how long for? — Mat Smith The other big stories this morning Engadget Podcast: Best of CES 2026 and a chat with Pebble’s founder Meta closes 550,000 accounts to comply with Australia’s kids social media ban The best winter tech to get you through the coldest months Meta appoints ex-Trump and Bush official as its new president and vice chair Netflix wins 7 awards at the Golden Globes Adolescence and KPop Demon Hunters picked up several each. Netflix Netflix’s hit show Adolescence received four awards, including best limited or anthology series. It also won best actor (Stephen Graham), supporting actor (Owen Cooper) and supporting actress (Erin Doherty) in a miniseries or television film. KPop Demon Hunters — which my nieces refuse to stop talking about — won best animated feature and best original song. “I just want to say this award goes to people who have had doors closed on them, and I can confidently say rejection is redirection. So never give up.” It’s not all good news. Netflix also won best performance in stand-up comedy on television for Ricky Gervais: Mortality. Apple and HBO Max each won three, and Hulu got one award on the night. Continue reading. NASA makes final preparations for its first crewed moon mission in over 50 years The agency is targeting a February launch date for Artemis 2. A few years ago, NASA announced it was pushing the Artemis 2 mission back to April 2026. The agency now says it could launch as early as February. NASA is finalizing preparations for the mission and will soon roll out the Space Launch System (SLS) rocket and the Orion spacecraft to the launch pad at the Kennedy Space Center in Florida. Artemis 2 is the first crewed mission to the moon since the Apollo program’s final flight in 1972. The 10-day mission will have four astronauts, who’ll test whether Orion’s critical life-support systems can sustain human passengers on future longer-duration missions. They will first orbit the Earth twice before making their way 4,700 miles beyond the far side of the moon. Continue reading. Lego’s first Pokémon sets are now available for pre-order Pikachu, Eevee, Venusaur, Charizard and Blastoise will ship February 27. Lego Pre-orders for the first three Lego-Pokémon kits are open now. One of the debut pocket monsters is, of course, Pikachu. You can build the 2,050-piece kit to show Pikachu either at rest or leaping out of an open Poké Ball into battle. It costs $200. There’s also a 587-piece model of Eevee, for $60. On the bigger side is a set featuring a trio of Pokémon: Venusaur, Charizard and Blastoise. This kit has 6,838 pieces and can show the group together in battle formation or separately in their own mini environments. It retails for $650 and is a bit much. Continue reading.This article originally appeared on Engadget at https://www.engadget.com/general/the-morning-after-engadget-newsletter-151521620.html?src=rss",
          "feed_position": 19,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2026-01/28879300-f057-11f0-baeb-ffbd608b486c"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/entertainment/streaming/best-streaming-devices-media-players-123021395.html",
          "published_at": "Tue, 13 Jan 2026 10:01:26 +0000",
          "title": "The best streaming devices for 2026",
          "standfirst": "With the dominance of smart TVs, streaming sticks and boxes may seem redundant — but if your smart set is slow or has a frustrating user interface, a streaming device will let you bypass your TV’s built-in OS and use Google TV, Fire TV, Apple TV or something else instead. There are a lot of streaming gadgets out there, all with different operating systems, memory capacities, video resolutions and bonus features, such as headphone connections and ambient modes that fill your screen with stills when you’re not watching. We tested options from the major brands and broke down exactly what each device gives you so you can pick the best streaming device for your TV. Table of contents The best TV streaming devices for 2026 What to look for in a TV streaming device How we tested and picked the best streaming devices Best streaming devices for 2026 What to look for in a TV streaming device Operating system and interface Google’s TV Streamer, the Apple TV 4K, Amazon’s Fire TV Sticks and Roku devices are the most popular players in the space. Three of those brands also come built into TVs, such as Fire, Google and Roku TVs, but the Apple TV 4K doesn't come pre-loaded on any set. Each one has a unique operating system and interface. This may be the biggest deciding factor for many people, as it determines how the home entertainment you want to watch is arranged and presented. We go into detail for each platform below, but all of them come with home screens that, to varying degrees, gather your apps in one place, present the movies and TV shows you’re currently watching and give you suggestions of other media streaming options. Nearly all streaming devices come with a remote that lets you search and do other operations using your voice, eliminating the need to hunt and peck at on-screen keyboards. They all offer “universal search,” in which searching for a title takes you to whichever app has it available. If you want to watch Wicked but don’t know where it’s playing, just push the voice button on the remote and say \"Wicked.” (We found simply saying the title or the genre you want sometimes works better than saying “Show me…” or “Search for…”) From the search results, hit the play button and the correct app will open and start playing — assuming you’ve previously logged into that app and, in most cases, have an active subscription. Connectivity Most streaming sticks connect to the internet via Wi-Fi, with the majority of them supporting Wi-Fi 5 or 6 protocols. Set-top boxes can also have Ethernet ports, so you can hardwire your internet connection to the device, which is typically faster than wireless. Streaming media players connect to your TV through an HDMI port, and most sticks hide behind the screen, while set-top boxes sit on a surface nearby. Nearly all units also plug into an AC outlet for power. Some sticks used to work by pulling power from a USB port on the TV, but increasingly, these devices are designed to plug into the wall. Video and audio features If your home theater setup has a screen that can display 4K content with Dolby Vision and HDR10, you’ll want a streaming device that supports those high-end formats. Of course, even the most top-shelf streamer can’t make a 1080p TV stream 4K. The series or movie also has to be transmitted in 4K and, increasingly, companies restrict higher-quality streaming to more expensive subscription plans. In short, every element needs to support the video or audio feature, otherwise the highest quality you’ll get will be the lowest of any component in the chain. Remotes Most remotes that come with streaming devices will allow you to control the power and volume of your TV. Some of the less expensive devices, however, don't have that feature, so you'll need to use your TV's remote control to turn it on, then use the streaming remote to navigate the streamer's interface. If your streamer's remote does offer power and volume controls, the setup process will usually calibrate your remote to your TV. If you want to use a soundbar, such as from Sonos or other brands, for audio you may also have to take the additional step of pairing your remote to the speaker. Voice control In addition to helping you find stuff to watch, streaming devices from Apple, Google and Amazon can answer questions about the weather, sports scores and general facts using built-in voice assistants. They can also act as smart home controllers to turn off connected smart bulbs or plugs and show feeds from smart cameras. Just remember, as with all smart home devices, compatibility is key. Fire TV devices work with Alexa-enabled smart home equipment; the Google TV Streamer lets you control Google Home devices; and Apple TV 4Ks play nice with HomeKit and other Apple devices. Rokus grant power over Roku’s smart home products, but also work with the other ecosystems. How we tested and picked the best streaming devices Like every gadget we test, we start by researching what’s worthy of reviewing. Then we get a hold of the devices ourselves and see how well they work. We don’t have a central Engadget lab; we test things in our own living rooms, on our own TV sets. We also figure that’s a better approximation of your own TV experience anyway. We began testing streaming devices as far back as 2007 with the first Apple TV device. Since then, we’ve tried out most of the major new releases to come along — from the Roku Stick back in 2014 to the 2024 Google TV Streamer 4K. A few years ago, we decided to compile the streaming devices we reviewed into this guide. Since then, we’ve updated our top picks using verdicts from our reviews, as well the testing we perform just for this guide. As new devices come out, we try them and, if something is worthy, we add it to our top picks on this list.This article originally appeared on Engadget at https://www.engadget.com/entertainment/streaming/best-streaming-devices-media-players-123021395.html?src=rss",
          "content": "With the dominance of smart TVs, streaming sticks and boxes may seem redundant — but if your smart set is slow or has a frustrating user interface, a streaming device will let you bypass your TV’s built-in OS and use Google TV, Fire TV, Apple TV or something else instead. There are a lot of streaming gadgets out there, all with different operating systems, memory capacities, video resolutions and bonus features, such as headphone connections and ambient modes that fill your screen with stills when you’re not watching. We tested options from the major brands and broke down exactly what each device gives you so you can pick the best streaming device for your TV. Table of contents The best TV streaming devices for 2026 What to look for in a TV streaming device How we tested and picked the best streaming devices Best streaming devices for 2026 What to look for in a TV streaming device Operating system and interface Google’s TV Streamer, the Apple TV 4K, Amazon’s Fire TV Sticks and Roku devices are the most popular players in the space. Three of those brands also come built into TVs, such as Fire, Google and Roku TVs, but the Apple TV 4K doesn't come pre-loaded on any set. Each one has a unique operating system and interface. This may be the biggest deciding factor for many people, as it determines how the home entertainment you want to watch is arranged and presented. We go into detail for each platform below, but all of them come with home screens that, to varying degrees, gather your apps in one place, present the movies and TV shows you’re currently watching and give you suggestions of other media streaming options. Nearly all streaming devices come with a remote that lets you search and do other operations using your voice, eliminating the need to hunt and peck at on-screen keyboards. They all offer “universal search,” in which searching for a title takes you to whichever app has it available. If you want to watch Wicked but don’t know where it’s playing, just push the voice button on the remote and say \"Wicked.” (We found simply saying the title or the genre you want sometimes works better than saying “Show me…” or “Search for…”) From the search results, hit the play button and the correct app will open and start playing — assuming you’ve previously logged into that app and, in most cases, have an active subscription. Connectivity Most streaming sticks connect to the internet via Wi-Fi, with the majority of them supporting Wi-Fi 5 or 6 protocols. Set-top boxes can also have Ethernet ports, so you can hardwire your internet connection to the device, which is typically faster than wireless. Streaming media players connect to your TV through an HDMI port, and most sticks hide behind the screen, while set-top boxes sit on a surface nearby. Nearly all units also plug into an AC outlet for power. Some sticks used to work by pulling power from a USB port on the TV, but increasingly, these devices are designed to plug into the wall. Video and audio features If your home theater setup has a screen that can display 4K content with Dolby Vision and HDR10, you’ll want a streaming device that supports those high-end formats. Of course, even the most top-shelf streamer can’t make a 1080p TV stream 4K. The series or movie also has to be transmitted in 4K and, increasingly, companies restrict higher-quality streaming to more expensive subscription plans. In short, every element needs to support the video or audio feature, otherwise the highest quality you’ll get will be the lowest of any component in the chain. Remotes Most remotes that come with streaming devices will allow you to control the power and volume of your TV. Some of the less expensive devices, however, don't have that feature, so you'll need to use your TV's remote control to turn it on, then use the streaming remote to navigate the streamer's interface. If your streamer's remote does offer power and volume controls, the setup process will usually calibrate your remote to your TV. If you want to use a soundbar, such as from Sonos or other brands, for audio you may also have to take the additional step of pairing your remote to the speaker. Voice control In addition to helping you find stuff to watch, streaming devices from Apple, Google and Amazon can answer questions about the weather, sports scores and general facts using built-in voice assistants. They can also act as smart home controllers to turn off connected smart bulbs or plugs and show feeds from smart cameras. Just remember, as with all smart home devices, compatibility is key. Fire TV devices work with Alexa-enabled smart home equipment; the Google TV Streamer lets you control Google Home devices; and Apple TV 4Ks play nice with HomeKit and other Apple devices. Rokus grant power over Roku’s smart home products, but also work with the other ecosystems. How we tested and picked the best streaming devices Like every gadget we test, we start by researching what’s worthy of reviewing. Then we get a hold of the devices ourselves and see how well they work. We don’t have a central Engadget lab; we test things in our own living rooms, on our own TV sets. We also figure that’s a better approximation of your own TV experience anyway. We began testing streaming devices as far back as 2007 with the first Apple TV device. Since then, we’ve tried out most of the major new releases to come along — from the Roku Stick back in 2014 to the 2024 Google TV Streamer 4K. A few years ago, we decided to compile the streaming devices we reviewed into this guide. Since then, we’ve updated our top picks using verdicts from our reviews, as well the testing we perform just for this guide. As new devices come out, we try them and, if something is worthy, we add it to our top picks on this list.This article originally appeared on Engadget at https://www.engadget.com/entertainment/streaming/best-streaming-devices-media-players-123021395.html?src=rss",
          "feed_position": 21
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/our-favorite-ugreen-3-in-1-wireless-charger-is-32-percent-off-right-now-214707069.html",
          "published_at": "Mon, 12 Jan 2026 21:47:07 +0000",
          "title": "Our favorite UGreen 3-in-1 wireless charger is 32 percent off right now",
          "standfirst": "Now that the winter holidays are well and truly past, now's the perfect time to take stock of your tech setup. If you were gifted (or gifted yourself) some new gear in December, make sure that you've got the proper accessories to keep that gear performing at its best. If a new way to power all those batteries would be a benefit, Amazon's currently running a discount on an excellent wireless charging pad. The UGREEN MagFlow Qi2 3-in-1 Charger Station 25W is on sale for $95. That's only a little bit above the lowest price we've ever seen for the product (which was $90), and it's still a 32 percent discount off its usual cost. This is our top pick for a 3-in-1 charging pad thanks to its versatility. The UGREEN can work equally well as a permanent fixture in your home or act as a portable charging station. It boasts a foldable design and has smart little design details to keep it feeling like a premium product. The Qi2 25W charging works across a range of iPhone models and accessories, such as AirPods. There's also a dedicated part of the pad's design for an Apple Watch, which uses a proprietary charging standard, to power up too. Just note that you'll need a newer model of phone and the latest iOS 26 in order to take full advantage of the 25W charging capability. The wireless pad also comes with both a charging plug and a cable. We felt this UGREEN model was a great value at $140, so being able to snag one for a third of the usual price is an even better deal. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/our-favorite-ugreen-3-in-1-wireless-charger-is-32-percent-off-right-now-214707069.html?src=rss",
          "content": "Now that the winter holidays are well and truly past, now's the perfect time to take stock of your tech setup. If you were gifted (or gifted yourself) some new gear in December, make sure that you've got the proper accessories to keep that gear performing at its best. If a new way to power all those batteries would be a benefit, Amazon's currently running a discount on an excellent wireless charging pad. The UGREEN MagFlow Qi2 3-in-1 Charger Station 25W is on sale for $95. That's only a little bit above the lowest price we've ever seen for the product (which was $90), and it's still a 32 percent discount off its usual cost. This is our top pick for a 3-in-1 charging pad thanks to its versatility. The UGREEN can work equally well as a permanent fixture in your home or act as a portable charging station. It boasts a foldable design and has smart little design details to keep it feeling like a premium product. The Qi2 25W charging works across a range of iPhone models and accessories, such as AirPods. There's also a dedicated part of the pad's design for an Apple Watch, which uses a proprietary charging standard, to power up too. Just note that you'll need a newer model of phone and the latest iOS 26 in order to take full advantage of the 25W charging capability. The wireless pad also comes with both a charging plug and a cable. We felt this UGREEN model was a great value at $140, so being able to snag one for a third of the usual price is an even better deal. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/our-favorite-ugreen-3-in-1-wireless-charger-is-32-percent-off-right-now-214707069.html?src=rss",
          "feed_position": 23
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/entertainment/legos-first-pokemon-sets-are-now-available-for-pre-order-205527102.html",
          "published_at": "Mon, 12 Jan 2026 20:55:27 +0000",
          "title": "Lego's first Pokémon sets are now available for pre-order",
          "standfirst": "We learned last March that Lego and Pokémon would be joining forces and the first results of their partnership are here. Pre-orders for all three kits are open now, with an expected ship date of February 27. As one might have guessed from the lightning bolts on the previous promotional image, one of the debut pocket monsters getting the brick treatment is Pikachu, complete with a Poké Ball. The 2,050-piece kit can be built to show Pikachu either leaping out of the open Poké Ball into battle or at rest staring up at the builder, closed Poké Ball between his paws. The Pikachu kit costs $200. There's also a 587-piece model of Eevee, which goes for $60. On the bigger side is a set featuring a trio of Pokémon: Venusaur, Charizard and Blastoise. This kit has 6,838 pieces and can show the group together in battle formation or separately in their own mini environments. It retails for $650. Lego Before you leap to pre-order pages, however, here's a word of caution. In Lego form, our little friends look…kinda strange? I'm not the biggest Lego builder, but I am a rather accomplished architect in Minecraft, so I am well aware of the innate challenge in constructing a rounded shape from square blocks. Take Pikachu, for instance. Part of his appeal is his chubby little cheeks. There are bricks with more rounded sides in this collection that hint at his usual rotundness, but the proportions of his face just feel a little off to me. I had the same reaction to the other figures as well, although Eevee seems to have fared a little better than the others. They're all sort of cute, but not nearly so cute as they are in other formats. But like I said, Lego is not my personal block of choice, so perhaps I'm in the minority here! If you love these bricky pocket monsters, then roll on over to Lego's website and snap up these kits faster than a Mewtwo. This article originally appeared on Engadget at https://www.engadget.com/entertainment/legos-first-pokemon-sets-are-now-available-for-pre-order-205527102.html?src=rss",
          "content": "We learned last March that Lego and Pokémon would be joining forces and the first results of their partnership are here. Pre-orders for all three kits are open now, with an expected ship date of February 27. As one might have guessed from the lightning bolts on the previous promotional image, one of the debut pocket monsters getting the brick treatment is Pikachu, complete with a Poké Ball. The 2,050-piece kit can be built to show Pikachu either leaping out of the open Poké Ball into battle or at rest staring up at the builder, closed Poké Ball between his paws. The Pikachu kit costs $200. There's also a 587-piece model of Eevee, which goes for $60. On the bigger side is a set featuring a trio of Pokémon: Venusaur, Charizard and Blastoise. This kit has 6,838 pieces and can show the group together in battle formation or separately in their own mini environments. It retails for $650. Lego Before you leap to pre-order pages, however, here's a word of caution. In Lego form, our little friends look…kinda strange? I'm not the biggest Lego builder, but I am a rather accomplished architect in Minecraft, so I am well aware of the innate challenge in constructing a rounded shape from square blocks. Take Pikachu, for instance. Part of his appeal is his chubby little cheeks. There are bricks with more rounded sides in this collection that hint at his usual rotundness, but the proportions of his face just feel a little off to me. I had the same reaction to the other figures as well, although Eevee seems to have fared a little better than the others. They're all sort of cute, but not nearly so cute as they are in other formats. But like I said, Lego is not my personal block of choice, so perhaps I'm in the minority here! If you love these bricky pocket monsters, then roll on over to Lego's website and snap up these kits faster than a Mewtwo. This article originally appeared on Engadget at https://www.engadget.com/entertainment/legos-first-pokemon-sets-are-now-available-for-pre-order-205527102.html?src=rss",
          "feed_position": 24,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2026-01/22428cc0-eff8-11f0-bff5-4310bbcdcba3"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/computing/laptops/dell-revives-its-xps-laptops-after-a-boneheaded-rebranding-001028029.html",
          "published_at": "Mon, 12 Jan 2026 20:25:47 +0000",
          "title": "Dell revives its XPS laptops after a boneheaded rebranding",
          "standfirst": "Last year, Dell killed off all of its PC brands, including the iconic XPS lineup, and replaced them with a simplified naming scheme. It was a move meant to make it easier for people to discern between the company's many brands, but in reality, it just just made the company's lineup even more confusing. We called it an unforced error at the time, but after seeing how much Dell's PC market share fell over 2025, it's fair to say that rebranding was an absolute marketing disaster. So, with its tail between its legs, Dell has returned to CES some welcome news for its fans: XPS lives! And the company plans to double-down on the brand in ways it never did before. Today, Dell revealed the new XPS 14 and 16 notebooks, which feature a more practical design than the previous models. There's a new function row with traditional keys, instead of the odd capacitive buttons that disappeared in sunlight. And while the company is sticking with its \"invisible\" trackpad, which sits flush alongside the wrist rest, there's now a light border around the edges that lets you feel exactly where the trackpad begins and ends.So, in short, Dell seems to have solved most of our recent complaints about the XPS lineup. To signify its commitment to the brand, it's also emblazoning the XPS logo on all of these new machines, replacing the previous Dell name. That’s something I could never imagine a less humbled Dell doing. The redesign also gave Dell room to shave off some weight and thickness from both machines. The XPS 14 weighs around three pounds now, a half-pound lighter than the previous generation, while the XPS 16 weighs 3.6 pounds, a whole pound lighter than before. The new cases make both machines look a lot more like Microsoft’s extra-subtle Surface Laptop, but that’s not necessarily a bad thing. Both systems are powered by Intel’s new Panther Lake Core Ultra Series 3 chips, and they also offer tandem OLED display options.Dell also briefly teased the return of a new XPS 13 later this year, which is set to be the company’s thinnest and lightest notebook ever. Dell says it’ll be cheaper than the XPS has been in the past.The new XPS 14 and 16 will be available on January 6, starting at $2,050 and $2,200, respectively. A Dell representative tells us these aren’t entry-level configurations, instead we can expect to see cheaper prices with lower specs in February.Update 1/6/26, 12:30p: Pricing updated to reflecrt new numbers from Dell. Originally, we were told they would start at $1,650 and $1,850.Update 1/12, 3:00p: Added a mention of lower entry-level configurations coming eventually.This article originally appeared on Engadget at https://www.engadget.com/computing/laptops/dell-revives-its-xps-laptops-after-a-boneheaded-rebranding-001028029.html?src=rss",
          "content": "Last year, Dell killed off all of its PC brands, including the iconic XPS lineup, and replaced them with a simplified naming scheme. It was a move meant to make it easier for people to discern between the company's many brands, but in reality, it just just made the company's lineup even more confusing. We called it an unforced error at the time, but after seeing how much Dell's PC market share fell over 2025, it's fair to say that rebranding was an absolute marketing disaster. So, with its tail between its legs, Dell has returned to CES some welcome news for its fans: XPS lives! And the company plans to double-down on the brand in ways it never did before. Today, Dell revealed the new XPS 14 and 16 notebooks, which feature a more practical design than the previous models. There's a new function row with traditional keys, instead of the odd capacitive buttons that disappeared in sunlight. And while the company is sticking with its \"invisible\" trackpad, which sits flush alongside the wrist rest, there's now a light border around the edges that lets you feel exactly where the trackpad begins and ends.So, in short, Dell seems to have solved most of our recent complaints about the XPS lineup. To signify its commitment to the brand, it's also emblazoning the XPS logo on all of these new machines, replacing the previous Dell name. That’s something I could never imagine a less humbled Dell doing. The redesign also gave Dell room to shave off some weight and thickness from both machines. The XPS 14 weighs around three pounds now, a half-pound lighter than the previous generation, while the XPS 16 weighs 3.6 pounds, a whole pound lighter than before. The new cases make both machines look a lot more like Microsoft’s extra-subtle Surface Laptop, but that’s not necessarily a bad thing. Both systems are powered by Intel’s new Panther Lake Core Ultra Series 3 chips, and they also offer tandem OLED display options.Dell also briefly teased the return of a new XPS 13 later this year, which is set to be the company’s thinnest and lightest notebook ever. Dell says it’ll be cheaper than the XPS has been in the past.The new XPS 14 and 16 will be available on January 6, starting at $2,050 and $2,200, respectively. A Dell representative tells us these aren’t entry-level configurations, instead we can expect to see cheaper prices with lower specs in February.Update 1/6/26, 12:30p: Pricing updated to reflecrt new numbers from Dell. Originally, we were told they would start at $1,650 and $1,850.Update 1/12, 3:00p: Added a mention of lower entry-level configurations coming eventually.This article originally appeared on Engadget at https://www.engadget.com/computing/laptops/dell-revives-its-xps-laptops-after-a-boneheaded-rebranding-001028029.html?src=rss",
          "feed_position": 25
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/computing/ces-2026-proved-the-pc-industry-is-hosed-this-year-174500314.html",
          "published_at": "Mon, 12 Jan 2026 20:25:38 +0000",
          "title": "CES 2026 proved the PC industry is hosed this year",
          "standfirst": "Dell's XPS 14 currently costs over $2,000. An AMD executive predicts that PC builders will likely make piecemeal upgrades this year, instead of building entirely new systems. And new AI supercomputers from NVIDIA and AMD are gobbling up the RAM market. At CES 2026, it was hard not to notice the dire year ahead for the computing industry, one that will likely lead to higher prices and more limited availability for consumer goods across the board.Really, though, the show just confirmed what was apparent since RAM prices skyrocketed over the last few months, driven by demand from AI datacenters. As Samsung's marketing leader, Wonjin Lee, told Bloomberg at CES: \"There's going to be issues around semiconductor supplies, and it's going to affect everyone. Prices are going up even as we speak.\"At first, it appeared that Dell's new XPS 14 and XPS 16 were among the earliest systems hit by these demands. Last year's models started at $1,699 and $1,899, respectively, and we were initially told the new models would actually come in cheaper at $1,650 and $1,850. At the moment, the XPS 14 starts at $2,050, while the XPS 16 is $2,200. A Dell representative tells us these aren’t entry-level configurations, instead we can expect to see cheaper systems below $2,000 in February. While those prices haven’t been finalized, the reps say it should be similar to the earlier figures we were given.It’s also worth noting that it didn't take much to configure the earlier models upwards of $2,000. It’s just unfortunate that Dell doesn’t have cheaper configurations available for the launch if its new systems, especially since they look so compelling. Meanwhile, Apple still hasn't budged its $1,599 MacBook Pro 14-inch pricing. At least Dell still comes in cheaper than the $2,499 MacBook Pro 16-inch.On the desktop front, AMD's David McAfee, Corporate Vice President and GM of Client Channel Business, noted that the longevity of the company's AM4 and AM5 platforms might be a boon for gamers, since they can upgrade their CPUs without buying new RAM kits and motherboards. That allows for a pathway to better performance without paying out the nose for over-priced RAM.\"I think that will be potentially a trend that we see in 2026 with more component upgrades, as opposed to full system swap outs and, and altogether rebuilds,\" he said in a group interview with Engadget and other outlets. \"Some of the most popular CPUs that are still running in gamers’ platforms are parts like the 2600 back to the Pinnacle Ridge days, or 3000 series... Stepping even from there into a little bit more modern 5,000 series processors in an AM4 socket and motherboard, there's a pretty big boost there.\"McAfee added that around 30 to 40 percent of AMD's business still revolves around the AM4 platform, even without the specter of a wild memory market.\"There's no product that has memory in it that's immune to some of these forces around DRAM pricing and, and what it's doing to the market,\" he said, when asked about potential GPU price increases. \"I think the, the truth is the volatility that we've seen over the past two months or so has really been unprecedented.\" Looking ahead, he said he expects prices to settle within the first three to six months of the year, but he didn't discuss his reasoning further. As an aside, he also noted that AMD's X3D chips, which feature 3D V-cache, actually don't see much of a hit from slower RAM. Their high amounts of onboard L2 and L3 cache make up for less ideal memory transfer speeds, McAfee said.That McAfee commented at all about the state of RAM is noteworthy. Every PC maker I’ve asked, including Dell and Acer, refused to comment on the volatile state of the memory industry ahead of CES. Perhaps they were hoping things would calm down before they had to price their new systems. Ultimately, they’re beholden to an increasingly limited supply of RAM. And where is all that memory going? At CES, NVIDIA announced its new Vera Rubin AI supercomputer, which supports up to 54TB of RAM across 36 Vera CPUs and 20.7TB of memory across 72 GPUs. AMD, as well, announced its new Helios AI rack, which supports up to 31TB of memory across 72 AMD Instinct MI455X GPUs. Given the endless appetite for computing to power AI model building and inferencing, there’s likely going to be a significant demand for these beastly systems.Put simply: Our global supply of memory is being sacrificed to appease the AI industry. That’s good news for the likes of OpenAI, Microsoft and NVIDIA, but bad news for anyone who cares about PCs and the consumer products we use every day. Get ready for a year of price hikes. Update 1/12, 3:00p: Added a mention of lower entry-level configurations coming eventually.This article originally appeared on Engadget at https://www.engadget.com/computing/ces-2026-proved-the-pc-industry-is-hosed-this-year-174500314.html?src=rss",
          "content": "Dell's XPS 14 currently costs over $2,000. An AMD executive predicts that PC builders will likely make piecemeal upgrades this year, instead of building entirely new systems. And new AI supercomputers from NVIDIA and AMD are gobbling up the RAM market. At CES 2026, it was hard not to notice the dire year ahead for the computing industry, one that will likely lead to higher prices and more limited availability for consumer goods across the board.Really, though, the show just confirmed what was apparent since RAM prices skyrocketed over the last few months, driven by demand from AI datacenters. As Samsung's marketing leader, Wonjin Lee, told Bloomberg at CES: \"There's going to be issues around semiconductor supplies, and it's going to affect everyone. Prices are going up even as we speak.\"At first, it appeared that Dell's new XPS 14 and XPS 16 were among the earliest systems hit by these demands. Last year's models started at $1,699 and $1,899, respectively, and we were initially told the new models would actually come in cheaper at $1,650 and $1,850. At the moment, the XPS 14 starts at $2,050, while the XPS 16 is $2,200. A Dell representative tells us these aren’t entry-level configurations, instead we can expect to see cheaper systems below $2,000 in February. While those prices haven’t been finalized, the reps say it should be similar to the earlier figures we were given.It’s also worth noting that it didn't take much to configure the earlier models upwards of $2,000. It’s just unfortunate that Dell doesn’t have cheaper configurations available for the launch if its new systems, especially since they look so compelling. Meanwhile, Apple still hasn't budged its $1,599 MacBook Pro 14-inch pricing. At least Dell still comes in cheaper than the $2,499 MacBook Pro 16-inch.On the desktop front, AMD's David McAfee, Corporate Vice President and GM of Client Channel Business, noted that the longevity of the company's AM4 and AM5 platforms might be a boon for gamers, since they can upgrade their CPUs without buying new RAM kits and motherboards. That allows for a pathway to better performance without paying out the nose for over-priced RAM.\"I think that will be potentially a trend that we see in 2026 with more component upgrades, as opposed to full system swap outs and, and altogether rebuilds,\" he said in a group interview with Engadget and other outlets. \"Some of the most popular CPUs that are still running in gamers’ platforms are parts like the 2600 back to the Pinnacle Ridge days, or 3000 series... Stepping even from there into a little bit more modern 5,000 series processors in an AM4 socket and motherboard, there's a pretty big boost there.\"McAfee added that around 30 to 40 percent of AMD's business still revolves around the AM4 platform, even without the specter of a wild memory market.\"There's no product that has memory in it that's immune to some of these forces around DRAM pricing and, and what it's doing to the market,\" he said, when asked about potential GPU price increases. \"I think the, the truth is the volatility that we've seen over the past two months or so has really been unprecedented.\" Looking ahead, he said he expects prices to settle within the first three to six months of the year, but he didn't discuss his reasoning further. As an aside, he also noted that AMD's X3D chips, which feature 3D V-cache, actually don't see much of a hit from slower RAM. Their high amounts of onboard L2 and L3 cache make up for less ideal memory transfer speeds, McAfee said.That McAfee commented at all about the state of RAM is noteworthy. Every PC maker I’ve asked, including Dell and Acer, refused to comment on the volatile state of the memory industry ahead of CES. Perhaps they were hoping things would calm down before they had to price their new systems. Ultimately, they’re beholden to an increasingly limited supply of RAM. And where is all that memory going? At CES, NVIDIA announced its new Vera Rubin AI supercomputer, which supports up to 54TB of RAM across 36 Vera CPUs and 20.7TB of memory across 72 GPUs. AMD, as well, announced its new Helios AI rack, which supports up to 31TB of memory across 72 AMD Instinct MI455X GPUs. Given the endless appetite for computing to power AI model building and inferencing, there’s likely going to be a significant demand for these beastly systems.Put simply: Our global supply of memory is being sacrificed to appease the AI industry. That’s good news for the likes of OpenAI, Microsoft and NVIDIA, but bad news for anyone who cares about PCs and the consumer products we use every day. Get ready for a year of price hikes. Update 1/12, 3:00p: Added a mention of lower entry-level configurations coming eventually.This article originally appeared on Engadget at https://www.engadget.com/computing/ces-2026-proved-the-pc-industry-is-hosed-this-year-174500314.html?src=rss",
          "feed_position": 26
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/orchestration/why-your-llm-bill-is-exploding-and-how-semantic-caching-can-cut-it-by-73",
          "published_at": "Mon, 12 Jan 2026 19:00:00 GMT",
          "title": "Why your LLM bill is exploding — and how semantic caching can cut it by 73%",
          "standfirst": "Our LLM API bill was growing 30% month-over-month. Traffic was increasing, but not that fast. When I analyzed our query logs, I found the real problem: Users ask the same questions in different ways.\"What&#x27;s your return policy?,\" \"How do I return something?\", and \"Can I get a refund?\" were all hitting our LLM separately, generating nearly identical responses, each incurring full API costs.Exact-match caching, the obvious first solution, captured only 18% of these redundant calls. The same semantic question, phrased differently, bypassed the cache entirely.So, I implemented semantic caching based on what queries mean, not how they&#x27;re worded. After implementing it, our cache hit rate increased to 67%, reducing LLM API costs by 73%. But getting there requires solving problems that naive implementations miss.Why exact-match caching falls shortTraditional caching uses query text as the cache key. This works when queries are identical:# Exact-match cachingcache_key = hash(query_text)if cache_key in cache: return cache[cache_key]But users don&#x27;t phrase questions identically. My analysis of 100,000 production queries found:Only 18% were exact duplicates of previous queries47% were semantically similar to previous queries (same intent, different wording)35% were genuinely novel queriesThat 47% represented massive cost savings we were missing. Each semantically-similar query triggered a full LLM call, generating a response nearly identical to one we&#x27;d already computed.Semantic caching architectureSemantic caching replaces text-based keys with embedding-based similarity lookup:class SemanticCache: def __init__(self, embedding_model, similarity_threshold=0.92): self.embedding_model = embedding_model self.threshold = similarity_threshold self.vector_store = VectorStore() # FAISS, Pinecone, etc. self.response_store = ResponseStore() # Redis, DynamoDB, etc. def get(self, query: str) -> Optional[str]: \"\"\"Return cached response if semantically similar query exists.\"\"\" query_embedding = self.embedding_model.encode(query) # Find most similar cached query matches = self.vector_store.search(query_embedding, top_k=1) if matches and matches[0].similarity >= self.threshold: cache_id = matches[0].id return self.response_store.get(cache_id) return None def set(self, query: str, response: str): \"\"\"Cache query-response pair.\"\"\" query_embedding = self.embedding_model.encode(query) cache_id = generate_id() self.vector_store.add(cache_id, query_embedding) self.response_store.set(cache_id, { &#x27;query&#x27;: query, &#x27;response&#x27;: response, &#x27;timestamp&#x27;: datetime.utcnow() })The key insight: Instead of hashing query text, I embed queries into vector space and find cached queries within a similarity threshold.The threshold problemThe similarity threshold is the critical parameter. Set it too high, and you miss valid cache hits. Set it too low, and you return wrong responses.Our initial threshold of 0.85 seemed reasonable; 85% similar should be \"the same question,\" right?Wrong. At 0.85, we got cache hits like:Query: \"How do I cancel my subscription?\"Cached: \"How do I cancel my order?\"Similarity: 0.87These are different questions with different answers. Returning the cached response would be incorrect.I discovered that optimal thresholds vary by query type:Query typeOptimal thresholdRationaleFAQ-style questions0.94High precision needed; wrong answers damage trustProduct searches0.88More tolerance for near-matchesSupport queries0.92Balance between coverage and accuracyTransactional queries0.97Very low tolerance for errorsI implemented query-type-specific thresholds:class AdaptiveSemanticCache: def __init__(self): self.thresholds = { &#x27;faq&#x27;: 0.94, &#x27;search&#x27;: 0.88, &#x27;support&#x27;: 0.92, &#x27;transactional&#x27;: 0.97, &#x27;default&#x27;: 0.92 } self.query_classifier = QueryClassifier() def get_threshold(self, query: str) -> float: query_type = self.query_classifier.classify(query) return self.thresholds.get(query_type, self.thresholds[&#x27;default&#x27;]) def get(self, query: str) -> Optional[str]: threshold = self.get_threshold(query) query_embedding = self.embedding_model.encode(query) matches = self.vector_store.search(query_embedding, top_k=1) if matches and matches[0].similarity >= threshold: return self.response_store.get(matches[0].id) return NoneThreshold tuning methodologyI couldn&#x27;t tune thresholds blindly. I needed ground truth on which query pairs were actually \"the same.\"Our methodology:Step 1: Sample query pairs. I sampled 5,000 query pairs at various similarity levels (0.80-0.99).Step 2: Human labeling. Annotators labeled each pair as \"same intent\" or \"different intent.\" I used three annotators per pair and took a majority vote.Step 3: Compute precision/recall curves. For each threshold, we computed:Precision: Of cache hits, what fraction had the same intent?Recall: Of same-intent pairs, what fraction did we cache-hit?def compute_precision_recall(pairs, labels, threshold): \"\"\"Compute precision and recall at given similarity threshold.\"\"\" predictions = [1 if pair.similarity >= threshold else 0 for pair in pairs] true_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 1) false_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 0) false_negatives = sum(1 for p, l in zip(predictions, labels) if p == 0 and l == 1) precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0 recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0 return precision, recallStep 4: Select threshold based on cost of errors. For FAQ queries where wrong answers damage trust, I optimized for precision (0.94 threshold gave 98% precision). For search queries where missing a cache hit just costs money, I optimized for recall (0.88 threshold).Latency overheadSemantic caching adds latency: You must embed the query and search the vector store before knowing whether to call the LLM.Our measurements:OperationLatency (p50)Latency (p99)Query embedding12ms28msVector search8ms19msTotal cache lookup20ms47msLLM API call850ms2400msThe 20ms overhead is negligible compared to the 850ms LLM call we avoid on cache hits. Even at p99, the 47ms overhead is acceptable.However, cache misses now take 20ms longer than before (embedding + search + LLM call). At our 67% hit rate, the math works out favorably:Before: 100% of queries × 850ms = 850ms averageAfter: (33% × 870ms) + (67% × 20ms) = 287ms + 13ms = 300ms averageNet latency improvement of 65% alongside the cost reduction.Cache invalidationCached responses go stale. Product information changes, policies update and yesterday&#x27;s correct answer becomes today&#x27;s wrong answer.I implemented three invalidation strategies:Time-based TTLSimple expiration based on content type:TTL_BY_CONTENT_TYPE = { &#x27;pricing&#x27;: timedelta(hours=4), # Changes frequently &#x27;policy&#x27;: timedelta(days=7), # Changes rarely &#x27;product_info&#x27;: timedelta(days=1), # Daily refresh &#x27;general_faq&#x27;: timedelta(days=14), # Very stable}Event-based invalidationWhen underlying data changes, invalidate related cache entries:class CacheInvalidator: def on_content_update(self, content_id: str, content_type: str): \"\"\"Invalidate cache entries related to updated content.\"\"\" # Find cached queries that referenced this content affected_queries = self.find_queries_referencing(content_id) for query_id in affected_queries: self.cache.invalidate(query_id) self.log_invalidation(content_id, len(affected_queries))Staleness detectionFor responses that might become stale without explicit events, I implemented periodic freshness checks:def check_freshness(self, cached_response: dict) -> bool: \"\"\"Verify cached response is still valid.\"\"\" # Re-run the query against current data fresh_response = self.generate_response(cached_response[&#x27;query&#x27;]) # Compare semantic similarity of responses cached_embedding = self.embed(cached_response[&#x27;response&#x27;]) fresh_embedding = self.embed(fresh_response) similarity = cosine_similarity(cached_embedding, fresh_embedding) # If responses diverged significantly, invalidate if similarity < 0.90: self.cache.invalidate(cached_response[&#x27;id&#x27;]) return False return TrueWe run freshness checks on a sample of cached entries daily, catching staleness that TTL and event-based invalidation miss.Production resultsAfter three months in production:MetricBeforeAfterChangeCache hit rate18%67%+272%LLM API costs$47K/month$12.7K/month-73%Average latency850ms300ms-65%False-positive rateN/A0.8%—Customer complaints (wrong answers)Baseline+0.3%Minimal increaseThe 0.8% false-positive rate (queries where we returned a cached response that was semantically incorrect) was within acceptable bounds. These cases occurred primarily at the boundaries of our threshold, where similarity was just above the cutoff but intent differed slightly.Pitfalls to avoidDon&#x27;t use a single global threshold. Different query types have different tolerance for errors. Tune thresholds per category.Don&#x27;t skip the embedding step on cache hits. You might be tempted to skip embedding overhead when returning cached responses, but you need the embedding for cache key generation. The overhead is unavoidable.Don&#x27;t forget invalidation. Semantic caching without invalidation strategy leads to stale responses that erode user trust. Build invalidation from day one.Don&#x27;t cache everything. Some queries shouldn&#x27;t be cached: Personalized responses, time-sensitive information, transactional confirmations. Build exclusion rules.def should_cache(self, query: str, response: str) -> bool: \"\"\"Determine if response should be cached.\"\" # Don&#x27;t cache personalized responses if self.contains_personal_info(response): return False # Don&#x27;t cache time-sensitive information if self.is_time_sensitive(query): return False # Don&#x27;t cache transactional confirmations if self.is_transactional(query): return False return TrueKey takeawaysSemantic caching is a practical pattern for LLM cost control that captures redundancy exact-match caching misses. The key challenges are threshold tuning (use query-type-specific thresholds based on precision/recall analysis) and cache invalidation (combine TTL, event-based and staleness detection).At 73% cost reduction, this was our highest-ROI optimization for production LLM systems. The implementation complexity is moderate, but the threshold tuning requires careful attention to avoid quality degradation.Sreenivasa Reddy Hulebeedu Reddy is a lead software engineer.",
          "content": "Our LLM API bill was growing 30% month-over-month. Traffic was increasing, but not that fast. When I analyzed our query logs, I found the real problem: Users ask the same questions in different ways.\"What&#x27;s your return policy?,\" \"How do I return something?\", and \"Can I get a refund?\" were all hitting our LLM separately, generating nearly identical responses, each incurring full API costs.Exact-match caching, the obvious first solution, captured only 18% of these redundant calls. The same semantic question, phrased differently, bypassed the cache entirely.So, I implemented semantic caching based on what queries mean, not how they&#x27;re worded. After implementing it, our cache hit rate increased to 67%, reducing LLM API costs by 73%. But getting there requires solving problems that naive implementations miss.Why exact-match caching falls shortTraditional caching uses query text as the cache key. This works when queries are identical:# Exact-match cachingcache_key = hash(query_text)if cache_key in cache: return cache[cache_key]But users don&#x27;t phrase questions identically. My analysis of 100,000 production queries found:Only 18% were exact duplicates of previous queries47% were semantically similar to previous queries (same intent, different wording)35% were genuinely novel queriesThat 47% represented massive cost savings we were missing. Each semantically-similar query triggered a full LLM call, generating a response nearly identical to one we&#x27;d already computed.Semantic caching architectureSemantic caching replaces text-based keys with embedding-based similarity lookup:class SemanticCache: def __init__(self, embedding_model, similarity_threshold=0.92): self.embedding_model = embedding_model self.threshold = similarity_threshold self.vector_store = VectorStore() # FAISS, Pinecone, etc. self.response_store = ResponseStore() # Redis, DynamoDB, etc. def get(self, query: str) -> Optional[str]: \"\"\"Return cached response if semantically similar query exists.\"\"\" query_embedding = self.embedding_model.encode(query) # Find most similar cached query matches = self.vector_store.search(query_embedding, top_k=1) if matches and matches[0].similarity >= self.threshold: cache_id = matches[0].id return self.response_store.get(cache_id) return None def set(self, query: str, response: str): \"\"\"Cache query-response pair.\"\"\" query_embedding = self.embedding_model.encode(query) cache_id = generate_id() self.vector_store.add(cache_id, query_embedding) self.response_store.set(cache_id, { &#x27;query&#x27;: query, &#x27;response&#x27;: response, &#x27;timestamp&#x27;: datetime.utcnow() })The key insight: Instead of hashing query text, I embed queries into vector space and find cached queries within a similarity threshold.The threshold problemThe similarity threshold is the critical parameter. Set it too high, and you miss valid cache hits. Set it too low, and you return wrong responses.Our initial threshold of 0.85 seemed reasonable; 85% similar should be \"the same question,\" right?Wrong. At 0.85, we got cache hits like:Query: \"How do I cancel my subscription?\"Cached: \"How do I cancel my order?\"Similarity: 0.87These are different questions with different answers. Returning the cached response would be incorrect.I discovered that optimal thresholds vary by query type:Query typeOptimal thresholdRationaleFAQ-style questions0.94High precision needed; wrong answers damage trustProduct searches0.88More tolerance for near-matchesSupport queries0.92Balance between coverage and accuracyTransactional queries0.97Very low tolerance for errorsI implemented query-type-specific thresholds:class AdaptiveSemanticCache: def __init__(self): self.thresholds = { &#x27;faq&#x27;: 0.94, &#x27;search&#x27;: 0.88, &#x27;support&#x27;: 0.92, &#x27;transactional&#x27;: 0.97, &#x27;default&#x27;: 0.92 } self.query_classifier = QueryClassifier() def get_threshold(self, query: str) -> float: query_type = self.query_classifier.classify(query) return self.thresholds.get(query_type, self.thresholds[&#x27;default&#x27;]) def get(self, query: str) -> Optional[str]: threshold = self.get_threshold(query) query_embedding = self.embedding_model.encode(query) matches = self.vector_store.search(query_embedding, top_k=1) if matches and matches[0].similarity >= threshold: return self.response_store.get(matches[0].id) return NoneThreshold tuning methodologyI couldn&#x27;t tune thresholds blindly. I needed ground truth on which query pairs were actually \"the same.\"Our methodology:Step 1: Sample query pairs. I sampled 5,000 query pairs at various similarity levels (0.80-0.99).Step 2: Human labeling. Annotators labeled each pair as \"same intent\" or \"different intent.\" I used three annotators per pair and took a majority vote.Step 3: Compute precision/recall curves. For each threshold, we computed:Precision: Of cache hits, what fraction had the same intent?Recall: Of same-intent pairs, what fraction did we cache-hit?def compute_precision_recall(pairs, labels, threshold): \"\"\"Compute precision and recall at given similarity threshold.\"\"\" predictions = [1 if pair.similarity >= threshold else 0 for pair in pairs] true_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 1) false_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 0) false_negatives = sum(1 for p, l in zip(predictions, labels) if p == 0 and l == 1) precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0 recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0 return precision, recallStep 4: Select threshold based on cost of errors. For FAQ queries where wrong answers damage trust, I optimized for precision (0.94 threshold gave 98% precision). For search queries where missing a cache hit just costs money, I optimized for recall (0.88 threshold).Latency overheadSemantic caching adds latency: You must embed the query and search the vector store before knowing whether to call the LLM.Our measurements:OperationLatency (p50)Latency (p99)Query embedding12ms28msVector search8ms19msTotal cache lookup20ms47msLLM API call850ms2400msThe 20ms overhead is negligible compared to the 850ms LLM call we avoid on cache hits. Even at p99, the 47ms overhead is acceptable.However, cache misses now take 20ms longer than before (embedding + search + LLM call). At our 67% hit rate, the math works out favorably:Before: 100% of queries × 850ms = 850ms averageAfter: (33% × 870ms) + (67% × 20ms) = 287ms + 13ms = 300ms averageNet latency improvement of 65% alongside the cost reduction.Cache invalidationCached responses go stale. Product information changes, policies update and yesterday&#x27;s correct answer becomes today&#x27;s wrong answer.I implemented three invalidation strategies:Time-based TTLSimple expiration based on content type:TTL_BY_CONTENT_TYPE = { &#x27;pricing&#x27;: timedelta(hours=4), # Changes frequently &#x27;policy&#x27;: timedelta(days=7), # Changes rarely &#x27;product_info&#x27;: timedelta(days=1), # Daily refresh &#x27;general_faq&#x27;: timedelta(days=14), # Very stable}Event-based invalidationWhen underlying data changes, invalidate related cache entries:class CacheInvalidator: def on_content_update(self, content_id: str, content_type: str): \"\"\"Invalidate cache entries related to updated content.\"\"\" # Find cached queries that referenced this content affected_queries = self.find_queries_referencing(content_id) for query_id in affected_queries: self.cache.invalidate(query_id) self.log_invalidation(content_id, len(affected_queries))Staleness detectionFor responses that might become stale without explicit events, I implemented periodic freshness checks:def check_freshness(self, cached_response: dict) -> bool: \"\"\"Verify cached response is still valid.\"\"\" # Re-run the query against current data fresh_response = self.generate_response(cached_response[&#x27;query&#x27;]) # Compare semantic similarity of responses cached_embedding = self.embed(cached_response[&#x27;response&#x27;]) fresh_embedding = self.embed(fresh_response) similarity = cosine_similarity(cached_embedding, fresh_embedding) # If responses diverged significantly, invalidate if similarity < 0.90: self.cache.invalidate(cached_response[&#x27;id&#x27;]) return False return TrueWe run freshness checks on a sample of cached entries daily, catching staleness that TTL and event-based invalidation miss.Production resultsAfter three months in production:MetricBeforeAfterChangeCache hit rate18%67%+272%LLM API costs$47K/month$12.7K/month-73%Average latency850ms300ms-65%False-positive rateN/A0.8%—Customer complaints (wrong answers)Baseline+0.3%Minimal increaseThe 0.8% false-positive rate (queries where we returned a cached response that was semantically incorrect) was within acceptable bounds. These cases occurred primarily at the boundaries of our threshold, where similarity was just above the cutoff but intent differed slightly.Pitfalls to avoidDon&#x27;t use a single global threshold. Different query types have different tolerance for errors. Tune thresholds per category.Don&#x27;t skip the embedding step on cache hits. You might be tempted to skip embedding overhead when returning cached responses, but you need the embedding for cache key generation. The overhead is unavoidable.Don&#x27;t forget invalidation. Semantic caching without invalidation strategy leads to stale responses that erode user trust. Build invalidation from day one.Don&#x27;t cache everything. Some queries shouldn&#x27;t be cached: Personalized responses, time-sensitive information, transactional confirmations. Build exclusion rules.def should_cache(self, query: str, response: str) -> bool: \"\"\"Determine if response should be cached.\"\" # Don&#x27;t cache personalized responses if self.contains_personal_info(response): return False # Don&#x27;t cache time-sensitive information if self.is_time_sensitive(query): return False # Don&#x27;t cache transactional confirmations if self.is_transactional(query): return False return TrueKey takeawaysSemantic caching is a practical pattern for LLM cost control that captures redundancy exact-match caching misses. The key challenges are threshold tuning (use query-type-specific thresholds based on precision/recall analysis) and cache invalidation (combine TTL, event-based and staleness detection).At 73% cost reduction, this was our highest-ROI optimization for production LLM systems. The implementation complexity is moderate, but the threshold tuning requires careful attention to avoid quality degradation.Sreenivasa Reddy Hulebeedu Reddy is a lead software engineer.",
          "feed_position": 5,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/7iyQoeSwdOqqpfcE0PFWgF/48db7d0305019eee107028d9f018d2ac/Semantic_caching.png?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/entertainment/paramount-wont-quit-files-suit-against-warner-bros-discovery-over-rejected-bid-175317166.html",
          "published_at": "Mon, 12 Jan 2026 17:53:17 +0000",
          "title": "Paramount won't quit, files suit against Warner Bros. Discovery over rejected bid",
          "standfirst": "Paramount Skydance just does not want to take no for an answer. After having multiple bids to acquire Warner Bros. Discovery (WBD) rejected, including a recent hostile bid that the WBD board recommended that shareholders reject, Paramount is turning to the courts and mounting a proxy fight. In a letter to shareholders on Monday, Paramount CEO David Ellison said the company has filed suit in Delaware Chancery Court seeking more disclosure about WBD’s pending Netflix deal and the process that led to its acceptance. Paramount argues WBD hasn’t provided “basic information” shareholders need to evaluate competing offers, including how WBD valued the planned cable-networks spinout Discovery Global (or Global Networks, depending on the filing). The Netflix acquisition would leave Discovery Global to become its own publicly traded company, while the Paramount offer included these assets. Paramount is also escalating the corporate pressure campaign, with Ellison saying it intends to nominate a slate of directors for election at WBD’s 2026 annual meeting. The end goal would be installing a board that would “engage” on Paramount’s offer under the terms of WBD’s merger agreement with Netflix. If WBD were to call a special meeting to approve the Netflix transaction before the annual meeting, Paramount says it will solicit proxy votes against the deal. It also plans to push a bylaw change requiring shareholders to approve any separation of Discovery Global. This change seems like Paramount stoking the flames (whether real or imagined) surrounding shareholders having their WBD shares bought out without the value of Discovery Global built-in under the Netflix merger. Paramount remains convinced that its offer is \"superior\" to that of Netflix, while WBD maintains Paramount's bid offers \"insufficient value\" and that Paramount has failed to submit a true best proposal \"despite clear direction from WBD on both the deficiencies and potential solutions.\" The lawsuit now aims to force WBD to spell out exactly how it arrived at recommending the Netflix deal over Paramount's bid. WBD expressed concerns over whether a potential Paramount deal would even reach closing, citing the substantial debt the smaller studio would have to take on to pull off a leveraged buyout.This article originally appeared on Engadget at https://www.engadget.com/entertainment/paramount-wont-quit-files-suit-against-warner-bros-discovery-over-rejected-bid-175317166.html?src=rss",
          "content": "Paramount Skydance just does not want to take no for an answer. After having multiple bids to acquire Warner Bros. Discovery (WBD) rejected, including a recent hostile bid that the WBD board recommended that shareholders reject, Paramount is turning to the courts and mounting a proxy fight. In a letter to shareholders on Monday, Paramount CEO David Ellison said the company has filed suit in Delaware Chancery Court seeking more disclosure about WBD’s pending Netflix deal and the process that led to its acceptance. Paramount argues WBD hasn’t provided “basic information” shareholders need to evaluate competing offers, including how WBD valued the planned cable-networks spinout Discovery Global (or Global Networks, depending on the filing). The Netflix acquisition would leave Discovery Global to become its own publicly traded company, while the Paramount offer included these assets. Paramount is also escalating the corporate pressure campaign, with Ellison saying it intends to nominate a slate of directors for election at WBD’s 2026 annual meeting. The end goal would be installing a board that would “engage” on Paramount’s offer under the terms of WBD’s merger agreement with Netflix. If WBD were to call a special meeting to approve the Netflix transaction before the annual meeting, Paramount says it will solicit proxy votes against the deal. It also plans to push a bylaw change requiring shareholders to approve any separation of Discovery Global. This change seems like Paramount stoking the flames (whether real or imagined) surrounding shareholders having their WBD shares bought out without the value of Discovery Global built-in under the Netflix merger. Paramount remains convinced that its offer is \"superior\" to that of Netflix, while WBD maintains Paramount's bid offers \"insufficient value\" and that Paramount has failed to submit a true best proposal \"despite clear direction from WBD on both the deficiencies and potential solutions.\" The lawsuit now aims to force WBD to spell out exactly how it arrived at recommending the Netflix deal over Paramount's bid. WBD expressed concerns over whether a potential Paramount deal would even reach closing, citing the substantial debt the smaller studio would have to take on to pull off a leveraged buyout.This article originally appeared on Engadget at https://www.engadget.com/entertainment/paramount-wont-quit-files-suit-against-warner-bros-discovery-over-rejected-bid-175317166.html?src=rss",
          "feed_position": 28
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/ai/apples-siri-ai-will-be-powered-by-gemini-153636649.html",
          "published_at": "Mon, 12 Jan 2026 17:03:13 +0000",
          "title": "Apple's Siri AI will be powered by Gemini",
          "standfirst": "Apple and Google have confirmed reports that the former will use Google Gemini’s models to help power the new version of Siri and other generative AI features. CNBC first reported the news; Apple and Google subsequently released a joint statement which reads:“Apple and Google have entered into a multi-year collaboration under which the next generation of Apple Foundation Models will be based on Google's Gemini models and cloud technology. These models will help power future Apple Intelligence features, including a more personalized Siri coming this year.After careful evaluation, Apple determined that Google's Al technology provides the most capable foundation for Apple Foundation Models and is excited about the innovative new experiences it will unlock for Apple users. Apple Intelligence will continue to run on Apple devices and Private Cloud Compute, while maintaining Apple's industry-leading privacy standards.”Apple first demoed a genAI version of Siri back at WWDC 2024. In March 2025, the company said it was delaying a major Siri update until this year, but it appears that Apple is not quite ready to publicly release a more capable version of the voice assistant. In June, it was reported that Apple was considering partnerships with OpenAI and Anthropic for Siri (the voice assistant can currently tap ChatGPT for certain queries as part of Apple Intelligence). Two months later, Google emerged as a possible contender. Those rumors intensified in November, when it was reported that Apple might build the new Siri using a custom version of Gemini that runs on its Private Cloud Compute servers — and that it would pay Google around $1 billion a year for the privilege. Update, January 12, 2026, 12:03PM ET: This story has been updated with a full joint statement from Apple and Google.This article originally appeared on Engadget at https://www.engadget.com/ai/apples-siri-ai-will-be-powered-by-gemini-153636649.html?src=rss",
          "content": "Apple and Google have confirmed reports that the former will use Google Gemini’s models to help power the new version of Siri and other generative AI features. CNBC first reported the news; Apple and Google subsequently released a joint statement which reads:“Apple and Google have entered into a multi-year collaboration under which the next generation of Apple Foundation Models will be based on Google's Gemini models and cloud technology. These models will help power future Apple Intelligence features, including a more personalized Siri coming this year.After careful evaluation, Apple determined that Google's Al technology provides the most capable foundation for Apple Foundation Models and is excited about the innovative new experiences it will unlock for Apple users. Apple Intelligence will continue to run on Apple devices and Private Cloud Compute, while maintaining Apple's industry-leading privacy standards.”Apple first demoed a genAI version of Siri back at WWDC 2024. In March 2025, the company said it was delaying a major Siri update until this year, but it appears that Apple is not quite ready to publicly release a more capable version of the voice assistant. In June, it was reported that Apple was considering partnerships with OpenAI and Anthropic for Siri (the voice assistant can currently tap ChatGPT for certain queries as part of Apple Intelligence). Two months later, Google emerged as a possible contender. Those rumors intensified in November, when it was reported that Apple might build the new Siri using a custom version of Gemini that runs on its Private Cloud Compute servers — and that it would pay Google around $1 billion a year for the privilege. Update, January 12, 2026, 12:03PM ET: This story has been updated with a full joint statement from Apple and Google.This article originally appeared on Engadget at https://www.engadget.com/ai/apples-siri-ai-will-be-powered-by-gemini-153636649.html?src=rss",
          "feed_position": 29
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/security/nvidia-rubin-rack-scale-encryption-enterprise-ai-security",
          "published_at": "Mon, 12 Jan 2026 16:00:00 GMT",
          "title": "Nvidia Rubin's rack-scale encryption signals a turning point for enterprise AI security",
          "standfirst": "Nvidia&#x27;s Vera Rubin NVL72, announced at CES 2026, encrypts every bus across 72 GPUs, 36 CPUs, and the entire NVLink fabric. It&#x27;s the first rack-scale platform to deliver confidential computing across CPU, GPU, and NVLink domains.For security leaders, this fundamentally shifts the conversation. Rather than attempting to secure complex hybrid cloud configurations through contractual trust with cloud providers, they can verify them cryptographically. That’s a critical distinction that matters when nation-state adversaries have proven they are capable of launching targeted cyberattacks at machine speed.The brutal economics of unprotected AIEpoch AI research shows frontier training costs have grown at 2.4x annually since 2016, which means billion-dollar training runs could be a reality within a few short years. Yet the infrastructure protecting these investments remains fundamentally insecure in most deployments. Security budgets created to protect frontier training models aren&#x27;t keeping up with the exceptionally fast pace of model training. The result is that more models are under threat as existing approaches can&#x27;t scale and keep up with adversaries&#x27; tradecraft.IBM&#x27;s 2025 Cost of Data Breach Report found that 13% of organizations experienced breaches of AI models or applications. Among those breached, 97% lacked proper AI access controls. Shadow AI incidents cost $4.63 million on average, or $670,000 more than standard breaches, with one in five breaches now involving unsanctioned tools that disproportionately expose customer PII (65%) and intellectual property (40%).Think about what this means for organizations spending $50 million or $500 million on a training run. Their model weights sit in multi-tenant environments where cloud providers can inspect the data. Hardware-level encryption that proves the environment hasn&#x27;t been tampered with changes that financial equation entirely.The GTG-1002 wake-up callIn November 2025, Anthropic disclosed something unprecedented: A Chinese state-sponsored group designated GTG-1002 had manipulated Claude Code to conduct what the company described as the first documented case of a large-scale cyberattack executed without substantial human intervention.State-sponsored adversaries turned it into an autonomous intrusion agent that discovered vulnerabilities, crafted exploits, harvested credentials, moved laterally through networks, and categorized stolen data by intelligence value. Human operators stepped in only at critical junctures. According to Anthropic&#x27;s analysis, the AI executed around 80 to 90% of all tactical work independently.The implications extend beyond this single incident. Attack surfaces that once required teams of experienced attackers can now be probed at machine speed by opponents with access to foundation models.Comparing the performance of Blackwell vs. RubinSpecificationBlackwell GB300 NVL72Rubin NVL72Inference compute (FP4)1.44 exaFLOPS3.6 exaFLOPSNVFP4 per GPU (inference)20 PFLOPS50 PFLOPSPer-GPU NVLink bandwidth1.8 TB/s3.6 TB/sRack NVLink bandwidth130 TB/s260 TB/sHBM bandwidth per GPU~8 TB/s~22 TB/sIndustry momentum and AMD&#x27;s alternativeNvidia isn&#x27;t operating in isolation. Research from the Confidential Computing Consortium and IDC, released in December, found that 75% of organizations are adopting confidential computing, with 18% already in production and 57% piloting deployments.\"Confidential Computing has grown from a niche concept into a vital strategy for data security and trusted AI innovation,\" said Nelly Porter, governing board chair of the Confidential Computing Consortium. Real barriers remain: attestation validation challenges affect 84% of respondents, and a skills gap hampers 75%.AMD&#x27;s Helios rack takes a different approach. Built on Meta&#x27;s Open Rack Wide specification, announced at OCP Global Summit in October 2025, it delivers approximately 2.9 exaflops of FP4 compute with 31 TB of HBM4 memory and 1.4 PB/s aggregate bandwidth. Where Nvidia designs confidential computing into every component, AMD prioritizes open standards through the Ultra Accelerator Link and Ultra Ethernet consortia. The competition between Nvidia and AMD is giving security leaders more of a choice than they otherwise would have had. Comparing the tradeoffs of Nvidia&#x27;s integrated approach versus AMD&#x27;s open-standards flexibility for their specific infrastructures and business-specific threat models is key.What security leaders are doing nowHardware-level confidentiality doesn&#x27;t replace zero-trust principles; it gives them teeth. What Nvidia and AMD are building lets security leaders verify trust cryptographically rather than assume it contractually. That&#x27;s a meaningful shift for anyone running sensitive workloads on shared infrastructure. And if the attestation claims hold up in production, this approach could let enterprises extend zero-trust enforcement across thousands of nodes without the policy sprawl and agent overhead that software-only implementations require.Before deployment: Verify attestation to confirm environments haven&#x27;t been tampered with. Cryptographic proof of compliance should be a prerequisite for signing contracts, not an afterthought or worse, a nice-to-have. If your cloud provider can&#x27;t demonstrate attestation capabilities, that&#x27;s a question worth raising in your next QBR.During operation: Maintain separate enclaves for training and inference, and include security teams in the model pipeline from the very start. IBM&#x27;s research showed 63% of breached organizations had no AI governance policy. You can&#x27;t bolt security on after development; that translates into an onramp for mediocre security design-ins and lengthy red teaming that catches bugs that needed to be engineered out of a model or app early.Across the organization: Run joint exercises between security and data science teams to surface vulnerabilities before attackers find them. Shadow AI accounted for 20% of breaches and exposed customer PII and IP at higher rates than other breach types.Bottom line The GTG-1002 campaign demonstrated that adversaries can now automate large-scale intrusions with minimal human oversight at scale. Nearly every organization that experienced an AI-related breach lacked proper access controls.Nvidia&#x27;s Vera Rubin NVL72 transforms racks from potential liabilities into cryptographically attested assets by encrypting every bus. AMD&#x27;s Helios offers an open-standards alternative. Hardware confidentiality alone won&#x27;t stop a determined adversary, but combined with strong governance and realistic threat exercises, rack-scale encryption gives security leaders the foundation they need to protect investments measured in hundreds of millions of dollars.The question facing CISOs isn&#x27;t whether attested infrastructure is worth it. It&#x27;s whether organizations building high-value AI models can afford to operate without it.",
          "content": "Nvidia&#x27;s Vera Rubin NVL72, announced at CES 2026, encrypts every bus across 72 GPUs, 36 CPUs, and the entire NVLink fabric. It&#x27;s the first rack-scale platform to deliver confidential computing across CPU, GPU, and NVLink domains.For security leaders, this fundamentally shifts the conversation. Rather than attempting to secure complex hybrid cloud configurations through contractual trust with cloud providers, they can verify them cryptographically. That’s a critical distinction that matters when nation-state adversaries have proven they are capable of launching targeted cyberattacks at machine speed.The brutal economics of unprotected AIEpoch AI research shows frontier training costs have grown at 2.4x annually since 2016, which means billion-dollar training runs could be a reality within a few short years. Yet the infrastructure protecting these investments remains fundamentally insecure in most deployments. Security budgets created to protect frontier training models aren&#x27;t keeping up with the exceptionally fast pace of model training. The result is that more models are under threat as existing approaches can&#x27;t scale and keep up with adversaries&#x27; tradecraft.IBM&#x27;s 2025 Cost of Data Breach Report found that 13% of organizations experienced breaches of AI models or applications. Among those breached, 97% lacked proper AI access controls. Shadow AI incidents cost $4.63 million on average, or $670,000 more than standard breaches, with one in five breaches now involving unsanctioned tools that disproportionately expose customer PII (65%) and intellectual property (40%).Think about what this means for organizations spending $50 million or $500 million on a training run. Their model weights sit in multi-tenant environments where cloud providers can inspect the data. Hardware-level encryption that proves the environment hasn&#x27;t been tampered with changes that financial equation entirely.The GTG-1002 wake-up callIn November 2025, Anthropic disclosed something unprecedented: A Chinese state-sponsored group designated GTG-1002 had manipulated Claude Code to conduct what the company described as the first documented case of a large-scale cyberattack executed without substantial human intervention.State-sponsored adversaries turned it into an autonomous intrusion agent that discovered vulnerabilities, crafted exploits, harvested credentials, moved laterally through networks, and categorized stolen data by intelligence value. Human operators stepped in only at critical junctures. According to Anthropic&#x27;s analysis, the AI executed around 80 to 90% of all tactical work independently.The implications extend beyond this single incident. Attack surfaces that once required teams of experienced attackers can now be probed at machine speed by opponents with access to foundation models.Comparing the performance of Blackwell vs. RubinSpecificationBlackwell GB300 NVL72Rubin NVL72Inference compute (FP4)1.44 exaFLOPS3.6 exaFLOPSNVFP4 per GPU (inference)20 PFLOPS50 PFLOPSPer-GPU NVLink bandwidth1.8 TB/s3.6 TB/sRack NVLink bandwidth130 TB/s260 TB/sHBM bandwidth per GPU~8 TB/s~22 TB/sIndustry momentum and AMD&#x27;s alternativeNvidia isn&#x27;t operating in isolation. Research from the Confidential Computing Consortium and IDC, released in December, found that 75% of organizations are adopting confidential computing, with 18% already in production and 57% piloting deployments.\"Confidential Computing has grown from a niche concept into a vital strategy for data security and trusted AI innovation,\" said Nelly Porter, governing board chair of the Confidential Computing Consortium. Real barriers remain: attestation validation challenges affect 84% of respondents, and a skills gap hampers 75%.AMD&#x27;s Helios rack takes a different approach. Built on Meta&#x27;s Open Rack Wide specification, announced at OCP Global Summit in October 2025, it delivers approximately 2.9 exaflops of FP4 compute with 31 TB of HBM4 memory and 1.4 PB/s aggregate bandwidth. Where Nvidia designs confidential computing into every component, AMD prioritizes open standards through the Ultra Accelerator Link and Ultra Ethernet consortia. The competition between Nvidia and AMD is giving security leaders more of a choice than they otherwise would have had. Comparing the tradeoffs of Nvidia&#x27;s integrated approach versus AMD&#x27;s open-standards flexibility for their specific infrastructures and business-specific threat models is key.What security leaders are doing nowHardware-level confidentiality doesn&#x27;t replace zero-trust principles; it gives them teeth. What Nvidia and AMD are building lets security leaders verify trust cryptographically rather than assume it contractually. That&#x27;s a meaningful shift for anyone running sensitive workloads on shared infrastructure. And if the attestation claims hold up in production, this approach could let enterprises extend zero-trust enforcement across thousands of nodes without the policy sprawl and agent overhead that software-only implementations require.Before deployment: Verify attestation to confirm environments haven&#x27;t been tampered with. Cryptographic proof of compliance should be a prerequisite for signing contracts, not an afterthought or worse, a nice-to-have. If your cloud provider can&#x27;t demonstrate attestation capabilities, that&#x27;s a question worth raising in your next QBR.During operation: Maintain separate enclaves for training and inference, and include security teams in the model pipeline from the very start. IBM&#x27;s research showed 63% of breached organizations had no AI governance policy. You can&#x27;t bolt security on after development; that translates into an onramp for mediocre security design-ins and lengthy red teaming that catches bugs that needed to be engineered out of a model or app early.Across the organization: Run joint exercises between security and data science teams to surface vulnerabilities before attackers find them. Shadow AI accounted for 20% of breaches and exposed customer PII and IP at higher rates than other breach types.Bottom line The GTG-1002 campaign demonstrated that adversaries can now automate large-scale intrusions with minimal human oversight at scale. Nearly every organization that experienced an AI-related breach lacked proper access controls.Nvidia&#x27;s Vera Rubin NVL72 transforms racks from potential liabilities into cryptographically attested assets by encrypting every bus. AMD&#x27;s Helios offers an open-standards alternative. Hardware confidentiality alone won&#x27;t stop a determined adversary, but combined with strong governance and realistic threat exercises, rack-scale encryption gives security leaders the foundation they need to protect investments measured in hundreds of millions of dollars.The question facing CISOs isn&#x27;t whether attested infrastructure is worth it. It&#x27;s whether organizations building high-value AI models can afford to operate without it.",
          "feed_position": 6,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/2YSkElI5OHmat9celGziVv/a23ebd445f5a38cd5062055b9dd3b15e/jenson_at_ces.jpg?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/apps/monarch-money-is-offering-50-percent-off-its-budgeting-app-for-new-users-204507767.html",
          "published_at": "Mon, 12 Jan 2026 15:45:37 +0000",
          "title": "Monarch Money is offering 50 percent off its budgeting app for new users",
          "standfirst": "The start of the new year is a great time to get your finances in order, and a good budgeting app can help with that. Instead of laboring over a spreadsheet, you can try one of our favorite budgeting apps for less than usual. Monarch Money is running a sale that gives new users 50 percent off one year of the service, bringing the final cost down to just $50. Just use the code NEWYEAR2026 at checkout to get the discount. Monarch Money makes for a capable and detailed budgeting companion. You can use the service via apps for iOS, Android, iPadOS or the web, and Monarch also offers a Chrome extension that can sync your Amazon and Target transactions and automatically categorize them. Like other budgeting apps, Monarch Money lets you connect multiple financial accounts and track your money based on where you spend it over time. Monarch offers two different approaches to tracking budgeting (flexible and category budgeting) depending on what fits your life best, and the ability to add a budget widget on your phone so you can know how you're tracking that month. How budgeting apps turn your raw transactions into visuals you can understand at a glance is one of the big things that differentiates one app from another, and Monarch Money offers multiple graphs and charts to look at for things like spending, investments or categories of your choice based on how you've labelled your expenses. The app can also monitor the spending of you and your partner all in one place, to make it easier to plan together. The main drawbacks Engadget found in testing Monarch Money were the app's learning curve, and the differences in features (and bugginess) between Monarch's web and mobile versions. Still, for 50 percent off, the Monarch Money is well worth experimenting with if you're trying to save money in 2026, especially if you want to do it collaboratively with a partner. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/apps/monarch-money-is-offering-50-percent-off-its-budgeting-app-for-new-users-204507767.html?src=rss",
          "content": "The start of the new year is a great time to get your finances in order, and a good budgeting app can help with that. Instead of laboring over a spreadsheet, you can try one of our favorite budgeting apps for less than usual. Monarch Money is running a sale that gives new users 50 percent off one year of the service, bringing the final cost down to just $50. Just use the code NEWYEAR2026 at checkout to get the discount. Monarch Money makes for a capable and detailed budgeting companion. You can use the service via apps for iOS, Android, iPadOS or the web, and Monarch also offers a Chrome extension that can sync your Amazon and Target transactions and automatically categorize them. Like other budgeting apps, Monarch Money lets you connect multiple financial accounts and track your money based on where you spend it over time. Monarch offers two different approaches to tracking budgeting (flexible and category budgeting) depending on what fits your life best, and the ability to add a budget widget on your phone so you can know how you're tracking that month. How budgeting apps turn your raw transactions into visuals you can understand at a glance is one of the big things that differentiates one app from another, and Monarch Money offers multiple graphs and charts to look at for things like spending, investments or categories of your choice based on how you've labelled your expenses. The app can also monitor the spending of you and your partner all in one place, to make it easier to plan together. The main drawbacks Engadget found in testing Monarch Money were the app's learning curve, and the differences in features (and bugginess) between Monarch's web and mobile versions. Still, for 50 percent off, the Monarch Money is well worth experimenting with if you're trying to save money in 2026, especially if you want to do it collaboratively with a partner. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/apps/monarch-money-is-offering-50-percent-off-its-budgeting-app-for-new-users-204507767.html?src=rss",
          "feed_position": 32
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/audible-deal-get-three-months-of-access-for-only-3-193859213.html",
          "published_at": "Mon, 12 Jan 2026 14:35:35 +0000",
          "title": "Audible deal: Get three months of access for only $3",
          "standfirst": "One way to read more in the new year is to incorporate audiobooks as part of your reading habit. Audible is having a sale right now that makes that easier and cheaper to do: you can get three months of access for only $1 per month, or a total of $3. The promotion runs through January 21. An Audible subscription grants one audiobook per month to keep. This can be selected from a massive catalog of new releases and bestsellers. The collection here has just about everything. However, it's easy to plow through a single book in a month. Users also get streaming access to thousands of curated titles. Think of it like Netflix for audiobooks. The catalog is limited, but it gets the job done in a pinch. Subscribers do get access to all Audible original content and they will receive discounts on purchasing audiobooks outright. In other words, it's a neat little service and well worth a buck. The regular price is $15, so make sure to cancel at the end of that three months if you aren't enjoying the platform. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/audible-deal-get-three-months-of-access-for-only-3-193859213.html?src=rss",
          "content": "One way to read more in the new year is to incorporate audiobooks as part of your reading habit. Audible is having a sale right now that makes that easier and cheaper to do: you can get three months of access for only $1 per month, or a total of $3. The promotion runs through January 21. An Audible subscription grants one audiobook per month to keep. This can be selected from a massive catalog of new releases and bestsellers. The collection here has just about everything. However, it's easy to plow through a single book in a month. Users also get streaming access to thousands of curated titles. Think of it like Netflix for audiobooks. The catalog is limited, but it gets the job done in a pinch. Subscribers do get access to all Audible original content and they will receive discounts on purchasing audiobooks outright. In other words, it's a neat little service and well worth a buck. The regular price is $15, so make sure to cancel at the end of that three months if you aren't enjoying the platform. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/audible-deal-get-three-months-of-access-for-only-3-193859213.html?src=rss",
          "feed_position": 34
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/apples-mac-mini-m4-is-back-on-sale-for-499-141615231.html",
          "published_at": "Mon, 12 Jan 2026 14:16:16 +0000",
          "title": "Apple's Mac mini M4 is back on sale for $499",
          "standfirst": "The holiday season is fully in the rear view mirror and real life is here to stay. But that doesn't mean the time for gifts is over — especially ones for yourself. You can still take advantage of great January sales on some awesome tech products. Take the Apple Mac mini M4, which is down to $500 from $599. The 17 percent discount gives you 16GB of RAM and 256GB of SSD for only about $20 more than the computer's Black Friday sale. Its beefier models are also on sale: opting for 512GB of SSD will cost you $690, down from $799, while also upping your RAM to 24GB is available for $890, dropping from $999. We gave the Apple Mac mini M4 a 90 in our review thanks in large part to its powerful chip. The M4 works very fast despite being in such a small device. It also offers front-facing headphone and USB-C ports. You can further upgrade to the Apple M4 Pro chip for $1,270, down from $1,399 — a nine percent discount. The Pro model also has Thunderbolt 5 support. Check out our coverage of the best Apple deals for more discounts, and follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/apples-mac-mini-m4-is-back-on-sale-for-499-141615231.html?src=rss",
          "content": "The holiday season is fully in the rear view mirror and real life is here to stay. But that doesn't mean the time for gifts is over — especially ones for yourself. You can still take advantage of great January sales on some awesome tech products. Take the Apple Mac mini M4, which is down to $500 from $599. The 17 percent discount gives you 16GB of RAM and 256GB of SSD for only about $20 more than the computer's Black Friday sale. Its beefier models are also on sale: opting for 512GB of SSD will cost you $690, down from $799, while also upping your RAM to 24GB is available for $890, dropping from $999. We gave the Apple Mac mini M4 a 90 in our review thanks in large part to its powerful chip. The M4 works very fast despite being in such a small device. It also offers front-facing headphone and USB-C ports. You can further upgrade to the Apple M4 Pro chip for $1,270, down from $1,399 — a nine percent discount. The Pro model also has Thunderbolt 5 support. Check out our coverage of the best Apple deals for more discounts, and follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/apples-mac-mini-m4-is-back-on-sale-for-499-141615231.html?src=rss",
          "feed_position": 35
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/entertainment/streaming/netflix-won-seven-awards-at-the-golden-globes-with-adolescence-and-kpop-demon-hunters-140006510.html",
          "published_at": "Mon, 12 Jan 2026 14:00:06 +0000",
          "title": "Netflix won seven awards at the Golden Globes with Adolescence and KPop Demon Hunters",
          "standfirst": "The 2026 Golden Globes took place on Sunday and it was another big night for streamers. Netflix took home seven awards, Apple and HBO Max each won three and Hulu got one. Netflix's hit show Adolescence received four awards alone, including best limited or anthology series. It also won for best actor (Stephen Graham), supporting actor (Owen Cooper) and supporting actress (Erin Doherty) in a miniseries or television film. KPop Demon Hunters — the sensation which became Netflix's most-watched title — won for best animated feature and best original song. \"I just want to say this award goes to people who have had doors closed on them, and I can confidently say rejection is redirection. So never give up. It is never too late to shine like you were born to be,\" singer-songwriter EJAE said in her acceptance speech for the song, Golden. Netflix also won for best performance in stand-up comedy on television for Ricky Gervais: Mortality. Apple TV took home two awards for The Studio: best television series musical or comedy and best performance by a male actor in a television series for Seth Rogen. The streamer also won for best performance by a lead actress in a television series drama thanks to Rhea Seehorn in Pluribus. The Pitt gave HBO Max two of its three awards, with trophies for best television series drama and best performance by a lead actor in a television series drama to Noah Wyle. Jean Smart rounded out the streamer's awards with best performance by a lead actress in a television series musical or comedy for Hacks. Hulu's award came through best performance by a lead actress in a limited or anthology series for Michelle Williams in Dying For Sex. This year also brought a first to the Golden Globes: the best podcast category. Amy Poehler won for Good Hang with Amy Poehler, a podcast that has featured interviews with everyone from Tina Fey to Quinta Brunson since debuting in March last year. Fellow nominees included Alex Cooper's Call Her Daddy and Armchair Expert with Dax Shepard. This article originally appeared on Engadget at https://www.engadget.com/entertainment/streaming/netflix-won-seven-awards-at-the-golden-globes-with-adolescence-and-kpop-demon-hunters-140006510.html?src=rss",
          "content": "The 2026 Golden Globes took place on Sunday and it was another big night for streamers. Netflix took home seven awards, Apple and HBO Max each won three and Hulu got one. Netflix's hit show Adolescence received four awards alone, including best limited or anthology series. It also won for best actor (Stephen Graham), supporting actor (Owen Cooper) and supporting actress (Erin Doherty) in a miniseries or television film. KPop Demon Hunters — the sensation which became Netflix's most-watched title — won for best animated feature and best original song. \"I just want to say this award goes to people who have had doors closed on them, and I can confidently say rejection is redirection. So never give up. It is never too late to shine like you were born to be,\" singer-songwriter EJAE said in her acceptance speech for the song, Golden. Netflix also won for best performance in stand-up comedy on television for Ricky Gervais: Mortality. Apple TV took home two awards for The Studio: best television series musical or comedy and best performance by a male actor in a television series for Seth Rogen. The streamer also won for best performance by a lead actress in a television series drama thanks to Rhea Seehorn in Pluribus. The Pitt gave HBO Max two of its three awards, with trophies for best television series drama and best performance by a lead actor in a television series drama to Noah Wyle. Jean Smart rounded out the streamer's awards with best performance by a lead actress in a television series musical or comedy for Hacks. Hulu's award came through best performance by a lead actress in a limited or anthology series for Michelle Williams in Dying For Sex. This year also brought a first to the Golden Globes: the best podcast category. Amy Poehler won for Good Hang with Amy Poehler, a podcast that has featured interviews with everyone from Tina Fey to Quinta Brunson since debuting in March last year. Fellow nominees included Alex Cooper's Call Her Daddy and Armchair Expert with Dax Shepard. This article originally appeared on Engadget at https://www.engadget.com/entertainment/streaming/netflix-won-seven-awards-at-the-golden-globes-with-adolescence-and-kpop-demon-hunters-140006510.html?src=rss",
          "feed_position": 36
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/get-apples-25w-magsafe-charger-for-only-30-right-now-141707104.html",
          "published_at": "Mon, 12 Jan 2026 13:36:26 +0000",
          "title": "Get Apple's 25W MagSafe charger for only $30 right now",
          "standfirst": "One way you can reduce the number of cables you have to deal with on the regular is by investing in a few wireless chargers. Those with iPhones should consider Apple's own MagSafe charger not only because of its sleek and effective design, but also because it's on sale right now at Amazon. The Qi2.2-rated MagSafe charger is down to $30 for the one-meter version, or $40 for the two-meter version. If you have an iPhone 16, iPhone 17 or iPhone Air, this cable can charge your device at 25W as long as it's connected to a 30W power adapter on the other end. While you'll need a more recent iPhone to get the fastest MagSafe charging speeds, the charger can wirelessly top up the battery of any iPhone from the last eight years (iPhone 8 and later). With older iPhones, the charging speed tops out at 15W. The cable works with AirPods wireless charging cases too — it's certified for Qi2.2 and Qi charging. The MagSafe charger is one of our favorite iPhone accessories, and would pair quite nicely with your new iPhone if you're picking up one of the latest models. If you're on the fence about that, be sure to check out our reviews of the iPhone 17, iPhone Pro/Pro Max and iPhone Air. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/get-apples-25w-magsafe-charger-for-only-30-right-now-141707104.html?src=rss",
          "content": "One way you can reduce the number of cables you have to deal with on the regular is by investing in a few wireless chargers. Those with iPhones should consider Apple's own MagSafe charger not only because of its sleek and effective design, but also because it's on sale right now at Amazon. The Qi2.2-rated MagSafe charger is down to $30 for the one-meter version, or $40 for the two-meter version. If you have an iPhone 16, iPhone 17 or iPhone Air, this cable can charge your device at 25W as long as it's connected to a 30W power adapter on the other end. While you'll need a more recent iPhone to get the fastest MagSafe charging speeds, the charger can wirelessly top up the battery of any iPhone from the last eight years (iPhone 8 and later). With older iPhones, the charging speed tops out at 15W. The cable works with AirPods wireless charging cases too — it's certified for Qi2.2 and Qi charging. The MagSafe charger is one of our favorite iPhone accessories, and would pair quite nicely with your new iPhone if you're picking up one of the latest models. If you're on the fence about that, be sure to check out our reviews of the iPhone 17, iPhone Pro/Pro Max and iPhone Air. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/get-apples-25w-magsafe-charger-for-only-30-right-now-141707104.html?src=rss",
          "feed_position": 37
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/this-elevationlabs-10-year-extended-battery-case-for-airtags-is-on-sale-for-only-16-162308983.html",
          "published_at": "Mon, 12 Jan 2026 13:14:15 +0000",
          "title": "This ElevationLabs 10-year extended battery case for AirTags is on sale for only $16",
          "standfirst": "ElevationLab makes a battery case for your AirTag that can power it for 10 years and the accessory is on sale now for 30 percent off. Normally retailing for $23, you can pick one up for $16. The TimeCapsule case uses two AA batteries to offer up to 14 times the lifespan of the CR2032 battery that powers an AirTag. The company based those estimates on Energizer Ultimate Lithium batteries, so your mileage may vary. Once an AirTag is seated inside the case, which is a compact 4.45 x 1.57 inches, it is sealed shut with four screws at the corners. The case is fiber-reinforced, according to Elevation Lab, and rated IP69 waterproof. The company says it’s intended for use cases where you might place an AirTag for long periods of time, like in a vehicle, a piece of luggage or a work bag. We've already got a couple of Elevation Lab products on our list for best AirTag accessories, so while we haven't reviewed the battery case, we tend to like this company's products. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/this-elevationlabs-10-year-extended-battery-case-for-airtags-is-on-sale-for-only-16-162308983.html?src=rss",
          "content": "ElevationLab makes a battery case for your AirTag that can power it for 10 years and the accessory is on sale now for 30 percent off. Normally retailing for $23, you can pick one up for $16. The TimeCapsule case uses two AA batteries to offer up to 14 times the lifespan of the CR2032 battery that powers an AirTag. The company based those estimates on Energizer Ultimate Lithium batteries, so your mileage may vary. Once an AirTag is seated inside the case, which is a compact 4.45 x 1.57 inches, it is sealed shut with four screws at the corners. The case is fiber-reinforced, according to Elevation Lab, and rated IP69 waterproof. The company says it’s intended for use cases where you might place an AirTag for long periods of time, like in a vehicle, a piece of luggage or a work bag. We've already got a couple of Elevation Lab products on our list for best AirTag accessories, so while we haven't reviewed the battery case, we tend to like this company's products. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/this-elevationlabs-10-year-extended-battery-case-for-airtags-is-on-sale-for-only-16-162308983.html?src=rss",
          "feed_position": 38
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/social-media/meta-closes-550000-accounts-to-comply-with-australias-kids-social-media-ban-130041356.html",
          "published_at": "Mon, 12 Jan 2026 13:00:41 +0000",
          "title": "Meta closes 550,000 accounts to comply with Australia's kids social media ban",
          "standfirst": "To comply with Australia's under-16 social media ban, Meta said on Medium that it has shut down nearly 550,00 accounts. That number includes 330,000 Instagram, 173,000 Facebook and 40,000 Threads accounts deemed to belong to children. \"Ongoing compliance with the law will be a multi-layered process that we will continue to refine, though our concerns about determining age online without an industry standard remain,\" the company wrote. Australia's minimum age social media ban, the first of its kind in the world for a democracy, went into effect on December 10. The ten platforms affected, including Facebook, Instagram, TikTok, Snapchat, X, Reddit and Twitch, must bar underage users or face a fine of up to $AUD 49.5 million ($33 million). Platforms are using a variety of means to determine age, including age inference based on activity and selfies. Some of those platforms aren't taking the ban lying down. Reddit, which launched a lawsuit against the Australian government, argued that it shouldn't have been included in the ban since it isn't a social media site, while adding that it comes with some \"serious privacy and political expression issues\" for users. Meta also expressed its opposition to the ban, citing a number of factors. It says taking social media out of the hands of teens can isolate them from getting support from online communities, and that the ban is only driving them to \"less regulated parts of the internet.\" It also sites inconsistent age verification methods and a lack of interest in compliance from teens and parents. However, the fact that Meta has removed almost 550,000 accounts just a month after the ban took affect shows that it is also affecting the company's bottom line. And Meta doesn't have a sterling record when it comes to teen safety, having previously downplayed the frequency of harm to children. This article originally appeared on Engadget at https://www.engadget.com/social-media/meta-closes-550000-accounts-to-comply-with-australias-kids-social-media-ban-130041356.html?src=rss",
          "content": "To comply with Australia's under-16 social media ban, Meta said on Medium that it has shut down nearly 550,00 accounts. That number includes 330,000 Instagram, 173,000 Facebook and 40,000 Threads accounts deemed to belong to children. \"Ongoing compliance with the law will be a multi-layered process that we will continue to refine, though our concerns about determining age online without an industry standard remain,\" the company wrote. Australia's minimum age social media ban, the first of its kind in the world for a democracy, went into effect on December 10. The ten platforms affected, including Facebook, Instagram, TikTok, Snapchat, X, Reddit and Twitch, must bar underage users or face a fine of up to $AUD 49.5 million ($33 million). Platforms are using a variety of means to determine age, including age inference based on activity and selfies. Some of those platforms aren't taking the ban lying down. Reddit, which launched a lawsuit against the Australian government, argued that it shouldn't have been included in the ban since it isn't a social media site, while adding that it comes with some \"serious privacy and political expression issues\" for users. Meta also expressed its opposition to the ban, citing a number of factors. It says taking social media out of the hands of teens can isolate them from getting support from online communities, and that the ban is only driving them to \"less regulated parts of the internet.\" It also sites inconsistent age verification methods and a lack of interest in compliance from teens and parents. However, the fact that Meta has removed almost 550,000 accounts just a month after the ban took affect shows that it is also affecting the company's bottom line. And Meta doesn't have a sterling record when it comes to teen safety, having previously downplayed the frequency of harm to children. This article originally appeared on Engadget at https://www.engadget.com/social-media/meta-closes-550000-accounts-to-comply-with-australias-kids-social-media-ban-130041356.html?src=rss",
          "feed_position": 39
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/big-tech/uk-regulator-ofcom-opens-a-formal-investigation-into-x-over-csam-scandal-120000312.html",
          "published_at": "Mon, 12 Jan 2026 12:00:00 +0000",
          "title": "UK regulator Ofcom opens a formal investigation into X over CSAM scandal",
          "standfirst": "The UK’s media regulator has opened a formal investigation into X under the Online Safety Act. \"There have been deeply concerning reports of the Grok AI chatbot account on X being used to create and share undressed images of people — which may amount to intimate image abuse or pornography — and sexualized images of children that may amount to child sexual abuse material (CSAM),\" Ofcom said.The investigation will focus on whether X has \"has complied with its duties to protect people in the UK from content that is illegal in the UK.\" That includes whether X is taking appropriate measures to prevent UK users from seeing \"priority\" illegal content, such as CSAM and non-consensual intimate images; if the platform is removing illegal content quickly after becoming aware of it; and whether X carried out an updated risk assessment before making \"any significant changes\" to the platform. The probe will also consider whether X assessed the risk that its platform poses to UK children and if it has ”highly effective age assurance to protect UK children from seeing pornography.”The regulator said it contacted X on January 5 and received a response by its January 9 deadline. Ofcom is conducting an \"expedited assessment of available evidence as a matter of urgency\" and added that it has asked xAI for \"urgent clarification\" on the steps the company is taking to protect UK users.\"Reports of Grok being used to create and share illegal non-consensual intimate images and child sexual abuse material on X have been deeply concerning,\" an Ofcom spokesperson said. \"Platforms must protect people in the UK from content that’s illegal in the UK, and we won’t hesitate to investigate where we suspect companies are failing in their duties, especially where there’s a risk of harm to children. We’ll progress this investigation as a matter of the highest priority, while ensuring we follow due process. As the UK’s independent online safety enforcement agency, it’s important we make sure our investigations are legally robust and fairly decided.\"If Ofcom deems that a company has broken the law, it can \"require platforms to take specific steps to come into compliance or to remedy harm caused by the breach.\" The regulator can additionally impose fines of up to £18 million ($24.3 million) or 10 percent of \"qualifying\" worldwide revenue, whichever of the two figures is higher. It can also seek a court order to stop payment providers or advertisers from working with a platform, or to require internet service providers to block a site in the UK. The UK government has said it would back any action that Ofcom takes against X. Reports over the weekend suggested that the UK had held discussions with allies over a coordinated response to Grok-generated deepfakes. Regulators elsewhere, including in India and the European Union, are also investigating X.Last week, the Grok account on X started telling users that its image generation and editing tools were being limited to paying subscribers. But as of Monday it was still possible for non-paying users to generate images through the Grok tab on the X website and app. Meanwhile, Malaysia and Indonesia became the first countries to block Grok, claiming that X’s chatbot does not have sufficient safeguards in place to prevent explicit AI-generated deepfakes of women and children from being created and disseminated on X. Indonesia temporarily blocked access to Grok on Saturday, as did Malaysia on Sunday, the Associated Press reports. \"The government sees non-consensual sexual deepfakes as a serious violation of human rights, dignity and the safety of citizens in the digital space,\" Indonesia’s Communication and Digital Affairs Minister Meutya Hafid said in a statement. Officials in the country said initial findings showed that Grok lacks effective controls to prevent users from creating and sharing sexually explicit deepfakes based on photos of Indonesian residents. The country's director general of digital space supervision, Alexander Sabar, said generating deepfakes can violate individuals' image and privacy rights when photos are shared or manipulated without consent, adding that they can lead to reputational, social and psychological harm.The Malaysian Communications and Multimedia Commission cited \"repeated misuse\" of Grok to generate explicit and non-consensual deepfakes, some of which involved women and children. The regulator said Grok will remain blocked in the country until X Corp and parent xAI establish strong enough safeguards. This article originally appeared on Engadget at https://www.engadget.com/big-tech/uk-regulator-ofcom-opens-a-formal-investigation-into-x-over-csam-scandal-120000312.html?src=rss",
          "content": "The UK’s media regulator has opened a formal investigation into X under the Online Safety Act. \"There have been deeply concerning reports of the Grok AI chatbot account on X being used to create and share undressed images of people — which may amount to intimate image abuse or pornography — and sexualized images of children that may amount to child sexual abuse material (CSAM),\" Ofcom said.The investigation will focus on whether X has \"has complied with its duties to protect people in the UK from content that is illegal in the UK.\" That includes whether X is taking appropriate measures to prevent UK users from seeing \"priority\" illegal content, such as CSAM and non-consensual intimate images; if the platform is removing illegal content quickly after becoming aware of it; and whether X carried out an updated risk assessment before making \"any significant changes\" to the platform. The probe will also consider whether X assessed the risk that its platform poses to UK children and if it has ”highly effective age assurance to protect UK children from seeing pornography.”The regulator said it contacted X on January 5 and received a response by its January 9 deadline. Ofcom is conducting an \"expedited assessment of available evidence as a matter of urgency\" and added that it has asked xAI for \"urgent clarification\" on the steps the company is taking to protect UK users.\"Reports of Grok being used to create and share illegal non-consensual intimate images and child sexual abuse material on X have been deeply concerning,\" an Ofcom spokesperson said. \"Platforms must protect people in the UK from content that’s illegal in the UK, and we won’t hesitate to investigate where we suspect companies are failing in their duties, especially where there’s a risk of harm to children. We’ll progress this investigation as a matter of the highest priority, while ensuring we follow due process. As the UK’s independent online safety enforcement agency, it’s important we make sure our investigations are legally robust and fairly decided.\"If Ofcom deems that a company has broken the law, it can \"require platforms to take specific steps to come into compliance or to remedy harm caused by the breach.\" The regulator can additionally impose fines of up to £18 million ($24.3 million) or 10 percent of \"qualifying\" worldwide revenue, whichever of the two figures is higher. It can also seek a court order to stop payment providers or advertisers from working with a platform, or to require internet service providers to block a site in the UK. The UK government has said it would back any action that Ofcom takes against X. Reports over the weekend suggested that the UK had held discussions with allies over a coordinated response to Grok-generated deepfakes. Regulators elsewhere, including in India and the European Union, are also investigating X.Last week, the Grok account on X started telling users that its image generation and editing tools were being limited to paying subscribers. But as of Monday it was still possible for non-paying users to generate images through the Grok tab on the X website and app. Meanwhile, Malaysia and Indonesia became the first countries to block Grok, claiming that X’s chatbot does not have sufficient safeguards in place to prevent explicit AI-generated deepfakes of women and children from being created and disseminated on X. Indonesia temporarily blocked access to Grok on Saturday, as did Malaysia on Sunday, the Associated Press reports. \"The government sees non-consensual sexual deepfakes as a serious violation of human rights, dignity and the safety of citizens in the digital space,\" Indonesia’s Communication and Digital Affairs Minister Meutya Hafid said in a statement. Officials in the country said initial findings showed that Grok lacks effective controls to prevent users from creating and sharing sexually explicit deepfakes based on photos of Indonesian residents. The country's director general of digital space supervision, Alexander Sabar, said generating deepfakes can violate individuals' image and privacy rights when photos are shared or manipulated without consent, adding that they can lead to reputational, social and psychological harm.The Malaysian Communications and Multimedia Commission cited \"repeated misuse\" of Grok to generate explicit and non-consensual deepfakes, some of which involved women and children. The regulator said Grok will remain blocked in the country until X Corp and parent xAI establish strong enough safeguards. This article originally appeared on Engadget at https://www.engadget.com/big-tech/uk-regulator-ofcom-opens-a-formal-investigation-into-x-over-csam-scandal-120000312.html?src=rss",
          "feed_position": 40
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/computing/accessories/best-laptop-power-bank-120040388.html",
          "published_at": "Mon, 12 Jan 2026 10:01:26 +0000",
          "title": "The best laptop power banks for 2026",
          "standfirst": "Laptop power banks come in handy if you often travel or work away from your desk. These larger batteries have enough capacity to refill your computer to around 75 percent, giving you many more productive hours. Of course, they can also charge smaller devices like your phone, camera or tablet a few times over. Nearly all portable laptop chargers fall below the 100-watt-hour mark so you can bring them in your carry-on when you fly. Additional features, like built-in cables and digital displays make these battery banks easier to use, too. We tested over a dozen options and put our thoughts below, so you can find the best laptop power bank for your next trip away from an outlet. Table of contents Best laptop power banks for 2026 What to look for in a laptop power bank How we test portable laptop chargers Other laptop power banks we tested Laptop power bank FAQs Recent updates Best laptop power banks for 2026 What to look for in a laptop power bank Flying with a laptop power bank Most portable batteries top out at around 27,000mAh so you can fly with them. The TSA currently limits the capacity carry-on batteries to 100Wh, which works out to around 27,500mAh for 3.6 volt lithium-ion batteries. Note that you’re not allowed to pack any batteries in your checked luggage, regardless of capacity. The TSA rules are intended to limit fire danger — and some airlines are implementing further restrictions due to recent on-board incidents. In March 2025, a Hong Kong flight was grounded after a battery pack caught fire in an overhead bin. A similar situation happened that same year in July on a domestic Delta flight, and again in August on a transatlantic KLM flight. As a result, some airlines, including Emirates, Southwest and others have announced further restrictions on flying with battery packs. Rules include limiting the number of allowed portable chargers and requiring flyers to keep power banks in clear view when using them to recharge a device. If the battery pack isn’t actively in use, however, most rules allow them to stay in your carry-on bag in the overhead bin. Before flying, it’s wise to check your airline’s policies. Capacity If you just need to keep a smartphone from dying before you can make it home, just about any of the best power banks will do. But if you need to revive multiple devices or the substantial battery of a laptop, you’ll want something with a high milliamp-hour​​ (mAh) capacity. A power bank capable of delivering enough power to a laptop will have a capacity between 20,000 and 27,000 mAh. If you want something even bigger than a laptop power bank, and don’t need to fly with it, you’ll likely want to look into portable power stations. These can be the size of a car battery or larger and can potentially fuel an entire weekend away. Another thing to keep in mind is that the capacity listed in a power bank's specs is not what will be delivered to your devices. As I mentioned, the capacity of these banks is around 25,000mAh. Even the huge battery on a 16-inch MacBook Pro or a Dell XPS 16 has a mAh rating of around 5,000 - 6,000mAh, so you might think you’d get five full charges but in reality, you only get about a single 70-percent charge. The voltage is different (typically 3.7V for the power bank and 11.4V for a laptop) which makes the watt-hours, or the amount of energy each battery can hold, different (working out to 92Wh for the battery and 72Wh for the built-in laptop batteries). On top of that, in order to feed a charge from a power bank to a laptop, a voltage conversion takes place and that dissipates a decent amount of energy. Without turning this into a physics lesson, this all means that a power bank with a 25,000mAh (or 92Wh) capacity will typically fill a 5,000mAh (or 72Wh) laptop battery to about 75 percent. In my tests, I averaged about a 60-percent efficiency rate between a power bank’s listed capacity and the actual charge delivered. Ports Every large power bank I’ve tested has at least three USB ports, with a mix of USB-C and USB-A, which should cover nearly any portable device you need to recharge — earbuds, phones, tablets, laptops, you name it. In addition to the different plug formats, some ports supply power at different wattages. For example, one built-in USB-C port might be rated for 60 watts, while the one next to it is rated for 100 watts. So if you’ve got a device that’s capable of 70W fast charging, such as the new MacBook Air, you’d want to opt for the 100W port to get the best charging speeds possible. Note that devices with a smaller wattage draw won’t be negatively affected by connecting to ports with high ratings. For example, a Galaxy S24 Ultra, capable of 45W super fast charging, is perfectly compatible with the 100W port. A device will only draw what it can take, regardless of what a port can supply. Just remember that the port, device and charging cable need to be at or above the desired wattage rating to achieve maximum charging rates. Some of these larger batteries also have AC ports. It might seem like a natural fit to plug in your laptop’s power adapter for a recharge. But really, the AC port should only be for devices that can’t use USB — such as a lamp or a printer. Plugging a power adapter into the AC port only wastes energy through conversion. First, the battery converts its DC power to supply the port with AC power, then the power adapter converts that AC power back to DC so your laptop can take it in. And as you’ll remember from physics class, each time energy is converted, some is lost to heat and other dissipations. Better to cut out the middleman and just send that DC power straight from the battery to the device. Also, you can use more than one port at a time with these devices; just remember that the speed of whatever you’re charging will likely go down, and of course, the battery is going to drain proportionally to what you’re refilling. Wireless charging Since I first started testing portable power banks a few years ago, wireless charging capabilities have noticeably improved. The first few I tried were painfully slow and not worth recommending. Now the wireless pads built into power banks are impressively fast — particularly, in my experience, when charging Samsung Galaxy phones (though the lack of a stabilizing magnetic connection like Apple’s MagSafe means they only work when rested flat on a pad). Most wireless charging connections can be used while other ports are also being employed, making them convenient for some mobile battlestation setups. Of course, wireless charging is always less efficient than wired, and recharging from an external battery is less efficient in general. If you want to waste as little energy as possible, you’re better off sticking to wired connections. Design All power banks are designed to be portable, but there’s a big difference between a pocket-friendly 5,000mAh battery and one of these laptop-compatible bruisers. Most of the latter weigh between a pound and a half to two pounds, which is a considerable addition to a backpack. Many of the options listed here have a display to tell you how much charge remains in the battery, which is helpful when you’re trying to judiciously meet out charges to your devices. If a bank has a wireless connection, the pad is usually on the flat top and any available AC connection is usually at one end. Both may require you to engage those charging methods. Don’t be like me and grumble loudly that you got a bum unit without pressing (and sometimes double pressing) all the buttons first. How we test portable laptop chargers For the past three years, I’ve been testing and using dozens of portable batteries for our other battery guide. Some of those batteries include the higher-capacity power banks you see here. I also got a hold of a few extra banks just for this guide to make sure we covered what’s available. I went for brands I’m already familiar with, as well as battery packs from well-received manufacturers I hadn’t tried before (like UGREEN and Lion Energy). I only considered banks with at least a 20,000mAh capacity and mostly stuck with those that rated 25,000mAh and higher. Here’s everything we tested: Zendure Supertank Pro Mophie Powerstation Pro XL Mophie Powerstation Pro AC Lion Energy Eclipse Mag Lion Energy Trek Baseus Blade Laptop Anker Prime 27,650mAh Goal Zero Sherpa 100 AC Anker Retractable Cable Laptop Bank HyperJuice 245W Anker Prime Power Bank (26K, 300W) UGreen Power Bank 25,000mAh 145W I tested each power bank with an Apple phone (iPhone 15 or 16), an Android phone (Galaxy S23 Ultra), a tablet (M1 iPad Air) and a laptop (16-inch MacBook Pro with the M1 Pro chip). Even though these banks can charge multiple devices at once, I refilled one at a time, to make side-by-side comparisons more straightforward. I drained the batteries of the phones and tablets to between zero and five percent and then didn’t use any device as it refilled. For the MacBook, I let it run down to 10 percent before plugging in the power bank. That's when most laptops give display a “connect to power” warning, as draining any battery to empty will compromise the battery life. I then used it as one might in a mobile office, with a Bluetooth keyboard and mouse, while connected to Wi-Fi and a VPN. For each test, I noted how long a completely charged battery took to get a device back to full and how much of the battery’s capacity was used up in one charge. I also noted things like portability, apparent durability, helpful features and overall design. For reference, here are the battery capacities of the devices I used: iPhone 15: 3,349mAh Galaxy S23 Ultra: 4,855mAh iPad Air (5th gen): 7,729mAh 16-inch M1 Pro MacBook Pro: 27,027mAh Other laptop power banks we tested HyperJuice 245W Hyper’s HyperJuice 245W brick looks great and has a hefty 27,000mAh capacity. The four USB-C ports can combine to output 245W of power and it got my MacBook Pro from nearly dead to 75 percent before depleting itself. When testing it with a Samsung Galaxy S23 Ultra, the handset got back up to a full charge in just over an hour. The screen tells you what each port is doing as well as displaying the amount of charge remaining in the pack itself. But the lack of port variety makes it feel less versatile than other picks on this list — the price is higher than our other options, too. Laptop power bank FAQs How do laptop power banks differ from phone power banks? The main difference is size. Phone power banks tend to have a capacity ranging from 5,000mAh to 20,000mAh and laptop powerbanks are typically rated between 20,000mAh and 27,000mAh. There’s no official definition, however. Laptop batteries are simply larger and need a bigger supply of power to give them a meaningful charge. How do you fast charge a power bank? You can charge a power bank exactly as fast as the power bank’s internal mechanisms will allow. Most batteries are limited in how quickly they can accept and deliver a charge to avoid dangerously overheating. But to make sure you’re charging a bank as quickly as possible, make sure the wall adapter and the USB-C cable you are using have a high wattage rating — using a 5W power brick and a 10W cable will take a lot longer to refill your bank than a 65W wall charger and a 100W cord. What size power bank do I need for a laptop? Look for a power bank with a rating of at least 20,000mAh. Slightly smaller batteries may work, but they won’t deliver a significant charge laptops. How many mAh to charge a laptop? A milliamp hour (mAh) is how much a battery can hold, and most portable batteries list their capacity using mAh. If you get a battery rated at 20,000mAh or above, it should be able to charge your laptop. Using mAh to discuss laptop batteries can be confusing. Due to differing voltages, you can’t directly compare the mAh ratings of a power bank battery to a laptop battery. Using watt-hours is a better gauge, as that calculation takes voltage into account. Recent updates November 2025: Updated our overall top pick to the Anker Laptop Power bank. Added a premium power bank pick. This article originally appeared on Engadget at https://www.engadget.com/computing/accessories/best-laptop-power-bank-120040388.html?src=rss",
          "content": "Laptop power banks come in handy if you often travel or work away from your desk. These larger batteries have enough capacity to refill your computer to around 75 percent, giving you many more productive hours. Of course, they can also charge smaller devices like your phone, camera or tablet a few times over. Nearly all portable laptop chargers fall below the 100-watt-hour mark so you can bring them in your carry-on when you fly. Additional features, like built-in cables and digital displays make these battery banks easier to use, too. We tested over a dozen options and put our thoughts below, so you can find the best laptop power bank for your next trip away from an outlet. Table of contents Best laptop power banks for 2026 What to look for in a laptop power bank How we test portable laptop chargers Other laptop power banks we tested Laptop power bank FAQs Recent updates Best laptop power banks for 2026 What to look for in a laptop power bank Flying with a laptop power bank Most portable batteries top out at around 27,000mAh so you can fly with them. The TSA currently limits the capacity carry-on batteries to 100Wh, which works out to around 27,500mAh for 3.6 volt lithium-ion batteries. Note that you’re not allowed to pack any batteries in your checked luggage, regardless of capacity. The TSA rules are intended to limit fire danger — and some airlines are implementing further restrictions due to recent on-board incidents. In March 2025, a Hong Kong flight was grounded after a battery pack caught fire in an overhead bin. A similar situation happened that same year in July on a domestic Delta flight, and again in August on a transatlantic KLM flight. As a result, some airlines, including Emirates, Southwest and others have announced further restrictions on flying with battery packs. Rules include limiting the number of allowed portable chargers and requiring flyers to keep power banks in clear view when using them to recharge a device. If the battery pack isn’t actively in use, however, most rules allow them to stay in your carry-on bag in the overhead bin. Before flying, it’s wise to check your airline’s policies. Capacity If you just need to keep a smartphone from dying before you can make it home, just about any of the best power banks will do. But if you need to revive multiple devices or the substantial battery of a laptop, you’ll want something with a high milliamp-hour​​ (mAh) capacity. A power bank capable of delivering enough power to a laptop will have a capacity between 20,000 and 27,000 mAh. If you want something even bigger than a laptop power bank, and don’t need to fly with it, you’ll likely want to look into portable power stations. These can be the size of a car battery or larger and can potentially fuel an entire weekend away. Another thing to keep in mind is that the capacity listed in a power bank's specs is not what will be delivered to your devices. As I mentioned, the capacity of these banks is around 25,000mAh. Even the huge battery on a 16-inch MacBook Pro or a Dell XPS 16 has a mAh rating of around 5,000 - 6,000mAh, so you might think you’d get five full charges but in reality, you only get about a single 70-percent charge. The voltage is different (typically 3.7V for the power bank and 11.4V for a laptop) which makes the watt-hours, or the amount of energy each battery can hold, different (working out to 92Wh for the battery and 72Wh for the built-in laptop batteries). On top of that, in order to feed a charge from a power bank to a laptop, a voltage conversion takes place and that dissipates a decent amount of energy. Without turning this into a physics lesson, this all means that a power bank with a 25,000mAh (or 92Wh) capacity will typically fill a 5,000mAh (or 72Wh) laptop battery to about 75 percent. In my tests, I averaged about a 60-percent efficiency rate between a power bank’s listed capacity and the actual charge delivered. Ports Every large power bank I’ve tested has at least three USB ports, with a mix of USB-C and USB-A, which should cover nearly any portable device you need to recharge — earbuds, phones, tablets, laptops, you name it. In addition to the different plug formats, some ports supply power at different wattages. For example, one built-in USB-C port might be rated for 60 watts, while the one next to it is rated for 100 watts. So if you’ve got a device that’s capable of 70W fast charging, such as the new MacBook Air, you’d want to opt for the 100W port to get the best charging speeds possible. Note that devices with a smaller wattage draw won’t be negatively affected by connecting to ports with high ratings. For example, a Galaxy S24 Ultra, capable of 45W super fast charging, is perfectly compatible with the 100W port. A device will only draw what it can take, regardless of what a port can supply. Just remember that the port, device and charging cable need to be at or above the desired wattage rating to achieve maximum charging rates. Some of these larger batteries also have AC ports. It might seem like a natural fit to plug in your laptop’s power adapter for a recharge. But really, the AC port should only be for devices that can’t use USB — such as a lamp or a printer. Plugging a power adapter into the AC port only wastes energy through conversion. First, the battery converts its DC power to supply the port with AC power, then the power adapter converts that AC power back to DC so your laptop can take it in. And as you’ll remember from physics class, each time energy is converted, some is lost to heat and other dissipations. Better to cut out the middleman and just send that DC power straight from the battery to the device. Also, you can use more than one port at a time with these devices; just remember that the speed of whatever you’re charging will likely go down, and of course, the battery is going to drain proportionally to what you’re refilling. Wireless charging Since I first started testing portable power banks a few years ago, wireless charging capabilities have noticeably improved. The first few I tried were painfully slow and not worth recommending. Now the wireless pads built into power banks are impressively fast — particularly, in my experience, when charging Samsung Galaxy phones (though the lack of a stabilizing magnetic connection like Apple’s MagSafe means they only work when rested flat on a pad). Most wireless charging connections can be used while other ports are also being employed, making them convenient for some mobile battlestation setups. Of course, wireless charging is always less efficient than wired, and recharging from an external battery is less efficient in general. If you want to waste as little energy as possible, you’re better off sticking to wired connections. Design All power banks are designed to be portable, but there’s a big difference between a pocket-friendly 5,000mAh battery and one of these laptop-compatible bruisers. Most of the latter weigh between a pound and a half to two pounds, which is a considerable addition to a backpack. Many of the options listed here have a display to tell you how much charge remains in the battery, which is helpful when you’re trying to judiciously meet out charges to your devices. If a bank has a wireless connection, the pad is usually on the flat top and any available AC connection is usually at one end. Both may require you to engage those charging methods. Don’t be like me and grumble loudly that you got a bum unit without pressing (and sometimes double pressing) all the buttons first. How we test portable laptop chargers For the past three years, I’ve been testing and using dozens of portable batteries for our other battery guide. Some of those batteries include the higher-capacity power banks you see here. I also got a hold of a few extra banks just for this guide to make sure we covered what’s available. I went for brands I’m already familiar with, as well as battery packs from well-received manufacturers I hadn’t tried before (like UGREEN and Lion Energy). I only considered banks with at least a 20,000mAh capacity and mostly stuck with those that rated 25,000mAh and higher. Here’s everything we tested: Zendure Supertank Pro Mophie Powerstation Pro XL Mophie Powerstation Pro AC Lion Energy Eclipse Mag Lion Energy Trek Baseus Blade Laptop Anker Prime 27,650mAh Goal Zero Sherpa 100 AC Anker Retractable Cable Laptop Bank HyperJuice 245W Anker Prime Power Bank (26K, 300W) UGreen Power Bank 25,000mAh 145W I tested each power bank with an Apple phone (iPhone 15 or 16), an Android phone (Galaxy S23 Ultra), a tablet (M1 iPad Air) and a laptop (16-inch MacBook Pro with the M1 Pro chip). Even though these banks can charge multiple devices at once, I refilled one at a time, to make side-by-side comparisons more straightforward. I drained the batteries of the phones and tablets to between zero and five percent and then didn’t use any device as it refilled. For the MacBook, I let it run down to 10 percent before plugging in the power bank. That's when most laptops give display a “connect to power” warning, as draining any battery to empty will compromise the battery life. I then used it as one might in a mobile office, with a Bluetooth keyboard and mouse, while connected to Wi-Fi and a VPN. For each test, I noted how long a completely charged battery took to get a device back to full and how much of the battery’s capacity was used up in one charge. I also noted things like portability, apparent durability, helpful features and overall design. For reference, here are the battery capacities of the devices I used: iPhone 15: 3,349mAh Galaxy S23 Ultra: 4,855mAh iPad Air (5th gen): 7,729mAh 16-inch M1 Pro MacBook Pro: 27,027mAh Other laptop power banks we tested HyperJuice 245W Hyper’s HyperJuice 245W brick looks great and has a hefty 27,000mAh capacity. The four USB-C ports can combine to output 245W of power and it got my MacBook Pro from nearly dead to 75 percent before depleting itself. When testing it with a Samsung Galaxy S23 Ultra, the handset got back up to a full charge in just over an hour. The screen tells you what each port is doing as well as displaying the amount of charge remaining in the pack itself. But the lack of port variety makes it feel less versatile than other picks on this list — the price is higher than our other options, too. Laptop power bank FAQs How do laptop power banks differ from phone power banks? The main difference is size. Phone power banks tend to have a capacity ranging from 5,000mAh to 20,000mAh and laptop powerbanks are typically rated between 20,000mAh and 27,000mAh. There’s no official definition, however. Laptop batteries are simply larger and need a bigger supply of power to give them a meaningful charge. How do you fast charge a power bank? You can charge a power bank exactly as fast as the power bank’s internal mechanisms will allow. Most batteries are limited in how quickly they can accept and deliver a charge to avoid dangerously overheating. But to make sure you’re charging a bank as quickly as possible, make sure the wall adapter and the USB-C cable you are using have a high wattage rating — using a 5W power brick and a 10W cable will take a lot longer to refill your bank than a 65W wall charger and a 100W cord. What size power bank do I need for a laptop? Look for a power bank with a rating of at least 20,000mAh. Slightly smaller batteries may work, but they won’t deliver a significant charge laptops. How many mAh to charge a laptop? A milliamp hour (mAh) is how much a battery can hold, and most portable batteries list their capacity using mAh. If you get a battery rated at 20,000mAh or above, it should be able to charge your laptop. Using mAh to discuss laptop batteries can be confusing. Due to differing voltages, you can’t directly compare the mAh ratings of a power bank battery to a laptop battery. Using watt-hours is a better gauge, as that calculation takes voltage into account. Recent updates November 2025: Updated our overall top pick to the Anker Laptop Power bank. Added a premium power bank pick. This article originally appeared on Engadget at https://www.engadget.com/computing/accessories/best-laptop-power-bank-120040388.html?src=rss",
          "feed_position": 41
        }
      ],
      "featured_image": "https://d29szjachogqwa.cloudfront.net/images/user-uploaded/EN_Age_Check_Flow_1_9823.png",
      "popularity_score": 2018.7193741666667
    },
    {
      "id": "cluster_25",
      "coverage": 2,
      "updated_at": "2026-01-13T15:53:10-05:00",
      "title": "Senate passes a bill that would let nonconsensual deepfake victims sue",
      "neutral_headline": "Senate passes a bill that would let nonconsensual deepfake victims sue",
      "items": [
        {
          "source": "The Verge",
          "url": "https://www.theverge.com/news/861531/defiance-act-senate-passage-deepfakes-grok",
          "published_at": "2026-01-13T15:53:10-05:00",
          "title": "Senate passes a bill that would let nonconsensual deepfake victims sue",
          "standfirst": "The Senate passed a bill that could give people who've found their likeness deepfaked into sexually-explicit images without their consent a new way to fight back. The Disrupt Explicit Forged Images and Non-Consensual Edits Act (DEFIANCE Act), would let victims sue the individuals who created the images for civil damages. The bill passed with unanimous [&#8230;]",
          "content": "The Senate passed a bill that could give people who've found their likeness deepfaked into sexually-explicit images without their consent a new way to fight back. The Disrupt Explicit Forged Images and Non-Consensual Edits Act (DEFIANCE Act), would let victims sue the individuals who created the images for civil damages. The bill passed with unanimous consent - meaning there was no roll-call vote, and no Senator objected to its passage on the floor Tuesday. It's meant to build on the work of the Take It Down Act, a law that criminalizes the distribution of nonconsensual intimate images (NCII) and requires social media platforms to promptly … Read the full story at The Verge.",
          "feed_position": 3
        },
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/260113/p38#a260113p38",
          "published_at": "Tue, 13 Jan 2026 13:25:01 -0500",
          "title": "The US Senate passes the Defiance Act that would allow victims to sue over nonconsensual, sexually explicit AI-generated images, amid furor over Grok images (Bloomberg)",
          "standfirst": "Bloomberg: The US Senate passes the Defiance Act that would allow victims to sue over nonconsensual, sexually explicit AI-generated images, amid furor over Grok images &mdash; The US Senate unanimously passed legislation Tuesday that would allow victims to sue over nonconsensual, sexually explicit AI-generated images &hellip;",
          "content": "Bloomberg: The US Senate passes the Defiance Act that would allow victims to sue over nonconsensual, sexually explicit AI-generated images, amid furor over Grok images &mdash; The US Senate unanimously passed legislation Tuesday that would allow victims to sue over nonconsensual, sexually explicit AI-generated images &hellip;",
          "feed_position": 11,
          "image_url": "http://www.techmeme.com/260113/i38.jpg"
        }
      ],
      "featured_image": "http://www.techmeme.com/260113/i38.jpg",
      "popularity_score": 2017.5499297222223
    },
    {
      "id": "cluster_27",
      "coverage": 2,
      "updated_at": "Tue, 13 Jan 2026 15:45:02 -0500",
      "title": "Anthropic shakes up its C-suite; chief product officer Mike Krieger will become the co-lead of its Labs incubator; head of product Ami Vora will become CPO (Hayden Field/The Verge)",
      "neutral_headline": "Anthropic shakes up C-suite to expand its internal incubator",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/260113/p41#a260113p41",
          "published_at": "Tue, 13 Jan 2026 15:45:02 -0500",
          "title": "Anthropic shakes up its C-suite; chief product officer Mike Krieger will become the co-lead of its Labs incubator; head of product Ami Vora will become CPO (Hayden Field/The Verge)",
          "standfirst": "Hayden Field / The Verge: Anthropic shakes up its C-suite; chief product officer Mike Krieger will become the co-lead of its Labs incubator; head of product Ami Vora will become CPO &mdash; &#65279;Chief product officer Mike Krieger will change roles and join the &ldquo;Labs&rdquo; team. &hellip; Mike Krieger, the Instagram co-founder &hellip;",
          "content": "Hayden Field / The Verge: Anthropic shakes up its C-suite; chief product officer Mike Krieger will become the co-lead of its Labs incubator; head of product Ami Vora will become CPO &mdash; &#65279;Chief product officer Mike Krieger will change roles and join the &ldquo;Labs&rdquo; team. &hellip; Mike Krieger, the Instagram co-founder &hellip;",
          "feed_position": 8,
          "image_url": "http://www.techmeme.com/260113/i41.jpg"
        },
        {
          "source": "The Verge",
          "url": "https://www.theverge.com/ai-artificial-intelligence/861475/anthropic-ai-c-suite-internal-incubator-labs-team-mike-krieger",
          "published_at": "2026-01-13T15:30:00-05:00",
          "title": "Anthropic shakes up C-suite to expand its internal incubator",
          "standfirst": "Mike Krieger, the Instagram co-founder who joined Anthropic two years ago as its chief product officer, is moving to a new focus at the AI startup: co-leading its internal incubator, dubbed the \"Labs\" team. The Anthropic Labs team started in mid-2024 with just two members; now, the company has decided to expand it, with a [&#8230;]",
          "content": "Mike Krieger, the Instagram co-founder who joined Anthropic two years ago as its chief product officer, is moving to a new focus at the AI startup: co-leading its internal incubator, dubbed the \"Labs\" team. The Anthropic Labs team started in mid-2024 with just two members; now, the company has decided to expand it, with a focus on building \"experimental products.\" Krieger's new title will be simply a member of technical staff reporting to Anthropic president Daniela Amodei, and he'll co-lead the Labs team with Ben Mann, who has been the company's product engineering lead. Ami Vora, the company's current \"head of product,\" will take over Kr … Read the full story at The Verge.",
          "feed_position": 5
        }
      ],
      "featured_image": "http://www.techmeme.com/260113/i41.jpg",
      "popularity_score": 2017.4143741666667
    },
    {
      "id": "cluster_32",
      "coverage": 2,
      "updated_at": "Tue, 13 Jan 2026 15:10:00 -0500",
      "title": "Roblox's AI-powered age verification system, which expanded globally last week using tech from a company called Persona, faces criticism for misidentifying ages (David Gilbert/Wired)",
      "neutral_headline": "Roblox’s AI-Powered Age Verification Is a Complete Mess",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/260113/p40#a260113p40",
          "published_at": "Tue, 13 Jan 2026 15:10:00 -0500",
          "title": "Roblox's AI-powered age verification system, which expanded globally last week using tech from a company called Persona, faces criticism for misidentifying ages (David Gilbert/Wired)",
          "standfirst": "David Gilbert / Wired: Roblox's AI-powered age verification system, which expanded globally last week using tech from a company called Persona, faces criticism for misidentifying ages &mdash; Kids are being identified as adults&mdash;and vice versa&mdash;on Roblox, while age-verified accounts are already being sold online.",
          "content": "David Gilbert / Wired: Roblox's AI-powered age verification system, which expanded globally last week using tech from a company called Persona, faces criticism for misidentifying ages &mdash; Kids are being identified as adults&mdash;and vice versa&mdash;on Roblox, while age-verified accounts are already being sold online.",
          "feed_position": 9,
          "image_url": "http://www.techmeme.com/260113/i40.jpg"
        },
        {
          "source": "Wired Tech",
          "url": "https://www.wired.com/story/robloxs-ai-powered-age-verification-is-a-complete-mess/",
          "published_at": "Tue, 13 Jan 2026 18:54:50 +0000",
          "title": "Roblox’s AI-Powered Age Verification Is a Complete Mess",
          "standfirst": "Kids are being identified as adults—and vice versa—on Roblox, while age-verified accounts are already being sold online.",
          "content": "Kids are being identified as adults—and vice versa—on Roblox, while age-verified accounts are already being sold online.",
          "feed_position": 2,
          "image_url": "https://media.wired.com/photos/69614fdaf0dd489f2654556d/master/pass/Roblox-Age-Verification-Culture-2247401752.jpg"
        }
      ],
      "featured_image": "http://www.techmeme.com/260113/i40.jpg",
      "popularity_score": 2016.8304852777778
    },
    {
      "id": "cluster_40",
      "coverage": 2,
      "updated_at": "Tue, 13 Jan 2026 19:28:15 +0000",
      "title": "Google’s updated Veo model can make vertical videos from reference images with 4K upscaling",
      "neutral_headline": "Google’s updated Veo model can make vertical videos from...",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/google/2026/01/googles-updated-veo-model-can-make-vertical-videos-from-reference-images-with-4k-upscaling/",
          "published_at": "Tue, 13 Jan 2026 19:28:15 +0000",
          "title": "Google’s updated Veo model can make vertical videos from reference images with 4K upscaling",
          "standfirst": "Google has also added the updated Veo tools to YouTube creator tools.",
          "content": "Google's Veo video AI made stunning leaps in fidelity in 2025, and Google isn't stopping in 2026. The company has announced an update for Veo 3.1 that adds new capabilities when you provide the model with reference material, known as Ingredients to Video. The results should be more consistent, and output supports vertical video and higher-resolution upscaling. With Ingredients to Video, you can provide the AI with up to three images to incorporate into the generated video. You can use that to provide the robot with characters to animate, backgrounds, and material textures. When you do that, the newly upgraded model will allegedly make fewer random alterations, hemming closer to the reference images. You can also generate multiple clips and even prompt for changes to the setting or style while keeping other elements consistent. Veo 3.1 Updates - Bring more creativity and expressiveness into your videos. Google is also expanding its support for mobile-first video in Veo. When using Ingredients to Video, you can now specify outputs in a 9:16 (vertical) ratio. That makes it ideal for posting on social apps like Instagram or TikTok, as well as uploading as a YouTube Short. So get ready for even more phone-centric slop. Google added support for vertical videos via a text prompt last year.Read full article Comments",
          "feed_position": 6,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/Veo-31-1152x648.jpg"
        },
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2026/01/13/googles-update-for-veo-3-1-lets-users-create-vertical-videos-through-reference-images/",
          "published_at": "Tue, 13 Jan 2026 17:00:00 +0000",
          "title": "Google&#8217;s update for Veo 3.1 lets users create vertical videos through reference images",
          "standfirst": "Google is now allowing users to create vertical videos using references image.",
          "content": "Google is now allowing users to create vertical videos using references image.",
          "feed_position": 8
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/Veo-31-1152x648.jpg",
      "popularity_score": 2016.1346519444444
    },
    {
      "id": "cluster_50",
      "coverage": 2,
      "updated_at": "Tue, 13 Jan 2026 18:54:00 +0000",
      "title": "Taiwan issues arrest warrant for Pete Lau, CEO of OnePlus",
      "neutral_headline": "Taiwan issues arrest warrant for Pete Lau, CEO of OnePlus",
      "items": [
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/mobile/smartphones/taiwan-issues-arrest-warrant-for-pete-lau-ceo-of-oneplus-185400997.html",
          "published_at": "Tue, 13 Jan 2026 18:54:00 +0000",
          "title": "Taiwan issues arrest warrant for Pete Lau, CEO of OnePlus",
          "standfirst": "Taiwanese officials have issued an arrest warrant for OnePlus CEO Pete Lau on allegations of illegally employing workers in Taiwan. Two Taiwanese citizens who worked for Lau have also been indicted. The China-based smartphone company has been accused of illegally recruiting more than 70 engineers from Taiwan. Members of the Shilin District Prosecutors Office claim that OnePlus reportedly set up a shell company in Hong Kong with a distinct name, then launched a branch in Taiwan in 2015 without government approval. The branch reportedly worked on research and development for OnePlus mobile phones. Taiwanese officials claim these actions by OnePlus violated the Cross-Strait Act, which is designed as a guide for relations between Taiwan and mainland China. One of the act’s provisions requires Chinese companies to obtain permission from the Taiwanese government to hire workers from Taiwan.This article originally appeared on Engadget at https://www.engadget.com/mobile/smartphones/taiwan-issues-arrest-warrant-for-pete-lau-ceo-of-oneplus-185400997.html?src=rss",
          "content": "Taiwanese officials have issued an arrest warrant for OnePlus CEO Pete Lau on allegations of illegally employing workers in Taiwan. Two Taiwanese citizens who worked for Lau have also been indicted. The China-based smartphone company has been accused of illegally recruiting more than 70 engineers from Taiwan. Members of the Shilin District Prosecutors Office claim that OnePlus reportedly set up a shell company in Hong Kong with a distinct name, then launched a branch in Taiwan in 2015 without government approval. The branch reportedly worked on research and development for OnePlus mobile phones. Taiwanese officials claim these actions by OnePlus violated the Cross-Strait Act, which is designed as a guide for relations between Taiwan and mainland China. One of the act’s provisions requires Chinese companies to obtain permission from the Taiwanese government to hire workers from Taiwan.This article originally appeared on Engadget at https://www.engadget.com/mobile/smartphones/taiwan-issues-arrest-warrant-for-pete-lau-ceo-of-oneplus-185400997.html?src=rss",
          "feed_position": 5
        },
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/260113/p36#a260113p36",
          "published_at": "Tue, 13 Jan 2026 13:05:00 -0500",
          "title": "Taiwan issues an arrest warrant for OnePlus CEO Pete Lau for allegedly illegally hiring 70+ Taiwanese engineers, as it tries to stop China from raiding workers (Li Liu/Bloomberg)",
          "standfirst": "Li Liu / Bloomberg: Taiwan issues an arrest warrant for OnePlus CEO Pete Lau for allegedly illegally hiring 70+ Taiwanese engineers, as it tries to stop China from raiding workers &mdash; Prosecutors in Taiwan issued an arrest warrant for the chief executive officer of the Chinese smartphone company OnePlus &hellip;",
          "content": "Li Liu / Bloomberg: Taiwan issues an arrest warrant for OnePlus CEO Pete Lau for allegedly illegally hiring 70+ Taiwanese engineers, as it tries to stop China from raiding workers &mdash; Prosecutors in Taiwan issued an arrest warrant for the chief executive officer of the Chinese smartphone company OnePlus &hellip;",
          "feed_position": 13,
          "image_url": "http://www.techmeme.com/260113/i36.jpg"
        }
      ],
      "featured_image": "http://www.techmeme.com/260113/i36.jpg",
      "popularity_score": 2015.5638186111112
    },
    {
      "id": "cluster_61",
      "coverage": 2,
      "updated_at": "Tue, 13 Jan 2026 13:15:02 -0500",
      "title": "The US FCC waives a rule that forced Verizon to unlock phones 60 days after they are activated, which could make it harder for people to switch from Verizon (Jon Brodkin/Ars Technica)",
      "neutral_headline": "Verizon to stop automatic unlocking of phones as FCC ends 60-day unlock rule",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/260113/p37#a260113p37",
          "published_at": "Tue, 13 Jan 2026 13:15:02 -0500",
          "title": "The US FCC waives a rule that forced Verizon to unlock phones 60 days after they are activated, which could make it harder for people to switch from Verizon (Jon Brodkin/Ars Technica)",
          "standfirst": "Jon Brodkin / Ars Technica: The US FCC waives a rule that forced Verizon to unlock phones 60 days after they are activated, which could make it harder for people to switch from Verizon &mdash; The Federal Communications Commission is letting Verizon lock phones to its network for longer periods, eliminating a requirement &hellip;",
          "content": "Jon Brodkin / Ars Technica: The US FCC waives a rule that forced Verizon to unlock phones 60 days after they are activated, which could make it harder for people to switch from Verizon &mdash; The Federal Communications Commission is letting Verizon lock phones to its network for longer periods, eliminating a requirement &hellip;",
          "feed_position": 12,
          "image_url": "http://www.techmeme.com/260113/i37.jpg"
        },
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2026/01/fcc-lets-verizon-lock-phones-for-longer-making-it-harder-to-switch-carriers/",
          "published_at": "Mon, 12 Jan 2026 22:07:23 +0000",
          "title": "Verizon to stop automatic unlocking of phones as FCC ends 60-day unlock rule",
          "standfirst": "FCC waives rule that forced Verizon to unlock phones 60 days after activation.",
          "content": "The Federal Communications Commission is letting Verizon lock phones to its network for longer periods, eliminating a requirement to unlock handsets 60 days after they are activated on its network. The change will make it harder for people to switch from Verizon to other carriers. The FCC today granted Verizon's petition for a waiver of the 60-day unlocking requirement. While the waiver is in effect, Verizon only has to comply with the CTIA trade group's voluntary unlocking policy. The CTIA policy calls for unlocking prepaid mobile devices one year after activation, while devices on postpaid plans can be unlocked after a contract, device financing plan, or early termination fee is paid. Unlocking a phone allows it to be used on another carrier's network. While Verizon was previously required to unlock phones automatically after 60 days, the CTIA code says carriers only have to unlock phones \"upon request\" from consumers. The FCC said the Verizon waiver will remain in effect until the agency \"decides on an appropriate industry-wide approach for the unlocking of handsets.\"Read full article Comments",
          "feed_position": 19,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/verizon-jerks-locked-phone-1152x648-1765486982.jpg"
        }
      ],
      "featured_image": "http://www.techmeme.com/260113/i37.jpg",
      "popularity_score": 2014.9143741666667
    },
    {
      "id": "cluster_66",
      "coverage": 2,
      "updated_at": "Tue, 13 Jan 2026 12:45:00 -0500",
      "title": "A look at Confer, an open-source AI assistant project from Signal creator Moxie Marlinspike that is designed to provide end-to-end encryption for AI chats (Dan Goodin/Ars Technica)",
      "neutral_headline": "Signal creator Moxie Marlinspike wants to do for AI what he did for messaging",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/260113/p35#a260113p35",
          "published_at": "Tue, 13 Jan 2026 12:45:00 -0500",
          "title": "A look at Confer, an open-source AI assistant project from Signal creator Moxie Marlinspike that is designed to provide end-to-end encryption for AI chats (Dan Goodin/Ars Technica)",
          "standfirst": "Dan Goodin / Ars Technica: A look at Confer, an open-source AI assistant project from Signal creator Moxie Marlinspike that is designed to provide end-to-end encryption for AI chats &mdash; Moxie Marlinspike&mdash;the pseudonym of an engineer who set a new standard for private messaging with the creation of the Signal Messenger &hellip;",
          "content": "Dan Goodin / Ars Technica: A look at Confer, an open-source AI assistant project from Signal creator Moxie Marlinspike that is designed to provide end-to-end encryption for AI chats &mdash; Moxie Marlinspike&mdash;the pseudonym of an engineer who set a new standard for private messaging with the creation of the Signal Messenger &hellip;",
          "feed_position": 14,
          "image_url": "http://www.techmeme.com/260113/i35.jpg"
        },
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/security/2026/01/signal-creator-moxie-marlinspike-wants-to-do-for-ai-what-he-did-for-messaging/",
          "published_at": "Tue, 13 Jan 2026 12:00:36 +0000",
          "title": "Signal creator Moxie Marlinspike wants to do for AI what he did for messaging",
          "standfirst": "Introducing Confer, an end-to-end AI assistant that just works.",
          "content": "Moxie Marlinspike—the pseudonym of an engineer who set a new standard for private messaging with the creation of the Signal Messenger—is now aiming to revolutionize AI chatbots in a similar way. His latest brainchild is Confer, an open source AI assistant that provides strong assurances that user data is unreadable to the platform operator, hackers, law enforcement, or any other party other than account holders. The service—including its large language models and back-end components—runs entirely on open source software that users can cryptographically verify is in place. Data and conversations originating from users and the resulting responses from the LLMs are encrypted in a trusted execution environment (TEE) that prevents even server administrators from peeking at or tampering with them. Conversations are stored by Confer in the same encrypted form, which uses a key that remains securely on users’ devices.Read full article Comments",
          "feed_position": 14,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/electronic-privacy-invasion-1152x648.jpg"
        }
      ],
      "featured_image": "http://www.techmeme.com/260113/i35.jpg",
      "popularity_score": 2014.413818611111
    },
    {
      "id": "cluster_3",
      "coverage": 1,
      "updated_at": "Tue, 13 Jan 2026 22:34:41 +0000",
      "title": "The RAM shortage’s silver lining: Less talk about “AI PCs”",
      "neutral_headline": "The RAM shortage’s silver lining: Less talk about “AI PCs”",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2026/01/the-ram-shortages-silver-lining-less-talk-about-ai-pcs/",
          "published_at": "Tue, 13 Jan 2026 22:34:41 +0000",
          "title": "The RAM shortage’s silver lining: Less talk about “AI PCs”",
          "standfirst": "“General interest in AI PCs has been wavering for a while ...\"",
          "content": "RAM prices have soared, which is bad news for people interested in buying, building, or upgrading a computer this year, but it's likely good news for people exasperated by talk of so-called AI PCs. As Ars Technica has reported, the growing demands of data centers, fueled by the AI boom, have led to a shortage of RAM and flash memory chips, driving prices to skyrocket. In an announcement today, Ben Yeh, principal analyst at technology research firm Omdia, said that in 2025, “mainstream PC memory and storage costs rose by 40 percent to 70 percent, resulting in cost increases being passed through to customers.”Read full article Comments",
          "feed_position": 0,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-1329130331-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-1329130331-1152x648.jpg",
      "popularity_score": 367.24187416666666
    },
    {
      "id": "cluster_1",
      "coverage": 1,
      "updated_at": "Tue, 13 Jan 2026 23:01:44 +0000",
      "title": "BMW’s first electric M car is coming in 2027—with one motor per wheel",
      "neutral_headline": "BMW’s first electric M car is coming in 2027—with one motor per wheel",
      "items": [
        {
          "source": "Ars Technica Main",
          "url": "https://arstechnica.com/cars/2026/01/bmws-first-electric-m-car-is-coming-in-2027-with-one-motor-per-wheel/",
          "published_at": "Tue, 13 Jan 2026 23:01:44 +0000",
          "title": "BMW’s first electric M car is coming in 2027—with one motor per wheel",
          "standfirst": "Here's what we know about the first BMW EV to wear a proper M badge.",
          "content": "BMW provided flights from Washington, DC, to Malaga, Spain, and accommodation so Ars could drive the iX3 and be briefed on the electric M Neue Klasse. Ars does not accept paid editorial content. Late last year, we drove BMW's new iX3. It's the first of a series of electric BMWs to use a newly developed platform, known as the \"Neue Klasse.\" Later this year, we'll see the first fully electric version of the 3 Series when the i3 sedan debuts. And next year, BMW enthusiasts will finally find out what the brand's M division—which infuses motorsport into the vehicles like few others—can do with an EV. There have been M-tuned EVs before now, more powerful variants of the i4, iX, and i7. And each time we've driven them, BMW has been at pains to point out that these weren't true M cars, not like the M3 or M5. Honestly, they weren't better than the cheaper, less powerful versions, something that won't be allowed for next year's performance EV, which might be called something like the iM3, assuming the naming convention remains logic-based. \"The next generation of models are set to establish a new benchmark in the high-performance vehicle segment,\" says Franciscus van Meel, managing director of BMW M GmbH. \"With the latest generation of Neue Klasse technology, we are taking the BMW M driving experience to a new level and will inspire our customers with outstanding, racetrack-ready driving dynamics for everyday use.\"Read full article Comments",
          "feed_position": 0,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/BMW-M-Electrified-Arjeplog-_046-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/BMW-M-Electrified-Arjeplog-_046-1152x648.jpg",
      "popularity_score": 358.6927075
    },
    {
      "id": "cluster_8",
      "coverage": 1,
      "updated_at": "Tue, 13 Jan 2026 22:07:21 +0000",
      "title": "Never-before-seen Linux malware is “far more advanced than typical”",
      "neutral_headline": "Never-before-seen Linux malware is “far more advanced than typical”",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/security/2026/01/never-before-seen-linux-malware-is-far-more-advanced-than-typical/",
          "published_at": "Tue, 13 Jan 2026 22:07:21 +0000",
          "title": "Never-before-seen Linux malware is “far more advanced than typical”",
          "standfirst": "VoidLink includes an unusually broad and advanced array of capabilities.",
          "content": "Researchers have discovered a never-before-seen framework that infects Linux machines with a wide assortment of modules that are notable for the range of advanced capabilities they provide to attackers. The framework, referred to as VoidLink by its source code, features more than 30 modules that can be used to customize capabilities to meet attackers' needs for each infected machine. These modules can provide additional stealth and specific tools for reconnaissance, privilege escalation, and lateral movement inside a compromised network. The components can be easily added or removed as objectives change over the course of a campaign. A focus on Linux inside the cloud VoidLink can target machines within popular cloud services by detecting if an infected machine is hosted inside AWS, GCP, Azure, Alibaba, and Tencent, and there are indications that developers plan to add detections for Huawei, DigitalOcean, and Vultr in future releases. To detect which cloud service hosts the machine, VoidLink examines metadata using the respective vendor’s API.Read full article Comments",
          "feed_position": 1,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2023/07/exploit-vulnerability-security.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2023/07/exploit-vulnerability-security.jpg",
      "popularity_score": 347.7863186111111
    },
    {
      "id": "cluster_13",
      "coverage": 1,
      "updated_at": "Tue, 13 Jan 2026 21:55:01 +0000",
      "title": "Lawsuit: DHS wants “unlimited subpoena authority” to unmask ICE critics",
      "neutral_headline": "Lawsuit: DHS wants “unlimited subpoena authority” to unmask ICE critics",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2026/01/instagram-user-fights-dhs-for-the-right-to-post-ice-sightings-anonymously/",
          "published_at": "Tue, 13 Jan 2026 21:55:01 +0000",
          "title": "Lawsuit: DHS wants “unlimited subpoena authority” to unmask ICE critics",
          "standfirst": "DHS is weirdly using import/export rules to expand its authority to identify online critics.",
          "content": "The US Department of Homeland Security (DHS) is fighting to unmask the owner of Facebook and Instagram accounts of a community watch group monitoring Immigration and Customs Enforcement (ICE) activity in Pennsylvania. Defending the right to post about ICE sightings anonymously is a Meta account holder for MontCo Community Watch, John Doe. Doe has alleged that when the DHS sent a \"summons\" to Meta asking for subscriber information, it infringed on core First Amendment-protected activity, i.e., the right to publish content critical of government agencies and officials without fear of government retaliation. He also accused DHS of ignoring federal rules and seeking to vastly expand its authority to subpoena information to unmask ICE's biggest critics online.Read full article Comments",
          "feed_position": 2,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2255126862-1024x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2255126862-1024x648.jpg",
      "popularity_score": 331.58076305555556
    },
    {
      "id": "cluster_33",
      "coverage": 1,
      "updated_at": "Tue, 13 Jan 2026 20:05:14 +0000",
      "title": "Microsoft vows to cover full power costs for energy-hungry AI data centers",
      "neutral_headline": "Microsoft vows to cover full power costs for energy-hungry AI data centers",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/ai/2026/01/microsoft-vows-to-cover-full-power-costs-for-energy-hungry-ai-data-centers/",
          "published_at": "Tue, 13 Jan 2026 20:05:14 +0000",
          "title": "Microsoft vows to cover full power costs for energy-hungry AI data centers",
          "standfirst": "Company responds to community concerns over electricity bills and water use.",
          "content": "On Tuesday, Microsoft announced a new initiative called \"Community-First AI Infrastructure\" that commits the company to paying full electricity costs for its data centers and refusing to seek local property tax reductions. As demand for generative AI services has increased over the past year, Big Tech companies have been racing to spin up massive new data centers for serving chatbots and image generators that can have profound economic effects on the surrounding areas where they are located. Among other concerns, communities across the country have grown concerned that data centers are driving up residential electricity rates through heavy power consumption and by straining water supplies due to server cooling needs. The International Energy Agency (IEA) projects that global data center electricity demand will more than double by 2030, reaching around 945 TWh, with the United States responsible for nearly half of total electricity demand growth over that period. This growth is happening while much of the country's electricity transmission infrastructure is more than 40 years old and under strain.Read full article Comments",
          "feed_position": 5,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/microsoft_datacenter_2025-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/microsoft_datacenter_2025-1152x648.jpg",
      "popularity_score": 326.7510408333333
    },
    {
      "id": "cluster_23",
      "coverage": 1,
      "updated_at": "Tue, 13 Jan 2026 21:13:07 +0000",
      "title": "Hegseth wants to integrate Musk’s Grok AI into military networks this month",
      "neutral_headline": "Hegseth wants to integrate Musk’s Grok AI into military networks this month",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/ai/2026/01/hegseth-wants-to-integrate-musks-grok-ai-into-military-networks-this-month/",
          "published_at": "Tue, 13 Jan 2026 21:13:07 +0000",
          "title": "Hegseth wants to integrate Musk’s Grok AI into military networks this month",
          "standfirst": "US defense secretary announces plans for integration despite recent controversies.",
          "content": "On Monday, US Defense Secretary Pete Hegseth said he plans to integrate Elon Musk's AI tool, Grok, into Pentagon networks later this month. During remarks at the SpaceX headquarters in Texas reported by The Guardian, Hegseth said the integration would place \"the world's leading AI models on every unclassified and classified network throughout our department.\" The announcement comes weeks after Grok drew international backlash for generating sexualized images of women and children, although the Department of Defense has not released official documentation confirming Hegseth's announced timeline or implementation details. During the same appearance, Hegseth rolled out what he called an \"AI acceleration strategy\" for the Department of Defense. The strategy, he said, will \"unleash experimentation, eliminate bureaucratic barriers, focus on investments, and demonstrate the execution approach needed to ensure we lead in military AI and that it grows more dominant into the future.\"Read full article Comments",
          "feed_position": 4,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/07/grok_header_1-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/07/grok_header_1-1152x648.jpg",
      "popularity_score": 325.88242972222224
    },
    {
      "id": "cluster_20",
      "coverage": 1,
      "updated_at": "Tue, 13 Jan 2026 21:25:49 +0000",
      "title": "FDA deletes warning on bogus autism therapies touted by RFK Jr.‘s allies",
      "neutral_headline": "FDA deletes warning on bogus autism therapies touted by RFK Jr.‘s allies",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/health/2026/01/warning-about-bogus-autism-treatments-vanishes-from-fdas-website/",
          "published_at": "Tue, 13 Jan 2026 21:25:49 +0000",
          "title": "FDA deletes warning on bogus autism therapies touted by RFK Jr.‘s allies",
          "standfirst": "The agency used to warn of chelation, used by RFK Jr.'s anti-vaccine ally David Geier.",
          "content": "For years, the Food and Drug Administration provided an informational webpage for parents warning them of the dangers of bogus autism treatments, some promoted by anti-vaccine activists and \"wellness\" companies. The page cited specifics scams and the \"significant health risks\" they pose. But, under anti-vaccine Health Secretary Robert F. Kennedy Jr.—who has numerous ties to the wellness industry—that FDA information webpage is now gone. It was quietly deleted at the end of last year, the Department of Health and Human Services confirmed to Ars Technica. The defunct webpage, titled \"Be Aware of Potentially Dangerous Products and Therapies that Claim to Treat Autism,\" provided parents and other consumers with an overview of the problem. It began with a short description of autism and some evidence-based, FDA-approved medications that can help manage autism symptoms. Then, the regulatory agency provided a list of some false claims and unproven, potentially dangerous treatments it had been working to combat. \"Some of these so-called therapies carry significant health risks,\" the FDA wrote.Read full article Comments",
          "feed_position": 3,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2255115344-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2255115344-1152x648.jpg",
      "popularity_score": 321.0940963888889
    },
    {
      "id": "cluster_41",
      "coverage": 1,
      "updated_at": "Tue, 13 Jan 2026 19:18:35 +0000",
      "title": "Starlink tries to stay online in Iran as regime jams signals during protests",
      "neutral_headline": "Starlink tries to stay online in Iran as regime jams signals during protests",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2026/01/starlink-tries-to-stay-online-in-iran-as-regime-jams-signals-during-protests/",
          "published_at": "Tue, 13 Jan 2026 19:18:35 +0000",
          "title": "Starlink tries to stay online in Iran as regime jams signals during protests",
          "standfirst": "Iran shut off Internet as it cracks down on protests; even Starlink has problems.",
          "content": "President Trump asked Elon Musk to get Starlink working more reliably in Iran to thwart the Iranian government's Internet shutdown. Starlink operator SpaceX was apparently already working on the problem before Trump reached out to Musk. Iran severed Internet connections and phone lines last week as the government conducted a violent crackdown on anti-government demonstrators, according to numerous reports, which say that thousands of people have been killed. Starlink hasn't been completely disabled. The government's jamming technology has reportedly caused Starlink packet loss of anywhere from 30 to 80 percent.Read full article Comments",
          "feed_position": 7,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/starlink-iran-1152x648-1768330630.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/starlink-iran-1152x648-1768330630.jpg",
      "popularity_score": 283.97354083333335
    },
    {
      "id": "cluster_49",
      "coverage": 1,
      "updated_at": "Tue, 13 Jan 2026 18:56:28 +0000",
      "title": "EPA moves to stop considering economic benefits of cleaner air",
      "neutral_headline": "EPA moves to stop considering economic benefits of cleaner air",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2026/01/epa-axes-benefits-from-cost-benefit-analysis-for-air-pollution-limits/",
          "published_at": "Tue, 13 Jan 2026 18:56:28 +0000",
          "title": "EPA moves to stop considering economic benefits of cleaner air",
          "standfirst": "New language criticizes “uncertainties” in longstanding EPA practice.",
          "content": "If you were to do a cost-benefit analysis of your lunch, it would be pretty difficult to do the calculation without the sandwich. But it appears that the US Environmental Protection Agency (EPA) is moving in this same direction—removing the benefit—when it comes to air pollution regulations. According to a New York Times report based on internal emails and documents—and demonstrated by a recently produced analysis on the EPA website—the EPA is changing its cost-benefit analysis process for common air pollutants. Instead of comparing the economic cost of a certain pollution limit to an estimate of the economic value of the resulting improvements in human health, the EPA will just qualitatively describe health benefits while carefully quantifying economic costs. Cost-benefit analysis has been a key component of EPA regulations. Any decision to raise or lower air quality standards or pollution limits includes evaluations of the cost that change, like the addition of new pollution control equipment at power plants, would incur, for example.Read full article Comments",
          "feed_position": 8,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2182255823-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2182255823-1152x648.jpg",
      "popularity_score": 283.6049297222222
    },
    {
      "id": "cluster_59",
      "coverage": 1,
      "updated_at": "Tue, 13 Jan 2026 18:24:03 +0000",
      "title": "Apple’s Mac and iPad creative apps get bundled into “Creator Studio” subscription",
      "neutral_headline": "Apple’s Mac and iPad creative apps get bundled into “Creator Studio” subscription",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2026/01/apple-creator-studio-bundles-final-cut-pro-logic-and-other-apps-for-129-year/",
          "published_at": "Tue, 13 Jan 2026 18:24:03 +0000",
          "title": "Apple’s Mac and iPad creative apps get bundled into “Creator Studio” subscription",
          "standfirst": "Video, audio, and image editing apps join up in one subscription on January 28.",
          "content": "Apple's professional creative apps have been slower to jump on the subscription bandwagon than those from Adobe or some of its other competitors, but the company is taking a step in that direction today. Starting on January 28, Apple will offer an Apple Creator Studio subscription for $13 a month, or $130 a year. Subscribers will get access to the Mac and (where applicable) iPad versions of Final Cut Pro, Logic Pro, Pixelmator Pro, Motion, Compressor, and MainStage, as well as \"intelligent features and premium content\" for the Mac, iPad, and iPhone versions of Keynote, Pages, Numbers, and Freeform. Apple says it will also offer a one-month free trial for the subscription and a discounted version for students at $3 a month, or $30 a year. Most of the apps also seem to be getting small feature updates to go along with the Creator Studio announcement. Final Cut will get a new Transcript Search feature that will allow you to dig through video footage by searching for specific dialogue, and a new Montage Maker feature \"will analyze and edit together a dynamic video based on the best visual moments within the footage.\" An updated Logic Pro \"helps creators deliver original music for their video content\" and adds a synth player to the app's lineup of \"AI Session Players.\"Read full article Comments",
          "feed_position": 9,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/Apple-Creator-Studio-lifestyle-Final-Cut-Pro-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/Apple-Creator-Studio-lifestyle-Final-Cut-Pro-1152x648.jpg",
      "popularity_score": 270.06465194444445
    },
    {
      "id": "cluster_69",
      "coverage": 1,
      "updated_at": "Tue, 13 Jan 2026 17:11:51 +0000",
      "title": "This one could use less power: The Jeep Wagoneer S EV",
      "neutral_headline": "This one could use less power: The Jeep Wagoneer S EV",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/cars/2026/01/this-one-could-use-less-power-the-jeep-wagoneer-s-ev/",
          "published_at": "Tue, 13 Jan 2026 17:11:51 +0000",
          "title": "This one could use less power: The Jeep Wagoneer S EV",
          "standfirst": "Poorly calibrated pedal mapping marred the Wagoneer S experience.",
          "content": "It's not really accurate to call the Wagoneer S Jeep's first electric vehicle. For several years now, Europeans have been able to buy the Jeep Avenger, a subcompact crossover that will surely never see American roads. But it is the first electric Jeep designed for American consumption. It's aimed at the highly competitive midsize SUV segment, which gets ever more crowded even as electrification faces a less certain future here. Indeed, the brand, along with its Stellantis sibling Chrysler, just shelved all its plug-in hybrids, discontinuing them just a few days ago. Like the little Avenger, the Wagoneer S makes use of one of parent company Stellantis' purpose-built EV platforms, one shared with the growly-sounding Dodge Charger. At 192.4 inches (4,886 mm) long, 74.8 inches (1,900 mm) wide, and 64.8 inches (1,645 mm) tall, it's a little larger than cars like the BMW iX3 or Audi Q6 e-tron but a little smaller than domestically designed rivals like the Cadillac Lyriq and Acura ZDX, which have particularly long wheelbases. I find it a rather handsome car, one that has to marry Jeep's Wagoneer styling cues with as many wind-smoothing and air-shaping elements as possible. The way the rear wing juts out above the tailgate window reminds me of a '90s rally hatchback, but it's the product of the designers and the engineers working on drag reduction. The overall drag coefficient is 0.29, and since Jeep actually publishes the frontal area, too, I can tell you the more important CdA number—where drag is multiplied by the frontal area—is 8.67 sq ft (0.805 m2).Read full article Comments",
          "feed_position": 11,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/WS025_007WS-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/WS025_007WS-1152x648.jpg",
      "popularity_score": 152.8613186111111
    },
    {
      "id": "cluster_73",
      "coverage": 1,
      "updated_at": "Tue, 13 Jan 2026 16:35:52 +0000",
      "title": "A new Titan emerges in Monarch: Legacy of Monsters S2 teaser",
      "neutral_headline": "A new Titan emerges in Monarch: Legacy of Monsters S2 teaser",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/culture/2026/01/a-new-titan-emerges-in-monarch-legacy-of-monsters-s2-teaser/",
          "published_at": "Tue, 13 Jan 2026 16:35:52 +0000",
          "title": "A new Titan emerges in Monarch: Legacy of Monsters S2 teaser",
          "standfirst": "Apple TV's sci-fi series is part of Legendary Entertainment’s MonsterVerse and is set after 2014's Godzilla.",
          "content": "It's looking to be a solid year for kaiju fans. Not only are we getting Godzilla Minus Zero in November—sequel to the critically acclaimed Godzilla Minus One (2023)—but Apple TV just released a teaser for the second season of Monarch: Legacy of Monsters, part of Legendary Entertainment’s MonsterVerse, which brought Godzilla, King Kong, and various other monsters (kaiju) created by Toho Co., Ltd into the same fold. (Spoilers for S1 below.) The first season picked up where 2014's Godzilla left off, specifically the introduction of Project Monarch, a secret organization established in the 1950s to study Godzilla and other kaiju—after attempts to kill Godzilla with nuclear weapons failed. The plot spans three generations and takes place in the 1950s and half a century later. In the first season, two siblings (Kate and Kentaro Randa) follow in their father’s footsteps to uncover their family’s connection to the secretive organization known as Monarch. Naturally, they find themselves in the world of monsters and discover Army officer Lee Shaw (Kurt Russell), a longtime family ally.Read full article Comments",
          "feed_position": 12,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/monarch2-1152x648-1768318930.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/monarch2-1152x648-1768318930.jpg",
      "popularity_score": 146.2615963888889
    },
    {
      "id": "cluster_62",
      "coverage": 1,
      "updated_at": "Tue, 13 Jan 2026 18:15:01 +0000",
      "title": "Scott Adams, Dilbert creator, dead at 68",
      "neutral_headline": "Scott Adams, Dilbert creator, dead at 68",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/culture/2026/01/scott-adams-dilbert-creator-dead-at-68/",
          "published_at": "Tue, 13 Jan 2026 18:15:01 +0000",
          "title": "Scott Adams, Dilbert creator, dead at 68",
          "standfirst": "\"I had an amazing life. I gave it everything I had.\"",
          "content": "Scott Adams, the creator of the Dilbert comic strip, died today of prostate cancer at 68. Adams satirized the world of cubicle-based IT and engineering in Dilbert, which at its height appeared in 2,000 daily newspapers and was later anthologized in numerous books. Dilbert was an engineer with few social skills, but he always knew more than his pointy-haired boss, a caricature of terrible supervisors everywhere who managed to make the life of those who actually knew what they were doing—the engineers—much harder than it needed to be.Read full article Comments",
          "feed_position": 10,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-1322060604-1024x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-1322060604-1024x648.jpg",
      "popularity_score": 144.91409638888888
    },
    {
      "id": "cluster_87",
      "coverage": 1,
      "updated_at": "Tue, 13 Jan 2026 12:15:19 +0000",
      "title": "Wild mushrooms keep killing people in California; 3 dead, 35 poisoned",
      "neutral_headline": "Mushrooms keep killing people in California; 3 dead, 35 poisoned",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/health/2026/01/wild-mushrooms-keep-killing-people-in-california-3-dead-35-poisoned/",
          "published_at": "Tue, 13 Jan 2026 12:15:19 +0000",
          "title": "Wild mushrooms keep killing people in California; 3 dead, 35 poisoned",
          "standfirst": "Officials have linked the poisonings to the death cap mushroom (Amanita phalloides).",
          "content": "A third person has died in a rash of poisonings from wild, foraged mushrooms in California, health officials report. Since November, a total of 35 people across the state have been poisoned by mushrooms, leading to three people receiving liver transplants in addition to the three deaths. Health officials in Sonoma County reported the latest death last week. Michael Stacey, Sonoma's interim health officer, attributed the cases and deaths to an extraordinary boom in the prevalence of death cap mushrooms (Amanita phalloides), noting that in an average year, the state sees fewer than five mushroom poisoning cases.Read full article Comments",
          "feed_position": 13,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/death-cap-mushroom.png"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/death-cap-mushroom.png",
      "popularity_score": 141.91909638888887
    },
    {
      "id": "cluster_116",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 22:27:50 +0000",
      "title": "Even Linus Torvalds is trying his hand at vibe coding (but just a little)",
      "neutral_headline": "Even Linus Torvalds is trying his hand at vibe coding (but just a little)",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/ai/2026/01/hobby-github-repo-shows-linus-torvalds-vibe-codes-sometimes/",
          "published_at": "Mon, 12 Jan 2026 22:27:50 +0000",
          "title": "Even Linus Torvalds is trying his hand at vibe coding (but just a little)",
          "standfirst": "\"But then I cut out the middle man—me.\"",
          "content": "Linux and Git creator Linus Torvalds' latest project contains code that was \"basically written by vibe coding,\" but you shouldn't read that to mean that Torvalds is embracing that approach for anything and everything. Torvalds sometimes works on small hobby projects over holiday breaks. Last year, he made guitar pedals. This year, he did some work on AudioNoise, which he calls \"another silly guitar-pedal-related repo.\" It creates random digital audio effects. Torvalds revealed that he had used an AI coding tool in the README for the repo:Read full article Comments",
          "feed_position": 18,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2015/08/LinuxCon_Europe_Linus_Torvalds_05-1152x648-1768254932.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2015/08/LinuxCon_Europe_Linus_Torvalds_05-1152x648-1768254932.jpg",
      "popularity_score": 139
    },
    {
      "id": "cluster_112",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 23:42:09 +0000",
      "title": "Anthropic launches Cowork, a Claude Code-like for general computing",
      "neutral_headline": "Anthropic launches Cowork, a Claude Code-like for general computing",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/ai/2026/01/anthropic-launches-cowork-a-claude-code-like-for-general-computing/",
          "published_at": "Mon, 12 Jan 2026 23:42:09 +0000",
          "title": "Anthropic launches Cowork, a Claude Code-like for general computing",
          "standfirst": "Users can give Claude access to a folder and tell it what to do for them.",
          "content": "Anthropic's agentic tool Claude Code has been an enormous hit with some software developers and hobbyists, and now the company is bringing that modality to more general office work with a new feature called Cowork. Built on the same foundations as Claude Code and baked into the macOS Claude desktop app, Cowork allows users to give Claude access to a specific folder on their computer and then give plain language instructions for tasks. Anthropic gave examples like filling out an expense report from a folder full of receipt photos, writing reports based on a big stack of digital notes, or reorganizing a folder (or cleaning up your desktop) based on a prompt.Read full article Comments",
          "feed_position": 15,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/Claude-Cowork-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/Claude-Cowork-1152x648.jpg",
      "popularity_score": 133
    },
    {
      "id": "cluster_113",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 23:04:33 +0000",
      "title": "You can now reserve a hotel room on the Moon for $250,000",
      "neutral_headline": "You can now reserve a hotel room on the Moon for $250,000",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2026/01/you-can-now-reserve-a-hotel-room-on-the-moon-for-250000/",
          "published_at": "Mon, 12 Jan 2026 23:04:33 +0000",
          "title": "You can now reserve a hotel room on the Moon for $250,000",
          "standfirst": "\"We can't keep everyone living on that first ship that sailed to North America.\"",
          "content": "A company called GRU Space publicly announced its intent to construct a series of increasingly sophisticated habitats on the Moon, culminating in a hotel inspired by the Palace of the Fine Arts in San Francisco. On Monday, the company invited those interested in a berth to plunk down a deposit between $250,000 and $1 million, qualifying them for a spot on one of its early lunar surface missions in as little as six years from now. It sounds crazy, doesn't it? After all, GRU Space had, as of late December when I spoke to founder Skyler Chan, a single full-time employee aside from himself. And Chan, in fact, only recently graduated from the University of California, Berkeley.Read full article Comments",
          "feed_position": 16,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/GRU_Stills_Hotel_04-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/GRU_Stills_Hotel_04-1152x648.jpg",
      "popularity_score": 133
    },
    {
      "id": "cluster_114",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 22:49:31 +0000",
      "title": "Paramount sues WBD over Netflix deal. WBD says Paramount’s price is still inadequate.",
      "neutral_headline": "Paramount sues WBD over Netflix deal. WBD says Paramount’s price is still inadequate.",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2026/01/paramount-sues-wbd-over-netflix-deal-wbd-says-paramounts-price-is-still-inadequate/",
          "published_at": "Mon, 12 Jan 2026 22:49:31 +0000",
          "title": "Paramount sues WBD over Netflix deal. WBD says Paramount’s price is still inadequate.",
          "standfirst": "WBD calls Paramount's lawsuit \"meritless\" and its offer deficient.",
          "content": "Paramount Skydance escalated its hostile takeover bid of Warner Bros. Discovery (WBD) today by filing a lawsuit in Delaware Chancery Court against WBD, declaring its intention to fight Netflix’s acquisition. In December, WBD agreed to sell its streaming and movie businesses to Netflix for $82.7 billion. The deal would see WBD’s Global Networks division, composed of WBD's legacy cable networks, spun out into a separate company called Discovery Global. But in December, Paramount submitted a hostile takeover bid and amended its bid for WBD. Subsequently, the company has aggressively tried to convince WBD’s shareholders that its $108.4 billion offer for all of WBD is superior to the Netflix deal. Today, Paramount CEO David Ellison wrote a letter to WBD shareholders informing them of Paramount’s lawsuit. The lawsuit requests the court to force WBD to disclose “how it valued the Global Networks stub equity, how it valued the overall Netflix transaction, how the purchase price reduction for debt works in the Netflix transaction, or even what the basis is for its ‘risk adjustment’” of Paramount’s $30 per share all-cash offer. Netflix’s offer equates to $27.72 per share, including $23.25 in cash and shares of Netflix common stock. Paramount hopes the information will encourage more WBD shareholders to tender their shares under Paramount's offer by the January 21 deadline.Read full article Comments",
          "feed_position": 17,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2215193098-1152x648-1768255617.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2215193098-1152x648-1768255617.jpg",
      "popularity_score": 133
    }
  ]
}