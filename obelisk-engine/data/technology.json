{
  "updated_at": "2025-10-14T11:16:27.333Z",
  "clusters": [
    {
      "id": "cluster_14",
      "coverage": 2,
      "updated_at": "Tue, 14 Oct 2025 09:01:26 +0000",
      "title": "The best MacBook for 2025: Which Apple laptop should you buy?",
      "neutral_headline": "Best MacBook for 2025: Which Apple laptop should you buy?",
      "items": [
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/computing/laptops/best-macbook-140032524.html",
          "published_at": "Tue, 14 Oct 2025 09:01:26 +0000",
          "title": "The best MacBook for 2025: Which Apple laptop should you buy?",
          "standfirst": "There wasn’t much Mac-related news at Apple’s latest to-do (to be expected at an iPhone event), aside from the announcement that macOS 26 would be available to the public on September 15. Historically, we’ve seen new Mac hardware in October and then again in spring. But as of right now, there are just two MacBooks in Apple’s lineup: MacBook Air models with the M4 chip and MacBook Pro models with the M4 chip. Within those two options lie a handful of choices, including chip type, screen size, storage capacity and more. This guide will help you suss out the Apple terminology and explain the options so you can pick the best MacBook configuration for your needs. Table of contents Best MacBooks for 2025 What about budget MacBooks? Factors to consider when buying a MacBook MacBooks specs comparison chart Best MacBook FAQs Best MacBooks for 2025 What about budget MacBooks? Historically, Apple kept the previous year’s MacBook Air in its lineup as a sort of budget option. But the company took a different approach with the release of the M4 MacBook Air. Instead of continuing to sell the older model, Apple discontinued the M3 Air and gave its newest computer a $100 price cut. Now, if you can even find a brand new M3 MacBook Air (typically from retailers like Amazon or B&H), it’s often more expensive than the M4 version. During sales like Amazon Prime Day, we’ve seen the newest M4 Air go for as little as $799. That effectively makes our overall pick a budget pick as well. Of course, $800 isn’t exactly a small investment either for college students or others on a budget. Especially when you can find some decent PCs for under $500. If you’re looking to save even more on a MacBook, we recommend checking out refurbished options directly from Apple, or even third party sellers like BackMarket. There are a few guidelines to keep in mind, which we go over in our refurbished guide, but mainly, you’ll want to shop from a reputable source that has a stated process and offers at least a year-long warranty. Using your old gear as a trade-in will bring down your final cost as well. Factors to consider when buying a MacBook Compared to PCs, Apple computers tend to have more streamlined specifications. The company has long been known for this simplicity, and the M-series “system-on-a-chip” condenses things even further. Prior to the M1 chip, Apple used Intel chips in its laptop and desktop computers. The M2 and M3 generations followed that first chip and currently sells MacBooks equipped with M4-series chips. You’ll find the standard M4 processor in the Air and the base-model Pro and the upgraded M4 Max and M4 Pro chips as options for the MacBook Pro (currently there is no M4 Ultra chip, as there was with the M3 series in the Mac Studio). All M-series chips combine, among other technologies, the CPU, graphics card and unified memory (RAM). Apple’s Neural Engine is included too, which is a specialized group of processor cores that handles machine learning tasks such as image analysis and voice recognition. While a unified chip means you have fewer decisions to make when picking a MacBook, there are still a few factors to consider, including specs like the number of CPU cores, amount of RAM, storage capacity, screen size, and, obviously, price. The finish color may be a minor consideration, but it's worth pointing out that the Pro comes in just two colors (Silver or Space Black) but the Air comes in four hues (Midnight, Starlight, Sky Blue and Silver). CPU cores The lowest-specced chip in a current-lineup MacBook is the standard M4 chip, which is found in all models of the MacBook Air and the base model MacBook Pro 14-inch. That chip houses a 10-core CPU and either an 8- or 10-core GPU. In total, there are three versions of the M4 chip: standard M4, M4 Pro and M4 Max (which are each a step up from their predecessors, the M3, M3 Pro and M3 Max chips). The burliest chip, the M4 Max is built with either a 14- or 16-core CPU and a 32- or 40-core GPU. Cores are, in essence, smaller processing units that can handle different tasks simultaneously. Having more of them translates to the computer being able to run multiple programs and applications at once, while also smoothly processing demanding tasks like video and photo editing and high-level gaming. In short, more cores allow for more advanced computing and better performance. But if your processing power needs fall below professional-level gaming and cinematic video and audio editing, getting the highest number of cores is likely overkill — and after all, more cores equals higher cost and more power usage. Photo by Devindra Hardawar/Engadget RAM Your options for RAM, or in Apple’s terminology, unified memory, varies, but with the switch to the M4 chip in all laptops, the lowest amount of RAM you can get is now 16GB. That’s a necessary spec-bump to accommodate the tech world’s favorite feature of the moment: AI or, in this case, Apple Intelligence (still AI, but Cupertino’s version). The M4 Pro chip has 24 or 48GB memory options, while the M4 Max chip supports 48, 64 or a whopping 128GB of RAM. You’ve likely heard the analogy comparing memory to the amount of workspace available on a literal desktop surface, whereas storage is the amount of drawers you have to store projects to work on later. The larger the worktop surface, the more projects you can work on at once. The bigger the drawers, the more you can save for later. In addition to supporting Apple Intelligence, more RAM is ideal for people who plan to work in multiple apps at once. And the more demanding each program is, the more RAM will be required. Extra memory can also come in handy if you’re the type who likes to have infinite numbers of tabs open on your browser. If your daily workflow doesn’t involve simultaneously using a vast number of memory-intensive programs, you can save yourself money and buy the RAM configuration that you’re most likely to actually use. For a long time, Apple continued to offer MacBooks with just 8GB of RAM, and we recommended upgrading to at least 16GB of RAM. With this being the standard today, grabbing a base model should be fine for most non-pro-level users. One thing to note is that, unlike most PCs, the RAM in a MacBook is not user-upgradable since it’s tied into the system-on-a-chip. If you think you might end up needing more memory, you should go for the spec upgrade up front. Storage capacity (SSD) Storage options range from 256GB of SSD for the base-model MacBook Air and 8TB of storage for the MacBook Pros with the M4 Max chip. If you want to rotate between a long roster of game titles or keep lots of high-res videos on hand, you’ll want more storage. If you’re mostly working with browser- and cloud-based applications, you can get away with a smaller-capacity configuration. That said, we recommend springing for 512GB of storage or more, if it’s within your budget. You’ll quickly feel the limits of a 256GB machine as it ages since the operating system alone takes up a good portion of that space. Having 1TB will feel even roomier and allow for more data storage over the life of your laptop. When Apple announced the iPhone 15, the company also announced new iCloud+ storage storage plans, with subscriptions that allow up to 12TB of storage shared among your iOS and MacOS devices. You could also transfer files to an external storage device. But if you don’t want to pay for a monthly subscription and prefer the convenience of having immediate access to your files, it’s best to get the highest amount of storage space your budget allows for at the outset. Screen size The MacBook Air comes in 13- or 15-inch sizes. Pro models have either 14- or 16-inch screens. A two-inch delta may not seem like much but, as Engadget’s Nathan Ingraham noted when he reviewed the then-new 15-inch M2-powered MacBook Air, a larger screen \"makes a surprising difference.” That’s especially true if you plan to use your laptop as an all-day productivity machine and won’t be using an external monitor. More space means you can more clearly view side-by-side windows and have a more immersive experience when watching shows or gaming. But screen size is one of the main factors influencing weight. The 13-inch MacBook Air M4 weighs 2.7 pounds, whereas the top-end 16-inch MacBook Pro with the Max chip weighs 4.7 pounds. If you plan to travel a lot or swap your work locations regularly, a smaller screen will make life easier in the long run. All MacBooks feature IPS LCD panels (in-plane switching, liquid crystal display), which Apple markets as Retina displays. The MacBook Air M4 has a Liquid Retina display and the Pro models have Liquid Retina XDR displays. “Liquid” refers to the way the lighted portion of the display “flows” within the contours of the screen, filling the rounded corners and curving around the camera notch. “XDR” is what Apple calls HDR (high dynamic range). You also get the option of a standard or nano-texture display on the MacBook Pro. The glass, which reduces glare and is also available on the Studio Display, iMac and iPad Pro, comes with a $150 price increase, but if you really don’t like reflections on your screen, it could be worth it. Compared to most other laptops, MacBook displays are notably bright, sharp and lush. But one feature worth pointing out is another Apple marketing term: ProMotion. It’s the company’s term to describe a screen with a higher, 120Hz refresh rate, which results in smoother scrolling and more fluid-looking graphics. Only MacBook Pros offer ProMotion; the Air maxes out at 60Hz, which is perfectly fine for everyday browsing and typical workdays. But if you want buttery-smooth motion from your display, you’ll have to shell out more money for an upgrade. Operating systems Software considerations won’t make much of a difference when deciding between MacBook models — all come with macOS installed. But if you’re switching from, say, a Windows PC, the operating system may be something to factor into your decision — though it’s probably less of an issue than it once was. Now that so much of the work we do on our computers is browser- and cloud-based, the learning curve between the two platforms isn’t as steep. Apps and programs like Gmail perform similarly regardless of what computer you’re using. Apple machines have historically had more limited support of AAA gaming titles, but even that is changing with more AAA games and better graphics coming to Macs. As for macOS, it’s getting better too. With macOS Tahoe 26, the Spotlight function is more advanced, making it easier to find apps and perform tasks straight from your keyboard. The software also implements Apple's unifying Liquid Glass design for a modern look that looks consistent across iOS and iPad devices. New enhanced iPhone continuity features also make MacBooks and the handset work better together. A revamped Shortcuts app is more powerful as well, giving users custom automations that leverage Apple Intelligence (the company’s own AI). Price When Apple announced the MacBook Air M4, it also delivered a bit of refreshing news: The latest model now starts $100 cheaper than the previous generation. So now, the least expensive MacBook is the 13-inch, M4-powered Air with 16GB of RAM and 256GB of storage for $999. Alternatively, you can spend up to $7,349 for the 16-inch MacBook Pro M4 Max with the nano-texture glass, 128GB of RAM and 8TB of storage. Chip type, screen size, memory and storage capacity all influence the final price, which is why guides like this can help you determine just what you need (and what you don’t) so you can get the most cost-effective machine for you. AppleCare is another cost to consider. The extended warranty plan from Apple covers repairs from accidents and offers free battery replacement and starts at $3.50 per month or $35 per year for MacBooks. We recommend the MacBook Air M4 for most people, and thanks to that $100 price cut, it’s also a good budget option. If you want something even cheaper, we recommend looking at refurbished M-series models from Apple. We think the 14- or 16-inch MacBook Pros are best for professionals. If you have extra money to spare once you’ve picked your machine, we recommend upgrading to at least 512GB of storage to make your machine as future-proof as possible. Of course, if you're just after the M4 chip and want the cheapest route to get it, you might consider the M4 Mac mini, which starts at $599 (though you'll have to supply the screen, mouse and keyboard). Best MacBooks spec comparison chart Product Superlative Tested configuration Tested battery life Rated battery life Apple MacBook Air M4 (13-inch) Best MacBook overall Apple M4, 16GB RAM, 256GB SSD 18.25 hours Up to 18 hours Apple MacBook Pro M4 (14-inch) Best MacBook for creatives Apple M4, 16GB RAM, 512GB SSD 34.25 hours Up to 22 hours Best MacBook FAQs What's the difference between MacBook Air and Pro? Both the MacBook Air and Pro models come with the M4 chip. MBP models have the option of more powerful M4 Pro or M4 Max chips. The Pro has a higher resolution screen with a higher peak brightness that supports up to 120Hz adaptive refresh rates and XDR (extreme dynamic range). The battery life on most Pro models is longer than on the Air models as well. Pro models also have more ports and more speakers. In short, the MacBook Air is aimed at everyday users looking for good productivity and entertainment capabilities, while Pro models are aimed at professionals who need a high-performance computer. What's the difference between macOS and Windows? MacOS is the operating system developed by Apple and used in all of its desktop and laptop computers. It can only be found in hardware made by Apple including MacBooks and iMacs. Microsoft’s Windows operating system can be found in the company’s own Surface laptops as well as computers made by a wide array of manufacturers, like Acer, Asus, Dell and Razer.This article originally appeared on Engadget at https://www.engadget.com/computing/laptops/best-macbook-140032524.html?src=rss",
          "content": "There wasn’t much Mac-related news at Apple’s latest to-do (to be expected at an iPhone event), aside from the announcement that macOS 26 would be available to the public on September 15. Historically, we’ve seen new Mac hardware in October and then again in spring. But as of right now, there are just two MacBooks in Apple’s lineup: MacBook Air models with the M4 chip and MacBook Pro models with the M4 chip. Within those two options lie a handful of choices, including chip type, screen size, storage capacity and more. This guide will help you suss out the Apple terminology and explain the options so you can pick the best MacBook configuration for your needs. Table of contents Best MacBooks for 2025 What about budget MacBooks? Factors to consider when buying a MacBook MacBooks specs comparison chart Best MacBook FAQs Best MacBooks for 2025 What about budget MacBooks? Historically, Apple kept the previous year’s MacBook Air in its lineup as a sort of budget option. But the company took a different approach with the release of the M4 MacBook Air. Instead of continuing to sell the older model, Apple discontinued the M3 Air and gave its newest computer a $100 price cut. Now, if you can even find a brand new M3 MacBook Air (typically from retailers like Amazon or B&H), it’s often more expensive than the M4 version. During sales like Amazon Prime Day, we’ve seen the newest M4 Air go for as little as $799. That effectively makes our overall pick a budget pick as well. Of course, $800 isn’t exactly a small investment either for college students or others on a budget. Especially when you can find some decent PCs for under $500. If you’re looking to save even more on a MacBook, we recommend checking out refurbished options directly from Apple, or even third party sellers like BackMarket. There are a few guidelines to keep in mind, which we go over in our refurbished guide, but mainly, you’ll want to shop from a reputable source that has a stated process and offers at least a year-long warranty. Using your old gear as a trade-in will bring down your final cost as well. Factors to consider when buying a MacBook Compared to PCs, Apple computers tend to have more streamlined specifications. The company has long been known for this simplicity, and the M-series “system-on-a-chip” condenses things even further. Prior to the M1 chip, Apple used Intel chips in its laptop and desktop computers. The M2 and M3 generations followed that first chip and currently sells MacBooks equipped with M4-series chips. You’ll find the standard M4 processor in the Air and the base-model Pro and the upgraded M4 Max and M4 Pro chips as options for the MacBook Pro (currently there is no M4 Ultra chip, as there was with the M3 series in the Mac Studio). All M-series chips combine, among other technologies, the CPU, graphics card and unified memory (RAM). Apple’s Neural Engine is included too, which is a specialized group of processor cores that handles machine learning tasks such as image analysis and voice recognition. While a unified chip means you have fewer decisions to make when picking a MacBook, there are still a few factors to consider, including specs like the number of CPU cores, amount of RAM, storage capacity, screen size, and, obviously, price. The finish color may be a minor consideration, but it's worth pointing out that the Pro comes in just two colors (Silver or Space Black) but the Air comes in four hues (Midnight, Starlight, Sky Blue and Silver). CPU cores The lowest-specced chip in a current-lineup MacBook is the standard M4 chip, which is found in all models of the MacBook Air and the base model MacBook Pro 14-inch. That chip houses a 10-core CPU and either an 8- or 10-core GPU. In total, there are three versions of the M4 chip: standard M4, M4 Pro and M4 Max (which are each a step up from their predecessors, the M3, M3 Pro and M3 Max chips). The burliest chip, the M4 Max is built with either a 14- or 16-core CPU and a 32- or 40-core GPU. Cores are, in essence, smaller processing units that can handle different tasks simultaneously. Having more of them translates to the computer being able to run multiple programs and applications at once, while also smoothly processing demanding tasks like video and photo editing and high-level gaming. In short, more cores allow for more advanced computing and better performance. But if your processing power needs fall below professional-level gaming and cinematic video and audio editing, getting the highest number of cores is likely overkill — and after all, more cores equals higher cost and more power usage. Photo by Devindra Hardawar/Engadget RAM Your options for RAM, or in Apple’s terminology, unified memory, varies, but with the switch to the M4 chip in all laptops, the lowest amount of RAM you can get is now 16GB. That’s a necessary spec-bump to accommodate the tech world’s favorite feature of the moment: AI or, in this case, Apple Intelligence (still AI, but Cupertino’s version). The M4 Pro chip has 24 or 48GB memory options, while the M4 Max chip supports 48, 64 or a whopping 128GB of RAM. You’ve likely heard the analogy comparing memory to the amount of workspace available on a literal desktop surface, whereas storage is the amount of drawers you have to store projects to work on later. The larger the worktop surface, the more projects you can work on at once. The bigger the drawers, the more you can save for later. In addition to supporting Apple Intelligence, more RAM is ideal for people who plan to work in multiple apps at once. And the more demanding each program is, the more RAM will be required. Extra memory can also come in handy if you’re the type who likes to have infinite numbers of tabs open on your browser. If your daily workflow doesn’t involve simultaneously using a vast number of memory-intensive programs, you can save yourself money and buy the RAM configuration that you’re most likely to actually use. For a long time, Apple continued to offer MacBooks with just 8GB of RAM, and we recommended upgrading to at least 16GB of RAM. With this being the standard today, grabbing a base model should be fine for most non-pro-level users. One thing to note is that, unlike most PCs, the RAM in a MacBook is not user-upgradable since it’s tied into the system-on-a-chip. If you think you might end up needing more memory, you should go for the spec upgrade up front. Storage capacity (SSD) Storage options range from 256GB of SSD for the base-model MacBook Air and 8TB of storage for the MacBook Pros with the M4 Max chip. If you want to rotate between a long roster of game titles or keep lots of high-res videos on hand, you’ll want more storage. If you’re mostly working with browser- and cloud-based applications, you can get away with a smaller-capacity configuration. That said, we recommend springing for 512GB of storage or more, if it’s within your budget. You’ll quickly feel the limits of a 256GB machine as it ages since the operating system alone takes up a good portion of that space. Having 1TB will feel even roomier and allow for more data storage over the life of your laptop. When Apple announced the iPhone 15, the company also announced new iCloud+ storage storage plans, with subscriptions that allow up to 12TB of storage shared among your iOS and MacOS devices. You could also transfer files to an external storage device. But if you don’t want to pay for a monthly subscription and prefer the convenience of having immediate access to your files, it’s best to get the highest amount of storage space your budget allows for at the outset. Screen size The MacBook Air comes in 13- or 15-inch sizes. Pro models have either 14- or 16-inch screens. A two-inch delta may not seem like much but, as Engadget’s Nathan Ingraham noted when he reviewed the then-new 15-inch M2-powered MacBook Air, a larger screen \"makes a surprising difference.” That’s especially true if you plan to use your laptop as an all-day productivity machine and won’t be using an external monitor. More space means you can more clearly view side-by-side windows and have a more immersive experience when watching shows or gaming. But screen size is one of the main factors influencing weight. The 13-inch MacBook Air M4 weighs 2.7 pounds, whereas the top-end 16-inch MacBook Pro with the Max chip weighs 4.7 pounds. If you plan to travel a lot or swap your work locations regularly, a smaller screen will make life easier in the long run. All MacBooks feature IPS LCD panels (in-plane switching, liquid crystal display), which Apple markets as Retina displays. The MacBook Air M4 has a Liquid Retina display and the Pro models have Liquid Retina XDR displays. “Liquid” refers to the way the lighted portion of the display “flows” within the contours of the screen, filling the rounded corners and curving around the camera notch. “XDR” is what Apple calls HDR (high dynamic range). You also get the option of a standard or nano-texture display on the MacBook Pro. The glass, which reduces glare and is also available on the Studio Display, iMac and iPad Pro, comes with a $150 price increase, but if you really don’t like reflections on your screen, it could be worth it. Compared to most other laptops, MacBook displays are notably bright, sharp and lush. But one feature worth pointing out is another Apple marketing term: ProMotion. It’s the company’s term to describe a screen with a higher, 120Hz refresh rate, which results in smoother scrolling and more fluid-looking graphics. Only MacBook Pros offer ProMotion; the Air maxes out at 60Hz, which is perfectly fine for everyday browsing and typical workdays. But if you want buttery-smooth motion from your display, you’ll have to shell out more money for an upgrade. Operating systems Software considerations won’t make much of a difference when deciding between MacBook models — all come with macOS installed. But if you’re switching from, say, a Windows PC, the operating system may be something to factor into your decision — though it’s probably less of an issue than it once was. Now that so much of the work we do on our computers is browser- and cloud-based, the learning curve between the two platforms isn’t as steep. Apps and programs like Gmail perform similarly regardless of what computer you’re using. Apple machines have historically had more limited support of AAA gaming titles, but even that is changing with more AAA games and better graphics coming to Macs. As for macOS, it’s getting better too. With macOS Tahoe 26, the Spotlight function is more advanced, making it easier to find apps and perform tasks straight from your keyboard. The software also implements Apple's unifying Liquid Glass design for a modern look that looks consistent across iOS and iPad devices. New enhanced iPhone continuity features also make MacBooks and the handset work better together. A revamped Shortcuts app is more powerful as well, giving users custom automations that leverage Apple Intelligence (the company’s own AI). Price When Apple announced the MacBook Air M4, it also delivered a bit of refreshing news: The latest model now starts $100 cheaper than the previous generation. So now, the least expensive MacBook is the 13-inch, M4-powered Air with 16GB of RAM and 256GB of storage for $999. Alternatively, you can spend up to $7,349 for the 16-inch MacBook Pro M4 Max with the nano-texture glass, 128GB of RAM and 8TB of storage. Chip type, screen size, memory and storage capacity all influence the final price, which is why guides like this can help you determine just what you need (and what you don’t) so you can get the most cost-effective machine for you. AppleCare is another cost to consider. The extended warranty plan from Apple covers repairs from accidents and offers free battery replacement and starts at $3.50 per month or $35 per year for MacBooks. We recommend the MacBook Air M4 for most people, and thanks to that $100 price cut, it’s also a good budget option. If you want something even cheaper, we recommend looking at refurbished M-series models from Apple. We think the 14- or 16-inch MacBook Pros are best for professionals. If you have extra money to spare once you’ve picked your machine, we recommend upgrading to at least 512GB of storage to make your machine as future-proof as possible. Of course, if you're just after the M4 chip and want the cheapest route to get it, you might consider the M4 Mac mini, which starts at $599 (though you'll have to supply the screen, mouse and keyboard). Best MacBooks spec comparison chart Product Superlative Tested configuration Tested battery life Rated battery life Apple MacBook Air M4 (13-inch) Best MacBook overall Apple M4, 16GB RAM, 256GB SSD 18.25 hours Up to 18 hours Apple MacBook Pro M4 (14-inch) Best MacBook for creatives Apple M4, 16GB RAM, 512GB SSD 34.25 hours Up to 22 hours Best MacBook FAQs What's the difference between MacBook Air and Pro? Both the MacBook Air and Pro models come with the M4 chip. MBP models have the option of more powerful M4 Pro or M4 Max chips. The Pro has a higher resolution screen with a higher peak brightness that supports up to 120Hz adaptive refresh rates and XDR (extreme dynamic range). The battery life on most Pro models is longer than on the Air models as well. Pro models also have more ports and more speakers. In short, the MacBook Air is aimed at everyday users looking for good productivity and entertainment capabilities, while Pro models are aimed at professionals who need a high-performance computer. What's the difference between macOS and Windows? MacOS is the operating system developed by Apple and used in all of its desktop and laptop computers. It can only be found in hardware made by Apple including MacBooks and iMacs. Microsoft’s Windows operating system can be found in the company’s own Surface laptops as well as computers made by a wide array of manufacturers, like Acer, Asus, Dell and Razer.This article originally appeared on Engadget at https://www.engadget.com/computing/laptops/best-macbook-140032524.html?src=rss",
          "feed_position": 0,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2023-11/e536a1d0-7c1e-11ee-9e77-9ea8e142b078"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/visa-just-launched-a-protocol-to-secure-the-ai-shopping-boom-heres-what-it",
          "published_at": "Tue, 14 Oct 2025 07:00:00 GMT",
          "title": "Visa just launched a protocol to secure the AI shopping boom — here’s what it means for merchants",
          "standfirst": "Visa is introducing a new security framework designed to solve one of the thorniest problems emerging in artificial intelligence-powered commerce: how retailers can tell the difference between legitimate AI shopping assistants and the malicious bots that plague their websites.The payments giant unveiled its Trusted Agent Protocol on Tuesday, establishing what it describes as foundational infrastructure for \"agentic commerce\" — a term for the rapidly growing practice of consumers delegating shopping tasks to AI agents that can search products, compare prices, and complete purchases autonomously.The protocol enables merchants to cryptographically verify that an AI agent browsing their site is authorized and trustworthy, rather than a bot designed to scrape pricing data, test stolen credit cards, or carry out other fraudulent activities.The launch comes as AI-driven traffic to U.S. retail websites has exploded by more than 4,700% over the past year, according to data from Adobe cited by Visa. That dramatic surge has created an acute challenge for merchants whose existing bot detection systems — designed to block automated traffic — now risk accidentally blocking legitimate AI shoppers along with bad actors.\"Merchants need additional tools that provide them with greater insight and transparency into agentic commerce activities to ensure they can participate safely,\" said Rubail Birwadker, Visa&#x27;s Global Head of Growth, in an exclusive interview with VentureBeat. \"Without common standards, potential risks include ecosystem fragmentation and the proliferation of closed loop models.\"The stakes are substantial. While 85% of shoppers who have used AI to shop report improved experiences, merchants face the prospect of either turning away legitimate AI-powered customers or exposing themselves to sophisticated bot attacks. Visa&#x27;s own data shows the company prevented $40 billion in fraudulent activity between October 2022 and September 2023, nearly double the previous year, much of it involving AI-powered enumeration attacks where bots systematically test combinations of card numbers until finding valid credentials.Inside the cryptographic handshake: How Visa verifies AI shopping agentsVisa&#x27;s Trusted Agent Protocol operates through what Birwadker describes as a \"cryptographic trust handshake\" between merchants and approved AI agents. The system works in three steps:First, AI agents must be approved and onboarded through Visa&#x27;s Intelligent Commerce program, where they undergo vetting to meet trust and reliability standards. Each approved agent receives a unique digital signature key — essentially a cryptographic credential that proves its identity.When an approved agent visits a merchant&#x27;s website, it creates a digital signature using its key and transmits three categories of information: Agent Intent (indicating the agent is trusted and intends to retrieve product details or make a purchase), Consumer Recognition (data showing whether the underlying consumer has an existing account with the merchant), and Payment Information (optional payment data to support checkout).Merchants or their infrastructure providers, such as content delivery networks, then validate these digital signatures against Visa&#x27;s registry of approved agents. \"Upon proper validation of these fields, the merchant can confirm the signature is a trusted agent,\" Birwadker explained.Crucially, Visa designed the protocol to require minimal changes to existing merchant infrastructure. Built on the HTTP Message Signature standard and aligned with Web Both Auth, the protocol works with existing web infrastructure without requiring merchants to overhaul their checkout pages. \"This is no-code functionality,\" Birwadker emphasized, though merchants may need to integrate with Visa&#x27;s Developer Center to access the verification system.The race for AI commerce standards: Visa faces competition from Google, OpenAI, and StripeVisa developed the protocol in collaboration with Cloudflare, the web infrastructure and security company that already provides bot management services to millions of websites. The partnership reflects Visa&#x27;s recognition that solving bot verification requires cooperation across the entire web stack, not just the payments layer.\"Trusted Agent Protocol supplements traditional bot management by providing merchants insights that enable agentic commerce,\" Birwadker said. \"Agents are providing additional context they otherwise would not, including what it intends to do, who the underlying consumer is, and payment information.\"The protocol arrives as multiple technology giants race to establish competing standards for AI commerce. Google recently introduced its Agent Protocol for Payments (AP2), while OpenAI and Stripe have discussed their own approaches to enabling AI agents to make purchases. Microsoft, Shopify, Adyen, Ant International, Checkout.com, Cybersource, Elavon, Fiserv, Nuvei, and Worldpay provided feedback during Trusted Agent Protocol&#x27;s development, according to Visa.When asked how Visa&#x27;s protocol relates to these competing efforts, Birwadker struck a collaborative tone. \"Both Google&#x27;s AP2 and Visa&#x27;s Trusted Agent Protocol are working toward the same goal of building trust in agent-initiated payments,\" he said. \"We are engaged with Google, OpenAI, and Stripe and are looking to create compatibility across the ecosystem.\"Visa says it is working with global standards bodies including the Internet Engineering Task Force (IETF), OpenID Foundation, and EMVCo to ensure the protocol can eventually become interoperable with other emerging standards. \"While these specifications apply to the Visa network in this initial phase, enabling agents to safely and securely act on a consumer&#x27;s behalf requires an open, ecosystem-wide approach,\" Birwadker noted.Who pays when AI agents go rogue? Unanswered questions about liability and authorizationThe protocol raises important questions about authorization and liability when AI agents make purchases on behalf of consumers. If an agent completes an unauthorized transaction — perhaps misunderstanding a user&#x27;s intent or exceeding its delegated authority — who bears responsibility?Birwadker emphasized that the protocol helps merchants \"leverage this information to enable experiences tied to existing consumer relationships and more secure checkout,\" but he did not provide specific details about how disputes would be handled when agents make unauthorized purchases. Visa&#x27;s existing fraud protection and chargeback systems would presumably apply, though the company has not yet published detailed guidance on agent-initiated transaction disputes.The protocol also places Visa in the position of gatekeeper for the emerging agentic commerce ecosystem. Because Visa determines which AI agents get approved for the Intelligent Commerce program and receive cryptographic credentials, the company effectively controls which agents merchants can easily trust. \"Agents are approved and onboarded through the Visa Intelligent Commerce program, ensuring they meet our standards for trust and reliability,\" Birwadker said, though he did not detail the specific criteria agents must meet or whether Visa charges fees for approval.This gatekeeping role could prove contentious, particularly if Visa&#x27;s approval process favors large technology companies over startups, or if the company faces pressure to block agents from competitors or politically controversial entities. Visa declined to provide details about how many agents it has approved so far or how long the vetting process typically takes.Visa&#x27;s legal battles and the long road to merchant adoptionThe protocol launch comes at a complex moment for Visa, which continues to navigate significant legal and regulatory challenges even as its core business remains robust. The company&#x27;s latest earnings report for the third quarter of fiscal year 2025 showed a 10% increase in net revenues to $9.2 billion, driven by resilient consumer spending and strong growth in cross-border transaction volume. For the full fiscal year ending September 30, 2024, Visa processed 289 billion transactions, with a total payments volume of $15.2 trillion. However, the company&#x27;s legal headwinds have intensified. In July 2025, a federal judge rejected a landmark $30 billion settlement that Visa and Mastercard had reached with merchants over long-disputed credit card swipe fees, sending the parties back to the negotiating table and extending the long-running legal battle.Simultaneously, Visa remains under investigation by the Department of Justice over its rules for routing debit card transactions, with regulators scrutinizing whether the company&#x27;s practices unlawfully limit merchant choice and stifle competition. These domestic challenges are mirrored abroad, where European regulators have continued their own antitrust investigations into the fee structures of both Visa and its primary competitor, Mastercard.Against this backdrop of regulatory pressure, Birwadker acknowledged that adoption of the Trusted Agent Protocol will take time. \"As agentic commerce continues to rise, we recognize that consumer trust is still in its early stages,\" he said. \"That&#x27;s why our focus through 2025 is on building foundational credibility and demonstrating real-world value.\"The protocol is available immediately in Visa&#x27;s Developer Center and on GitHub, with agent onboarding already active and merchant integration resources available. But Birwadker declined to provide specific targets for how many merchants might adopt the protocol by the end of 2026. \"Adoption is aligned with the momentum we&#x27;re already seeing,\" he said. \"The launch of our protocol marks another big step — it&#x27;s not just a technical milestone, but a signal that the industry is beginning to unify.\"Industry analysts say merchant adoption will likely depend on how quickly agentic commerce grows as a percentage of overall e-commerce. While AI-driven traffic has surged dramatically, much of that consists of agents browsing and researching rather than completing purchases. If AI agents begin accounting for a significant share of completed transactions, merchants will face stronger incentives to adopt verification systems like Visa&#x27;s protocol.From fraud detection to AI gatekeeping: Visa&#x27;s $10 billion bet on artificial intelligenceVisa&#x27;s move reflects broader strategic bets on AI across the financial services industry. The company has invested $10 billion in technology over the past five years to reduce fraud and increase network security, with AI and machine learning central to those efforts. Visa&#x27;s fraud detection system analyzes over 500 different attributes for each transaction, using AI models to assign real-time risk scores to the 300 billion annual transactions flowing through its network.\"Every single one of those transactions has been processed by AI,\" James Mirfin, Visa&#x27;s global head of risk and identity solutions, said in a July 2024 CNBC interview discussing the company&#x27;s fraud prevention efforts. \"If you see a new type of fraud happening, our model will see that, it will catch it, it will score those transactions as high risk and then our customers can decide not to approve those transactions.\"The company has also moved aggressively into new payment territories beyond its core card business. In January 2025, Visa partnered with Elon Musk&#x27;s X (formerly Twitter) to provide the infrastructure for a digital wallet and peer-to-peer payment service called the X Money Account, competing with services like Venmo and Zelle. That deal marked Visa&#x27;s first major partnership in the social media payments space and reflected the company&#x27;s recognition that payment flows are increasingly happening outside traditional e-commerce channels.The agentic commerce protocol represents an extension of this strategy — an attempt to ensure Visa remains central to payment flows even as the mechanics of shopping shift from direct human interaction to AI intermediation. Jack Forestell, Visa&#x27;s Chief Product & Strategy Officer, framed the protocol in expansive terms: \"We believe the entire payments ecosystem has a responsibility to ensure sellers trust AI agents with the same confidence they place in their most valued customers and networks.\"The coming battle for control of AI shoppingThe real test for Visa&#x27;s protocol won&#x27;t be technical — it will be political. As AI agents become a larger force in retail, whoever controls the verification infrastructure controls access to hundreds of billions of dollars in commerce. Visa&#x27;s position as gatekeeper gives it enormous leverage, but also makes it a target.Merchants chafing under Visa&#x27;s existing fee structure and facing multiple antitrust investigations may resist ceding even more power to the payments giant. Competitors like Google and OpenAI, each with their own ambitions in commerce, have little incentive to let Visa dictate standards. Regulators already scrutinizing Visa&#x27;s market dominance will surely examine whether its agent approval process unfairly advantages certain players.And there&#x27;s a deeper question lurking beneath the technical specifications and corporate partnerships: In an economy increasingly mediated by AI, who decides which algorithms get to spend our money? Visa is making an aggressive bid to be that arbiter, wrapping its answer in the language of security and interoperability. Whether merchants, consumers, and regulators accept that proposition will determine not just the fate of the Trusted Agent Protocol, but the structure of AI-powered commerce itself.For now, Visa is moving forward with the confidence of a company that has weathered disruption before. But in the emerging world of agentic commerce, being too trusted might prove just as dangerous as not being trusted enough.",
          "content": "Visa is introducing a new security framework designed to solve one of the thorniest problems emerging in artificial intelligence-powered commerce: how retailers can tell the difference between legitimate AI shopping assistants and the malicious bots that plague their websites.The payments giant unveiled its Trusted Agent Protocol on Tuesday, establishing what it describes as foundational infrastructure for \"agentic commerce\" — a term for the rapidly growing practice of consumers delegating shopping tasks to AI agents that can search products, compare prices, and complete purchases autonomously.The protocol enables merchants to cryptographically verify that an AI agent browsing their site is authorized and trustworthy, rather than a bot designed to scrape pricing data, test stolen credit cards, or carry out other fraudulent activities.The launch comes as AI-driven traffic to U.S. retail websites has exploded by more than 4,700% over the past year, according to data from Adobe cited by Visa. That dramatic surge has created an acute challenge for merchants whose existing bot detection systems — designed to block automated traffic — now risk accidentally blocking legitimate AI shoppers along with bad actors.\"Merchants need additional tools that provide them with greater insight and transparency into agentic commerce activities to ensure they can participate safely,\" said Rubail Birwadker, Visa&#x27;s Global Head of Growth, in an exclusive interview with VentureBeat. \"Without common standards, potential risks include ecosystem fragmentation and the proliferation of closed loop models.\"The stakes are substantial. While 85% of shoppers who have used AI to shop report improved experiences, merchants face the prospect of either turning away legitimate AI-powered customers or exposing themselves to sophisticated bot attacks. Visa&#x27;s own data shows the company prevented $40 billion in fraudulent activity between October 2022 and September 2023, nearly double the previous year, much of it involving AI-powered enumeration attacks where bots systematically test combinations of card numbers until finding valid credentials.Inside the cryptographic handshake: How Visa verifies AI shopping agentsVisa&#x27;s Trusted Agent Protocol operates through what Birwadker describes as a \"cryptographic trust handshake\" between merchants and approved AI agents. The system works in three steps:First, AI agents must be approved and onboarded through Visa&#x27;s Intelligent Commerce program, where they undergo vetting to meet trust and reliability standards. Each approved agent receives a unique digital signature key — essentially a cryptographic credential that proves its identity.When an approved agent visits a merchant&#x27;s website, it creates a digital signature using its key and transmits three categories of information: Agent Intent (indicating the agent is trusted and intends to retrieve product details or make a purchase), Consumer Recognition (data showing whether the underlying consumer has an existing account with the merchant), and Payment Information (optional payment data to support checkout).Merchants or their infrastructure providers, such as content delivery networks, then validate these digital signatures against Visa&#x27;s registry of approved agents. \"Upon proper validation of these fields, the merchant can confirm the signature is a trusted agent,\" Birwadker explained.Crucially, Visa designed the protocol to require minimal changes to existing merchant infrastructure. Built on the HTTP Message Signature standard and aligned with Web Both Auth, the protocol works with existing web infrastructure without requiring merchants to overhaul their checkout pages. \"This is no-code functionality,\" Birwadker emphasized, though merchants may need to integrate with Visa&#x27;s Developer Center to access the verification system.The race for AI commerce standards: Visa faces competition from Google, OpenAI, and StripeVisa developed the protocol in collaboration with Cloudflare, the web infrastructure and security company that already provides bot management services to millions of websites. The partnership reflects Visa&#x27;s recognition that solving bot verification requires cooperation across the entire web stack, not just the payments layer.\"Trusted Agent Protocol supplements traditional bot management by providing merchants insights that enable agentic commerce,\" Birwadker said. \"Agents are providing additional context they otherwise would not, including what it intends to do, who the underlying consumer is, and payment information.\"The protocol arrives as multiple technology giants race to establish competing standards for AI commerce. Google recently introduced its Agent Protocol for Payments (AP2), while OpenAI and Stripe have discussed their own approaches to enabling AI agents to make purchases. Microsoft, Shopify, Adyen, Ant International, Checkout.com, Cybersource, Elavon, Fiserv, Nuvei, and Worldpay provided feedback during Trusted Agent Protocol&#x27;s development, according to Visa.When asked how Visa&#x27;s protocol relates to these competing efforts, Birwadker struck a collaborative tone. \"Both Google&#x27;s AP2 and Visa&#x27;s Trusted Agent Protocol are working toward the same goal of building trust in agent-initiated payments,\" he said. \"We are engaged with Google, OpenAI, and Stripe and are looking to create compatibility across the ecosystem.\"Visa says it is working with global standards bodies including the Internet Engineering Task Force (IETF), OpenID Foundation, and EMVCo to ensure the protocol can eventually become interoperable with other emerging standards. \"While these specifications apply to the Visa network in this initial phase, enabling agents to safely and securely act on a consumer&#x27;s behalf requires an open, ecosystem-wide approach,\" Birwadker noted.Who pays when AI agents go rogue? Unanswered questions about liability and authorizationThe protocol raises important questions about authorization and liability when AI agents make purchases on behalf of consumers. If an agent completes an unauthorized transaction — perhaps misunderstanding a user&#x27;s intent or exceeding its delegated authority — who bears responsibility?Birwadker emphasized that the protocol helps merchants \"leverage this information to enable experiences tied to existing consumer relationships and more secure checkout,\" but he did not provide specific details about how disputes would be handled when agents make unauthorized purchases. Visa&#x27;s existing fraud protection and chargeback systems would presumably apply, though the company has not yet published detailed guidance on agent-initiated transaction disputes.The protocol also places Visa in the position of gatekeeper for the emerging agentic commerce ecosystem. Because Visa determines which AI agents get approved for the Intelligent Commerce program and receive cryptographic credentials, the company effectively controls which agents merchants can easily trust. \"Agents are approved and onboarded through the Visa Intelligent Commerce program, ensuring they meet our standards for trust and reliability,\" Birwadker said, though he did not detail the specific criteria agents must meet or whether Visa charges fees for approval.This gatekeeping role could prove contentious, particularly if Visa&#x27;s approval process favors large technology companies over startups, or if the company faces pressure to block agents from competitors or politically controversial entities. Visa declined to provide details about how many agents it has approved so far or how long the vetting process typically takes.Visa&#x27;s legal battles and the long road to merchant adoptionThe protocol launch comes at a complex moment for Visa, which continues to navigate significant legal and regulatory challenges even as its core business remains robust. The company&#x27;s latest earnings report for the third quarter of fiscal year 2025 showed a 10% increase in net revenues to $9.2 billion, driven by resilient consumer spending and strong growth in cross-border transaction volume. For the full fiscal year ending September 30, 2024, Visa processed 289 billion transactions, with a total payments volume of $15.2 trillion. However, the company&#x27;s legal headwinds have intensified. In July 2025, a federal judge rejected a landmark $30 billion settlement that Visa and Mastercard had reached with merchants over long-disputed credit card swipe fees, sending the parties back to the negotiating table and extending the long-running legal battle.Simultaneously, Visa remains under investigation by the Department of Justice over its rules for routing debit card transactions, with regulators scrutinizing whether the company&#x27;s practices unlawfully limit merchant choice and stifle competition. These domestic challenges are mirrored abroad, where European regulators have continued their own antitrust investigations into the fee structures of both Visa and its primary competitor, Mastercard.Against this backdrop of regulatory pressure, Birwadker acknowledged that adoption of the Trusted Agent Protocol will take time. \"As agentic commerce continues to rise, we recognize that consumer trust is still in its early stages,\" he said. \"That&#x27;s why our focus through 2025 is on building foundational credibility and demonstrating real-world value.\"The protocol is available immediately in Visa&#x27;s Developer Center and on GitHub, with agent onboarding already active and merchant integration resources available. But Birwadker declined to provide specific targets for how many merchants might adopt the protocol by the end of 2026. \"Adoption is aligned with the momentum we&#x27;re already seeing,\" he said. \"The launch of our protocol marks another big step — it&#x27;s not just a technical milestone, but a signal that the industry is beginning to unify.\"Industry analysts say merchant adoption will likely depend on how quickly agentic commerce grows as a percentage of overall e-commerce. While AI-driven traffic has surged dramatically, much of that consists of agents browsing and researching rather than completing purchases. If AI agents begin accounting for a significant share of completed transactions, merchants will face stronger incentives to adopt verification systems like Visa&#x27;s protocol.From fraud detection to AI gatekeeping: Visa&#x27;s $10 billion bet on artificial intelligenceVisa&#x27;s move reflects broader strategic bets on AI across the financial services industry. The company has invested $10 billion in technology over the past five years to reduce fraud and increase network security, with AI and machine learning central to those efforts. Visa&#x27;s fraud detection system analyzes over 500 different attributes for each transaction, using AI models to assign real-time risk scores to the 300 billion annual transactions flowing through its network.\"Every single one of those transactions has been processed by AI,\" James Mirfin, Visa&#x27;s global head of risk and identity solutions, said in a July 2024 CNBC interview discussing the company&#x27;s fraud prevention efforts. \"If you see a new type of fraud happening, our model will see that, it will catch it, it will score those transactions as high risk and then our customers can decide not to approve those transactions.\"The company has also moved aggressively into new payment territories beyond its core card business. In January 2025, Visa partnered with Elon Musk&#x27;s X (formerly Twitter) to provide the infrastructure for a digital wallet and peer-to-peer payment service called the X Money Account, competing with services like Venmo and Zelle. That deal marked Visa&#x27;s first major partnership in the social media payments space and reflected the company&#x27;s recognition that payment flows are increasingly happening outside traditional e-commerce channels.The agentic commerce protocol represents an extension of this strategy — an attempt to ensure Visa remains central to payment flows even as the mechanics of shopping shift from direct human interaction to AI intermediation. Jack Forestell, Visa&#x27;s Chief Product & Strategy Officer, framed the protocol in expansive terms: \"We believe the entire payments ecosystem has a responsibility to ensure sellers trust AI agents with the same confidence they place in their most valued customers and networks.\"The coming battle for control of AI shoppingThe real test for Visa&#x27;s protocol won&#x27;t be technical — it will be political. As AI agents become a larger force in retail, whoever controls the verification infrastructure controls access to hundreds of billions of dollars in commerce. Visa&#x27;s position as gatekeeper gives it enormous leverage, but also makes it a target.Merchants chafing under Visa&#x27;s existing fee structure and facing multiple antitrust investigations may resist ceding even more power to the payments giant. Competitors like Google and OpenAI, each with their own ambitions in commerce, have little incentive to let Visa dictate standards. Regulators already scrutinizing Visa&#x27;s market dominance will surely examine whether its agent approval process unfairly advantages certain players.And there&#x27;s a deeper question lurking beneath the technical specifications and corporate partnerships: In an economy increasingly mediated by AI, who decides which algorithms get to spend our money? Visa is making an aggressive bid to be that arbiter, wrapping its answer in the language of security and interoperability. Whether merchants, consumers, and regulators accept that proposition will determine not just the fate of the Trusted Agent Protocol, but the structure of AI-powered commerce itself.For now, Visa is moving forward with the confidence of a company that has weathered disruption before. But in the emerging world of agentic commerce, being too trusted might prove just as dangerous as not being trusted enough.",
          "feed_position": 0,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/4TWzzYHcsJXmxU7VYnIlKV/896cc598ae137fe841c13309d6aae3df/nuneybits_Vector_art_of_a_Visa_credit_card_c3c9580a-d8ce-4402-9763-bd56dc26549e.webp"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/self-improving-language-models-are-becoming-reality-with-mits-updated-seal",
          "published_at": "Mon, 13 Oct 2025 22:51:00 GMT",
          "title": "Self-improving language models are becoming reality with MIT's updated SEAL technique",
          "standfirst": "Researchers at the Massachusetts Institute of Technology (MIT) are gaining renewed attention for developing and open sourcing a technique that allows large language models (LLMs) — like those underpinning ChatGPT and most modern AI chatbots — to improve themselves by generating synthetic data to fine-tune upon. The technique, known as SEAL (Self-Adapting LLMs), was first described in a paper published back in June and covered by VentureBeat at the time.A significantly expanded and updated version of the paper was released last month, as well as open source code posted on Github (under an MIT License, allowing for commercial and enterprise usage), and is making new waves among AI power users on the social network X this week.SEAL allows LLMs to autonomously generate and apply their own fine-tuning strategies. Unlike conventional models that rely on fixed external data and human-crafted optimization pipelines, SEAL enables models to evolve by producing their own synthetic training data and corresponding optimization directives.The development comes from a team affiliated with MIT’s Improbable AI Lab, including Adam Zweiger, Jyothish Pari, Han Guo, Ekin Akyürek, Yoon Kim, and Pulkit Agrawal. Their research was recently presented at the 39th Conference on Neural Information Processing Systems (NeurIPS 2025).Background: From “Beyond Static AI” to Self-Adaptive SystemsEarlier this year, VentureBeat first reported on SEAL as an early-stage framework that allowed language models to generate and train on their own synthetic data — a potential remedy for the stagnation of pretrained models once deployed. At that stage, SEAL was framed as a proof-of-concept that could let enterprise AI agents continuously learn in dynamic environments without manual retraining.Since then, the research has advanced considerably. The new version expands on the prior framework by demonstrating that SEAL’s self-adaptation ability scales with model size, integrates reinforcement learning more effectively to reduce catastrophic forgetting, and formalizes SEAL’s dual-loop structure (inner supervised fine-tuning and outer reinforcement optimization) for reproducibility. The updated paper also introduces evaluations across different prompting formats, improved stability during learning cycles, and a discussion of practical deployment challenges at inference time.Addressing the Limitations of Static ModelsWhile LLMs have demonstrated remarkable capabilities in text generation and understanding, their adaptation to new tasks or knowledge is often manual, brittle, or dependent on context. SEAL challenges this status quo by equipping models with the ability to generate what the authors call “self-edits” — natural language outputs that specify how the model should update its weights.These self-edits may take the form of reformulated information, logical implications, or tool configurations for augmentation and training. Once generated, the model fine-tunes itself based on these edits. The process is guided by reinforcement learning, where the reward signal comes from improved performance on a downstream task.The design mimics how human learners might rephrase or reorganize study materials to better internalize information. This restructuring of knowledge before assimilation serves as a key advantage over models that passively consume new data “as-is.”Performance Across TasksSEAL has been tested across two main domains: knowledge incorporation and few-shot learning.In the knowledge incorporation setting, the researchers evaluated how well a model could internalize new factual content from passages similar to those in the SQuAD dataset, a benchmark reading comprehension dataset introduced by Stanford University in 2016, consisting of over 100,000 crowd-sourced question–answer pairs based on Wikipedia articles (Rajpurkar et al., 2016). Rather than fine-tuning directly on passage text, the model generated synthetic implications of the passage and then fine-tuned on them. After two rounds of reinforcement learning, the model improved question-answering accuracy from 33.5% to 47.0% on a no-context version of SQuAD — surpassing results obtained using synthetic data generated by GPT-4.1.In the few-shot learning setting, SEAL was evaluated using a subset of the ARC benchmark, where tasks require reasoning from only a few examples. Here, SEAL generated self-edits specifying data augmentations and hyperparameters. After reinforcement learning, the success rate in correctly solving held-out tasks jumped to 72.5%, up from 20% using self-edits generated without reinforcement learning. Models that relied solely on in-context learning without any adaptation scored 0%.Technical FrameworkSEAL operates using a two-loop structure: an inner loop performs supervised fine-tuning based on the self-edit, while an outer loop uses reinforcement learning to refine the policy that generates those self-edits.The reinforcement learning algorithm used is based on ReSTEM, which combines sampling with filtered behavior cloning. During training, only self-edits that lead to performance improvements are reinforced. This approach effectively teaches the model which kinds of edits are most beneficial for learning.For efficiency, SEAL applies LoRA-based fine-tuning rather than full parameter updates, enabling rapid experimentation and low-cost adaptation.Strengths and LimitationsThe researchers report that SEAL can produce high-utility training data with minimal supervision, outperforming even large external models like GPT-4.1 in specific tasks. They also demonstrate that SEAL generalizes beyond its original setup: it continues to perform well when scaling from single-pass updates to multi-document continued pretraining scenarios.However, the framework is not without limitations. One issue is catastrophic forgetting, where updates to incorporate new information can degrade performance on previously learned tasks. In response to this concern, co-author Jyo Pari told VentureBeat via email that reinforcement learning (RL) appears to mitigate forgetting more effectively than standard supervised fine-tuning (SFT), citing a recent paper on the topic. He added that combining this insight with SEAL could lead to new variants where SEAL learns not just training data, but reward functions.Another challenge is computational overhead: evaluating each self-edit requires fine-tuning and performance testing, which can take 30–45 seconds per edit — significantly more than standard reinforcement learning tasks. As Jyo explained, “Training SEAL is non-trivial because it requires 2 loops of optimization, an outer RL one and an inner SFT one. At inference time, updating model weights will also require new systems infrastructure.” He emphasized the need for future research into deployment systems as a critical path to making SEAL practical.Additionally, SEAL’s current design assumes the presence of paired tasks and reference answers for every context, limiting its direct applicability to unlabeled corpora. However, Jyo clarified that as long as there is a downstream task with a computable reward, SEAL can be trained to adapt accordingly—even in safety-critical domains. In principle, a SEAL-trained model could learn to avoid training on harmful or malicious inputs if guided by the appropriate reward signal.AI Community ReactionsThe AI research and builder community has reacted with a mix of excitement and speculation to the SEAL paper. On X, formerly Twitter, several prominent AI-focused accounts weighed in on the potential impact.User @VraserX, a self-described educator and AI enthusiast, called SEAL “the birth of continuous self-learning AI” and predicted that models like OpenAI&#x27;s GPT-6 could adopt similar architecture. In their words, SEAL represents “the end of the frozen-weights era,” ushering in systems that evolve as the world around them changes. They highlighted SEAL&#x27;s ability to form persistent memories, repair knowledge, and learn from real-time data, comparing it to a foundational step toward models that don’t just use information but absorb it.Meanwhile, @alex_prompter, co-founder of an AI-powered marketing venture, framed SEAL as a leap toward models that literally rewrite themselves. “MIT just built an AI that can rewrite its own code to get smarter,” he wrote. Citing the paper’s key results — a 40% boost in factual recall and outperforming GPT-4.1 using self-generated data — he described the findings as confirmation that “LLMs that finetune themselves are no longer sci-fi.”The enthusiasm reflects a broader appetite in the AI space for models that can evolve without constant retraining or human oversight — particularly in rapidly changing domains or personalized use cases.Future Directions and Open QuestionsIn response to questions about scaling SEAL to larger models and tasks, Jyo pointed to experiments (Appendix B.7) showing that as model size increases, so does their self-adaptation ability. He compared this to students improving their study techniques over time — larger models are simply better at generating useful self-edits.When asked whether SEAL generalizes to new prompting styles, he confirmed it does, citing Table 10 in the paper. However, he also acknowledged that the team has not yet tested SEAL’s ability to transfer across entirely new domains or model architectures. “SEAL is an initial work showcasing the possibilities,” he said. “But it requires much more testing.” He added that generalization may improve as SEAL is trained on a broader distribution of tasks.Interestingly, the team found that only a few reinforcement learning steps already led to measurable performance gains. “This is exciting,” Jyo noted, “because it means that with more compute, we could hopefully get even more improvements.” He suggested future experiments could explore more advanced reinforcement learning methods beyond ReSTEM, such as Group Relative Policy Optimization (GRPO).Toward More Adaptive and Agentic ModelsSEAL represents a step toward models that can autonomously improve over time, both by integrating new knowledge and by reconfiguring how they learn. The authors envision future extensions where SEAL could assist in self-pretraining, continual learning, and the development of agentic systems — models that interact with evolving environments and adapt incrementally.In such settings, a model could use SEAL to synthesize weight updates after each interaction, gradually internalizing behaviors or insights. This could reduce the need for repeated supervision and manual intervention, particularly in data-constrained or specialized domains.As public web text becomes saturated and further scaling of LLMs becomes bottlenecked by data availability, self-directed approaches like SEAL could play a critical role in pushing the boundaries of what LLMs can achieve.You can access the SEAL project, including code and further documentation, at: https://jyopari.github.io/posts/seal",
          "content": "Researchers at the Massachusetts Institute of Technology (MIT) are gaining renewed attention for developing and open sourcing a technique that allows large language models (LLMs) — like those underpinning ChatGPT and most modern AI chatbots — to improve themselves by generating synthetic data to fine-tune upon. The technique, known as SEAL (Self-Adapting LLMs), was first described in a paper published back in June and covered by VentureBeat at the time.A significantly expanded and updated version of the paper was released last month, as well as open source code posted on Github (under an MIT License, allowing for commercial and enterprise usage), and is making new waves among AI power users on the social network X this week.SEAL allows LLMs to autonomously generate and apply their own fine-tuning strategies. Unlike conventional models that rely on fixed external data and human-crafted optimization pipelines, SEAL enables models to evolve by producing their own synthetic training data and corresponding optimization directives.The development comes from a team affiliated with MIT’s Improbable AI Lab, including Adam Zweiger, Jyothish Pari, Han Guo, Ekin Akyürek, Yoon Kim, and Pulkit Agrawal. Their research was recently presented at the 39th Conference on Neural Information Processing Systems (NeurIPS 2025).Background: From “Beyond Static AI” to Self-Adaptive SystemsEarlier this year, VentureBeat first reported on SEAL as an early-stage framework that allowed language models to generate and train on their own synthetic data — a potential remedy for the stagnation of pretrained models once deployed. At that stage, SEAL was framed as a proof-of-concept that could let enterprise AI agents continuously learn in dynamic environments without manual retraining.Since then, the research has advanced considerably. The new version expands on the prior framework by demonstrating that SEAL’s self-adaptation ability scales with model size, integrates reinforcement learning more effectively to reduce catastrophic forgetting, and formalizes SEAL’s dual-loop structure (inner supervised fine-tuning and outer reinforcement optimization) for reproducibility. The updated paper also introduces evaluations across different prompting formats, improved stability during learning cycles, and a discussion of practical deployment challenges at inference time.Addressing the Limitations of Static ModelsWhile LLMs have demonstrated remarkable capabilities in text generation and understanding, their adaptation to new tasks or knowledge is often manual, brittle, or dependent on context. SEAL challenges this status quo by equipping models with the ability to generate what the authors call “self-edits” — natural language outputs that specify how the model should update its weights.These self-edits may take the form of reformulated information, logical implications, or tool configurations for augmentation and training. Once generated, the model fine-tunes itself based on these edits. The process is guided by reinforcement learning, where the reward signal comes from improved performance on a downstream task.The design mimics how human learners might rephrase or reorganize study materials to better internalize information. This restructuring of knowledge before assimilation serves as a key advantage over models that passively consume new data “as-is.”Performance Across TasksSEAL has been tested across two main domains: knowledge incorporation and few-shot learning.In the knowledge incorporation setting, the researchers evaluated how well a model could internalize new factual content from passages similar to those in the SQuAD dataset, a benchmark reading comprehension dataset introduced by Stanford University in 2016, consisting of over 100,000 crowd-sourced question–answer pairs based on Wikipedia articles (Rajpurkar et al., 2016). Rather than fine-tuning directly on passage text, the model generated synthetic implications of the passage and then fine-tuned on them. After two rounds of reinforcement learning, the model improved question-answering accuracy from 33.5% to 47.0% on a no-context version of SQuAD — surpassing results obtained using synthetic data generated by GPT-4.1.In the few-shot learning setting, SEAL was evaluated using a subset of the ARC benchmark, where tasks require reasoning from only a few examples. Here, SEAL generated self-edits specifying data augmentations and hyperparameters. After reinforcement learning, the success rate in correctly solving held-out tasks jumped to 72.5%, up from 20% using self-edits generated without reinforcement learning. Models that relied solely on in-context learning without any adaptation scored 0%.Technical FrameworkSEAL operates using a two-loop structure: an inner loop performs supervised fine-tuning based on the self-edit, while an outer loop uses reinforcement learning to refine the policy that generates those self-edits.The reinforcement learning algorithm used is based on ReSTEM, which combines sampling with filtered behavior cloning. During training, only self-edits that lead to performance improvements are reinforced. This approach effectively teaches the model which kinds of edits are most beneficial for learning.For efficiency, SEAL applies LoRA-based fine-tuning rather than full parameter updates, enabling rapid experimentation and low-cost adaptation.Strengths and LimitationsThe researchers report that SEAL can produce high-utility training data with minimal supervision, outperforming even large external models like GPT-4.1 in specific tasks. They also demonstrate that SEAL generalizes beyond its original setup: it continues to perform well when scaling from single-pass updates to multi-document continued pretraining scenarios.However, the framework is not without limitations. One issue is catastrophic forgetting, where updates to incorporate new information can degrade performance on previously learned tasks. In response to this concern, co-author Jyo Pari told VentureBeat via email that reinforcement learning (RL) appears to mitigate forgetting more effectively than standard supervised fine-tuning (SFT), citing a recent paper on the topic. He added that combining this insight with SEAL could lead to new variants where SEAL learns not just training data, but reward functions.Another challenge is computational overhead: evaluating each self-edit requires fine-tuning and performance testing, which can take 30–45 seconds per edit — significantly more than standard reinforcement learning tasks. As Jyo explained, “Training SEAL is non-trivial because it requires 2 loops of optimization, an outer RL one and an inner SFT one. At inference time, updating model weights will also require new systems infrastructure.” He emphasized the need for future research into deployment systems as a critical path to making SEAL practical.Additionally, SEAL’s current design assumes the presence of paired tasks and reference answers for every context, limiting its direct applicability to unlabeled corpora. However, Jyo clarified that as long as there is a downstream task with a computable reward, SEAL can be trained to adapt accordingly—even in safety-critical domains. In principle, a SEAL-trained model could learn to avoid training on harmful or malicious inputs if guided by the appropriate reward signal.AI Community ReactionsThe AI research and builder community has reacted with a mix of excitement and speculation to the SEAL paper. On X, formerly Twitter, several prominent AI-focused accounts weighed in on the potential impact.User @VraserX, a self-described educator and AI enthusiast, called SEAL “the birth of continuous self-learning AI” and predicted that models like OpenAI&#x27;s GPT-6 could adopt similar architecture. In their words, SEAL represents “the end of the frozen-weights era,” ushering in systems that evolve as the world around them changes. They highlighted SEAL&#x27;s ability to form persistent memories, repair knowledge, and learn from real-time data, comparing it to a foundational step toward models that don’t just use information but absorb it.Meanwhile, @alex_prompter, co-founder of an AI-powered marketing venture, framed SEAL as a leap toward models that literally rewrite themselves. “MIT just built an AI that can rewrite its own code to get smarter,” he wrote. Citing the paper’s key results — a 40% boost in factual recall and outperforming GPT-4.1 using self-generated data — he described the findings as confirmation that “LLMs that finetune themselves are no longer sci-fi.”The enthusiasm reflects a broader appetite in the AI space for models that can evolve without constant retraining or human oversight — particularly in rapidly changing domains or personalized use cases.Future Directions and Open QuestionsIn response to questions about scaling SEAL to larger models and tasks, Jyo pointed to experiments (Appendix B.7) showing that as model size increases, so does their self-adaptation ability. He compared this to students improving their study techniques over time — larger models are simply better at generating useful self-edits.When asked whether SEAL generalizes to new prompting styles, he confirmed it does, citing Table 10 in the paper. However, he also acknowledged that the team has not yet tested SEAL’s ability to transfer across entirely new domains or model architectures. “SEAL is an initial work showcasing the possibilities,” he said. “But it requires much more testing.” He added that generalization may improve as SEAL is trained on a broader distribution of tasks.Interestingly, the team found that only a few reinforcement learning steps already led to measurable performance gains. “This is exciting,” Jyo noted, “because it means that with more compute, we could hopefully get even more improvements.” He suggested future experiments could explore more advanced reinforcement learning methods beyond ReSTEM, such as Group Relative Policy Optimization (GRPO).Toward More Adaptive and Agentic ModelsSEAL represents a step toward models that can autonomously improve over time, both by integrating new knowledge and by reconfiguring how they learn. The authors envision future extensions where SEAL could assist in self-pretraining, continual learning, and the development of agentic systems — models that interact with evolving environments and adapt incrementally.In such settings, a model could use SEAL to synthesize weight updates after each interaction, gradually internalizing behaviors or insights. This could reduce the need for repeated supervision and manual intervention, particularly in data-constrained or specialized domains.As public web text becomes saturated and further scaling of LLMs becomes bottlenecked by data availability, self-directed approaches like SEAL could play a critical role in pushing the boundaries of what LLMs can achieve.You can access the SEAL project, including code and further documentation, at: https://jyopari.github.io/posts/seal",
          "feed_position": 1,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/4816Y0YfsXKIENLGFsuaG6/a4620bd99d25c8fe32ab054bd16ff390/cfr0z3n_a_cybernetic_seal_looks_up_with_cute_alert_eyes_under_a_a6f43d56-7792-4d4f-bc1e-18b6dd2f5e4e.png"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/researchers-find-that-retraining-only-small-parts-of-ai-models-can-cut-costs",
          "published_at": "Mon, 13 Oct 2025 22:39:00 GMT",
          "title": "Researchers find that retraining only small parts of AI models can cut costs and prevent forgetting",
          "standfirst": "Enterprises often find that when they fine-tune models, one effective approach to making a large language model (LLM) fit for purpose and grounded in data is to have the model lose some of its abilities. After fine-tuning, some models “forget” how to perform certain tasks or other tasks they already learned. Research from the University of Illinois Urbana-Champaign proposes a new method for retraining models that avoids “catastrophic forgetting,” in which the model loses some of its prior knowledge. The paper focuses on two specific LLMs that generate responses from images: LLaVA and Qwen 2.5-VL.The approach encourages enterprises to retrain only narrow parts of an LLM to avoid retraining the entire model and incurring a significant increase in compute costs. The team claims that catastrophic forgetting isn’t true memory loss, but rather a side effect of bias drift. “Training a new LMM can cost millions of dollars, weeks of time, and emit hundreds of tons of CO2, so finding ways to more efficiently and effectively update existing models is a pressing concern,” the team wrote in the paper. “Guided by this result, we explore tuning recipes that preserve learning while limiting output shift.”The researchers focused on a multi-layer perceptron (MLP), the model&#x27;s internal decision-making component. Catastrophic forgetting The researchers wanted first to verify the existence and the cause of catastrophic forgetting in models. To do this, they created a set of target tasks for the models to complete. The models were then fine-tuned and evaluated to determine whether they led to substantial forgetting. But as the process went on, the researchers found that the models were recovering some of their abilities. “We also noticed a surprising result, that the model performance would drop significantly in held out benchmarks after training on the counting task, it would mostly recover on PathVQA, another specialized task that is not well represented in the benchmarks,” they said. “Meanwhile, while performing the forgetting mitigation experiments, we also tried separately tuning only the self-attention projection (SA Proj) or MLP layers, motivated by the finding that tuning only the LLM was generally better than tuning the full model. This led to another very surprising result – that tuning only self-attention projection layers led to very good learning of the target tasks with no drop in performance in held out tasks, even after training all five target tasks in a sequence.”The researchers said they believe that “what looks like forgetting or interference after fine-tuning on a narrow target task is actually bias in the output distribution due to the task distribution shift.”Narrow retrainingThat finding turned out to be the key to the experiment. The researchers noted that tuning the MLP increases the likelihood of “outputting numeric tokens and a highly correlated drop in held out task accuracy.” What it showed is that a model forgetting some of its knowledge is only temporary and not a long-term matter. “To avoid biasing the output distribution, we tune the MLP up/gating projections while keeping the down projection frozen, and find that it achieves similar learning to full MLP tuning with little forgetting,” the researchers said. This allows for a more straightforward and more reproducible method for fine-tuning a model. By focusing on a narrow segment of the model, rather than a wholesale retraining, enterprises can cut compute costs. It also allows better control of output drift. However, the research focuses only on two models, specifically those dealing with vision and language. The researchers noted that due to limited resources, they are unable to try the experiment with other models.Their findings, however, can be extended to other LLMs, especially for different modalities.",
          "content": "Enterprises often find that when they fine-tune models, one effective approach to making a large language model (LLM) fit for purpose and grounded in data is to have the model lose some of its abilities. After fine-tuning, some models “forget” how to perform certain tasks or other tasks they already learned. Research from the University of Illinois Urbana-Champaign proposes a new method for retraining models that avoids “catastrophic forgetting,” in which the model loses some of its prior knowledge. The paper focuses on two specific LLMs that generate responses from images: LLaVA and Qwen 2.5-VL.The approach encourages enterprises to retrain only narrow parts of an LLM to avoid retraining the entire model and incurring a significant increase in compute costs. The team claims that catastrophic forgetting isn’t true memory loss, but rather a side effect of bias drift. “Training a new LMM can cost millions of dollars, weeks of time, and emit hundreds of tons of CO2, so finding ways to more efficiently and effectively update existing models is a pressing concern,” the team wrote in the paper. “Guided by this result, we explore tuning recipes that preserve learning while limiting output shift.”The researchers focused on a multi-layer perceptron (MLP), the model&#x27;s internal decision-making component. Catastrophic forgetting The researchers wanted first to verify the existence and the cause of catastrophic forgetting in models. To do this, they created a set of target tasks for the models to complete. The models were then fine-tuned and evaluated to determine whether they led to substantial forgetting. But as the process went on, the researchers found that the models were recovering some of their abilities. “We also noticed a surprising result, that the model performance would drop significantly in held out benchmarks after training on the counting task, it would mostly recover on PathVQA, another specialized task that is not well represented in the benchmarks,” they said. “Meanwhile, while performing the forgetting mitigation experiments, we also tried separately tuning only the self-attention projection (SA Proj) or MLP layers, motivated by the finding that tuning only the LLM was generally better than tuning the full model. This led to another very surprising result – that tuning only self-attention projection layers led to very good learning of the target tasks with no drop in performance in held out tasks, even after training all five target tasks in a sequence.”The researchers said they believe that “what looks like forgetting or interference after fine-tuning on a narrow target task is actually bias in the output distribution due to the task distribution shift.”Narrow retrainingThat finding turned out to be the key to the experiment. The researchers noted that tuning the MLP increases the likelihood of “outputting numeric tokens and a highly correlated drop in held out task accuracy.” What it showed is that a model forgetting some of its knowledge is only temporary and not a long-term matter. “To avoid biasing the output distribution, we tune the MLP up/gating projections while keeping the down projection frozen, and find that it achieves similar learning to full MLP tuning with little forgetting,” the researchers said. This allows for a more straightforward and more reproducible method for fine-tuning a model. By focusing on a narrow segment of the model, rather than a wholesale retraining, enterprises can cut compute costs. It also allows better control of output drift. However, the research focuses only on two models, specifically those dealing with vision and language. The researchers noted that due to limited resources, they are unable to try the experiment with other models.Their findings, however, can be extended to other LLMs, especially for different modalities.",
          "feed_position": 2,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/wvhlzgUGzNikEzmtJxvNB/ed69fb909b090e1e7e6b81ff61abf8b0/crimedy7_illustration_of_a_sculptor_creating_a_robot_from_a_p_501bf165-0b44-4bb1-9608-1025a42400b7_1.png"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/computing/windows-10-support-ends-tomorrow-but-heres-how-to-get-an-extra-year-for-free-125118505.html",
          "published_at": "Mon, 13 Oct 2025 14:25:30 +0000",
          "title": "Windows 10 support ends tomorrow, but here's how to get an extra year for free",
          "standfirst": "You'll get access to Windows 10 a little longer by doing this. (Getty Images) Are you still running Windows 10 on your PC? Starting tomorrow, October 14, Microsoft is moving the software to \"end of life\" status. What does that mean for you? While Windows 10 PCs will continue to work after that date, they'll stop getting important security updates by default. That leaves you with three choices if you want to make sure your computer remains secure: You can choose to upgrade to Windows 11 for free if your computer is compatible. You can buy a new PC that already has Windows 11 pre-installed (or opt for an alternative, like a Mac or a Chromebook). Forget about Windows 11 right now and sign up for the Extended Security Updates (ESU), which lets you kick the can down the road for a year. The last one is easy — and can now be done for free in many cases — so we'll focus on that one here. We'll walk you through the steps of keeping Windows 10 on your PC… for now, at least. How to sign up for Windows 10 Extended Security Updates on your computer We can question Microsoft's motives for killing off Windows 10, even though it works perfectly well on most older PCs. But without those periodic security updates, your PC will become increasingly susceptible to malware with each passing week. To that end, enrolling in Extended Security Updates (ESU) will give you another year of using Windows 10 securely. At one point, Microsoft suggested the 12-month extension would require a $30 fee. While that's still an option, there's now a free path for Windows 10 users in the US. Here's how to make it happen. Step 1: Make sure your PC is up to date You can find out if your computer is up-to-date by going into your Settings > System > About, then scroll down to see what version you're running. If not, you'll want to make sure you also install all the Windows 10 updates available. Step 2: Make sure you're using an administrator account If you share a computer with multiple people in your household, make sure you're signed in to the administrator account. Typically, it's the first account created on the computer. You'll know it's the right one when you see \"Administrator\" under the name. (You can double-check under Settings > Your Info.) Step 3: Verify if your PC is eligible to upgrade to Windows 11 (or not) If you see an option to upgrade to Windows 11, just do that. It's free and it keeps you in the Windows loop. Otherwise, continue following the steps below so you can keep your computer safe with security updates. Step 4: Enroll in Extended Security Updates Sign up for ESU by selecting Update & Security from the Settings menu. Click the \"Enroll Now\" sign-up link, as pictured below. Again, you may see an option to download Windows 11 if your computer meets the requirements (again, definitely do that if you see it). Find out if you need to update your computer. (Screenshot/Engadget) If you're not seeing the \"Enroll now\" link, you probably need to update and install the latest Windows 10 updates (as noted above). By enrolling in Extended Security Updates, you'll have another year before you need to upgrade to Windows 11. (Screenshots/Engadget) Step 5: Choose your upgrade method Next up is choosing how you want to enroll, and you have a few options. The easiest way is to back up your PC settings. It's free, but it takes a little bit of time since you'll need to back up your data. Again, you'll need to be using your administrator account to get started. Back up your PC before you enroll in ESU. (ExplainingComputers via YouTube) That said, the free option here comes with two catches, at least for users in the US. (European users will get the free option with no strings attached.) The first is that you'll be linking your Windows login to Microsoft's cloud-based online service. Most users have likely already done this (if they're using CoPilot, Office 365, GamePass, OneDrive or one of Microsoft's other various online services). But if you've specifically opted for a local login to Windows, the price you're paying for this \"free\" extension is joining the cloud-connected Microsoft universe. The other potential issue is that the free backup only applies to the first 5 GB of storage. Anything more, and you’ll need to pay up for Microsoft's OneDrive services. But thankfully, you can turn off anything you don't want to back up by going to Settings > OneDrive and toggling off options like Documents, Pictures and Videos to get in under the free threshold to start. Once you're signed in, a window will pop up that says \"Add this device to receive Extended Security Updates.\" Click Add Device to enroll it. Click Done. A note: Thanks to YouTube's Explaining Computers channel, where we grabbed the screenshot above (since our test PC was already signed up for cloud backups, and didn't provide the splash screen to choose options). You can watch their full video if you'd like a deeper dive into the process. That's it, you're done! (Until next year) You've got 12 more months to figure out an alternative upgrade path to Windows 11. If anything changes next year, we'll update this story with what your next steps are. You did it right if you see this window. (Screenshot/Engadget) This article originally appeared on Engadget at https://www.engadget.com/computing/windows-10-support-ends-tomorrow-but-heres-how-to-get-an-extra-year-for-free-125118505.html?src=rss",
          "content": "You'll get access to Windows 10 a little longer by doing this. (Getty Images) Are you still running Windows 10 on your PC? Starting tomorrow, October 14, Microsoft is moving the software to \"end of life\" status. What does that mean for you? While Windows 10 PCs will continue to work after that date, they'll stop getting important security updates by default. That leaves you with three choices if you want to make sure your computer remains secure: You can choose to upgrade to Windows 11 for free if your computer is compatible. You can buy a new PC that already has Windows 11 pre-installed (or opt for an alternative, like a Mac or a Chromebook). Forget about Windows 11 right now and sign up for the Extended Security Updates (ESU), which lets you kick the can down the road for a year. The last one is easy — and can now be done for free in many cases — so we'll focus on that one here. We'll walk you through the steps of keeping Windows 10 on your PC… for now, at least. How to sign up for Windows 10 Extended Security Updates on your computer We can question Microsoft's motives for killing off Windows 10, even though it works perfectly well on most older PCs. But without those periodic security updates, your PC will become increasingly susceptible to malware with each passing week. To that end, enrolling in Extended Security Updates (ESU) will give you another year of using Windows 10 securely. At one point, Microsoft suggested the 12-month extension would require a $30 fee. While that's still an option, there's now a free path for Windows 10 users in the US. Here's how to make it happen. Step 1: Make sure your PC is up to date You can find out if your computer is up-to-date by going into your Settings > System > About, then scroll down to see what version you're running. If not, you'll want to make sure you also install all the Windows 10 updates available. Step 2: Make sure you're using an administrator account If you share a computer with multiple people in your household, make sure you're signed in to the administrator account. Typically, it's the first account created on the computer. You'll know it's the right one when you see \"Administrator\" under the name. (You can double-check under Settings > Your Info.) Step 3: Verify if your PC is eligible to upgrade to Windows 11 (or not) If you see an option to upgrade to Windows 11, just do that. It's free and it keeps you in the Windows loop. Otherwise, continue following the steps below so you can keep your computer safe with security updates. Step 4: Enroll in Extended Security Updates Sign up for ESU by selecting Update & Security from the Settings menu. Click the \"Enroll Now\" sign-up link, as pictured below. Again, you may see an option to download Windows 11 if your computer meets the requirements (again, definitely do that if you see it). Find out if you need to update your computer. (Screenshot/Engadget) If you're not seeing the \"Enroll now\" link, you probably need to update and install the latest Windows 10 updates (as noted above). By enrolling in Extended Security Updates, you'll have another year before you need to upgrade to Windows 11. (Screenshots/Engadget) Step 5: Choose your upgrade method Next up is choosing how you want to enroll, and you have a few options. The easiest way is to back up your PC settings. It's free, but it takes a little bit of time since you'll need to back up your data. Again, you'll need to be using your administrator account to get started. Back up your PC before you enroll in ESU. (ExplainingComputers via YouTube) That said, the free option here comes with two catches, at least for users in the US. (European users will get the free option with no strings attached.) The first is that you'll be linking your Windows login to Microsoft's cloud-based online service. Most users have likely already done this (if they're using CoPilot, Office 365, GamePass, OneDrive or one of Microsoft's other various online services). But if you've specifically opted for a local login to Windows, the price you're paying for this \"free\" extension is joining the cloud-connected Microsoft universe. The other potential issue is that the free backup only applies to the first 5 GB of storage. Anything more, and you’ll need to pay up for Microsoft's OneDrive services. But thankfully, you can turn off anything you don't want to back up by going to Settings > OneDrive and toggling off options like Documents, Pictures and Videos to get in under the free threshold to start. Once you're signed in, a window will pop up that says \"Add this device to receive Extended Security Updates.\" Click Add Device to enroll it. Click Done. A note: Thanks to YouTube's Explaining Computers channel, where we grabbed the screenshot above (since our test PC was already signed up for cloud backups, and didn't provide the splash screen to choose options). You can watch their full video if you'd like a deeper dive into the process. That's it, you're done! (Until next year) You've got 12 more months to figure out an alternative upgrade path to Windows 11. If anything changes next year, we'll update this story with what your next steps are. You did it right if you see this window. (Screenshot/Engadget) This article originally appeared on Engadget at https://www.engadget.com/computing/windows-10-support-ends-tomorrow-but-heres-how-to-get-an-extra-year-for-free-125118505.html?src=rss",
          "feed_position": 12,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2025-10/c836b6e0-a60d-11f0-aff0-71a091f199fd"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/this-new-ai-technique-creates-digital-twin-consumers-and-it-could-kill-the",
          "published_at": "Mon, 13 Oct 2025 13:00:00 GMT",
          "title": "This new AI technique creates ‘digital twin’ consumers, and it could kill the traditional survey industry",
          "standfirst": "A new research paper quietly published last week outlines a breakthrough method that allows large language models (LLMs) to simulate human consumer behavior with startling accuracy, a development that could reshape the multi-billion-dollar market research industry. The technique promises to create armies of synthetic consumers who can provide not just realistic product ratings, but also the qualitative reasoning behind them, at a scale and speed currently unattainable.For years, companies have sought to use AI for market research, but have been stymied by a fundamental flaw: when asked to provide a numerical rating on a scale of 1 to 5, LLMs produce unrealistic and poorly distributed responses. A new paper, \"LLMs Reproduce Human Purchase Intent via Semantic Similarity Elicitation of Likert Ratings,\" submitted to the pre-print server arXiv on October 9th proposes an elegant solution that sidesteps this problem entirely.The international team of researchers, led by Benjamin F. Maier, developed a method they call semantic similarity rating (SSR). Instead of asking an LLM for a number, SSR prompts the model for a rich, textual opinion on a product. This text is then converted into a numerical vector — an \"embedding\" — and its similarity is measured against a set of pre-defined reference statements. For example, a response of \"I would absolutely buy this, it&#x27;s exactly what I&#x27;m looking for\" would be semantically closer to the reference statement for a \"5\" rating than to the statement for a \"1.\"The results are striking. Tested against a massive real-world dataset from a leading personal care corporation — comprising 57 product surveys and 9,300 human responses — the SSR method achieved 90% of human test-retest reliability. Crucially, the distribution of AI-generated ratings was statistically almost indistinguishable from the human panel. The authors state, \"This framework enables scalable consumer research simulations while preserving traditional survey metrics and interpretability.\"A timely solution as AI threatens survey integrityThis development arrives at a critical time, as the integrity of traditional online survey panels is increasingly under threat from AI. A 2024 analysis from the Stanford Graduate School of Business highlighted a growing problem of human survey-takers using chatbots to generate their answers. These AI-generated responses were found to be \"suspiciously nice,\" overly verbose, and lacking the \"snark\" and authenticity of genuine human feedback, leading to what researchers called a \"homogenization\" of data that could mask serious issues like discrimination or product flaws.Maier&#x27;s research offers a starkly different approach: instead of fighting to purge contaminated data, it creates a controlled environment for generating high-fidelity synthetic data from the ground up.\"What we&#x27;re seeing is a pivot from defense to offense,\" said one analyst not affiliated with the study. \"The Stanford paper showed the chaos of uncontrolled AI polluting human datasets. This new paper shows the order and utility of controlled AI creating its own datasets. For a Chief Data Officer, this is the difference between cleaning a contaminated well and tapping into a fresh spring.\"From text to intent: The technical leap behind the synthetic consumerThe technical validity of the new method hinges on the quality of the text embeddings, a concept explored in a 2022 paper in EPJ Data Science. That research argued for a rigorous \"construct validity\" framework to ensure that text embeddings — the numerical representations of text — truly \"measure what they are supposed to.\" The success of the SSR method suggests its embeddings effectively capture the nuances of purchase intent. For this new technique to be widely adopted, enterprises will need to be confident that the underlying models are not just generating plausible text, but are mapping that text to scores in a way that is robust and meaningful.The approach also represents a significant leap from prior research, which has largely focused on using text embeddings to analyze and predict ratings from existing online reviews. A 2022 study, for example, evaluated the performance of models like BERT and word2vec in predicting review scores on retail sites, finding that newer models like BERT performed better for general use. The new research moves beyond analyzing existing data to generating novel, predictive insights before a product even hits the market.The dawn of the digital focus groupFor technical decision-makers, the implications are profound. The ability to spin up a \"digital twin\" of a target consumer segment and test product concepts, ad copy, or packaging variations in a matter of hours could drastically accelerate innovation cycles. As the paper notes, these synthetic respondents also provide \"rich qualitative feedback explaining their ratings,\" offering a treasure trove of data for product development that is both scalable and interpretable. While the era of human-only focus groups is far from over, this research provides the most compelling evidence yet that their synthetic counterparts are ready for business.But the business case extends beyond speed and scale. Consider the economics: a traditional survey panel for a national product launch might cost tens of thousands of dollars and take weeks to field. An SSR-based simulation could deliver comparable insights in a fraction of the time, at a fraction of the cost, and with the ability to iterate instantly based on findings. For companies in fast-moving consumer goods categories — where the window between concept and shelf can determine market leadership — this velocity advantage could be decisive.There are, of course, caveats. The method was validated on personal care products; its performance on complex B2B purchasing decisions, luxury goods, or culturally specific products remains unproven. And while the paper demonstrates that SSR can replicate aggregate human behavior, it does not claim to predict individual consumer choices. The technique works at the population level, not the person level — a distinction that matters greatly for applications like personalized marketing.Yet even with these limitations, the research is a watershed. While the era of human-only focus groups is far from over, this paper provides the most compelling evidence yet that their synthetic counterparts are ready for business. The question is no longer whether AI can simulate consumer sentiment, but whether enterprises can move fast enough to capitalize on it before their competitors do.",
          "content": "A new research paper quietly published last week outlines a breakthrough method that allows large language models (LLMs) to simulate human consumer behavior with startling accuracy, a development that could reshape the multi-billion-dollar market research industry. The technique promises to create armies of synthetic consumers who can provide not just realistic product ratings, but also the qualitative reasoning behind them, at a scale and speed currently unattainable.For years, companies have sought to use AI for market research, but have been stymied by a fundamental flaw: when asked to provide a numerical rating on a scale of 1 to 5, LLMs produce unrealistic and poorly distributed responses. A new paper, \"LLMs Reproduce Human Purchase Intent via Semantic Similarity Elicitation of Likert Ratings,\" submitted to the pre-print server arXiv on October 9th proposes an elegant solution that sidesteps this problem entirely.The international team of researchers, led by Benjamin F. Maier, developed a method they call semantic similarity rating (SSR). Instead of asking an LLM for a number, SSR prompts the model for a rich, textual opinion on a product. This text is then converted into a numerical vector — an \"embedding\" — and its similarity is measured against a set of pre-defined reference statements. For example, a response of \"I would absolutely buy this, it&#x27;s exactly what I&#x27;m looking for\" would be semantically closer to the reference statement for a \"5\" rating than to the statement for a \"1.\"The results are striking. Tested against a massive real-world dataset from a leading personal care corporation — comprising 57 product surveys and 9,300 human responses — the SSR method achieved 90% of human test-retest reliability. Crucially, the distribution of AI-generated ratings was statistically almost indistinguishable from the human panel. The authors state, \"This framework enables scalable consumer research simulations while preserving traditional survey metrics and interpretability.\"A timely solution as AI threatens survey integrityThis development arrives at a critical time, as the integrity of traditional online survey panels is increasingly under threat from AI. A 2024 analysis from the Stanford Graduate School of Business highlighted a growing problem of human survey-takers using chatbots to generate their answers. These AI-generated responses were found to be \"suspiciously nice,\" overly verbose, and lacking the \"snark\" and authenticity of genuine human feedback, leading to what researchers called a \"homogenization\" of data that could mask serious issues like discrimination or product flaws.Maier&#x27;s research offers a starkly different approach: instead of fighting to purge contaminated data, it creates a controlled environment for generating high-fidelity synthetic data from the ground up.\"What we&#x27;re seeing is a pivot from defense to offense,\" said one analyst not affiliated with the study. \"The Stanford paper showed the chaos of uncontrolled AI polluting human datasets. This new paper shows the order and utility of controlled AI creating its own datasets. For a Chief Data Officer, this is the difference between cleaning a contaminated well and tapping into a fresh spring.\"From text to intent: The technical leap behind the synthetic consumerThe technical validity of the new method hinges on the quality of the text embeddings, a concept explored in a 2022 paper in EPJ Data Science. That research argued for a rigorous \"construct validity\" framework to ensure that text embeddings — the numerical representations of text — truly \"measure what they are supposed to.\" The success of the SSR method suggests its embeddings effectively capture the nuances of purchase intent. For this new technique to be widely adopted, enterprises will need to be confident that the underlying models are not just generating plausible text, but are mapping that text to scores in a way that is robust and meaningful.The approach also represents a significant leap from prior research, which has largely focused on using text embeddings to analyze and predict ratings from existing online reviews. A 2022 study, for example, evaluated the performance of models like BERT and word2vec in predicting review scores on retail sites, finding that newer models like BERT performed better for general use. The new research moves beyond analyzing existing data to generating novel, predictive insights before a product even hits the market.The dawn of the digital focus groupFor technical decision-makers, the implications are profound. The ability to spin up a \"digital twin\" of a target consumer segment and test product concepts, ad copy, or packaging variations in a matter of hours could drastically accelerate innovation cycles. As the paper notes, these synthetic respondents also provide \"rich qualitative feedback explaining their ratings,\" offering a treasure trove of data for product development that is both scalable and interpretable. While the era of human-only focus groups is far from over, this research provides the most compelling evidence yet that their synthetic counterparts are ready for business.But the business case extends beyond speed and scale. Consider the economics: a traditional survey panel for a national product launch might cost tens of thousands of dollars and take weeks to field. An SSR-based simulation could deliver comparable insights in a fraction of the time, at a fraction of the cost, and with the ability to iterate instantly based on findings. For companies in fast-moving consumer goods categories — where the window between concept and shelf can determine market leadership — this velocity advantage could be decisive.There are, of course, caveats. The method was validated on personal care products; its performance on complex B2B purchasing decisions, luxury goods, or culturally specific products remains unproven. And while the paper demonstrates that SSR can replicate aggregate human behavior, it does not claim to predict individual consumer choices. The technique works at the population level, not the person level — a distinction that matters greatly for applications like personalized marketing.Yet even with these limitations, the research is a watershed. While the era of human-only focus groups is far from over, this paper provides the most compelling evidence yet that their synthetic counterparts are ready for business. The question is no longer whether AI can simulate consumer sentiment, but whether enterprises can move fast enough to capitalize on it before their competitors do.",
          "feed_position": 3,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/51ORdJ4l6mg6ZSTkJpGFOw/85fa4483c879359d1872778236f4fc20/nuneybits_Vector_art_of_AI_shopper_silhouette_e82a806b-f70d-4b1d-9418-71b2ffd65ea4.webp"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/salesforce-bets-on-ai-agents-to-fix-what-it-calls-a-usd7-billion-problem-in",
          "published_at": "Mon, 13 Oct 2025 12:00:00 GMT",
          "title": "Salesforce bets on AI 'agents' to fix what it calls a $7 billion problem in enterprise software",
          "standfirst": "As 50,000 attendees descend on Salesforce&#x27;s Dreamforce conference this week, the enterprise software giant is making its most aggressive bet yet on artificial intelligence agents, positioning itself as the antidote to what it calls an industry-wide \"pilot purgatory\" where 95% of enterprise AI projects never reach production.The company on Monday launched Agentforce 360, a sweeping reimagination of its entire product portfolio designed to transform businesses into what it calls \"agentic enterprises\" — organizations where AI agents work alongside humans to handle up to 40% of work across sales, service, marketing, and operations.\"We are truly in the agentic AI era, and I think it&#x27;s probably the biggest revolution, the biggest transition in technology I&#x27;ve ever experienced in my career,\" said Parker Harris, Salesforce&#x27;s co-founder and chief technology officer, during a recent press briefing. \"In the future, 40% of the work in the Fortune 1000 is probably going to be done by AI, and it&#x27;s going to be humans and AI actually working together.\"The announcement comes at a pivotal moment for Salesforce, which has deployed more than 12,000 AI agent implementations over the past year while building what Harris called a \"$7 billion business\" around its AI platform. Yet the launch also arrives amid unusual turbulence, as CEO Marc Benioff faces fierce backlash for recent comments supporting President Trump and suggesting National Guard troops should patrol San Francisco streets.Why 95% of enterprise AI projects never launchThe stakes are enormous. While companies have rushed to experiment with AI following ChatGPT&#x27;s emergence two years ago, most enterprise deployments have stalled before reaching production, according to recent MIT research that Salesforce executives cited extensively.\"Customers have invested a lot in AI, but they&#x27;re not getting the value,\" said Srini Tallapragada, Salesforce&#x27;s president and chief engineering and customer success officer. \"95% of enterprise AI pilots fail before production. It&#x27;s not because of lack of intent. People want to do this. Everybody understands the power of the technology. But why is it so hard?\"The answer, according to Tallapragada, is that AI tools remain disconnected from enterprise workflows, data, and governance systems. \"You&#x27;re writing prompts, prompts, you&#x27;re getting frustrated because the context is not there,\" he said, describing what he called a \"prompt doom loop.\"Salesforce&#x27;s solution is a deeply integrated platform connecting what it calls four ingredients: the Agentforce 360 agent platform, Data 360 for unified data access, Customer 360 apps containing business logic, and Slack as the \"conversational interface\" where humans and agents collaborate.Slack becomes the front door to SalesforcePerhaps the most significant strategic shift is the elevation of Slack — acquired by Salesforce in 2019 for $27.7 billion — as the primary interface for Salesforce itself. The company is effectively reimagining its traditional Lightning interface around Slack channels, where sales deals, service cases, and data insights will surface conversationally rather than through forms and dashboards.\"Imagine that you maybe don&#x27;t log into Salesforce, you don&#x27;t see Salesforce, but it&#x27;s there. It&#x27;s coming to you in Slack, because that&#x27;s where you&#x27;re getting your work done,\" Harris explained.The strategy includes embedding Salesforce&#x27;s Agentforce agents for sales, IT service, HR service, and analytics directly into Slack, alongside a completely rebuilt Slackbot that acts as a personal AI companion. The company is also launching \"Channel Expert,\" an always-on agent that provides instant answers from channel conversations.To enable third-party AI tools to access Slack&#x27;s conversational data, Salesforce is releasing a Real-Time Search API and Model Context Protocol server. Partners including OpenAI, Anthropic, Google, Perplexity, Writer, Dropbox, Notion, and Cursor are building agents that will live natively in Slack.\"The best way to see the power of the platform is through the AI apps and agents already being built,\" Rob Seaman, a Salesforce executive, said during a technical briefing, citing examples of startups \"achieving tens of thousands of customers that have it installed in 120 days or less.\"Voice and IT service take aim at new marketsBeyond Slack integration, Salesforce announced major expansions into voice-based interactions and employee service. Agentforce Voice, now generally available, transforms traditional IVR systems into natural conversations that can update CRM records, trigger workflows, and seamlessly hand off to human agents.The IT Service offering represents Salesforce&#x27;s most direct challenge to ServiceNow, the market leader. Mudhu Sudhakar, who joined Salesforce two months ago as senior vice president for IT and HR Service, positioned the product as a fundamental reimagining of employee support.\"Legacy IT service management is very portals, forms, tickets focused, manual process,\" Sudhakar said. \"What we had a few key tenets: conversation first and agent first, really focused on having a conversational experience for the people requesting the support and for the people providing the support.\"The IT Service platform includes what Salesforce describes as 25+ specialized agents and 100+ pre-built workflows and connectors that can handle everything from password resets to complex incident management.Early customers report dramatic efficiency gainsCustomer results suggest the approach is gaining traction. Reddit reduced average support resolution time from 8.9 minutes to 1.4 minutes — an 84% improvement — while deflecting 46% of cases entirely to AI agents. \"This efficiency has allowed us to provide on-demand help for complex tasks and boost advertiser satisfaction scores by 20%,\" said John Thompson, Reddit&#x27;s VP of sales strategy and operations, in a statement.Engine, a travel management company, reduced average handle time by 15%, saving over $2 million annually. OpenTable resolved 70% of restaurant and diner inquiries autonomously. And 1-800Accountant achieved a 90% case deflection rate during the critical tax week period.Salesforce&#x27;s own internal deployments may be most telling. Tallapragada&#x27;s customer success organization now handles 1.8 million AI-powered conversations weekly, with metrics published at help.salesforce.com showing how many agents answer versus escalating to humans.Even more significantly, Salesforce has deployed AI-powered sales development representatives to follow up on leads that would previously have gone uncontacted due to cost constraints. \"Now, Agentforce has an SDR which is doing thousands of leads following up,\" Tallapragada explained. The company also increased proactive customer outreach by 40% by shifting staff from reactive support.The trust layer problem enterprises can&#x27;t ignoreGiven enterprise concerns about AI reliability, Salesforce has invested heavily in what it calls the \"trust layer\" — audit trails, compliance checks, and observability tools that let organizations monitor agent behavior at scale.\"You should think of an agent as a human. Digital labor. You need to manage performance just like a human. And you need these audit trails,\" Tallapragada explained.The company encountered this challenge firsthand when its own agent deployment scaled. \"When we started at Agentforce at Salesforce, we would track every message, which is great until 1,000, 3,000,\" Tallapragada said. \"Once you have a million chats, there&#x27;s no human, we cannot do it.\"The platform now includes \"Agentforce Grid\" for searching across millions of conversations to identify and fix problematic patterns. The company also introduced Agent Script, a new scripting language that allows developers to define precise guardrails and deterministic controls for agent behavior.Data infrastructure gets a major upgradeUnderlying the agent capabilities is significant infrastructure investment. Salesforce&#x27;s Data 360 includes \"Intelligent Context,\" which automatically extracts structured information from unstructured content like PDFs, diagrams, and flowcharts using what the company describes as \"AI-powered unstructured data pipelines.\"The company is also collaborating with Databricks, dbt Labs, and Snowflake on the \"Universal Semantic Interchange,\" an attempt to standardize how different platforms define business metrics. The pending $8 billion acquisition of Informatica, expected to close soon, will expand metadata management capabilities across the enterprise.The competitive landscape keeps intensifyingSalesforce&#x27;s aggressive AI agent push comes as virtually every major enterprise software vendor pursues similar strategies. Microsoft has embedded Copilot across its product line, Google offers agent capabilities through Vertex AI and Gemini, and ServiceNow has launched its own agentic offerings.When asked how Salesforce&#x27;s announcement compared to OpenAI&#x27;s recent releases, Tallapragada emphasized that customers will use multiple AI tools simultaneously. \"Most of the time I&#x27;m seeing they&#x27;re using OpenAI, they&#x27;re using Gemini, they&#x27;re using Anthropic, just like Salesforce, we use all three,\" he said.The real differentiation, executives argued, lies not in the AI models but in the integration with business processes and data. Harris framed the competition in terms familiar from Salesforce&#x27;s founding: \"26 years ago, we just said, let&#x27;s make Salesforce automation as easy as buying a book on Amazon.com. We&#x27;re doing that same thing. We want to make agentic AI as easy as buying a book on Amazon.\"The company&#x27;s customer success stories are impressive but remain a small fraction of its customer base. With 150,000 Salesforce customers and one million Slack customers, the 12,000 Agentforce deployments represent roughly 8% penetration — strong for a one-year-old product line, but hardly ubiquitous.The company&#x27;s stock, down roughly 28% year to date with a Relative Strength rating of just 15, suggests investors remain skeptical. This week&#x27;s Dreamforce demonstrations — and the months of customer deployments that follow — will begin to provide answers to whether Salesforce can finally move enterprise AI from pilots to production at scale, or whether the \"$7 billion business\" remains more aspiration than reality.",
          "content": "As 50,000 attendees descend on Salesforce&#x27;s Dreamforce conference this week, the enterprise software giant is making its most aggressive bet yet on artificial intelligence agents, positioning itself as the antidote to what it calls an industry-wide \"pilot purgatory\" where 95% of enterprise AI projects never reach production.The company on Monday launched Agentforce 360, a sweeping reimagination of its entire product portfolio designed to transform businesses into what it calls \"agentic enterprises\" — organizations where AI agents work alongside humans to handle up to 40% of work across sales, service, marketing, and operations.\"We are truly in the agentic AI era, and I think it&#x27;s probably the biggest revolution, the biggest transition in technology I&#x27;ve ever experienced in my career,\" said Parker Harris, Salesforce&#x27;s co-founder and chief technology officer, during a recent press briefing. \"In the future, 40% of the work in the Fortune 1000 is probably going to be done by AI, and it&#x27;s going to be humans and AI actually working together.\"The announcement comes at a pivotal moment for Salesforce, which has deployed more than 12,000 AI agent implementations over the past year while building what Harris called a \"$7 billion business\" around its AI platform. Yet the launch also arrives amid unusual turbulence, as CEO Marc Benioff faces fierce backlash for recent comments supporting President Trump and suggesting National Guard troops should patrol San Francisco streets.Why 95% of enterprise AI projects never launchThe stakes are enormous. While companies have rushed to experiment with AI following ChatGPT&#x27;s emergence two years ago, most enterprise deployments have stalled before reaching production, according to recent MIT research that Salesforce executives cited extensively.\"Customers have invested a lot in AI, but they&#x27;re not getting the value,\" said Srini Tallapragada, Salesforce&#x27;s president and chief engineering and customer success officer. \"95% of enterprise AI pilots fail before production. It&#x27;s not because of lack of intent. People want to do this. Everybody understands the power of the technology. But why is it so hard?\"The answer, according to Tallapragada, is that AI tools remain disconnected from enterprise workflows, data, and governance systems. \"You&#x27;re writing prompts, prompts, you&#x27;re getting frustrated because the context is not there,\" he said, describing what he called a \"prompt doom loop.\"Salesforce&#x27;s solution is a deeply integrated platform connecting what it calls four ingredients: the Agentforce 360 agent platform, Data 360 for unified data access, Customer 360 apps containing business logic, and Slack as the \"conversational interface\" where humans and agents collaborate.Slack becomes the front door to SalesforcePerhaps the most significant strategic shift is the elevation of Slack — acquired by Salesforce in 2019 for $27.7 billion — as the primary interface for Salesforce itself. The company is effectively reimagining its traditional Lightning interface around Slack channels, where sales deals, service cases, and data insights will surface conversationally rather than through forms and dashboards.\"Imagine that you maybe don&#x27;t log into Salesforce, you don&#x27;t see Salesforce, but it&#x27;s there. It&#x27;s coming to you in Slack, because that&#x27;s where you&#x27;re getting your work done,\" Harris explained.The strategy includes embedding Salesforce&#x27;s Agentforce agents for sales, IT service, HR service, and analytics directly into Slack, alongside a completely rebuilt Slackbot that acts as a personal AI companion. The company is also launching \"Channel Expert,\" an always-on agent that provides instant answers from channel conversations.To enable third-party AI tools to access Slack&#x27;s conversational data, Salesforce is releasing a Real-Time Search API and Model Context Protocol server. Partners including OpenAI, Anthropic, Google, Perplexity, Writer, Dropbox, Notion, and Cursor are building agents that will live natively in Slack.\"The best way to see the power of the platform is through the AI apps and agents already being built,\" Rob Seaman, a Salesforce executive, said during a technical briefing, citing examples of startups \"achieving tens of thousands of customers that have it installed in 120 days or less.\"Voice and IT service take aim at new marketsBeyond Slack integration, Salesforce announced major expansions into voice-based interactions and employee service. Agentforce Voice, now generally available, transforms traditional IVR systems into natural conversations that can update CRM records, trigger workflows, and seamlessly hand off to human agents.The IT Service offering represents Salesforce&#x27;s most direct challenge to ServiceNow, the market leader. Mudhu Sudhakar, who joined Salesforce two months ago as senior vice president for IT and HR Service, positioned the product as a fundamental reimagining of employee support.\"Legacy IT service management is very portals, forms, tickets focused, manual process,\" Sudhakar said. \"What we had a few key tenets: conversation first and agent first, really focused on having a conversational experience for the people requesting the support and for the people providing the support.\"The IT Service platform includes what Salesforce describes as 25+ specialized agents and 100+ pre-built workflows and connectors that can handle everything from password resets to complex incident management.Early customers report dramatic efficiency gainsCustomer results suggest the approach is gaining traction. Reddit reduced average support resolution time from 8.9 minutes to 1.4 minutes — an 84% improvement — while deflecting 46% of cases entirely to AI agents. \"This efficiency has allowed us to provide on-demand help for complex tasks and boost advertiser satisfaction scores by 20%,\" said John Thompson, Reddit&#x27;s VP of sales strategy and operations, in a statement.Engine, a travel management company, reduced average handle time by 15%, saving over $2 million annually. OpenTable resolved 70% of restaurant and diner inquiries autonomously. And 1-800Accountant achieved a 90% case deflection rate during the critical tax week period.Salesforce&#x27;s own internal deployments may be most telling. Tallapragada&#x27;s customer success organization now handles 1.8 million AI-powered conversations weekly, with metrics published at help.salesforce.com showing how many agents answer versus escalating to humans.Even more significantly, Salesforce has deployed AI-powered sales development representatives to follow up on leads that would previously have gone uncontacted due to cost constraints. \"Now, Agentforce has an SDR which is doing thousands of leads following up,\" Tallapragada explained. The company also increased proactive customer outreach by 40% by shifting staff from reactive support.The trust layer problem enterprises can&#x27;t ignoreGiven enterprise concerns about AI reliability, Salesforce has invested heavily in what it calls the \"trust layer\" — audit trails, compliance checks, and observability tools that let organizations monitor agent behavior at scale.\"You should think of an agent as a human. Digital labor. You need to manage performance just like a human. And you need these audit trails,\" Tallapragada explained.The company encountered this challenge firsthand when its own agent deployment scaled. \"When we started at Agentforce at Salesforce, we would track every message, which is great until 1,000, 3,000,\" Tallapragada said. \"Once you have a million chats, there&#x27;s no human, we cannot do it.\"The platform now includes \"Agentforce Grid\" for searching across millions of conversations to identify and fix problematic patterns. The company also introduced Agent Script, a new scripting language that allows developers to define precise guardrails and deterministic controls for agent behavior.Data infrastructure gets a major upgradeUnderlying the agent capabilities is significant infrastructure investment. Salesforce&#x27;s Data 360 includes \"Intelligent Context,\" which automatically extracts structured information from unstructured content like PDFs, diagrams, and flowcharts using what the company describes as \"AI-powered unstructured data pipelines.\"The company is also collaborating with Databricks, dbt Labs, and Snowflake on the \"Universal Semantic Interchange,\" an attempt to standardize how different platforms define business metrics. The pending $8 billion acquisition of Informatica, expected to close soon, will expand metadata management capabilities across the enterprise.The competitive landscape keeps intensifyingSalesforce&#x27;s aggressive AI agent push comes as virtually every major enterprise software vendor pursues similar strategies. Microsoft has embedded Copilot across its product line, Google offers agent capabilities through Vertex AI and Gemini, and ServiceNow has launched its own agentic offerings.When asked how Salesforce&#x27;s announcement compared to OpenAI&#x27;s recent releases, Tallapragada emphasized that customers will use multiple AI tools simultaneously. \"Most of the time I&#x27;m seeing they&#x27;re using OpenAI, they&#x27;re using Gemini, they&#x27;re using Anthropic, just like Salesforce, we use all three,\" he said.The real differentiation, executives argued, lies not in the AI models but in the integration with business processes and data. Harris framed the competition in terms familiar from Salesforce&#x27;s founding: \"26 years ago, we just said, let&#x27;s make Salesforce automation as easy as buying a book on Amazon.com. We&#x27;re doing that same thing. We want to make agentic AI as easy as buying a book on Amazon.\"The company&#x27;s customer success stories are impressive but remain a small fraction of its customer base. With 150,000 Salesforce customers and one million Slack customers, the 12,000 Agentforce deployments represent roughly 8% penetration — strong for a one-year-old product line, but hardly ubiquitous.The company&#x27;s stock, down roughly 28% year to date with a Relative Strength rating of just 15, suggests investors remain skeptical. This week&#x27;s Dreamforce demonstrations — and the months of customer deployments that follow — will begin to provide answers to whether Salesforce can finally move enterprise AI from pilots to production at scale, or whether the \"$7 billion business\" remains more aspiration than reality.",
          "feed_position": 4,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/4ElJnVoqNF5EdTdQ2qbc94/6c61cde31557a66a34fdfbbfaa9e57ed/nuneybits_Vector_art_of_businessperson_guiding_robot_f1c9b0fc-59ad-455c-b635-5551c598aa7b.webp"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/home/smart-home/best-smart-led-light-bulbs-143022856.html",
          "published_at": "Mon, 13 Oct 2025 09:00:36 +0000",
          "title": "The best smart LED light bulbs for 2025",
          "standfirst": "Smart LED light bulbs are one of the easiest ways to get into the IoT space. These smart lighting solutions let you control your home’s illumination from your phone and other connected devices, and in addition to that practicality, they also inject some fun into your space. Color-changing bulbs have a plethora of RGB options for you to customize the lighting mood for your next movie night, date night or game day, or you can opt for cozy warm white light when you need to unwind at the end of a long day.It goes without saying that many of these smart LED light bulbs work with Amazon’s Alexa and the Google Assistant, so if you already have a smart home setup in the works, you can find one that fits into your chosen ecosystem. And arguably the best thing about these devices is that they can fit into any budget; affordable and advanced options have flooded the space over the past few years. We’ve tested out a bunch of smart lights over the years, and these are our current favorites. Table of contents Best smart lights for 2025 Other smart bulbs we tested What to look for in smart light bulbs Smart light bulb FAQs Best smart lights for 2025 Other smart bulbs we’ve tested Nanoleaf Smarter Kit While we’ve recommended Nanoleaf’s Smarter Kits in guides in the past, they’re a bit more niche than other smart lights on this list. They’re best for adding flare to your living room or game-streaming setup as they come in different shapes like hexagons and triangles and can sync with music. In addition to different colors, light animations and schedules, Nanoleaf’s Smart Kits also support Amazon Alexa and Google Assistant voice commands. What to look for in smart light bulbs Connectivity (To hub or not to hub) One of the biggest appeals of smart lighting solutions is being able to control them from your phone. Most of them are able to do so by connecting to it via Wi-Fi or Bluetooth, or via an external hub, which handles the connection for them. Bluetooth connectivity limits the range in which you’ll be able to control the light, so it’s only best for a limited number of bulbs and ones you don’t expect to control when you’re away. Wi-Fi color-changing bulbs are easy to set up and can be cheaper overall since they don’t require a hub to connect them. However, having something like a central Zigbee hub can make your whole system more reliable since its connection is separate from your home’s network. For that reason, hub-based bulbs tend to be more expandable, so we mainly recommend those if you want to eventually have dozens of smart lights around your home. White or color? Most color-changing bulbs you’ll find today are “white and color” bulbs, meaning they can glow in vibrant RGB color-options like blues, pinks, greens and everything in between, as well as shine with different temperatures of white. But there are some white-only bulbs out there, and they are often a bit more affordable than their color counterparts. While we recommend springing for the white-and-color devices, if you’d prefer white only, make sure you’re getting a bulb that can span the color temperature spectrum (typically from about 2000 to 5000 Kelvin), offering you various levels of cool and warm white light. App features One of the perks of smart lighting solutions is the amount of control you have over them thanks to their various app-control capabilities. Most companion apps let you do things like set lighting schedules and timers, group individual lights into room designations and create your own custom light “scenes” with different RGB options. But we have seen other features that aren’t as ubiquitous like vacation mode for automatically turning lights on and off to enhance your home security, and sync with media, which changes the colors of lights depending on the music you’re listening to or the game you’re currently live-streaming. Smart home compatibility If you use a smart assistant like Amazon’s Alexa or the Google Assistant regularly, make sure the smart lights or smart switches work with your favorite. All of the bulbs we tested supported both Amazon’s and Google’s virtual assistants, allowing you to use voice commands to turn lights on and off, dim them with a virtual dimmer and more. The wildcard here is Siri and Apple’s HomeKit; while numerous smart bulbs have added HomeKit support, not all lights are compatible with Apple’s smart home system. Expandability We alluded to this above, but you’ll want to consider how many smart lights you eventually want in your home. Some brands and lighting systems are easier to expand than others, and we generally recommend going for hub-based bulbs if you plan on putting smart lights in every room in your home. If you’re only looking to deck out your home office or living room with some fancy color-changing bulbs, Wi-Fi options should serve you well. Thankfully, these are some of the most affordable smart home devices you can get, so even if you don’t have a clear answer to this question now, you can reconsider your options down the line if you do decide to outfit your home with multiple smart bulbs. Smart light bulb FAQs What’s the best smart light bulb for Alexa? There is no best smart light bulb for Alexa. Amazon doesn’t make its own smart bulbs (like it does for smart plugs and thermostats), but rather there are dozens of smart lights made by third-parties that work with Alexa — including all of the ones we tested. Before picking the best smart light bulb for you, make sure to check the voice assistants that the contenders support. You’ll find that most smart light bulbs available today work with Amazon’s Alexa and the Google Assistant, and plenty of them also have support for Apple’s Siri and HomeKit. Can you put a smart bulb in any lamp? Smart light bulbs can go into most modern light fixtures — but just like regular bulbs, they need to be the right shape/size for the fixture. A standard A19 smart light bulb should work properly in most table, floor and other lamps. If you have a fixture that takes a specific type of bulb, look for smart bulbs that will fit properly. Do smart light bulbs use electricity when off? Smart light bulbs do use a negligible amount of electricity when their fixtures are turned off. This is due to the fact that the smart bulb needs to stay in constant contact with your home’s internet connection or Bluetooth in order to work properly. However, their energy-saving benefits usually outweigh the small amount of power they consume even while turned off.This article originally appeared on Engadget at https://www.engadget.com/home/smart-home/best-smart-led-light-bulbs-143022856.html?src=rss",
          "content": "Smart LED light bulbs are one of the easiest ways to get into the IoT space. These smart lighting solutions let you control your home’s illumination from your phone and other connected devices, and in addition to that practicality, they also inject some fun into your space. Color-changing bulbs have a plethora of RGB options for you to customize the lighting mood for your next movie night, date night or game day, or you can opt for cozy warm white light when you need to unwind at the end of a long day.It goes without saying that many of these smart LED light bulbs work with Amazon’s Alexa and the Google Assistant, so if you already have a smart home setup in the works, you can find one that fits into your chosen ecosystem. And arguably the best thing about these devices is that they can fit into any budget; affordable and advanced options have flooded the space over the past few years. We’ve tested out a bunch of smart lights over the years, and these are our current favorites. Table of contents Best smart lights for 2025 Other smart bulbs we tested What to look for in smart light bulbs Smart light bulb FAQs Best smart lights for 2025 Other smart bulbs we’ve tested Nanoleaf Smarter Kit While we’ve recommended Nanoleaf’s Smarter Kits in guides in the past, they’re a bit more niche than other smart lights on this list. They’re best for adding flare to your living room or game-streaming setup as they come in different shapes like hexagons and triangles and can sync with music. In addition to different colors, light animations and schedules, Nanoleaf’s Smart Kits also support Amazon Alexa and Google Assistant voice commands. What to look for in smart light bulbs Connectivity (To hub or not to hub) One of the biggest appeals of smart lighting solutions is being able to control them from your phone. Most of them are able to do so by connecting to it via Wi-Fi or Bluetooth, or via an external hub, which handles the connection for them. Bluetooth connectivity limits the range in which you’ll be able to control the light, so it’s only best for a limited number of bulbs and ones you don’t expect to control when you’re away. Wi-Fi color-changing bulbs are easy to set up and can be cheaper overall since they don’t require a hub to connect them. However, having something like a central Zigbee hub can make your whole system more reliable since its connection is separate from your home’s network. For that reason, hub-based bulbs tend to be more expandable, so we mainly recommend those if you want to eventually have dozens of smart lights around your home. White or color? Most color-changing bulbs you’ll find today are “white and color” bulbs, meaning they can glow in vibrant RGB color-options like blues, pinks, greens and everything in between, as well as shine with different temperatures of white. But there are some white-only bulbs out there, and they are often a bit more affordable than their color counterparts. While we recommend springing for the white-and-color devices, if you’d prefer white only, make sure you’re getting a bulb that can span the color temperature spectrum (typically from about 2000 to 5000 Kelvin), offering you various levels of cool and warm white light. App features One of the perks of smart lighting solutions is the amount of control you have over them thanks to their various app-control capabilities. Most companion apps let you do things like set lighting schedules and timers, group individual lights into room designations and create your own custom light “scenes” with different RGB options. But we have seen other features that aren’t as ubiquitous like vacation mode for automatically turning lights on and off to enhance your home security, and sync with media, which changes the colors of lights depending on the music you’re listening to or the game you’re currently live-streaming. Smart home compatibility If you use a smart assistant like Amazon’s Alexa or the Google Assistant regularly, make sure the smart lights or smart switches work with your favorite. All of the bulbs we tested supported both Amazon’s and Google’s virtual assistants, allowing you to use voice commands to turn lights on and off, dim them with a virtual dimmer and more. The wildcard here is Siri and Apple’s HomeKit; while numerous smart bulbs have added HomeKit support, not all lights are compatible with Apple’s smart home system. Expandability We alluded to this above, but you’ll want to consider how many smart lights you eventually want in your home. Some brands and lighting systems are easier to expand than others, and we generally recommend going for hub-based bulbs if you plan on putting smart lights in every room in your home. If you’re only looking to deck out your home office or living room with some fancy color-changing bulbs, Wi-Fi options should serve you well. Thankfully, these are some of the most affordable smart home devices you can get, so even if you don’t have a clear answer to this question now, you can reconsider your options down the line if you do decide to outfit your home with multiple smart bulbs. Smart light bulb FAQs What’s the best smart light bulb for Alexa? There is no best smart light bulb for Alexa. Amazon doesn’t make its own smart bulbs (like it does for smart plugs and thermostats), but rather there are dozens of smart lights made by third-parties that work with Alexa — including all of the ones we tested. Before picking the best smart light bulb for you, make sure to check the voice assistants that the contenders support. You’ll find that most smart light bulbs available today work with Amazon’s Alexa and the Google Assistant, and plenty of them also have support for Apple’s Siri and HomeKit. Can you put a smart bulb in any lamp? Smart light bulbs can go into most modern light fixtures — but just like regular bulbs, they need to be the right shape/size for the fixture. A standard A19 smart light bulb should work properly in most table, floor and other lamps. If you have a fixture that takes a specific type of bulb, look for smart bulbs that will fit properly. Do smart light bulbs use electricity when off? Smart light bulbs do use a negligible amount of electricity when their fixtures are turned off. This is due to the fact that the smart bulb needs to stay in constant contact with your home’s internet connection or Bluetooth in order to work properly. However, their energy-saving benefits usually outweigh the small amount of power they consume even while turned off.This article originally appeared on Engadget at https://www.engadget.com/home/smart-home/best-smart-led-light-bulbs-143022856.html?src=rss",
          "feed_position": 17
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/breaking-the-bottleneck-why-ai-demands-an-ssd-first-future",
          "published_at": "Mon, 13 Oct 2025 04:00:00 GMT",
          "title": "Breaking the bottleneck: Why AI demands an SSD-first future",
          "standfirst": "Presented by SolidigmAs AI adoption surges, data centers face a critical bottleneck in storage — and traditional HDDs are at the center of it. Data that once sat idle as cold archives is now being pulled into frequent use to build more accurate models and deliver better inference results. This shift from cold data to warm data demands low-latency, high-throughput storage that can handle parallel computations. HDDs will remain the workhorse for low-cost cold storage, but without rethinking their role, the high-capacity storage layer risks becoming the weakest link in the AI factory.\"Modern AI workloads, combined with data center constraints, have created new challenges for HDDs,\" says Jeff Janukowicz, research vice president at IDC. \"While HDD suppliers are addressing data storage growth by offering larger drives, this often comes at the expense of slower performance. As a result, the concept of &#x27;nearline SSDs&#x27; is becoming an increasingly relevant topic of discussion within the industry.\" Today, AI operators need to maximize GPU utilization, manage network-attached storage efficiently, and scale compute — all while cutting costs on increasingly scarce power and space. In an environment where every watt and every square inch counts, says Roger Corell, senior director of AI and leadership marketing at Solidigm, success requires more than a technical refresh. It calls for a deeper realignment.“It speaks to the tectonic shift in the value of data for AI,” Corell says. “That’s where high-capacity SSDs come into play. Along with capacity, they bring performance and efficiency -- enabling exabyte-scale storage pipelines to keep pace with the relentless pace of data set size. All of that consumes power and space, so we need to do it as efficiently as possible to enable more GPU scale in this constrained environment.”High-capacity SSDs aren’t just displacing HDDs — they’re removing one of the biggest bottlenecks on the AI factory floor. By delivering massive gains in performance, efficiency, and density, SSDs free up the power and space needed to push GPU scale further. It’s less a storage upgrade than a structural shift in how data infrastructure is designed for the AI era.HDDs vs. SDDs: More than just a hardware refreshHDDs have impressive mechanical designs, but they&#x27;re made up of many moving parts that at scale use more energy, take up more space, and fail at a higher rate than solid state drives. The reliance on spinning platters and mechanical read/write heads inherently limits Input/Output Operations Per Second (IOPS), creating bottlenecks for AI workloads that demand low latency, high concurrency, and sustained throughput. HDDs also struggle with latency-sensitive tasks, as the physical act of seeking data introduces mechanical delays unsuited for real-time AI inference and training. Moreover, their power and cooling requirements increase significantly under frequent and intensive data access, reducing efficiency as data scales and warms.In contrast, the SSD-based VAST storage solution reduces energy usage by ~$1M a year, and in an AI environment where every watt matters, this is a huge advantage for SSDs. To demonstrate, Solidigm and VAST Data completed a study examining the economics of data storage at exabyte scale — a quadrillion bytes, or a billion gigabytes, with an analysis of storage power consumption versus HDDs over a 10-year period. As a starting reference point, you’d need four 30TB HDDs to equal the capacity of a single 122TB Solidigm SSD. After factoring in VAST’s data reduction techniques made possible by the superior performance of SSDs, the exabyte solution comprises 3,738 Solidigm SSDs vs over 40,000 high-capacity HDDs. The study found that the SSD-based VAST solution consumes 77% less storage energy. Minimizing data center footprints\"We’re shipping 122-terabyte drives to some of the top OEMs and leading AI cloud service providers in the world,\" Corell says. \"When you compare an all-122TB SSD to hybrid HDD + TLC SSD configuration, they&#x27;re getting a nine-to-one savings in data center footprint. And yes, it’s important in these massive data centers that are building their own nuclear reactors and signing hefty power purchase agreements with renewable energy providers, but it’s increasingly important as you get to the regional data centers, the local data centers, and all the way out to your edge deployments where space can come at a premium.\"That nine-to-one savings goes beyond space and power — it lets organizations fit infrastructure into previously unavailable spaces, expand GPU scale, or build smaller footprints.\"If you’re given X amount of land and Y amount of power, you’re going to use it. You’re AI\" Corell explains, “where every watt and square inch counts, so why not use it in the most efficient way? Get the most efficient storage possible on the planet and enable greater GPU scale within that envelope that you have to fit in. On an ongoing basis, it’s going to save you operational cost as well. You have 90 percent fewer storage bays to maintain, and the cost associated with that is gone.\"Another often-overlooked element, the (much) larger physical footprint of data stored on mechanical HDDs results in a greater construction materials footprint. Collectively, concrete and steel production accounts for over 15% of global greenhouse gas emissions. By reducing the physical footprint of storage, high-capacity SSDs can help reduce embodied concrete and steel-based emissions by more than 80% compared to HDDs. And in the last phase of the sustainability life cycle, which is drive end-of-life, there will be 90% percent fewer drives to disposition. . Reshaping cold and archival storage strategiesThe move to SDD isn&#x27;t just a storage upgrade; it&#x27;s a fundamental realignment of data infrastructure strategy in the AI era, and it&#x27;s picking up speed.\"Big hyperscalers are looking to wring the most out of their existing infrastructure, doing unnatural acts, if you will, with HDDs like overprovisioning them to near 90% to try to wring out as many IOPS per terabyte as possible, but they’re beginning to come around,\" Corell says. \"Once they turn to a modern all high-capacity storage infrastructure, the industry at large will be on that trajectory. Plus, we&#x27;re starting to see these lessons learned on the value of modern storage in AI applied to other segments as well, such as big data analytics, HPC, and many more.\"While all-flash solutions are being embraced almost universally, there will always be a place for HDDs, he adds. HDDs will persist in usages like archival, cold storage, and scenarios where pure cost per gigabyte concerns outweigh the need for real-time access. But as the token economy heats up and enterprises realize value in monetizing data, the warm and warming data segments will continue to grow. Solving power challenges of the futureNow in its 4th generation, with more than 122 cumulative exabytes shipped to date, Solidigm’s QLC (Quad-Level Cell) technology has led the industry in balancing higher drive capacities with cost efficiency.\"We don’t think of storage as just storing bits and bytes. We think about how we can develop these amazing drives that are able to deliver benefits at a solution level,\" Corell says. \"The shining star on that is our recently launched, E1.S, designed specifically for dense and efficient storage in direct attach storage configurations for the next-generation fanless GPU server.\"The Solidigm D7-PS1010 E1.S is a breakthrough, the industry’s first eSSD with single-sided direct-to-chip liquid cooling technology. Solidigm worked with NVIDIA to address the dual challenges of heat management and cost efficiency, while delivering the high performance required for demanding AI workloads. \"We’re rapidly moving to an environment where all critical IT components will be direct-to-chip liquid-cooled on the direct attach side,\" he says. \"I think the market needs to be looking at their approach to cooling, because power limitations, power challenges are not going to abate in my lifetime, at least. They need to be applying a neocloud mindset to how they’re architecting the most efficient infrastructure.\"Increasingly complex inference is pushing against a memory wall, which makes storage architecture a front-line design challenge, not an afterthought. High-capacity SSDs, paired with liquid cooling and efficient design, are emerging as the only path to meet AI’s escalating demands. The mandate now is to build infrastructure not just for efficiency, but for storage that can efficiently scale as data grows. The organizations that realign storage now will be the ones able to scale AI tomorrow.Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.",
          "content": "Presented by SolidigmAs AI adoption surges, data centers face a critical bottleneck in storage — and traditional HDDs are at the center of it. Data that once sat idle as cold archives is now being pulled into frequent use to build more accurate models and deliver better inference results. This shift from cold data to warm data demands low-latency, high-throughput storage that can handle parallel computations. HDDs will remain the workhorse for low-cost cold storage, but without rethinking their role, the high-capacity storage layer risks becoming the weakest link in the AI factory.\"Modern AI workloads, combined with data center constraints, have created new challenges for HDDs,\" says Jeff Janukowicz, research vice president at IDC. \"While HDD suppliers are addressing data storage growth by offering larger drives, this often comes at the expense of slower performance. As a result, the concept of &#x27;nearline SSDs&#x27; is becoming an increasingly relevant topic of discussion within the industry.\" Today, AI operators need to maximize GPU utilization, manage network-attached storage efficiently, and scale compute — all while cutting costs on increasingly scarce power and space. In an environment where every watt and every square inch counts, says Roger Corell, senior director of AI and leadership marketing at Solidigm, success requires more than a technical refresh. It calls for a deeper realignment.“It speaks to the tectonic shift in the value of data for AI,” Corell says. “That’s where high-capacity SSDs come into play. Along with capacity, they bring performance and efficiency -- enabling exabyte-scale storage pipelines to keep pace with the relentless pace of data set size. All of that consumes power and space, so we need to do it as efficiently as possible to enable more GPU scale in this constrained environment.”High-capacity SSDs aren’t just displacing HDDs — they’re removing one of the biggest bottlenecks on the AI factory floor. By delivering massive gains in performance, efficiency, and density, SSDs free up the power and space needed to push GPU scale further. It’s less a storage upgrade than a structural shift in how data infrastructure is designed for the AI era.HDDs vs. SDDs: More than just a hardware refreshHDDs have impressive mechanical designs, but they&#x27;re made up of many moving parts that at scale use more energy, take up more space, and fail at a higher rate than solid state drives. The reliance on spinning platters and mechanical read/write heads inherently limits Input/Output Operations Per Second (IOPS), creating bottlenecks for AI workloads that demand low latency, high concurrency, and sustained throughput. HDDs also struggle with latency-sensitive tasks, as the physical act of seeking data introduces mechanical delays unsuited for real-time AI inference and training. Moreover, their power and cooling requirements increase significantly under frequent and intensive data access, reducing efficiency as data scales and warms.In contrast, the SSD-based VAST storage solution reduces energy usage by ~$1M a year, and in an AI environment where every watt matters, this is a huge advantage for SSDs. To demonstrate, Solidigm and VAST Data completed a study examining the economics of data storage at exabyte scale — a quadrillion bytes, or a billion gigabytes, with an analysis of storage power consumption versus HDDs over a 10-year period. As a starting reference point, you’d need four 30TB HDDs to equal the capacity of a single 122TB Solidigm SSD. After factoring in VAST’s data reduction techniques made possible by the superior performance of SSDs, the exabyte solution comprises 3,738 Solidigm SSDs vs over 40,000 high-capacity HDDs. The study found that the SSD-based VAST solution consumes 77% less storage energy. Minimizing data center footprints\"We’re shipping 122-terabyte drives to some of the top OEMs and leading AI cloud service providers in the world,\" Corell says. \"When you compare an all-122TB SSD to hybrid HDD + TLC SSD configuration, they&#x27;re getting a nine-to-one savings in data center footprint. And yes, it’s important in these massive data centers that are building their own nuclear reactors and signing hefty power purchase agreements with renewable energy providers, but it’s increasingly important as you get to the regional data centers, the local data centers, and all the way out to your edge deployments where space can come at a premium.\"That nine-to-one savings goes beyond space and power — it lets organizations fit infrastructure into previously unavailable spaces, expand GPU scale, or build smaller footprints.\"If you’re given X amount of land and Y amount of power, you’re going to use it. You’re AI\" Corell explains, “where every watt and square inch counts, so why not use it in the most efficient way? Get the most efficient storage possible on the planet and enable greater GPU scale within that envelope that you have to fit in. On an ongoing basis, it’s going to save you operational cost as well. You have 90 percent fewer storage bays to maintain, and the cost associated with that is gone.\"Another often-overlooked element, the (much) larger physical footprint of data stored on mechanical HDDs results in a greater construction materials footprint. Collectively, concrete and steel production accounts for over 15% of global greenhouse gas emissions. By reducing the physical footprint of storage, high-capacity SSDs can help reduce embodied concrete and steel-based emissions by more than 80% compared to HDDs. And in the last phase of the sustainability life cycle, which is drive end-of-life, there will be 90% percent fewer drives to disposition. . Reshaping cold and archival storage strategiesThe move to SDD isn&#x27;t just a storage upgrade; it&#x27;s a fundamental realignment of data infrastructure strategy in the AI era, and it&#x27;s picking up speed.\"Big hyperscalers are looking to wring the most out of their existing infrastructure, doing unnatural acts, if you will, with HDDs like overprovisioning them to near 90% to try to wring out as many IOPS per terabyte as possible, but they’re beginning to come around,\" Corell says. \"Once they turn to a modern all high-capacity storage infrastructure, the industry at large will be on that trajectory. Plus, we&#x27;re starting to see these lessons learned on the value of modern storage in AI applied to other segments as well, such as big data analytics, HPC, and many more.\"While all-flash solutions are being embraced almost universally, there will always be a place for HDDs, he adds. HDDs will persist in usages like archival, cold storage, and scenarios where pure cost per gigabyte concerns outweigh the need for real-time access. But as the token economy heats up and enterprises realize value in monetizing data, the warm and warming data segments will continue to grow. Solving power challenges of the futureNow in its 4th generation, with more than 122 cumulative exabytes shipped to date, Solidigm’s QLC (Quad-Level Cell) technology has led the industry in balancing higher drive capacities with cost efficiency.\"We don’t think of storage as just storing bits and bytes. We think about how we can develop these amazing drives that are able to deliver benefits at a solution level,\" Corell says. \"The shining star on that is our recently launched, E1.S, designed specifically for dense and efficient storage in direct attach storage configurations for the next-generation fanless GPU server.\"The Solidigm D7-PS1010 E1.S is a breakthrough, the industry’s first eSSD with single-sided direct-to-chip liquid cooling technology. Solidigm worked with NVIDIA to address the dual challenges of heat management and cost efficiency, while delivering the high performance required for demanding AI workloads. \"We’re rapidly moving to an environment where all critical IT components will be direct-to-chip liquid-cooled on the direct attach side,\" he says. \"I think the market needs to be looking at their approach to cooling, because power limitations, power challenges are not going to abate in my lifetime, at least. They need to be applying a neocloud mindset to how they’re architecting the most efficient infrastructure.\"Increasingly complex inference is pushing against a memory wall, which makes storage architecture a front-line design challenge, not an afterthought. High-capacity SSDs, paired with liquid cooling and efficient design, are emerging as the only path to meet AI’s escalating demands. The mandate now is to build infrastructure not just for efficiency, but for storage that can efficiently scale as data grows. The organizations that realign storage now will be the ones able to scale AI tomorrow.Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.",
          "feed_position": 5,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/4rRjfWe1S9u2XgvoKASwHM/ddafa77e43acdade763516ae3aa9a5fe/Solidigm_2.png"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/we-keep-talking-about-ai-agents-but-do-we-ever-know-what-they-are",
          "published_at": "Sun, 12 Oct 2025 19:00:00 GMT",
          "title": "We keep talking about AI agents, but do we ever know what they are?",
          "standfirst": "Imagine you do two things on a Monday morning.First, you ask a chatbot to summarize your new emails. Next, you ask an AI tool to figure out why your top competitor grew so fast last quarter. The AI silently gets to work. It scours financial reports, news articles and social media sentiment. It cross-references that data with your internal sales numbers, drafts a strategy outlining three potential reasons for the competitor&#x27;s success and schedules a 30-minute meeting with your team to present its findings.We&#x27;re calling both of these \"AI agents,\" but they represent worlds of difference in intelligence, capability and the level of trust we place in them. This ambiguity creates a fog that makes it difficult to build, evaluate, and safely govern these powerful new tools. If we can&#x27;t agree on what we&#x27;re building, how can we know when we&#x27;ve succeeded?This post won&#x27;t try to sell you on yet another definitive framework. Instead, think of it as a survey of the current landscape of agent autonomy, a map to help us all navigate the terrain together.What are we even talking about? Defining an \"AI agent\"Before we can measure an agent&#x27;s autonomy, we need to agree on what an \"agent\" actually is. The most widely accepted starting point comes from the foundational textbook on AI, Stuart Russell and Peter Norvig’s “Artificial Intelligence: A Modern Approach.” They define an agent as anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators. A thermostat is a simple agent: Its sensor perceives the room temperature, and its actuator acts by turning the heat on or off.ReAct Model for AI Agents (Credit: Confluent) That classic definition provides a solid mental model. For today&#x27;s technology, we can translate it into four key components that make up a modern AI agent:Perception (the \"senses\"): This is how an agent takes in information about its digital or physical environment. It&#x27;s the input stream that allows the agent to understand the current state of the world relevant to its task.Reasoning engine (the \"brain\"): This is the core logic that processes the perceptions and decides what to do next. For modern agents, this is typically powered by a large language model (LLM). The engine is responsible for planning, breaking down large goals into smaller steps, handling errors and choosing the right tools for the job.Action (the \"hands\"): This is how an agent affects its environment to move closer to its goal. The ability to take action via tools is what gives an agent its power.Goal/objective: This is the overarching task or purpose that guides all of the agent&#x27;s actions. It is the \"why\" that turns a collection of tools into a purposeful system. The goal can be simple (\"Find the best price for this book\") or complex (\"Launch the marketing campaign for our new product\")Putting it all together, a true agent is a full-body system. The reasoning engine is the brain, but it’s useless without the senses (perception) to understand the world and the hands (actions) to change it. This complete system, all guided by a central goal, is what creates genuine agency.With these components in mind, the distinction we made earlier becomes clear. A standard chatbot isn&#x27;t a true agent. It perceives your question and acts by providing an answer, but it lacks an overarching goal and the ability to use external tools to accomplish it.An agent, on the other hand, is software that has agency. It has the capacity to act independently and dynamically toward a goal. And it&#x27;s this capacity that makes a discussion about the levels of autonomy so important.Learning from the past: How we learned to classify autonomyThe dizzying pace of AI can make it feel like we&#x27;re navigating uncharted territory. But when it comes to classifying autonomy, we’re not starting from scratch. Other industries have been working on this problem for decades, and their playbooks offer powerful lessons for the world of AI agents.The core challenge is always the same: How do you create a clear, shared language for the gradual handover of responsibility from a human to a machine?SAE levels of driving automationPerhaps the most successful framework comes from the automotive industry. The SAE J3016 standard defines six levels of driving automation, from Level 0 (fully manual) to Level 5 (fully autonomous).The SAE J3016 Levels of Driving Automation (Credit: SAE International) What makes this model so effective isn&#x27;t its technical detail, but its focus on two simple concepts:Dynamic driving task (DDT): This is everything involved in the real-time act of driving: steering, braking, accelerating and monitoring the road.Operational design domain (ODD): These are the specific conditions under which the system is designed to work. For example, \"only on divided highways\" or \"only in clear weather during the daytime.\"The question for each level is simple: Who is doing the DDT, and what is the ODD? At Level 2, the human must supervise at all times. At Level 3, the car handles the DDT within its ODD, but the human must be ready to take over. At Level 4, the car can handle everything within its ODD, and if it encounters a problem, it can safely pull over on its own.The key insight for AI agents: A robust framework isn&#x27;t about the sophistication of the AI \"brain.\" It&#x27;s about clearly defining the division of responsibility between human and machine under specific, well-defined conditions.Aviation&#x27;s 10 Levels of AutomationWhile the SAE’s six levels are great for broad classification, aviation offers a more granular model for systems designed for close human-machine collaboration. The Parasuraman, Sheridan, and Wickens model proposes a detailed 10-level spectrum of automation.Levels of Automation of Decision and Action Selection for Aviation (Credit: The MITRE Corporation)This framework is less about full autonomy and more about the nuances of interaction. For example:At Level 3, the computer \"narrows the selection down to a few\" for the human to choose from.At Level 6, the computer \"allows the human a restricted time to veto before it executes\" an action.At Level 9, the computer \"informs the human only if it, the computer, decides to.\"The key insight for AI agents: This model is perfect for describing the collaborative \"centaur\" systems we&#x27;re seeing today. Most AI agents won&#x27;t be fully autonomous (Level 10) but will exist somewhere on this spectrum, acting as a co-pilot that suggests, executes with approval or acts with a veto window.Robotics and unmanned systemsFinally, the world of robotics brings in another critical dimension: context. The National Institute of Standards and Technology&#x27;s (NIST) Autonomy Levels for Unmanned Systems (ALFUS) framework was designed for systems like drones and industrial robots.The Three-Axis Model for ALFUS (Credit: NIST) Its main contribution is adding context to the definition of autonomy, assessing it along three axes:Human independence: How much human supervision is required?Mission complexity: How difficult or unstructured is the task?Environmental complexity: How predictable and stable is the environment in which the agent operates?The key insight for AI agents: This framework reminds us that autonomy isn&#x27;t a single number. An agent performing a simple task in a stable, predictable digital environment (like sorting files in a single folder) is fundamentally less autonomous than an agent performing a complex task across the chaotic, unpredictable environment of the open internet, even if the level of human supervision is the same.The emerging frameworks for AI agentsHaving looked at the lessons from automotive, aviation and robotics, we can now examine the emerging frameworks designed for AI agents. While the field is still new and no single standard has won out, most proposals fall into three distinct, but often overlapping, categories based on the primary question they seek to answer.Category 1: The \"What can it do?\" frameworks (capability-focused)These frameworks classify agents based on their underlying technical architecture and what they are capable of achieving. They provide a roadmap for developers, outlining a progression of increasingly sophisticated technical milestones that often correspond directly to code patterns.A prime example of this developer-centric approach comes from Hugging Face. Their framework uses a star rating to show the gradual shift in control from human to AI:Five Levels of AI Agent Autonomy, as proposed by HuggingFace (Credit: Hugging Face) Zero stars (simple processor): The AI has no impact on the program&#x27;s flow. It simply processes information and its output is displayed, like a print statement. The human is in complete control.One star (router): The AI makes a basic decision that directs program flow, like choosing between two predefined paths (if/else). The human still defines how everything is done.Two stars (tool call): The AI chooses which predefined tool to use and what arguments to use with it. The human has defined the available tools, but the AI decides how to execute them.Three stars (multi-step agent): The AI now controls the iteration loop. It decides which tool to use, when to use it and whether to continue working on the task.Four stars (fully autonomous): The AI can generate and execute entirely new code to accomplish a goal, going beyond the predefined tools it was given.Strengths: This model is excellent for engineers. It&#x27;s concrete, maps directly to code and clearly benchmarks the transfer of executive control to the AI. Weaknesses: It is highly technical and less intuitive for non-developers trying to understand an agent&#x27;s real-world impact.Category 2: The \"How do we work together?\" frameworks (interaction-focused)This second category defines autonomy not by the agent’s internal skills, but by the nature of its relationship with the human user. The central question is: Who is in control, and how do we collaborate?This approach often mirrors the nuance we saw in the aviation models. For instance, a framework detailed in the paper Levels of Autonomy for AI Agents defines levels based on the user&#x27;s role:L1 - user as an operator: The human is in direct control (like a person using Photoshop with AI-assist features).L4 - user as an approver: The agent proposes a full plan or action, and the human must give a simple \"yes\" or \"no\" before it proceeds.L5 - user as an observer: The agent has full autonomy to pursue a goal and simply reports its progress and results back to the human.Levels of Autonomy for AI AgentsStrengths: These frameworks are highly intuitive and user-centric. They directly address the critical issues of control, trust, and oversight.Weaknesses: An agent with simple capabilities and one with highly advanced reasoning could both fall into the \"Approver\" level, so this approach can sometimes obscure the underlying technical sophistication.Category 3: The \"Who is responsible?\" frameworks (governance-focused)The final category is less concerned with how an agent works and more with what happens when it fails. These frameworks are designed to help answer crucial questions about law, safety and ethics.Think tanks like Germany&#x27;s Stiftung Neue VTrantwortung have analyzed AI agents through the lens of legal liability. Their work aims to classify agents in a way that helps regulators determine who is responsible for an agent&#x27;s actions: The user who deployed it, the developer who built it or the company that owns the platform it runs on?This perspective is essential for navigating complex regulations like the EU&#x27;s Artificial Intelligence Act, which will treat AI systems differently based on the level of risk they pose.Strengths: This approach is absolutely essential for real-world deployment. It forces the difficult but necessary conversations about accountability that build public trust.Weaknesses: It&#x27;s more of a legal or policy guide than a technical roadmap for developers.A comprehensive understanding requires looking at all three questions at once: An agent&#x27;s capabilities, how we interact with it and who is responsible for the outcome..Identifying the gaps and challengesLooking at the landscape of autonomy frameworks shows us that no single model is sufficient because the true challenges lie in the gaps between them, in areas that are incredibly difficult to define and measure.What is the \"Road\" for a digital agent?The SAE framework for self-driving cars gave us the powerful concept of an ODD, the specific conditions under which a system can operate safely. For a car, that might be \"divided highways, in clear weather, during the day.\" This is a great solution for a physical environment, but what’s the ODD for a digital agent?The \"road\" for an agent is the entire internet. An infinite, chaotic and constantly changing environment. Websites get redesigned overnight, APIs are deprecated and social norms in online communities shift. How do we define a \"safe\" operational boundary for an agent that can browse websites, access databases and interact with third-party services? Answering this is one of the biggest unsolved problems. Without a clear digital ODD, we can&#x27;t make the same safety guarantees that are becoming standard in the automotive world.This is why, for now, the most effective and reliable agents operate within well-defined, closed-world scenarios. As I argued in a recent VentureBeat article, forgetting the open-world fantasies and focusing on \"bounded problems\" is the key to real-world success. This means defining a clear, limited set of tools, data sources and potential actions. Beyond simple tool useToday&#x27;s agents are getting very good at executing straightforward plans. If you tell one to \"find the price of this item using Tool A, then book a meeting with Tool B,\" it can often succeed. But true autonomy requires much more. Many systems today hit a technical wall when faced with tasks that require:Long-term reasoning and planning: Agents struggle to create and adapt complex, multi-step plans in the face of uncertainty. They can follow a recipe, but they can&#x27;t yet invent one from scratch when things go wrong.Robust self-correction: What happens when an API call fails or a website returns an unexpected error? A truly autonomous agent needs the resilience to diagnose the problem, form a new hypothesis and try a different approach, all without a human stepping in.Composability: The future likely involves not one agent, but a team of specialized agents working together. Getting them to collaborate reliably, to pass information back and forth, delegate tasks and resolve conflicts is a monumental software engineering challenge that we are just beginning to tackle.The elephant in the room: Alignment and controlThis is the most critical challenge of all, because it&#x27;s not just technical, it&#x27;s deeply human. Alignment is the problem of ensuring an agent&#x27;s goals and actions are consistent with our intentions and values, even when those values are complex, unstated or nuanced.Imagine you give an agent the seemingly harmless goal of \"maximizing customer engagement for our new product.\" The agent might correctly determine that the most effective strategy is to send a dozen notifications a day to every user. The agent has achieved its literal goal perfectly, but it has violated the unstated, common-sense goal of \"don&#x27;t be incredibly annoying.\"This is a failure of alignment.The core difficulty, which organizations like the AI Alignment Forum are dedicated to studying, is that it is incredibly hard to specify fuzzy, complex human preferences in the precise, literal language of code. As agents become more powerful, ensuring they are not just capable but also safe, predictable and aligned with our true intent becomes the most important challenge we face.The future is agentic (and collaborative)The path forward for AI agents is not a single leap to a god-like super-intelligence, but a more practical and collaborative journey. The immense challenges of open-world reasoning and perfect alignment mean that the future is a team effort.We will see less of the single, all-powerful agent and more of an \"agentic mesh\" — a network of specialized agents, each operating within a bounded domain, working together to tackle complex problems. More importantly, they will work with us. The most valuable and safest applications will keep a human on the loop, casting them as a co-pilot or strategist to augment our intellect with the speed of machine execution. This \"centaur\" model will be the most effective and responsible path forward.The frameworks we&#x27;ve explored aren’t just theoretical. They’re practical tools for building trust, assigning responsibility and setting clear expectations. They help developers define limits and leaders shape vision, laying the groundwork for AI to become a dependable partner in our work and lives.Sean Falconer is Confluent&#x27;s AI entrepreneur in residence.",
          "content": "Imagine you do two things on a Monday morning.First, you ask a chatbot to summarize your new emails. Next, you ask an AI tool to figure out why your top competitor grew so fast last quarter. The AI silently gets to work. It scours financial reports, news articles and social media sentiment. It cross-references that data with your internal sales numbers, drafts a strategy outlining three potential reasons for the competitor&#x27;s success and schedules a 30-minute meeting with your team to present its findings.We&#x27;re calling both of these \"AI agents,\" but they represent worlds of difference in intelligence, capability and the level of trust we place in them. This ambiguity creates a fog that makes it difficult to build, evaluate, and safely govern these powerful new tools. If we can&#x27;t agree on what we&#x27;re building, how can we know when we&#x27;ve succeeded?This post won&#x27;t try to sell you on yet another definitive framework. Instead, think of it as a survey of the current landscape of agent autonomy, a map to help us all navigate the terrain together.What are we even talking about? Defining an \"AI agent\"Before we can measure an agent&#x27;s autonomy, we need to agree on what an \"agent\" actually is. The most widely accepted starting point comes from the foundational textbook on AI, Stuart Russell and Peter Norvig’s “Artificial Intelligence: A Modern Approach.” They define an agent as anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators. A thermostat is a simple agent: Its sensor perceives the room temperature, and its actuator acts by turning the heat on or off.ReAct Model for AI Agents (Credit: Confluent) That classic definition provides a solid mental model. For today&#x27;s technology, we can translate it into four key components that make up a modern AI agent:Perception (the \"senses\"): This is how an agent takes in information about its digital or physical environment. It&#x27;s the input stream that allows the agent to understand the current state of the world relevant to its task.Reasoning engine (the \"brain\"): This is the core logic that processes the perceptions and decides what to do next. For modern agents, this is typically powered by a large language model (LLM). The engine is responsible for planning, breaking down large goals into smaller steps, handling errors and choosing the right tools for the job.Action (the \"hands\"): This is how an agent affects its environment to move closer to its goal. The ability to take action via tools is what gives an agent its power.Goal/objective: This is the overarching task or purpose that guides all of the agent&#x27;s actions. It is the \"why\" that turns a collection of tools into a purposeful system. The goal can be simple (\"Find the best price for this book\") or complex (\"Launch the marketing campaign for our new product\")Putting it all together, a true agent is a full-body system. The reasoning engine is the brain, but it’s useless without the senses (perception) to understand the world and the hands (actions) to change it. This complete system, all guided by a central goal, is what creates genuine agency.With these components in mind, the distinction we made earlier becomes clear. A standard chatbot isn&#x27;t a true agent. It perceives your question and acts by providing an answer, but it lacks an overarching goal and the ability to use external tools to accomplish it.An agent, on the other hand, is software that has agency. It has the capacity to act independently and dynamically toward a goal. And it&#x27;s this capacity that makes a discussion about the levels of autonomy so important.Learning from the past: How we learned to classify autonomyThe dizzying pace of AI can make it feel like we&#x27;re navigating uncharted territory. But when it comes to classifying autonomy, we’re not starting from scratch. Other industries have been working on this problem for decades, and their playbooks offer powerful lessons for the world of AI agents.The core challenge is always the same: How do you create a clear, shared language for the gradual handover of responsibility from a human to a machine?SAE levels of driving automationPerhaps the most successful framework comes from the automotive industry. The SAE J3016 standard defines six levels of driving automation, from Level 0 (fully manual) to Level 5 (fully autonomous).The SAE J3016 Levels of Driving Automation (Credit: SAE International) What makes this model so effective isn&#x27;t its technical detail, but its focus on two simple concepts:Dynamic driving task (DDT): This is everything involved in the real-time act of driving: steering, braking, accelerating and monitoring the road.Operational design domain (ODD): These are the specific conditions under which the system is designed to work. For example, \"only on divided highways\" or \"only in clear weather during the daytime.\"The question for each level is simple: Who is doing the DDT, and what is the ODD? At Level 2, the human must supervise at all times. At Level 3, the car handles the DDT within its ODD, but the human must be ready to take over. At Level 4, the car can handle everything within its ODD, and if it encounters a problem, it can safely pull over on its own.The key insight for AI agents: A robust framework isn&#x27;t about the sophistication of the AI \"brain.\" It&#x27;s about clearly defining the division of responsibility between human and machine under specific, well-defined conditions.Aviation&#x27;s 10 Levels of AutomationWhile the SAE’s six levels are great for broad classification, aviation offers a more granular model for systems designed for close human-machine collaboration. The Parasuraman, Sheridan, and Wickens model proposes a detailed 10-level spectrum of automation.Levels of Automation of Decision and Action Selection for Aviation (Credit: The MITRE Corporation)This framework is less about full autonomy and more about the nuances of interaction. For example:At Level 3, the computer \"narrows the selection down to a few\" for the human to choose from.At Level 6, the computer \"allows the human a restricted time to veto before it executes\" an action.At Level 9, the computer \"informs the human only if it, the computer, decides to.\"The key insight for AI agents: This model is perfect for describing the collaborative \"centaur\" systems we&#x27;re seeing today. Most AI agents won&#x27;t be fully autonomous (Level 10) but will exist somewhere on this spectrum, acting as a co-pilot that suggests, executes with approval or acts with a veto window.Robotics and unmanned systemsFinally, the world of robotics brings in another critical dimension: context. The National Institute of Standards and Technology&#x27;s (NIST) Autonomy Levels for Unmanned Systems (ALFUS) framework was designed for systems like drones and industrial robots.The Three-Axis Model for ALFUS (Credit: NIST) Its main contribution is adding context to the definition of autonomy, assessing it along three axes:Human independence: How much human supervision is required?Mission complexity: How difficult or unstructured is the task?Environmental complexity: How predictable and stable is the environment in which the agent operates?The key insight for AI agents: This framework reminds us that autonomy isn&#x27;t a single number. An agent performing a simple task in a stable, predictable digital environment (like sorting files in a single folder) is fundamentally less autonomous than an agent performing a complex task across the chaotic, unpredictable environment of the open internet, even if the level of human supervision is the same.The emerging frameworks for AI agentsHaving looked at the lessons from automotive, aviation and robotics, we can now examine the emerging frameworks designed for AI agents. While the field is still new and no single standard has won out, most proposals fall into three distinct, but often overlapping, categories based on the primary question they seek to answer.Category 1: The \"What can it do?\" frameworks (capability-focused)These frameworks classify agents based on their underlying technical architecture and what they are capable of achieving. They provide a roadmap for developers, outlining a progression of increasingly sophisticated technical milestones that often correspond directly to code patterns.A prime example of this developer-centric approach comes from Hugging Face. Their framework uses a star rating to show the gradual shift in control from human to AI:Five Levels of AI Agent Autonomy, as proposed by HuggingFace (Credit: Hugging Face) Zero stars (simple processor): The AI has no impact on the program&#x27;s flow. It simply processes information and its output is displayed, like a print statement. The human is in complete control.One star (router): The AI makes a basic decision that directs program flow, like choosing between two predefined paths (if/else). The human still defines how everything is done.Two stars (tool call): The AI chooses which predefined tool to use and what arguments to use with it. The human has defined the available tools, but the AI decides how to execute them.Three stars (multi-step agent): The AI now controls the iteration loop. It decides which tool to use, when to use it and whether to continue working on the task.Four stars (fully autonomous): The AI can generate and execute entirely new code to accomplish a goal, going beyond the predefined tools it was given.Strengths: This model is excellent for engineers. It&#x27;s concrete, maps directly to code and clearly benchmarks the transfer of executive control to the AI. Weaknesses: It is highly technical and less intuitive for non-developers trying to understand an agent&#x27;s real-world impact.Category 2: The \"How do we work together?\" frameworks (interaction-focused)This second category defines autonomy not by the agent’s internal skills, but by the nature of its relationship with the human user. The central question is: Who is in control, and how do we collaborate?This approach often mirrors the nuance we saw in the aviation models. For instance, a framework detailed in the paper Levels of Autonomy for AI Agents defines levels based on the user&#x27;s role:L1 - user as an operator: The human is in direct control (like a person using Photoshop with AI-assist features).L4 - user as an approver: The agent proposes a full plan or action, and the human must give a simple \"yes\" or \"no\" before it proceeds.L5 - user as an observer: The agent has full autonomy to pursue a goal and simply reports its progress and results back to the human.Levels of Autonomy for AI AgentsStrengths: These frameworks are highly intuitive and user-centric. They directly address the critical issues of control, trust, and oversight.Weaknesses: An agent with simple capabilities and one with highly advanced reasoning could both fall into the \"Approver\" level, so this approach can sometimes obscure the underlying technical sophistication.Category 3: The \"Who is responsible?\" frameworks (governance-focused)The final category is less concerned with how an agent works and more with what happens when it fails. These frameworks are designed to help answer crucial questions about law, safety and ethics.Think tanks like Germany&#x27;s Stiftung Neue VTrantwortung have analyzed AI agents through the lens of legal liability. Their work aims to classify agents in a way that helps regulators determine who is responsible for an agent&#x27;s actions: The user who deployed it, the developer who built it or the company that owns the platform it runs on?This perspective is essential for navigating complex regulations like the EU&#x27;s Artificial Intelligence Act, which will treat AI systems differently based on the level of risk they pose.Strengths: This approach is absolutely essential for real-world deployment. It forces the difficult but necessary conversations about accountability that build public trust.Weaknesses: It&#x27;s more of a legal or policy guide than a technical roadmap for developers.A comprehensive understanding requires looking at all three questions at once: An agent&#x27;s capabilities, how we interact with it and who is responsible for the outcome..Identifying the gaps and challengesLooking at the landscape of autonomy frameworks shows us that no single model is sufficient because the true challenges lie in the gaps between them, in areas that are incredibly difficult to define and measure.What is the \"Road\" for a digital agent?The SAE framework for self-driving cars gave us the powerful concept of an ODD, the specific conditions under which a system can operate safely. For a car, that might be \"divided highways, in clear weather, during the day.\" This is a great solution for a physical environment, but what’s the ODD for a digital agent?The \"road\" for an agent is the entire internet. An infinite, chaotic and constantly changing environment. Websites get redesigned overnight, APIs are deprecated and social norms in online communities shift. How do we define a \"safe\" operational boundary for an agent that can browse websites, access databases and interact with third-party services? Answering this is one of the biggest unsolved problems. Without a clear digital ODD, we can&#x27;t make the same safety guarantees that are becoming standard in the automotive world.This is why, for now, the most effective and reliable agents operate within well-defined, closed-world scenarios. As I argued in a recent VentureBeat article, forgetting the open-world fantasies and focusing on \"bounded problems\" is the key to real-world success. This means defining a clear, limited set of tools, data sources and potential actions. Beyond simple tool useToday&#x27;s agents are getting very good at executing straightforward plans. If you tell one to \"find the price of this item using Tool A, then book a meeting with Tool B,\" it can often succeed. But true autonomy requires much more. Many systems today hit a technical wall when faced with tasks that require:Long-term reasoning and planning: Agents struggle to create and adapt complex, multi-step plans in the face of uncertainty. They can follow a recipe, but they can&#x27;t yet invent one from scratch when things go wrong.Robust self-correction: What happens when an API call fails or a website returns an unexpected error? A truly autonomous agent needs the resilience to diagnose the problem, form a new hypothesis and try a different approach, all without a human stepping in.Composability: The future likely involves not one agent, but a team of specialized agents working together. Getting them to collaborate reliably, to pass information back and forth, delegate tasks and resolve conflicts is a monumental software engineering challenge that we are just beginning to tackle.The elephant in the room: Alignment and controlThis is the most critical challenge of all, because it&#x27;s not just technical, it&#x27;s deeply human. Alignment is the problem of ensuring an agent&#x27;s goals and actions are consistent with our intentions and values, even when those values are complex, unstated or nuanced.Imagine you give an agent the seemingly harmless goal of \"maximizing customer engagement for our new product.\" The agent might correctly determine that the most effective strategy is to send a dozen notifications a day to every user. The agent has achieved its literal goal perfectly, but it has violated the unstated, common-sense goal of \"don&#x27;t be incredibly annoying.\"This is a failure of alignment.The core difficulty, which organizations like the AI Alignment Forum are dedicated to studying, is that it is incredibly hard to specify fuzzy, complex human preferences in the precise, literal language of code. As agents become more powerful, ensuring they are not just capable but also safe, predictable and aligned with our true intent becomes the most important challenge we face.The future is agentic (and collaborative)The path forward for AI agents is not a single leap to a god-like super-intelligence, but a more practical and collaborative journey. The immense challenges of open-world reasoning and perfect alignment mean that the future is a team effort.We will see less of the single, all-powerful agent and more of an \"agentic mesh\" — a network of specialized agents, each operating within a bounded domain, working together to tackle complex problems. More importantly, they will work with us. The most valuable and safest applications will keep a human on the loop, casting them as a co-pilot or strategist to augment our intellect with the speed of machine execution. This \"centaur\" model will be the most effective and responsible path forward.The frameworks we&#x27;ve explored aren’t just theoretical. They’re practical tools for building trust, assigning responsibility and setting clear expectations. They help developers define limits and leaders shape vision, laying the groundwork for AI to become a dependable partner in our work and lives.Sean Falconer is Confluent&#x27;s AI entrepreneur in residence.",
          "feed_position": 6,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/5b0hTPUKh7PB9D7CfWwO22/3b31fdc3f95c670ce5096758d8760ccd/Agent_autonomy.png"
        }
      ],
      "featured_image": "https://s.yimg.com/os/creatr-uploaded-images/2023-11/e536a1d0-7c1e-11ee-9e77-9ea8e142b078",
      "popularity_score": 2017.7496297222222,
      "ai_summary": [
        "The guide helps users understand Apple's MacBook options and terminology.",
        "It explains the factors to consider when purchasing a MacBook.",
        "The guide compares MacBook specifications to aid in decision-making.",
        "It addresses budget MacBook options, including refurbished choices.",
        "The guide recommends checking out refurbished options from Apple or third parties."
      ]
    },
    {
      "id": "cluster_37",
      "coverage": 2,
      "updated_at": "Mon, 13 Oct 2025 23:30:01 -0400",
      "title": "Researchers used an off-the-shelf system to compile a vast collection of private data, including T-Mobile users' calls and texts, sent by satellites unencrypted (Wired)",
      "neutral_headline": "Researchers compiled private data from satellites unencrypted",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/251013/p50#a251013p50",
          "published_at": "Mon, 13 Oct 2025 23:30:01 -0400",
          "title": "Researchers used an off-the-shelf system to compile a vast collection of private data, including T-Mobile users' calls and texts, sent by satellites unencrypted (Wired)",
          "standfirst": "Wired: Researchers used an off-the-shelf system to compile a vast collection of private data, including T-Mobile users' calls and texts, sent by satellites unencrypted &mdash; With just $800 in basic equipment, researchers found a stunning variety of data&mdash;including thousands of T-Mobile users' calls &hellip;",
          "content": "Wired: Researchers used an off-the-shelf system to compile a vast collection of private data, including T-Mobile users' calls and texts, sent by satellites unencrypted &mdash; With just $800 in basic equipment, researchers found a stunning variety of data&mdash;including thousands of T-Mobile users' calls &hellip;",
          "feed_position": 12,
          "image_url": "http://www.techmeme.com/251013/i50.jpg"
        },
        {
          "source": "Wired Tech",
          "url": "https://www.wired.com/story/satellites-are-leaking-the-worlds-secrets-calls-texts-military-and-corporate-data/",
          "published_at": "Tue, 14 Oct 2025 01:00:00 +0000",
          "title": "Satellites Are Leaking the World’s Secrets: Calls, Texts, Military and Corporate Data",
          "standfirst": "With just $800 in basic equipment, researchers found a stunning variety of data—including thousands of T-Mobile users’ calls and texts and even US military communications—sent by satellites unencrypted.",
          "content": "With just $800 in basic equipment, researchers found a stunning variety of data—including thousands of T-Mobile users’ calls and texts and even US military communications—sent by satellites unencrypted.",
          "feed_position": 3,
          "image_url": "https://media.wired.com/photos/68e961a8dc203222538eb046/master/pass/security_satellites_leak_data.jpg"
        }
      ],
      "featured_image": "http://www.techmeme.com/251013/i50.jpg",
      "popularity_score": 2012.2260186111112,
      "ai_summary": [
        "Researchers used an off-the-shelf system to collect private data.",
        "The data included T-Mobile users' calls and texts.",
        "The data was sent by satellites without encryption.",
        "The researchers used approximately $800 in basic equipment.",
        "The data also included US military communications."
      ]
    },
    {
      "id": "cluster_23",
      "coverage": 1,
      "updated_at": "Tue, 14 Oct 2025 07:05:16 +0000",
      "title": "SpaceX finally got exactly what it needed from Starship V2",
      "neutral_headline": "SpaceX Achieves Goal with Starship V2, Version 3 Coming",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2025/10/after-year-of-hardships-spacexs-starship-finally-flirts-with-perfection/",
          "published_at": "Tue, 14 Oct 2025 07:05:16 +0000",
          "title": "SpaceX finally got exactly what it needed from Starship V2",
          "standfirst": "This was the last flight of SpaceX's second-gen Starship design. Version 3 arrives next year.",
          "content": "SpaceX closed a troubled but instructive chapter in its Starship rocket program Monday with a near-perfect test flight that carried the stainless steel spacecraft halfway around the world from South Texas to the Indian Ocean. The rocket's 33 methane-fueled Raptor engines roared to life at 6:23 pm CDT (7:23 pm EDT; 23:23 UTC), throttling up to generate some 16.7 million pounds of thrust, by large measure more powerful than any rocket before Starship. Moments later, the 404-foot-tall (123.1-meter) rocket began a vertical climb away from SpaceX's test site in Starbase, Texas, near the US-Mexico border. From then on, the rocket executed its flight plan like clockwork. This was arguably SpaceX's most successful Starship test flight to date. The only flight with a similar claim occurred one year ago Monday, when the company caught the rocket's Super Heavy booster back at the launch pad after soaring to the uppermost fringes of the atmosphere. But that flight didn't accomplish as much in space.Read full article Comments",
          "feed_position": 0,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/starshipflight11recap1-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/starshipflight11recap1-1152x648.jpg",
      "popularity_score": 356.81351861111114,
      "ai_summary": [
        "SpaceX successfully completed the final flight of its second-generation Starship.",
        "The next version, Starship V3, is scheduled to arrive next year.",
        "This flight marked the end of the second-gen Starship design.",
        "SpaceX is preparing for the launch of the next generation.",
        "The company is working on the development of Starship V3."
      ]
    },
    {
      "id": "cluster_52",
      "coverage": 1,
      "updated_at": "Mon, 13 Oct 2025 21:36:35 +0000",
      "title": "Hackers can steal 2FA codes and private messages from Android phones",
      "neutral_headline": "Hackers Steal 2FA Codes and Messages from Android Phones",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/security/2025/10/no-fix-yet-for-attack-that-lets-hackers-pluck-2fa-codes-from-android-phones/",
          "published_at": "Mon, 13 Oct 2025 21:36:35 +0000",
          "title": "Hackers can steal 2FA codes and private messages from Android phones",
          "standfirst": "Malicious app required to make \"Pixnapping\" attack work requires no permissions.",
          "content": "Android devices are vulnerable to a new attack that can covertly steal 2FA codes, location timelines, and other private data in less than 30 seconds. The new attack, named Pixnapping by the team of academic researchers who devised it, requires a victim to first install a malicious app on an Android phone or tablet. The app, which requires no system permissions, can then effectively read data that any other installed app displays on the screen. Pixnapping has been demonstrated on Google Pixel phones and the Samsung Galaxy S25 phone and likely could be modified to work on other models with additional work. Google released mitigations last month, but the researchers said a modified version of the attack works even when the update is installed. Like taking a screenshot Pixnapping attacks begin with the malicious app invoking Android programming interfaces that cause the authenticator or other targeted apps to send sensitive information to the device screen. The malicious app then runs graphical operations on individual pixels of interest to the attacker. Pixnapping then exploits a side channel that allows the malicious app to map the pixels at those coordinates to letters, numbers, or shapes.Read full article Comments",
          "feed_position": 1,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/01/008-family-galaxy-s25ultra-titaniumsilverblue-s25plus-navy-s25-icyblue-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/01/008-family-galaxy-s25ultra-titaniumsilverblue-s25plus-navy-s25-icyblue-1152x648.jpg",
      "popularity_score": 339.3354630555556,
      "ai_summary": [
        "Hackers can steal 2FA codes and private messages from Android phones.",
        "The \"Pixnapping\" attack requires a malicious app to function.",
        "The malicious app does not require any special permissions.",
        "This attack allows hackers to access sensitive user information.",
        "The attack exploits vulnerabilities in Android phone security."
      ]
    },
    {
      "id": "cluster_62",
      "coverage": 1,
      "updated_at": "Mon, 13 Oct 2025 19:52:23 +0000",
      "title": "Google’s Photoshop-killer AI model is coming to search, Photos, and NotebookLM",
      "neutral_headline": "Google’s Photoshop-killer AI model is coming to search, Photos, and NotebookLM",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/google/2025/10/googles-nano-banana-ai-image-editor-is-coming-to-search-photos-and-notebooklm/",
          "published_at": "Mon, 13 Oct 2025 19:52:23 +0000",
          "title": "Google’s Photoshop-killer AI model is coming to search, Photos, and NotebookLM",
          "standfirst": "After more than 5 billion AI image edits, Nano Banana is expanding.",
          "content": "Google began experimenting with conversational image editing earlier this year in the dev-focused AI studio, but the feature didn't remain experimental for long. Over the summer, Google rolled out the \"Nano Banana\" image-editing model in Gemini 2.5 Flash. You can use this feature to modify images with just a prompt, and now you don't even need to go to Gemini to use it. Google says Nano Banana is now coming to search, Google Photos, and NotebookLM. The AI image editor is coming to search via Lens and AI Mode. For Lens, you can simply open the app (iOS and Android) and snap a photo to get started. When the rollout is complete, you'll see a \"Create\" button at the bottom, with a banana icon. Tap that to enter a prompt, telling the AI how you'd like the photo changed. When you begin an edit in Lens, the Google app will display the results and offer the chance for follow-up edits in the AI Mode interface. Google is always looking for more ways to get people plugged into its conversational search bot, so there's also a separate way to access Nano Banana there. Simply select the \"Create image\" tool and enter your prompt to create an image. You can then continue the conversation to have Nano Banana change the image.Read full article Comments",
          "feed_position": 3,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/BananaLens-2096x1182.width-2200.format-webp-copy-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/BananaLens-2096x1182.width-2200.format-webp-copy-1152x648.jpg",
      "popularity_score": 334.5987963888889,
      "ai_summary": [
        "Google's AI image editing model, Nano Banana, is expanding its reach.",
        "The model will be integrated into Google Search, Photos, and NotebookLM.",
        "Nano Banana has been used for over 5 billion AI image edits.",
        "The expansion aims to provide more users with AI image editing tools.",
        "Google is integrating the model into its core services."
      ]
    },
    {
      "id": "cluster_58",
      "coverage": 1,
      "updated_at": "Mon, 13 Oct 2025 20:48:21 +0000",
      "title": "Measles outbreak in SC sends 150 unvaccinated kids into 21-day quarantine",
      "neutral_headline": "Measles Outbreak in SC Leads to Quarantine for Unvaccinated Kids",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/health/2025/10/over-150-unvaccinated-kids-quarantined-for-21-days-in-sc-measles-outbreak/",
          "published_at": "Mon, 13 Oct 2025 20:48:21 +0000",
          "title": "Measles outbreak in SC sends 150 unvaccinated kids into 21-day quarantine",
          "standfirst": "The outbreak includes at least seven confirmed cases, but some are clearly being missed.",
          "content": "Health officials in South Carolina are warning that the highly infectious measles virus is spreading undetected in communities in the northern part of the state, specifically Spartanburg and Greenville counties. Last week, officials in Greenville identified an eighth measles case that is potentially linked to the outbreak. Seven outbreak cases had been confirmed since September 25 in neighboring Spartanburg, where transmission was identified in two schools: Fairforest Elementary and Global Academy, a public charter school. Across those two schools, at least 153 unvaccinated children were exposed to the virus and have been put in a 21-day quarantine, during which they are barred from attending school, state officials said in a press conference. Twenty-one days is the maximum incubation period, spanning from when a person is exposed to when they would develop a rash if infected.Read full article Comments",
          "feed_position": 2,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/02/GettyImages-2152300024-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/02/GettyImages-2152300024-1152x648.jpg",
      "popularity_score": 318.53157416666664,
      "ai_summary": [
        "A measles outbreak in South Carolina has led to quarantines.",
        "150 unvaccinated children are under a 21-day quarantine.",
        "The outbreak includes at least seven confirmed cases of measles.",
        "Some cases are likely being missed in the outbreak.",
        "Public health officials are working to contain the outbreak."
      ]
    },
    {
      "id": "cluster_64",
      "coverage": 1,
      "updated_at": "Mon, 13 Oct 2025 19:27:10 +0000",
      "title": "Starship’s elementary era ends today with mega-rocket’s 11th test flight",
      "neutral_headline": "Starship's Elementary Era Ends with Eleventh Test Flight",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2025/10/starships-elementary-era-ends-today-with-mega-rockets-11th-test-flight/",
          "published_at": "Mon, 13 Oct 2025 19:27:10 +0000",
          "title": "Starship’s elementary era ends today with mega-rocket’s 11th test flight",
          "standfirst": "\"The final phase of Starship’s trajectory on Flight 11 includes a dynamic banking maneuver.\"",
          "content": "SpaceX is set to launch the 11th full-scale test flight of the company's Starship rocket Monday evening, with hopes of capping a tumultuous year with a successful one-hour voyage from South Texas to the Indian Ocean. Liftoff of the Super Heavy booster with the Starship upper stage is scheduled for 6:15 pm CDT (7:15 pm EDT; 23:15 UTC). SpaceX has a 75-minute window to launch Monday. You can watch a livestream of the flight here. SpaceX's control team, positioned a couple of miles away from the launch pad at Starbase, Texas, will oversee the loading of more than 10.5 million pounds of super-cold methane and liquid oxygen into the two-stage rocket beginning about an hour before liftoff. In the final minutes of the countdown, the world's largest rocket will undergo a steering check, and the launch director will give a final \"go\" for launch.Read full article Comments",
          "feed_position": 4,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/starshipflight11pre1-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/starshipflight11pre1-1152x648.jpg",
      "popularity_score": 302.1785186111111,
      "ai_summary": [
        "Starship's Flight 11 marked the end of its elementary era.",
        "The final phase of the flight included a dynamic banking maneuver.",
        "The mega-rocket completed its eleventh test flight.",
        "SpaceX is continuing to test and refine the Starship design.",
        "The flight included a dynamic banking maneuver."
      ]
    },
    {
      "id": "cluster_72",
      "coverage": 1,
      "updated_at": "Mon, 13 Oct 2025 17:16:56 +0000",
      "title": "Apple’s streaming service gets harder to tell apart from its streaming app, box",
      "neutral_headline": "Apple Simplifies Streaming Service by Dropping the \"+",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/apple/2025/10/apple-tv-streaming-service-is-renamed-to-just-apple-tv/",
          "published_at": "Mon, 13 Oct 2025 17:16:56 +0000",
          "title": "Apple’s streaming service gets harder to tell apart from its streaming app, box",
          "standfirst": "Apple unifies its remaining streaming offerings by dropping the \"+\".",
          "content": "Apple has lightly rebranded its video-on-demand streaming service. The Netflix rival that has brought us critically acclaimed shows and movies like Slow Horses and The Lost Bus has gone from Apple TV+ to Apple TV. Apple announced the name change today in a press release that was primarily about the film F1: The Movie coming to its streaming service on December 12. Unlike previous announcements, however, today’s release referred to the streaming service as Apple TV, instead of Apple TV+. The announcement reads: Apple TV+ is now simply Apple TV, with a vibrant new identity. Apple didn’t specify how its streaming service’s “identity” has changed at all. As of this writing, accessing Apple’s streaming service via a browser or smart TV app still shows the original Apple TV+ branding.Read full article Comments",
          "feed_position": 6,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2024/12/sever2-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2024/12/sever2-1152x648.jpg",
      "popularity_score": 287.00796305555554,
      "ai_summary": [
        "Apple is unifying its streaming offerings.",
        "The company is dropping the \"+\" from its streaming service.",
        "This change aims to make the service easier to understand.",
        "Apple is streamlining its streaming app and box.",
        "The change simplifies the user experience."
      ]
    },
    {
      "id": "cluster_69",
      "coverage": 1,
      "updated_at": "Mon, 13 Oct 2025 18:52:06 +0000",
      "title": "To shield kids, California hikes fake nude fines to $250K max",
      "neutral_headline": "California Increases Fines for Fake Nude Images",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/10/to-shield-kids-california-hikes-fake-nude-fines-to-250k-max/",
          "published_at": "Mon, 13 Oct 2025 18:52:06 +0000",
          "title": "To shield kids, California hikes fake nude fines to $250K max",
          "standfirst": "California cracks down on AI as child safety concerns grow.",
          "content": "California is cracking down on AI technology deemed too harmful for kids, attacking two increasingly notorious child safety fronts: companion bots and deepfake pornography. On Monday, Governor Gavin Newsom signed the first-ever US law regulating companion bots after several teen suicides sparked lawsuits. Moving forward, California will require any companion bot platforms—including ChatGPT, Grok, Character.AI, and the like—to create and make public \"protocols to identify and address users’ suicidal ideation or expressions of self-harm.\"Read full article Comments",
          "feed_position": 5,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-1262115351-2-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-1262115351-2-1152x648.jpg",
      "popularity_score": 286.59407416666664,
      "ai_summary": [
        "California is increasing fines for creating fake nude images.",
        "The maximum fine for creating fake nude images is $250,000.",
        "The state is responding to growing child safety concerns.",
        "The legislation aims to protect children from AI-generated content.",
        "California is cracking down on the misuse of AI technology."
      ]
    },
    {
      "id": "cluster_78",
      "coverage": 1,
      "updated_at": "Mon, 13 Oct 2025 16:15:53 +0000",
      "title": "Why Signal’s post-quantum makeover is an amazing engineering achievement",
      "neutral_headline": "Why Signal’s post-quantum makeover is an amazing engineering achievement",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/security/2025/10/why-signals-post-quantum-makeover-is-an-amazing-engineering-achievement/",
          "published_at": "Mon, 13 Oct 2025 16:15:53 +0000",
          "title": "Why Signal’s post-quantum makeover is an amazing engineering achievement",
          "standfirst": "New design sets a high standard for post-quantum readiness.",
          "content": "The encryption protecting communications against criminal and nation-state snooping is under threat. As private industry and governments get closer to building useful quantum computers, the algorithms protecting Bitcoin wallets, encrypted web visits, and other sensitive secrets will be useless. No one doubts the day will come, but as the now-common joke in cryptography circles observes, experts have been forecasting this cryptocalypse will arrive in the next 15 to 30 years for the past 30 years. The uncertainty has created something of an existential dilemma: Should network architects spend the billions of dollars required to wean themselves off quantum-vulnerable algorithms now, or should they prioritize their limited security budgets fighting more immediate threats such as ransomware and espionage attacks? Given the expense and no clear deadline, it’s little wonder that less than half of all TLS connections made inside the Cloudflare network and only 18 percent of Fortune 500 networks support quantum-resistant TLS connections. It's all but certain that many fewer organizations still are supporting quantum-ready encryption in less prominent protocols. Triumph of the cypherpunks One exception to the industry-wide lethargy is the engineering team that designs the Signal Protocol, the open source engine that powers the world’s most robust and resilient form of end-to-end encryption for multiple private chat apps, most notably the Signal Messenger. Eleven days ago, the nonprofit entity that develops the protocol, Signal Messenger LLC, published a 5,900-word write-up describing its latest updates that bring Signal a significant step toward being fully quantum-resistant.Read full article Comments",
          "feed_position": 7,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/signal-quantum-security-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/signal-quantum-security-1152x648.jpg",
      "popularity_score": 269.99046305555555,
      "ai_summary": [
        "Signal's post-quantum makeover is an impressive engineering feat.",
        "The new design sets a high standard for post-quantum readiness.",
        "Signal is preparing for the future of encryption.",
        "The update enhances the security of the messaging app.",
        "The engineering achievement is significant for secure communication."
      ]
    },
    {
      "id": "cluster_80",
      "coverage": 1,
      "updated_at": "Mon, 13 Oct 2025 15:33:42 +0000",
      "title": "Hans Koenigsmann, who investigated all of SpaceX’s rocket failures, is going to space",
      "neutral_headline": "Hans Koenigsmann, who investigated all of SpaceX’s rocket failures, is going to space",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2025/10/one-of-spacexs-earliest-employees-is-going-to-space-via-blue-origin/",
          "published_at": "Mon, 13 Oct 2025 15:33:42 +0000",
          "title": "Hans Koenigsmann, who investigated all of SpaceX’s rocket failures, is going to space",
          "standfirst": "\"I've always been interested to experience Max Q from the inside.\"",
          "content": "Hans Koenigsmann is one of SpaceX's earliest, longest-tenured, and most-revered employees. When Elon Musk started the company in 2002, he was joined by two other \"founding\" employees, Tom Mueller in propulsion and Chris Thompson in structures. Koenigsmann was the next hire, brought on to develop avionics for the Falcon 1 rocket. Koenigsmann remained at the company for two decades before leaving SpaceX in late 2021. During that time, he transitioned from avionics to lead mission assurance and safety while also spearheading every major failure investigation of the Falcon 9 rocket. He was a beloved leader and mentor for his employees within the company's demanding culture.Read full article Comments",
          "feed_position": 9,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-621034912-1024x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-621034912-1024x648.jpg",
      "popularity_score": 266.2874075,
      "ai_summary": [
        "Hans Koenigsmann, a SpaceX veteran, is going to space.",
        "Koenigsmann investigated all of SpaceX's rocket failures.",
        "He is interested in experiencing Max Q from the inside.",
        "Koenigsmann has been involved with SpaceX for many years.",
        "He is preparing for his first spaceflight."
      ]
    },
    {
      "id": "cluster_79",
      "coverage": 1,
      "updated_at": "Mon, 13 Oct 2025 15:49:12 +0000",
      "title": "4chan fined $26K for refusing to assess risks under UK Online Safety Act",
      "neutral_headline": "4chan fined $26K for refusing to assess risks under UK Online Safety Act",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/10/4chan-fined-26k-for-refusing-to-assess-risks-under-uk-online-safety-act/",
          "published_at": "Mon, 13 Oct 2025 15:49:12 +0000",
          "title": "4chan fined $26K for refusing to assess risks under UK Online Safety Act",
          "standfirst": "4chan fine may test if US will intervene to block UK’s Online Safety Act.",
          "content": "A battle over the United Kingdom's Online Safety Act (OSA) heated up Monday as UK regulator Ofcom fined the notorious image-hosting board 4chan about $26,000 for failing to provide a risk assessment detailing the potential harms of illegal content hosted on its forum. In a press release provided to Ars, Ofcom said 4chan refused to respond to two requests for information that the regulator considered \"routine.\" The first asked for the risk assessment and the second for 4chan's \"qualifying worldwide revenue.\" 4chan was anticipating the Monday fine, noting in a lawsuit—which was jointly filed with the online trolling forum Kiwi Farms in August and seeks to permanently enjoin Ofcom from enforcing OSA—that Ofcom had made it clear that because 4chan ignored Ofcom's emails, the fine was coming.Read full article Comments",
          "feed_position": 8,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-1814351886.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-1814351886.jpg",
      "popularity_score": 253.54574083333333,
      "ai_summary": [
        "4chan was fined $26,000 for not assessing risks.",
        "The fine is related to the UK Online Safety Act.",
        "The case may test US intervention in the UK law.",
        "The UK is enforcing its Online Safety Act.",
        "The fine is a result of 4chan's non-compliance."
      ]
    },
    {
      "id": "cluster_110",
      "coverage": 1,
      "updated_at": "Sun, 12 Oct 2025 20:50:35 +0000",
      "title": "New Starfleet Academy trailer debuts at NYCC",
      "neutral_headline": "New Starfleet Academy trailer debuts at NYCC",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/culture/2025/10/new-starfleet-academy-trailer-debuts-at-nycc/",
          "published_at": "Sun, 12 Oct 2025 20:50:35 +0000",
          "title": "New Starfleet Academy trailer debuts at NYCC",
          "standfirst": "Also: Our first look at S4 of Star Trek: Strange New Worlds.",
          "content": "The Star Trek universe panel at New York Comic Con (NYCC) this weekend concluded with a brand-new trailer for Star Trek: Starfleet Academy. The 10-episode series will follow the exploits of the first new crop of cadets in a century. Per the official premise: Star Trek: Starfleet Academy introduces viewers to a young group of cadets who come together to pursue a common dream of hope and optimism. Under the watchful and demanding eyes of their instructors, they discover what it takes to become Starfleet officers as they navigate blossoming friendships, explosive rivalries, first loves and a new enemy that threatens both the Academy and the Federation itself. The new cadets include Sandro Rosta as human orphan Caleb Mir; Karim Diané as a Klingon cadet, Jay-Den Kraag; Kerrice Brooks as Sam (Series Acclimation Mil), the first Kasquain to attend the Academy; George Hawkins as Darem Reymi, a Khionian cadet who wants to be a captain; Bella Shepard as Genesis Lythe, a Dar-Sha cadet; and Zoë Steiner as Tarima Sadal, daughter of the president of Betazed. In addition, Holly Hunter plays Academy chancellor Captain Nahla Ake; Gina Yashere plays Cadet Master Lura Thok; Becky Lynch plays a member of Starfleet bridge crew; and Paul Giamatti plays a half-Klingon, half-Tellarite character named Nus Braka, the series' chief villain.Read full article Comments",
          "feed_position": 13,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/academy1-1152x648-1760302457.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/academy1-1152x648-1760302457.jpg",
      "popularity_score": 145,
      "ai_summary": [
        "A new trailer for Starfleet Academy was shown at NYCC.",
        "The trailer provided a first look at the new series.",
        "The event also featured a look at Star Trek: Strange New Worlds S4.",
        "Fans received new content from the Star Trek universe.",
        "The trailer generated excitement for the upcoming series."
      ]
    },
    {
      "id": "cluster_82",
      "coverage": 1,
      "updated_at": "Mon, 13 Oct 2025 15:14:44 +0000",
      "title": "Layoffs, a “coding error,” chaos: Trump admin ravages the health dept.",
      "neutral_headline": "Layoffs, a “coding error,” chaos: Trump admin ravages the health dept",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/health/2025/10/mass-firings-at-us-health-dept-partially-reversed-but-still-devastating/",
          "published_at": "Mon, 13 Oct 2025 15:14:44 +0000",
          "title": "Layoffs, a “coding error,” chaos: Trump admin ravages the health dept.",
          "standfirst": "Reports suggest the hardest hit is the CDC, which is already struggling to function.",
          "content": "Federal health agencies are reeling from mass layoffs on Friday that appear to have particularly devastated the Centers for Disease Control and Prevention, despite some terminations being rescinded on Saturday. Numbers are still sketchy, but reports from Friday indicate that more than 4,000 federal workers overall were initially targeted for layoffs. The Trump administration linked the firings to the ongoing government shutdown, which legal experts have suggested is illegal. Unions representing federal workers have already filed a lawsuit challenging the move. Of the reported 4,000 terminations, about 1,100 to 1,200 were among employees in the Department of Health and Human Services (HHS). HHS is a massive department that houses critical federal agencies, including the Centers for Disease Control and Prevention, the National Institutes of Health, the Food and Drug Administration, and the Centers for Medicare & Medicaid Services, among others. Before Trump's second term, the HHS workforce was about 82,000, but that was slashed to about 62,000 earlier this year amid initial cuts and efforts to push civil servants out.Read full article Comments",
          "feed_position": 10,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/08/GettyImages-2228551722-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/08/GettyImages-2228551722-1152x648.jpg",
      "popularity_score": 133,
      "ai_summary": [
        "The Trump administration's actions are impacting the health department.",
        "Reports indicate the CDC is struggling to function.",
        "The CDC is reportedly the hardest hit by the changes.",
        "The health department is facing layoffs and other issues.",
        "A \"coding error\" is also contributing to the chaos."
      ]
    },
    {
      "id": "cluster_86",
      "coverage": 1,
      "updated_at": "Mon, 13 Oct 2025 14:57:01 +0000",
      "title": "Keep losing your key fob? Ford’s new “Truckle” is the answer.",
      "neutral_headline": "Ford Introduces Truckle for Keyless Vehicle Access",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/cars/2025/10/keep-losing-your-key-fob-fords-new-truckle-is-the-answer/",
          "published_at": "Mon, 13 Oct 2025 14:57:01 +0000",
          "title": "Keep losing your key fob? Ford’s new “Truckle” is the answer.",
          "standfirst": "Keep your pants up and your F-150 unlocked at the same time.",
          "content": "I came across possibly one of the weirdest official automotive accessories this morning, courtesy of a friend's social media feed. It's called the \"Truckle,\" and it's a hand-crafted silver and bronze belt buckle that might be the envy of every other cowboy out there, since this one has a place to keep your F-150's key fob without ruining the lines of your jeans. The Truckle was designed by Utah-based A Cut Above Buckles, with a hand-engraved F-150 on the bump in the front. Behind the truck? Storage space for a Ford truck key fob, which should fit any F-150 from model year 2018 onward. \"You can put your key fob in the buckle—all your remote features work while it’s in the buckle,\" designer Andy Andrews told the Detroit Free Press. \"Once you have it in there, you're not going to lose that key fob. You’re not going to be scratching your head (wondering) where it’s at. It's right there with you in the Truckle.\"Read full article Comments",
          "feed_position": 11,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/Truckle_7-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/Truckle_7-1152x648.jpg",
      "popularity_score": 133,
      "ai_summary": [
        "Ford's Truckle is a wearable device designed to unlock and start F-150 trucks.",
        "It aims to prevent drivers from locking keys inside the vehicle.",
        "The device is intended to be worn on a belt loop or similar location.",
        "Truckle is a solution for those who frequently misplace or lose their key fobs.",
        "It offers a convenient alternative to traditional key fobs for truck owners."
      ]
    },
    {
      "id": "cluster_90",
      "coverage": 1,
      "updated_at": "Mon, 13 Oct 2025 14:10:25 +0000",
      "title": "Software update bricks some Jeep 4xe hybrids over the weekend",
      "neutral_headline": "Jeep 4xe Hybrid Software Update Causes Vehicle Issues",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/cars/2025/10/software-update-bricks-some-jeep-4xe-hybrids-over-the-weekend/",
          "published_at": "Mon, 13 Oct 2025 14:10:25 +0000",
          "title": "Software update bricks some Jeep 4xe hybrids over the weekend",
          "standfirst": "Jeep has pulled the update; owners are advised to ignore it if it already downloaded.",
          "content": "Owners of some Jeep Wrangler 4xe hybrids have been left stranded after installing an over-the-air software update this weekend. The automaker pushed out a telematics update for the Uconnect infotainment system that evidently wasn't ready, resulting in cars losing power while driving and then becoming stranded. Stranded Jeep owners have been detailing their experiences in forum and Reddit posts, as well as on YouTube. The buggy update doesn't appear to brick the car immediately. Instead, the failure appears to occur while driving—a far more serious problem. For some, this happened close to home and at low speed, but others claim to have experienced a powertrain failure at highway speeds. Jeep pulled the update after reports of problems, but the software had already downloaded to many owners' cars by then. A member of Stellantis' social engagement team told 4xe owners at a Jeep forum to ignore the update pop-up if they haven't installed it yet.Read full article Comments",
          "feed_position": 12,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/2024Wrangler4xe-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/2024Wrangler4xe-1152x648.jpg",
      "popularity_score": 133,
      "ai_summary": [
        "A recent software update caused some Jeep 4xe hybrid vehicles to become unusable.",
        "Jeep has withdrawn the problematic software update to prevent further issues.",
        "Owners who have not yet installed the update are advised to ignore it.",
        "The update was intended to improve vehicle performance and functionality.",
        "Jeep is working on a fix to address the problems caused by the update."
      ]
    }
  ]
}