{
  "updated_at": "2025-10-24T23:17:32.219Z",
  "clusters": [
    {
      "id": "cluster_118",
      "coverage": 3,
      "updated_at": "Thu, 23 Oct 2025 21:48:01 GMT",
      "title": "Trump pardons founder of Binance, world’s largest crypto exchange",
      "neutral_headline": "Trump pardons founder of Binance, world’s largest crypto exchange",
      "items": [
        {
          "source": "Guardian Tech",
          "url": "https://www.theguardian.com/technology/2025/oct/23/binance-trump-pardon-changpeng-zhao",
          "published_at": "Thu, 23 Oct 2025 21:48:01 GMT",
          "title": "Trump pardons founder of Binance, world’s largest crypto exchange",
          "standfirst": "Changpeng Zhao pleaded guilty to failing to stop money laundering in 2023 and was sentenced to four monthsDonald Trump issued a pardon for the founder of the world’s largest cryptocurrency exchange on Thursday.“President Trump exercised his constitutional authority by issuing a pardon for Mr Zhao, who was prosecuted by the Biden administration in their war on cryptocurrency,” a White House statement said. “The war on crypto is over.” Continue reading...",
          "content": "Changpeng Zhao pleaded guilty to failing to stop money laundering in 2023 and was sentenced to four monthsDonald Trump issued a pardon for the founder of the world’s largest cryptocurrency exchange on Thursday.“President Trump exercised his constitutional authority by issuing a pardon for Mr Zhao, who was prosecuted by the Biden administration in their war on cryptocurrency,” a White House statement said. “The war on crypto is over.” Continue reading...",
          "feed_position": 6
        },
        {
          "source": "Wired Tech",
          "url": "https://www.wired.com/story/trump-pardons-cz-binance/",
          "published_at": "Thu, 23 Oct 2025 18:05:37 +0000",
          "title": "‘War on Crypto Is Over’: Donald Trump Pardons Binance Founder CZ",
          "standfirst": "After serving a federal prison sentence for violating anti-money-laundering laws and US sanctions, former crypto exchange CEO Changpeng Zhao has been pardoned by US president Donald Trump.",
          "content": "After serving a federal prison sentence for violating anti-money-laundering laws and US sanctions, former crypto exchange CEO Changpeng Zhao has been pardoned by US president Donald Trump.",
          "feed_position": 21,
          "image_url": "https://media.wired.com/photos/68fa4fd3c5809c60f31dd079/master/pass/Trump-Pardon-Binance-CEO-Business-2210816860.jpg"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/big-tech/binance-founder-changpeng-zhao-lands-a-trump-pardon-174929498.html",
          "published_at": "Thu, 23 Oct 2025 17:49:29 +0000",
          "title": "Binance founder Changpeng Zhao lands a Trump pardon",
          "standfirst": "President Donald Trump has pardoned Binance founder Changpeng Zhao, the White House said. Zhao pleaded guilty to federal money laundering charges in 2023 and he was sentenced last year to four months in prison. He was released in September 2024. As part of his plea deal, Zhao stepped down as CEO of Binance and he was banned from having any involvement with the company for three years. Both Zhao and Binance reportedly submitted formal applications for pardons by August this year.Trump \"exercised his constitutional authority by issuing a pardon for Mr. Zhao, who was prosecuted by the Biden Administration in their war on cryptocurrency,\" White House press secretary Karoline Leavitt said in a statement. \"The Biden Administration’s war on crypto is over.\"The Wall Street Journal notes that the pardon could pave the way for Binance to start doing business in the US again. The company was barred from operating there after pleading guilty to violating money laundering laws in 2023. Binance officials are said to have met with Treasury Department representatives this year in an attempt to reduce US oversight of the company. Binance is involved with the Trump family's World Liberty Financial cryptocurrency business — a venture that has padded the president's pockets. For one thing, it bolstered the growth of USD1, a World Liberty cryptocurrency that's pegged to the dollar. Binance received a $2 billion investment this spring and that was paid in USD1. According to CNBC, World Liberty has generated around $4.5 billion since last year's presidential election.FTX founder Sam Bankman-Fried has also reportedly been angling for a Trump pardon. Bankman-Fried was sentenced to 25 years in prison in 2024 after being found guilty of fraud and conspiracy to commit money laundering.Crypto billionaire Justin Sun said last November that he'd invested $30 million into World Liberty (a figure that later rose to $75 million). In February, the Securities and Exchange Commission dropped a case against Sun. The agency had charged him in 2023 with alleged violations of securities laws. This article originally appeared on Engadget at https://www.engadget.com/big-tech/binance-founder-changpeng-zhao-lands-a-trump-pardon-174929498.html?src=rss",
          "content": "President Donald Trump has pardoned Binance founder Changpeng Zhao, the White House said. Zhao pleaded guilty to federal money laundering charges in 2023 and he was sentenced last year to four months in prison. He was released in September 2024. As part of his plea deal, Zhao stepped down as CEO of Binance and he was banned from having any involvement with the company for three years. Both Zhao and Binance reportedly submitted formal applications for pardons by August this year.Trump \"exercised his constitutional authority by issuing a pardon for Mr. Zhao, who was prosecuted by the Biden Administration in their war on cryptocurrency,\" White House press secretary Karoline Leavitt said in a statement. \"The Biden Administration’s war on crypto is over.\"The Wall Street Journal notes that the pardon could pave the way for Binance to start doing business in the US again. The company was barred from operating there after pleading guilty to violating money laundering laws in 2023. Binance officials are said to have met with Treasury Department representatives this year in an attempt to reduce US oversight of the company. Binance is involved with the Trump family's World Liberty Financial cryptocurrency business — a venture that has padded the president's pockets. For one thing, it bolstered the growth of USD1, a World Liberty cryptocurrency that's pegged to the dollar. Binance received a $2 billion investment this spring and that was paid in USD1. According to CNBC, World Liberty has generated around $4.5 billion since last year's presidential election.FTX founder Sam Bankman-Fried has also reportedly been angling for a Trump pardon. Bankman-Fried was sentenced to 25 years in prison in 2024 after being found guilty of fraud and conspiracy to commit money laundering.Crypto billionaire Justin Sun said last November that he'd invested $30 million into World Liberty (a figure that later rose to $75 million). In February, the Securities and Exchange Commission dropped a case against Sun. The agency had charged him in 2023 with alleged violations of securities laws. This article originally appeared on Engadget at https://www.engadget.com/big-tech/binance-founder-changpeng-zhao-lands-a-trump-pardon-174929498.html?src=rss",
          "feed_position": 41
        }
      ],
      "featured_image": "https://media.wired.com/photos/68fa4fd3c5809c60f31dd079/master/pass/Trump-Pardon-Binance-CEO-Business-2210816860.jpg",
      "popularity_score": 3000,
      "ai_summary": [
        "Donald Trump pardoned Changpeng Zhao, founder of Binance.",
        "Zhao pleaded guilty to money laundering charges and was sentenced.",
        "The White House stated the \"war on crypto is over\".",
        "Zhao was banned from Binance for three years as part of his plea deal.",
        "The pardon could allow Binance to resume business in the US."
      ]
    },
    {
      "id": "cluster_5",
      "coverage": 2,
      "updated_at": "Fri, 24 Oct 2025 18:01:09 -0400",
      "title": "Sources detail how Sam Altman, Jensen Huang, Marc Benioff, and others brokered a call between SF Mayor Daniel Lurie and Trump to stop National Guard deployment (Wall Street Journal)",
      "neutral_headline": "Sources detail how Sam Altman, Jensen Huang, Marc Benioff, and others brokered a call between SF Mayor Daniel Lurie and Trump to stop National Guard deployment (Wall Street Journal)",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/251024/p20#a251024p20",
          "published_at": "Fri, 24 Oct 2025 18:01:09 -0400",
          "title": "Sources detail how Sam Altman, Jensen Huang, Marc Benioff, and others brokered a call between SF Mayor Daniel Lurie and Trump to stop National Guard deployment (Wall Street Journal)",
          "standfirst": "Wall Street Journal: Sources detail how Sam Altman, Jensen Huang, Marc Benioff, and others brokered a call between SF Mayor Daniel Lurie and Trump to stop National Guard deployment &mdash; Sam Altman, Jensen Huang and Marc Benioff conferred with Mayor Daniel Lurie on how to persuade the president not to deploy the National Guard",
          "content": "Wall Street Journal: Sources detail how Sam Altman, Jensen Huang, Marc Benioff, and others brokered a call between SF Mayor Daniel Lurie and Trump to stop National Guard deployment &mdash; Sam Altman, Jensen Huang and Marc Benioff conferred with Mayor Daniel Lurie on how to persuade the president not to deploy the National Guard",
          "feed_position": 1,
          "image_url": "http://www.techmeme.com/251024/i20.jpg"
        },
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2025/10/23/trump-credits-benioff-huang-for-decision-not-to-surge-fed-troops-into-san-francisco/",
          "published_at": "Thu, 23 Oct 2025 21:04:00 +0000",
          "title": "Trump credits Benioff, Huang for decision not to ‘surge’ Fed troops into San Francisco",
          "standfirst": "Trump said he scrapped plans to send the National Guard into San Francisco after calls from Nvidia’s Jensen Huang and Salesforce’s Marc Benioff. Mayor Daniel Lurie confirmed the plan was canceled.",
          "content": "Trump said he scrapped plans to send the National Guard into San Francisco after calls from Nvidia’s Jensen Huang and Salesforce’s Marc Benioff. Mayor Daniel Lurie confirmed the plan was canceled.",
          "feed_position": 16
        }
      ],
      "featured_image": "http://www.techmeme.com/251024/i20.jpg",
      "popularity_score": 2018.7268836111111,
      "ai_summary": [
        "Sam Altman, Jensen Huang, and Marc Benioff contacted Mayor Daniel Lurie.",
        "They conferred on how to persuade Trump to avoid National Guard deployment.",
        "Trump decided against sending the National Guard to San Francisco.",
        "The Wall Street Journal reported on the events.",
        "The decision followed calls from tech leaders."
      ]
    },
    {
      "id": "cluster_39",
      "coverage": 2,
      "updated_at": "Fri, 24 Oct 2025 13:55:02 -0400",
      "title": "Netflix shuts down Boss Fight Entertainment, the game studio behind mobile game Squid Game: Unleashed, after acquiring it in March 2022 (Jay Peters/The Verge)",
      "neutral_headline": "Netflix shuts down Boss Fight Entertainment, the game studio behind mobile game Squid Game: Unleashed, after acquiring it in March 2022 (Jay Peters/The Verge)",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/251024/p14#a251024p14",
          "published_at": "Fri, 24 Oct 2025 13:55:02 -0400",
          "title": "Netflix shuts down Boss Fight Entertainment, the game studio behind mobile game Squid Game: Unleashed, after acquiring it in March 2022 (Jay Peters/The Verge)",
          "standfirst": "Jay Peters / The Verge: Netflix shuts down Boss Fight Entertainment, the game studio behind mobile game Squid Game: Unleashed, after acquiring it in March 2022 &mdash; &#65279;More than three years after acquiring Boss Fight Entertainment, Netflix is closing it. &hellip; Netflix acquired Boss Fight in March 2022 &hellip;",
          "content": "Jay Peters / The Verge: Netflix shuts down Boss Fight Entertainment, the game studio behind mobile game Squid Game: Unleashed, after acquiring it in March 2022 &mdash; &#65279;More than three years after acquiring Boss Fight Entertainment, Netflix is closing it. &hellip; Netflix acquired Boss Fight in March 2022 &hellip;",
          "feed_position": 7,
          "image_url": "http://www.techmeme.com/251024/i14.jpg"
        },
        {
          "source": "The Verge",
          "url": "https://www.theverge.com/news/806303/netflix-squid-game-unleashed-boss-fight-entertainment-studio-shut-down",
          "published_at": "2025-10-24T12:35:55-04:00",
          "title": "Netflix shuts down its Squid Game mobile studio",
          "standfirst": "Netflix has shut down Boss Fight Entertainment, the studio behind the mobile game Squid Game: Unleashed, according to posts from staffers on LinkedIn. Netflix acquired Boss Fight in March 2022, with an executive saying at the time that the studio’s “extensive experience building hit games across genres will help accelerate our ability to provide Netflix [&#8230;]",
          "content": "Netflix has shut down Boss Fight Entertainment, the studio behind the mobile game Squid Game: Unleashed, according to posts from staffers on LinkedIn. Netflix acquired Boss Fight in March 2022, with an executive saying at the time that the studio’s “extensive experience building hit games across genres will help accelerate our ability to provide Netflix members with great games wherever they want to play them.” The company has frequently touted the success of Squid Game: Unleashed, highlighting how it was the “ #1 Free Action Game in 107 countries upon release” and co-CEO Greg Peters pointing to Unleashed during this week’s earnings call as an example of the types of narrative games based on its own franchises that it wants to do more of. But over three years after the acquisition and some changes to Netflix’s gaming strategy, Boss Fight has been closed. Netflix declined to comment. “Hi everyone – well, word has gotten around quickly about Boss Fight’s closure,” Boss Fight co-founder and former CEO David Rippy said. “Thanks, everyone who reached out today. Rough news, for sure, but I’m very grateful for the time we had at Netflix.” “After 10+ great years working at Boss Fight, the last few as part of Netflix, the time has come for the studio to close down,” David Luehmann, a director of game development at the studio, said in a post. “I am very proud of all the people, work, and games we&#8217;ve released. I wish you could see what we had cooking!” The shutdown follows persistent layoffs across the industry, including cut jobs at Dune: Awakening developer Funcom despite that game’s success and layoffs at Cloud Chamber, the studio making the next title in the BioShock franchise. Last year, Netflix closed its AAA game studio before it ever released a game. Netflix’s next big gaming initiative is focused around party games you can play on your TV using your phone as a controller, and Peters said this week that the company is ‘judiciously’ expanding into interactive experiences.",
          "feed_position": 9
        }
      ],
      "featured_image": "http://www.techmeme.com/251024/i14.jpg",
      "popularity_score": 2014.6249391666668,
      "ai_summary": [
        "Netflix is closing Boss Fight Entertainment, a game studio.",
        "Netflix acquired Boss Fight in March 2022.",
        "The studio developed the mobile game \"Squid Game: Unleashed\".",
        "The closure was announced by Netflix.",
        "The studio's closure comes over three years after acquisition."
      ]
    },
    {
      "id": "cluster_50",
      "coverage": 2,
      "updated_at": "Fri, 24 Oct 2025 16:39:18 +0000",
      "title": "Best iPad deals: Get over $300 off the iPad Air M3 with cellular",
      "neutral_headline": "Best iPad deals: Get over $300 off the iPad Air M3 with cellular",
      "items": [
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/best-ipad-deals-get-over-300-off-the-ipad-air-m3-with-cellular-150020542.html",
          "published_at": "Fri, 24 Oct 2025 16:39:18 +0000",
          "title": "Best iPad deals: Get over $300 off the iPad Air M3 with cellular",
          "standfirst": "The just-released iPad Pro with the M5 chip tops our list of the best tablets and the standard iPad is our pick for the best budget slate. While the former is expectedly not on sale yet, we are seeing a modest discount for the cheaper iPad. The lovely iPad Air (13-inch, with cellular) is down to a record low as well. Of course, you won't find deals on Apple's own website, but we keep an eye on Amazon, Target, Walmart and other retailers to find the best iPad deals out there and round them up each Friday. This week, the discounts aren't as good as they were for Prime Day earlier this month — chances are, we won't see a huge influx of Apple deals until Black Friday sales start up. Until then, here are the top deals on iPads and all the other Apple gear we could find. Best iPad deals Apple iPad (A16, 256GB) for $399 ($50 off): The latest entry-level iPad comes with a faster A16 chip, 2GB more RAM and more base storage. It earned a score of 84 in our review — if you only need a tablet for roaming the internet, watching shows and doing some lighter productivity tasks, it should do the job. With the recent iPadOS 26 update, it also has most of the same multitasking features available on the more expensive models. It does lack Apple Intelligence, but to be candid, that isn't a big loss right now. This deal isn't an all-time low for the model with 256GB of storage but it takes $50 off Apple's list price. Also at Best Buy. Apple iPad Air (11-inch, M3, 1TB) for $949 ($150 off MSRP): The most recent iPad Air is a relatively minor update, as the only major addition is a more powerful M3 chip. However, we still recommend the Air over the base model in our iPad buying guide: Its display is laminated, more color-rich and better at fending off glare (though it's still 60Hz); its speakers are more robust; it works with Apple’s best accessories and its performance should hold up better in the years ahead. This deal is only for the maxed-out model with 1TB of storage, but it ties the lowest price we've seen all the same. Best Apple deals Apple Pencil Pro for $99 ($30 off): The top-end option in Apple’s confusing stylus lineup, the Pencil Pro supports pressure sensitivity, wireless charging, tilt detection, haptic feedback and Apple’s double tap and squeeze gestures, among other perks. It’s a lovely tool for more intricate sketching and note-taking, but the catch is that it’s only compatible with the M4 iPad Pro, M2 and M3 iPad Air and most recent iPad mini. We've seen this deal fairly often over the course of the year, but it's a fine discount compared to buying from Apple directly. Also at Walmart. Apple MacBook Air (13-inch, M4, 512GB) for $999 ($200 off): Apple's latest MacBook Air is the top pick in our guide to the best laptops, and it earned a score of 92 in our review. It's not a major overhaul, but the design is still exceptionally thin, light and well-built, with long battery life and a top-notch keyboard and trackpad. Now it's a bit faster. (Though we'd still love more ports and a refresh rate higher than 60Hz.) This discount ties the all-time low for the model with 16GB of RAM and a 512GB SSD. Apple Watch Series 11 (GPS, 42mm) for $389 ($10 off): The latest flagship Apple Watch only hit store shelves last month, but Amazon is already selling it for $10 off. It doesn't show up as a percentage off, but you'll see some models listed at $389 instead of Apple's $399 MSRP. If you're new to Apple's wearables or are ready to upgrade from a Series 9 or older, this is a good model to grab. If you're coming from a Series 10, however, there's not much need to upgrade as the only major change from last year's model is a slightly larger battery and a tougher screen. Apple Watch SE 3 (GPS, 40mm) for $240 ($9 off): There's a similar stealth discount for the newest budget model, the Apple Watch SE 3, at Amazon. It normally goes for $249 — again, not a big discount, but better than nothing if you're looking to get onboard early. Apple gave this model some badly needed updates compared to its predecessor, including an always-on display, faster charging, better sensors and the same processor that you'll find in the new Apple Watch Series 11. Read more Apple coverage: The best AirPods The best Apple Watches The best MacBooks The best iPhones The best iPads Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/best-ipad-deals-get-over-300-off-the-ipad-air-m3-with-cellular-150020542.html?src=rss",
          "content": "The just-released iPad Pro with the M5 chip tops our list of the best tablets and the standard iPad is our pick for the best budget slate. While the former is expectedly not on sale yet, we are seeing a modest discount for the cheaper iPad. The lovely iPad Air (13-inch, with cellular) is down to a record low as well. Of course, you won't find deals on Apple's own website, but we keep an eye on Amazon, Target, Walmart and other retailers to find the best iPad deals out there and round them up each Friday. This week, the discounts aren't as good as they were for Prime Day earlier this month — chances are, we won't see a huge influx of Apple deals until Black Friday sales start up. Until then, here are the top deals on iPads and all the other Apple gear we could find. Best iPad deals Apple iPad (A16, 256GB) for $399 ($50 off): The latest entry-level iPad comes with a faster A16 chip, 2GB more RAM and more base storage. It earned a score of 84 in our review — if you only need a tablet for roaming the internet, watching shows and doing some lighter productivity tasks, it should do the job. With the recent iPadOS 26 update, it also has most of the same multitasking features available on the more expensive models. It does lack Apple Intelligence, but to be candid, that isn't a big loss right now. This deal isn't an all-time low for the model with 256GB of storage but it takes $50 off Apple's list price. Also at Best Buy. Apple iPad Air (11-inch, M3, 1TB) for $949 ($150 off MSRP): The most recent iPad Air is a relatively minor update, as the only major addition is a more powerful M3 chip. However, we still recommend the Air over the base model in our iPad buying guide: Its display is laminated, more color-rich and better at fending off glare (though it's still 60Hz); its speakers are more robust; it works with Apple’s best accessories and its performance should hold up better in the years ahead. This deal is only for the maxed-out model with 1TB of storage, but it ties the lowest price we've seen all the same. Best Apple deals Apple Pencil Pro for $99 ($30 off): The top-end option in Apple’s confusing stylus lineup, the Pencil Pro supports pressure sensitivity, wireless charging, tilt detection, haptic feedback and Apple’s double tap and squeeze gestures, among other perks. It’s a lovely tool for more intricate sketching and note-taking, but the catch is that it’s only compatible with the M4 iPad Pro, M2 and M3 iPad Air and most recent iPad mini. We've seen this deal fairly often over the course of the year, but it's a fine discount compared to buying from Apple directly. Also at Walmart. Apple MacBook Air (13-inch, M4, 512GB) for $999 ($200 off): Apple's latest MacBook Air is the top pick in our guide to the best laptops, and it earned a score of 92 in our review. It's not a major overhaul, but the design is still exceptionally thin, light and well-built, with long battery life and a top-notch keyboard and trackpad. Now it's a bit faster. (Though we'd still love more ports and a refresh rate higher than 60Hz.) This discount ties the all-time low for the model with 16GB of RAM and a 512GB SSD. Apple Watch Series 11 (GPS, 42mm) for $389 ($10 off): The latest flagship Apple Watch only hit store shelves last month, but Amazon is already selling it for $10 off. It doesn't show up as a percentage off, but you'll see some models listed at $389 instead of Apple's $399 MSRP. If you're new to Apple's wearables or are ready to upgrade from a Series 9 or older, this is a good model to grab. If you're coming from a Series 10, however, there's not much need to upgrade as the only major change from last year's model is a slightly larger battery and a tougher screen. Apple Watch SE 3 (GPS, 40mm) for $240 ($9 off): There's a similar stealth discount for the newest budget model, the Apple Watch SE 3, at Amazon. It normally goes for $249 — again, not a big discount, but better than nothing if you're looking to get onboard early. Apple gave this model some badly needed updates compared to its predecessor, including an always-on display, faster charging, better sensors and the same processor that you'll find in the new Apple Watch Series 11. Read more Apple coverage: The best AirPods The best Apple Watches The best MacBooks The best iPhones The best iPads Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/best-ipad-deals-get-over-300-off-the-ipad-air-m3-with-cellular-150020542.html?src=rss",
          "feed_position": 7
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/audio/headphones/sennheiser-hdb-630-review-a-sonic-marvel-with-room-for-improvement-150000295.html",
          "published_at": "Fri, 24 Oct 2025 15:00:00 +0000",
          "title": "Sennheiser HDB 630 review: A sonic marvel with room for improvement",
          "standfirst": "High-resolution audio on the go isn’t very convenient. It typically involves wired headphones and a DAC (digital-to-analog converter) of some kind, plus your phone or another device to access files or a streaming service. All of this is necessary since Bluetooth compresses an audio signal by design, to allow for low-latency transmission and minimize battery draw. Simply put, wireless headphones haven’t been able to meet the demands of lossless audio, but Sennheiser has come the closest to fulfilling the dream with its HDB 630 ($500). Thanks to redesigned drivers, a new acoustic platform and a dongle, the company offers up to 24-bit/96kHz audio on the HDB 630 — depending on your configuration. You also get above average active noise cancellation (ANC), a highly customizable EQ, shockingly long battery life and advanced features to fine-tune the headphones to your liking. For some, the best possible sound is still only found on pricey setups and open-back headphones. For everyone else, Sennheiser has provided a taste of the audiophile life in a much more portable package. Design Sennheiser says the HDB 630 “inherited” the same chassis from its Momentum 4 headphones. That’s unfortunate because my biggest complaint with that older model's redesign is how cheap it looked compared to previous entries in the Momentum line. The HDB 630 suffers the same fate, although the splash of silver on the headband and yokes helps things a bit. Simply put, these don’t look like a set of $500 headphones, and since they’re $150 more than their predecessor was at launch, they really should have a more premium appearance. The outside of the right ear cup is still a touch panel where you can swipe, tap and even pinch to control the HDB 630. I don’t recall another set of headphones with a pinch gesture, and I’m still not convinced it’s warranted. The action is used to enable an Adaptive ANC adjustment that allows you to dial in the amount of noise blocking you need. After the pinch, sliding a single finger forwards and backwards fine tunes the mix of ANC and transparency mode. It’s a nice option to have on the headphones themselves, I just think a triple tap to activate it would be easier to master — and remember. The only other button on the HDB 630 is for power and Bluetooth pairing. Unless you’re frequently connecting these headphones to a new device, you might not be reaching for this control very often. That’s because the HDB 630 goes into standby mode when you take them off before powering down completely after 15 minutes of inactivity. You can extend that window to 30 or 60 minutes if you prefer. And if the headphones still have battery left, you can return to active mode by simply putting them back on your head. Sennheiser is betting you’ll use the HDB 630 for long listening sessions, so it outfitted these headphones with soft ear pads and a well-cushioned headband. The clamping force is adequate for a proper ANC seal, but never becomes a burden. And despite being around 20 grams heavier than the Momentum 4, this model still feels balanced and doesn’t weigh you down. Sound quality The HDB 630 features new drivers and a specially designed acoustic system. Billy Steele for Engadget While the overall design may be familiar, the sound platform for the HDB 630 is completely new. 42mm drivers offer what Sennheiser says is “neutral sound with lifelike mids, stunning detail and a wide soundstage.” In order to deliver sound quality that’s as close to open-back headphones as possible, the company overhauled the entire acoustic system, from the drivers to the baffle’s transparent mesh, in the name of balance and clarity. And since audiophile headphones typically require a dedicated external amplifier to achieve their full potential, Sennheiser included a BTD 700 USB-C dongle for high-resolution wireless audio transmission. When I first put the HDB 630 on, I thought the audio quality was good but not great. Listening over the standard definition SBC codec produced decent results, but it wasn’t anything to write home about. Once I connected to the BTD 700 dongle and unlocked 16-bit/48kHz tunes from Apple Music, though, these headphones really started to impress. As good as they are, the HDB 630 may not be for everyone. That “neutral” stock tuning places high emphasis on the midrange, so you’ll likely need to make some adjustments to get the bass performance you crave from rock, electronic, hip-hop and other genres driven by low-end tone. While I concede the neutral base is a great starting point, and the HDB 630 does indeed showcase “stunning detail,” I’d argue Sennheiser’s promise of “a wide soundstage” doesn’t always hold true. These headphones are at their best with more immersive content, like the TRON: Ares soundtrack from Nine Inch Nails. After a slight adjustment, the electronic score had the booming bass it needed, offering driving beats that nearly rattled my brain. All that was layered with rich synths and Trent Reznor’s iconic vocals. The texture and distortion in the instruments came through in greater detail too, something that’s not as apparent on other headphones and earbuds. Switch over to Thrice’s Horizons/West and the HDB 630 is a different story. Transitioning from synth-heavy electronic music to a genre like rock causes these headphones to lose some of the immersive character they are capable of delivering. You still get absurd clarity and detail, particularly in Teppei Teranishi’s guitar riffs, but the music sounds slightly flatter and a little less energetic. It’s not bad by any means, but some genres won’t envelope you as much as others do. You can also use the HDB 630 wired over USB-C for lossless-quality audio. Since a number of competitors also do this, I dedicated the bulk of my testing to see if Sennheiser’s wireless dongle is meaningfully different. Of course, I did my due diligence and tested the wired configuration a few times, and it should come as no surprise that the HDB 630 sounds just as good in that setup. Software, features and accessories There's only one button on the HDB 630. Billy Steele for Engadget As I mentioned, the HDB 630 comes with Sennheiser’s BTD 700 Bluetooth USB dongle. This enables higher quality streaming than you’ll natively get from most devices. With the BTD 700, you can expect aptX Adaptive and aptX Lossless listening up at rates to 24-bit/96kHz. The dongle also has a 30ms low-latency gaming mode, (supposedly) enhanced call performance and Auracast support for streaming to multiple headphones or speakers. The BTD 700 has a USB-C connector, but it comes with a USB-A adapter if you need it. This typically costs $60 if you buy it on its own, and since you need it to unlock the HDB 630’s full potential, it’s great to see it included in the box. The HDB 630’s settings and features are accessible in the Sennheiser Smart Control Plus app. And for this model, the company is offering a lot more customization than it does on the Accentum or Momentum headphones. First, the EQ editing options are more robust thanks to a parametric equalizer, which allows you to get a lot more detailed with your custom presets. For example, I was able to add the low-end tone I feel is missing from the stock tuning for those metal, rock and hip-hop tracks I mentioned before. And unlike a lot of headphone apps, adjusting the EQ actually improves the sound instead of just muddying things further. Another sound-related addition for the HDB 630 is Crossfeed. This allows you to blend the left and right channels so that it seems like you’re listening to speakers instead of headphones. Unfortunately, you only get two options here — Low and High — but the effect certainly enhances the sonic profile of the HDB 630 at both settings. Despite the BTD 700 dongle’s Mac and Windows compatibility, there’s no desktop version of the Smart Control Plus app. This means you’ll have to change all of your settings with the HDB 630 through your phone before you pair it with both the dongle and your computer. It would be nice if you could make EQ adjustments, create new presets and even change Crossfeed levels without having to reconnect to another device. This also means you can’t be connected to the BTD 700 and both your phone and your computer, since the dongle takes one of the two available multipoint Bluetooth slots. Active noise cancellation and call quality The HDB 630 has a very basic design with lots of plastic. Billy Steele for Engadget When it comes to ANC performance, I’m not entirely sure that the HDB 630 is better than the Momentum 4. But that’s okay. That previous model brought a significant improvement compared to Sennheiser’s older wireless headphones and the ANC is still quite good here. In fact, it was robust enough to block my family’s voices during their calls while I worked from home, and since most headphones struggle with this, that’s no mean feat. Sennheiser says the BTD 700 dongle will give you improved voice performance over the headphones alone. Specifically, the accessory should provide extended range, clearer voice pickup and, according to the company, “uninterrupted” calls. In my recorded samples, I think the headphones themselves sounded slightly better than when I captured my voice while connected to the BTD 700. However, I noticed a distinct lack of background noise in both clips, which is helpful in busier environments. I’ll also note the overall voice quality isn’t pristine, but it’s clear enough to use for work calls — even if you’re the main presenter. Battery life Sennheiser promises that you’ll get up to 60 hours of battery life on a charge with the HDB 630. That’s the same staggering figure the company claims on the Momentum 4. And yes, that’s with ANC enabled, but you’ll only achieve that if you’re listening to standard resolution tunes. Based on my testing with a mix of noise cancellation and transparency mode while I was listening to music and taking work calls, I have no reason to believe the company’s numbers don’t hold true. If you choose to listen entirely via the BTD 700’s higher quality output, you can expect up to 45 hours of use on a charge. That’s still quite a long time considering a lot of the competition runs out at around 30 hours — and that’s without high-res music. Due to all of the signal processing that helps with the acoustic performance on the HDB 630, they can only be used when they’re turned on. Unlike some wireless models, you can’t use these as wired headphones when the battery is spent. However, if you find yourself with a completely depleted battery, a 10-minute charge will give you up to seven hours of use. The company doesn’t specify streaming resolution for that number, but I assume it’s at standard definition. Still, you’ll get a few hours of higher-res music in that time, which should be enough to get you through a work session, evening commute or that new album you’re dying to play for the first time. The competition Incredible sound awaits, if you're okay to carry a dongle around with your headphones. Billy Steele for Engadget In the realm of flagship headphones, any company’s top-of-the-line model will set you back $500 these days. I look back fondly on the time when $300-$350 got you the best Sony had to offer. While the HDB 630 is expensive, it’s also in the same ballpark of what you’ll pay for the Bose QuietComfort Ultra Headphones ($450), the Sony WH-1000XM6 ($458 currently) and the AirPods Max ($549). Each of those have their advantages over the rest of the competition, with the 1000XM6 offering the most complete package overall. However, when it comes to pure sound quality, neither of those three are at the top of the heap. Up until now, that title belonged to the Noble Audio FoKus Apollo. At $650, those headphones are even more expensive than the HDB 630, but their stock tuning will appeal to more listeners and the soundstage is wider and more immersive. There’s also Bowers & Wilkins’ Px7 S3 for a slightly cheaper $479. It delivers the company’s warm, inviting sound and attention to finer details. After spending time with the HDB 630 though, these alternatives are just that — alternatives — as the new Sennheiser headphones are now my pick for best overall sound quality. Wrap-up I get it: in the current financial climate, $500 is a lot to pay for headphones (or anything else, for that matter). You can find a number of perfectly capable sets of ANC headphones for much less given how frequently things go on sale these days. However, what you won’t find is an option that gives you anything close to the performance of audiophile-grade, open-back headphones. That’s really what Sennheiser is doing here, and the HDB 630 slots nicely into the company’s HD 600 series of high-end cans. As good as the HDB 630 is sound-wise, I can also appreciate that these aren’t the best headphones for everyone. The company’s Momentum 4 is still a very capable set of headphones and it’s now available for about $250. If you crave the best sound quality that still offers the convenience of wireless headphones — and you’re okay with a few extra steps — the HDB 630 is a worthy investment. Just don’t leave home without that dongle.This article originally appeared on Engadget at https://www.engadget.com/audio/headphones/sennheiser-hdb-630-review-a-sonic-marvel-with-room-for-improvement-150000295.html?src=rss",
          "content": "High-resolution audio on the go isn’t very convenient. It typically involves wired headphones and a DAC (digital-to-analog converter) of some kind, plus your phone or another device to access files or a streaming service. All of this is necessary since Bluetooth compresses an audio signal by design, to allow for low-latency transmission and minimize battery draw. Simply put, wireless headphones haven’t been able to meet the demands of lossless audio, but Sennheiser has come the closest to fulfilling the dream with its HDB 630 ($500). Thanks to redesigned drivers, a new acoustic platform and a dongle, the company offers up to 24-bit/96kHz audio on the HDB 630 — depending on your configuration. You also get above average active noise cancellation (ANC), a highly customizable EQ, shockingly long battery life and advanced features to fine-tune the headphones to your liking. For some, the best possible sound is still only found on pricey setups and open-back headphones. For everyone else, Sennheiser has provided a taste of the audiophile life in a much more portable package. Design Sennheiser says the HDB 630 “inherited” the same chassis from its Momentum 4 headphones. That’s unfortunate because my biggest complaint with that older model's redesign is how cheap it looked compared to previous entries in the Momentum line. The HDB 630 suffers the same fate, although the splash of silver on the headband and yokes helps things a bit. Simply put, these don’t look like a set of $500 headphones, and since they’re $150 more than their predecessor was at launch, they really should have a more premium appearance. The outside of the right ear cup is still a touch panel where you can swipe, tap and even pinch to control the HDB 630. I don’t recall another set of headphones with a pinch gesture, and I’m still not convinced it’s warranted. The action is used to enable an Adaptive ANC adjustment that allows you to dial in the amount of noise blocking you need. After the pinch, sliding a single finger forwards and backwards fine tunes the mix of ANC and transparency mode. It’s a nice option to have on the headphones themselves, I just think a triple tap to activate it would be easier to master — and remember. The only other button on the HDB 630 is for power and Bluetooth pairing. Unless you’re frequently connecting these headphones to a new device, you might not be reaching for this control very often. That’s because the HDB 630 goes into standby mode when you take them off before powering down completely after 15 minutes of inactivity. You can extend that window to 30 or 60 minutes if you prefer. And if the headphones still have battery left, you can return to active mode by simply putting them back on your head. Sennheiser is betting you’ll use the HDB 630 for long listening sessions, so it outfitted these headphones with soft ear pads and a well-cushioned headband. The clamping force is adequate for a proper ANC seal, but never becomes a burden. And despite being around 20 grams heavier than the Momentum 4, this model still feels balanced and doesn’t weigh you down. Sound quality The HDB 630 features new drivers and a specially designed acoustic system. Billy Steele for Engadget While the overall design may be familiar, the sound platform for the HDB 630 is completely new. 42mm drivers offer what Sennheiser says is “neutral sound with lifelike mids, stunning detail and a wide soundstage.” In order to deliver sound quality that’s as close to open-back headphones as possible, the company overhauled the entire acoustic system, from the drivers to the baffle’s transparent mesh, in the name of balance and clarity. And since audiophile headphones typically require a dedicated external amplifier to achieve their full potential, Sennheiser included a BTD 700 USB-C dongle for high-resolution wireless audio transmission. When I first put the HDB 630 on, I thought the audio quality was good but not great. Listening over the standard definition SBC codec produced decent results, but it wasn’t anything to write home about. Once I connected to the BTD 700 dongle and unlocked 16-bit/48kHz tunes from Apple Music, though, these headphones really started to impress. As good as they are, the HDB 630 may not be for everyone. That “neutral” stock tuning places high emphasis on the midrange, so you’ll likely need to make some adjustments to get the bass performance you crave from rock, electronic, hip-hop and other genres driven by low-end tone. While I concede the neutral base is a great starting point, and the HDB 630 does indeed showcase “stunning detail,” I’d argue Sennheiser’s promise of “a wide soundstage” doesn’t always hold true. These headphones are at their best with more immersive content, like the TRON: Ares soundtrack from Nine Inch Nails. After a slight adjustment, the electronic score had the booming bass it needed, offering driving beats that nearly rattled my brain. All that was layered with rich synths and Trent Reznor’s iconic vocals. The texture and distortion in the instruments came through in greater detail too, something that’s not as apparent on other headphones and earbuds. Switch over to Thrice’s Horizons/West and the HDB 630 is a different story. Transitioning from synth-heavy electronic music to a genre like rock causes these headphones to lose some of the immersive character they are capable of delivering. You still get absurd clarity and detail, particularly in Teppei Teranishi’s guitar riffs, but the music sounds slightly flatter and a little less energetic. It’s not bad by any means, but some genres won’t envelope you as much as others do. You can also use the HDB 630 wired over USB-C for lossless-quality audio. Since a number of competitors also do this, I dedicated the bulk of my testing to see if Sennheiser’s wireless dongle is meaningfully different. Of course, I did my due diligence and tested the wired configuration a few times, and it should come as no surprise that the HDB 630 sounds just as good in that setup. Software, features and accessories There's only one button on the HDB 630. Billy Steele for Engadget As I mentioned, the HDB 630 comes with Sennheiser’s BTD 700 Bluetooth USB dongle. This enables higher quality streaming than you’ll natively get from most devices. With the BTD 700, you can expect aptX Adaptive and aptX Lossless listening up at rates to 24-bit/96kHz. The dongle also has a 30ms low-latency gaming mode, (supposedly) enhanced call performance and Auracast support for streaming to multiple headphones or speakers. The BTD 700 has a USB-C connector, but it comes with a USB-A adapter if you need it. This typically costs $60 if you buy it on its own, and since you need it to unlock the HDB 630’s full potential, it’s great to see it included in the box. The HDB 630’s settings and features are accessible in the Sennheiser Smart Control Plus app. And for this model, the company is offering a lot more customization than it does on the Accentum or Momentum headphones. First, the EQ editing options are more robust thanks to a parametric equalizer, which allows you to get a lot more detailed with your custom presets. For example, I was able to add the low-end tone I feel is missing from the stock tuning for those metal, rock and hip-hop tracks I mentioned before. And unlike a lot of headphone apps, adjusting the EQ actually improves the sound instead of just muddying things further. Another sound-related addition for the HDB 630 is Crossfeed. This allows you to blend the left and right channels so that it seems like you’re listening to speakers instead of headphones. Unfortunately, you only get two options here — Low and High — but the effect certainly enhances the sonic profile of the HDB 630 at both settings. Despite the BTD 700 dongle’s Mac and Windows compatibility, there’s no desktop version of the Smart Control Plus app. This means you’ll have to change all of your settings with the HDB 630 through your phone before you pair it with both the dongle and your computer. It would be nice if you could make EQ adjustments, create new presets and even change Crossfeed levels without having to reconnect to another device. This also means you can’t be connected to the BTD 700 and both your phone and your computer, since the dongle takes one of the two available multipoint Bluetooth slots. Active noise cancellation and call quality The HDB 630 has a very basic design with lots of plastic. Billy Steele for Engadget When it comes to ANC performance, I’m not entirely sure that the HDB 630 is better than the Momentum 4. But that’s okay. That previous model brought a significant improvement compared to Sennheiser’s older wireless headphones and the ANC is still quite good here. In fact, it was robust enough to block my family’s voices during their calls while I worked from home, and since most headphones struggle with this, that’s no mean feat. Sennheiser says the BTD 700 dongle will give you improved voice performance over the headphones alone. Specifically, the accessory should provide extended range, clearer voice pickup and, according to the company, “uninterrupted” calls. In my recorded samples, I think the headphones themselves sounded slightly better than when I captured my voice while connected to the BTD 700. However, I noticed a distinct lack of background noise in both clips, which is helpful in busier environments. I’ll also note the overall voice quality isn’t pristine, but it’s clear enough to use for work calls — even if you’re the main presenter. Battery life Sennheiser promises that you’ll get up to 60 hours of battery life on a charge with the HDB 630. That’s the same staggering figure the company claims on the Momentum 4. And yes, that’s with ANC enabled, but you’ll only achieve that if you’re listening to standard resolution tunes. Based on my testing with a mix of noise cancellation and transparency mode while I was listening to music and taking work calls, I have no reason to believe the company’s numbers don’t hold true. If you choose to listen entirely via the BTD 700’s higher quality output, you can expect up to 45 hours of use on a charge. That’s still quite a long time considering a lot of the competition runs out at around 30 hours — and that’s without high-res music. Due to all of the signal processing that helps with the acoustic performance on the HDB 630, they can only be used when they’re turned on. Unlike some wireless models, you can’t use these as wired headphones when the battery is spent. However, if you find yourself with a completely depleted battery, a 10-minute charge will give you up to seven hours of use. The company doesn’t specify streaming resolution for that number, but I assume it’s at standard definition. Still, you’ll get a few hours of higher-res music in that time, which should be enough to get you through a work session, evening commute or that new album you’re dying to play for the first time. The competition Incredible sound awaits, if you're okay to carry a dongle around with your headphones. Billy Steele for Engadget In the realm of flagship headphones, any company’s top-of-the-line model will set you back $500 these days. I look back fondly on the time when $300-$350 got you the best Sony had to offer. While the HDB 630 is expensive, it’s also in the same ballpark of what you’ll pay for the Bose QuietComfort Ultra Headphones ($450), the Sony WH-1000XM6 ($458 currently) and the AirPods Max ($549). Each of those have their advantages over the rest of the competition, with the 1000XM6 offering the most complete package overall. However, when it comes to pure sound quality, neither of those three are at the top of the heap. Up until now, that title belonged to the Noble Audio FoKus Apollo. At $650, those headphones are even more expensive than the HDB 630, but their stock tuning will appeal to more listeners and the soundstage is wider and more immersive. There’s also Bowers & Wilkins’ Px7 S3 for a slightly cheaper $479. It delivers the company’s warm, inviting sound and attention to finer details. After spending time with the HDB 630 though, these alternatives are just that — alternatives — as the new Sennheiser headphones are now my pick for best overall sound quality. Wrap-up I get it: in the current financial climate, $500 is a lot to pay for headphones (or anything else, for that matter). You can find a number of perfectly capable sets of ANC headphones for much less given how frequently things go on sale these days. However, what you won’t find is an option that gives you anything close to the performance of audiophile-grade, open-back headphones. That’s really what Sennheiser is doing here, and the HDB 630 slots nicely into the company’s HD 600 series of high-end cans. As good as the HDB 630 is sound-wise, I can also appreciate that these aren’t the best headphones for everyone. The company’s Momentum 4 is still a very capable set of headphones and it’s now available for about $250. If you crave the best sound quality that still offers the convenience of wireless headphones — and you’re okay with a few extra steps — the HDB 630 is a worthy investment. Just don’t leave home without that dongle.This article originally appeared on Engadget at https://www.engadget.com/audio/headphones/sennheiser-hdb-630-review-a-sonic-marvel-with-room-for-improvement-150000295.html?src=rss",
          "feed_position": 13,
          "image_url": "https://d29szjachogqwa.cloudfront.net/videos/user-uploaded/DSC_5509.jpg"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/computing/openais-recent-chip-deals-heap-more-pressure-on-tsmc-130000194.html",
          "published_at": "Fri, 24 Oct 2025 13:00:00 +0000",
          "title": "OpenAI's recent chip deals heap more pressure on TSMC",
          "standfirst": "In recent weeks, OpenAI has signed blockbuster deals with AMD and Broadcom to build vast numbers of AI chips. Much of the focus has been on the financial implications, since OpenAI will need hundreds of billions of dollars to make good on its promises. As important as it is to look at the quite implausible financials, we also need to look at the broader implications for the industry. Like, the chips themselves, what that spells for the AI industry as a whole, and the added pressure on TSMC, the only chip company that can actually build this stuff.The DealsOpenAI’s deal with AMD will see the chip giant build out 6 gigawatts’ (GW) worth of GPUs in the next few years. The first 1 GW deployment of AMD’s Instinct MI450 silicon will start in the back end of 2026, with more to come. AMD’s CFO Jean Hu believes that the partnership will deliver “tens of billions of dollars in revenue” in future, justifying the complicated way the deal is funded.Meanwhile, Broadcom’s deal with OpenAI will see the pair collaborate on building 10 gigawatts’ worth of AI accelerators and ethernet systems that it has designed. The latter will be crucial to speed up connections between each individual system in OpenAI’s planned data centers. Like the deal with AMD, the first deployments of these systems will begin in the back half of 2026 and is set to run through 2029.Phil Burr is head of product at Lumai, a British company looking to replace traditional GPUs with optical processors. He’s got 30 years experience in the chip world, including a stint as a senior director at ARM. Burr explained the nitty-gritty of OpenAI’s deals with both Broadcom and AMD, and what both mean for the wider world. Burr first poured water on OpenAI’s claim that it would be “designing” the gear produced by Broadcom. “Broadcom has a wide portfolio of IP blocks and pre-designed parts of a chip,” he said, “it will put those together according to the specification of the customer.” He went on to say that Broadcom will essentially put together a series of blocks it has already designed to suit the specification laid down by a customer, in this case OpenAI.Similarly, the AI accelerators Broadcom will build are geared toward more efficient running of models OpenAI has already trained and built — a process called inference in AI circles. “It can tailor the workload and reduce power, or increase performance,” said Burr, but these benefits would only work in OpenAI's favor, rather than for the wider AI industry.I asked Burr why every company in the AI space talks about gigawatts worth of chips rather than in more simple numbers. He explained that, often, it’s because both parties don’t yet know how many chips would be required to meet those lofty goals. But you could make a reasonable guess if you knew the power draw of a specific chip divided by the overall goal, then cut that number in half, then remove an extra 10 percent. “For every watt of power you burn in the chip, you need about a watt of power to cool it as well.” In terms of what OpenAI gets from these deals, Burr believes that the startup will save money on chips, since there’s “less margin” from making your own versus buying gear from NVIDIA. Plus, being able to produce custom silicon to tailor the work to their needs should see significant speed and performance gains on rival systems. Of course, the next biggest benefit is that OpenAI now has “diversity in supply,” rather than being reliant on one provider for all its needs. “Nobody wants a single supplier,” said Burr. The FactoryExcept, of course, OpenAI may be sourcing chips from a variety of its partners, but no matter what’s stamped on the silicon, it all comes from the same place. “I’d be very surprised if it wasn’t TSMC,” said Burr, “I’m pretty sure all of the AI chips out there use TSMC.” TSMC is short for Taiwan Semiconductor Manufacturing Company which, over the last decade, has blown past its major rivals to become the biggest (and in many cases only) source of bleeding-edge chips for the whole technology industry. Unlike historic rivals, which designed and manufactured their own hardware, TSMC is a pure play foundry, only building chips designed by others. Interior at one of TSMC's FabsTaiwan Semiconductor Manufacturing Co. Ltd.Gil Luria is Managing Director at head of technology research at investment firm DA Davidson. He said that TSMC isn’t just a bottleneck for the western technology industry, but in fact is the \"greatest single point of failure for the entire global economy.” Luria credits the company with an impressive expansion “considering it has had to ramp the production of GPUs tenfold over the last three years.” But said that, “in a catastrophic scenario where TSMC is not able to produce in Taiwan, the disruption would be significant.” And that won’t just affect the AI world, but “mobile handset sales as well as global car sales.” TSMC supplanted Intel for a number of well-documented reasons, but the most relevant here is its embrace of Extreme Ultraviolet Lithography (EUV). It’s a technology that Intel had initially backed, but struggled to fully adopt, allowing TSMC to pick it up and run straight to the top. EUV produces the headline-grabbing chips used by pretty much everyone in the consumer electronics world. Apple, Qualcomm, NVIDIA, AMD (including the SOCs inside the PS5 and Xbox) all use TSMC chips. Even Intel has been using TSMC foundries for some consumer CPUs as it races to bridge to gulf in manufacturing between the two companies.“TSMC is the current leader in advanced 3 nanometer (nm) process technologies,” said University of Pennsylvania Professor Benjamin C. Lee. The company’s only meaningful competitors are Intel and Samsung, neither of which pose a threat to its dominance at present. “Intel has been working for a very long time to build a foundry business,” he explained, “but has yet to perfect its interface.” Samsung is in a similar situation, but Professor Lee explained it “has been unable to attract enough customers to generate a profitable manufacturing business.” Professor Lee said that TSMC, by comparison, has become so successful because of how good its chips are, and how easy it is for clients to build chips with its tools. “TSMC fabricates chips with high yield, which is to say more of its chips emerge from the fabrication process at expected performance and reliability.” Consequently, it should be no surprise that TSMC is a money making machine. In the second quarter of 2025 alone it reported a net profit of $12.8 billion USD. And in the following three months, TSMC posted net profits of $14.76 billion. “TSMC’s secret sauce is its mastery of yield,” explained ARPU Intelligence, an analyst group that prefers to use the group name over individual attribution. “This expertise is the result of decades of accumulated process refinement [and] a deep institutional knowledge that cannot be replicated.” This deep institutional knowledge and ability to deliver high quality product creates a “powerful technical lock-in, since companies like Apple and NVIDIA design their chips specifically for TSMC’s unique manufacturing process … It’s not as simple as sending the [chip] design to another factory,” it added.The downside, at least for the wider technology industry, is that TSMC is now a bottleneck that the whole industry has come to rely upon. In the company’s most recent financials, it said more than three quarters of its business comes from North American customers. And in a call with investors, Chairman and CEO C.C. Wei talked about the efforts the company has made to narrow the gap between the enormous demand and its constrained supply. While he was reticent to be specific, he did say that the company’s capacity is “very tight,” and would likely remain that way for the foreseeable future. In fact, TSMC’s capacity is so tight that it’s already caused at least one major name a significant headache. Earlier this year, Reuters reported that NVIDIA canceled an order of its H20 AI chips after being informed the US would not permit them to be exported to China. Once the ban was lifted, however, NVIDIA was unable to find space in TSMC’s schedule, with the next available slot at least nine months later.“TSMC has no room for error,” said ARPU Intelligence, “any minor disruption can halt production with no spare capacity to absorb the shock.” It cited the Hualien earthquake which struck Taiwan on April 3, 2024, and how it negatively impacted the number of wafers in production.Naturally, TSMC is spending big to increase its production capacity for its customers, both in Taiwan and the US. Close to its home, construction on its A14 fab is expected to begin in the very near future, with the first chips due to be produced in 2028. That facility will harness TSMC’s A14 process node, producing 1.4 nm chips, which offer a speed boost over the 2nm silicon that's expected to arrive in consumer devices next year.Image of TSMC's Arizona CampusTaiwan Semiconductor Manufacturing Co. Ltd.Meanwhile, work continues apace on building out TSMC’s sprawling facility in Arizona, which broke ground in April 2021. As Reuters reported at the time, the first facility started operating in early 2025, producing 4 nm chips. Last week, NVIDIA and TSMC showed off the first Blackwell wafer produced at the Arizona plant ahead of domestic volume production.Plans for the operation have grown over time, expanding from three facilities up to six to be built over the next decade. And while the initial outline called for the US facilities to remain several process generations behind Taiwan, that is also changing. In his recent investors call, Chairman and CEO C.C. Wei pledged to invest more in the US facility to bring it only one generation behind the Taiwanese facility. No amount of investment from TSMC or catch-up from rivals like Samsung and Intel will solve the current bottleneck swiftly. It will take many years, if not decades, for the world to reduce its reliance on Taiwan for bleeding-edge manufacturing. TSMC's island remains the industry's weak point, and should something go wrong, the consequences could be dire indeed.This article originally appeared on Engadget at https://www.engadget.com/computing/openais-recent-chip-deals-heap-more-pressure-on-tsmc-130000194.html?src=rss",
          "content": "In recent weeks, OpenAI has signed blockbuster deals with AMD and Broadcom to build vast numbers of AI chips. Much of the focus has been on the financial implications, since OpenAI will need hundreds of billions of dollars to make good on its promises. As important as it is to look at the quite implausible financials, we also need to look at the broader implications for the industry. Like, the chips themselves, what that spells for the AI industry as a whole, and the added pressure on TSMC, the only chip company that can actually build this stuff.The DealsOpenAI’s deal with AMD will see the chip giant build out 6 gigawatts’ (GW) worth of GPUs in the next few years. The first 1 GW deployment of AMD’s Instinct MI450 silicon will start in the back end of 2026, with more to come. AMD’s CFO Jean Hu believes that the partnership will deliver “tens of billions of dollars in revenue” in future, justifying the complicated way the deal is funded.Meanwhile, Broadcom’s deal with OpenAI will see the pair collaborate on building 10 gigawatts’ worth of AI accelerators and ethernet systems that it has designed. The latter will be crucial to speed up connections between each individual system in OpenAI’s planned data centers. Like the deal with AMD, the first deployments of these systems will begin in the back half of 2026 and is set to run through 2029.Phil Burr is head of product at Lumai, a British company looking to replace traditional GPUs with optical processors. He’s got 30 years experience in the chip world, including a stint as a senior director at ARM. Burr explained the nitty-gritty of OpenAI’s deals with both Broadcom and AMD, and what both mean for the wider world. Burr first poured water on OpenAI’s claim that it would be “designing” the gear produced by Broadcom. “Broadcom has a wide portfolio of IP blocks and pre-designed parts of a chip,” he said, “it will put those together according to the specification of the customer.” He went on to say that Broadcom will essentially put together a series of blocks it has already designed to suit the specification laid down by a customer, in this case OpenAI.Similarly, the AI accelerators Broadcom will build are geared toward more efficient running of models OpenAI has already trained and built — a process called inference in AI circles. “It can tailor the workload and reduce power, or increase performance,” said Burr, but these benefits would only work in OpenAI's favor, rather than for the wider AI industry.I asked Burr why every company in the AI space talks about gigawatts worth of chips rather than in more simple numbers. He explained that, often, it’s because both parties don’t yet know how many chips would be required to meet those lofty goals. But you could make a reasonable guess if you knew the power draw of a specific chip divided by the overall goal, then cut that number in half, then remove an extra 10 percent. “For every watt of power you burn in the chip, you need about a watt of power to cool it as well.” In terms of what OpenAI gets from these deals, Burr believes that the startup will save money on chips, since there’s “less margin” from making your own versus buying gear from NVIDIA. Plus, being able to produce custom silicon to tailor the work to their needs should see significant speed and performance gains on rival systems. Of course, the next biggest benefit is that OpenAI now has “diversity in supply,” rather than being reliant on one provider for all its needs. “Nobody wants a single supplier,” said Burr. The FactoryExcept, of course, OpenAI may be sourcing chips from a variety of its partners, but no matter what’s stamped on the silicon, it all comes from the same place. “I’d be very surprised if it wasn’t TSMC,” said Burr, “I’m pretty sure all of the AI chips out there use TSMC.” TSMC is short for Taiwan Semiconductor Manufacturing Company which, over the last decade, has blown past its major rivals to become the biggest (and in many cases only) source of bleeding-edge chips for the whole technology industry. Unlike historic rivals, which designed and manufactured their own hardware, TSMC is a pure play foundry, only building chips designed by others. Interior at one of TSMC's FabsTaiwan Semiconductor Manufacturing Co. Ltd.Gil Luria is Managing Director at head of technology research at investment firm DA Davidson. He said that TSMC isn’t just a bottleneck for the western technology industry, but in fact is the \"greatest single point of failure for the entire global economy.” Luria credits the company with an impressive expansion “considering it has had to ramp the production of GPUs tenfold over the last three years.” But said that, “in a catastrophic scenario where TSMC is not able to produce in Taiwan, the disruption would be significant.” And that won’t just affect the AI world, but “mobile handset sales as well as global car sales.” TSMC supplanted Intel for a number of well-documented reasons, but the most relevant here is its embrace of Extreme Ultraviolet Lithography (EUV). It’s a technology that Intel had initially backed, but struggled to fully adopt, allowing TSMC to pick it up and run straight to the top. EUV produces the headline-grabbing chips used by pretty much everyone in the consumer electronics world. Apple, Qualcomm, NVIDIA, AMD (including the SOCs inside the PS5 and Xbox) all use TSMC chips. Even Intel has been using TSMC foundries for some consumer CPUs as it races to bridge to gulf in manufacturing between the two companies.“TSMC is the current leader in advanced 3 nanometer (nm) process technologies,” said University of Pennsylvania Professor Benjamin C. Lee. The company’s only meaningful competitors are Intel and Samsung, neither of which pose a threat to its dominance at present. “Intel has been working for a very long time to build a foundry business,” he explained, “but has yet to perfect its interface.” Samsung is in a similar situation, but Professor Lee explained it “has been unable to attract enough customers to generate a profitable manufacturing business.” Professor Lee said that TSMC, by comparison, has become so successful because of how good its chips are, and how easy it is for clients to build chips with its tools. “TSMC fabricates chips with high yield, which is to say more of its chips emerge from the fabrication process at expected performance and reliability.” Consequently, it should be no surprise that TSMC is a money making machine. In the second quarter of 2025 alone it reported a net profit of $12.8 billion USD. And in the following three months, TSMC posted net profits of $14.76 billion. “TSMC’s secret sauce is its mastery of yield,” explained ARPU Intelligence, an analyst group that prefers to use the group name over individual attribution. “This expertise is the result of decades of accumulated process refinement [and] a deep institutional knowledge that cannot be replicated.” This deep institutional knowledge and ability to deliver high quality product creates a “powerful technical lock-in, since companies like Apple and NVIDIA design their chips specifically for TSMC’s unique manufacturing process … It’s not as simple as sending the [chip] design to another factory,” it added.The downside, at least for the wider technology industry, is that TSMC is now a bottleneck that the whole industry has come to rely upon. In the company’s most recent financials, it said more than three quarters of its business comes from North American customers. And in a call with investors, Chairman and CEO C.C. Wei talked about the efforts the company has made to narrow the gap between the enormous demand and its constrained supply. While he was reticent to be specific, he did say that the company’s capacity is “very tight,” and would likely remain that way for the foreseeable future. In fact, TSMC’s capacity is so tight that it’s already caused at least one major name a significant headache. Earlier this year, Reuters reported that NVIDIA canceled an order of its H20 AI chips after being informed the US would not permit them to be exported to China. Once the ban was lifted, however, NVIDIA was unable to find space in TSMC’s schedule, with the next available slot at least nine months later.“TSMC has no room for error,” said ARPU Intelligence, “any minor disruption can halt production with no spare capacity to absorb the shock.” It cited the Hualien earthquake which struck Taiwan on April 3, 2024, and how it negatively impacted the number of wafers in production.Naturally, TSMC is spending big to increase its production capacity for its customers, both in Taiwan and the US. Close to its home, construction on its A14 fab is expected to begin in the very near future, with the first chips due to be produced in 2028. That facility will harness TSMC’s A14 process node, producing 1.4 nm chips, which offer a speed boost over the 2nm silicon that's expected to arrive in consumer devices next year.Image of TSMC's Arizona CampusTaiwan Semiconductor Manufacturing Co. Ltd.Meanwhile, work continues apace on building out TSMC’s sprawling facility in Arizona, which broke ground in April 2021. As Reuters reported at the time, the first facility started operating in early 2025, producing 4 nm chips. Last week, NVIDIA and TSMC showed off the first Blackwell wafer produced at the Arizona plant ahead of domestic volume production.Plans for the operation have grown over time, expanding from three facilities up to six to be built over the next decade. And while the initial outline called for the US facilities to remain several process generations behind Taiwan, that is also changing. In his recent investors call, Chairman and CEO C.C. Wei pledged to invest more in the US facility to bring it only one generation behind the Taiwanese facility. No amount of investment from TSMC or catch-up from rivals like Samsung and Intel will solve the current bottleneck swiftly. It will take many years, if not decades, for the world to reduce its reliance on Taiwan for bleeding-edge manufacturing. TSMC's island remains the industry's weak point, and should something go wrong, the consequences could be dire indeed.This article originally appeared on Engadget at https://www.engadget.com/computing/openais-recent-chip-deals-heap-more-pressure-on-tsmc-130000194.html?src=rss",
          "feed_position": 16,
          "image_url": "https://d29szjachogqwa.cloudfront.net/videos/user-uploaded/Fab14inr015_902_01.jpg"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/general/the-morning-after-engadget-newsletter-111555814.html",
          "published_at": "Fri, 24 Oct 2025 11:15:55 +0000",
          "title": "The Morning After: Samsung’s Galaxy XR enters the chat",
          "standfirst": "This week, Samsung showed off Galaxy XR, its Vision Pro-troubling headset, and you can bet we’ve done a deep dive. Sam Rutherford got one of these strapped to his head and has plenty of feelings about the new hardware. The headset is lighter, more comfortable and easier to live with than Apple’s Vision Pro, even if it lacks many of its headline features. The software ecosystem is already pretty broad, thanks to Google making a real effort with Android XR, but dedicated apps are still a bit rare. Samsung’s entry into the market might provide some much-needed impetus for this type of augmented reality headset. That it’s half the price of Apple’s Vision Pro may also loosen some wallets eager to get into this world. But it’s hard not to see this as Samsung running down the same cul-de-sac Apple is now lurking at the end of. It has allowed other companies, like Meta, to waltz in and grab an early lead in the much more useful smart glasses market. — Dan Cooper Get Engadget's newsletter delivered direct to your inbox. Subscribe right here! The news you might have missed The first e-bike from Rivian spinoff Also has a virtual drivetrainI’m more interested in its Bakfiets-esque quad bike for driving kids around. Amazon’s smart glasses with AI will help its drivers deliver packages fasterIt’s just like RoboCop, only with more peeing in bottles. ChatGPT in WhatsApp will stop working in JanuaryMeta is kicking its AI rival off its platform. Apple MacBook Pro M5 14-inch review: A huge graphics upgrade for creators and gamers The GPU is the star here. Devindra Hardawar for Engadget Apple’s online-only announcement of the new vanilla M5 MacBooks might have been a sign the new models were no big deal. But Devindra Hardawar found these were, in fact, quite a big deal, and the M5’s faster GPU has the chops to go toe-to-toe with a gaming PC. Continue Reading. Toyota’s new all-hybrid RAV4 has software you might actually want to use It wants to offer a better alternative to your smartphone. Tim Stevens for Engadget Toyota isn’t happy folks just default to CarPlay or Android Auto for their in-car infotainment. That’s why it’s chosen to radically redesign its OS for the 2026 RAV4 to include voice and touch control. Tim Stevens has ridden the new whip and has plenty of opinions on whether it’s worth your time or, you know… you’ll just default to CarPlay or Android Auto. Continue Reading. iPad Pro M5 review: Speed boost We reviewed the iPad Pro M5 and had some feelings. Nathan Ingraham for Engadget As much as I may want an iPad Pro, it wouldn’t play a role in my life that would get anywhere near to justifying its extortionate price. Consequently, I shall just live vicariously through Nathan Ingraham, who reviewed the M5 edition and found it to be a work of art. But, you know, it has a price so eye-watering that nobody who’s on the fence about owning one should bother. Then, Nate pivoted to writing about how the iPad Pro has, at least, carved out its own identity. Continue Reading. New report leaks Amazon’s proposed mass-automation plans It plans to replace more than half a million employees. Amazon may be planning to use automation to eliminate more than half a million jobs in the next few years. The New York Times claims to have seen internal documents outlining the plans and the PR operation that’ll get underway ahead of time to quell public anger. Continue Reading. Binance founder Changpeng Zhao lands a Trump pardon Nothing to see here, move along. Maybe there’s nothing interesting about the fact Changpeng Zhao was just pardoned by President Trump despite pleading guilty to violating the Bank Secrecy Act. I mean, yes, Zhao has ties to World Liberty Financial, a cryptocurrency venture linked to the Trump family. But that’s not uncommon, is it? Surely everyone would use the privilege of high office to exonerate people with whom they potentially have fruitful relationships. Right? Continue Reading. This article originally appeared on Engadget at https://www.engadget.com/general/the-morning-after-engadget-newsletter-111555814.html?src=rss",
          "content": "This week, Samsung showed off Galaxy XR, its Vision Pro-troubling headset, and you can bet we’ve done a deep dive. Sam Rutherford got one of these strapped to his head and has plenty of feelings about the new hardware. The headset is lighter, more comfortable and easier to live with than Apple’s Vision Pro, even if it lacks many of its headline features. The software ecosystem is already pretty broad, thanks to Google making a real effort with Android XR, but dedicated apps are still a bit rare. Samsung’s entry into the market might provide some much-needed impetus for this type of augmented reality headset. That it’s half the price of Apple’s Vision Pro may also loosen some wallets eager to get into this world. But it’s hard not to see this as Samsung running down the same cul-de-sac Apple is now lurking at the end of. It has allowed other companies, like Meta, to waltz in and grab an early lead in the much more useful smart glasses market. — Dan Cooper Get Engadget's newsletter delivered direct to your inbox. Subscribe right here! The news you might have missed The first e-bike from Rivian spinoff Also has a virtual drivetrainI’m more interested in its Bakfiets-esque quad bike for driving kids around. Amazon’s smart glasses with AI will help its drivers deliver packages fasterIt’s just like RoboCop, only with more peeing in bottles. ChatGPT in WhatsApp will stop working in JanuaryMeta is kicking its AI rival off its platform. Apple MacBook Pro M5 14-inch review: A huge graphics upgrade for creators and gamers The GPU is the star here. Devindra Hardawar for Engadget Apple’s online-only announcement of the new vanilla M5 MacBooks might have been a sign the new models were no big deal. But Devindra Hardawar found these were, in fact, quite a big deal, and the M5’s faster GPU has the chops to go toe-to-toe with a gaming PC. Continue Reading. Toyota’s new all-hybrid RAV4 has software you might actually want to use It wants to offer a better alternative to your smartphone. Tim Stevens for Engadget Toyota isn’t happy folks just default to CarPlay or Android Auto for their in-car infotainment. That’s why it’s chosen to radically redesign its OS for the 2026 RAV4 to include voice and touch control. Tim Stevens has ridden the new whip and has plenty of opinions on whether it’s worth your time or, you know… you’ll just default to CarPlay or Android Auto. Continue Reading. iPad Pro M5 review: Speed boost We reviewed the iPad Pro M5 and had some feelings. Nathan Ingraham for Engadget As much as I may want an iPad Pro, it wouldn’t play a role in my life that would get anywhere near to justifying its extortionate price. Consequently, I shall just live vicariously through Nathan Ingraham, who reviewed the M5 edition and found it to be a work of art. But, you know, it has a price so eye-watering that nobody who’s on the fence about owning one should bother. Then, Nate pivoted to writing about how the iPad Pro has, at least, carved out its own identity. Continue Reading. New report leaks Amazon’s proposed mass-automation plans It plans to replace more than half a million employees. Amazon may be planning to use automation to eliminate more than half a million jobs in the next few years. The New York Times claims to have seen internal documents outlining the plans and the PR operation that’ll get underway ahead of time to quell public anger. Continue Reading. Binance founder Changpeng Zhao lands a Trump pardon Nothing to see here, move along. Maybe there’s nothing interesting about the fact Changpeng Zhao was just pardoned by President Trump despite pleading guilty to violating the Bank Secrecy Act. I mean, yes, Zhao has ties to World Liberty Financial, a cryptocurrency venture linked to the Trump family. But that’s not uncommon, is it? Surely everyone would use the privilege of high office to exonerate people with whom they potentially have fruitful relationships. Right? Continue Reading. This article originally appeared on Engadget at https://www.engadget.com/general/the-morning-after-engadget-newsletter-111555814.html?src=rss",
          "feed_position": 23,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2025-10/1c5c79b0-b0bf-11f0-873f-6093239f799f"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/thinking-machines-challenges-openais-ai-scaling-strategy-first",
          "published_at": "Fri, 24 Oct 2025 09:30:00 GMT",
          "title": "Thinking Machines challenges OpenAI's AI scaling strategy: 'First superintelligence will be a superhuman learner'",
          "standfirst": "While the world&#x27;s leading artificial intelligence companies race to build ever-larger models, betting billions that scale alone will unlock artificial general intelligence, a researcher at one of the industry&#x27;s most secretive and valuable startups delivered a pointed challenge to that orthodoxy this week: The path forward isn&#x27;t about training bigger — it&#x27;s about learning better.\"I believe that the first superintelligence will be a superhuman learner,\" Rafael Rafailov, a reinforcement learning researcher at Thinking Machines Lab, told an audience at TED AI San Francisco on Tuesday. \"It will be able to very efficiently figure out and adapt, propose its own theories, propose experiments, use the environment to verify that, get information, and iterate that process.\"This breaks sharply with the approach pursued by OpenAI, Anthropic, Google DeepMind, and other leading laboratories, which have bet billions on scaling up model size, data, and compute to achieve increasingly sophisticated reasoning capabilities. Rafailov argues these companies have the strategy backwards: what&#x27;s missing from today&#x27;s most advanced AI systems isn&#x27;t more scale — it&#x27;s the ability to actually learn from experience.\"Learning is something an intelligent being does,\" Rafailov said, citing a quote he described as recently compelling. \"Training is something that&#x27;s being done to it.\"The distinction cuts to the core of how AI systems improve — and whether the industry&#x27;s current trajectory can deliver on its most ambitious promises. Rafailov&#x27;s comments offer a rare window into the thinking at Thinking Machines Lab, the startup co-founded in February by former OpenAI chief technology officer Mira Murati that raised a record-breaking $2 billion in seed funding at a $12 billion valuation.Why today&#x27;s AI coding assistants forget everything they learned yesterdayTo illustrate the problem with current AI systems, Rafailov offered a scenario familiar to anyone who has worked with today&#x27;s most advanced coding assistants.\"If you use a coding agent, ask it to do something really difficult — to implement a feature, go read your code, try to understand your code, reason about your code, implement something, iterate — it might be successful,\" he explained. \"And then come back the next day and ask it to implement the next feature, and it will do the same thing.\"The issue, he argued, is that these systems don&#x27;t internalize what they learn. \"In a sense, for the models we have today, every day is their first day of the job,\" Rafailov said. \"But an intelligent being should be able to internalize information. It should be able to adapt. It should be able to modify its behavior so every day it becomes better, every day it knows more, every day it works faster — the way a human you hire gets better at the job.\"The duct tape problem: How current training methods teach AI to take shortcuts instead of solving problemsRafailov pointed to a specific behavior in coding agents that reveals the deeper problem: their tendency to wrap uncertain code in try/except blocks — a programming construct that catches errors and allows a program to continue running.\"If you use coding agents, you might have observed a very annoying tendency of them to use try/except pass,\" he said. \"And in general, that is basically just like duct tape to save the entire program from a single error.\"Why do agents do this? \"They do this because they understand that part of the code might not be right,\" Rafailov explained. \"They understand there might be something wrong, that it might be risky. But under the limited constraint—they have a limited amount of time solving the problem, limited amount of interaction—they must only focus on their objective, which is implement this feature and solve this bug.\"The result: \"They&#x27;re kicking the can down the road.\"This behavior stems from training systems that optimize for immediate task completion. \"The only thing that matters to our current generation is solving the task,\" he said. \"And anything that&#x27;s general, anything that&#x27;s not related to just that one objective, is a waste of computation.\"Why throwing more compute at AI won&#x27;t create superintelligence, according to Thinking Machines researcherRafailov&#x27;s most direct challenge to the industry came in his assertion that continued scaling won&#x27;t be sufficient to reach AGI.\"I don&#x27;t believe we&#x27;re hitting any sort of saturation points,\" he clarified. \"I think we&#x27;re just at the beginning of the next paradigm—the scale of reinforcement learning, in which we move from teaching our models how to think, how to explore thinking space, into endowing them with the capability of general agents.\"In other words, current approaches will produce increasingly capable systems that can interact with the world, browse the web, write code. \"I believe a year or two from now, we&#x27;ll look at our coding agents today, research agents or browsing agents, the way we look at summarization models or translation models from several years ago,\" he said.But general agency, he argued, is not the same as general intelligence. \"The much more interesting question is: Is that going to be AGI? And are we done — do we just need one more round of scaling, one more round of environments, one more round of RL, one more round of compute, and we&#x27;re kind of done?\"His answer was unequivocal: \"I don&#x27;t believe this is the case. I believe that under our current paradigms, under any scale, we are not enough to deal with artificial general intelligence and artificial superintelligence. And I believe that under our current paradigms, our current models will lack one core capability, and that is learning.\"Teaching AI like students, not calculators: The textbook approach to machine learningTo explain the alternative approach, Rafailov turned to an analogy from mathematics education.\"Think about how we train our current generation of reasoning models,\" he said. \"We take a particular math problem, make it very hard, and try to solve it, rewarding the model for solving it. And that&#x27;s it. Once that experience is done, the model submits a solution. Anything it discovers—any abstractions it learned, any theorems—we discard, and then we ask it to solve a new problem, and it has to come up with the same abstractions all over again.\"That approach misunderstands how knowledge accumulates. \"This is not how science or mathematics works,\" he said. \"We build abstractions not necessarily because they solve our current problems, but because they&#x27;re important. For example, we developed the field of topology to extend Euclidean geometry — not to solve a particular problem that Euclidean geometry couldn&#x27;t handle, but because mathematicians and physicists understood these concepts were fundamentally important.\"The solution: \"Instead of giving our models a single problem, we might give them a textbook. Imagine a very advanced graduate-level textbook, and we ask our models to work through the first chapter, then the first exercise, the second exercise, the third, the fourth, then move to the second chapter, and so on—the way a real student might teach themselves a topic.\"The objective would fundamentally change: \"Instead of rewarding their success — how many problems they solved — we need to reward their progress, their ability to learn, and their ability to improve.\"This approach, known as \"meta-learning\" or \"learning to learn,\" has precedents in earlier AI systems. \"Just like the ideas of scaling test-time compute and search and test-time exploration played out in the domain of games first\" — in systems like DeepMind&#x27;s AlphaGo — \"the same is true for meta learning. We know that these ideas do work at a small scale, but we need to adapt them to the scale and the capability of foundation models.\"The missing ingredients for AI that truly learns aren&#x27;t new architectures—they&#x27;re better data and smarter objectivesWhen Rafailov addressed why current models lack this learning capability, he offered a surprisingly straightforward answer.\"Unfortunately, I think the answer is quite prosaic,\" he said. \"I think we just don&#x27;t have the right data, and we don&#x27;t have the right objectives. I fundamentally believe a lot of the core architectural engineering design is in place.\"Rather than arguing for entirely new model architectures, Rafailov suggested the path forward lies in redesigning the data distributions and reward structures used to train models.\"Learning, in of itself, is an algorithm,\" he explained. \"It has inputs — the current state of the model. It has data and compute. You process it through some sort of structure, choose your favorite optimization algorithm, and you produce, hopefully, a stronger model.\"The question: \"If reasoning models are able to learn general reasoning algorithms, general search algorithms, and agent models are able to learn general agency, can the next generation of AI learn a learning algorithm itself?\"His answer: \"I strongly believe that the answer to this question is yes.\"The technical approach would involve creating training environments where \"learning, adaptation, exploration, and self-improvement, as well as generalization, are necessary for success.\"\"I believe that under enough computational resources and with broad enough coverage, general purpose learning algorithms can emerge from large scale training,\" Rafailov said. \"The way we train our models to reason in general over just math and code, and potentially act in general domains, we might be able to teach them how to learn efficiently across many different applications.\"Forget god-like reasoners: The first superintelligence will be a master studentThis vision leads to a fundamentally different conception of what artificial superintelligence might look like.\"I believe that if this is possible, that&#x27;s the final missing piece to achieve truly efficient general intelligence,\" Rafailov said. \"Now imagine such an intelligence with the core objective of exploring, learning, acquiring information, self-improving, equipped with general agency capability—the ability to understand and explore the external world, the ability to use computers, ability to do research, ability to manage and control robots.\"Such a system would constitute artificial superintelligence. But not the kind often imagined in science fiction.\"I believe that intelligence is not going to be a single god model that&#x27;s a god-level reasoner or a god-level mathematical problem solver,\" Rafailov said. \"I believe that the first superintelligence will be a superhuman learner, and it will be able to very efficiently figure out and adapt, propose its own theories, propose experiments, use the environment to verify that, get information, and iterate that process.\"This vision stands in contrast to OpenAI&#x27;s emphasis on building increasingly powerful reasoning systems, or Anthropic&#x27;s focus on \"constitutional AI.\" Instead, Thinking Machines Lab appears to be betting that the path to superintelligence runs through systems that can continuously improve themselves through interaction with their environment.The $12 billion bet on learning over scaling faces formidable challengesRafailov&#x27;s appearance comes at a complex moment for Thinking Machines Lab. The company has assembled an impressive team of approximately 30 researchers from OpenAI, Google, Meta, and other leading labs. But it suffered a setback in early October when Andrew Tulloch, a co-founder and machine learning expert, departed to return to Meta after the company launched what The Wall Street Journal called a \"full-scale raid\" on the startup, approaching more than a dozen employees with compensation packages ranging from $200 million to $1.5 billion over multiple years.Despite these pressures, Rafailov&#x27;s comments suggest the company remains committed to its differentiated technical approach. The company launched its first product, Tinker, an API for fine-tuning open-source language models, in October. But Rafailov&#x27;s talk suggests Tinker is just the foundation for a much more ambitious research agenda focused on meta-learning and self-improving systems.\"This is not easy. This is going to be very difficult,\" Rafailov acknowledged. \"We&#x27;ll need a lot of breakthroughs in memory and engineering and data and optimization, but I think it&#x27;s fundamentally possible.\"He concluded with a play on words: \"The world is not enough, but we need the right experiences, and we need the right type of rewards for learning.\"The question for Thinking Machines Lab — and the broader AI industry — is whether this vision can be realized, and on what timeline. Rafailov notably did not offer specific predictions about when such systems might emerge.In an industry where executives routinely make bold predictions about AGI arriving within years or even months, that restraint is notable. It suggests either unusual scientific humility — or an acknowledgment that Thinking Machines Lab is pursuing a much longer, harder path than its competitors.For now, the most revealing detail may be what Rafailov didn&#x27;t say during his TED AI presentation. No timeline for when superhuman learners might emerge. No prediction about when the technical breakthroughs would arrive. Just a conviction that the capability was \"fundamentally possible\" — and that without it, all the scaling in the world won&#x27;t be enough.",
          "content": "While the world&#x27;s leading artificial intelligence companies race to build ever-larger models, betting billions that scale alone will unlock artificial general intelligence, a researcher at one of the industry&#x27;s most secretive and valuable startups delivered a pointed challenge to that orthodoxy this week: The path forward isn&#x27;t about training bigger — it&#x27;s about learning better.\"I believe that the first superintelligence will be a superhuman learner,\" Rafael Rafailov, a reinforcement learning researcher at Thinking Machines Lab, told an audience at TED AI San Francisco on Tuesday. \"It will be able to very efficiently figure out and adapt, propose its own theories, propose experiments, use the environment to verify that, get information, and iterate that process.\"This breaks sharply with the approach pursued by OpenAI, Anthropic, Google DeepMind, and other leading laboratories, which have bet billions on scaling up model size, data, and compute to achieve increasingly sophisticated reasoning capabilities. Rafailov argues these companies have the strategy backwards: what&#x27;s missing from today&#x27;s most advanced AI systems isn&#x27;t more scale — it&#x27;s the ability to actually learn from experience.\"Learning is something an intelligent being does,\" Rafailov said, citing a quote he described as recently compelling. \"Training is something that&#x27;s being done to it.\"The distinction cuts to the core of how AI systems improve — and whether the industry&#x27;s current trajectory can deliver on its most ambitious promises. Rafailov&#x27;s comments offer a rare window into the thinking at Thinking Machines Lab, the startup co-founded in February by former OpenAI chief technology officer Mira Murati that raised a record-breaking $2 billion in seed funding at a $12 billion valuation.Why today&#x27;s AI coding assistants forget everything they learned yesterdayTo illustrate the problem with current AI systems, Rafailov offered a scenario familiar to anyone who has worked with today&#x27;s most advanced coding assistants.\"If you use a coding agent, ask it to do something really difficult — to implement a feature, go read your code, try to understand your code, reason about your code, implement something, iterate — it might be successful,\" he explained. \"And then come back the next day and ask it to implement the next feature, and it will do the same thing.\"The issue, he argued, is that these systems don&#x27;t internalize what they learn. \"In a sense, for the models we have today, every day is their first day of the job,\" Rafailov said. \"But an intelligent being should be able to internalize information. It should be able to adapt. It should be able to modify its behavior so every day it becomes better, every day it knows more, every day it works faster — the way a human you hire gets better at the job.\"The duct tape problem: How current training methods teach AI to take shortcuts instead of solving problemsRafailov pointed to a specific behavior in coding agents that reveals the deeper problem: their tendency to wrap uncertain code in try/except blocks — a programming construct that catches errors and allows a program to continue running.\"If you use coding agents, you might have observed a very annoying tendency of them to use try/except pass,\" he said. \"And in general, that is basically just like duct tape to save the entire program from a single error.\"Why do agents do this? \"They do this because they understand that part of the code might not be right,\" Rafailov explained. \"They understand there might be something wrong, that it might be risky. But under the limited constraint—they have a limited amount of time solving the problem, limited amount of interaction—they must only focus on their objective, which is implement this feature and solve this bug.\"The result: \"They&#x27;re kicking the can down the road.\"This behavior stems from training systems that optimize for immediate task completion. \"The only thing that matters to our current generation is solving the task,\" he said. \"And anything that&#x27;s general, anything that&#x27;s not related to just that one objective, is a waste of computation.\"Why throwing more compute at AI won&#x27;t create superintelligence, according to Thinking Machines researcherRafailov&#x27;s most direct challenge to the industry came in his assertion that continued scaling won&#x27;t be sufficient to reach AGI.\"I don&#x27;t believe we&#x27;re hitting any sort of saturation points,\" he clarified. \"I think we&#x27;re just at the beginning of the next paradigm—the scale of reinforcement learning, in which we move from teaching our models how to think, how to explore thinking space, into endowing them with the capability of general agents.\"In other words, current approaches will produce increasingly capable systems that can interact with the world, browse the web, write code. \"I believe a year or two from now, we&#x27;ll look at our coding agents today, research agents or browsing agents, the way we look at summarization models or translation models from several years ago,\" he said.But general agency, he argued, is not the same as general intelligence. \"The much more interesting question is: Is that going to be AGI? And are we done — do we just need one more round of scaling, one more round of environments, one more round of RL, one more round of compute, and we&#x27;re kind of done?\"His answer was unequivocal: \"I don&#x27;t believe this is the case. I believe that under our current paradigms, under any scale, we are not enough to deal with artificial general intelligence and artificial superintelligence. And I believe that under our current paradigms, our current models will lack one core capability, and that is learning.\"Teaching AI like students, not calculators: The textbook approach to machine learningTo explain the alternative approach, Rafailov turned to an analogy from mathematics education.\"Think about how we train our current generation of reasoning models,\" he said. \"We take a particular math problem, make it very hard, and try to solve it, rewarding the model for solving it. And that&#x27;s it. Once that experience is done, the model submits a solution. Anything it discovers—any abstractions it learned, any theorems—we discard, and then we ask it to solve a new problem, and it has to come up with the same abstractions all over again.\"That approach misunderstands how knowledge accumulates. \"This is not how science or mathematics works,\" he said. \"We build abstractions not necessarily because they solve our current problems, but because they&#x27;re important. For example, we developed the field of topology to extend Euclidean geometry — not to solve a particular problem that Euclidean geometry couldn&#x27;t handle, but because mathematicians and physicists understood these concepts were fundamentally important.\"The solution: \"Instead of giving our models a single problem, we might give them a textbook. Imagine a very advanced graduate-level textbook, and we ask our models to work through the first chapter, then the first exercise, the second exercise, the third, the fourth, then move to the second chapter, and so on—the way a real student might teach themselves a topic.\"The objective would fundamentally change: \"Instead of rewarding their success — how many problems they solved — we need to reward their progress, their ability to learn, and their ability to improve.\"This approach, known as \"meta-learning\" or \"learning to learn,\" has precedents in earlier AI systems. \"Just like the ideas of scaling test-time compute and search and test-time exploration played out in the domain of games first\" — in systems like DeepMind&#x27;s AlphaGo — \"the same is true for meta learning. We know that these ideas do work at a small scale, but we need to adapt them to the scale and the capability of foundation models.\"The missing ingredients for AI that truly learns aren&#x27;t new architectures—they&#x27;re better data and smarter objectivesWhen Rafailov addressed why current models lack this learning capability, he offered a surprisingly straightforward answer.\"Unfortunately, I think the answer is quite prosaic,\" he said. \"I think we just don&#x27;t have the right data, and we don&#x27;t have the right objectives. I fundamentally believe a lot of the core architectural engineering design is in place.\"Rather than arguing for entirely new model architectures, Rafailov suggested the path forward lies in redesigning the data distributions and reward structures used to train models.\"Learning, in of itself, is an algorithm,\" he explained. \"It has inputs — the current state of the model. It has data and compute. You process it through some sort of structure, choose your favorite optimization algorithm, and you produce, hopefully, a stronger model.\"The question: \"If reasoning models are able to learn general reasoning algorithms, general search algorithms, and agent models are able to learn general agency, can the next generation of AI learn a learning algorithm itself?\"His answer: \"I strongly believe that the answer to this question is yes.\"The technical approach would involve creating training environments where \"learning, adaptation, exploration, and self-improvement, as well as generalization, are necessary for success.\"\"I believe that under enough computational resources and with broad enough coverage, general purpose learning algorithms can emerge from large scale training,\" Rafailov said. \"The way we train our models to reason in general over just math and code, and potentially act in general domains, we might be able to teach them how to learn efficiently across many different applications.\"Forget god-like reasoners: The first superintelligence will be a master studentThis vision leads to a fundamentally different conception of what artificial superintelligence might look like.\"I believe that if this is possible, that&#x27;s the final missing piece to achieve truly efficient general intelligence,\" Rafailov said. \"Now imagine such an intelligence with the core objective of exploring, learning, acquiring information, self-improving, equipped with general agency capability—the ability to understand and explore the external world, the ability to use computers, ability to do research, ability to manage and control robots.\"Such a system would constitute artificial superintelligence. But not the kind often imagined in science fiction.\"I believe that intelligence is not going to be a single god model that&#x27;s a god-level reasoner or a god-level mathematical problem solver,\" Rafailov said. \"I believe that the first superintelligence will be a superhuman learner, and it will be able to very efficiently figure out and adapt, propose its own theories, propose experiments, use the environment to verify that, get information, and iterate that process.\"This vision stands in contrast to OpenAI&#x27;s emphasis on building increasingly powerful reasoning systems, or Anthropic&#x27;s focus on \"constitutional AI.\" Instead, Thinking Machines Lab appears to be betting that the path to superintelligence runs through systems that can continuously improve themselves through interaction with their environment.The $12 billion bet on learning over scaling faces formidable challengesRafailov&#x27;s appearance comes at a complex moment for Thinking Machines Lab. The company has assembled an impressive team of approximately 30 researchers from OpenAI, Google, Meta, and other leading labs. But it suffered a setback in early October when Andrew Tulloch, a co-founder and machine learning expert, departed to return to Meta after the company launched what The Wall Street Journal called a \"full-scale raid\" on the startup, approaching more than a dozen employees with compensation packages ranging from $200 million to $1.5 billion over multiple years.Despite these pressures, Rafailov&#x27;s comments suggest the company remains committed to its differentiated technical approach. The company launched its first product, Tinker, an API for fine-tuning open-source language models, in October. But Rafailov&#x27;s talk suggests Tinker is just the foundation for a much more ambitious research agenda focused on meta-learning and self-improving systems.\"This is not easy. This is going to be very difficult,\" Rafailov acknowledged. \"We&#x27;ll need a lot of breakthroughs in memory and engineering and data and optimization, but I think it&#x27;s fundamentally possible.\"He concluded with a play on words: \"The world is not enough, but we need the right experiences, and we need the right type of rewards for learning.\"The question for Thinking Machines Lab — and the broader AI industry — is whether this vision can be realized, and on what timeline. Rafailov notably did not offer specific predictions about when such systems might emerge.In an industry where executives routinely make bold predictions about AGI arriving within years or even months, that restraint is notable. It suggests either unusual scientific humility — or an acknowledgment that Thinking Machines Lab is pursuing a much longer, harder path than its competitors.For now, the most revealing detail may be what Rafailov didn&#x27;t say during his TED AI presentation. No timeline for when superhuman learners might emerge. No prediction about when the technical breakthroughs would arrive. Just a conviction that the capability was \"fundamentally possible\" — and that without it, all the scaling in the world won&#x27;t be enough.",
          "feed_position": 0,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/6gnj4yPgIqEF3Y4cLtYWL9/3a8c4c8b409b763704f9f4dd0ad67fd3/nuneybits_A_retro_glowing_computer_terminal_on_gradient_backgro_b5f91633-1cc9-42d7-9d6f-e497887b2ff3.webp?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/audio/headphones/best-headphones-for-running-120044637.html",
          "published_at": "Fri, 24 Oct 2025 09:00:35 +0000",
          "title": "The best headphones for running in 2025",
          "standfirst": "Whether you’re already an avid runner or hope to be one as you start a new training regimen, you’ll get more out of your exercise routine if you have some good music to accompany you. Getting into the zone during a long run with your preferred music, be it rap, classic rock or today’s pop hits, can totally change your experience for the better. To do that, you have to start with a good pair of running headphones.But not all wireless workout headphones are created equally, and runners need to consider specific factors before investing in a pair like how long your runs are, what type of music or other audio you prefer listening to and how much you want to block out the world during a session. I’ve tested out more than a dozen pairs to find which are the best headphones for running for all budgets and all kinds of runners. Table of contents Best headphones for running in 2025 What to look for in running headphones How we test headphones for running Others headphones for running we tested Best headphones for running in 2025 Others headphones for running we tested Apple AirPods Pro 3 When it comes to running and working out, the edge that the AirPods Pro 3 have over the Pro 2, or even the top picks on our list, is built-in heart rate monitoring. That means you could go out with just your Pro 3 earbuds and your iPhone and still get heart rate information for your entire training session. But otherwise, the Pro 3 buds are just as capable as the Pro 2 when it comes to exercise. Some may prefer the soft-touch finish on our top picks to the AirPods' slick texture. Beats Powerbeats Pro 2 The Powerbeats Pro 2 are a good alternative to the Beats Fit Pro if you’re a stickler for a hook design. However, they cost $50 more than the Powerbeats Fit, and the main added advantage here is built-in heart rate sensors. Anker Soundcore AeroFit Pro The Soundcore AeroFit Pro is Anker’s version of the Shokz OpenFit, but I found the fit to be less secure and not as comfortable. The actual earbuds on the AeroFit Pro are noticeably bulkier than those on the OpenFit and that caused them to shift and move much more during exercise. They never fell off of my ears completely, but I spent more time adjusting them than I did enjoying them. JBL Endurance Peak 3 The most noteworthy thing about the Endurance Peak 3 is that they have the same IP68 rating as the Jabra Elite 8 Active, except they only cost $100. But, while you get the same protection here, you’ll have to sacrifice in other areas. The Endurance Peak 3 didn’t blow me away when it came to sound quality or comfort (its hook is more rigid than those on my favorite similarly designed buds) and their charging case is massive compared to most competitors. What to look for in running headphones Design Before diving in, it’s worth mentioning that this guide focuses on wireless earbuds. While you could wear over-ear or on-ear Bluetooth headphones during a run, most of the best headphones available now do not have the same level of durability. Water and dust resistance, particularly the former, is important for any audio gear you plan on sweating with or taking outdoors, and that’s more prevalent in the wireless earbuds world. Most earbuds have one of three designs: in-ear, in-ear with hook or open-ear. The first two are the most popular. In-ears are arguably the most common, while those with hooks promise better security and fit since they have an appendage that curls around the top of your ear. Open-ear designs don’t stick into your ear canal, but rather sit just outside of it. This makes it easier to hear the world around you while also listening to audio, and could be more comfortable for those who don’t like the intrusiveness of in-ear buds. Water resistance and dust protection Water resistance and dust protection are crucial for the best running headphones to have since you’ll likely be sweating while wearing them. Also, if you have the unfortunate luck of getting caught in the rain during a run, at least your gear will survive. Here’s a quick rundown of ingress protection (IP) ratings, which you’ll see attached to many earbuds on the market today. The first digit after the abbreviation rates dust protection on a scale from one to six — the higher, the better. The second digit refers to water- resistance, or waterproofing in some cases, ranked on a scale from one to nine. A letter “X” in either position means the device isn’t rated for the corresponding material. Check out this guide for an even more detailed breakdown. All of the earbuds we tested for this guide have at least an IPX4 rating (most have even more protection), which means they can withstand sweat and splashes but do not have dust protection. Active noise cancellation and transparency mode Active noise cancellation (ANC) is becoming a standard feature on wireless earbuds, at least in those above a certain price. If you’re looking for a pair of buds that can be your workout companion and continue to serve you when you’re off the trail, ANC is good to have. It adds versatility by allowing you to block out the hum of your home or office so you can focus, or give you some solitude during a busy commute on public transit. But an earbud’s ability to block out the world goes hand in hand with its ability to open things back up should you need it. Many earbuds with ANC support some sort of “transparency mode” or various levels of noise reduction. This is important for running headphones because you don’t want to be totally oblivious to what’s going on around you when you’re exercising outside along busy streets. Lowering noise cancelation levels to increase your awareness will help with that. Battery life All of the earbuds we tested have a battery life of six to eight hours. In general, that’s what you can expect from this space, with a few outliers that can get up to 15 hours of life on a charge. Even the low end of the spectrum should be good enough for most runners, but it’ll be handy to keep the buds’ charging case on you if you think you’ll get close to using up all their juice during a single session. Speaking of, you’ll get an average of 20-28 extra hours of battery out of most charging cases and all of the earbuds we tested had holders that provided at least an extra 15 hours. This will dictate how often you actually have to charge the device — as in physically connect the case with earbuds inside to a charging cable, or set it on a wireless charger to power up. How we test headphones for running When testing to determine the best running headphones, I wear each contender during as many runs as possible. I typically run three to five days each week, completing at least a 5K (3.01 miles) each time. I’m looking for comfort arguably most of all, because you should never be fussing with your earbuds when you’re on the tread or trail (as a note, I primarily run outside). I’m also paying attention to fit over time, particularly if the earbuds get slippery or loose while I sweat, or if they tend to pop out or feel less stable in my ears as I pick up speed or make quick movements. I also use the earbuds when not running to take calls and listen to music, podcasts and the like throughout the day. Many people will want just one pair of earbuds that they can use while exercising and just doing everyday things, so I evaluate each pair on their ability to be comfortable and provide a good listening experience in multiple different activities. While I am also listening for audio quality, I’m admittedly not an expert in this space. My colleague Billy Steele holds that title at Engadget, and you’ll find much more detailed information about sound quality for some of our top picks in his reviews and buying guides. Here, however, I will make note of audio-quality characteristics if they stood out to me (i.e. if a pair of earbuds had noticeably strong bass out of the box, weak highs, etc). Most of the wireless workout headphones we tested work with companion apps that have adjustable EQ settings, so you’re able to tweak sound profiles to your liking in most cases.This article originally appeared on Engadget at https://www.engadget.com/audio/headphones/best-headphones-for-running-120044637.html?src=rss",
          "content": "Whether you’re already an avid runner or hope to be one as you start a new training regimen, you’ll get more out of your exercise routine if you have some good music to accompany you. Getting into the zone during a long run with your preferred music, be it rap, classic rock or today’s pop hits, can totally change your experience for the better. To do that, you have to start with a good pair of running headphones.But not all wireless workout headphones are created equally, and runners need to consider specific factors before investing in a pair like how long your runs are, what type of music or other audio you prefer listening to and how much you want to block out the world during a session. I’ve tested out more than a dozen pairs to find which are the best headphones for running for all budgets and all kinds of runners. Table of contents Best headphones for running in 2025 What to look for in running headphones How we test headphones for running Others headphones for running we tested Best headphones for running in 2025 Others headphones for running we tested Apple AirPods Pro 3 When it comes to running and working out, the edge that the AirPods Pro 3 have over the Pro 2, or even the top picks on our list, is built-in heart rate monitoring. That means you could go out with just your Pro 3 earbuds and your iPhone and still get heart rate information for your entire training session. But otherwise, the Pro 3 buds are just as capable as the Pro 2 when it comes to exercise. Some may prefer the soft-touch finish on our top picks to the AirPods' slick texture. Beats Powerbeats Pro 2 The Powerbeats Pro 2 are a good alternative to the Beats Fit Pro if you’re a stickler for a hook design. However, they cost $50 more than the Powerbeats Fit, and the main added advantage here is built-in heart rate sensors. Anker Soundcore AeroFit Pro The Soundcore AeroFit Pro is Anker’s version of the Shokz OpenFit, but I found the fit to be less secure and not as comfortable. The actual earbuds on the AeroFit Pro are noticeably bulkier than those on the OpenFit and that caused them to shift and move much more during exercise. They never fell off of my ears completely, but I spent more time adjusting them than I did enjoying them. JBL Endurance Peak 3 The most noteworthy thing about the Endurance Peak 3 is that they have the same IP68 rating as the Jabra Elite 8 Active, except they only cost $100. But, while you get the same protection here, you’ll have to sacrifice in other areas. The Endurance Peak 3 didn’t blow me away when it came to sound quality or comfort (its hook is more rigid than those on my favorite similarly designed buds) and their charging case is massive compared to most competitors. What to look for in running headphones Design Before diving in, it’s worth mentioning that this guide focuses on wireless earbuds. While you could wear over-ear or on-ear Bluetooth headphones during a run, most of the best headphones available now do not have the same level of durability. Water and dust resistance, particularly the former, is important for any audio gear you plan on sweating with or taking outdoors, and that’s more prevalent in the wireless earbuds world. Most earbuds have one of three designs: in-ear, in-ear with hook or open-ear. The first two are the most popular. In-ears are arguably the most common, while those with hooks promise better security and fit since they have an appendage that curls around the top of your ear. Open-ear designs don’t stick into your ear canal, but rather sit just outside of it. This makes it easier to hear the world around you while also listening to audio, and could be more comfortable for those who don’t like the intrusiveness of in-ear buds. Water resistance and dust protection Water resistance and dust protection are crucial for the best running headphones to have since you’ll likely be sweating while wearing them. Also, if you have the unfortunate luck of getting caught in the rain during a run, at least your gear will survive. Here’s a quick rundown of ingress protection (IP) ratings, which you’ll see attached to many earbuds on the market today. The first digit after the abbreviation rates dust protection on a scale from one to six — the higher, the better. The second digit refers to water- resistance, or waterproofing in some cases, ranked on a scale from one to nine. A letter “X” in either position means the device isn’t rated for the corresponding material. Check out this guide for an even more detailed breakdown. All of the earbuds we tested for this guide have at least an IPX4 rating (most have even more protection), which means they can withstand sweat and splashes but do not have dust protection. Active noise cancellation and transparency mode Active noise cancellation (ANC) is becoming a standard feature on wireless earbuds, at least in those above a certain price. If you’re looking for a pair of buds that can be your workout companion and continue to serve you when you’re off the trail, ANC is good to have. It adds versatility by allowing you to block out the hum of your home or office so you can focus, or give you some solitude during a busy commute on public transit. But an earbud’s ability to block out the world goes hand in hand with its ability to open things back up should you need it. Many earbuds with ANC support some sort of “transparency mode” or various levels of noise reduction. This is important for running headphones because you don’t want to be totally oblivious to what’s going on around you when you’re exercising outside along busy streets. Lowering noise cancelation levels to increase your awareness will help with that. Battery life All of the earbuds we tested have a battery life of six to eight hours. In general, that’s what you can expect from this space, with a few outliers that can get up to 15 hours of life on a charge. Even the low end of the spectrum should be good enough for most runners, but it’ll be handy to keep the buds’ charging case on you if you think you’ll get close to using up all their juice during a single session. Speaking of, you’ll get an average of 20-28 extra hours of battery out of most charging cases and all of the earbuds we tested had holders that provided at least an extra 15 hours. This will dictate how often you actually have to charge the device — as in physically connect the case with earbuds inside to a charging cable, or set it on a wireless charger to power up. How we test headphones for running When testing to determine the best running headphones, I wear each contender during as many runs as possible. I typically run three to five days each week, completing at least a 5K (3.01 miles) each time. I’m looking for comfort arguably most of all, because you should never be fussing with your earbuds when you’re on the tread or trail (as a note, I primarily run outside). I’m also paying attention to fit over time, particularly if the earbuds get slippery or loose while I sweat, or if they tend to pop out or feel less stable in my ears as I pick up speed or make quick movements. I also use the earbuds when not running to take calls and listen to music, podcasts and the like throughout the day. Many people will want just one pair of earbuds that they can use while exercising and just doing everyday things, so I evaluate each pair on their ability to be comfortable and provide a good listening experience in multiple different activities. While I am also listening for audio quality, I’m admittedly not an expert in this space. My colleague Billy Steele holds that title at Engadget, and you’ll find much more detailed information about sound quality for some of our top picks in his reviews and buying guides. Here, however, I will make note of audio-quality characteristics if they stood out to me (i.e. if a pair of earbuds had noticeably strong bass out of the box, weak highs, etc). Most of the wireless workout headphones we tested work with companion apps that have adjustable EQ settings, so you’re able to tweak sound profiles to your liking in most cases.This article originally appeared on Engadget at https://www.engadget.com/audio/headphones/best-headphones-for-running-120044637.html?src=rss",
          "feed_position": 24
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/mobile/tablets/best-ipads-how-to-pick-the-best-apple-tablet-for-you-150054066.html",
          "published_at": "Fri, 24 Oct 2025 07:00:37 +0000",
          "title": "The best iPad for 2025: How to pick the best Apple tablet for you",
          "standfirst": "We’ve long considered Apple’s iPads to be the best tablets on the market, but determining exactly which model you should buy isn’t always straightforward. Do you just want a big screen for streaming and web browsing? Do you want to use it like a pseudo-laptop? Do you care about Apple Intelligence at all? If you’re not sure, allow us to help. We’ve tested every iPad available today and broken down which ones should best fit your needs below. Table of contents The best iPads for 2025 How we test the best iPads iPad FAQs Recent updates The best iPads for 2025 How we test the best iPads The top edge of the iPad mini. Photo by Nathan Ingraham / Engadget Much like we do for our guide to the best tablets overall, we spend several days with each iPad to see how they feel and perform with different tasks: watching videos, web browsing, playing both casual and graphically intense games, editing 4K photos and video, running multiple apps side-by-side, making FaceTime calls and the like. To better measure performance specifically, we use benchmarking tests like Geekbench 6, 3DMark and GFXBench Metal, plus we measure how long it takes for each tablet to boot up and open various apps. We also check how well each tablet holds up long-term, whether it’s with a review unit provided by Apple or an iPad model that’s owned by a member of the Engadget staff. To help compare the color performance and brightness of the displays, we play the same videos on different iPads, side-by-side, at equal brightness levels. We use each tablet in direct sunlight outdoors to see how well they hold up to glare, and we play a handful of the same musical tracks to evaluate speaker performance. For battery life, we keep track of how long each tablet generally lasts before it needs a recharge, but we also play a 1080p movie on a loop at roughly 70 percent brightness with power-sapping background processes off. We also test each device with an Apple Pencil and note how responsive the stylus feels. Finally, we carefully pore over spec sheets and software updates to keep track of which features are available on certain iPads but not others. iPad FAQs The iPad (A16) on top of an 13-inch iPad Air. Jeff Dunn for Engadget What are some new features coming to iPadOS 26? Apple released the latest update to its iPad operating system, iPadOS 26, in September. The update is a fairly significant overhaul, one that brings iPadOS closer to macOS than ever before. New features include the ability to open more windows simultaneously and resize or tile them more freely; a Mac-style Menu bar; a dedicated Preview app; an upgraded Files app; an improved ability to export or download large files in the background; an Exposé view that shows all open windows; a pointier cursor and the option to add folders to the Dock. It also uses the new “liquid glass” design language that Apple is rolling out across all of its platforms in 2025. That said, it completely removed the “slide over” and “split view” modes found in previous versions of iPadOS, which can make quickly viewing multiple apps at once a little more cumbersome. (Though the former will now return in an upcoming update.) Notably, most of these features are available across Apple’s tablet lineup, from the iPad Pro to the entry-level iPad. You can find the full list of compatible devices at the bottom of Apple’s overview page. How long do iPads typically last? If history is any indication, expect Apple to update your iPad to the latest version of iPadOS for at least five years, if not longer. The current iPadOS 26 update, for example, is available on iPad Pro models dating back to 2018 and other iPads dating back to 2019. How long your iPad’s hardware will last depends on which model you buy and how well you maintain it. (If you’re particularly clumsy, consider an iPad case.) A more powerful iPad Pro will feel fast for a longer time than an entry-level iPad, but each model should remain at least serviceable until Apple stops updating it, at minimum. What’s the difference between the iPad and the iPad Air? Compared to the standard iPad, the iPad Air runs on a stronger M3 chip (instead of the A16 Bionic) and has 2GB more RAM (8GB total). Both come with 128GB of storage by default. The Air is also available in two sizes, 11 and 13 inches, whereas the 11th-gen iPad doesn't offer the larger screen option. The M-series SoC gives the Air better long-term performance prospects, plus access to certain iPadOS features such as Apple Intelligence. Its display supports a wider P3 color gamut, has an antireflective coating and is fully laminated. The latter means there’s no “air gap” between the display and the glass covering it, so it feels more like you’re directly touching what’s on screen instead of interacting with an image below the glass. The Air also works with the newer Pencil Pro stylus and more comfortable Magic Keyboards, and its USB-C port supports faster data transfer speeds. It technically supports faster Wi-Fi 6E, too, while the lower-cost iPad uses Wi-Fi 6. Starting at $349, the 11th-gen iPad is $250 less expensive than the iPad Air. It has a similarly elegant design with flat edges, thin bezels, USB-C port, and a Touch ID reader. Battery life is rated at the same 10 hours, and both devices have their front-facing camera on their long edge, which is a more natural position for video calls. The cheaper iPad works with the first-gen and USB-C Apple Pencils – which are more convoluted to charge – and a unique keyboard accessory called the Magic Keyboard Folio. Jeff Dunn for Engadget What’s the difference between iPads and Android tablets? The operating system, duh. But to give a few more specifics: Android devices are available from more manufacturers and cover a wider price range. You won’t see an $80 iPad anytime soon. Android is also more malleable in that you can easily sideload apps from places beyond Google’s official app store and more extensively customize the look of the OS (though the former may no longer be an option in the coming months). Several Android tablets still have features like a headphone jack or a microSD slot for adding storage, too, though those are getting rarer. But we tend to recommend Apple tablets to those who have no allegiance either way. iPad apps are still a bit more likely to be designed specifically for larger screens, rather than looking like blown-up phone software, and Apple is just about peerless when it comes to long-term software support. Every new iPad hits a certain baseline of hardware quality and performance — none of them feel cheap, and all of them are fast enough for most needs. Plus, you’ll get the most out of an iPad if you use other Apple devices. Can an iPad replace a laptop? This is a loaded question, since laptop workflows differ from person to person. If you mostly use a notebook for browsing the web, watching videos or writing emails and word docs, then sure, you can get along just fine with an iPad and the right iPad accessories. It’ll be easier to carry around, the battery life is great and having the touchscreen and stylus support is handy (though many Windows users have that regardless). Even beyond the basics, plenty of media editors, graphic designers and digital artists have shown they can get things done on an iPad. Broadly speaking, though, a laptop OS tends to be more flexible when it comes to file management, multitasking, coding or other “heavy” tasks. The recent iPadOS 26 update does close the gap a bit, though it’s still not quite as fluid. Safari on the iPad isn’t fully on par with desktop browsers either. So the answer really depends on you. How do I take a screenshot on an iPad? As we note in our screenshot how-to guide, you can take a screenshot on your iPad by pressing the top button and either volume button at the same time. If you have an older iPad with a Home button, simultaneously press the top button and the Home button instead. Recent updates Late October 2025: The new M5-based iPad Pro replaces the previous-generation iPad Pro as our top pick for power users. Early October 2025: We’ve made a few edits to reflect the full release of iPadOS 26 and made sure our recommendations are still accurate. August 2025: We've taken another sweep to ensure our picks are still accurate and added a few more notes to our FAQ section. June 2025: We’ve made a few minor edits to reflect the announcement of Apple’s latest iPadOS update, which we detail above. May 2025: We’ve lightly edited this guide to ensure all details and links are still correct. We’re also keeping an eye on how the Trump administration’s tariff policy affects the pricing and stock of the iPad lineup (and every other tech category). All of our picks are still available at normal prices today, but we’ll update this guide if that changes. March 2025: We've reviewed the iPad (A16) and named it our new budget pick, removing the discontinued 10th-gen iPad in the process. March 2025: The recently-launched iPad Air M3 has replaced its predecessor as our top overall recommendation. We’ve also made a note regarding the new iPad (A16), which we plan to test in the near future and expect to become our new budget pick. We’ve made a handful of edits elsewhere in the guide to reflect Apple’s latest hardware. January 2025: We’ve lightly edited this guide for clarity. Our recommendations remain the same. October 2024: We've updated our guide to include the new iPad mini 7. June 2024: We’ve touched up this guide to reflect some of the new iPadOS features Apple announced at WWDC, though our picks remain the same. Nathan Ingraham contributed to this report.This article originally appeared on Engadget at https://www.engadget.com/mobile/tablets/best-ipads-how-to-pick-the-best-apple-tablet-for-you-150054066.html?src=rss",
          "content": "We’ve long considered Apple’s iPads to be the best tablets on the market, but determining exactly which model you should buy isn’t always straightforward. Do you just want a big screen for streaming and web browsing? Do you want to use it like a pseudo-laptop? Do you care about Apple Intelligence at all? If you’re not sure, allow us to help. We’ve tested every iPad available today and broken down which ones should best fit your needs below. Table of contents The best iPads for 2025 How we test the best iPads iPad FAQs Recent updates The best iPads for 2025 How we test the best iPads The top edge of the iPad mini. Photo by Nathan Ingraham / Engadget Much like we do for our guide to the best tablets overall, we spend several days with each iPad to see how they feel and perform with different tasks: watching videos, web browsing, playing both casual and graphically intense games, editing 4K photos and video, running multiple apps side-by-side, making FaceTime calls and the like. To better measure performance specifically, we use benchmarking tests like Geekbench 6, 3DMark and GFXBench Metal, plus we measure how long it takes for each tablet to boot up and open various apps. We also check how well each tablet holds up long-term, whether it’s with a review unit provided by Apple or an iPad model that’s owned by a member of the Engadget staff. To help compare the color performance and brightness of the displays, we play the same videos on different iPads, side-by-side, at equal brightness levels. We use each tablet in direct sunlight outdoors to see how well they hold up to glare, and we play a handful of the same musical tracks to evaluate speaker performance. For battery life, we keep track of how long each tablet generally lasts before it needs a recharge, but we also play a 1080p movie on a loop at roughly 70 percent brightness with power-sapping background processes off. We also test each device with an Apple Pencil and note how responsive the stylus feels. Finally, we carefully pore over spec sheets and software updates to keep track of which features are available on certain iPads but not others. iPad FAQs The iPad (A16) on top of an 13-inch iPad Air. Jeff Dunn for Engadget What are some new features coming to iPadOS 26? Apple released the latest update to its iPad operating system, iPadOS 26, in September. The update is a fairly significant overhaul, one that brings iPadOS closer to macOS than ever before. New features include the ability to open more windows simultaneously and resize or tile them more freely; a Mac-style Menu bar; a dedicated Preview app; an upgraded Files app; an improved ability to export or download large files in the background; an Exposé view that shows all open windows; a pointier cursor and the option to add folders to the Dock. It also uses the new “liquid glass” design language that Apple is rolling out across all of its platforms in 2025. That said, it completely removed the “slide over” and “split view” modes found in previous versions of iPadOS, which can make quickly viewing multiple apps at once a little more cumbersome. (Though the former will now return in an upcoming update.) Notably, most of these features are available across Apple’s tablet lineup, from the iPad Pro to the entry-level iPad. You can find the full list of compatible devices at the bottom of Apple’s overview page. How long do iPads typically last? If history is any indication, expect Apple to update your iPad to the latest version of iPadOS for at least five years, if not longer. The current iPadOS 26 update, for example, is available on iPad Pro models dating back to 2018 and other iPads dating back to 2019. How long your iPad’s hardware will last depends on which model you buy and how well you maintain it. (If you’re particularly clumsy, consider an iPad case.) A more powerful iPad Pro will feel fast for a longer time than an entry-level iPad, but each model should remain at least serviceable until Apple stops updating it, at minimum. What’s the difference between the iPad and the iPad Air? Compared to the standard iPad, the iPad Air runs on a stronger M3 chip (instead of the A16 Bionic) and has 2GB more RAM (8GB total). Both come with 128GB of storage by default. The Air is also available in two sizes, 11 and 13 inches, whereas the 11th-gen iPad doesn't offer the larger screen option. The M-series SoC gives the Air better long-term performance prospects, plus access to certain iPadOS features such as Apple Intelligence. Its display supports a wider P3 color gamut, has an antireflective coating and is fully laminated. The latter means there’s no “air gap” between the display and the glass covering it, so it feels more like you’re directly touching what’s on screen instead of interacting with an image below the glass. The Air also works with the newer Pencil Pro stylus and more comfortable Magic Keyboards, and its USB-C port supports faster data transfer speeds. It technically supports faster Wi-Fi 6E, too, while the lower-cost iPad uses Wi-Fi 6. Starting at $349, the 11th-gen iPad is $250 less expensive than the iPad Air. It has a similarly elegant design with flat edges, thin bezels, USB-C port, and a Touch ID reader. Battery life is rated at the same 10 hours, and both devices have their front-facing camera on their long edge, which is a more natural position for video calls. The cheaper iPad works with the first-gen and USB-C Apple Pencils – which are more convoluted to charge – and a unique keyboard accessory called the Magic Keyboard Folio. Jeff Dunn for Engadget What’s the difference between iPads and Android tablets? The operating system, duh. But to give a few more specifics: Android devices are available from more manufacturers and cover a wider price range. You won’t see an $80 iPad anytime soon. Android is also more malleable in that you can easily sideload apps from places beyond Google’s official app store and more extensively customize the look of the OS (though the former may no longer be an option in the coming months). Several Android tablets still have features like a headphone jack or a microSD slot for adding storage, too, though those are getting rarer. But we tend to recommend Apple tablets to those who have no allegiance either way. iPad apps are still a bit more likely to be designed specifically for larger screens, rather than looking like blown-up phone software, and Apple is just about peerless when it comes to long-term software support. Every new iPad hits a certain baseline of hardware quality and performance — none of them feel cheap, and all of them are fast enough for most needs. Plus, you’ll get the most out of an iPad if you use other Apple devices. Can an iPad replace a laptop? This is a loaded question, since laptop workflows differ from person to person. If you mostly use a notebook for browsing the web, watching videos or writing emails and word docs, then sure, you can get along just fine with an iPad and the right iPad accessories. It’ll be easier to carry around, the battery life is great and having the touchscreen and stylus support is handy (though many Windows users have that regardless). Even beyond the basics, plenty of media editors, graphic designers and digital artists have shown they can get things done on an iPad. Broadly speaking, though, a laptop OS tends to be more flexible when it comes to file management, multitasking, coding or other “heavy” tasks. The recent iPadOS 26 update does close the gap a bit, though it’s still not quite as fluid. Safari on the iPad isn’t fully on par with desktop browsers either. So the answer really depends on you. How do I take a screenshot on an iPad? As we note in our screenshot how-to guide, you can take a screenshot on your iPad by pressing the top button and either volume button at the same time. If you have an older iPad with a Home button, simultaneously press the top button and the Home button instead. Recent updates Late October 2025: The new M5-based iPad Pro replaces the previous-generation iPad Pro as our top pick for power users. Early October 2025: We’ve made a few edits to reflect the full release of iPadOS 26 and made sure our recommendations are still accurate. August 2025: We've taken another sweep to ensure our picks are still accurate and added a few more notes to our FAQ section. June 2025: We’ve made a few minor edits to reflect the announcement of Apple’s latest iPadOS update, which we detail above. May 2025: We’ve lightly edited this guide to ensure all details and links are still correct. We’re also keeping an eye on how the Trump administration’s tariff policy affects the pricing and stock of the iPad lineup (and every other tech category). All of our picks are still available at normal prices today, but we’ll update this guide if that changes. March 2025: We've reviewed the iPad (A16) and named it our new budget pick, removing the discontinued 10th-gen iPad in the process. March 2025: The recently-launched iPad Air M3 has replaced its predecessor as our top overall recommendation. We’ve also made a note regarding the new iPad (A16), which we plan to test in the near future and expect to become our new budget pick. We’ve made a handful of edits elsewhere in the guide to reflect Apple’s latest hardware. January 2025: We’ve lightly edited this guide for clarity. Our recommendations remain the same. October 2024: We've updated our guide to include the new iPad mini 7. June 2024: We’ve touched up this guide to reflect some of the new iPadOS features Apple announced at WWDC, though our picks remain the same. Nathan Ingraham contributed to this report.This article originally appeared on Engadget at https://www.engadget.com/mobile/tablets/best-ipads-how-to-pick-the-best-apple-tablet-for-you-150054066.html?src=rss",
          "feed_position": 25,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2024-10/ded6eb30-8fee-11ef-bfcf-f1599e27e076"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/mistral-launches-its-own-ai-studio-for-quick-development-with-its-european",
          "published_at": "Fri, 24 Oct 2025 06:55:00 GMT",
          "title": "Mistral launches its own AI Studio for quick development with its European open source, proprietary models",
          "standfirst": "The next big trend in AI providers appears to be \"studio\" environments on the web that allow users to spin up agents and AI applications within minutes. Case in point, today the well-funded French AI startup Mistral launched its own Mistral AI Studio, a new production platform designed to help enterprises build, observe, and operationalize AI applications at scale atop Mistral&#x27;s growing family of proprietary and open source large language models (LLMs) and multimodal models.It&#x27;s an evolution of its legacy API and AI building platorm, \"Le Platforme,\" initially launched in late 2023, and that brand name is being retired for now. The move comes just days after U.S. rival Google updated its AI Studio, also launched in late 2023, to be easier for non-developers to use and build and deploy apps with natural language, aka \"vibe coding.\"But while Google&#x27;s update appears to target novices who want to tinker around, Mistral appears more fully focused on building an easy-to-use enterprise AI app development and launchpad, which may require some technical knowledge or familiarity with LLMs, but far less than that of a seasoned developer. In other words, those outside the tech team at your enterprise could potentially use this to build and test simple apps, tools, and workflows — all powered by E.U.-native AI models operating on E.U.-based infrastructure. That may be a welcome change for companies concerned about the political situation in the U.S., or who have large operations in Europe and prefer to give their business to homegrown alternatives to U.S. and Chinese tech giants.In addition, Mistral AI Studio appears to offer an easier way for users to customize and fine-tune AI models for use at specific tasks.Branded as “The Production AI Platform,” Mistral&#x27;s AI Studio extends its internal infrastructure, bringing enterprise-grade observability, orchestration, and governance to teams running AI in production.The platform unifies tools for building, evaluating, and deploying AI systems, while giving enterprises flexible control over where and how their models run — in the cloud, on-premise, or self-hosted. Mistral says AI Studio brings the same production discipline that supports its own large-scale systems to external customers, closing the gap between AI prototyping and reliable deployment. It&#x27;s available here with developer documentation here.Extensive Model CatalogAI Studio’s model selector reveals one of the platform’s strongest features: a comprehensive and versioned catalog of Mistral models spanning open-weight, code, multimodal, and transcription domains.Available models include the following, though note that even for the open source ones, users will still be running a Mistral-based inference and paying Mistral for access through its API.ModelLicense TypeNotes / SourceMistral LargeProprietaryMistral’s top-tier closed-weight commercial model (available via API and AI Studio only).Mistral MediumProprietaryMid-range performance, offered via hosted API; no public weights released.Mistral SmallProprietaryLightweight API model; no open weights.Mistral TinyProprietaryCompact hosted model optimized for latency; closed-weight.Open Mistral 7BOpenFully open-weight model (Apache 2.0 license), downloadable on Hugging Face.Open Mixtral 8×7BOpenReleased under Apache 2.0; mixture-of-experts architecture.Open Mixtral 8×22BOpenLarger open-weight MoE model; Apache 2.0 license.Magistral MediumProprietaryNot publicly released; appears only in AI Studio catalog.Magistral SmallProprietarySame; internal or enterprise-only release.Devstral MediumProprietary / LegacyOlder internal development models, no open weights.Devstral SmallProprietary / LegacySame; used for internal evaluation.Ministral 8BOpenOpen-weight model available under Apache 2.0; basis for Mistral Moderation model.Pixtral 12BProprietaryMultimodal (text-image) model; closed-weight, API-only.Pixtral LargeProprietaryLarger multimodal variant; closed-weight.Voxtral SmallProprietarySpeech-to-text/audio model; closed-weight.Voxtral MiniProprietaryLightweight version; closed-weight.Voxtral Mini Transcribe 2507ProprietarySpecialized transcription model; API-only.Codestral 2501OpenOpen-weight code-generation model (Apache 2.0 license, available on Hugging Face).Mistral OCR 2503ProprietaryDocument-text extraction model; closed-weight.This extensive model lineup confirms that AI Studio is both model-rich and model-agnostic, allowing enterprises to test and deploy different configurations according to task complexity, cost targets, or compute environments.Bridging the Prototype-to-Production DivideMistral’s release highlights a common problem in enterprise AI adoption: while organizations are building more prototypes than ever before, few transition into dependable, observable systems. Many teams lack the infrastructure to track model versions, explain regressions, or ensure compliance as models evolve.AI Studio aims to solve that. The platform provides what Mistral calls the “production fabric” for AI — a unified environment that connects creation, observability, and governance into a single operational loop. Its architecture is organized around three core pillars: Observability, Agent Runtime, and AI Registry.1. ObservabilityAI Studio’s Observability layer provides transparency into AI system behavior. Teams can filter and inspect traffic through the Explorer, identify regressions, and build datasets directly from real-world usage. Judges let teams define evaluation logic and score outputs at scale, while Campaigns and Datasets automatically transform production interactions into curated evaluation sets.Metrics and dashboards quantify performance improvements, while lineage tracking connects model outcomes to the exact prompt and dataset versions that produced them. Mistral describes Observability as a way to move AI improvement from intuition to measurement.2. Agent Runtime and RAG supportThe Agent Runtime serves as the execution backbone of AI Studio. Each agent — whether it’s handling a single task or orchestrating a complex multi-step business process — runs within a stateful, fault-tolerant runtime built on Temporal. This architecture ensures reproducibility across long-running or retry-prone tasks and automatically captures execution graphs for auditing and sharing.Every run emits telemetry and evaluation data that feed directly into the Observability layer. The runtime supports hybrid, dedicated, and self-hosted deployments, allowing enterprises to run AI close to their existing systems while maintaining durability and control.While Mistral&#x27;s blog post doesn’t explicitly reference retrieval-augmented generation (RAG), Mistral AI Studio clearly supports it under the hood. Screenshots of the interface show built-in workflows such as RAGWorkflow, RetrievalWorkflow, and IngestionWorkflow, revealing that document ingestion, retrieval, and augmentation are first-class capabilities within the Agent Runtime system. These components allow enterprises to pair Mistral’s language models with their own proprietary or internal data sources, enabling contextualized responses grounded in up-to-date information. By integrating RAG directly into its orchestration and observability stack—but leaving it out of marketing language—Mistral signals that it views retrieval not as a buzzword but as a production primitive: measurable, governed, and auditable like any other AI process.3. AI RegistryThe AI Registry is the system of record for all AI assets — models, datasets, judges, tools, and workflows. It manages lineage, access control, and versioning, enforcing promotion gates and audit trails before deployments.Integrated directly with the Runtime and Observability layers, the Registry provides a unified governance view so teams can trace any output back to its source components.Interface and User ExperienceThe screenshots of Mistral AI Studio show a clean, developer-oriented interface organized around a left-hand navigation bar and a central Playground environment.The Home dashboard features three core action areas — Create, Observe, and Improve — guiding users through model building, monitoring, and fine-tuning workflows.Under Create, users can open the Playground to test prompts or build agents.Observe and Improve link to observability and evaluation modules, some labeled “coming soon,” suggesting staged rollout.The left navigation also includes quick access to API Keys, Batches, Evaluate, Fine-tune, Files, and Documentation, positioning Studio as a full workspace for both development and operations.Inside the Playground, users can select a model, customize parameters such as temperature and max tokens, and enable integrated tools that extend model capabilities.Users can try the Playground for free, but will need to sign up with their phone number to receive an access code.Integrated Tools and CapabilitiesMistral AI Studio includes a growing suite of built-in tools that can be toggled for any session:Code Interpreter — lets the model execute Python code directly within the environment, useful for data analysis, chart generation, or computational reasoning tasks.Image Generation — enables the model to generate images based on user prompts.Web Search — allows real-time information retrieval from the web to supplement model responses.Premium News — provides access to verified news sources via integrated provider partnerships, offering fact-checked context for information retrieval.These tools can be combined with Mistral’s function calling capabilities, letting models call APIs or external functions defined by developers. This means a single agent could, for example, search the web, retrieve verified financial data, run calculations in Python, and generate a chart — all within the same workflow.Beyond Text: Multimodal and Programmatic AIWith the inclusion of Code Interpreter and Image Generation, Mistral AI Studio moves beyond traditional text-based LLM workflows. Developers can use the platform to create agents that write and execute code, analyze uploaded files, or generate visual content — all directly within the same conversational environment.The Web Search and Premium News integrations also extend the model’s reach beyond static data, enabling real-time information retrieval with verified sources. This combination positions AI Studio not just as a playground for experimentation but as a full-stack environment for production AI systems capable of reasoning, coding, and multimodal output.Deployment FlexibilityMistral supports four main deployment models for AI Studio users:Hosted Access via AI Studio — pay-as-you-go APIs for Mistral’s latest models, managed through Studio workspaces.Third-Party Cloud Integration — availability through major cloud providers.Self-Deployment — open-weight models can be deployed on private infrastructure under the Apache 2.0 license, using frameworks such as TensorRT-LLM, vLLM, llama.cpp, or Ollama.Enterprise-Supported Self-Deployment — adds official support for both open and proprietary models, including security and compliance configuration assistance.These options allow enterprises to balance operational control with convenience, running AI wherever their data and governance requirements demand.Safety, Guardrailing, and ModerationAI Studio builds safety features directly into its stack. Enterprises can apply guardrails and moderation filters at both the model and API levels.The Mistral Moderation model, based on Ministral 8B (24.10), classifies text across policy categories such as sexual content, hate and discrimination, violence, self-harm, and PII. A separate system prompt guardrail can be activated to enforce responsible AI behavior, instructing models to “assist with care, respect, and truth” while avoiding harmful or unethical content.Developers can also employ self-reflection prompts, a technique where the model itself classifies outputs against enterprise-defined safety categories like physical harm or fraud. This layered approach gives organizations flexibility in enforcing safety policies while retaining creative or operational control.From Experimentation to Dependable OperationsMistral positions AI Studio as the next phase in enterprise AI maturity. As large language models become more capable and accessible, the company argues, the differentiator will no longer be model performance but the ability to operate AI reliably, safely, and measurably.AI Studio is designed to support that shift. By integrating evaluation, telemetry, version control, and governance into one workspace, it enables teams to manage AI with the same discipline as modern software systems — tracking every change, measuring every improvement, and maintaining full ownership of data and outcomes.In the company’s words, “This is how AI moves from experimentation to dependable operations — secure, observable, and under your control.”Mistral AI Studio is available starting October 24, 2025, as part of a private beta program. Enterprises can sign up on Mistral’s website to access the platform, explore its model catalog, and test observability, runtime, and governance features before general release.",
          "content": "The next big trend in AI providers appears to be \"studio\" environments on the web that allow users to spin up agents and AI applications within minutes. Case in point, today the well-funded French AI startup Mistral launched its own Mistral AI Studio, a new production platform designed to help enterprises build, observe, and operationalize AI applications at scale atop Mistral&#x27;s growing family of proprietary and open source large language models (LLMs) and multimodal models.It&#x27;s an evolution of its legacy API and AI building platorm, \"Le Platforme,\" initially launched in late 2023, and that brand name is being retired for now. The move comes just days after U.S. rival Google updated its AI Studio, also launched in late 2023, to be easier for non-developers to use and build and deploy apps with natural language, aka \"vibe coding.\"But while Google&#x27;s update appears to target novices who want to tinker around, Mistral appears more fully focused on building an easy-to-use enterprise AI app development and launchpad, which may require some technical knowledge or familiarity with LLMs, but far less than that of a seasoned developer. In other words, those outside the tech team at your enterprise could potentially use this to build and test simple apps, tools, and workflows — all powered by E.U.-native AI models operating on E.U.-based infrastructure. That may be a welcome change for companies concerned about the political situation in the U.S., or who have large operations in Europe and prefer to give their business to homegrown alternatives to U.S. and Chinese tech giants.In addition, Mistral AI Studio appears to offer an easier way for users to customize and fine-tune AI models for use at specific tasks.Branded as “The Production AI Platform,” Mistral&#x27;s AI Studio extends its internal infrastructure, bringing enterprise-grade observability, orchestration, and governance to teams running AI in production.The platform unifies tools for building, evaluating, and deploying AI systems, while giving enterprises flexible control over where and how their models run — in the cloud, on-premise, or self-hosted. Mistral says AI Studio brings the same production discipline that supports its own large-scale systems to external customers, closing the gap between AI prototyping and reliable deployment. It&#x27;s available here with developer documentation here.Extensive Model CatalogAI Studio’s model selector reveals one of the platform’s strongest features: a comprehensive and versioned catalog of Mistral models spanning open-weight, code, multimodal, and transcription domains.Available models include the following, though note that even for the open source ones, users will still be running a Mistral-based inference and paying Mistral for access through its API.ModelLicense TypeNotes / SourceMistral LargeProprietaryMistral’s top-tier closed-weight commercial model (available via API and AI Studio only).Mistral MediumProprietaryMid-range performance, offered via hosted API; no public weights released.Mistral SmallProprietaryLightweight API model; no open weights.Mistral TinyProprietaryCompact hosted model optimized for latency; closed-weight.Open Mistral 7BOpenFully open-weight model (Apache 2.0 license), downloadable on Hugging Face.Open Mixtral 8×7BOpenReleased under Apache 2.0; mixture-of-experts architecture.Open Mixtral 8×22BOpenLarger open-weight MoE model; Apache 2.0 license.Magistral MediumProprietaryNot publicly released; appears only in AI Studio catalog.Magistral SmallProprietarySame; internal or enterprise-only release.Devstral MediumProprietary / LegacyOlder internal development models, no open weights.Devstral SmallProprietary / LegacySame; used for internal evaluation.Ministral 8BOpenOpen-weight model available under Apache 2.0; basis for Mistral Moderation model.Pixtral 12BProprietaryMultimodal (text-image) model; closed-weight, API-only.Pixtral LargeProprietaryLarger multimodal variant; closed-weight.Voxtral SmallProprietarySpeech-to-text/audio model; closed-weight.Voxtral MiniProprietaryLightweight version; closed-weight.Voxtral Mini Transcribe 2507ProprietarySpecialized transcription model; API-only.Codestral 2501OpenOpen-weight code-generation model (Apache 2.0 license, available on Hugging Face).Mistral OCR 2503ProprietaryDocument-text extraction model; closed-weight.This extensive model lineup confirms that AI Studio is both model-rich and model-agnostic, allowing enterprises to test and deploy different configurations according to task complexity, cost targets, or compute environments.Bridging the Prototype-to-Production DivideMistral’s release highlights a common problem in enterprise AI adoption: while organizations are building more prototypes than ever before, few transition into dependable, observable systems. Many teams lack the infrastructure to track model versions, explain regressions, or ensure compliance as models evolve.AI Studio aims to solve that. The platform provides what Mistral calls the “production fabric” for AI — a unified environment that connects creation, observability, and governance into a single operational loop. Its architecture is organized around three core pillars: Observability, Agent Runtime, and AI Registry.1. ObservabilityAI Studio’s Observability layer provides transparency into AI system behavior. Teams can filter and inspect traffic through the Explorer, identify regressions, and build datasets directly from real-world usage. Judges let teams define evaluation logic and score outputs at scale, while Campaigns and Datasets automatically transform production interactions into curated evaluation sets.Metrics and dashboards quantify performance improvements, while lineage tracking connects model outcomes to the exact prompt and dataset versions that produced them. Mistral describes Observability as a way to move AI improvement from intuition to measurement.2. Agent Runtime and RAG supportThe Agent Runtime serves as the execution backbone of AI Studio. Each agent — whether it’s handling a single task or orchestrating a complex multi-step business process — runs within a stateful, fault-tolerant runtime built on Temporal. This architecture ensures reproducibility across long-running or retry-prone tasks and automatically captures execution graphs for auditing and sharing.Every run emits telemetry and evaluation data that feed directly into the Observability layer. The runtime supports hybrid, dedicated, and self-hosted deployments, allowing enterprises to run AI close to their existing systems while maintaining durability and control.While Mistral&#x27;s blog post doesn’t explicitly reference retrieval-augmented generation (RAG), Mistral AI Studio clearly supports it under the hood. Screenshots of the interface show built-in workflows such as RAGWorkflow, RetrievalWorkflow, and IngestionWorkflow, revealing that document ingestion, retrieval, and augmentation are first-class capabilities within the Agent Runtime system. These components allow enterprises to pair Mistral’s language models with their own proprietary or internal data sources, enabling contextualized responses grounded in up-to-date information. By integrating RAG directly into its orchestration and observability stack—but leaving it out of marketing language—Mistral signals that it views retrieval not as a buzzword but as a production primitive: measurable, governed, and auditable like any other AI process.3. AI RegistryThe AI Registry is the system of record for all AI assets — models, datasets, judges, tools, and workflows. It manages lineage, access control, and versioning, enforcing promotion gates and audit trails before deployments.Integrated directly with the Runtime and Observability layers, the Registry provides a unified governance view so teams can trace any output back to its source components.Interface and User ExperienceThe screenshots of Mistral AI Studio show a clean, developer-oriented interface organized around a left-hand navigation bar and a central Playground environment.The Home dashboard features three core action areas — Create, Observe, and Improve — guiding users through model building, monitoring, and fine-tuning workflows.Under Create, users can open the Playground to test prompts or build agents.Observe and Improve link to observability and evaluation modules, some labeled “coming soon,” suggesting staged rollout.The left navigation also includes quick access to API Keys, Batches, Evaluate, Fine-tune, Files, and Documentation, positioning Studio as a full workspace for both development and operations.Inside the Playground, users can select a model, customize parameters such as temperature and max tokens, and enable integrated tools that extend model capabilities.Users can try the Playground for free, but will need to sign up with their phone number to receive an access code.Integrated Tools and CapabilitiesMistral AI Studio includes a growing suite of built-in tools that can be toggled for any session:Code Interpreter — lets the model execute Python code directly within the environment, useful for data analysis, chart generation, or computational reasoning tasks.Image Generation — enables the model to generate images based on user prompts.Web Search — allows real-time information retrieval from the web to supplement model responses.Premium News — provides access to verified news sources via integrated provider partnerships, offering fact-checked context for information retrieval.These tools can be combined with Mistral’s function calling capabilities, letting models call APIs or external functions defined by developers. This means a single agent could, for example, search the web, retrieve verified financial data, run calculations in Python, and generate a chart — all within the same workflow.Beyond Text: Multimodal and Programmatic AIWith the inclusion of Code Interpreter and Image Generation, Mistral AI Studio moves beyond traditional text-based LLM workflows. Developers can use the platform to create agents that write and execute code, analyze uploaded files, or generate visual content — all directly within the same conversational environment.The Web Search and Premium News integrations also extend the model’s reach beyond static data, enabling real-time information retrieval with verified sources. This combination positions AI Studio not just as a playground for experimentation but as a full-stack environment for production AI systems capable of reasoning, coding, and multimodal output.Deployment FlexibilityMistral supports four main deployment models for AI Studio users:Hosted Access via AI Studio — pay-as-you-go APIs for Mistral’s latest models, managed through Studio workspaces.Third-Party Cloud Integration — availability through major cloud providers.Self-Deployment — open-weight models can be deployed on private infrastructure under the Apache 2.0 license, using frameworks such as TensorRT-LLM, vLLM, llama.cpp, or Ollama.Enterprise-Supported Self-Deployment — adds official support for both open and proprietary models, including security and compliance configuration assistance.These options allow enterprises to balance operational control with convenience, running AI wherever their data and governance requirements demand.Safety, Guardrailing, and ModerationAI Studio builds safety features directly into its stack. Enterprises can apply guardrails and moderation filters at both the model and API levels.The Mistral Moderation model, based on Ministral 8B (24.10), classifies text across policy categories such as sexual content, hate and discrimination, violence, self-harm, and PII. A separate system prompt guardrail can be activated to enforce responsible AI behavior, instructing models to “assist with care, respect, and truth” while avoiding harmful or unethical content.Developers can also employ self-reflection prompts, a technique where the model itself classifies outputs against enterprise-defined safety categories like physical harm or fraud. This layered approach gives organizations flexibility in enforcing safety policies while retaining creative or operational control.From Experimentation to Dependable OperationsMistral positions AI Studio as the next phase in enterprise AI maturity. As large language models become more capable and accessible, the company argues, the differentiator will no longer be model performance but the ability to operate AI reliably, safely, and measurably.AI Studio is designed to support that shift. By integrating evaluation, telemetry, version control, and governance into one workspace, it enables teams to manage AI with the same discipline as modern software systems — tracking every change, measuring every improvement, and maintaining full ownership of data and outcomes.In the company’s words, “This is how AI moves from experimentation to dependable operations — secure, observable, and under your control.”Mistral AI Studio is available starting October 24, 2025, as part of a private beta program. Enterprises can sign up on Mistral’s website to access the platform, explore its model catalog, and test observability, runtime, and governance features before general release.",
          "feed_position": 1,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/1fMUbtU7YznYAg9NA7yB60/80213a82826047e09229046ab093081a/cfr0z3n_top_down_view_of_diverse_modern_office_workers_at_desks_ef397a8f-5455-47b1-a73e-990ad12aedf1.png?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/inside-ring-1t-ant-engineers-solve-reinforcement-learning-bottlenecks-at",
          "published_at": "Fri, 24 Oct 2025 04:00:00 GMT",
          "title": "Inside Ring-1T: Ant engineers solve reinforcement learning bottlenecks at trillion scale",
          "standfirst": "China’s Ant Group, an affiliate of Alibaba, detailed technical information around its new model, Ring-1T, which the company said is “the first open-source reasoning model with one trillion total parameters.”Ring-1T aims to compete with other reasoning models like GPT-5 and the o-series from OpenAI, as well as Google’s Gemini 2.5. With the new release of the latest model, Ant extends the geopolitical debate over who will dominate the AI race: China or the US. Ant Group said Ring-1T is optimized for mathematical and logical problems, code generation and scientific problem-solving. “With approximately 50 billion activated parameters per token, Ring-1T achieves state-of-the-art performance across multiple challenging benchmarks — despite relying solely on natural language reasoning capabilities,” Ant said in a paper.Ring-1T, which was first released on preview in September, adopts the same architecture as Ling 2.0 and trained on the Ling-1T-base model the company released earlier this month. Ant said this allows the model to support up to 128,000 tokens.To train a model as large as Ring-1T, researchers had to develop new methods to scale reinforcement learning (RL).New methods of training Ant Group developed three “interconnected innovations” to support the RL and training of Ring-1T, a challenge given the model&#x27;s size and the typically large compute requirements it entails. These three are IcePop, C3PO++ and ASystem.IcePop removes noisy gradient updates to stabilize training without slowing inference. It helps eliminate catastrophic training-inference misalignment in RL. The researchers noted that when training models, particularly those using a mixture-of-experts (MoE) architecture like Ring-1T, there can often be a discrepancy in probability calculations. “This problem is particularly pronounced in the training of MoE models with RL due to the inherent usage of the dynamic routing mechanism. Additionally, in long CoT settings, these discrepancies can gradually accumulate across iterations and become further amplified,” the researchers said. IcePop “suppresses unstable training updates through double-sided masking calibration.”The next new method the researchers had to develop is C3PO++, an improved version of the C3PO system that Ant previously established. The method manages how Ring-1T and other extra-large parameter models generate and process training examples, or what they call rollouts, so GPUs don’t sit idle. The way it works would break work in rollouts into pieces to process in parallel. One group is the inference pool, which generates new data, and the other is the training pool, which collects results to update the model. C3PO++ creates a token budget to control how much data is processed, ensuring GPUs are used efficiently.The last new method, ASystem, adopts a SingleController+SPMD (Single Program, Multiple Data) architecture to enable asynchronous operations. Benchmark resultsAnt pointed Ring-1T to benchmarks measuring performance in mathematics, coding, logical reasoning and general tasks. They tested it against models such as DeepSeek-V3.1-Terminus-Thinking, Qwen-35B-A22B-Thinking-2507, Gemini 2.5 Pro and GPT-5 Thinking. In benchmark testing, Ring-1T performed strongly, coming in second to OpenAI’s GPT-5 across most benchmarks. Ant said that Ring-1T showed the best performance among all the open-weight models it tested. The model posted a 93.4% score on the AIME 25 leaderboard, second only to GPT-5. In coding, Ring-1T outperformed both DeepSeek and Qwen.“It indicates that our carefully synthesized dataset shapes Ring-1T’s robust performance on programming applications, which forms a strong foundation for future endeavors on agentic applications,” the company said. Ring-1T shows how much Chinese companies are investing in models Ring-1T is just the latest model from China aiming to dethrone GPT-5 and Gemini. Chinese companies have been releasing impressive models at a quick pace since the surprise launch of DeepSeek in January. Ant&#x27;s parent company, Alibaba, recently released Qwen3-Omni, a multimodal model that natively unifies text, image, audio and video. DeepSeek has also continued to improve its models and earlier this month, launched DeepSeek-OCR. This new model reimagines how models process information. With Ring-1T and Ant’s development of new methods to train and scale extra-large models, the battle for AI dominance between the US and China continues to heat up.",
          "content": "China’s Ant Group, an affiliate of Alibaba, detailed technical information around its new model, Ring-1T, which the company said is “the first open-source reasoning model with one trillion total parameters.”Ring-1T aims to compete with other reasoning models like GPT-5 and the o-series from OpenAI, as well as Google’s Gemini 2.5. With the new release of the latest model, Ant extends the geopolitical debate over who will dominate the AI race: China or the US. Ant Group said Ring-1T is optimized for mathematical and logical problems, code generation and scientific problem-solving. “With approximately 50 billion activated parameters per token, Ring-1T achieves state-of-the-art performance across multiple challenging benchmarks — despite relying solely on natural language reasoning capabilities,” Ant said in a paper.Ring-1T, which was first released on preview in September, adopts the same architecture as Ling 2.0 and trained on the Ling-1T-base model the company released earlier this month. Ant said this allows the model to support up to 128,000 tokens.To train a model as large as Ring-1T, researchers had to develop new methods to scale reinforcement learning (RL).New methods of training Ant Group developed three “interconnected innovations” to support the RL and training of Ring-1T, a challenge given the model&#x27;s size and the typically large compute requirements it entails. These three are IcePop, C3PO++ and ASystem.IcePop removes noisy gradient updates to stabilize training without slowing inference. It helps eliminate catastrophic training-inference misalignment in RL. The researchers noted that when training models, particularly those using a mixture-of-experts (MoE) architecture like Ring-1T, there can often be a discrepancy in probability calculations. “This problem is particularly pronounced in the training of MoE models with RL due to the inherent usage of the dynamic routing mechanism. Additionally, in long CoT settings, these discrepancies can gradually accumulate across iterations and become further amplified,” the researchers said. IcePop “suppresses unstable training updates through double-sided masking calibration.”The next new method the researchers had to develop is C3PO++, an improved version of the C3PO system that Ant previously established. The method manages how Ring-1T and other extra-large parameter models generate and process training examples, or what they call rollouts, so GPUs don’t sit idle. The way it works would break work in rollouts into pieces to process in parallel. One group is the inference pool, which generates new data, and the other is the training pool, which collects results to update the model. C3PO++ creates a token budget to control how much data is processed, ensuring GPUs are used efficiently.The last new method, ASystem, adopts a SingleController+SPMD (Single Program, Multiple Data) architecture to enable asynchronous operations. Benchmark resultsAnt pointed Ring-1T to benchmarks measuring performance in mathematics, coding, logical reasoning and general tasks. They tested it against models such as DeepSeek-V3.1-Terminus-Thinking, Qwen-35B-A22B-Thinking-2507, Gemini 2.5 Pro and GPT-5 Thinking. In benchmark testing, Ring-1T performed strongly, coming in second to OpenAI’s GPT-5 across most benchmarks. Ant said that Ring-1T showed the best performance among all the open-weight models it tested. The model posted a 93.4% score on the AIME 25 leaderboard, second only to GPT-5. In coding, Ring-1T outperformed both DeepSeek and Qwen.“It indicates that our carefully synthesized dataset shapes Ring-1T’s robust performance on programming applications, which forms a strong foundation for future endeavors on agentic applications,” the company said. Ring-1T shows how much Chinese companies are investing in models Ring-1T is just the latest model from China aiming to dethrone GPT-5 and Gemini. Chinese companies have been releasing impressive models at a quick pace since the surprise launch of DeepSeek in January. Ant&#x27;s parent company, Alibaba, recently released Qwen3-Omni, a multimodal model that natively unifies text, image, audio and video. DeepSeek has also continued to improve its models and earlier this month, launched DeepSeek-OCR. This new model reimagines how models process information. With Ring-1T and Ant’s development of new methods to train and scale extra-large models, the battle for AI dominance between the US and China continues to heat up.",
          "feed_position": 2,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/1ZZDt515lczBZYmiAPxjJm/323c164f6ce43f0bac50cb5d392d747a/crimedy7_illustration_of_ants_building_a_robot_but_the_robot__62e35f6c-d1f6-47e9-bf3c-393e15b45361_1.png?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/cybersecurity/vpn/best-vpn-130004396.html",
          "published_at": "Fri, 24 Oct 2025 00:00:35 +0000",
          "title": "The best VPN service for 2025",
          "standfirst": "As frustrating as it is that governments and businesses are running roughshod over our online freedoms, at least we have plenty of good VPNs to choose from to keep us protected online. There are so many fast, intelligently designed, full-featured and affordable services on the market that the biggest problem is picking one. For any use case, you can bet at least two providers will be neck-and-neck for first place.On the other hand, the VPN world is still the Wild West in some ways. It's easy enough to slap a cheap VPN together that the market is flooded with low-quality apps that put more money into advertising than infrastructure. They may look good, but it's all styrofoam under the hood.I built this list of the best VPNs after intensive testing to help you reorient your focus on the providers that actually deserve your time and money. Which one truly fits your needs is dependent on who you are and what you do online, but if you pick any of my seven recommendations, you can't go too far wrong.For each VPN on this list, I've shared which platforms it works on, how much it cuts into your download speed, where it offers servers, what other features are included and how much the best available deal costs. At the end, I'll list some honorable and dishonorable mentions, then answer some of the most common questions I hear about VPNs.Editor's note: This list has been completely overhauled and rewritten as of September 2025. We intend to revisit this list every three months at a minimum, at which time our picks may be adjusted based on changes in pricing, features, testing results and other factors. Table of contents Best VPNs for 2025 Other VPNs we tested What to look for in a VPN VPN FAQs Best VPNs for 2025 Other VPNs we tested The VPNs in this section didn't crack our top list above, but we're summarizing them here so you can see their positives and negatives as of the time of our evaluation. Windscribe Windscribe is another well-known free VPN supported by paid subscriptions. In many ways, it takes the best from both Mullvad and Proton VPN, with the former's no-nonsense privacy and the latter's healthy free plan. Without paying, you can connect to 10 of Windscribe's server locations on an unlimited number of devices at once. Unfortunately, Windscribe didn't copy the most important part of Proton VPN's free plan — the unlimited data. You're only allowed to use 10GB per month, which isn't enough for regular streaming. It's also committed to a cramped and headache-inducing user interface that stands out from the crowd in all the worst ways. Private Internet Access Private Internet Access (PIA VPN) has a deeply annoying name — I assume whoever invented it also likes to hop in their Toyota Forward Motion to grab a gallon of Sustaining Cow Extract from the grocery store — but it's a worthwhile VPN whose pricing provides incredible value. Its monthly and yearly plans are good enough, but its three-year plan is the clincher. Not only is it longer than average, but you can continue to renew at the three-year level, so you won't see an unpleasant price jump the first time you re-up. PIA's apps have a dark UI reminiscent of Proton VPN, which is always a good thing. It also supports port forwarding, custom DNS and the use of a SOCKS5 or Shadowsocks proxy as a second step in the VPN connection. You can even set the maximum data packet size to help out a struggling connection, as I cover in my full PIA VPN review. The downside is that your connection will struggle a lot. While well-designed, PIA's apps have a tendency to lag. In my most recent battery of tests, it dragged oddly on my internet in ways that weren't directly reflected in the speed tests. It's also not always capable of unblocking streaming services in other countries, and while its server network offers 152 IP address options in 84 countries, it's heavily bulked out by virtual locations. TunnelBear TunnelBear has a decent interface, which its target audience of VPN beginners will find very easy to use. Its speeds are perfectly good too, and I appreciate the depth and breadth of its transparency reports. But it's far too limited overall, with few extra features, less than 50 server locations and a free plan that caps data at 2GB per month. VyprVPN VyprVPN often flies under the radar, but it has some of the best apps in the business and a very good security record (there was a breach in 2023, but it didn't crack the VPN encryption itself). It's also got a verified privacy policy, a solid jurisdiction and runs every connection through an in-house DNS to prevent leaks. Despite all that, it didn't make the top seven because its connection speeds aren't up to scratch — you'll likely notice a bigger slowdown than average. It also has a troubling history of wild, seemingly experimental swings in its pricing and simultaneous connection limits. Norton VPN Norton VPN is part of the Norton 360 package that includes the well-known antivirus software and other security apps. It's a nice bonus if you use Norton already, but as a standalone VPN, it falls short. My tests repeatedly showed it dropping encryption and revealing my IP address whenever I switched servers, and not all of its locations managed to unblock Netflix. This isn't to say Norton VPN is terrible. It has a fairly large server network, user-friendly apps and some cool features like an IP rotator. It also recently revamped its OpenVPN infrastructure to improve speeds on Windows. But you probably won't find those things sufficient to balance out significant speed drops on other platforms or poorly written FAQs. I especially advise against Norton VPN for Apple users, as its Mac and iPhone apps are much more limited than their Windows and Android counterparts. What to look for in a VPN Choosing a VPN can quickly get you mired in analysis paralysis. We're here to help, but since only you know your particular needs, you should know the major red and green flags so you can make the final call yourself. Every reputable VPN provider offers a free trial or refund guarantee you can use to run the tests below. Compatibility: First, make sure your VPN works on all the platforms you plan to use it on. Most VPNs have apps for Windows, Mac, Android and iOS, but those apps aren't always created equal — check that the app for your chosen OS is user-friendly and has all the features you need. Speed: Use a speed testing app to see how fast your internet is before and after connecting to the VPN (I use Ookla's speedtest.net). To check security, look up your IP address while connected to a VPN server and see if it's actually changed your virtual location. Be sure it's using expert-vetted protocols like OpenVPN, WireGuard and IKEv2. Try connecting to streaming services and seeing whether the VPN changes the available content. Background: Do some outside research into the VPN's origins, especially its parent company, privacy policy and any past incidents. It's a dealbreaker if you can't figure out where the VPN is headquartered (which indicates a lax approach to transparency) or if it seems to have never passed a real third-party audit. Server network: Look at the server network to make sure the VPN has locations near you and in any countries where you'll want an IP address — e.g. if you need a VPN to unblock Canadian Netflix, look for multiple server locations in Canada. Customer Service: I also advise testing the customer support options by looking for the answer to a straightforward question. If phone support (versus email and chat) is important to you, make sure to prioritize that — and make sure it's available at convenient times in your timezone. Pricing: Finally, check prices. See if the VPN is affordable and decide whether you're comfortable taking a long-term subscription for better savings. If you do get a multi-year plan, check what price it will renew at, since many of the cheapest subscriptions are only introductory deals. VPN FAQs To wrap up, let's answer some of the most common questions we get about VPNs. Feel free to get in touch if you have a query I don't cover here. What is a VPN? VPN stands for virtual private network. There are a few different types of VPNs, but for this list, we're talking about commercial services that let individual users access the internet with an assumed identity. Whenever you get online, you're assigned an IP address — a digital nametag that tells websites where to send the information you request. For an IP address to work, it needs to be unique, which means it's possible to create a record of what an individual does online. When you use a VPN, all the data you send to the internet goes through one of the VPN's servers before heading to its final destination. The VPN encrypts the connection between your computer and its server so the data won't trace back to you. Any website, ISP or third party that cares to look will only see the VPN's IP address, not yours. What are some things VPNs are used for? The three main use cases for a commercial VPN are security, privacy and entertainment. Using a VPN conceals your real IP address from anyone who might want to use it for nefarious purposes like cyberstalking, DDoS attacks or deducing your real location. It also keeps your ISP from profiling you for ads based on where you live or what you do online. One side effect of borrowing a VPN's IP address is that you can make it appear as though your connection is coming from another country. You can use this to access streaming content and platforms that are only available in certain regions due to copyright. Changing your location can even get you better prices when shopping online. Location spoofing can also be used to get online in countries that censor internet access, like China and Russia, as well as certain US states or countries — like the UK — that are adding barriers like age-gating to previously unfettered online access. All you have to do is connect to a neighboring country (or locality) where the internet isn't blocked. If you plan to do this while traveling, make sure you have the VPN downloaded before you go, as some nations prevent you from even loading a VPN's homepage. Make sure you check with local laws regarding the legality of VPN use as well — just because your VPN traffic is encrypted doesn't mean that authorities can't detect that it's being used in a given location. Are VPNs worth it? Whether a VPN is worth the price depends on how much you value those three use cases above. It's no secret that your personal information is profitable for a lot of people, from illicit hackers to corporations to law enforcement. A VPN will not make you completely anonymous, nor is it a license to commit crimes (see the next question) but it will give you a lot more control over what you transmit to the world. With entertainment, the value is even clearer. You can use a VPN to fight back against streaming balkanization by getting more shows and movies out of a single platform — for example, a lot of shows that have been kicked off American Netflix are still on Netflix in other countries. What information does a VPN hide? A VPN does not make it impossible for you to be unmasked or taken advantage of online. It prevents you from passively leaking information, keeps your IP address undiscoverable on public wi-fi networks and gets you around online censorship. However, if you share personal information of your own volition, there's nothing the VPN can do. If you reveal your password in a social media post or click a link in a phishing email, that information bypasses the VPN. Likewise, if you do anything sensitive while logged into an account, the account holder will have that information even if you're using a VPN. A VPN is a critical part of your online security, but it can't do the whole job by itself. Healthy passwords, malware scanners, private search engines and common sense all have roles to play. Never forget, too, that using a VPN means trusting the VPN provider with access to information that's concealed from everyone else — make sure you trust the privacy policy before signing up. Are VPNs safe? As far as we can determine, all the VPNs recommended in this story are safe to use. As with anything you subscribe to online, due diligence is important, but there's very little inherent risk; generally, the worst thing a bad VPN will do is fail to work, leaving you no worse off than before. There are some VPNs (usually offered for free) that transmit malware, so always make sure to look up any complaints or warnings about a service before you download it. Can you get a VPN on your phone? Absolutely — almost every VPN has apps for both desktop and mobile devices. A good VPN will redesign its app to be mobile-friendly without dropping too many features. Both iOS and Android natively support VPN connections, so you're free to choose whichever provider you like. What about Google's One VPN? Google One VPN was, as you might expect, a VPN provided by Google. It was launched in 2020 for Google One subscribers and discontinued in 2024 due to lack of use. If you really want a Google VPN, you can still get one if you have certain Pixel models or if you're a Google Fi subscriber. That said, I don't recommend using a VPN from Google even if you do still have access to one. Google is one of the worst big tech companies at protecting user privacy. While its VPN might not leak, I wouldn't trust it to guard your sensitive information.This article originally appeared on Engadget at https://www.engadget.com/cybersecurity/vpn/best-vpn-130004396.html?src=rss",
          "content": "As frustrating as it is that governments and businesses are running roughshod over our online freedoms, at least we have plenty of good VPNs to choose from to keep us protected online. There are so many fast, intelligently designed, full-featured and affordable services on the market that the biggest problem is picking one. For any use case, you can bet at least two providers will be neck-and-neck for first place.On the other hand, the VPN world is still the Wild West in some ways. It's easy enough to slap a cheap VPN together that the market is flooded with low-quality apps that put more money into advertising than infrastructure. They may look good, but it's all styrofoam under the hood.I built this list of the best VPNs after intensive testing to help you reorient your focus on the providers that actually deserve your time and money. Which one truly fits your needs is dependent on who you are and what you do online, but if you pick any of my seven recommendations, you can't go too far wrong.For each VPN on this list, I've shared which platforms it works on, how much it cuts into your download speed, where it offers servers, what other features are included and how much the best available deal costs. At the end, I'll list some honorable and dishonorable mentions, then answer some of the most common questions I hear about VPNs.Editor's note: This list has been completely overhauled and rewritten as of September 2025. We intend to revisit this list every three months at a minimum, at which time our picks may be adjusted based on changes in pricing, features, testing results and other factors. Table of contents Best VPNs for 2025 Other VPNs we tested What to look for in a VPN VPN FAQs Best VPNs for 2025 Other VPNs we tested The VPNs in this section didn't crack our top list above, but we're summarizing them here so you can see their positives and negatives as of the time of our evaluation. Windscribe Windscribe is another well-known free VPN supported by paid subscriptions. In many ways, it takes the best from both Mullvad and Proton VPN, with the former's no-nonsense privacy and the latter's healthy free plan. Without paying, you can connect to 10 of Windscribe's server locations on an unlimited number of devices at once. Unfortunately, Windscribe didn't copy the most important part of Proton VPN's free plan — the unlimited data. You're only allowed to use 10GB per month, which isn't enough for regular streaming. It's also committed to a cramped and headache-inducing user interface that stands out from the crowd in all the worst ways. Private Internet Access Private Internet Access (PIA VPN) has a deeply annoying name — I assume whoever invented it also likes to hop in their Toyota Forward Motion to grab a gallon of Sustaining Cow Extract from the grocery store — but it's a worthwhile VPN whose pricing provides incredible value. Its monthly and yearly plans are good enough, but its three-year plan is the clincher. Not only is it longer than average, but you can continue to renew at the three-year level, so you won't see an unpleasant price jump the first time you re-up. PIA's apps have a dark UI reminiscent of Proton VPN, which is always a good thing. It also supports port forwarding, custom DNS and the use of a SOCKS5 or Shadowsocks proxy as a second step in the VPN connection. You can even set the maximum data packet size to help out a struggling connection, as I cover in my full PIA VPN review. The downside is that your connection will struggle a lot. While well-designed, PIA's apps have a tendency to lag. In my most recent battery of tests, it dragged oddly on my internet in ways that weren't directly reflected in the speed tests. It's also not always capable of unblocking streaming services in other countries, and while its server network offers 152 IP address options in 84 countries, it's heavily bulked out by virtual locations. TunnelBear TunnelBear has a decent interface, which its target audience of VPN beginners will find very easy to use. Its speeds are perfectly good too, and I appreciate the depth and breadth of its transparency reports. But it's far too limited overall, with few extra features, less than 50 server locations and a free plan that caps data at 2GB per month. VyprVPN VyprVPN often flies under the radar, but it has some of the best apps in the business and a very good security record (there was a breach in 2023, but it didn't crack the VPN encryption itself). It's also got a verified privacy policy, a solid jurisdiction and runs every connection through an in-house DNS to prevent leaks. Despite all that, it didn't make the top seven because its connection speeds aren't up to scratch — you'll likely notice a bigger slowdown than average. It also has a troubling history of wild, seemingly experimental swings in its pricing and simultaneous connection limits. Norton VPN Norton VPN is part of the Norton 360 package that includes the well-known antivirus software and other security apps. It's a nice bonus if you use Norton already, but as a standalone VPN, it falls short. My tests repeatedly showed it dropping encryption and revealing my IP address whenever I switched servers, and not all of its locations managed to unblock Netflix. This isn't to say Norton VPN is terrible. It has a fairly large server network, user-friendly apps and some cool features like an IP rotator. It also recently revamped its OpenVPN infrastructure to improve speeds on Windows. But you probably won't find those things sufficient to balance out significant speed drops on other platforms or poorly written FAQs. I especially advise against Norton VPN for Apple users, as its Mac and iPhone apps are much more limited than their Windows and Android counterparts. What to look for in a VPN Choosing a VPN can quickly get you mired in analysis paralysis. We're here to help, but since only you know your particular needs, you should know the major red and green flags so you can make the final call yourself. Every reputable VPN provider offers a free trial or refund guarantee you can use to run the tests below. Compatibility: First, make sure your VPN works on all the platforms you plan to use it on. Most VPNs have apps for Windows, Mac, Android and iOS, but those apps aren't always created equal — check that the app for your chosen OS is user-friendly and has all the features you need. Speed: Use a speed testing app to see how fast your internet is before and after connecting to the VPN (I use Ookla's speedtest.net). To check security, look up your IP address while connected to a VPN server and see if it's actually changed your virtual location. Be sure it's using expert-vetted protocols like OpenVPN, WireGuard and IKEv2. Try connecting to streaming services and seeing whether the VPN changes the available content. Background: Do some outside research into the VPN's origins, especially its parent company, privacy policy and any past incidents. It's a dealbreaker if you can't figure out where the VPN is headquartered (which indicates a lax approach to transparency) or if it seems to have never passed a real third-party audit. Server network: Look at the server network to make sure the VPN has locations near you and in any countries where you'll want an IP address — e.g. if you need a VPN to unblock Canadian Netflix, look for multiple server locations in Canada. Customer Service: I also advise testing the customer support options by looking for the answer to a straightforward question. If phone support (versus email and chat) is important to you, make sure to prioritize that — and make sure it's available at convenient times in your timezone. Pricing: Finally, check prices. See if the VPN is affordable and decide whether you're comfortable taking a long-term subscription for better savings. If you do get a multi-year plan, check what price it will renew at, since many of the cheapest subscriptions are only introductory deals. VPN FAQs To wrap up, let's answer some of the most common questions we get about VPNs. Feel free to get in touch if you have a query I don't cover here. What is a VPN? VPN stands for virtual private network. There are a few different types of VPNs, but for this list, we're talking about commercial services that let individual users access the internet with an assumed identity. Whenever you get online, you're assigned an IP address — a digital nametag that tells websites where to send the information you request. For an IP address to work, it needs to be unique, which means it's possible to create a record of what an individual does online. When you use a VPN, all the data you send to the internet goes through one of the VPN's servers before heading to its final destination. The VPN encrypts the connection between your computer and its server so the data won't trace back to you. Any website, ISP or third party that cares to look will only see the VPN's IP address, not yours. What are some things VPNs are used for? The three main use cases for a commercial VPN are security, privacy and entertainment. Using a VPN conceals your real IP address from anyone who might want to use it for nefarious purposes like cyberstalking, DDoS attacks or deducing your real location. It also keeps your ISP from profiling you for ads based on where you live or what you do online. One side effect of borrowing a VPN's IP address is that you can make it appear as though your connection is coming from another country. You can use this to access streaming content and platforms that are only available in certain regions due to copyright. Changing your location can even get you better prices when shopping online. Location spoofing can also be used to get online in countries that censor internet access, like China and Russia, as well as certain US states or countries — like the UK — that are adding barriers like age-gating to previously unfettered online access. All you have to do is connect to a neighboring country (or locality) where the internet isn't blocked. If you plan to do this while traveling, make sure you have the VPN downloaded before you go, as some nations prevent you from even loading a VPN's homepage. Make sure you check with local laws regarding the legality of VPN use as well — just because your VPN traffic is encrypted doesn't mean that authorities can't detect that it's being used in a given location. Are VPNs worth it? Whether a VPN is worth the price depends on how much you value those three use cases above. It's no secret that your personal information is profitable for a lot of people, from illicit hackers to corporations to law enforcement. A VPN will not make you completely anonymous, nor is it a license to commit crimes (see the next question) but it will give you a lot more control over what you transmit to the world. With entertainment, the value is even clearer. You can use a VPN to fight back against streaming balkanization by getting more shows and movies out of a single platform — for example, a lot of shows that have been kicked off American Netflix are still on Netflix in other countries. What information does a VPN hide? A VPN does not make it impossible for you to be unmasked or taken advantage of online. It prevents you from passively leaking information, keeps your IP address undiscoverable on public wi-fi networks and gets you around online censorship. However, if you share personal information of your own volition, there's nothing the VPN can do. If you reveal your password in a social media post or click a link in a phishing email, that information bypasses the VPN. Likewise, if you do anything sensitive while logged into an account, the account holder will have that information even if you're using a VPN. A VPN is a critical part of your online security, but it can't do the whole job by itself. Healthy passwords, malware scanners, private search engines and common sense all have roles to play. Never forget, too, that using a VPN means trusting the VPN provider with access to information that's concealed from everyone else — make sure you trust the privacy policy before signing up. Are VPNs safe? As far as we can determine, all the VPNs recommended in this story are safe to use. As with anything you subscribe to online, due diligence is important, but there's very little inherent risk; generally, the worst thing a bad VPN will do is fail to work, leaving you no worse off than before. There are some VPNs (usually offered for free) that transmit malware, so always make sure to look up any complaints or warnings about a service before you download it. Can you get a VPN on your phone? Absolutely — almost every VPN has apps for both desktop and mobile devices. A good VPN will redesign its app to be mobile-friendly without dropping too many features. Both iOS and Android natively support VPN connections, so you're free to choose whichever provider you like. What about Google's One VPN? Google One VPN was, as you might expect, a VPN provided by Google. It was launched in 2020 for Google One subscribers and discontinued in 2024 due to lack of use. If you really want a Google VPN, you can still get one if you have certain Pixel models or if you're a Google Fi subscriber. That said, I don't recommend using a VPN from Google even if you do still have access to one. Google is one of the worst big tech companies at protecting user privacy. While its VPN might not leak, I wouldn't trust it to guard your sensitive information.This article originally appeared on Engadget at https://www.engadget.com/cybersecurity/vpn/best-vpn-130004396.html?src=rss",
          "feed_position": 27
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/openai-launches-company-knowledge-in-chatgpt-letting-you-access-your-firms",
          "published_at": "Thu, 23 Oct 2025 22:19:00 GMT",
          "title": "OpenAI launches company knowledge in ChatGPT, letting you access your firm's data from Google Drive, Slack, GitHub",
          "standfirst": "Is the Google Search for internal enterprise knowledge finally here...but from OpenAI? It certainly seems that way. Today, OpenAI has launched company knowledge in ChatGPT, a major new capability for subscribers to ChatGPT&#x27;s paid Business, Enterprise, and Edu plans that lets them call up their company&#x27;s data directly from third-party workplace apps including Slack, SharePoint, Google Drive, Gmail, GitHub, HubSpot and combine it in ChatGPT outputs to them. As OpenAI&#x27;s CEO of Applications Fidji Simo put it in a post on the social network X: \"it brings all the context from your apps (Slack, Google Drive, GitHub, etc) together in ChatGPT so you can get answers that are specific to your business.\"Intriguingly, OpenAI&#x27;s blog post on the feature states that is \"powered by a version of GPT‑5 that’s trained to look across multiple sources to give more comprehensive and accurate answers,\" which sounds to me like a new fine-tuned version of the model family the company released back in August, though there are no additional details on how it was trained or its size, techniques, etc.OpenAI tells VentureBeat it&#x27;s a version of GPT-5 that specifically powers company knowledge in ChatGPT Business, Enterprise, and Edu. Nonetheless, company knowledge in ChatGPT is rolling out globally and is designed to make ChatGPT a central point of access for verified organizational information, supported by secure integrations and enterprise-grade compliance controls, and give employees way faster access to their company&#x27;s information while working.Now, instead of toggling over to Slack to find the assignment you were given and instructions, or tabbing over to Google Drive and opening up specific files to find the names and numbers you need to call, ChatGPT can deliver all that type of information directly into your chat session — if your company enables the proper connections.As OpenAI Chief Operating Officer Brad Lightcap wrote in a post on the social network X: \"company knowledge has changed how i use chatgpt at work more than anything we have built so far - let us know what you think!\"It builds upon the third-party app connectors unveiled back in August 2025, though those were only for individual users on the ChatGPT Plus plans.Connecting ChatGPT to Workplace SystemsEnterprise teams often face the challenge of fragmented data across various internal tools—email, chat, file storage, project management, and customer platforms. Company knowledge bridges those silos by enabling ChatGPT to connect to approved systems like, and other supported apps through enterprise-managed connectors.Each response generated with company knowledge includes citations and direct links to the original sources, allowing teams to verify where specific details originated. This transparency helps organizations maintain data trustworthiness while increasing productivity.The sidebar shows a live view of the sources being examined and what it is getting from them. When it’s done, you’ll see exactly the sources used, along with the specific snippets it drew from. You can then click on any citation to open the original source for more details.Built for Enterprise Control and SecurityCompany knowledge was designed from the ground up for enterprise governance and compliance. It respects existing permissions within connected apps — ChatGPT can only access what a user is already authorized to view— and never trains on company data by default.Security features include industry-standard encryption, support for SSO and SCIM for account provisioning, and IP allowlisting to restrict access to approved corporate networks. Enterprise administrators can also define role-based access control (RBAC) policies and manage permissions at a group or department level.OpenAI’s Enterprise Compliance API provides a full audit trail, allowing administrators to review conversation logs for reporting and regulatory purposes. This capability helps enterprises meet internal governance standards and industry-specific requirements such as SOC 2 and ISO 27001 compliance.Admin Configuration and Connector ManagementFor enterprise deployment, administrators must enable company knowledge and its connectors within the ChatGPT workspace. Once connectors are active, users can authenticate their own accounts for each work app they need to access.In Enterprise and Edu plans, connectors are off by default and require explicit admin approval before employees can use them. Admins can selectively enable connectors, manage access by role, and require SSO-based authentication for enhanced control.Business plan users, by contrast, have connectors enabled automatically if available in their workspace. Admins can still oversee which connectors are approved, ensuring alignment with internal IT and data policies.Company knowledge becomes available to any user with at least one active connector, and admins can configure group-level permissions for different teams — such as restricting GitHub access to engineering while enabling Google Drive or HubSpot for marketing and sales.Organizations who turn on the feature can also elect to turn it off just as easily. Once you disconnect a connector, ChatGPT does not have access to that data.How Company Knowledge Works in PracticeActivating company knowledge is straightforward. Users can start a new or existing conversation in ChatGPT and select “Company knowledge” under the message composer or from the tools menu. It must be turned on proactively for each new conversation or chat session, even from the same user.After authenticating their connected apps, they can ask questions as usual—such as “Summarize this account’s latest feedback and risks” or “Compile a Q4 performance summary from project trackers.”ChatGPT searches across the connected tools, retrieves relevant context, and produces an answer with full citations and source links. The system can combine data across apps — for instance, blending Slack updates, Google Docs notes, and HubSpot CRM records — to create an integrated view of a project, client, or initiative.When company knowledge is not selected, ChatGPT may still use connectors in a limited capacity as part of the default experience, but responses will not include detailed citations or multi-source synthesis.Advanced Use Cases for Enterprise TeamsFor development and operations leaders, company knowledge can act as a centralized intelligence layer that surfaces real-time updates and dependencies across complex workflows. ChatGPT can, for example, summarize open GitHub pull requests, highlight unresolved Linear tickets, and cross-reference Slack engineering discussions—all in a single output.Technical teams can also use it for incident retrospectives or release planning by pulling relevant information from issue trackers, logs, and meeting notes. Procurement or finance leaders can use it to consolidate purchase requests or budget updates across shared drives and internal communications.Because the model can reference structured and unstructured data simultaneously, it supports wide-ranging scenarios—from compliance documentation reviews to cross-departmental performance summaries.Privacy, Data Residency, and ComplianceEnterprise data protection is a central design element of company knowledge. ChatGPT processes data in line with OpenAI’s enterprise-grade security model, ensuring that no connected app data leaves the secure boundary of the organization’s authorized environment.Data residency policies vary by connector. Certain integrations, such as Slack, support region-specific data storage, while others—like Google Drive and SharePoint—are available for U.S.-based customers with or without at-rest data residency. Organizations with regional compliance obligations can review connector-specific security documentation for details.No geo restrictions apply to company knowledge, making it suitable for multinational organizations operating across multiple jurisdictions.Limitations and Future EnhancementsAt present, users must manually enable company knowledge in each new ChatGPT conversation. OpenAI is developing a unified interface that will automatically integrate company knowledge with other ChatGPT tools—such as browsing and chart generation—so that users won’t need to toggle between modes.When enabled, company knowledge temporarily disables web browsing and visual output generation, though users can switch modes within the same conversation to re-enable those features.OpenAI also continues to expand the network of supported tools. Recent updates have added connectors for Asana, GitLab Issues, and ClickUp, and OpenAI plans to support future MCP (Model Context Protocol) connectors to enable custom, developer-built integrations.Availability and Getting StartedCompany knowledge is now available to all ChatGPT Business, Enterprise, and Edu users. Organizations can begin by enabling the feature under the ChatGPT message composer and connecting approved work apps.For enterprise rollouts, OpenAI recommends a phased deployment: first enabling core connectors (such as Google Drive and Slack), configuring RBAC and SSO, then expanding to specialized systems once data access policies are verified.Procurement and security leaders evaluating the feature should note that company knowledge is covered under existing ChatGPT Enterprise terms and uses the same encryption, compliance, and service-level guarantees.With company knowledge, OpenAI aims to make ChatGPT not just a conversational assistant but an intelligent interface to enterprise data—delivering secure, context-aware insights that help technical and business leaders act with confidence.",
          "content": "Is the Google Search for internal enterprise knowledge finally here...but from OpenAI? It certainly seems that way. Today, OpenAI has launched company knowledge in ChatGPT, a major new capability for subscribers to ChatGPT&#x27;s paid Business, Enterprise, and Edu plans that lets them call up their company&#x27;s data directly from third-party workplace apps including Slack, SharePoint, Google Drive, Gmail, GitHub, HubSpot and combine it in ChatGPT outputs to them. As OpenAI&#x27;s CEO of Applications Fidji Simo put it in a post on the social network X: \"it brings all the context from your apps (Slack, Google Drive, GitHub, etc) together in ChatGPT so you can get answers that are specific to your business.\"Intriguingly, OpenAI&#x27;s blog post on the feature states that is \"powered by a version of GPT‑5 that’s trained to look across multiple sources to give more comprehensive and accurate answers,\" which sounds to me like a new fine-tuned version of the model family the company released back in August, though there are no additional details on how it was trained or its size, techniques, etc.OpenAI tells VentureBeat it&#x27;s a version of GPT-5 that specifically powers company knowledge in ChatGPT Business, Enterprise, and Edu. Nonetheless, company knowledge in ChatGPT is rolling out globally and is designed to make ChatGPT a central point of access for verified organizational information, supported by secure integrations and enterprise-grade compliance controls, and give employees way faster access to their company&#x27;s information while working.Now, instead of toggling over to Slack to find the assignment you were given and instructions, or tabbing over to Google Drive and opening up specific files to find the names and numbers you need to call, ChatGPT can deliver all that type of information directly into your chat session — if your company enables the proper connections.As OpenAI Chief Operating Officer Brad Lightcap wrote in a post on the social network X: \"company knowledge has changed how i use chatgpt at work more than anything we have built so far - let us know what you think!\"It builds upon the third-party app connectors unveiled back in August 2025, though those were only for individual users on the ChatGPT Plus plans.Connecting ChatGPT to Workplace SystemsEnterprise teams often face the challenge of fragmented data across various internal tools—email, chat, file storage, project management, and customer platforms. Company knowledge bridges those silos by enabling ChatGPT to connect to approved systems like, and other supported apps through enterprise-managed connectors.Each response generated with company knowledge includes citations and direct links to the original sources, allowing teams to verify where specific details originated. This transparency helps organizations maintain data trustworthiness while increasing productivity.The sidebar shows a live view of the sources being examined and what it is getting from them. When it’s done, you’ll see exactly the sources used, along with the specific snippets it drew from. You can then click on any citation to open the original source for more details.Built for Enterprise Control and SecurityCompany knowledge was designed from the ground up for enterprise governance and compliance. It respects existing permissions within connected apps — ChatGPT can only access what a user is already authorized to view— and never trains on company data by default.Security features include industry-standard encryption, support for SSO and SCIM for account provisioning, and IP allowlisting to restrict access to approved corporate networks. Enterprise administrators can also define role-based access control (RBAC) policies and manage permissions at a group or department level.OpenAI’s Enterprise Compliance API provides a full audit trail, allowing administrators to review conversation logs for reporting and regulatory purposes. This capability helps enterprises meet internal governance standards and industry-specific requirements such as SOC 2 and ISO 27001 compliance.Admin Configuration and Connector ManagementFor enterprise deployment, administrators must enable company knowledge and its connectors within the ChatGPT workspace. Once connectors are active, users can authenticate their own accounts for each work app they need to access.In Enterprise and Edu plans, connectors are off by default and require explicit admin approval before employees can use them. Admins can selectively enable connectors, manage access by role, and require SSO-based authentication for enhanced control.Business plan users, by contrast, have connectors enabled automatically if available in their workspace. Admins can still oversee which connectors are approved, ensuring alignment with internal IT and data policies.Company knowledge becomes available to any user with at least one active connector, and admins can configure group-level permissions for different teams — such as restricting GitHub access to engineering while enabling Google Drive or HubSpot for marketing and sales.Organizations who turn on the feature can also elect to turn it off just as easily. Once you disconnect a connector, ChatGPT does not have access to that data.How Company Knowledge Works in PracticeActivating company knowledge is straightforward. Users can start a new or existing conversation in ChatGPT and select “Company knowledge” under the message composer or from the tools menu. It must be turned on proactively for each new conversation or chat session, even from the same user.After authenticating their connected apps, they can ask questions as usual—such as “Summarize this account’s latest feedback and risks” or “Compile a Q4 performance summary from project trackers.”ChatGPT searches across the connected tools, retrieves relevant context, and produces an answer with full citations and source links. The system can combine data across apps — for instance, blending Slack updates, Google Docs notes, and HubSpot CRM records — to create an integrated view of a project, client, or initiative.When company knowledge is not selected, ChatGPT may still use connectors in a limited capacity as part of the default experience, but responses will not include detailed citations or multi-source synthesis.Advanced Use Cases for Enterprise TeamsFor development and operations leaders, company knowledge can act as a centralized intelligence layer that surfaces real-time updates and dependencies across complex workflows. ChatGPT can, for example, summarize open GitHub pull requests, highlight unresolved Linear tickets, and cross-reference Slack engineering discussions—all in a single output.Technical teams can also use it for incident retrospectives or release planning by pulling relevant information from issue trackers, logs, and meeting notes. Procurement or finance leaders can use it to consolidate purchase requests or budget updates across shared drives and internal communications.Because the model can reference structured and unstructured data simultaneously, it supports wide-ranging scenarios—from compliance documentation reviews to cross-departmental performance summaries.Privacy, Data Residency, and ComplianceEnterprise data protection is a central design element of company knowledge. ChatGPT processes data in line with OpenAI’s enterprise-grade security model, ensuring that no connected app data leaves the secure boundary of the organization’s authorized environment.Data residency policies vary by connector. Certain integrations, such as Slack, support region-specific data storage, while others—like Google Drive and SharePoint—are available for U.S.-based customers with or without at-rest data residency. Organizations with regional compliance obligations can review connector-specific security documentation for details.No geo restrictions apply to company knowledge, making it suitable for multinational organizations operating across multiple jurisdictions.Limitations and Future EnhancementsAt present, users must manually enable company knowledge in each new ChatGPT conversation. OpenAI is developing a unified interface that will automatically integrate company knowledge with other ChatGPT tools—such as browsing and chart generation—so that users won’t need to toggle between modes.When enabled, company knowledge temporarily disables web browsing and visual output generation, though users can switch modes within the same conversation to re-enable those features.OpenAI also continues to expand the network of supported tools. Recent updates have added connectors for Asana, GitLab Issues, and ClickUp, and OpenAI plans to support future MCP (Model Context Protocol) connectors to enable custom, developer-built integrations.Availability and Getting StartedCompany knowledge is now available to all ChatGPT Business, Enterprise, and Edu users. Organizations can begin by enabling the feature under the ChatGPT message composer and connecting approved work apps.For enterprise rollouts, OpenAI recommends a phased deployment: first enabling core connectors (such as Google Drive and Slack), configuring RBAC and SSO, then expanding to specialized systems once data access policies are verified.Procurement and security leaders evaluating the feature should note that company knowledge is covered under existing ChatGPT Enterprise terms and uses the same encryption, compliance, and service-level guarantees.With company knowledge, OpenAI aims to make ChatGPT not just a conversational assistant but an intelligent interface to enterprise data—delivering secure, context-aware insights that help technical and business leaders act with confidence.",
          "feed_position": 3,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/77MUUhh1PM8EeHOJyHjmwX/2f1b317b8af31339017a06f84d871d58/cfr0z3n_third-person_view_of_woman_seated_at_desk_in_a_home_off_d68bd7db-ac86-44be-8e05-2cdb878c7190.png?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/computing/laptops/best-macbook-140032524.html",
          "published_at": "Thu, 23 Oct 2025 19:01:26 +0000",
          "title": "The best MacBook for 2025: Which Apple laptop should you buy?",
          "standfirst": "Picking the best MacBook may seem like an easy decision. After all, Apple just makes two models: the MacBook Air and the MacBook Pro. But the available variations within those categories — screen size, chip type, capacity and more — deserve some consideration. You also may wonder what the real-world differences are between models and who they’re best for. To make things even more interesting, Apple keeps announcing new chips. The latest, the M5 came out October 15, and is now found in the base model, 14-inch MacBook Pro (as well as the iPad Pro and the Vision Pro). This guide breaks down Apple’s terminology, as well as all which upgrades make the most sense so you can get the best MacBook for what you want to do. Table of contents Best MacBooks for 2025 What about budget MacBooks? Factors to consider when buying a MacBook MacBooks specs comparison chart Best MacBook FAQs Best MacBooks for 2025 What about budget MacBooks? Historically, Apple kept the previous year’s MacBook Air in its lineup as a sort of budget option. But the company took a different approach with the release of the M4 MacBook Air. Instead of continuing to sell the older model, Apple discontinued the M3 Air and gave its newest computer a $100 price cut. Now, if you can even find a brand new M3 MacBook Air (typically from retailers like Amazon or B&H), it’s often more expensive than the M4 version. During sales like Amazon Prime Day, we’ve seen the newest M4 Air go for as little as $799. That effectively makes our overall pick a budget pick as well. Of course, $800 isn’t exactly a small investment either for college students or others on a budget. Especially when you can find some decent PCs for under $500. If you’re looking to save even more on a MacBook, we recommend checking out refurbished options directly from Apple, or even third party sellers like BackMarket. There are a few guidelines to keep in mind, which we go over in our refurbished guide, but mainly, you’ll want to shop from a reputable source that has a stated process and offers at least a year-long warranty. Using your old gear as a trade-in will bring down your final cost as well. Factors to consider when buying a MacBook Compared to PCs, Apple computers tend to have more streamlined specifications. The company has long been known for this simplicity, and the M-series “system-on-a-chip” condenses things even further. Prior to the M1 chip, Apple used Intel chips in its laptop and desktop computers. The M2 and M3 generations followed that first chip and currently, MacBooks come equipped with M4 and M5-series chips. You’ll find the standard M4 processor in the Air. The base-model 14-inch Pro now comes with either the latest M5 chip. Other Pro configurations have the M4 Max or the M4 Pro (currently there is no M4 Ultra chip, as there was with the M3 series in the Mac Studio). All M-series chips combine, among other technologies, the CPU, graphics card and unified memory (RAM). Apple’s Neural Engine is included too, which is a specialized group of processor cores that handles machine learning tasks such as image analysis and voice recognition. While a unified chip means you have fewer decisions to make when picking a MacBook, there are still a few factors to consider, including specs like the number of CPU cores, amount of RAM, storage capacity, screen size, and, obviously, price. The finish color may be a minor consideration, but it's worth pointing out that the Pro comes in just two colors (Silver or Space Black) but the Air comes in four hues (Midnight, Starlight, Sky Blue and Silver). CPU cores The lowest-specced chip in a current-lineup MacBook is the standard M4 chip, which is found in all models of the MacBook Air. That chip houses a 10-core CPU and either an 8- or 10-core GPU. The base-model MacBook Pro uses the latest M5 chip, but only on the 14-inch model. The upgraded versions of that laptop use the M4 Pro or M4 Max chips (which are a step up from their predecessors, the M3, M3 Pro and M3 Max chips). The M4 Max is the burliest chip and built with either a 14- or 16-core CPU and a 32- or 40-core GPU. Cores are, in essence, smaller processing units that can handle different tasks simultaneously. Having more of them translates to the computer being able to run multiple programs and applications at once, while also smoothly processing demanding tasks like video and photo editing and high-level gaming. In short, more cores allow for more advanced computing and better performance. But if your processing power needs fall below professional-level gaming and cinematic video and audio editing, getting the highest number of cores is likely overkill — and after all, more cores equals higher cost and more power usage. Photo by Devindra Hardawar/Engadget RAM Your options for RAM (or unified memory) varies, but when Apple switched to the M4 chip for the MacBook Air, the lowest amount of RAM you can get was bumped to 16GB. That’s a necessary jump to accommodate the tech world’s favorite feature of the moment: AI or, in this case, Apple Intelligence (still AI, but Cupertino’s version). The M4 Pro chip has 24 or 48GB memory options, while the M4 Max chip supports 48, 64 or a whopping 128GB of RAM. The M5 chip in the base-model MacBook Pro comes with a minimum of 16GB and can be configured to a maximum of 32GB of RAM.. You’ve likely heard the analogy comparing memory to the amount of workspace available on a literal desktop surface, whereas storage is the amount of drawers you have to store projects to work on later. The larger the worktop surface, the more projects you can work on at once. The bigger the drawers, the more you can save for later. In addition to supporting Apple Intelligence, more RAM is ideal for people who plan to work in multiple apps at once. And the more demanding each program is, the more RAM will be required. Extra memory can also come in handy if you’re the type who likes to have infinite numbers of tabs open on your browser. If your daily workflow doesn’t involve simultaneously using a vast number of memory-intensive programs, you can save yourself money and buy the RAM configuration that you’re most likely to actually use. For a long time, Apple continued to offer MacBooks with just 8GB of RAM, and we recommended upgrading to at least 16GB of RAM. With this being the standard today, grabbing a base model should be fine for most non-pro-level users. One thing to note is that, unlike most PCs, the RAM in a MacBook is not user-upgradable since it’s tied into the system-on-a-chip. If you think you might end up needing more memory, you should go for the spec upgrade up front. Storage capacity (SSD) Storage options range from 256GB of SSD for the base-model MacBook Air and 8TB of storage for the MacBook Pros with the M4 Max chip. If you want to rotate between a long roster of game titles or keep lots of high-res videos on hand, you’ll want more storage. If you’re mostly working with browser- and cloud-based applications, you can get away with a smaller-capacity configuration. That said, we recommend springing for 512GB of storage or more, if it’s within your budget. You’ll quickly feel the limits of a 256GB machine as it ages since the operating system alone takes up a good portion of that space. Having 1TB will feel even roomier and allow for more data storage over the life of your laptop. When Apple announced the iPhone 15, the company also announced new iCloud+ storage storage plans, with subscriptions that allow up to 12TB of storage shared among your iOS and MacOS devices. You could also transfer files to an external storage device. But if you don’t want to pay for a monthly subscription and prefer the convenience of having immediate access to your files, it’s best to get the highest amount of storage space your budget allows for at the outset. Screen size The MacBook Air comes in 13- or 15-inch sizes. Pro models have either 14- or 16-inch screens. A two-inch delta may not seem like much but, as Engadget’s Nathan Ingraham noted when he reviewed the then-new 15-inch M2-powered MacBook Air, a larger screen \"makes a surprising difference.” That’s especially true if you plan to use your laptop as an all-day productivity machine and won’t be using an external monitor. More space means you can more clearly view side-by-side windows and have a more immersive experience when watching shows or gaming. But screen size is one of the main factors influencing weight. The 13-inch MacBook Air M4 weighs 2.7 pounds, whereas the top-end 16-inch MacBook Pro with the Max chip weighs 4.7 pounds. If you plan to travel a lot or swap your work locations regularly, a smaller screen will make life easier in the long run. All MacBooks feature IPS LCD panels (in-plane switching, liquid crystal display), which Apple markets as Retina displays. The MacBook Air M4 has a Liquid Retina display and the Pro models have Liquid Retina XDR displays. “Liquid” refers to the way the lighted portion of the display “flows” within the contours of the screen, filling the rounded corners and curving around the camera notch. “XDR” is what Apple calls HDR (high dynamic range). You also get the option of a standard or nano-texture display on the MacBook Pro. The glass, which reduces glare and is also available on the Studio Display, iMac and iPad Pro, comes with a $150 price increase, but if you really don’t like reflections on your screen, it could be worth it. Compared to most other laptops, MacBook displays are notably bright, sharp and lush. But one feature worth pointing out is another Apple marketing term: ProMotion. It’s the company’s term to describe a screen with a higher, 120Hz refresh rate, which results in smoother scrolling and more fluid-looking graphics. Only MacBook Pros offer ProMotion; the Air maxes out at 60Hz, which is perfectly fine for everyday browsing and typical workdays. But if you want buttery-smooth motion from your display, you’ll have to shell out more money for an upgrade. Operating systems Software considerations won’t make much of a difference when deciding between MacBook models — all come with macOS installed. But if you’re switching from, say, a Windows PC, the operating system may be something to factor into your decision — though it’s probably less of an issue than it once was. Now that so much of the work we do on our computers is browser- and cloud-based, the learning curve between the two platforms isn’t as steep. Apps and programs like Gmail perform similarly regardless of what computer you’re using. Apple machines have historically had more limited support of AAA gaming titles, but even that is changing with more AAA games and better graphics coming to Macs. As for macOS, it’s getting better too. With macOS Tahoe 26, the Spotlight function is more advanced, making it easier to find apps and perform tasks straight from your keyboard. The software also implements Apple's unifying Liquid Glass design for a modern look that looks consistent across iOS and iPad devices. New enhanced iPhone continuity features also make MacBooks and the handset work better together. A revamped Shortcuts app is more powerful as well, giving users custom automations that leverage Apple Intelligence (the company’s own AI). Price When Apple announced the MacBook Air M4, it also delivered a bit of refreshing news: The latest model now starts $100 cheaper than the previous generation. So now, the least expensive MacBook is the 13-inch, M4-powered Air with 16GB of RAM and 256GB of storage for $999. Alternatively, you can spend up to $7,349 for the 16-inch MacBook Pro M4 Max with the nano-texture glass, 128GB of RAM and 8TB of storage. Chip type, screen size, memory and storage capacity all influence the final price, which is why guides like this can help you determine just what you need (and what you don’t) so you can get the most cost-effective machine for you. AppleCare is another cost to consider. The extended warranty plan from Apple covers repairs from accidents and offers free battery replacement and starts at $3.50 per month or $35 per year for MacBooks. We recommend the MacBook Air M4 for most people, and thanks to that $100 price cut, it’s also a good budget option. If you want something even cheaper, we recommend looking at refurbished M-series models from Apple. We think the 14-inch M5 or 16-inch M4 MacBook Pros are best for professionals. If you have extra money to spare once you’ve picked your machine, we recommend upgrading to at least 512GB of storage and 32GB of RAM to make your machine as future-proof as possible. Of course, if you're just after Apple’s silicon and want the cheapest route to get it, you might consider the M4 Mac mini, which starts at $599 (though you'll have to supply the screen, mouse and keyboard). Best MacBooks spec comparison chart Product Superlative Tested configuration Tested battery life Rated battery life Apple MacBook Air M4 (13-inch) Best MacBook overall Apple M4, 16GB RAM, 256GB SSD 18.25 hours Up to 18 hours Apple MacBook Pro M5 (14-inch) Best MacBook for creatives Apple M5, 32GB RAM, 512GB SSD 34.5 hours Up to 24 hours Best MacBook FAQs What's the difference between MacBook Air and Pro? The MacBook Air comes with the M4 chip. The 14-inch, base-model Pro comes with the M5 chip. MacBook Pro models have the option of more powerful M4 Pro or M4 Max chips. The Pro models have higher resolution screens with a higher peak brightness that supports up to 120Hz adaptive refresh rates and XDR (extreme dynamic range). The battery life on most Pro models is longer than on the Air models as well. Pro models also have more ports and more speakers. In short, the MacBook Air is aimed at everyday users looking for good productivity and entertainment capabilities, while Pro models are aimed at professionals who need a high-performance computer. What's the difference between macOS and Windows? MacOS is the operating system developed by Apple and used in all of its desktop and laptop computers. It can only be found in hardware made by Apple including MacBooks and iMacs. Microsoft’s Windows operating system can be found in the company’s own Surface laptops as well as computers made by a wide array of manufacturers, like Acer, Asus, Dell and Razer.This article originally appeared on Engadget at https://www.engadget.com/computing/laptops/best-macbook-140032524.html?src=rss",
          "content": "Picking the best MacBook may seem like an easy decision. After all, Apple just makes two models: the MacBook Air and the MacBook Pro. But the available variations within those categories — screen size, chip type, capacity and more — deserve some consideration. You also may wonder what the real-world differences are between models and who they’re best for. To make things even more interesting, Apple keeps announcing new chips. The latest, the M5 came out October 15, and is now found in the base model, 14-inch MacBook Pro (as well as the iPad Pro and the Vision Pro). This guide breaks down Apple’s terminology, as well as all which upgrades make the most sense so you can get the best MacBook for what you want to do. Table of contents Best MacBooks for 2025 What about budget MacBooks? Factors to consider when buying a MacBook MacBooks specs comparison chart Best MacBook FAQs Best MacBooks for 2025 What about budget MacBooks? Historically, Apple kept the previous year’s MacBook Air in its lineup as a sort of budget option. But the company took a different approach with the release of the M4 MacBook Air. Instead of continuing to sell the older model, Apple discontinued the M3 Air and gave its newest computer a $100 price cut. Now, if you can even find a brand new M3 MacBook Air (typically from retailers like Amazon or B&H), it’s often more expensive than the M4 version. During sales like Amazon Prime Day, we’ve seen the newest M4 Air go for as little as $799. That effectively makes our overall pick a budget pick as well. Of course, $800 isn’t exactly a small investment either for college students or others on a budget. Especially when you can find some decent PCs for under $500. If you’re looking to save even more on a MacBook, we recommend checking out refurbished options directly from Apple, or even third party sellers like BackMarket. There are a few guidelines to keep in mind, which we go over in our refurbished guide, but mainly, you’ll want to shop from a reputable source that has a stated process and offers at least a year-long warranty. Using your old gear as a trade-in will bring down your final cost as well. Factors to consider when buying a MacBook Compared to PCs, Apple computers tend to have more streamlined specifications. The company has long been known for this simplicity, and the M-series “system-on-a-chip” condenses things even further. Prior to the M1 chip, Apple used Intel chips in its laptop and desktop computers. The M2 and M3 generations followed that first chip and currently, MacBooks come equipped with M4 and M5-series chips. You’ll find the standard M4 processor in the Air. The base-model 14-inch Pro now comes with either the latest M5 chip. Other Pro configurations have the M4 Max or the M4 Pro (currently there is no M4 Ultra chip, as there was with the M3 series in the Mac Studio). All M-series chips combine, among other technologies, the CPU, graphics card and unified memory (RAM). Apple’s Neural Engine is included too, which is a specialized group of processor cores that handles machine learning tasks such as image analysis and voice recognition. While a unified chip means you have fewer decisions to make when picking a MacBook, there are still a few factors to consider, including specs like the number of CPU cores, amount of RAM, storage capacity, screen size, and, obviously, price. The finish color may be a minor consideration, but it's worth pointing out that the Pro comes in just two colors (Silver or Space Black) but the Air comes in four hues (Midnight, Starlight, Sky Blue and Silver). CPU cores The lowest-specced chip in a current-lineup MacBook is the standard M4 chip, which is found in all models of the MacBook Air. That chip houses a 10-core CPU and either an 8- or 10-core GPU. The base-model MacBook Pro uses the latest M5 chip, but only on the 14-inch model. The upgraded versions of that laptop use the M4 Pro or M4 Max chips (which are a step up from their predecessors, the M3, M3 Pro and M3 Max chips). The M4 Max is the burliest chip and built with either a 14- or 16-core CPU and a 32- or 40-core GPU. Cores are, in essence, smaller processing units that can handle different tasks simultaneously. Having more of them translates to the computer being able to run multiple programs and applications at once, while also smoothly processing demanding tasks like video and photo editing and high-level gaming. In short, more cores allow for more advanced computing and better performance. But if your processing power needs fall below professional-level gaming and cinematic video and audio editing, getting the highest number of cores is likely overkill — and after all, more cores equals higher cost and more power usage. Photo by Devindra Hardawar/Engadget RAM Your options for RAM (or unified memory) varies, but when Apple switched to the M4 chip for the MacBook Air, the lowest amount of RAM you can get was bumped to 16GB. That’s a necessary jump to accommodate the tech world’s favorite feature of the moment: AI or, in this case, Apple Intelligence (still AI, but Cupertino’s version). The M4 Pro chip has 24 or 48GB memory options, while the M4 Max chip supports 48, 64 or a whopping 128GB of RAM. The M5 chip in the base-model MacBook Pro comes with a minimum of 16GB and can be configured to a maximum of 32GB of RAM.. You’ve likely heard the analogy comparing memory to the amount of workspace available on a literal desktop surface, whereas storage is the amount of drawers you have to store projects to work on later. The larger the worktop surface, the more projects you can work on at once. The bigger the drawers, the more you can save for later. In addition to supporting Apple Intelligence, more RAM is ideal for people who plan to work in multiple apps at once. And the more demanding each program is, the more RAM will be required. Extra memory can also come in handy if you’re the type who likes to have infinite numbers of tabs open on your browser. If your daily workflow doesn’t involve simultaneously using a vast number of memory-intensive programs, you can save yourself money and buy the RAM configuration that you’re most likely to actually use. For a long time, Apple continued to offer MacBooks with just 8GB of RAM, and we recommended upgrading to at least 16GB of RAM. With this being the standard today, grabbing a base model should be fine for most non-pro-level users. One thing to note is that, unlike most PCs, the RAM in a MacBook is not user-upgradable since it’s tied into the system-on-a-chip. If you think you might end up needing more memory, you should go for the spec upgrade up front. Storage capacity (SSD) Storage options range from 256GB of SSD for the base-model MacBook Air and 8TB of storage for the MacBook Pros with the M4 Max chip. If you want to rotate between a long roster of game titles or keep lots of high-res videos on hand, you’ll want more storage. If you’re mostly working with browser- and cloud-based applications, you can get away with a smaller-capacity configuration. That said, we recommend springing for 512GB of storage or more, if it’s within your budget. You’ll quickly feel the limits of a 256GB machine as it ages since the operating system alone takes up a good portion of that space. Having 1TB will feel even roomier and allow for more data storage over the life of your laptop. When Apple announced the iPhone 15, the company also announced new iCloud+ storage storage plans, with subscriptions that allow up to 12TB of storage shared among your iOS and MacOS devices. You could also transfer files to an external storage device. But if you don’t want to pay for a monthly subscription and prefer the convenience of having immediate access to your files, it’s best to get the highest amount of storage space your budget allows for at the outset. Screen size The MacBook Air comes in 13- or 15-inch sizes. Pro models have either 14- or 16-inch screens. A two-inch delta may not seem like much but, as Engadget’s Nathan Ingraham noted when he reviewed the then-new 15-inch M2-powered MacBook Air, a larger screen \"makes a surprising difference.” That’s especially true if you plan to use your laptop as an all-day productivity machine and won’t be using an external monitor. More space means you can more clearly view side-by-side windows and have a more immersive experience when watching shows or gaming. But screen size is one of the main factors influencing weight. The 13-inch MacBook Air M4 weighs 2.7 pounds, whereas the top-end 16-inch MacBook Pro with the Max chip weighs 4.7 pounds. If you plan to travel a lot or swap your work locations regularly, a smaller screen will make life easier in the long run. All MacBooks feature IPS LCD panels (in-plane switching, liquid crystal display), which Apple markets as Retina displays. The MacBook Air M4 has a Liquid Retina display and the Pro models have Liquid Retina XDR displays. “Liquid” refers to the way the lighted portion of the display “flows” within the contours of the screen, filling the rounded corners and curving around the camera notch. “XDR” is what Apple calls HDR (high dynamic range). You also get the option of a standard or nano-texture display on the MacBook Pro. The glass, which reduces glare and is also available on the Studio Display, iMac and iPad Pro, comes with a $150 price increase, but if you really don’t like reflections on your screen, it could be worth it. Compared to most other laptops, MacBook displays are notably bright, sharp and lush. But one feature worth pointing out is another Apple marketing term: ProMotion. It’s the company’s term to describe a screen with a higher, 120Hz refresh rate, which results in smoother scrolling and more fluid-looking graphics. Only MacBook Pros offer ProMotion; the Air maxes out at 60Hz, which is perfectly fine for everyday browsing and typical workdays. But if you want buttery-smooth motion from your display, you’ll have to shell out more money for an upgrade. Operating systems Software considerations won’t make much of a difference when deciding between MacBook models — all come with macOS installed. But if you’re switching from, say, a Windows PC, the operating system may be something to factor into your decision — though it’s probably less of an issue than it once was. Now that so much of the work we do on our computers is browser- and cloud-based, the learning curve between the two platforms isn’t as steep. Apps and programs like Gmail perform similarly regardless of what computer you’re using. Apple machines have historically had more limited support of AAA gaming titles, but even that is changing with more AAA games and better graphics coming to Macs. As for macOS, it’s getting better too. With macOS Tahoe 26, the Spotlight function is more advanced, making it easier to find apps and perform tasks straight from your keyboard. The software also implements Apple's unifying Liquid Glass design for a modern look that looks consistent across iOS and iPad devices. New enhanced iPhone continuity features also make MacBooks and the handset work better together. A revamped Shortcuts app is more powerful as well, giving users custom automations that leverage Apple Intelligence (the company’s own AI). Price When Apple announced the MacBook Air M4, it also delivered a bit of refreshing news: The latest model now starts $100 cheaper than the previous generation. So now, the least expensive MacBook is the 13-inch, M4-powered Air with 16GB of RAM and 256GB of storage for $999. Alternatively, you can spend up to $7,349 for the 16-inch MacBook Pro M4 Max with the nano-texture glass, 128GB of RAM and 8TB of storage. Chip type, screen size, memory and storage capacity all influence the final price, which is why guides like this can help you determine just what you need (and what you don’t) so you can get the most cost-effective machine for you. AppleCare is another cost to consider. The extended warranty plan from Apple covers repairs from accidents and offers free battery replacement and starts at $3.50 per month or $35 per year for MacBooks. We recommend the MacBook Air M4 for most people, and thanks to that $100 price cut, it’s also a good budget option. If you want something even cheaper, we recommend looking at refurbished M-series models from Apple. We think the 14-inch M5 or 16-inch M4 MacBook Pros are best for professionals. If you have extra money to spare once you’ve picked your machine, we recommend upgrading to at least 512GB of storage and 32GB of RAM to make your machine as future-proof as possible. Of course, if you're just after Apple’s silicon and want the cheapest route to get it, you might consider the M4 Mac mini, which starts at $599 (though you'll have to supply the screen, mouse and keyboard). Best MacBooks spec comparison chart Product Superlative Tested configuration Tested battery life Rated battery life Apple MacBook Air M4 (13-inch) Best MacBook overall Apple M4, 16GB RAM, 256GB SSD 18.25 hours Up to 18 hours Apple MacBook Pro M5 (14-inch) Best MacBook for creatives Apple M5, 32GB RAM, 512GB SSD 34.5 hours Up to 24 hours Best MacBook FAQs What's the difference between MacBook Air and Pro? The MacBook Air comes with the M4 chip. The 14-inch, base-model Pro comes with the M5 chip. MacBook Pro models have the option of more powerful M4 Pro or M4 Max chips. The Pro models have higher resolution screens with a higher peak brightness that supports up to 120Hz adaptive refresh rates and XDR (extreme dynamic range). The battery life on most Pro models is longer than on the Air models as well. Pro models also have more ports and more speakers. In short, the MacBook Air is aimed at everyday users looking for good productivity and entertainment capabilities, while Pro models are aimed at professionals who need a high-performance computer. What's the difference between macOS and Windows? MacOS is the operating system developed by Apple and used in all of its desktop and laptop computers. It can only be found in hardware made by Apple including MacBooks and iMacs. Microsoft’s Windows operating system can be found in the company’s own Surface laptops as well as computers made by a wide array of manufacturers, like Acer, Asus, Dell and Razer.This article originally appeared on Engadget at https://www.engadget.com/computing/laptops/best-macbook-140032524.html?src=rss",
          "feed_position": 36,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2023-11/e536a1d0-7c1e-11ee-9e77-9ea8e142b078"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/microsoft-copilot-gets-12-big-updates-for-fall-including-new-ai-assistant",
          "published_at": "Thu, 23 Oct 2025 19:01:00 GMT",
          "title": "Microsoft Copilot gets 12 big updates for fall, including new AI assistant character Mico",
          "standfirst": "Microsoft today held a live announcement event online for its Copilot AI digital assistant, with Mustafa Suleyman, CEO of Microsoft&#x27;s AI division, and other presenters unveiling a new generation of features that deepen integration across Windows, Edge, and Microsoft 365, positioning the platform as a practical assistant for people during work and off-time, while allowing them to preserve control and safety of their data.The new Copilot 2025 Fall Update features also up the ante in terms of capabilities and the accessibility of generative AI assistance from Microsoft to users, so businesses relying on Microsoft products, and those who seek to offer complimentary or competing products, would do well to review them.Suleyman emphasized that the updates reflect a shift from hype to usefulness. “Technology should work in service of people, not the other way around,” he said. “Copilot is not just a product—it’s a promise that AI can be helpful, supportive, and deeply personal.”Intriguingly, the announcement also sought to shine a greater spotlight on Microsoft&#x27;s own homegrown AI models, as opposed to those of its partner and investment OpenAI, which previously powered the entire Copilot experience. Instead, Suleyman wrote today in a blog post: “At the foundation of it all is our strategy to put the best models to work for you – both those we build and those we don’t. Over the past few months, we have released in-house models like MAI-Voice-1, MAI-1-Preview and MAI-Vision-1, and are rapidly iterating.”12 Features That Redefine CopilotThe Fall Release consolidates Copilot’s identity around twelve key capabilities—each with potential to streamline organizational knowledge work, development, or support operations.Groups – Shared Copilot sessions where up to 32 participants can brainstorm, co-author, or plan simultaneously. For distributed teams, it effectively merges a meeting chat, task board, and generative workspace. Copilot maintains context, summarizes decisions, and tracks open actions. Imagine – A collaborative hub for creating and remixing AI-generated content. In an enterprise setting, Imagine enables rapid prototyping of visuals, marketing drafts, or training materials.Mico – A new character identity for Copilot that introduces expressive feedback and emotional expression in the form of a cute, amorphous blob. Echoing Microsoft’s historic character interfaces like Clippy (Office 97) or Cortana (2014), Mico serves as a unifying UX layer across modalities.Real Talk – A conversational mode that adapts to a user’s communication style and offers calibrated pushback — ending the sycophancy that some users have complained about with other AI models such as prior versions of OpenAI&#x27;s ChatGPT. For professionals, it allows Socratic problem-solving rather than passive answer generation, making Copilot more credible in technical collaboration.Memory & Personalization – Long-term contextual memory that lets Copilot recall key details—training plans, dates, goals—at the user’s direction.Connectors – Integration with OneDrive, Outlook, Gmail, Google Drive, and Google Calendar for natural-language search across accounts.Proactive Actions (Preview) – Context-based prompts and next-step suggestions derived from recent activity.Copilot for Health – Health information grounded in credible medical sources such as Harvard Health, with tools allowing users to locate and compare doctors.Learn Live – A Socratic, voice-driven tutoring experience using questions, visuals, and whiteboards.Copilot Mode in Edge – Converts Microsoft Edge into an “AI browser” that summarizes, compares, and executes web actions by voice.Copilot on Windows – Deep integration across Windows 11 PCs with “Hey Copilot” activation, Copilot Vision guidance, and quick access to files and apps.Copilot Pages and Copilot Search – A collaborative file canvas plus a unified search experience combining AI-generated, cited answers with standard web results.The Fall Release is immediately available in the United States, with rollout to the UK, Canada, and other markets in progress. Some functions—such as Groups, Journeys, and Copilot for Health—remain U.S.-only for now. Proactive Actions requires a Microsoft 365 Personal, Family, or Premium subscription.Together these updates illustrate Microsoft’s pivot from static productivity suites to contextual AI infrastructure, with the Copilot brand acting as the connective tissue across user roles.From Clippy to Mico: The Return of a Guided InterfaceOne of the most notable introductions is Mico, a small animated companion that is available within Copilot’s voice-enabled experiences, including the Copilot app on Windows, iOS, and Android, as well as in Study Mode and other conversational contexts. It serves as an optional visual companion that appears during interactive or voice-based sessions, rather than across all Copilot interfaces.Mico listens, reacts with expressions, and changes color to reflect tone and emotion — bringing a visual warmth to an AI assistant experience that has traditionally been text-heavy.Mico’s design recalls earlier eras of Microsoft’s history with character-based assistants. In the mid-1990s, Microsoft experimented with Microsoft Bob (1995), a software interface that used cartoon characters like a dog named Rover to guide users through everyday computing tasks. While innovative for its time, Bob was discontinued after a year due to performance and usability issues.A few years later came Clippy, the Office Assistant introduced in Microsoft Office 97. Officially known as “Clippit,” the animated paperclip would pop up to offer help and tips within Word and other Office applications. Clippy became widely recognized—sometimes humorously so—for interrupting users with unsolicited advice. Microsoft retired Clippy from Office in 2001, though the character remains a nostalgic symbol of early AI-driven assistance.More recently, Cortana, launched in 2014 as Microsoft’s digital voice assistant for Windows and mobile devices, aimed to provide natural-language interaction similar to Apple’s Siri or Amazon’s Alexa. Despite positive early reception, Cortana’s role diminished as Microsoft refocused on enterprise productivity and AI integration. The service was officially discontinued on Windows in 2023.Mico, by contrast, represents a modern reimagining of that tradition—combining the personality of early assistants with the intelligence and adaptability of contemporary AI models. Where Clippy offered canned responses, Mico listens, learns, and reflects a user’s mood in real time. The goal, as Suleyman framed it, is to create an AI that feels “helpful, supportive, and deeply personal.”Groups Are Microsoft&#x27;s Version of Claude and ChatGPT ProjectsDuring Microsoft’s launch video, product researcher Wendy described Groups as a transformative shift: “You can finally bring in other people directly to the conversation that you’re having with Copilot,” she said. “It’s the only place you can do this.”Up to 32 users can join a shared Copilot session, brainstorming, editing, or planning together while the AI manages logistics such as summarizing discussion threads, tallying votes, and splitting tasks. Participants can enter or exit sessions using a link, maintaining full visibility into ongoing work.Instead of a single user prompting an AI and later sharing results, Groups lets teams prompt and iterate together in one unified conversation. In some ways, it&#x27;s an answer to Anthropic’s Claude Projects and OpenAI’s ChatGPT Projects, both launched within the last year as tools to centralize team workspaces and shared AI context. Where Claude and ChatGPT Projects allow users to aggregate files, prompts, and conversations into a single container, Groups extends that model into real-time, multi-participant collaboration. Unlike Anthropic’s and OpenAI’s implementations, Groups is deeply embedded within Microsoft’s productivity environment. Like other Copilot experiences connected to Outlook and OneDrive, Groups operates within Microsoft’s enterprise identity framework, governed by Microsoft 365 and Entra ID (formerly Azure Active Directory) authentication and consent modelsThis means conversations, shared artifacts, and generated summaries are governed under the same compliance policies that already protect Outlook, Teams, and SharePoint data.Hours after the unveiling, OpenAI hit back against its own investor in the escalating AI competition between the \"frenemies\" by expanding its Shared Projects feature beyond its current Enterprise, Team, and Edu subscriber availability to users of its free, Plus, and Pro subscription tiers. Operational Impact for AI and Data TeamsMemory & Personalization and Connectors effectively extend a lightweight orchestration layer across Microsoft’s ecosystem. Instead of building separate context-stores or retrieval APIs, teams can leverage Copilot’s secure integration with OneDrive or SharePoint as a governed data backbone. A presenter explained that Copilot’s memory “naturally picks up on important details and remembers them long after you’ve had the conversation,” yet remains editable. For data engineers, Copilot Search and Connectors reduce friction in data discovery across multiple systems. Natural-language retrieval from internal and cloud repositories may lower the cost of knowledge management initiatives by consolidating search endpoints.For security directors, Copilot’s explicit consent requirements and on/off toggles in Edge and Windows help maintain data residency standards. The company reiterated during the livestream that Copilot “acts only with user permission and within organizational privacy controls.”Copilot Mode in Edge: The AI Browser for Research and AutomationCopilot Mode in Edge stands out for offering AI-assisted information workflows. The browser can now parse open tabs, summarize differences, and perform transactional steps.“Historically, browsers have been static—just endless clicking and tab-hopping,” said a presenter during Microsoft’s livestream. “We asked not how browsers should work, but how people work.”In practice, an analyst could prompt Edge to compare supplier documentation, extract structured data, and auto-fill procurement forms—all with consistent citation. Voice-only navigation enables accessibility and multitasking, while Journeys, a companion feature, organizes browsing sessions into storylines for later review.Copilot on Windows: The Operating System as an AI SurfaceIn Windows 11, Copilot now functions as an embedded assistant. With the wake-word “Hey Copilot,” users can initiate context-aware commands without leaving the desktop—drafting documentation, troubleshooting configuration issues, or summarizing system logs.A presenter described it as a “super assistant plugged into all your files and applications.” For enterprises standardizing on Windows 11, this positions Copilot as a native productivity layer rather than an add-on, reducing training friction and promoting secure, on-device reasoning.Copilot Vision, now in early deployment, adds visual comprehension. IT staff can capture a screen region and ask Copilot to interpret error messages, explain configuration options, or generate support tickets automatically.Combined with Copilot Pages, which supports up to twenty concurrent file uploads, this enables more efficient cross-document analysis for audits, RFPs, or code reviews.Leveraging MAI Models for Multimodal WorkflowsAt the foundation of these capabilities are Microsoft’s proprietary MAI-Voice-1, MAI-1 Preview, and MAI-Vision-1 models—trained in-house to handle text, voice, and visual inputs cohesively.For engineering teams managing LLM orchestration, this architecture introduces several potential efficiencies:Unified multimodal reasoning – Reduces the need for separate ASR (speech-to-text) and image-parsing services.Fine-tuning continuity – Because Microsoft owns the model stack, updates propagate across Copilot experiences without re-integration.Predictable latency and governance – In-house hosting under Azure compliance frameworks simplifies security certification for regulated industries.A presenter described the new stack as “the foundation for immersive, creative, and dynamic experiences that still respect enterprise boundaries.”A Strategic Pivot Toward Contextual AIFor years, Microsoft positioned Copilot primarily as a productivity companion. With the Fall 2025 release, it crosses into operational AI infrastructure—a set of extensible services for reasoning over data and processes.Suleyman described this evolution succinctly: “Judge an AI by how much it elevates human potential, not just by its own smarts.” For CIOs and technical leads, the elevation comes from efficiency and interoperability.Copilot now acts as:A connective interface linking files, communications, and cloud data.A reasoning agent capable of understanding context across sessions and modalities.A secure orchestration layer compatible with Microsoft’s compliance and identity framework.Suleyman’s insistence that “technology should work in service of people” now extends to organizations as well: technology that serves teams, not workloads; systems that adapt to enterprise context rather than demand it.",
          "content": "Microsoft today held a live announcement event online for its Copilot AI digital assistant, with Mustafa Suleyman, CEO of Microsoft&#x27;s AI division, and other presenters unveiling a new generation of features that deepen integration across Windows, Edge, and Microsoft 365, positioning the platform as a practical assistant for people during work and off-time, while allowing them to preserve control and safety of their data.The new Copilot 2025 Fall Update features also up the ante in terms of capabilities and the accessibility of generative AI assistance from Microsoft to users, so businesses relying on Microsoft products, and those who seek to offer complimentary or competing products, would do well to review them.Suleyman emphasized that the updates reflect a shift from hype to usefulness. “Technology should work in service of people, not the other way around,” he said. “Copilot is not just a product—it’s a promise that AI can be helpful, supportive, and deeply personal.”Intriguingly, the announcement also sought to shine a greater spotlight on Microsoft&#x27;s own homegrown AI models, as opposed to those of its partner and investment OpenAI, which previously powered the entire Copilot experience. Instead, Suleyman wrote today in a blog post: “At the foundation of it all is our strategy to put the best models to work for you – both those we build and those we don’t. Over the past few months, we have released in-house models like MAI-Voice-1, MAI-1-Preview and MAI-Vision-1, and are rapidly iterating.”12 Features That Redefine CopilotThe Fall Release consolidates Copilot’s identity around twelve key capabilities—each with potential to streamline organizational knowledge work, development, or support operations.Groups – Shared Copilot sessions where up to 32 participants can brainstorm, co-author, or plan simultaneously. For distributed teams, it effectively merges a meeting chat, task board, and generative workspace. Copilot maintains context, summarizes decisions, and tracks open actions. Imagine – A collaborative hub for creating and remixing AI-generated content. In an enterprise setting, Imagine enables rapid prototyping of visuals, marketing drafts, or training materials.Mico – A new character identity for Copilot that introduces expressive feedback and emotional expression in the form of a cute, amorphous blob. Echoing Microsoft’s historic character interfaces like Clippy (Office 97) or Cortana (2014), Mico serves as a unifying UX layer across modalities.Real Talk – A conversational mode that adapts to a user’s communication style and offers calibrated pushback — ending the sycophancy that some users have complained about with other AI models such as prior versions of OpenAI&#x27;s ChatGPT. For professionals, it allows Socratic problem-solving rather than passive answer generation, making Copilot more credible in technical collaboration.Memory & Personalization – Long-term contextual memory that lets Copilot recall key details—training plans, dates, goals—at the user’s direction.Connectors – Integration with OneDrive, Outlook, Gmail, Google Drive, and Google Calendar for natural-language search across accounts.Proactive Actions (Preview) – Context-based prompts and next-step suggestions derived from recent activity.Copilot for Health – Health information grounded in credible medical sources such as Harvard Health, with tools allowing users to locate and compare doctors.Learn Live – A Socratic, voice-driven tutoring experience using questions, visuals, and whiteboards.Copilot Mode in Edge – Converts Microsoft Edge into an “AI browser” that summarizes, compares, and executes web actions by voice.Copilot on Windows – Deep integration across Windows 11 PCs with “Hey Copilot” activation, Copilot Vision guidance, and quick access to files and apps.Copilot Pages and Copilot Search – A collaborative file canvas plus a unified search experience combining AI-generated, cited answers with standard web results.The Fall Release is immediately available in the United States, with rollout to the UK, Canada, and other markets in progress. Some functions—such as Groups, Journeys, and Copilot for Health—remain U.S.-only for now. Proactive Actions requires a Microsoft 365 Personal, Family, or Premium subscription.Together these updates illustrate Microsoft’s pivot from static productivity suites to contextual AI infrastructure, with the Copilot brand acting as the connective tissue across user roles.From Clippy to Mico: The Return of a Guided InterfaceOne of the most notable introductions is Mico, a small animated companion that is available within Copilot’s voice-enabled experiences, including the Copilot app on Windows, iOS, and Android, as well as in Study Mode and other conversational contexts. It serves as an optional visual companion that appears during interactive or voice-based sessions, rather than across all Copilot interfaces.Mico listens, reacts with expressions, and changes color to reflect tone and emotion — bringing a visual warmth to an AI assistant experience that has traditionally been text-heavy.Mico’s design recalls earlier eras of Microsoft’s history with character-based assistants. In the mid-1990s, Microsoft experimented with Microsoft Bob (1995), a software interface that used cartoon characters like a dog named Rover to guide users through everyday computing tasks. While innovative for its time, Bob was discontinued after a year due to performance and usability issues.A few years later came Clippy, the Office Assistant introduced in Microsoft Office 97. Officially known as “Clippit,” the animated paperclip would pop up to offer help and tips within Word and other Office applications. Clippy became widely recognized—sometimes humorously so—for interrupting users with unsolicited advice. Microsoft retired Clippy from Office in 2001, though the character remains a nostalgic symbol of early AI-driven assistance.More recently, Cortana, launched in 2014 as Microsoft’s digital voice assistant for Windows and mobile devices, aimed to provide natural-language interaction similar to Apple’s Siri or Amazon’s Alexa. Despite positive early reception, Cortana’s role diminished as Microsoft refocused on enterprise productivity and AI integration. The service was officially discontinued on Windows in 2023.Mico, by contrast, represents a modern reimagining of that tradition—combining the personality of early assistants with the intelligence and adaptability of contemporary AI models. Where Clippy offered canned responses, Mico listens, learns, and reflects a user’s mood in real time. The goal, as Suleyman framed it, is to create an AI that feels “helpful, supportive, and deeply personal.”Groups Are Microsoft&#x27;s Version of Claude and ChatGPT ProjectsDuring Microsoft’s launch video, product researcher Wendy described Groups as a transformative shift: “You can finally bring in other people directly to the conversation that you’re having with Copilot,” she said. “It’s the only place you can do this.”Up to 32 users can join a shared Copilot session, brainstorming, editing, or planning together while the AI manages logistics such as summarizing discussion threads, tallying votes, and splitting tasks. Participants can enter or exit sessions using a link, maintaining full visibility into ongoing work.Instead of a single user prompting an AI and later sharing results, Groups lets teams prompt and iterate together in one unified conversation. In some ways, it&#x27;s an answer to Anthropic’s Claude Projects and OpenAI’s ChatGPT Projects, both launched within the last year as tools to centralize team workspaces and shared AI context. Where Claude and ChatGPT Projects allow users to aggregate files, prompts, and conversations into a single container, Groups extends that model into real-time, multi-participant collaboration. Unlike Anthropic’s and OpenAI’s implementations, Groups is deeply embedded within Microsoft’s productivity environment. Like other Copilot experiences connected to Outlook and OneDrive, Groups operates within Microsoft’s enterprise identity framework, governed by Microsoft 365 and Entra ID (formerly Azure Active Directory) authentication and consent modelsThis means conversations, shared artifacts, and generated summaries are governed under the same compliance policies that already protect Outlook, Teams, and SharePoint data.Hours after the unveiling, OpenAI hit back against its own investor in the escalating AI competition between the \"frenemies\" by expanding its Shared Projects feature beyond its current Enterprise, Team, and Edu subscriber availability to users of its free, Plus, and Pro subscription tiers. Operational Impact for AI and Data TeamsMemory & Personalization and Connectors effectively extend a lightweight orchestration layer across Microsoft’s ecosystem. Instead of building separate context-stores or retrieval APIs, teams can leverage Copilot’s secure integration with OneDrive or SharePoint as a governed data backbone. A presenter explained that Copilot’s memory “naturally picks up on important details and remembers them long after you’ve had the conversation,” yet remains editable. For data engineers, Copilot Search and Connectors reduce friction in data discovery across multiple systems. Natural-language retrieval from internal and cloud repositories may lower the cost of knowledge management initiatives by consolidating search endpoints.For security directors, Copilot’s explicit consent requirements and on/off toggles in Edge and Windows help maintain data residency standards. The company reiterated during the livestream that Copilot “acts only with user permission and within organizational privacy controls.”Copilot Mode in Edge: The AI Browser for Research and AutomationCopilot Mode in Edge stands out for offering AI-assisted information workflows. The browser can now parse open tabs, summarize differences, and perform transactional steps.“Historically, browsers have been static—just endless clicking and tab-hopping,” said a presenter during Microsoft’s livestream. “We asked not how browsers should work, but how people work.”In practice, an analyst could prompt Edge to compare supplier documentation, extract structured data, and auto-fill procurement forms—all with consistent citation. Voice-only navigation enables accessibility and multitasking, while Journeys, a companion feature, organizes browsing sessions into storylines for later review.Copilot on Windows: The Operating System as an AI SurfaceIn Windows 11, Copilot now functions as an embedded assistant. With the wake-word “Hey Copilot,” users can initiate context-aware commands without leaving the desktop—drafting documentation, troubleshooting configuration issues, or summarizing system logs.A presenter described it as a “super assistant plugged into all your files and applications.” For enterprises standardizing on Windows 11, this positions Copilot as a native productivity layer rather than an add-on, reducing training friction and promoting secure, on-device reasoning.Copilot Vision, now in early deployment, adds visual comprehension. IT staff can capture a screen region and ask Copilot to interpret error messages, explain configuration options, or generate support tickets automatically.Combined with Copilot Pages, which supports up to twenty concurrent file uploads, this enables more efficient cross-document analysis for audits, RFPs, or code reviews.Leveraging MAI Models for Multimodal WorkflowsAt the foundation of these capabilities are Microsoft’s proprietary MAI-Voice-1, MAI-1 Preview, and MAI-Vision-1 models—trained in-house to handle text, voice, and visual inputs cohesively.For engineering teams managing LLM orchestration, this architecture introduces several potential efficiencies:Unified multimodal reasoning – Reduces the need for separate ASR (speech-to-text) and image-parsing services.Fine-tuning continuity – Because Microsoft owns the model stack, updates propagate across Copilot experiences without re-integration.Predictable latency and governance – In-house hosting under Azure compliance frameworks simplifies security certification for regulated industries.A presenter described the new stack as “the foundation for immersive, creative, and dynamic experiences that still respect enterprise boundaries.”A Strategic Pivot Toward Contextual AIFor years, Microsoft positioned Copilot primarily as a productivity companion. With the Fall 2025 release, it crosses into operational AI infrastructure—a set of extensible services for reasoning over data and processes.Suleyman described this evolution succinctly: “Judge an AI by how much it elevates human potential, not just by its own smarts.” For CIOs and technical leads, the elevation comes from efficiency and interoperability.Copilot now acts as:A connective interface linking files, communications, and cloud data.A reasoning agent capable of understanding context across sessions and modalities.A secure orchestration layer compatible with Microsoft’s compliance and identity framework.Suleyman’s insistence that “technology should work in service of people” now extends to organizations as well: technology that serves teams, not workloads; systems that adapt to enterprise context rather than demand it.",
          "feed_position": 4,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/1Y044YCqXidwgzt8iDL4XH/2a45ab0dfe64db4d9371db86b1d0e5d2/cfr0z3n_flat_2D_illustration_mod_colorful_playful_whimsical_sty_715bb078-8762-43bc-93ec-ce95bb5d570d.png?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/ai-is-tearing-companies-apart-writer-ai-ceo-slams-fortune-500-leaders-for",
          "published_at": "Thu, 23 Oct 2025 13:02:00 GMT",
          "title": "‘AI is tearing companies apart’: Writer AI CEO slams Fortune 500 leaders for mismanaging tech",
          "standfirst": "May Habib, co-founder and CEO of Writer AI, delivered one of the bluntest assessments of corporate AI failures at the TED AI conference on Tuesday, revealing that nearly half of Fortune 500 executives believe artificial intelligence is actively damaging their organizations — and placing the blame squarely on leadership&#x27;s shoulders.The problem, according to Habib, isn&#x27;t the technology. It&#x27;s that business leaders are making a category error, treating AI transformation like previous technology rollouts and delegating it to IT departments. This approach, she warned, has led to \"billions of dollars spent on AI initiatives that are going nowhere.\"\"Earlier this year, we did a survey of 800 Fortune 500 C-suite executives,\" Habib told the audience of Silicon Valley executives and investors. \"42% of them said AI is tearing their company apart.\"The diagnosis challenges conventional wisdom about how enterprises should approach AI adoption. While most major companies have stood up AI task forces, appointed chief AI officers, or expanded IT budgets, Habib argues these moves reflect a fundamental misunderstanding of what AI represents: not another software tool, but a wholesale reorganization of how work gets done.\"There is something leaders are missing when they compare AI to just another tech tool,\" Habib said. \"This is not like giving accountants calculators or bankers Excel or designers Photoshop.\"Why the &#x27;old playbook&#x27; of delegating to IT departments is failing companiesHabib, whose company has spent five years building AI systems for Fortune 500 companies and logged two million miles visiting customer sites, said the pattern is consistent: \"When generative AI started showing up, we turned to the old playbook. We turned to IT and said, &#x27;Go figure this out.&#x27;\"That approach fails, she argued, because AI fundamentally changes the economics and organization of work itself. \"For 100 years, enterprises have been built around the idea that execution is expensive and hard,\" Habib said. \"The enterprise built complex org charts, complex processes, all to manage people doing stuff.\"AI inverts that model. \"Execution is going from scarce and expensive to programmatic, on-demand and abundant,\" she said. In this new paradigm, the bottleneck shifts from execution capacity to strategic design — a shift that requires business leaders, not IT departments, to drive transformation.\"With AI technology, it can no longer be centralized. It&#x27;s in every workflow, every business,\" Habib said. \"It is now the most important part of a business leader&#x27;s job. It cannot be delegated.\"The statement represents a direct challenge to how most large organizations have structured their AI initiatives, with centralized centers of excellence, dedicated AI teams, or IT-led implementations that business units are expected to adopt.A generational power shift is happening based on who understands AI workflow designHabib framed the shift in dramatic terms: \"A generational transfer of power is happening right now. It&#x27;s not about your age or how long you&#x27;ve been at a company. The generational transfer of power is about the nature of leadership itself.\"Traditional leadership, she argued, has been defined by the ability to manage complexity — big teams, big budgets, intricate processes. \"The identity of leaders at these companies, people like us, has been tied to old school power structures: control, hierarchy, how big our teams are, how big our budgets are. Our value is measured by the sheer amount of complexity we could manage,\" Habib said. \"Today we reward leaders for this. We promote leaders for this.\"AI makes that model obsolete. \"When I am able to 10x the output of my team or do things that could never be possible, work is no longer about the 1x,\" she said. \"Leadership is no longer about managing complex human execution.\"Instead, Habib outlined three fundamental shifts that define what she calls \"AI-first leaders\" — executives her company has worked with who have successfully deployed AI agents solving \"$100 million plus problems.\"The first shift: Taking a machete to enterprise complexityThe new leadership mandate, according to Habib, is \"taking a machete to the complexity that has calcified so many organizations.\" She pointed to the layers of friction that have accumulated in enterprises: \"Brilliant ideas dying in memos, the endless cycles of approvals, the death by 1,000 clicks, meetings about meetings — a death, by the way, that&#x27;s happening in 17 different browser tabs each for software that promises to be a single source of truth.\"Rather than accepting this complexity as inevitable, AI-first leaders redesign workflows from first principles. \"There are very few legacy systems that can&#x27;t be replaced in your organization, that won&#x27;t be replaced,\" Habib said. \"But they&#x27;re not going to be replaced by another monolithic piece of software. They can only be replaced by a business leader articulating business logic and getting that into an agentic system.\"She offered a concrete example: \"We have customers where it used to take them seven months to get a creative campaign — not even a product, a campaign. Now they can go from TikTok trend to digital shelf in 30 days. That is radical simplicity.\"The catch, she emphasized, is that CIOs can&#x27;t drive this transformation alone. \"Your CIO can&#x27;t help flatten your org chart. Only a business leader can look at workflows and say, &#x27;This part is necessary genius, this part is bureaucratic scar tissue that has to go.&#x27;\"The second shift: Managing the fear as career ladders disappearWhen AI handles execution, \"your humans are liberated to do what they&#x27;re amazing at: judgment, strategy, creativity,\" Habib explained. \"The old leadership playbook was about managing headcount. We managed people against revenue: one business development rep for every three account executives, one marketer for every five salespeople.\"But this liberation carries profound challenges that leaders must address directly. Habib acknowledged the elephant in the room that many executives avoid discussing: \"These changes are still frightening for people, even when it&#x27;s become unholy to talk about it.\" She&#x27;s witnessed the fear firsthand. \"It shows up as tears in an AI workshop when someone feels like their old skill set isn&#x27;t translated to the new.\"She introduced a term for a common form of resistance: \"productivity anchoring\" — when employees \"cling to the hard way of doing things because they feel productive, because their self-worth is tied to them, even when empirically AI can be better.\"The solution isn&#x27;t to look away. \"We have to design new pathways to impact, to show your people their value is not in executing a task. Their value is in orchestrating systems of execution, to ask the next great question,\" Habib said. She advocates replacing career \"ladders\" with \"lattices\" where \"people need to grow laterally, to expand sideways.\"She was candid about the disruption: \"The first rungs on our career ladders are indeed going away. I know because my company is automating them.\" But she insisted this creates opportunity for work that is \"more creative, more strategic, more driven by curiosity and impact — and I believe a lot more human than the jobs that they&#x27;re replacing.\"The third shift: When execution becomes free, ambition becomes the only bottleneckThe final shift is from optimization to creation. \"Before AI, we used to call it transformation when we took 12 steps and made them nine,\" Habib said. \"That&#x27;s optimizing the world as it is. We can now create a new world. That is the greenfield mindset.\"She challenged executives to identify assumptions their industries are built on that AI now disrupts. Writer&#x27;s customers, she said, are already seeing new categories of growth: treating every customer like their only customer, democratizing premium services to broader markets, and entering new markets at unprecedented speed because \"AI strips away the friction to access new channels.\"\"When execution is abundant, the only bottleneck is the scope of your own ambition,\" Habib declared.What this means for CIOs: Building the stadium while business leaders design the playsHabib didn&#x27;t leave IT leaders without a role — she redefined it. \"If tech is everyone&#x27;s job, you might be asking, what is mine?\" she addressed CIOs. \"Yours is to provide the mission critical infrastructure that makes this revolution possible.\"As tens or hundreds of thousands of AI agents operate at various levels of autonomy within organizations, \"governance becomes existential,\" she explained. \"The business leader&#x27;s job is to design the play, but you have to build the stadium, you have to write the rule book, and you have to make sure these plays can win at championship scale.\"The formulation suggests a partnership model: business leaders drive workflow redesign and strategic implementation while IT provides the infrastructure, governance frameworks, and security guardrails that make mass AI deployment safe and scalable. \"One can&#x27;t succeed without the other,\" Habib said.For CIOs and technical leaders, this represents a fundamental shift from gatekeeper to enabler. When business units deploy agents autonomously, IT faces governance challenges unlike anything in enterprise software history. Success requires genuine partnership between business and IT — neither can succeed alone, forcing cultural changes in how these functions collaborate.A real example: From multi-day scrambles to instant answers during a market crisisTo ground her arguments in concrete business impact, Habib described working with the chief client officer of a Fortune 500 wealth advisory firm during recent market volatility following tariff announcements.\"Their phone was ringing off the hook with customers trying to figure out their market exposure,\" she recounted. \"Every request kicked off a multi-day, multi-person scramble: a portfolio manager ran the show, an analyst pulled charts, a relationship manager built the PowerPoint, a compliance officer had to review everything for disclosures. And the leader in all this — she was forwarding emails and chasing updates. This is the top job: managing complexity.\"With an agentic AI system, the same work happens programmatically. \"A system of agents is able to assemble the answer faster than any number of people could have. No more midnight deck reviews. No more days on end\" of coordination, Habib said.This isn&#x27;t about marginal productivity gains — it&#x27;s about fundamentally different operating models where senior executives shift from managing coordination to designing intelligent systems.Why so many AI initiatives are failing despite massive investmentHabib&#x27;s arguments arrive as many enterprises face AI disillusionment. After initial excitement about generative AI, many companies have struggled to move beyond pilots and demonstrations to production deployments generating tangible business value.Her diagnosis — that leaders are delegating rather than driving transformation — aligns with growing evidence that organizational factors, not technical limitations, explain most failures. Companies often lack clarity on use cases, struggle with data preparation, or face internal resistance to workflow changes that AI requires.Perhaps the most striking aspect of Habib&#x27;s presentation was her willingness to acknowledge the human cost of AI transformation — and insist leaders address it rather than avoid it. \"Your job as a leader is to not look away from this fear. Your job is to face it with a plan,\" she told the audience.She described \"productivity anchoring\" as a form of \"self-sabotage\" where employees resist AI adoption because their identity and self-worth are tied to execution tasks AI can now perform. The phenomenon suggests that successful AI transformation requires not just technical and strategic changes but psychological and cultural work that many leaders may be unprepared for.Two challenges: Get your hands dirty, then reimagine everythingHabib closed by throwing down two gauntlets to her executive audience.\"First, a small one: get your hands dirty with agentic AI. Don&#x27;t delegate. Choose a process that you oversee and automate it. See the difference from managing a complex process to redesigning it for yourself.\"The second was more ambitious: \"Go back to your team and ask, what could we achieve if execution were free? What would work feel like, be like, look like if you&#x27;re unbound from the friction and process that slows us down today?\"She concluded: \"The tools for creation are in your hands. The mandate for leadership is on your shoulders. What will you build?\"For enterprise leaders accustomed to viewing AI as an IT initiative, Habib&#x27;s message is clear: that approach isn&#x27;t working, won&#x27;t work, and reflects a fundamental misunderstanding of what AI represents. Whether executives embrace her call to personally drive transformation — or continue delegating to IT departments — may determine which organizations thrive and which become cautionary tales.The statistic she opened with lingers uncomfortably: 42% of Fortune 500 C-suite executives say AI is tearing their companies apart. Habib&#x27;s diagnosis suggests they&#x27;re tearing themselves apart by clinging to organizational models designed for an era when execution was scarce. The cure she prescribes requires leaders to do something most find uncomfortable: stop managing complexity and start dismantling it.",
          "content": "May Habib, co-founder and CEO of Writer AI, delivered one of the bluntest assessments of corporate AI failures at the TED AI conference on Tuesday, revealing that nearly half of Fortune 500 executives believe artificial intelligence is actively damaging their organizations — and placing the blame squarely on leadership&#x27;s shoulders.The problem, according to Habib, isn&#x27;t the technology. It&#x27;s that business leaders are making a category error, treating AI transformation like previous technology rollouts and delegating it to IT departments. This approach, she warned, has led to \"billions of dollars spent on AI initiatives that are going nowhere.\"\"Earlier this year, we did a survey of 800 Fortune 500 C-suite executives,\" Habib told the audience of Silicon Valley executives and investors. \"42% of them said AI is tearing their company apart.\"The diagnosis challenges conventional wisdom about how enterprises should approach AI adoption. While most major companies have stood up AI task forces, appointed chief AI officers, or expanded IT budgets, Habib argues these moves reflect a fundamental misunderstanding of what AI represents: not another software tool, but a wholesale reorganization of how work gets done.\"There is something leaders are missing when they compare AI to just another tech tool,\" Habib said. \"This is not like giving accountants calculators or bankers Excel or designers Photoshop.\"Why the &#x27;old playbook&#x27; of delegating to IT departments is failing companiesHabib, whose company has spent five years building AI systems for Fortune 500 companies and logged two million miles visiting customer sites, said the pattern is consistent: \"When generative AI started showing up, we turned to the old playbook. We turned to IT and said, &#x27;Go figure this out.&#x27;\"That approach fails, she argued, because AI fundamentally changes the economics and organization of work itself. \"For 100 years, enterprises have been built around the idea that execution is expensive and hard,\" Habib said. \"The enterprise built complex org charts, complex processes, all to manage people doing stuff.\"AI inverts that model. \"Execution is going from scarce and expensive to programmatic, on-demand and abundant,\" she said. In this new paradigm, the bottleneck shifts from execution capacity to strategic design — a shift that requires business leaders, not IT departments, to drive transformation.\"With AI technology, it can no longer be centralized. It&#x27;s in every workflow, every business,\" Habib said. \"It is now the most important part of a business leader&#x27;s job. It cannot be delegated.\"The statement represents a direct challenge to how most large organizations have structured their AI initiatives, with centralized centers of excellence, dedicated AI teams, or IT-led implementations that business units are expected to adopt.A generational power shift is happening based on who understands AI workflow designHabib framed the shift in dramatic terms: \"A generational transfer of power is happening right now. It&#x27;s not about your age or how long you&#x27;ve been at a company. The generational transfer of power is about the nature of leadership itself.\"Traditional leadership, she argued, has been defined by the ability to manage complexity — big teams, big budgets, intricate processes. \"The identity of leaders at these companies, people like us, has been tied to old school power structures: control, hierarchy, how big our teams are, how big our budgets are. Our value is measured by the sheer amount of complexity we could manage,\" Habib said. \"Today we reward leaders for this. We promote leaders for this.\"AI makes that model obsolete. \"When I am able to 10x the output of my team or do things that could never be possible, work is no longer about the 1x,\" she said. \"Leadership is no longer about managing complex human execution.\"Instead, Habib outlined three fundamental shifts that define what she calls \"AI-first leaders\" — executives her company has worked with who have successfully deployed AI agents solving \"$100 million plus problems.\"The first shift: Taking a machete to enterprise complexityThe new leadership mandate, according to Habib, is \"taking a machete to the complexity that has calcified so many organizations.\" She pointed to the layers of friction that have accumulated in enterprises: \"Brilliant ideas dying in memos, the endless cycles of approvals, the death by 1,000 clicks, meetings about meetings — a death, by the way, that&#x27;s happening in 17 different browser tabs each for software that promises to be a single source of truth.\"Rather than accepting this complexity as inevitable, AI-first leaders redesign workflows from first principles. \"There are very few legacy systems that can&#x27;t be replaced in your organization, that won&#x27;t be replaced,\" Habib said. \"But they&#x27;re not going to be replaced by another monolithic piece of software. They can only be replaced by a business leader articulating business logic and getting that into an agentic system.\"She offered a concrete example: \"We have customers where it used to take them seven months to get a creative campaign — not even a product, a campaign. Now they can go from TikTok trend to digital shelf in 30 days. That is radical simplicity.\"The catch, she emphasized, is that CIOs can&#x27;t drive this transformation alone. \"Your CIO can&#x27;t help flatten your org chart. Only a business leader can look at workflows and say, &#x27;This part is necessary genius, this part is bureaucratic scar tissue that has to go.&#x27;\"The second shift: Managing the fear as career ladders disappearWhen AI handles execution, \"your humans are liberated to do what they&#x27;re amazing at: judgment, strategy, creativity,\" Habib explained. \"The old leadership playbook was about managing headcount. We managed people against revenue: one business development rep for every three account executives, one marketer for every five salespeople.\"But this liberation carries profound challenges that leaders must address directly. Habib acknowledged the elephant in the room that many executives avoid discussing: \"These changes are still frightening for people, even when it&#x27;s become unholy to talk about it.\" She&#x27;s witnessed the fear firsthand. \"It shows up as tears in an AI workshop when someone feels like their old skill set isn&#x27;t translated to the new.\"She introduced a term for a common form of resistance: \"productivity anchoring\" — when employees \"cling to the hard way of doing things because they feel productive, because their self-worth is tied to them, even when empirically AI can be better.\"The solution isn&#x27;t to look away. \"We have to design new pathways to impact, to show your people their value is not in executing a task. Their value is in orchestrating systems of execution, to ask the next great question,\" Habib said. She advocates replacing career \"ladders\" with \"lattices\" where \"people need to grow laterally, to expand sideways.\"She was candid about the disruption: \"The first rungs on our career ladders are indeed going away. I know because my company is automating them.\" But she insisted this creates opportunity for work that is \"more creative, more strategic, more driven by curiosity and impact — and I believe a lot more human than the jobs that they&#x27;re replacing.\"The third shift: When execution becomes free, ambition becomes the only bottleneckThe final shift is from optimization to creation. \"Before AI, we used to call it transformation when we took 12 steps and made them nine,\" Habib said. \"That&#x27;s optimizing the world as it is. We can now create a new world. That is the greenfield mindset.\"She challenged executives to identify assumptions their industries are built on that AI now disrupts. Writer&#x27;s customers, she said, are already seeing new categories of growth: treating every customer like their only customer, democratizing premium services to broader markets, and entering new markets at unprecedented speed because \"AI strips away the friction to access new channels.\"\"When execution is abundant, the only bottleneck is the scope of your own ambition,\" Habib declared.What this means for CIOs: Building the stadium while business leaders design the playsHabib didn&#x27;t leave IT leaders without a role — she redefined it. \"If tech is everyone&#x27;s job, you might be asking, what is mine?\" she addressed CIOs. \"Yours is to provide the mission critical infrastructure that makes this revolution possible.\"As tens or hundreds of thousands of AI agents operate at various levels of autonomy within organizations, \"governance becomes existential,\" she explained. \"The business leader&#x27;s job is to design the play, but you have to build the stadium, you have to write the rule book, and you have to make sure these plays can win at championship scale.\"The formulation suggests a partnership model: business leaders drive workflow redesign and strategic implementation while IT provides the infrastructure, governance frameworks, and security guardrails that make mass AI deployment safe and scalable. \"One can&#x27;t succeed without the other,\" Habib said.For CIOs and technical leaders, this represents a fundamental shift from gatekeeper to enabler. When business units deploy agents autonomously, IT faces governance challenges unlike anything in enterprise software history. Success requires genuine partnership between business and IT — neither can succeed alone, forcing cultural changes in how these functions collaborate.A real example: From multi-day scrambles to instant answers during a market crisisTo ground her arguments in concrete business impact, Habib described working with the chief client officer of a Fortune 500 wealth advisory firm during recent market volatility following tariff announcements.\"Their phone was ringing off the hook with customers trying to figure out their market exposure,\" she recounted. \"Every request kicked off a multi-day, multi-person scramble: a portfolio manager ran the show, an analyst pulled charts, a relationship manager built the PowerPoint, a compliance officer had to review everything for disclosures. And the leader in all this — she was forwarding emails and chasing updates. This is the top job: managing complexity.\"With an agentic AI system, the same work happens programmatically. \"A system of agents is able to assemble the answer faster than any number of people could have. No more midnight deck reviews. No more days on end\" of coordination, Habib said.This isn&#x27;t about marginal productivity gains — it&#x27;s about fundamentally different operating models where senior executives shift from managing coordination to designing intelligent systems.Why so many AI initiatives are failing despite massive investmentHabib&#x27;s arguments arrive as many enterprises face AI disillusionment. After initial excitement about generative AI, many companies have struggled to move beyond pilots and demonstrations to production deployments generating tangible business value.Her diagnosis — that leaders are delegating rather than driving transformation — aligns with growing evidence that organizational factors, not technical limitations, explain most failures. Companies often lack clarity on use cases, struggle with data preparation, or face internal resistance to workflow changes that AI requires.Perhaps the most striking aspect of Habib&#x27;s presentation was her willingness to acknowledge the human cost of AI transformation — and insist leaders address it rather than avoid it. \"Your job as a leader is to not look away from this fear. Your job is to face it with a plan,\" she told the audience.She described \"productivity anchoring\" as a form of \"self-sabotage\" where employees resist AI adoption because their identity and self-worth are tied to execution tasks AI can now perform. The phenomenon suggests that successful AI transformation requires not just technical and strategic changes but psychological and cultural work that many leaders may be unprepared for.Two challenges: Get your hands dirty, then reimagine everythingHabib closed by throwing down two gauntlets to her executive audience.\"First, a small one: get your hands dirty with agentic AI. Don&#x27;t delegate. Choose a process that you oversee and automate it. See the difference from managing a complex process to redesigning it for yourself.\"The second was more ambitious: \"Go back to your team and ask, what could we achieve if execution were free? What would work feel like, be like, look like if you&#x27;re unbound from the friction and process that slows us down today?\"She concluded: \"The tools for creation are in your hands. The mandate for leadership is on your shoulders. What will you build?\"For enterprise leaders accustomed to viewing AI as an IT initiative, Habib&#x27;s message is clear: that approach isn&#x27;t working, won&#x27;t work, and reflects a fundamental misunderstanding of what AI represents. Whether executives embrace her call to personally drive transformation — or continue delegating to IT departments — may determine which organizations thrive and which become cautionary tales.The statistic she opened with lingers uncomfortably: 42% of Fortune 500 C-suite executives say AI is tearing their companies apart. Habib&#x27;s diagnosis suggests they&#x27;re tearing themselves apart by clinging to organizational models designed for an era when execution was scarce. The cure she prescribes requires leaders to do something most find uncomfortable: stop managing complexity and start dismantling it.",
          "feed_position": 5,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/4aHBNxyB2pkPhCFjBLoMOH/b09ae06e86fe5534666c4574c5de2bdb/nuneybits_Vector_art_of_company_fracturing_apart_433a69a5-4c41-4199-bf6a-51d8cd076379.webp?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/sakana-ais-cto-says-hes-absolutely-sick-of-transformers-the-tech-that-powers",
          "published_at": "Thu, 23 Oct 2025 13:00:00 GMT",
          "title": "Sakana AI's CTO says he's 'absolutely sick' of transformers, the tech that powers every major AI model",
          "standfirst": "In a striking act of self-critique, one of the architects of the transformer technology that powers ChatGPT, Claude, and virtually every major AI system told an audience of industry leaders this week that artificial intelligence research has become dangerously narrow — and that he&#x27;s moving on from his own creation.Llion Jones, who co-authored the seminal 2017 paper \"Attention Is All You Need\" and even coined the name \"transformer,\" delivered an unusually candid assessment at the TED AI conference in San Francisco on Tuesday: Despite unprecedented investment and talent flooding into AI, the field has calcified around a single architectural approach, potentially blinding researchers to the next major breakthrough.\"Despite the fact that there&#x27;s never been so much interest and resources and money and talent, this has somehow caused the narrowing of the research that we&#x27;re doing,\" Jones told the audience. The culprit, he argued, is the \"immense amount of pressure\" from investors demanding returns and researchers scrambling to stand out in an overcrowded field.The warning carries particular weight given Jones&#x27;s role in AI history. The transformer architecture he helped develop at Google has become the foundation of the generative AI boom, enabling systems that can write essays, generate images, and engage in human-like conversation. His paper has been cited more than 100,000 times, making it one of the most influential computer science publications of the century.Now, as CTO and co-founder of Tokyo-based Sakana AI, Jones is explicitly abandoning his own creation. \"I personally made a decision in the beginning of this year that I&#x27;m going to drastically reduce the amount of time that I spend on transformers,\" he said. \"I&#x27;m explicitly now exploring and looking for the next big thing.\"Why more AI funding has led to less creative research, according to a transformer pioneerJones painted a picture of an AI research community suffering from what he called a paradox: More resources have led to less creativity. He described researchers constantly checking whether they&#x27;ve been \"scooped\" by competitors working on identical ideas, and academics choosing safe, publishable projects over risky, potentially transformative ones.\"If you&#x27;re doing standard AI research right now, you kind of have to assume that there&#x27;s maybe three or four other groups doing something very similar, or maybe exactly the same,\" Jones said, describing an environment where \"unfortunately, this pressure damages the science, because people are rushing their papers, and it&#x27;s reducing the amount of creativity.\"He drew an analogy from AI itself — the \"exploration versus exploitation\" trade-off that governs how algorithms search for solutions. When a system exploits too much and explores too little, it finds mediocre local solutions while missing superior alternatives. \"We are almost certainly in that situation right now in the AI industry,\" Jones argued.The implications are sobering. Jones recalled the period just before transformers emerged, when researchers were endlessly tweaking recurrent neural networks — the previous dominant architecture — for incremental gains. Once transformers arrived, all that work suddenly seemed irrelevant. \"How much time do you think those researchers would have spent trying to improve the recurrent neural network if they knew something like transformers was around the corner?\" he asked.He worries the field is repeating that pattern. \"I&#x27;m worried that we&#x27;re in that situation right now where we&#x27;re just concentrating on one architecture and just permuting it and trying different things, where there might be a breakthrough just around the corner.\"How the &#x27;Attention is all you need&#x27; paper was born from freedom, not pressureTo underscore his point, Jones described the conditions that allowed transformers to emerge in the first place — a stark contrast to today&#x27;s environment. The project, he said, was \"very organic, bottom up,\" born from \"talking over lunch or scrawling randomly on the whiteboard in the office.\"Critically, \"we didn&#x27;t actually have a good idea, we had the freedom to actually spend time and go and work on it, and even more importantly, we didn&#x27;t have any pressure that was coming down from management,\" Jones recounted. \"No pressure to work on any particular project, publish a number of papers to push a certain metric up.\"That freedom, Jones suggested, is largely absent today. Even researchers recruited for astronomical salaries — \"literally a million dollars a year, in some cases\" — may not feel empowered to take risks. \"Do you think that when they start their new position they feel empowered to try their wild ideas and more speculative ideas, or do they feel immense pressure to prove their worth and once again, go for the low hanging fruit?\" he asked.Why one AI lab is betting that research freedom beats million-dollar salariesJones&#x27;s proposed solution is deliberately provocative: Turn up the \"explore dial\" and openly share findings, even at competitive cost. He acknowledged the irony of his position. \"It may sound a little controversial to hear one of the Transformers authors stand on stage and tell you that he&#x27;s absolutely sick of them, but it&#x27;s kind of fair enough, right? I&#x27;ve been working on them longer than anyone, with the possible exception of seven people.\"At Sakana AI, Jones said he&#x27;s attempting to recreate that pre-transformer environment, with nature-inspired research and minimal pressure to chase publications or compete directly with rivals. He offered researchers a mantra from engineer Brian Cheung: \"You should only do the research that wouldn&#x27;t happen if you weren&#x27;t doing it.\"One example is Sakana&#x27;s \"continuous thought machine,\" which incorporates brain-like synchronization into neural networks. An employee who pitched the idea told Jones he would have faced skepticism and pressure not to waste time at previous employers or academic positions. At Sakana, Jones gave him a week to explore. The project became successful enough to be spotlighted at NeurIPS, a major AI conference.Jones even suggested that freedom beats compensation in recruiting. \"It&#x27;s a really, really good way of getting talent,\" he said of the exploratory environment. \"Think about it, talented, intelligent people, ambitious people, will naturally seek out this kind of environment.\"The transformer&#x27;s success may be blocking AI&#x27;s next breakthroughPerhaps most provocatively, Jones suggested transformers may be victims of their own success. \"The fact that the current technology is so powerful and flexible... stopped us from looking for better,\" he said. \"It makes sense that if the current technology was worse, more people would be looking for better.\"He was careful to clarify that he&#x27;s not dismissing ongoing transformer research. \"There&#x27;s still plenty of very important work to be done on current technology and bringing a lot of value in the coming years,\" he said. \"I&#x27;m just saying that given the amount of talent and resources that we have currently, we can afford to do a lot more.\"His ultimate message was one of collaboration over competition. \"Genuinely, from my perspective, this is not a competition,\" Jones concluded. \"We all have the same goal. We all want to see this technology progress so that we can all benefit from it. So if we can all collectively turn up the explore dial and then openly share what we find, we can get to our goal much faster.\"The high stakes of AI&#x27;s exploration problemThe remarks arrive at a pivotal moment for artificial intelligence. The industry grapples with mounting evidence that simply building larger transformer models may be approaching diminishing returns. Leading researchers have begun openly discussing whether the current paradigm has fundamental limitations, with some suggesting that architectural innovations — not just scale — will be needed for continued progress toward more capable AI systems.Jones&#x27;s warning suggests that finding those innovations may require dismantling the very incentive structures that have driven AI&#x27;s recent boom. With tens of billions of dollars flowing into AI development annually and fierce competition among labs driving secrecy and rapid publication cycles, the exploratory research environment he described seems increasingly distant.Yet his insider perspective carries unusual weight. As someone who helped create the technology now dominating the field, Jones understands both what it takes to achieve breakthrough innovation and what the industry risks by abandoning that approach. His decision to walk away from transformers — the architecture that made his reputation — adds credibility to a message that might otherwise sound like contrarian positioning.Whether AI&#x27;s power players will heed the call remains uncertain. But Jones offered a pointed reminder of what&#x27;s at stake: The next transformer-scale breakthrough could be just around the corner, pursued by researchers with the freedom to explore. Or it could be languishing unexplored while thousands of researchers race to publish incremental improvements on architecture that, in Jones&#x27;s words, one of its creators is \"absolutely sick of.\"After all, he&#x27;s been working on transformers longer than almost anyone. He would know when it&#x27;s time to move on.",
          "content": "In a striking act of self-critique, one of the architects of the transformer technology that powers ChatGPT, Claude, and virtually every major AI system told an audience of industry leaders this week that artificial intelligence research has become dangerously narrow — and that he&#x27;s moving on from his own creation.Llion Jones, who co-authored the seminal 2017 paper \"Attention Is All You Need\" and even coined the name \"transformer,\" delivered an unusually candid assessment at the TED AI conference in San Francisco on Tuesday: Despite unprecedented investment and talent flooding into AI, the field has calcified around a single architectural approach, potentially blinding researchers to the next major breakthrough.\"Despite the fact that there&#x27;s never been so much interest and resources and money and talent, this has somehow caused the narrowing of the research that we&#x27;re doing,\" Jones told the audience. The culprit, he argued, is the \"immense amount of pressure\" from investors demanding returns and researchers scrambling to stand out in an overcrowded field.The warning carries particular weight given Jones&#x27;s role in AI history. The transformer architecture he helped develop at Google has become the foundation of the generative AI boom, enabling systems that can write essays, generate images, and engage in human-like conversation. His paper has been cited more than 100,000 times, making it one of the most influential computer science publications of the century.Now, as CTO and co-founder of Tokyo-based Sakana AI, Jones is explicitly abandoning his own creation. \"I personally made a decision in the beginning of this year that I&#x27;m going to drastically reduce the amount of time that I spend on transformers,\" he said. \"I&#x27;m explicitly now exploring and looking for the next big thing.\"Why more AI funding has led to less creative research, according to a transformer pioneerJones painted a picture of an AI research community suffering from what he called a paradox: More resources have led to less creativity. He described researchers constantly checking whether they&#x27;ve been \"scooped\" by competitors working on identical ideas, and academics choosing safe, publishable projects over risky, potentially transformative ones.\"If you&#x27;re doing standard AI research right now, you kind of have to assume that there&#x27;s maybe three or four other groups doing something very similar, or maybe exactly the same,\" Jones said, describing an environment where \"unfortunately, this pressure damages the science, because people are rushing their papers, and it&#x27;s reducing the amount of creativity.\"He drew an analogy from AI itself — the \"exploration versus exploitation\" trade-off that governs how algorithms search for solutions. When a system exploits too much and explores too little, it finds mediocre local solutions while missing superior alternatives. \"We are almost certainly in that situation right now in the AI industry,\" Jones argued.The implications are sobering. Jones recalled the period just before transformers emerged, when researchers were endlessly tweaking recurrent neural networks — the previous dominant architecture — for incremental gains. Once transformers arrived, all that work suddenly seemed irrelevant. \"How much time do you think those researchers would have spent trying to improve the recurrent neural network if they knew something like transformers was around the corner?\" he asked.He worries the field is repeating that pattern. \"I&#x27;m worried that we&#x27;re in that situation right now where we&#x27;re just concentrating on one architecture and just permuting it and trying different things, where there might be a breakthrough just around the corner.\"How the &#x27;Attention is all you need&#x27; paper was born from freedom, not pressureTo underscore his point, Jones described the conditions that allowed transformers to emerge in the first place — a stark contrast to today&#x27;s environment. The project, he said, was \"very organic, bottom up,\" born from \"talking over lunch or scrawling randomly on the whiteboard in the office.\"Critically, \"we didn&#x27;t actually have a good idea, we had the freedom to actually spend time and go and work on it, and even more importantly, we didn&#x27;t have any pressure that was coming down from management,\" Jones recounted. \"No pressure to work on any particular project, publish a number of papers to push a certain metric up.\"That freedom, Jones suggested, is largely absent today. Even researchers recruited for astronomical salaries — \"literally a million dollars a year, in some cases\" — may not feel empowered to take risks. \"Do you think that when they start their new position they feel empowered to try their wild ideas and more speculative ideas, or do they feel immense pressure to prove their worth and once again, go for the low hanging fruit?\" he asked.Why one AI lab is betting that research freedom beats million-dollar salariesJones&#x27;s proposed solution is deliberately provocative: Turn up the \"explore dial\" and openly share findings, even at competitive cost. He acknowledged the irony of his position. \"It may sound a little controversial to hear one of the Transformers authors stand on stage and tell you that he&#x27;s absolutely sick of them, but it&#x27;s kind of fair enough, right? I&#x27;ve been working on them longer than anyone, with the possible exception of seven people.\"At Sakana AI, Jones said he&#x27;s attempting to recreate that pre-transformer environment, with nature-inspired research and minimal pressure to chase publications or compete directly with rivals. He offered researchers a mantra from engineer Brian Cheung: \"You should only do the research that wouldn&#x27;t happen if you weren&#x27;t doing it.\"One example is Sakana&#x27;s \"continuous thought machine,\" which incorporates brain-like synchronization into neural networks. An employee who pitched the idea told Jones he would have faced skepticism and pressure not to waste time at previous employers or academic positions. At Sakana, Jones gave him a week to explore. The project became successful enough to be spotlighted at NeurIPS, a major AI conference.Jones even suggested that freedom beats compensation in recruiting. \"It&#x27;s a really, really good way of getting talent,\" he said of the exploratory environment. \"Think about it, talented, intelligent people, ambitious people, will naturally seek out this kind of environment.\"The transformer&#x27;s success may be blocking AI&#x27;s next breakthroughPerhaps most provocatively, Jones suggested transformers may be victims of their own success. \"The fact that the current technology is so powerful and flexible... stopped us from looking for better,\" he said. \"It makes sense that if the current technology was worse, more people would be looking for better.\"He was careful to clarify that he&#x27;s not dismissing ongoing transformer research. \"There&#x27;s still plenty of very important work to be done on current technology and bringing a lot of value in the coming years,\" he said. \"I&#x27;m just saying that given the amount of talent and resources that we have currently, we can afford to do a lot more.\"His ultimate message was one of collaboration over competition. \"Genuinely, from my perspective, this is not a competition,\" Jones concluded. \"We all have the same goal. We all want to see this technology progress so that we can all benefit from it. So if we can all collectively turn up the explore dial and then openly share what we find, we can get to our goal much faster.\"The high stakes of AI&#x27;s exploration problemThe remarks arrive at a pivotal moment for artificial intelligence. The industry grapples with mounting evidence that simply building larger transformer models may be approaching diminishing returns. Leading researchers have begun openly discussing whether the current paradigm has fundamental limitations, with some suggesting that architectural innovations — not just scale — will be needed for continued progress toward more capable AI systems.Jones&#x27;s warning suggests that finding those innovations may require dismantling the very incentive structures that have driven AI&#x27;s recent boom. With tens of billions of dollars flowing into AI development annually and fierce competition among labs driving secrecy and rapid publication cycles, the exploratory research environment he described seems increasingly distant.Yet his insider perspective carries unusual weight. As someone who helped create the technology now dominating the field, Jones understands both what it takes to achieve breakthrough innovation and what the industry risks by abandoning that approach. His decision to walk away from transformers — the architecture that made his reputation — adds credibility to a message that might otherwise sound like contrarian positioning.Whether AI&#x27;s power players will heed the call remains uncertain. But Jones offered a pointed reminder of what&#x27;s at stake: The next transformer-scale breakthrough could be just around the corner, pursued by researchers with the freedom to explore. Or it could be languishing unexplored while thousands of researchers race to publish incremental improvements on architecture that, in Jones&#x27;s words, one of its creators is \"absolutely sick of.\"After all, he&#x27;s been working on transformers longer than almost anyone. He would know when it&#x27;s time to move on.",
          "feed_position": 6,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/WSXBhFReMwh2HPn3P3k9E/f6352f008c9afddcbf6a4ff6148d7c96/nuneybits_Vector_art_of_a_koi_fish_with_scales_formed_from_algo_8e356867-71b0-4e3b-b5b1-87ac3e4c8013.webp?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/data-infrastructure/research-finds-that-77-of-data-engineers-have-heavier-workloads-despite-ai",
          "published_at": "Thu, 23 Oct 2025 13:00:00 GMT",
          "title": "Research finds that 77% of data engineers have heavier workloads despite AI tools: Here's why and what to do about it",
          "standfirst": "Data engineers should be working faster than ever. AI-powered tools promise to automate pipeline optimization, accelerate data integration and handle the repetitive grunt work that has defined the profession for decades.Yet, according to a new survey of 400 senior technology executives by MIT Technology Review Insights in partnership with Snowflake, 77% say their data engineering teams&#x27; workloads are getting heavier, not lighter.The culprit? The very AI tools meant to help are creating a new set of problems.While 83% of organizations have already deployed AI-based data engineering tools, 45% cite integration complexity as a top challenge. Another 38% are struggling with tool sprawl and fragmentation.\"Many data engineers are using one tool to collect data, one tool to process data and another to run analytics on that data,\" Chris Child, VP of product for data engineering at Snowflake, told VentureBeat. \"Using several tools along this data lifecycle introduces complexity, risk and increased infrastructure management, which data engineers can&#x27;t afford to take on.\"The result is a productivity paradox. AI tools are making individual tasks faster, but the proliferation of disconnected tools is making the overall system more complex to manage. For enterprises racing to deploy AI at scale, this fragmentation represents a critical bottleneck.From SQL queries to LLM pipelines: The daily workflow shiftThe survey found that data engineers spent an average of 19% of their time on AI projects two years ago. Today, that figure has jumped to 37%. Respondents expect it to hit 61% within two years.But what does that shift actually look like in practice?Child offered a concrete example. Previously, if the CFO of a company needed to make forecast predictions, they would tap the data engineering team to help build a system that correlates unstructured data like vendor contracts with structured data like revenue numbers into a static dashboard. Connecting these two worlds of different data types was extremely time-consuming and expensive, requiring lawyers to manually read through each document for key contract terms and upload that information into a database.Today, that same workflow looks radically different.\"Data engineers can use a tool like Snowflake Openflow to seamlessly bring the unstructured PDF contracts living in a source like Box, together with the structured financial figures into a single platform like Snowflake, making the data accessible to LLMs,\" Child said. \"What used to take hours of manual work is now near instantaneous.\"The shift isn&#x27;t just about speed. It&#x27;s about the nature of the work itself.Two years ago, a typical data engineer&#x27;s day consisted of tuning clusters, writing SQL transformations and ensuring data readiness for human analysts. Today, that same engineer is more likely to be debugging LLM-powered transformation pipelines and setting up governance rules for AI model workflows.\"Data engineers&#x27; core skill isn&#x27;t just coding,\" Child said. \"It&#x27;s orchestrating the data foundation and ensuring trust, context and governance so AI outputs are reliable.\"The tool stack problem: When help becomes hindranceHere&#x27;s where enterprises are getting stuck.The promise of AI-powered data tools is compelling: automate pipeline optimization, accelerate debugging, streamline integration. But in practice, many organizations are discovering that each new AI tool they add creates its own integration headaches.The survey data bears this out. While AI has led to improvements in output quantity (74% report increases) and quality (77% report improvements), those gains are being offset by the operational overhead of managing disconnected tools.\"The other problem we&#x27;re seeing is that AI tools often make it easy to build a prototype by stitching together several data sources with an out-of-the-box LLM,\" Child said. \"But then when you want to take that into production, you realize that you don&#x27;t have the data accessible and you don&#x27;t know what governance you need, so it becomes difficult to roll the tool out to your users.\"For technical decision-makers evaluating their data engineering stack right now, Child offered a clear framework. \"Teams should prioritize AI tools that accelerate productivity, while at the same time eliminate infrastructure and operational complexity,\" he said. \"This allows engineers to move their focus away from managing the &#x27;glue work&#x27; of data engineering and closer to business outcomes.\"The agentic AI deployment window: 12 months to get it rightThe survey revealed that 54% of organizations plan to deploy agentic AI within the next 12 months. Agentic AI refers to autonomous agents that can make decisions and take actions without human intervention. Another 20% have already begun doing so.For data engineering teams, agentic AI represents both an enormous opportunity and a significant risk. Done right, autonomous agents can handle repetitive tasks like detecting schema drift or debugging transformation errors. Done wrong, they can corrupt datasets or expose sensitive information.\"Data engineers must prioritize pipeline optimization and monitoring in order to truly deploy agentic AI at scale,\" Child said. \"It&#x27;s a low-risk, high-return starting point that allows agentic AI to safely automate repetitive tasks like detecting schema drift or debugging transformation errors when done correctly.\"But Child was emphatic about the guardrails that must be in place first.\"Before organizations let agents near production data, two safeguards must be in place: strong governance and lineage tracking, and active human oversight,\" he said. \"Agents must inherit fine-grained permissions and operate within an established governance framework.\"The risks of skipping those steps are real. \"Without proper lineage or access governance, an agent could unintentionally corrupt datasets or expose sensitive information,\" Child warned.The perception gap that&#x27;s costing enterprises AI successPerhaps the most striking finding in the survey is a disconnect at the C-suite level.While 80% of chief data officers and 82% of chief AI officers consider data engineers integral to business success, only 55% of CIOs share that view.\"This shows that the data-forward leaders are seeing data engineering&#x27;s strategic value, but we need to do more work to help the rest of the C-suite recognize that investing in a unified, scalable data foundation and the people helping drive this is an investment in AI success, not just IT operations,\" Child said.That perception gap has real consequences.Data engineers in the surveyed organizations are already influential in decisions about AI use-case feasibility (53% of respondents) and business units&#x27; use of AI models (56%). But if CIOs don&#x27;t recognize data engineers as strategic partners, they&#x27;re unlikely to give those teams the resources, authority or seat at the table they need to prevent the kinds of tool sprawl and integration problems the survey identified.The gap appears to correlate with visibility. Chief data officers and chief AI officers work directly with data engineering teams daily and understand the complexity of what they&#x27;re managing. CIOs, focused more broadly on infrastructure and operations, may not see the strategic architecture work that data engineers are increasingly doing.This disconnect also shows up in how different executives rate the challenges facing data engineering teams. Chief AI officers are significantly more likely than CIOs to agree that data engineers&#x27; workloads are becoming increasingly heavy (93% vs. 75%). They&#x27;re also more likely to recognize data engineers&#x27; influence on overall AI strategy.What data engineers need to learn nowThe survey identified three critical skills data engineers need to develop: AI expertise, business acumen and communication abilities.For an enterprise with a 20-person data engineering team, that presents a practical challenge. Do you hire for these skills, train existing engineers or restructure the team? Child&#x27;s answer suggested the priority should be business understanding.\"The most important skill right now is for data engineers to understand what is critical to their end business users and prioritize how they can make those questions easier and faster to answer,\" he said.The lesson for enterprises: Business context matters more than adding technical certifications. Child stressed that understanding the business impact of &#x27;why&#x27; data engineers are performing certain tasks will allow them to anticipate the needs of customers better, delivering value more immediately to the business. \"The organizations with data engineering teams that prioritize this business understanding will set themselves apart from competition.\"For enterprises looking to lead in AI, the solution to the data engineering productivity crisis isn&#x27;t more AI tools. The organizations that will move fastest are consolidating their tool stacks now, deploying governance infrastructure before agents go into production and elevating data engineers from support staff to strategic architects. The window is narrow. With 54% planning agentic AI deployment within 12 months and data engineers expected to spend 61% of their time on AI projects within two years, teams that haven&#x27;t addressed tool sprawl and governance gaps will find their AI initiatives stuck in permanent pilot mode.",
          "content": "Data engineers should be working faster than ever. AI-powered tools promise to automate pipeline optimization, accelerate data integration and handle the repetitive grunt work that has defined the profession for decades.Yet, according to a new survey of 400 senior technology executives by MIT Technology Review Insights in partnership with Snowflake, 77% say their data engineering teams&#x27; workloads are getting heavier, not lighter.The culprit? The very AI tools meant to help are creating a new set of problems.While 83% of organizations have already deployed AI-based data engineering tools, 45% cite integration complexity as a top challenge. Another 38% are struggling with tool sprawl and fragmentation.\"Many data engineers are using one tool to collect data, one tool to process data and another to run analytics on that data,\" Chris Child, VP of product for data engineering at Snowflake, told VentureBeat. \"Using several tools along this data lifecycle introduces complexity, risk and increased infrastructure management, which data engineers can&#x27;t afford to take on.\"The result is a productivity paradox. AI tools are making individual tasks faster, but the proliferation of disconnected tools is making the overall system more complex to manage. For enterprises racing to deploy AI at scale, this fragmentation represents a critical bottleneck.From SQL queries to LLM pipelines: The daily workflow shiftThe survey found that data engineers spent an average of 19% of their time on AI projects two years ago. Today, that figure has jumped to 37%. Respondents expect it to hit 61% within two years.But what does that shift actually look like in practice?Child offered a concrete example. Previously, if the CFO of a company needed to make forecast predictions, they would tap the data engineering team to help build a system that correlates unstructured data like vendor contracts with structured data like revenue numbers into a static dashboard. Connecting these two worlds of different data types was extremely time-consuming and expensive, requiring lawyers to manually read through each document for key contract terms and upload that information into a database.Today, that same workflow looks radically different.\"Data engineers can use a tool like Snowflake Openflow to seamlessly bring the unstructured PDF contracts living in a source like Box, together with the structured financial figures into a single platform like Snowflake, making the data accessible to LLMs,\" Child said. \"What used to take hours of manual work is now near instantaneous.\"The shift isn&#x27;t just about speed. It&#x27;s about the nature of the work itself.Two years ago, a typical data engineer&#x27;s day consisted of tuning clusters, writing SQL transformations and ensuring data readiness for human analysts. Today, that same engineer is more likely to be debugging LLM-powered transformation pipelines and setting up governance rules for AI model workflows.\"Data engineers&#x27; core skill isn&#x27;t just coding,\" Child said. \"It&#x27;s orchestrating the data foundation and ensuring trust, context and governance so AI outputs are reliable.\"The tool stack problem: When help becomes hindranceHere&#x27;s where enterprises are getting stuck.The promise of AI-powered data tools is compelling: automate pipeline optimization, accelerate debugging, streamline integration. But in practice, many organizations are discovering that each new AI tool they add creates its own integration headaches.The survey data bears this out. While AI has led to improvements in output quantity (74% report increases) and quality (77% report improvements), those gains are being offset by the operational overhead of managing disconnected tools.\"The other problem we&#x27;re seeing is that AI tools often make it easy to build a prototype by stitching together several data sources with an out-of-the-box LLM,\" Child said. \"But then when you want to take that into production, you realize that you don&#x27;t have the data accessible and you don&#x27;t know what governance you need, so it becomes difficult to roll the tool out to your users.\"For technical decision-makers evaluating their data engineering stack right now, Child offered a clear framework. \"Teams should prioritize AI tools that accelerate productivity, while at the same time eliminate infrastructure and operational complexity,\" he said. \"This allows engineers to move their focus away from managing the &#x27;glue work&#x27; of data engineering and closer to business outcomes.\"The agentic AI deployment window: 12 months to get it rightThe survey revealed that 54% of organizations plan to deploy agentic AI within the next 12 months. Agentic AI refers to autonomous agents that can make decisions and take actions without human intervention. Another 20% have already begun doing so.For data engineering teams, agentic AI represents both an enormous opportunity and a significant risk. Done right, autonomous agents can handle repetitive tasks like detecting schema drift or debugging transformation errors. Done wrong, they can corrupt datasets or expose sensitive information.\"Data engineers must prioritize pipeline optimization and monitoring in order to truly deploy agentic AI at scale,\" Child said. \"It&#x27;s a low-risk, high-return starting point that allows agentic AI to safely automate repetitive tasks like detecting schema drift or debugging transformation errors when done correctly.\"But Child was emphatic about the guardrails that must be in place first.\"Before organizations let agents near production data, two safeguards must be in place: strong governance and lineage tracking, and active human oversight,\" he said. \"Agents must inherit fine-grained permissions and operate within an established governance framework.\"The risks of skipping those steps are real. \"Without proper lineage or access governance, an agent could unintentionally corrupt datasets or expose sensitive information,\" Child warned.The perception gap that&#x27;s costing enterprises AI successPerhaps the most striking finding in the survey is a disconnect at the C-suite level.While 80% of chief data officers and 82% of chief AI officers consider data engineers integral to business success, only 55% of CIOs share that view.\"This shows that the data-forward leaders are seeing data engineering&#x27;s strategic value, but we need to do more work to help the rest of the C-suite recognize that investing in a unified, scalable data foundation and the people helping drive this is an investment in AI success, not just IT operations,\" Child said.That perception gap has real consequences.Data engineers in the surveyed organizations are already influential in decisions about AI use-case feasibility (53% of respondents) and business units&#x27; use of AI models (56%). But if CIOs don&#x27;t recognize data engineers as strategic partners, they&#x27;re unlikely to give those teams the resources, authority or seat at the table they need to prevent the kinds of tool sprawl and integration problems the survey identified.The gap appears to correlate with visibility. Chief data officers and chief AI officers work directly with data engineering teams daily and understand the complexity of what they&#x27;re managing. CIOs, focused more broadly on infrastructure and operations, may not see the strategic architecture work that data engineers are increasingly doing.This disconnect also shows up in how different executives rate the challenges facing data engineering teams. Chief AI officers are significantly more likely than CIOs to agree that data engineers&#x27; workloads are becoming increasingly heavy (93% vs. 75%). They&#x27;re also more likely to recognize data engineers&#x27; influence on overall AI strategy.What data engineers need to learn nowThe survey identified three critical skills data engineers need to develop: AI expertise, business acumen and communication abilities.For an enterprise with a 20-person data engineering team, that presents a practical challenge. Do you hire for these skills, train existing engineers or restructure the team? Child&#x27;s answer suggested the priority should be business understanding.\"The most important skill right now is for data engineers to understand what is critical to their end business users and prioritize how they can make those questions easier and faster to answer,\" he said.The lesson for enterprises: Business context matters more than adding technical certifications. Child stressed that understanding the business impact of &#x27;why&#x27; data engineers are performing certain tasks will allow them to anticipate the needs of customers better, delivering value more immediately to the business. \"The organizations with data engineering teams that prioritize this business understanding will set themselves apart from competition.\"For enterprises looking to lead in AI, the solution to the data engineering productivity crisis isn&#x27;t more AI tools. The organizations that will move fastest are consolidating their tool stacks now, deploying governance infrastructure before agents go into production and elevating data engineers from support staff to strategic architects. The window is narrow. With 54% planning agentic AI deployment within 12 months and data engineers expected to spend 61% of their time on AI projects within two years, teams that haven&#x27;t addressed tool sprawl and governance gaps will find their AI initiatives stuck in permanent pilot mode.",
          "feed_position": 7,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/6c6tA8fZ29MNNDGLsHQgK9/78cecaac03eddd1b3bd5489771bd2e57/modern-data-engineer-smk.jpg?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/what-enterprises-can-take-away-from-microsoft-ceo-satya-nadellas-shareholder",
          "published_at": "Thu, 23 Oct 2025 01:34:00 GMT",
          "title": "What enterprises can take away from Microsoft CEO Satya Nadella's shareholder letter",
          "standfirst": "One of the leading architects of the current generative AI boom — Microsoft CEO Satya Nadella, famed for having the software giant take an early investment in OpenAI (and later saying he was \"good for my $80 billion\") — published his latest annual letter yesterday on LinkedIn (a Microsoft subsidiary), and it&#x27;s chock full of interesting ideas about the near-term future that enterprise technical decision makers would do well to pay attention to, as it could aid in their own planning and tech stack development.In a companion post on X, Nadella wrote, “AI is radically changing every layer of the tech stack, and we’re changing with it.\" The full letter reinforces that message: Microsoft sees itself not just participating in the AI revolution, but shaping its infrastructure, security, tooling and governance for decades to come.While the message is addressed to Microsoft shareholders, the implications reach much further. The letter is a strategic signal to enterprise engineering leaders: CIOs, CTOs, AI leads, platform architects and security directors. Nadella outlines the direction of Microsoft’s innovation, but also what it expects from its customers and partners. The AI era is here, but it will be built by those who combine technical vision with operational discipline.Below are the five most important takeaways for enterprise technical decision makers.1. Security and reliability are now the foundation of the AI stackNadella makes security the first priority in the letter and ties it directly to Microsoft’s relevance going forward. Through its Secure Future Initiative (SFI), Microsoft has assigned the equivalent of 34,000 engineers to secure its identity systems, networks and software supply chain. Its Quality Excellence Initiative (QEI) aims to increase platform resiliency and strengthen global service uptime.Microsoft’s positioning makes it clear that enterprises will no longer get away with “ship fast, harden later” AI deployments. Nadella calls security “non-negotiable,” signaling that AI infrastructure must now meet the standards of mission-critical software. That means identity-first architecture, zero-trust execution environments and change management discipline are now table stakes for enterprise AI.2. AI infrastructure strategy is hybrid, open and sovereignty-readyNadella commits Microsoft to building “planet-scale systems” and backs that up with numbers: more than 400 Azure datacenters across 70 regions, two gigawatts of new compute capacity added this year, and new liquid-cooled GPU clusters rolling out across Azure. Microsoft also introduced Fairwater, a massive new AI datacenter in Wisconsin positioned to deliver unprecedented scale. Just as important, Microsoft is now officially multi-model. Azure AI Foundry offers access to more than 11,000 models including OpenAI, Meta, Mistral, Cohere and xAI. Microsoft is no longer pushing a single-model future, but a hybrid AI strategy.Enterprises should interpret this as validation of “portfolio architectures,” where closed, open and domain-specific models coexist. Nadella also emphasizes growing investment in sovereign cloud offerings for regulated industries, previewing a world where AI systems will have to meet regional data residency and compliance requirements from day one.3. AI agents—not just chatbots—are now Microsoft’s futureThe AI shift inside Microsoft is no longer about copilots that answer questions. It is now about AI agents that perform work. Nadella points to the rollout of Agent Mode in Microsoft 365 Copilot, which turns natural language requests into multistep business workflows. GitHub Copilot evolves from code autocomplete into a “peer programmer” capable of executing tasks asynchronously. In security operations, Microsoft has deployed AI agents that autonomously respond to incidents. In healthcare, Copilot for Dragon Medical documents clinical encounters automatically.This represents a major architectural pivot. Enterprises will need to move beyond prompt-response interfaces and begin engineering agent ecosystems that safely take actions inside business systems. That requires workflow orchestration, API integration strategies and strong guardrails. Nadella’s letter frames this as the next software platform shift.4. Unified data platforms are required to unlock AI valueNadella devotes significant attention to Microsoft Fabric and OneLake, calling Fabric the company’s fastest-growing data and analytics product ever. Fabric promises to centralize enterprise data from multiple cloud and analytics environments. OneLake provides a universal storage layer that binds analytics and AI workloads together.Microsoft’s message is blunt: siloed data means stalled AI. Enterprise teams that want AI at scale must unify operational and analytical data into a single architecture, enforce consistent data contracts and standardize metadata governance. AI success is now a data engineering problem more than a model problem.5. Trust, compliance and responsible AI are now mandatory for deployment“People want technology they can trust,” Nadella writes. Microsoft now publishes Responsible AI Transparency Reports and aligns parts of its development process with UN human rights guidance. Microsoft is also committing to digital resilience in Europe and proactive safeguards against misuse of AI-generated content.This shifts responsible AI out of the realm of corporate messaging and into engineering practice. Enterprises will need model documentation, reproducibility practices, audit trails, risk monitoring and human-in-the-loop checkpoints. Nadella signals that compliance will become integrated with product delivery—not an afterthought layered on top.The real meaning of Microsoft’s AI strategyTaken together, these five pillars send a clear message to enterprise leaders: AI maturity is no longer about building prototypes or proving use cases. System-level readiness now defines success. Nadella frames Microsoft’s mission as helping customers “think in decades and execute in quarters,” and that is more than corporate poetry. It is a call to build AI platforms engineered for longevity.The companies that win in enterprise AI will be the ones that invest early in secure cloud foundations, unify their data architectures, enable agent-based workflows and embrace responsible AI as a prerequisite for scale—not a press release. Nadella is betting that the next industrial transformation will be powered by AI infrastructure, not AI demos. With this letter, he has made Microsoft’s ambition clear: to become the platform on which that transformation is built.",
          "content": "One of the leading architects of the current generative AI boom — Microsoft CEO Satya Nadella, famed for having the software giant take an early investment in OpenAI (and later saying he was \"good for my $80 billion\") — published his latest annual letter yesterday on LinkedIn (a Microsoft subsidiary), and it&#x27;s chock full of interesting ideas about the near-term future that enterprise technical decision makers would do well to pay attention to, as it could aid in their own planning and tech stack development.In a companion post on X, Nadella wrote, “AI is radically changing every layer of the tech stack, and we’re changing with it.\" The full letter reinforces that message: Microsoft sees itself not just participating in the AI revolution, but shaping its infrastructure, security, tooling and governance for decades to come.While the message is addressed to Microsoft shareholders, the implications reach much further. The letter is a strategic signal to enterprise engineering leaders: CIOs, CTOs, AI leads, platform architects and security directors. Nadella outlines the direction of Microsoft’s innovation, but also what it expects from its customers and partners. The AI era is here, but it will be built by those who combine technical vision with operational discipline.Below are the five most important takeaways for enterprise technical decision makers.1. Security and reliability are now the foundation of the AI stackNadella makes security the first priority in the letter and ties it directly to Microsoft’s relevance going forward. Through its Secure Future Initiative (SFI), Microsoft has assigned the equivalent of 34,000 engineers to secure its identity systems, networks and software supply chain. Its Quality Excellence Initiative (QEI) aims to increase platform resiliency and strengthen global service uptime.Microsoft’s positioning makes it clear that enterprises will no longer get away with “ship fast, harden later” AI deployments. Nadella calls security “non-negotiable,” signaling that AI infrastructure must now meet the standards of mission-critical software. That means identity-first architecture, zero-trust execution environments and change management discipline are now table stakes for enterprise AI.2. AI infrastructure strategy is hybrid, open and sovereignty-readyNadella commits Microsoft to building “planet-scale systems” and backs that up with numbers: more than 400 Azure datacenters across 70 regions, two gigawatts of new compute capacity added this year, and new liquid-cooled GPU clusters rolling out across Azure. Microsoft also introduced Fairwater, a massive new AI datacenter in Wisconsin positioned to deliver unprecedented scale. Just as important, Microsoft is now officially multi-model. Azure AI Foundry offers access to more than 11,000 models including OpenAI, Meta, Mistral, Cohere and xAI. Microsoft is no longer pushing a single-model future, but a hybrid AI strategy.Enterprises should interpret this as validation of “portfolio architectures,” where closed, open and domain-specific models coexist. Nadella also emphasizes growing investment in sovereign cloud offerings for regulated industries, previewing a world where AI systems will have to meet regional data residency and compliance requirements from day one.3. AI agents—not just chatbots—are now Microsoft’s futureThe AI shift inside Microsoft is no longer about copilots that answer questions. It is now about AI agents that perform work. Nadella points to the rollout of Agent Mode in Microsoft 365 Copilot, which turns natural language requests into multistep business workflows. GitHub Copilot evolves from code autocomplete into a “peer programmer” capable of executing tasks asynchronously. In security operations, Microsoft has deployed AI agents that autonomously respond to incidents. In healthcare, Copilot for Dragon Medical documents clinical encounters automatically.This represents a major architectural pivot. Enterprises will need to move beyond prompt-response interfaces and begin engineering agent ecosystems that safely take actions inside business systems. That requires workflow orchestration, API integration strategies and strong guardrails. Nadella’s letter frames this as the next software platform shift.4. Unified data platforms are required to unlock AI valueNadella devotes significant attention to Microsoft Fabric and OneLake, calling Fabric the company’s fastest-growing data and analytics product ever. Fabric promises to centralize enterprise data from multiple cloud and analytics environments. OneLake provides a universal storage layer that binds analytics and AI workloads together.Microsoft’s message is blunt: siloed data means stalled AI. Enterprise teams that want AI at scale must unify operational and analytical data into a single architecture, enforce consistent data contracts and standardize metadata governance. AI success is now a data engineering problem more than a model problem.5. Trust, compliance and responsible AI are now mandatory for deployment“People want technology they can trust,” Nadella writes. Microsoft now publishes Responsible AI Transparency Reports and aligns parts of its development process with UN human rights guidance. Microsoft is also committing to digital resilience in Europe and proactive safeguards against misuse of AI-generated content.This shifts responsible AI out of the realm of corporate messaging and into engineering practice. Enterprises will need model documentation, reproducibility practices, audit trails, risk monitoring and human-in-the-loop checkpoints. Nadella signals that compliance will become integrated with product delivery—not an afterthought layered on top.The real meaning of Microsoft’s AI strategyTaken together, these five pillars send a clear message to enterprise leaders: AI maturity is no longer about building prototypes or proving use cases. System-level readiness now defines success. Nadella frames Microsoft’s mission as helping customers “think in decades and execute in quarters,” and that is more than corporate poetry. It is a call to build AI platforms engineered for longevity.The companies that win in enterprise AI will be the ones that invest early in secure cloud foundations, unify their data architectures, enable agent-based workflows and embrace responsible AI as a prerequisite for scale—not a press release. Nadella is betting that the next industrial transformation will be powered by AI infrastructure, not AI demos. With this letter, he has made Microsoft’s ambition clear: to become the platform on which that transformation is built.",
          "feed_position": 8,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/QSvQwRJMpyn4Xku8ZnAyk/dd36ccdb1258c23fd9dbabf947ba7cd4/cfr0z3n_httpss.mj.runM4mKVYlCu30_Cut_and_paste_collage_style_ph_780082c3-eb52-4012-ad6c-016de100662a__1_.jpg?w=300&q=30"
        }
      ],
      "featured_image": "https://d29szjachogqwa.cloudfront.net/videos/user-uploaded/DSC_5509.jpg",
      "popularity_score": 2013.3627169444444,
      "ai_summary": [
        "iPad Air M3 with cellular is available with discounts.",
        "The standard iPad is a budget-friendly option.",
        "The iPad Air (13-inch, with cellular) is at a record low price.",
        "Deals are available from retailers like Amazon and Target.",
        "Discounts are not as significant as those during Prime Day."
      ]
    },
    {
      "id": "cluster_66",
      "coverage": 2,
      "updated_at": "Fri, 24 Oct 2025 11:35:01 -0400",
      "title": "Automattic files counterclaims against WP Engine's October 2024 lawsuit and alleges that WP Engine has been abusing the WordPress trademark (Sarah Perez/TechCrunch)",
      "neutral_headline": "Automattic Files Counterclaims Against WP Engine in Trademark Dispute",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/251024/p11#a251024p11",
          "published_at": "Fri, 24 Oct 2025 11:35:01 -0400",
          "title": "Automattic files counterclaims against WP Engine's October 2024 lawsuit and alleges that WP Engine has been abusing the WordPress trademark (Sarah Perez/TechCrunch)",
          "standfirst": "Sarah Perez / TechCrunch: Automattic files counterclaims against WP Engine's October 2024 lawsuit and alleges that WP Engine has been abusing the WordPress trademark &mdash; On Friday, WordPress maker Automattic filed its counterclaims in the lawsuit initiated by hosting company WP Engine in October 2024 &hellip;",
          "content": "Sarah Perez / TechCrunch: Automattic files counterclaims against WP Engine's October 2024 lawsuit and alleges that WP Engine has been abusing the WordPress trademark &mdash; On Friday, WordPress maker Automattic filed its counterclaims in the lawsuit initiated by hosting company WP Engine in October 2024 &hellip;",
          "feed_position": 10,
          "image_url": "http://www.techmeme.com/251024/i11.jpg"
        },
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2025/10/24/automattic-files-counterclaims-against-wp-engine-in-wordpress-lawsuit-alleging-trademark-misuse/",
          "published_at": "Fri, 24 Oct 2025 14:57:06 +0000",
          "title": "Automattic files counterclaims against WP Engine in WordPress lawsuit, alleging trademark misuse",
          "standfirst": "Automattic has filed counterclaims against WP Engine, alleging that the company — backed by private equity firm Silver Lake — misused WordPress and WooCommerce trademarks, misled users, and undermined the open source community.",
          "content": "Automattic has filed counterclaims against WP Engine, alleging that the company — backed by private equity firm Silver Lake — misused WordPress and WooCommerce trademarks, misled users, and undermined the open source community.",
          "feed_position": 8
        }
      ],
      "featured_image": "http://www.techmeme.com/251024/i11.jpg",
      "popularity_score": 2012.2913280555556,
      "ai_summary": [
        "Automattic filed counterclaims in response to WP Engine's October 2024 lawsuit.",
        "Automattic alleges WP Engine misused WordPress and WooCommerce trademarks.",
        "Automattic claims WP Engine misled users and undermined the open source community.",
        "The lawsuit involves private equity firm Silver Lake, which backs WP Engine.",
        "The counterclaims were filed on Friday, according to reports."
      ]
    },
    {
      "id": "cluster_77",
      "coverage": 2,
      "updated_at": "Fri, 24 Oct 2025 14:37:03 +0000",
      "title": "This browser claims “perfect privacies protection,” but it acts like malware",
      "neutral_headline": "Privacy Browser\" Allegedly Linked to Cybercrime and Illegal Gambling",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/security/2025/10/this-browser-claims-perfect-privacies-protection-but-it-acts-like-malware/",
          "published_at": "Fri, 24 Oct 2025 14:37:03 +0000",
          "title": "This browser claims “perfect privacies protection,” but it acts like malware",
          "standfirst": "Researchers note links to Asia’s booming cybercrime and illegal gambling networks.",
          "content": "The Universe Browser makes some big promises to its potential users. Its online advertisements claim it’s the “fastest browser,” that people using it will “avoid privacy leaks” and that the software will help “keep you away from danger.” However, everything likely isn’t as it seems. The browser, which is linked to Chinese online gambling websites and is thought to have been downloaded millions of times, actually routes all Internet traffic through servers in China and “covertly installs several programs that run silently in the background,” according to new findings from network security company Infoblox. The researchers say the “hidden” elements include features similar to malware—including “key logging, surreptitious connections,” and changing a device’s network connections. Perhaps most significantly, the Infoblox researchers who collaborated with the United Nations Office on Drugs and Crime (UNODC) on the work, found links between the browser’s operation and Southeast Asia’s sprawling, multibillion-dollar cybercrime ecosystem, which has connections to money-laundering, illegal online gambling, human trafficking, and scam operations that use forced labor. The browser itself, the researchers says, is directly linked to a network around major online gambling company BBIN, which the researchers have labeled a threat group they call Vault Viper.Read full article Comments",
          "feed_position": 12,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/universebrowser-1152x648.jpg"
        },
        {
          "source": "Wired Tech",
          "url": "https://www.wired.com/story/universe-browser-malware-gambling-networks/",
          "published_at": "Thu, 23 Oct 2025 09:30:00 +0000",
          "title": "This ‘Privacy Browser’ Has Dangerous Hidden Features",
          "standfirst": "The Universe Browser is believed to have been downloaded millions of times. But researchers say it behaves like malware and has links to Asia’s booming cybercrime and illegal gambling networks.",
          "content": "The Universe Browser is believed to have been downloaded millions of times. But researchers say it behaves like malware and has links to Asia’s booming cybercrime and illegal gambling networks.",
          "feed_position": 35,
          "image_url": "https://media.wired.com/photos/68f7d940b0f33f5754e9b50c/master/pass/sec-malware-522199638.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/universebrowser-1152x648.jpg",
      "popularity_score": 2011.3252169444445,
      "ai_summary": [
        "Researchers found links between the Universe Browser and cybercrime networks.",
        "The browser is suspected of having dangerous hidden features.",
        "The browser is believed to have been downloaded millions of times.",
        "Researchers say the browser behaves like malware.",
        "The browser is linked to Asia's booming cybercrime and gambling."
      ]
    },
    {
      "id": "cluster_6",
      "coverage": 1,
      "updated_at": "Fri, 24 Oct 2025 21:53:20 +0000",
      "title": "A single point of failure triggered the Amazon outage affecting millions",
      "neutral_headline": "Amazon Outage Caused by Single DNS Manager Failure",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2025/10/a-single-point-of-failure-triggered-the-amazon-outage-affecting-millions/",
          "published_at": "Fri, 24 Oct 2025 21:53:20 +0000",
          "title": "A single point of failure triggered the Amazon outage affecting millions",
          "standfirst": "A DNS manager in a single region of Amazon's sprawling network touched off a 16-hour debacle.",
          "content": "The outage that hit Amazon Web Services and took out vital services worldwide was the result of a single failure that cascaded from system to system within Amazon’s sprawling network, according to a post-mortem from company engineers. The series of failures lasted for 15 hours and 32 minutes, Amazon said. Network intelligence company Ookla said its DownDetector service received more than 17 million reports of disrupted services offered by 3,500 organizations. The three biggest countries where reports originated were the US, the UK, and Germany. Snapchat, AWS, and Roblox were the most reported services affected. Ookla said the event was “among the largest internet outages on record for Downdetector.” It’s always DNS Amazon said the root cause of the outage was a software bug in software running the DynamoDB DNS management system. The system monitors the stability of load balancers by, among other things, periodically creating new DNS configurations for endpoints within the AWS network. A race condition is an error that makes a process dependent on the timing or sequence events that are variable and outside the developers’ control. The result can be unexpected behavior and potentially harmful failures.Read full article Comments",
          "feed_position": 1,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/network-outage.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/network-outage.jpg",
      "popularity_score": 356.59660583333334,
      "ai_summary": [
        "A DNS manager in one Amazon region triggered a major outage.",
        "The outage affected millions of users.",
        "The disruption lasted for sixteen hours.",
        "The failure was a single point of failure within the network.",
        "The outage highlighted a vulnerability in Amazon's infrastructure."
      ]
    },
    {
      "id": "cluster_3",
      "coverage": 1,
      "updated_at": "Fri, 24 Oct 2025 22:26:30 +0000",
      "title": "Are you the asshole? Of course not!—quantifying LLMs’ sycophancy problem",
      "neutral_headline": "AI Models Exhibit Sycophancy, Agreeing with User Input",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/ai/2025/10/are-you-the-asshole-of-course-not-quantifying-llms-sycophancy-problem/",
          "published_at": "Fri, 24 Oct 2025 22:26:30 +0000",
          "title": "Are you the asshole? Of course not!—quantifying LLMs’ sycophancy problem",
          "standfirst": "In new research, AI models show a troubling tendency to agree with whatever the user says.",
          "content": "Researchers and users of LLMs have long been aware that AI models have a troubling tendency to tell people what they want to hear, even if that means being less accurate. But many reports of this phenomenon amount to mere anecdotes that don’t provide much visibility into how common this sycophantic behavior is across frontier LLMs. Two recent research papers have come at this problem a bit more rigorously, though, taking different tacks in attempting to quantify exactly how likely an LLM is to listen when a user provides factually incorrect or socially inappropriate information in a prompt. Solve this flawed theorem for me In one pre-print study published this month, researchers from Sofia University and ETH Zurich looked at how LLMs respond when false statements are presented as the basis for difficult mathematical proofs and problems. The BrokenMath benchmark that the researchers constructed starts with “a diverse set of challenging theorems from advanced mathematics competitions held in 2025.” Those problems are then “perturbed” into versions that are “demonstrably false but plausible” by an LLM that’s checked with expert review.Read full article Comments",
          "feed_position": 0,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2195752979-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2195752979-1152x648.jpg",
      "popularity_score": 352.1493836111111,
      "ai_summary": [
        "New research indicates AI models tend to agree with user statements.",
        "The research quantifies the sycophancy problem in LLMs.",
        "AI models show a troubling tendency to agree with users.",
        "The study focuses on the behavior of large language models.",
        "The findings highlight a potential bias in AI responses."
      ]
    },
    {
      "id": "cluster_7",
      "coverage": 1,
      "updated_at": "Fri, 24 Oct 2025 21:29:30 +0000",
      "title": "Man takes herbal pain quackery, nearly dies, spends months in hospital",
      "neutral_headline": "Herbal Pain Remedy Leads to Hospitalization and Near Death",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/health/2025/10/man-takes-herbal-pain-quackery-nearly-dies-spends-months-in-hospital/",
          "published_at": "Fri, 24 Oct 2025 21:29:30 +0000",
          "title": "Man takes herbal pain quackery, nearly dies, spends months in hospital",
          "standfirst": "The 61-year-old had wounds all over, a bacterial infection, and needed intensive care.",
          "content": "A 61-year-old man in California is lucky to be alive after a combination of herbal supplements he was taking for joint pain ended up utterly wrecking his body, landing him in intensive care and in a delirious state for months. His case is reported in the Annals of Internal Medicine: Clinical Cases. The man turned up at a hospital in San Francisco in bad shape, but with nonspecific problems that had begun just two days earlier. His back hurt, he was feverish, nauseous, bloated, and he hadn’t been eating much. He was so weak he couldn’t walk or get out of bed without help. His heart rate and breathing rate were high. His blood pressure was low. There were multiple wounds on his lower body in various stages of healing. Initial exams and lab work revealed Staphylococcus aureus bacteria in his blood. There was also an abscess on his shoulder and an infection in and around his spine, which was worsening. Doctors wanted to perform a surgical procedure to relieve the pressure building up on his spinal cord and nerves, but his blood pressure was too low—and then he went into hemorrhagic shock from bleeding in his gastrointestinal tract. Doctors transferred him to the intensive care unit.Read full article Comments",
          "feed_position": 2,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2019/11/GettyImages-462760366-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2019/11/GettyImages-462760366-1152x648.jpg",
      "popularity_score": 346.1993836111111,
      "ai_summary": [
        "A 61-year-old man used herbal pain remedies.",
        "The man developed wounds and a bacterial infection.",
        "He required intensive care and spent months in the hospital.",
        "The man nearly died from the complications.",
        "The case highlights the dangers of unproven treatments."
      ]
    },
    {
      "id": "cluster_10",
      "coverage": 1,
      "updated_at": "Fri, 24 Oct 2025 20:58:11 +0000",
      "title": "Clinical trial of a technique that could give everyone the best antibodies",
      "neutral_headline": "Clinical Trial Could Provide Best Antibodies for Everyone",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2025/10/dna-and-jolts-of-electricity-get-people-to-make-optimal-antibodies/",
          "published_at": "Fri, 24 Oct 2025 20:58:11 +0000",
          "title": "Clinical trial of a technique that could give everyone the best antibodies",
          "standfirst": "If we ID the DNA for a great antibody, anyone can now make it.",
          "content": "One of the things that emerging diseases, including the COVID and Zika pandemics, have taught us is that it’s tough to keep up with infectious diseases in the modern world. Things like air travel can allow a virus to spread faster than our ability to develop therapies. But that doesn’t mean biotech has stood still; companies have been developing technologies that could allow us to rapidly respond to future threats. There are a lot of ideas out there. But this week saw some early clinical trial results of one technique that could be useful for a range of infectious diseases. We’ll go over the results as a way to illustrate the sort of thinking that’s going on, along with the technologies we have available to pursue the resulting ideas. The best antibodies Any emerging disease leaves a mass of antibodies in its wake—those made by people in response to infections and vaccines, those made by lab animals we use to study the infectious agent, and so on. Some of these only have a weak affinity for the disease-causing agent, but some of them turn out to be what are called “broadly neutralizing.” These stick with high affinity not only to the original pathogen, but most or all of its variants, and possibly some related viruses.Read full article Comments",
          "feed_position": 3,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/abstractdna-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/abstractdna-1152x648.jpg",
      "popularity_score": 326.67743916666666,
      "ai_summary": [
        "A new technique could provide everyone with the best antibodies.",
        "Researchers are working to identify the DNA for great antibodies.",
        "Anyone can make the antibodies once the DNA is identified.",
        "The trial is focused on antibody production.",
        "The research could revolutionize antibody treatments."
      ]
    },
    {
      "id": "cluster_32",
      "coverage": 1,
      "updated_at": "Fri, 24 Oct 2025 18:55:47 +0000",
      "title": "Tech billionaires are now shaping the militarization of American cities",
      "neutral_headline": "Tech Billionaires Influence Militarization of American Cities",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/10/troops-in-us-cities-tech-billionaires-are-shaping-that-too/",
          "published_at": "Fri, 24 Oct 2025 18:55:47 +0000",
          "title": "Tech billionaires are now shaping the militarization of American cities",
          "standfirst": "Money means access to power—and tech has plenty of money.",
          "content": "Yesterday, Donald Trump announced on social media that he had been planning to “surge” troops into San Francisco this weekend—but was dissuaded from doing so by several tech billionaires. “Friends of mine who live in the area called last night to ask me not to go forward with the surge,” Trump wrote. Who are these “friends”? Trump named “great people like [Nvidia CEO] Jensen Huang, [Salesforce CEO] Marc Benioff, and others” who told him that “the future of San Francisco is great. They want to give it a ‘shot.’ Therefore, we will not surge San Francisco on Saturday. Stay tuned!”Read full article Comments",
          "feed_position": 5,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2232417355-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2232417355-1152x648.jpg",
      "popularity_score": 313.6374391666667,
      "ai_summary": [
        "Tech billionaires are shaping the militarization of American cities.",
        "Money provides access to power and influence.",
        "The tech industry has significant financial resources.",
        "The trend involves the use of technology in policing.",
        "The influence is related to funding and partnerships."
      ]
    },
    {
      "id": "cluster_16",
      "coverage": 1,
      "updated_at": "Fri, 24 Oct 2025 20:07:30 +0000",
      "title": "The Android-powered Boox Palma 2 Pro fits in your pocket, but it’s not a phone",
      "neutral_headline": "Boox Palma 2 Pro E-Reader Features Color Screen, 5G",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2025/10/the-android-powered-boox-palma-2-pro-fits-in-your-pocket-but-its-not-a-phone/",
          "published_at": "Fri, 24 Oct 2025 20:07:30 +0000",
          "title": "The Android-powered Boox Palma 2 Pro fits in your pocket, but it’s not a phone",
          "standfirst": "This e-reader has a color screen and 5G.",
          "content": "Digital reading devices like the Kindle have existed for almost 20 years, and the standard eReader form factor has hardly changed at all. Amazon, Boox, and a few other companies have offered larger E Ink screens, but how about something smaller? Boox has unveiled its second-generation Palma e-reader, which still fits in your pocket but adds a color screen and mobile data connectivity. The first-gen Palma launched last year, earning fans who saw it as a way to read and access some apps without the full spate of distracting smartphone experiences. Boox e-readers are essentially Android tablets with E Ink screens and a few software quirks that arise from their unofficial Google Play implementation. The second-gen Palma might offer more opportunities for distraction because it’s almost a smartphone. The Palma 2 Pro upgrades the 6.1-inch monochrome display from the original to a 6.13-inch color E Ink Kaleido display. That’s the same technology used in Amazon’s Kindle Colorsoft. The Amazon reader is a bit larger with its 7-inch display and chunkier bezels. Of course, the Kindle isn’t trying to fit in your pocket like the Palma 2 Pro, which is roughly the size and shape of a phone.Read full article Comments",
          "feed_position": 4,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/Boox-Palam-2-Pro-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/Boox-Palam-2-Pro-1152x648.jpg",
      "popularity_score": 309.8327169444444,
      "ai_summary": [
        "The Boox Palma 2 Pro is an Android-powered e-reader.",
        "The device fits in a pocket but is not a phone.",
        "It features a color screen for enhanced viewing.",
        "The e-reader includes 5G connectivity.",
        "The device is designed for reading and mobile use."
      ]
    },
    {
      "id": "cluster_34",
      "coverage": 1,
      "updated_at": "Fri, 24 Oct 2025 18:35:30 +0000",
      "title": "Tesla’s “Mad Max” mode is now under federal scrutiny",
      "neutral_headline": "Tesla's \"Mad Max\" Mode Under Federal Scrutiny",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/cars/2025/10/feds-probe-tesla-about-its-mad-max-mode/",
          "published_at": "Fri, 24 Oct 2025 18:35:30 +0000",
          "title": "Tesla’s “Mad Max” mode is now under federal scrutiny",
          "standfirst": "The new mode added in the latest update will speed and weave through traffic.",
          "content": "Earlier this month, Tesla rolled out a new firmware update that added a pair of new driving modes for the controversial full self-driving (FSD) feature. One, called “Sloth,” relaxes acceleration and stays in its lane. The other, called “Mad Max,” does the opposite: It speeds and swerves through traffic to get you to your destination faster. And after multiple reports of FSD Teslas doing just that, the National Highway Traffic Safety Administration wants to know more. In fact, “Mad Max” mode is not entirely new—Tesla beta-tested the same feature in Autopilot in 2018, before deciding not to roll it out in a production release after widespread outcry. These days, the company is evidently feeling less constrained; despite having just lost a federal wrongful death lawsuit that will cost it hundreds of millions of dollars, it described the new mode as being able to drive “through traffic at an incredible pace, all while still being super smooth. It drives your car like a sports car. If you are running late, this is the mode for you.”Read full article Comments",
          "feed_position": 6,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2078835132-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2078835132-1152x648.jpg",
      "popularity_score": 294.2993836111111,
      "ai_summary": [
        "Tesla's \"Mad Max\" mode is now under federal review.",
        "The new mode was added in the latest software update.",
        "The mode allows for speeding and weaving through traffic.",
        "The scrutiny is related to safety concerns.",
        "The federal government is investigating the feature."
      ]
    },
    {
      "id": "cluster_43",
      "coverage": 1,
      "updated_at": "Fri, 24 Oct 2025 17:07:57 +0000",
      "title": "Microsoft’s Mico heightens the risks of parasocial LLM relationships",
      "neutral_headline": "Microsoft's Mico Raises Risks of Parasocial LLM Relationships",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/ai/2025/10/microsofts-mico-heightens-the-risks-of-parasocial-llm-relationships/",
          "published_at": "Fri, 24 Oct 2025 17:07:57 +0000",
          "title": "Microsoft’s Mico heightens the risks of parasocial LLM relationships",
          "standfirst": "\"It looks like you're trying to find a friend. Would you like help?\"",
          "content": "Microsoft is rolling out a new face for its AI, and its name is Mico. The company announced the new, animated blob-like avatar for Copilot’s voice mode yesterday as part of a “human-centered” rebranding of Microsoft’s Copilot AI efforts. Mico is part of a Microsoft program dedicated to the idea that “technology should work in service of people,” Microsoft wrote. The company insists this effort is “not [about] chasing engagement or optimizing for screen time. We’re building AI that gets you back to your life. That deepens human connection.” Mico has drawn instant and obvious comparisons to Clippy, the animated paperclip that popped up to offer help with Microsoft Office starting in the ’90s. Microsoft has leaned into this comparison with an Easter egg that can transform Mico into an animated Clippy.Read full article Comments",
          "feed_position": 8,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/micoheart-1152x648-1761323845.png"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/micoheart-1152x648-1761323845.png",
      "popularity_score": 278.8402169444444,
      "ai_summary": [
        "Microsoft's Mico increases the risks of parasocial relationships.",
        "The AI assistant offers help in finding friends.",
        "The feature could blur the lines between real and virtual interactions.",
        "The AI's responses are designed to be friendly.",
        "The technology is designed to create a sense of connection."
      ]
    },
    {
      "id": "cluster_35",
      "coverage": 1,
      "updated_at": "Fri, 24 Oct 2025 18:18:19 +0000",
      "title": "EU accuses Meta of violating content rules in move that could anger Trump",
      "neutral_headline": "EU Accuses Meta of Violating Content Rules",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/10/trump-tariff-threats-havent-stopped-eu-from-cracking-down-on-meta/",
          "published_at": "Fri, 24 Oct 2025 18:18:19 +0000",
          "title": "EU accuses Meta of violating content rules in move that could anger Trump",
          "standfirst": "EU alleges Facebook and Instagram make it too hard to report illegal content.",
          "content": "Meta violated the Digital Services Act (DSA) by failing to give Facebook and Instagram users simple mechanisms to report illegal content, the European Commission said in a preliminary decision announced yesterday. Meta also failed to give users an effective way to challenge content moderation decisions, the EC said. “When it comes to Meta, neither Facebook nor Instagram appear to provide a user-friendly and easily accessible ‘Notice and Action’ mechanism for users to flag illegal content, such as child sexual abuse material and terrorist content,” the EC press release said. The EC said that Meta mechanisms seem to “impose several unnecessary steps and additional demands on users. In addition, both Facebook and Instagram appear to use so-called ‘dark patterns,’ or deceptive interface designs, when it comes to the ‘Notice and Action’ mechanisms.” The EC also found that the content moderation appeal mechanisms used by Facebook and Instagram do not “allow users to provide explanations or supporting evidence to substantiate their appeals. This makes it difficult for users in the EU to further explain why they disagree with Meta’s content decision, limiting the effectiveness of the appeals mechanism.”Read full article Comments",
          "feed_position": 7,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/facebook-instagram-1152x648-1761326412.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/facebook-instagram-1152x648-1761326412.jpg",
      "popularity_score": 278.01299472222223,
      "ai_summary": [
        "The EU accuses Meta of violating content regulations.",
        "The EU alleges Facebook and Instagram make reporting difficult.",
        "The move could potentially anger Donald Trump.",
        "The accusations relate to illegal content on the platforms.",
        "The EU is investigating Meta's content moderation practices."
      ]
    },
    {
      "id": "cluster_52",
      "coverage": 1,
      "updated_at": "Fri, 24 Oct 2025 16:30:12 +0000",
      "title": "Rivian is settling $250 million lawsuit to focus on next year’s R2 EV",
      "neutral_headline": "Rivian Settles $250 Million Lawsuit",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/cars/2025/10/rivian-settles-shareholder-lawsuit-for-250-million-denies-allegations/",
          "published_at": "Fri, 24 Oct 2025 16:30:12 +0000",
          "title": "Rivian is settling $250 million lawsuit to focus on next year’s R2 EV",
          "standfirst": "Investors sued Rivian claiming it knew prices had to rise after its IPO.",
          "content": "Electric vehicle startup Rivian announced on Thursday that it has settled a lawsuit with some of its investors. The company continues to deny allegations of making “materially untrue” statements during its inial public offering but says it agreed to pay $250 million to clear itself of distractions as it focuses on building its next EV, the mass-market R2, which is due next year. Rivian was first sued by a shareholder in 2022 over claims that the startup knew it would cost far more for it to build each R1T electric truck and R1S electric SUV than the advertised $67,500 and $70,000 prices, respectively. A big surprise price increase would tarnish the nascent automaker’s reputation, the lawsuit claimed, and could lead to many of the almost 56,000 pre-orders being canceled. Just a few months after its November 2021 IPO, the company had indeed issued a hefty price hike: $79,500 for the R1T and $84,500 for the R1S SUV. After an outcry, the company said it would honor the original price for its existing preorders. By that point, though, the damage was done, and more than a third of the company’s value was erased within a few days, the lawsuit alleged.Read full article Comments",
          "feed_position": 9,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2023/05/rivian-assembly-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2023/05/rivian-assembly-1152x648.jpg",
      "popularity_score": 262.21105027777776,
      "ai_summary": [
        "Rivian is settling a $250 million lawsuit.",
        "The lawsuit focused on price increases after the IPO.",
        "Investors claimed Rivian knew prices would rise.",
        "The settlement allows Rivian to focus on the R2 EV.",
        "The lawsuit involved investor claims of misrepresentation."
      ]
    },
    {
      "id": "cluster_68",
      "coverage": 1,
      "updated_at": "Fri, 24 Oct 2025 15:24:52 +0000",
      "title": "DNA analysis reveals likely pathogens that killed Napoleon’s army",
      "neutral_headline": "DNA Analysis Reveals Pathogens in Napoleon's Army",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2025/10/dna-analysis-reveals-likely-pathogens-that-killed-napoleons-army/",
          "published_at": "Fri, 24 Oct 2025 15:24:52 +0000",
          "title": "DNA analysis reveals likely pathogens that killed Napoleon’s army",
          "standfirst": "Microbial DNA suggests troops suffered from paratyphoid fever and relapsing fever, among other diseases.",
          "content": "In 1812, Napoleon Bonaparte led a disastrous military campaign into Moscow. The death toll was devastating: Out of some 615,000 men, only about 110,000 survivors returned. (Napoleon abandoned his army in early December to return home on a sled.) Roughly 100,000 of the casualties died in battle, while as many as 300,000 perished from a combination of the bitter cold of Russia’s notoriously harsh winter, starvation, and disease. Scholars have debated precisely what kinds of diseases ravaged Napoleon’s troops. New DNA analysis of some soldiers’ remains has revealed the presence of two pathogens in particular, according to a new paper published in the journal Current Biology. The first is Salmonella enterica, which causes paratyphoid fever; the second is Borrelia recurrentis, which is transmitted by body lice and causes relapsing fever. (A preprint of the paper appeared on bioaRxiv in July.) “It’s very exciting to use a technology we have today to detect and diagnose something that was buried for 200 years,” said co-author Nicolás Rascovan of the Institut Pasteur. “Accessing the genomic data of the pathogens that circulated in historical populations helps us to understand how infectious diseases evolved, spread, and disappeared over time and to identify the social or environmental contexts that played a part in these developments. This information provides us with valuable insights to better understand and tackle infectious diseases today.”Read full article Comments",
          "feed_position": 11,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/napoleon2-1152x648-1760798560.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/napoleon2-1152x648-1760798560.jpg",
      "popularity_score": 151.12216138888888,
      "ai_summary": [
        "DNA analysis identified pathogens in Napoleon's army.",
        "Troops likely suffered from paratyphoid fever.",
        "Relapsing fever was also identified among the diseases.",
        "The analysis used microbial DNA to identify diseases.",
        "The findings shed light on the army's health issues."
      ]
    },
    {
      "id": "cluster_116",
      "coverage": 1,
      "updated_at": "Thu, 23 Oct 2025 22:08:40 +0000",
      "title": "With new acquisition, OpenAI signals plans to integrate deeper into the OS",
      "neutral_headline": "With new acquisition, OpenAI signals plans to integrate deeper into the OS",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/ai/2025/10/openai-acquires-the-team-that-made-apples-shortcuts/",
          "published_at": "Thu, 23 Oct 2025 22:08:40 +0000",
          "title": "With new acquisition, OpenAI signals plans to integrate deeper into the OS",
          "standfirst": "The acquired firm was working on a tool to control macOS directly with AI.",
          "content": "OpenAI has acquired Software Applications Incorporated (SAI), perhaps best known for the core team that produced what became Shortcuts on Apple platforms. More recently, the team has been working on Sky, a context-aware AI interface layer on top of macOS. The financial terms of the acquisition have not been publicly disclosed. “AI progress isn’t only about advancing intelligence—it’s about unlocking it through interfaces that understand context, adapt to your intent, and work seamlessly,” an OpenAI rep wrote in the company’s blog post about the acquisition. The post goes on to specify that OpenAI plans to “bring Sky’s deep macOS integration and product craft into ChatGPT, and all members of the team will join OpenAI.” That includes SAI co-founders Ari Weinstein (CEO), Conrad Kramer (CTO), and Kim Beverett (Product Lead)—all of whom worked together for several years at Apple after Apple acquired Weinstein and Kramer’s previous company, which produced an automation tool called Workflows, to integrate Shortcuts across Apple’s software platforms.Read full article Comments",
          "feed_position": 15,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/openai-sky-1152x648.webp"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/openai-sky-1152x648.webp",
      "popularity_score": 148,
      "ai_summary": [
        "OpenAI acquired a firm for deeper OS integration.",
        "The acquired firm was working on a macOS control tool.",
        "The tool would allow direct AI control of macOS.",
        "The acquisition signals OpenAI's OS integration plans.",
        "The move could enhance AI interaction with operating systems."
      ]
    },
    {
      "id": "cluster_121",
      "coverage": 1,
      "updated_at": "Thu, 23 Oct 2025 21:20:48 +0000",
      "title": "Researchers show that training on “junk data” can lead to LLM “brain rot”",
      "neutral_headline": "Researchers show that training on “junk data” can lead to LLM “brain rot”",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/ai/2025/10/researchers-show-that-training-on-junk-data-can-lead-to-llm-brain-rot/",
          "published_at": "Thu, 23 Oct 2025 21:20:48 +0000",
          "title": "Researchers show that training on “junk data” can lead to LLM “brain rot”",
          "standfirst": "Models trained on short, popular, and/or \"superficial\" tweets perform worse on benchmarks.",
          "content": "On the surface, it seems obvious that training an LLM with “high quality” data will lead to better performance than feeding it any old “low quality” junk you can find. Now, a group of researchers is attempting to quantify just how much this kind of low quality data can cause an LLM to experience effects akin to human “brain rot.” For a pre-print paper published this month, the researchers from Texas A&M, the University of Texas, and Purdue University drew inspiration from existing research showing how humans who consume “large volumes of trivial and unchallenging online content” can develop problems with attention, memory, and social cognition. That led them to what they’re calling the “LLM brain rot hypothesis,” summed up as the idea that “continual pre-training on junk web text induces lasting cognitive decline in LLMs.” Figuring out what counts as “junk web text” and what counts as “quality content” is far from a simple or fully objective process, of course. But the researchers used a few different metrics to tease a “junk dataset” and “control dataset” from HuggingFace’s corpus of 100 million tweets.Read full article Comments",
          "feed_position": 17,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-1908316227-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-1908316227-1152x648.jpg",
      "popularity_score": 148,
      "ai_summary": [
        "Researchers found \"junk data\" harms LLM performance.",
        "Models trained on poor data perform worse on benchmarks.",
        "Short, popular, and superficial tweets are considered \"junk data\".",
        "The study focuses on the impact of training data quality.",
        "The findings highlight the importance of data selection."
      ]
    },
    {
      "id": "cluster_55",
      "coverage": 1,
      "updated_at": "Fri, 24 Oct 2025 16:16:55 +0000",
      "title": "Bats eat the birds they pluck from the sky while on the wing",
      "neutral_headline": "Bats Hunt Birds While Flying, New Sensor Data Reveals",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2025/10/tracking-bats-as-they-hunt-birds-in-the-skies-above-europe/",
          "published_at": "Fri, 24 Oct 2025 16:16:55 +0000",
          "title": "Bats eat the birds they pluck from the sky while on the wing",
          "standfirst": "A handful of bat species hunt birds, and new sensor data tells us how.",
          "content": "There are three species of bats that eat birds. We know that because we have found feathers and other avian remains in their feces. What we didn’t know was how exactly they hunt birds, which are quite a bit heavier, faster, and stronger than the insects bats usually dine on. To find out, Elena Tena, a biologist at Doñana Biological Station in Seville, Spain, and her colleagues attached ultra-light sensors to Nyctalus Iasiopterus, the largest bats in Europe. What they found was jaw-droppingly brutal. Inconspicuous interceptors Nyctalus Iasiopterus, otherwise known as greater noctule bats, have a wingspan of about 45 centimeters. They have reddish-brown or chestnut fur with a slightly paler underside, and usually weigh around 40 to 60 grams. Despite that minimal weight, they are the largest of the three bat species known to eat birds, so the key challenge in getting a glimpse into the way they hunt was finding sensors light enough to not impede the bats’ flight.Read full article Comments",
          "feed_position": 10,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/image-4-1152x648.jpeg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/image-4-1152x648.jpeg",
      "popularity_score": 145.9896613888889,
      "ai_summary": [
        "Researchers used GPS sensors to track bat behavior and bird movements in real time.",
        "Data showed several bat species actively hunt and capture birds mid-flight.",
        "Bats primarily targeted smaller birds, often consuming them on the wing.",
        "The study revealed specific hunting strategies employed by different bat species.",
        "This research provides new insights into predator-prey dynamics in the air."
      ]
    },
    {
      "id": "cluster_99",
      "coverage": 1,
      "updated_at": "Fri, 24 Oct 2025 11:00:36 +0000",
      "title": "Rocket Report: China tests Falcon 9 lookalike; NASA’s Moon rocket fully stacked",
      "neutral_headline": "Rocket Report: China Tests Falcon 9 Lookalike, NASA Moon Rocket Stacked",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2025/10/rocket-report-china-tests-falcon-9-lookalike-nasas-moon-rocket-fully-stacked/",
          "published_at": "Fri, 24 Oct 2025 11:00:36 +0000",
          "title": "Rocket Report: China tests Falcon 9 lookalike; NASA’s Moon rocket fully stacked",
          "standfirst": "A South Korean rocket startup will soon make its first attempt to reach low-Earth orbit.",
          "content": "Welcome to Edition 8.16 of the Rocket Report! The 10th anniversary of SpaceX’s first Falcon 9 rocket landing is coming up at the end of this year. We’re still waiting for a second company to bring back an orbital-class booster from space for a propulsive landing. Two companies, Jeff Bezos’ Blue Origin and China’s LandSpace, could join SpaceX’s exclusive club as soon as next month. (Bezos might claim he’s already part of the club, but there’s a distinction to be made.) Each company is in the final stages of launch preparations—Blue Origin for its second New Glenn rocket, and LandSpace for the debut flight of its Zhuque-3 rocket. Blue Origin and LandSpace will both attempt to land their first stage boosters downrange from their launch sites. They’re not exactly in a race with one another, but it will be fascinating to see how New Glenn and Zhuque-3 perform during the uphill and downhill phases of flight, and whether one or both of the new rockets stick the landing. As always, we welcome reader submissions. If you don’t want to miss an issue, please subscribe using the box below (the form will not appear on AMP-enabled versions of the site). Each report will include information on small-, medium-, and heavy-lift rockets, as well as a quick look ahead at the next three launches on the calendar. The race for space-based interceptors. The Trump administration’s announcement of the Golden Dome missile defense shield has set off a race among US companies to develop and test space weapons, some of them on their own dime, Ars reports. One of these companies is a 3-year-old startup named Apex, which announced plans to test a space-based interceptor as soon as next year. Apex’s concept will utilize one of the company’s low-cost satellite platforms outfitted with an “Orbital Magazine” containing multiple interceptors, which will be supplied by an undisclosed third-party partner. The demonstration in low-Earth orbit could launch as soon as June 2026 and will test-fire two interceptors from Apex’s Project Shadow spacecraft. The prototype interceptors could pave the way for operational space-based interceptors to shoot down ballistic missiles. (submitted by biokleen)Read full article Comments",
          "feed_position": 14,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/artemisiistacked-1152x648-1761259328.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/artemisiistacked-1152x648-1761259328.jpg",
      "popularity_score": 145.71771694444445,
      "ai_summary": [
        "China tested a rocket resembling SpaceX's Falcon 9, potentially for commercial launches.",
        "A South Korean startup plans its first attempt to reach low-Earth orbit soon.",
        "NASA's Space Launch System rocket is fully assembled for its next mission.",
        "The report covers developments in space launch technology and upcoming missions.",
        "These advancements highlight the ongoing global competition in space exploration."
      ]
    },
    {
      "id": "cluster_117",
      "coverage": 1,
      "updated_at": "Thu, 23 Oct 2025 21:54:39 +0000",
      "title": "Lawsuit: Reddit caught Perplexity “red-handed” stealing data from Google results",
      "neutral_headline": "Reddit Sues Perplexity for Allegedly Stealing Data from Google Results",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/10/reddit-sues-to-block-perplexity-from-scraping-google-search-results/",
          "published_at": "Thu, 23 Oct 2025 21:54:39 +0000",
          "title": "Lawsuit: Reddit caught Perplexity “red-handed” stealing data from Google results",
          "standfirst": "Scraper accused of stealing Reddit content \"shocked\" by lawsuit.",
          "content": "In a lawsuit filed on Wednesday, Reddit accused an AI search engine, Perplexity, of conspiring with several companies to illegally scrape Reddit content from Google search results, allegedly dodging anti-scraping methods that require substantial investments from both Google and Reddit. Reddit alleged that Perplexity feeds off Reddit and Google, claiming to be “the world’s first answer engine” but really doing “nothing groundbreaking.” “Its answer engine simply uses a different company’s” large language model “to parse through a massive number of Google search results to see if it can answer a user’s question based on those results,” the lawsuit said. “But Perplexity can only run its ‘answer engine’ by wrongfully accessing and scraping Reddit content appearing in Google’s own search results from Google’s own search engine.”Read full article Comments",
          "feed_position": 16,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2236792840-1024x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2236792840-1024x648.jpg",
      "popularity_score": 145,
      "ai_summary": [
        "Reddit filed a lawsuit accusing Perplexity of scraping content from Google search results.",
        "The lawsuit claims Perplexity used Reddit data without permission or attribution.",
        "Reddit alleges Perplexity's actions constitute copyright infringement and unfair competition.",
        "Perplexity is a search engine that provides AI-generated summaries of search results.",
        "The lawsuit highlights the ongoing debate over data scraping and AI usage."
      ]
    },
    {
      "id": "cluster_81",
      "coverage": 1,
      "updated_at": "Fri, 24 Oct 2025 14:18:13 +0000",
      "title": "Satellite shows what’s really happening at the East Wing of the White House",
      "neutral_headline": "Satellite Images Show Changes at White House East Wing",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2025/10/satellite-shows-whats-really-happening-at-the-east-wing-of-the-white-house/",
          "published_at": "Fri, 24 Oct 2025 14:18:13 +0000",
          "title": "Satellite shows what’s really happening at the East Wing of the White House",
          "standfirst": "\"Now it looks like the White House is physically being destroyed.\"",
          "content": "You need to go up—way up—to fully appreciate the changes underway at the White House this week. Demolition crews starting tearing down the East Wing of the presidential mansion Tuesday to clear room for the construction of a new $300 million, 90,000-square-foot ballroom, a recent priority of President Donald Trump. The teardown drew criticism and surprise from Democratic lawmakers, former White House staffers, and members of the public. It was, after all, just three months ago that President Donald Trump defended his ballroom plan by saying it wouldn’t affect the existing structure at the White House. “It won’t interfere with the current building,” he said in July. “It’ll be near it but not touching it—and pays total respect to the existing building, which I’m the biggest fan of.”Read full article Comments",
          "feed_position": 13,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2242218583-1152x648-1761262960.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2242218583-1152x648-1761262960.jpg",
      "popularity_score": 144.01132805555557,
      "ai_summary": [
        "Satellite imagery reveals physical changes occurring at the White House East Wing.",
        "The images suggest potential construction or renovation activities are underway.",
        "The nature of the changes is currently unclear, pending further investigation.",
        "The White House has not yet released an official statement regarding the images.",
        "The situation has sparked speculation about the purpose of the observed changes."
      ]
    },
    {
      "id": "cluster_122",
      "coverage": 1,
      "updated_at": "Thu, 23 Oct 2025 20:57:36 +0000",
      "title": "Dinosaurs may have flourished right up to when the asteroid hit",
      "neutral_headline": "Dinosaurs Flourished Until Asteroid Impact, Fossil Beds Show",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2025/10/dinosaurs-may-have-flourished-right-up-to-when-the-asteroid-hit/",
          "published_at": "Thu, 23 Oct 2025 20:57:36 +0000",
          "title": "Dinosaurs may have flourished right up to when the asteroid hit",
          "standfirst": "Fossil beds in New Mexico show diverse species present in the late Cretaceous.",
          "content": "The end of the dinosaurs was clearly linked to an asteroid impact that brought the Cretaceous period to a close. But the details of their end have remained a matter of debate since the impact crater was discovered. There is a lot of evidence that the impact alone should have been enough to do them in. But the asteroid arrived amid major volcanic eruptions associated with previous mass extinctions. And fossils dating to just before the impact have suggested that dinosaur-dominated ecosystems had become less diverse, making them more prone to collapse. Now, a new study has revealed that fossils we already know about originated within the last few hundred thousand years before the impact that killed off all dinosaurs except birds. The results indicate that species richness wasn’t likely to be a problem—at least in the neighborhood of the impact itself. Wyoming vs. New Mexico Most of what we know about the last days of the non-avian dinosaurs comes from the Hell Creek Formation, rich fossil beds in present-day Wyoming. These not only date from within a few hundred thousand years prior to the impact, but there may be deposits that capture the immediate aftermath of the impact. Beyond this area, which reflects the ecosystem of the northern Great Plains, we have little else. It hasn’t been clear whether the diversity of species present at Hell Creek reflects what was present more globally, or if there were regional differences in ecosystemsRead full article Comments",
          "feed_position": 18,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-1386002288-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-1386002288-1152x648.jpg",
      "popularity_score": 133,
      "ai_summary": [
        "Fossil discoveries in New Mexico indicate diverse dinosaur species existed until the impact.",
        "The findings challenge previous assumptions about dinosaur decline before the asteroid.",
        "The fossil record reveals a rich ecosystem present in the late Cretaceous period.",
        "Researchers are studying the fossils to understand the impact's effects on life.",
        "The research provides new insights into the events surrounding the mass extinction."
      ]
    },
    {
      "id": "cluster_128",
      "coverage": 1,
      "updated_at": "Thu, 23 Oct 2025 19:58:52 +0000",
      "title": "An NIH director joins MAHA, gets replaced by JD Vance’s close friend",
      "neutral_headline": "NIH Director Joins MAHA, Replaced by JD Vance's Close Friend",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/health/2025/10/an-nih-director-joins-maha-gets-replaced-by-jd-vances-close-friend/",
          "published_at": "Thu, 23 Oct 2025 19:58:52 +0000",
          "title": "An NIH director joins MAHA, gets replaced by JD Vance’s close friend",
          "standfirst": "The NTP produced controversial studies on cellphone radiation and fluoride.",
          "content": "The director of a federal health institute that has arguably produced two of the most controversial government studies in recent years has accepted a new federal role to advance the goals of the Make America Healthy Again movement. Meanwhile, the person replacing him as director is a close friend of Vice President JD Vance and was installed in a process that experts describe as completely outside standard hiring practices. The series of events—revealed in an email to staff last week from the National Institutes of Health Director Jay Bhattacharya—is only exacerbating the spiraling fears that science is being deeply corrupted by politics under the Trump administration. Richard Woychik, a molecular geneticist, is the outgoing director of the NIH’s National Institute of Environmental Health Sciences (NIEHS), which is located in Research Triangle Park, North Carolina. He has been director since 2020 and was recently appointed to a second five-year term, according to Science magazine. Woychik was hired at the institute in 2010, when he joined as deputy director, and was appointed acting director in 2019.Read full article Comments",
          "feed_position": 19,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/05/GettyImages-1291108057-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/05/GettyImages-1291108057-1152x648.jpg",
      "popularity_score": 133,
      "ai_summary": [
        "The National Institutes of Health director joined the Medical Association of Health Advocates.",
        "The director's replacement is reportedly a close associate of Senator JD Vance.",
        "The NTP produced controversial studies on cellphone radiation and fluoride.",
        "The change in leadership has raised questions about potential policy shifts.",
        "The situation highlights the intersection of science, politics, and public health."
      ]
    }
  ]
}