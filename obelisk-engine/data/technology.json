{
  "updated_at": "2025-10-18T23:16:45.171Z",
  "clusters": [
    {
      "id": "cluster_11",
      "coverage": 2,
      "updated_at": "Sat, 18 Oct 2025 13:25:30 -0400",
      "title": "WhatsApp updates its Business API terms to ban general-purpose chatbots starting January 15, 2026, affecting WhatsApp assistants of OpenAI, Perplexity, others (Ivan Mehta/TechCrunch)",
      "neutral_headline": "Ivan Mehta / TechCrunch: WhatsApp updates its Business API terms to ban general-purpose chatbots starting January 15, 2026, affecting WhatsApp assistants of OpenAI, Perplexity, others &mdash; Meta-owned chat app WhatsApp changed its business API policy this week to ban general-purpose chatbots from its platform",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/251018/p11#a251018p11",
          "published_at": "Sat, 18 Oct 2025 13:25:30 -0400",
          "title": "WhatsApp updates its Business API terms to ban general-purpose chatbots starting January 15, 2026, affecting WhatsApp assistants of OpenAI, Perplexity, others (Ivan Mehta/TechCrunch)",
          "standfirst": "Ivan Mehta / TechCrunch: WhatsApp updates its Business API terms to ban general-purpose chatbots starting January 15, 2026, affecting WhatsApp assistants of OpenAI, Perplexity, others &mdash; Meta-owned chat app WhatsApp changed its business API policy this week to ban general-purpose chatbots from its platform.",
          "content": "Ivan Mehta / TechCrunch: WhatsApp updates its Business API terms to ban general-purpose chatbots starting January 15, 2026, affecting WhatsApp assistants of OpenAI, Perplexity, others &mdash; Meta-owned chat app WhatsApp changed its business API policy this week to ban general-purpose chatbots from its platform.",
          "feed_position": 2,
          "image_url": "http://www.techmeme.com/251018/i11.jpg"
        },
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2025/10/18/whatssapp-changes-its-terms-to-bar-general-purpose-chatbots-from-its-platform/",
          "published_at": "Sat, 18 Oct 2025 14:12:14 +0000",
          "title": "WhatsApp changes its terms to bar general purpose chatbots from its platform",
          "standfirst": "WhatsApp is banning general purpose chatbots from using its Business API",
          "content": "WhatsApp is banning general purpose chatbots from using its Business API",
          "feed_position": 6
        }
      ],
      "featured_image": "http://www.techmeme.com/251018/i11.jpg",
      "popularity_score": 2014.1457858333333,
      "ai_summary": [
        "WhatsApp is updating its Business API terms.",
        "The update bans general-purpose chatbots from the platform.",
        "The ban takes effect starting January 15, 2026.",
        "The change affects chatbots like those from OpenAI and Perplexity.",
        "Meta, the owner of WhatsApp, made the policy change this week."
      ]
    },
    {
      "id": "cluster_41",
      "coverage": 2,
      "updated_at": "Sat, 18 Oct 2025 11:00:00 +0000",
      "title": "A spooky NES platformer, more N++ and other new indie games worth checking out",
      "neutral_headline": "Indie Games Roundup: New Releases and Steam Next Fest",
      "items": [
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/gaming/a-spooky-nes-platformer-more-n-and-other-new-indie-games-worth-checking-out-110000259.html",
          "published_at": "Sat, 18 Oct 2025 11:00:00 +0000",
          "title": "A spooky NES platformer, more N++ and other new indie games worth checking out",
          "standfirst": "Welcome to our latest roundup of what's going on in the indie game space. I've reluctantly paused Ball x Pit for long enough to share some neat new releases and more details on upcoming games — some of which are arriving very soon. We've got a notable update for a classic as well. Steam Next Fest is taking place at the minute, and you still have until Monday to join in by checking out some of the many, many demos that have gone live for the event. Thanks partially (okay, almost entirely) to being unable to escape Ball x Pit, I've only tried a fewm Next Fest demos so far. I'm a fan of Aerial_Knight's Never Yield and after last year's sequel, it's cool to see solo developer Neil Jones (aka Aerial_Knight) trying something totally different. Aerial_Knight’s DropShot is a skydiving first-person shooter with finger guns and dragons. It’s a single-player game in which the aim is to take out your opponents and reach the ground first. Like Jones' previous games, it's stylish and fast-paced. I'm planning to check out the full game when it arrives down the line.It certainly helps to be a fast, accurate typer when you put words together for a living, but I wasn't quick or precise enough to win any rounds in the Final Sentence demo. This is a battle royale for up to 100 players in which you're at a typewriter and have to bash out sentences (or other strings of letters, numbers and symbols) in a race to the finish. If you run out of time, make too many mistakes or don't win, it's lights out, courtesy of the masked figure with a revolver who’s standing in front of you. There are some nice touches here. Having to type out the rules in the first few rounds is a clever idea on the part of developer Button Mash. I haven't won a round myself yet (I finished in second place a couple of times), but watched some streamers play. It's very funny when the winning player flips the bird at the guy holding a revolver in front of them. Final Sentence is coming to Steam later this year. Maybe I'll have learned how to spell \"sphinx\" by then.There are a few other Next Fest demos I'd like to try this weekend, namely:Crashout Crew (Overcooked-style co-op chaos with forklifts)Reanimal (co-op horror from the team behind Little Nightmares and Little Nightmares 2)Slots & Daggers (as if I need another slot-machine-based roguelite in my life right now) The Last Caretaker (sci-fi survival)Goodnight Universe (we'll get to that)I've been looking forward to Skate Story for forever, but I think I'm going to skip that demo. I'm already sold and I'm fine with waiting a couple more months before playing the whole thing.There are a couple of showcases coming up next week that might be worth keeping an eye on. The third annual edition of DreadXP's indie horror showcase is set for 1PM ET on October 23. You can catch that on the publisher's YouTube channel.Two hours later, you'll be able to tune into the Galaxies Autumn showcase. This will feature more than 50 games, including world premieres, gameplay trailers and other announcements. Games that will be featured include PowerWash Simulator 2, Mouse: PI For Hire and Denshattack, all of which are firmly on my to-play list.New releasesMister Scary is a weird little guy. I love when you get to play a game as a weird little guy. The game of the same name is a spooky NES homebrew platformer from Calgames. Mister Scary can stomp on his enemies, or freeze or burn them after eating a snack. When Mister Scary ducks, he becomes immune to damage because he's taking a nap. I appreciate that. Nothing scary can happen while you're snoozing.Mister Scary is $10 on Itch. You'll need to plug the ROM into a NES emulator to become Mister Scary.The only reason I still have Flash Player installed on my PC is so I can open N, which sits on my desktop, once in a while. I've been playing that classic freeware platformer for a long time, and now there's a good reason for many people to revisit the third entry in the series. As a thank you to the N++ community, Metanet Software is releasing a free update to mark the 10th anniversary of the game's PS4 debut. TEN++ is said to include the developer's \"most challenging levels yet.\" Given how darn tough these games are already, that's saying something. The update is available now on Steam and it's coming to the console versions of N++ soon.The Cabin Factory is an anomaly-hunting (i.e. spot the difference) game in the style of The Exit 8. You'll examine horror-themed cabins that are built for use in movies and theme parks to make sure they aren't actually haunted. If you spot an anomaly, you'll want to get out of the cabin post haste.This $3 horror walking sim from International Cat Studios and publisher Future Friends Games debuted on Steam last year, and it just hit consoles in time for Halloween. It's out now on PS4, PS5, Xbox One, Xbox Series X/S and Nintendo Switch.Upcoming After CloverPit and Ball x Pit, I was planning to take a break from roguelikes/roguelites before diving into Hades 2. Alas, the latest game from the legendary Ron Gilbert now has a release date, and it's very soon!In Death by Scrolling, the aim is to collect enough gold to pay a ferryman so you can escape purgatory. However, there's a wall of fire coming after you the whole time, so you'll need to keep moving in order to try to stay alive. You'll also need to avoid or stun an unkillable grim reaper as you collect gold and gems that unlock upgrades. Death by Scrolling is from Gilbert's Terrible Toybox and MicroProse Software. It's coming to Steam on October 28.There's a lot going on in Silly Polly Beast. It's safe to say this game is a shooter, but the release date trailer rapidly flits between perspectives and genres. There's an emphasis on survival horror, along with puzzles and stealth segments. Polly will even sometimes remove the board that's strapped to her back for some skateboarding sequences. This is said to have a story that morphs and evolves as much as the gameplay does. After escaping her hellish orphanage, Polly lands right in the underworld and has to navigate her way out of that too. Developer Andrei Chernyshov and publisher Top Hat Studios are behind Silly Polly Beast. It's coming to Steam, PS4, PS5, Xbox One, Xbox Series X/S and Nintendo Switch on October 28.Also coming to Steam on October 28 is a project from Autoscopia Interactive that's designed to be played in a single sitting. As Long As You’re Here is a first-person game that places you in the role of a woman with Alzheimer’s disease. Her memories of the past, including her late brother, blend into the present as Annie settles into living with her family. As Long As You’re Here started as a student project by Marlène Delrive, who was trying to better understand what her grandmother was experiencing in her final years. \"The aim is to create a mature and nuanced experience that shows the difficult repercussions of losing not only your memory, but also your agency and sense of time and place,\" the developers said.Let's close things out with a new trailer for Goodnight Universe. This is a cinematic adventure in which you play as a six-month-old baby. This particular infant is incredibly intelligent and has psychic powers. Isaac simply desires familial love and acceptance but (shock horror!) a tech company wants to take away the tot.As with Nice Dreams' last game (the stupendous Before Your Eyes), you control Goodnight Universe with your peepers via your device's camera. It seems fascinating, and I really have to check out the Next Fest demo. Publisher Skybound Games is bringing it to Steam, Xbox Series X/S, PS5, Nintendo Switch and Switch 2 on November 11.This article originally appeared on Engadget at https://www.engadget.com/gaming/a-spooky-nes-platformer-more-n-and-other-new-indie-games-worth-checking-out-110000259.html?src=rss",
          "content": "Welcome to our latest roundup of what's going on in the indie game space. I've reluctantly paused Ball x Pit for long enough to share some neat new releases and more details on upcoming games — some of which are arriving very soon. We've got a notable update for a classic as well. Steam Next Fest is taking place at the minute, and you still have until Monday to join in by checking out some of the many, many demos that have gone live for the event. Thanks partially (okay, almost entirely) to being unable to escape Ball x Pit, I've only tried a fewm Next Fest demos so far. I'm a fan of Aerial_Knight's Never Yield and after last year's sequel, it's cool to see solo developer Neil Jones (aka Aerial_Knight) trying something totally different. Aerial_Knight’s DropShot is a skydiving first-person shooter with finger guns and dragons. It’s a single-player game in which the aim is to take out your opponents and reach the ground first. Like Jones' previous games, it's stylish and fast-paced. I'm planning to check out the full game when it arrives down the line.It certainly helps to be a fast, accurate typer when you put words together for a living, but I wasn't quick or precise enough to win any rounds in the Final Sentence demo. This is a battle royale for up to 100 players in which you're at a typewriter and have to bash out sentences (or other strings of letters, numbers and symbols) in a race to the finish. If you run out of time, make too many mistakes or don't win, it's lights out, courtesy of the masked figure with a revolver who’s standing in front of you. There are some nice touches here. Having to type out the rules in the first few rounds is a clever idea on the part of developer Button Mash. I haven't won a round myself yet (I finished in second place a couple of times), but watched some streamers play. It's very funny when the winning player flips the bird at the guy holding a revolver in front of them. Final Sentence is coming to Steam later this year. Maybe I'll have learned how to spell \"sphinx\" by then.There are a few other Next Fest demos I'd like to try this weekend, namely:Crashout Crew (Overcooked-style co-op chaos with forklifts)Reanimal (co-op horror from the team behind Little Nightmares and Little Nightmares 2)Slots & Daggers (as if I need another slot-machine-based roguelite in my life right now) The Last Caretaker (sci-fi survival)Goodnight Universe (we'll get to that)I've been looking forward to Skate Story for forever, but I think I'm going to skip that demo. I'm already sold and I'm fine with waiting a couple more months before playing the whole thing.There are a couple of showcases coming up next week that might be worth keeping an eye on. The third annual edition of DreadXP's indie horror showcase is set for 1PM ET on October 23. You can catch that on the publisher's YouTube channel.Two hours later, you'll be able to tune into the Galaxies Autumn showcase. This will feature more than 50 games, including world premieres, gameplay trailers and other announcements. Games that will be featured include PowerWash Simulator 2, Mouse: PI For Hire and Denshattack, all of which are firmly on my to-play list.New releasesMister Scary is a weird little guy. I love when you get to play a game as a weird little guy. The game of the same name is a spooky NES homebrew platformer from Calgames. Mister Scary can stomp on his enemies, or freeze or burn them after eating a snack. When Mister Scary ducks, he becomes immune to damage because he's taking a nap. I appreciate that. Nothing scary can happen while you're snoozing.Mister Scary is $10 on Itch. You'll need to plug the ROM into a NES emulator to become Mister Scary.The only reason I still have Flash Player installed on my PC is so I can open N, which sits on my desktop, once in a while. I've been playing that classic freeware platformer for a long time, and now there's a good reason for many people to revisit the third entry in the series. As a thank you to the N++ community, Metanet Software is releasing a free update to mark the 10th anniversary of the game's PS4 debut. TEN++ is said to include the developer's \"most challenging levels yet.\" Given how darn tough these games are already, that's saying something. The update is available now on Steam and it's coming to the console versions of N++ soon.The Cabin Factory is an anomaly-hunting (i.e. spot the difference) game in the style of The Exit 8. You'll examine horror-themed cabins that are built for use in movies and theme parks to make sure they aren't actually haunted. If you spot an anomaly, you'll want to get out of the cabin post haste.This $3 horror walking sim from International Cat Studios and publisher Future Friends Games debuted on Steam last year, and it just hit consoles in time for Halloween. It's out now on PS4, PS5, Xbox One, Xbox Series X/S and Nintendo Switch.Upcoming After CloverPit and Ball x Pit, I was planning to take a break from roguelikes/roguelites before diving into Hades 2. Alas, the latest game from the legendary Ron Gilbert now has a release date, and it's very soon!In Death by Scrolling, the aim is to collect enough gold to pay a ferryman so you can escape purgatory. However, there's a wall of fire coming after you the whole time, so you'll need to keep moving in order to try to stay alive. You'll also need to avoid or stun an unkillable grim reaper as you collect gold and gems that unlock upgrades. Death by Scrolling is from Gilbert's Terrible Toybox and MicroProse Software. It's coming to Steam on October 28.There's a lot going on in Silly Polly Beast. It's safe to say this game is a shooter, but the release date trailer rapidly flits between perspectives and genres. There's an emphasis on survival horror, along with puzzles and stealth segments. Polly will even sometimes remove the board that's strapped to her back for some skateboarding sequences. This is said to have a story that morphs and evolves as much as the gameplay does. After escaping her hellish orphanage, Polly lands right in the underworld and has to navigate her way out of that too. Developer Andrei Chernyshov and publisher Top Hat Studios are behind Silly Polly Beast. It's coming to Steam, PS4, PS5, Xbox One, Xbox Series X/S and Nintendo Switch on October 28.Also coming to Steam on October 28 is a project from Autoscopia Interactive that's designed to be played in a single sitting. As Long As You’re Here is a first-person game that places you in the role of a woman with Alzheimer’s disease. Her memories of the past, including her late brother, blend into the present as Annie settles into living with her family. As Long As You’re Here started as a student project by Marlène Delrive, who was trying to better understand what her grandmother was experiencing in her final years. \"The aim is to create a mature and nuanced experience that shows the difficult repercussions of losing not only your memory, but also your agency and sense of time and place,\" the developers said.Let's close things out with a new trailer for Goodnight Universe. This is a cinematic adventure in which you play as a six-month-old baby. This particular infant is incredibly intelligent and has psychic powers. Isaac simply desires familial love and acceptance but (shock horror!) a tech company wants to take away the tot.As with Nice Dreams' last game (the stupendous Before Your Eyes), you control Goodnight Universe with your peepers via your device's camera. It seems fascinating, and I really have to check out the Next Fest demo. Publisher Skybound Games is bringing it to Steam, Xbox Series X/S, PS5, Nintendo Switch and Switch 2 on November 11.This article originally appeared on Engadget at https://www.engadget.com/gaming/a-spooky-nes-platformer-more-n-and-other-new-indie-games-worth-checking-out-110000259.html?src=rss",
          "feed_position": 8
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/abstract-or-die-why-ai-enterprises-cant-afford-rigid-vector-stacks",
          "published_at": "Sat, 18 Oct 2025 09:00:00 GMT",
          "title": "Abstract or die: Why AI enterprises can't afford rigid vector stacks",
          "standfirst": "Vector databases (DBs), once specialist research instruments, have become widely used infrastructure in just a few years. They power today&#x27;s semantic search, recommendation engines, anti-fraud measures and gen AI applications across industries. There are a deluge of options: PostgreSQL with pgvector, MySQL HeatWave, DuckDB VSS, SQLite VSS, Pinecone, Weaviate, Milvus and several others.The riches of choices sound like a boon to companies. But just beneath, a growing problem looms: Stack instability. New vector DBs appear each quarter, with disparate APIs, indexing schemes and performance trade-offs. Today&#x27;s ideal choice may look dated or limiting tomorrow.To business AI teams, volatility translates into lock-in risks and migration hell. Most projects begin life with lightweight engines like DuckDB or SQLite for prototyping, then move to Postgres, MySQL or a cloud-native service in production. Each switch involves rewriting queries, reshaping pipelines, and slowing down deployments.This re-engineering merry-go-round undermines the very speed and agility that AI adoption is supposed to bring.Why portability matters nowCompanies have a tricky balancing act:Experiment quickly with minimal overhead, in hopes of trying and getting early value; Scale safely on stable, production-quality infrastructure without months of refactoring;Be nimble in a world where new and better backends arrive nearly every month. Without portability, organizations stagnate. They have technical debt from recursive code paths, are hesitant to adopt new technology and cannot move prototypes to production at pace. In effect, the database is a bottleneck rather than an accelerator.Portability, or the ability to move underlying infrastructure without re-encoding the application, is ever more a strategic requirement for enterprises rolling out AI at scale.Abstraction as infrastructureThe solution is not to pick the \"perfect\" vector database (there isn&#x27;t one), but to change how enterprises think about the problem.In software engineering, the adapter pattern provides a stable interface while hiding underlying complexity. Historically, we&#x27;ve seen how this principle reshaped entire industries:ODBC/JDBC gave enterprises a single way to query relational databases, reducing the risk of being tied to Oracle, MySQL or SQL Server; Apache Arrow standardized columnar data formats, so data systems could play nice together; ONNX created a vendor-agnostic format for machine learning (ML) models, bringing TensorFlow, PyTorch, etc. together; Kubernetes abstracted infrastructure details, so workloads could run the same everywhere on clouds;any-llm (Mozilla AI) now makes it possible to have one API across lots of large language model (LLM) vendors, so playing with AI is safer. All these abstractions led to adoption by lowering switching costs. They turned broken ecosystems into solid, enterprise-level infrastructure.Vector databases are also at the same tipping point.The adapter approach to vectorsInstead of having application code directly bound to some specific vector backend, companies can compile against an abstraction layer that normalizes operations like inserts, queries and filtering.This doesn&#x27;t necessarily eliminate the need to choose a backend; it makes that choice less rigid. Development teams can start with DuckDB or SQLite in the lab, then scale up to Postgres or MySQL for production and ultimately adopt a special-purpose cloud vector DB without having to re-architect the application.Open source efforts like Vectorwrap are early examples of this approach, presenting a single Python API to Postgres, MySQL, DuckDB and SQLite. They demonstrate the power of abstraction to accelerate prototyping, reduce lock-in risk and support hybrid architectures employing numerous backends.Why businesses should careFor leaders of data infrastructure and decision-makers for AI, abstraction offers three benefits:Speed from prototype to productionTeams are able to prototype on lightweight local environments and scale without expensive rewrites.Reduced vendor riskOrganizations can adopt new backends as they emerge without long migration projects by decoupling app code from specific databases.Hybrid flexibilityCompanies can mix transactional, analytical and specialized vector DBs under one architecture, all behind an aggregated interface.The result is data layer agility, and that&#x27;s more and more the difference between fast and slow companies.A broader movement in open sourceWhat&#x27;s happening in the vector space is one example of a bigger trend: Open-source abstractions as critical infrastructure.In data formats: Apache ArrowIn ML models: ONNXIn orchestration: KubernetesIn AI APIs: Any-LLM and other such frameworksThese projects succeed, not by adding new capability, but by removing friction. They enable enterprises to move more quickly, hedge bets and evolve along with the ecosystem.Vector DB adapters continue this legacy, transforming a high-speed, fragmented space into infrastructure that enterprises can truly depend on.The future of vector DB portabilityThe landscape of vector DBs will not converge anytime soon. Instead, the number of options will grow, and every vendor will tune for different use cases, scale, latency, hybrid search, compliance or cloud platform integration.Abstraction becomes strategy in this case. Companies adopting portable approaches will be capable of:Prototyping boldlyDeploying in a flexible mannerScaling rapidly to new techIt&#x27;s possible we&#x27;ll eventually see a \"JDBC for vectors,\" a universal standard that codifies queries and operations across backends. Until then, open-source abstractions are laying the groundwork.ConclusionEnterprises adopting AI cannot afford to be slowed by database lock-in. As the vector ecosystem evolves, the winners will be those who treat abstraction as infrastructure, building against portable interfaces rather than binding themselves to any single backend.The decades-long lesson of software engineering is simple: Standards and abstractions lead to adoption. For vector DBs, that revolution has already begun.Mihir Ahuja is an AI/ML engineer and open-source contributor based in San Francisco.",
          "content": "Vector databases (DBs), once specialist research instruments, have become widely used infrastructure in just a few years. They power today&#x27;s semantic search, recommendation engines, anti-fraud measures and gen AI applications across industries. There are a deluge of options: PostgreSQL with pgvector, MySQL HeatWave, DuckDB VSS, SQLite VSS, Pinecone, Weaviate, Milvus and several others.The riches of choices sound like a boon to companies. But just beneath, a growing problem looms: Stack instability. New vector DBs appear each quarter, with disparate APIs, indexing schemes and performance trade-offs. Today&#x27;s ideal choice may look dated or limiting tomorrow.To business AI teams, volatility translates into lock-in risks and migration hell. Most projects begin life with lightweight engines like DuckDB or SQLite for prototyping, then move to Postgres, MySQL or a cloud-native service in production. Each switch involves rewriting queries, reshaping pipelines, and slowing down deployments.This re-engineering merry-go-round undermines the very speed and agility that AI adoption is supposed to bring.Why portability matters nowCompanies have a tricky balancing act:Experiment quickly with minimal overhead, in hopes of trying and getting early value; Scale safely on stable, production-quality infrastructure without months of refactoring;Be nimble in a world where new and better backends arrive nearly every month. Without portability, organizations stagnate. They have technical debt from recursive code paths, are hesitant to adopt new technology and cannot move prototypes to production at pace. In effect, the database is a bottleneck rather than an accelerator.Portability, or the ability to move underlying infrastructure without re-encoding the application, is ever more a strategic requirement for enterprises rolling out AI at scale.Abstraction as infrastructureThe solution is not to pick the \"perfect\" vector database (there isn&#x27;t one), but to change how enterprises think about the problem.In software engineering, the adapter pattern provides a stable interface while hiding underlying complexity. Historically, we&#x27;ve seen how this principle reshaped entire industries:ODBC/JDBC gave enterprises a single way to query relational databases, reducing the risk of being tied to Oracle, MySQL or SQL Server; Apache Arrow standardized columnar data formats, so data systems could play nice together; ONNX created a vendor-agnostic format for machine learning (ML) models, bringing TensorFlow, PyTorch, etc. together; Kubernetes abstracted infrastructure details, so workloads could run the same everywhere on clouds;any-llm (Mozilla AI) now makes it possible to have one API across lots of large language model (LLM) vendors, so playing with AI is safer. All these abstractions led to adoption by lowering switching costs. They turned broken ecosystems into solid, enterprise-level infrastructure.Vector databases are also at the same tipping point.The adapter approach to vectorsInstead of having application code directly bound to some specific vector backend, companies can compile against an abstraction layer that normalizes operations like inserts, queries and filtering.This doesn&#x27;t necessarily eliminate the need to choose a backend; it makes that choice less rigid. Development teams can start with DuckDB or SQLite in the lab, then scale up to Postgres or MySQL for production and ultimately adopt a special-purpose cloud vector DB without having to re-architect the application.Open source efforts like Vectorwrap are early examples of this approach, presenting a single Python API to Postgres, MySQL, DuckDB and SQLite. They demonstrate the power of abstraction to accelerate prototyping, reduce lock-in risk and support hybrid architectures employing numerous backends.Why businesses should careFor leaders of data infrastructure and decision-makers for AI, abstraction offers three benefits:Speed from prototype to productionTeams are able to prototype on lightweight local environments and scale without expensive rewrites.Reduced vendor riskOrganizations can adopt new backends as they emerge without long migration projects by decoupling app code from specific databases.Hybrid flexibilityCompanies can mix transactional, analytical and specialized vector DBs under one architecture, all behind an aggregated interface.The result is data layer agility, and that&#x27;s more and more the difference between fast and slow companies.A broader movement in open sourceWhat&#x27;s happening in the vector space is one example of a bigger trend: Open-source abstractions as critical infrastructure.In data formats: Apache ArrowIn ML models: ONNXIn orchestration: KubernetesIn AI APIs: Any-LLM and other such frameworksThese projects succeed, not by adding new capability, but by removing friction. They enable enterprises to move more quickly, hedge bets and evolve along with the ecosystem.Vector DB adapters continue this legacy, transforming a high-speed, fragmented space into infrastructure that enterprises can truly depend on.The future of vector DB portabilityThe landscape of vector DBs will not converge anytime soon. Instead, the number of options will grow, and every vendor will tune for different use cases, scale, latency, hybrid search, compliance or cloud platform integration.Abstraction becomes strategy in this case. Companies adopting portable approaches will be capable of:Prototyping boldlyDeploying in a flexible mannerScaling rapidly to new techIt&#x27;s possible we&#x27;ll eventually see a \"JDBC for vectors,\" a universal standard that codifies queries and operations across backends. Until then, open-source abstractions are laying the groundwork.ConclusionEnterprises adopting AI cannot afford to be slowed by database lock-in. As the vector ecosystem evolves, the winners will be those who treat abstraction as infrastructure, building against portable interfaces rather than binding themselves to any single backend.The decades-long lesson of software engineering is simple: Standards and abstractions lead to adoption. For vector DBs, that revolution has already begun.Mihir Ahuja is an AI/ML engineer and open-source contributor based in San Francisco.",
          "feed_position": 0,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/2Xu1YqwqTUMJMVWnRrKy7M/23b8a69e94d864a0306f259b1cd29397/u7277289442_A_rendering_of_animated_friendly_AI_agents_with_h_9b63c83c-c489-4154-a498-3bdfdc3a5c5b_3.png"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/developers-can-now-add-live-google-maps-data-to-gemini-powered-ai-app",
          "published_at": "Fri, 17 Oct 2025 22:31:00 GMT",
          "title": "Developers can now add live Google Maps data to Gemini-powered AI app outputs",
          "standfirst": "Google is adding a new feature for third-party developers building atop its Gemini AI models that rivals like OpenAI&#x27;s ChatGPT, Anthropic&#x27;s Claude, and the growing array of Chinese open source options are unlikely to get anytime soon: grounding with Google Maps.This addition allows developers to connect Google&#x27;s Gemini AI models&#x27; reasoning capabilities with live geospatial data from Google Maps, enabling applications to deliver detailed, location-relevant responses to user queries—such as business hours, reviews, or the atmosphere of a specific venue. By tapping into data from over 250 million places, developers can now build more intelligent and responsive location-aware experiences.This is particularly useful for applications where proximity, real-time availability, or location-specific personalization matter—such as local search, delivery services, real estate, and travel planning. When the user’s location is known, developers can pass latitude and longitude into the request to enhance the response quality.By tightly integrating real-time and historical Maps data into the Gemini API, Google enables applications to generate grounded, location-specific responses with factual accuracy and contextual depth that are uniquely possible through its mapping infrastructure.Merging AI and Geospatial IntelligenceThe new feature is accessible in Google AI Studio, where developers can try a live demo powered by the Gemini Live API. Models that support the grounding with Google Maps include:Gemini 2.5 ProGemini 2.5 FlashGemini 2.5 Flash-LiteGemini 2.0 FlashIn one demonstration, a user asked for Italian restaurant recommendations in Chicago. The assistant, leveraging Maps data, retrieved top-rated options and clarified a misspelled restaurant name before locating the correct venue with accurate business details.Developers can also retrieve a context token to embed a Google Maps widget in their app’s user interface. This interactive component displays photos, reviews, and other familiar content typically found in Google Maps.Integration is handled via the generateContent method in the Gemini API, where developers include googleMaps as a tool. They can also enable a Maps widget by setting a parameter in the request. The widget, rendered using a returned context token, can provide a visual layer alongside the AI-generated text.Use Cases Across IndustriesThe Maps grounding tool is designed to support a wide range of practical use cases:Itinerary generation: Travel apps can create detailed daily plans with routing, timing, and venue information.Personalized local recommendations: Real estate platforms can highlight listings near kid-friendly amenities like schools and parks.Detailed location queries: Applications can provide specific information, such as whether a cafe offers outdoor seating, using community reviews and Maps metadata.Developers are encouraged to only enable the tool when geographic context is relevant, to optimize both performance and cost. According to the developer documentation, pricing starts at $25 per 1,000 grounded prompts — a steep sum for those trafficking in numerous queries.Combining Search and Maps for Enhanced ContextDevelopers can use Grounding with Google Maps alongside Grounding with Google Search in the same request.While the Maps tool contributes factual data—like addresses, hours, and ratings—the Search tool adds broader context from web content, such as news or event listings.For example, when asked about live music on Beale Street, the combined tools provide venue details from Maps and event times from Search. According to Google, internal testing shows that using both tools together leads to significantly improved response quality.Unfortunately, it doesn&#x27;t appear that the Google Maps grounding includes live vehicular traffic data — at least not yet.Customization and Developer FlexibilityThe experience is built for customization. Developers can tweak system prompts, choose from different Gemini models, and configure voice settings to tailor interactions. The demo app in Google AI Studio is also remixable, enabling developers to test ideas, add features, and iterate on designs within a flexible development environment.The API returns structured metadata—including source links, place IDs, and citation spans—that developers can use to build inline citations or verify the AI-generated outputs. This supports transparency and enhances trust in user-facing applications. Google also requires that Maps-based sources be attributed clearly and linked back to the source using their URI.Implementation Considerations for AI BuildersFor technical teams integrating this capability, Google recommends:Passing user location context when known, for better results.Displaying Google Maps source links directly beneath the relevant content.Only enabling the tool when the query clearly involves geographic context.Monitoring latency and disabling grounding when performance is critical.Grounding with Google Maps is currently available globally, though prohibited in several territories (including China, Iran, North Korea, and Cuba), and not permitted for emergency response use cases.Availability and AccessGrounding with Google Maps is now generally available through the Gemini API. With this release, Google continues to expand the capabilities of the Gemini API, empowering developers to build AI-driven applications that understand and respond to the world around them.",
          "content": "Google is adding a new feature for third-party developers building atop its Gemini AI models that rivals like OpenAI&#x27;s ChatGPT, Anthropic&#x27;s Claude, and the growing array of Chinese open source options are unlikely to get anytime soon: grounding with Google Maps.This addition allows developers to connect Google&#x27;s Gemini AI models&#x27; reasoning capabilities with live geospatial data from Google Maps, enabling applications to deliver detailed, location-relevant responses to user queries—such as business hours, reviews, or the atmosphere of a specific venue. By tapping into data from over 250 million places, developers can now build more intelligent and responsive location-aware experiences.This is particularly useful for applications where proximity, real-time availability, or location-specific personalization matter—such as local search, delivery services, real estate, and travel planning. When the user’s location is known, developers can pass latitude and longitude into the request to enhance the response quality.By tightly integrating real-time and historical Maps data into the Gemini API, Google enables applications to generate grounded, location-specific responses with factual accuracy and contextual depth that are uniquely possible through its mapping infrastructure.Merging AI and Geospatial IntelligenceThe new feature is accessible in Google AI Studio, where developers can try a live demo powered by the Gemini Live API. Models that support the grounding with Google Maps include:Gemini 2.5 ProGemini 2.5 FlashGemini 2.5 Flash-LiteGemini 2.0 FlashIn one demonstration, a user asked for Italian restaurant recommendations in Chicago. The assistant, leveraging Maps data, retrieved top-rated options and clarified a misspelled restaurant name before locating the correct venue with accurate business details.Developers can also retrieve a context token to embed a Google Maps widget in their app’s user interface. This interactive component displays photos, reviews, and other familiar content typically found in Google Maps.Integration is handled via the generateContent method in the Gemini API, where developers include googleMaps as a tool. They can also enable a Maps widget by setting a parameter in the request. The widget, rendered using a returned context token, can provide a visual layer alongside the AI-generated text.Use Cases Across IndustriesThe Maps grounding tool is designed to support a wide range of practical use cases:Itinerary generation: Travel apps can create detailed daily plans with routing, timing, and venue information.Personalized local recommendations: Real estate platforms can highlight listings near kid-friendly amenities like schools and parks.Detailed location queries: Applications can provide specific information, such as whether a cafe offers outdoor seating, using community reviews and Maps metadata.Developers are encouraged to only enable the tool when geographic context is relevant, to optimize both performance and cost. According to the developer documentation, pricing starts at $25 per 1,000 grounded prompts — a steep sum for those trafficking in numerous queries.Combining Search and Maps for Enhanced ContextDevelopers can use Grounding with Google Maps alongside Grounding with Google Search in the same request.While the Maps tool contributes factual data—like addresses, hours, and ratings—the Search tool adds broader context from web content, such as news or event listings.For example, when asked about live music on Beale Street, the combined tools provide venue details from Maps and event times from Search. According to Google, internal testing shows that using both tools together leads to significantly improved response quality.Unfortunately, it doesn&#x27;t appear that the Google Maps grounding includes live vehicular traffic data — at least not yet.Customization and Developer FlexibilityThe experience is built for customization. Developers can tweak system prompts, choose from different Gemini models, and configure voice settings to tailor interactions. The demo app in Google AI Studio is also remixable, enabling developers to test ideas, add features, and iterate on designs within a flexible development environment.The API returns structured metadata—including source links, place IDs, and citation spans—that developers can use to build inline citations or verify the AI-generated outputs. This supports transparency and enhances trust in user-facing applications. Google also requires that Maps-based sources be attributed clearly and linked back to the source using their URI.Implementation Considerations for AI BuildersFor technical teams integrating this capability, Google recommends:Passing user location context when known, for better results.Displaying Google Maps source links directly beneath the relevant content.Only enabling the tool when the query clearly involves geographic context.Monitoring latency and disabling grounding when performance is critical.Grounding with Google Maps is currently available globally, though prohibited in several territories (including China, Iran, North Korea, and Cuba), and not permitted for emergency response use cases.Availability and AccessGrounding with Google Maps is now generally available through the Gemini API. With this release, Google continues to expand the capabilities of the Gemini API, empowering developers to build AI-driven applications that understand and respond to the world around them.",
          "feed_position": 1,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/2ehvkDsQ7NC7wQuECOcDGS/cb89da9ccaf13f2888f005ac93cecffa/cfr0z3n_realistic_graphic_novel_hyper_detailed_flat_illustratio_8aae43d5-1e50-4ebd-8760-ea28dbc0f328.png"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/wearables/meta-ray-ban-display-review-chunky-frames-with-impressive-abilities-193127070.html",
          "published_at": "Fri, 17 Oct 2025 19:31:27 +0000",
          "title": "Meta Ray-Ban Display review: Chunky frames with impressive abilities",
          "standfirst": "I've been wearing the $800 Meta Ray-Ban Display glasses daily for ten days and I'm still a bit conflicted. On one hand, I'm still not entirely comfortable with how they look. I've worn them on the bus, at the office, on walks around my neighborhood and during hangouts with friends. Each time, I'm very aware that I probably look a bit strange. On the other hand, there's a lot I really like about using these glasses. The built-in display has helped me look at my phone less throughout the day. The neural band feels more innovative than any wrist-based device I've tried. Together, it feels like a significant milestone for smart glasses overall. But it's also very much a first-generation device with some issues that still need to be worked out. Chunky statement glasses or hideously nerdy? To once again state the obvious: The frames are extremely chunky and too wide for my face. The dark black frames I tried for this review unfortunately accentuate the extra thickness. I won't pretend it's my best look and I did feel a bit self-conscious at times wearing these in public. Meta also makes a light brown \"sand\" color that I tried at the Connect event, and I think that color is a bit more flattering, even if the frames are just as oversized. (Sidenote: Smart glasses companies, please, please make your frames available in something other than black!) But, everyone has a different face shape, skin tone and general ability to \"pull off\" what one of my friends charitably described as \"chunky statement glasses.\" What looks not-great on my face, may look good on someone else. I really wish Meta could have squeezed this tech into slightly smaller frames, but I did get more used to the look the more I wore them. Overall, I do think the size is a reasonable tradeoff for a first-generation product that's pretty clearly aimed at early adopters. Here's how they look in the lighter \"sand\" color. Karissa Bell for Engadget The reason the glasses are so thick compared with Meta's other frames is because there are a lot of extra components to power the display, including a mini projector and waveguide. And, at 69 grams, the display glasses are noticeably heavier. I didn't find it particularly uncomfortable at first, but there is a noticeable pressure after six or seven hours of wear. Plus, the extra weight and width also made them consistently slide down my nose. I'm not sure I'd feel comfortable wearing these on a bike ride or a jog as I'd worry about them falling off. While I tested these, I was very interested to get reactions from friends and family. I didn't get many positive comments about how they looked on my face, though a few particularly generous colleagues assured me I was \"pulling them off.\" But seeing people's reactions as soon as the display activated was another matter. Almost everyone has had the same initial reaction: \"whoa.\" Quality display with some limitations As I discussed in my initial impressions, these glasses have a monocular display on the right side, so it doesn't offer the kind of immersive AR I experienced with the Orion prototype last year. You have to look slightly up and to the right to focus on the full-color display. It's impressively bright and clear, but doesn't overtake your vision. At 20 degrees, the field of view is small, but it never felt like a limitation. Because the content you see isn't meant to be immersive, it never feels like what's on the display is being cut off or like you have to adjust where you're looking to properly see it. The display itself has three main menus: an app launcher, a kind of home screen where you can access Meta AI and view notifications and a settings page for adjusting brightness, volume and other preferences. The display is in the right lens. Karissa Bell for Engadget For now, there are only a handful of Meta-created \"apps\" available. You can check your Instagram, WhatsApp and Messenger inboxes and chat with Meta AI. There's also a simple maps app for walking navigation, a music/audio player, camera and live translation and captioning features. There's also a mini puzzle game called \"Hypertrail.\" One of my favorite integrations was the ability to check Instagram DMs. Not only can you quickly read and respond to messages, you can watch Reels sent by your friends. While the video quality isn't as high as what you'd see on your phone, there's something very cool about quickly watching a clip without having to pull out your phone. Meta is also working on a standalone Reels experience that I'm very much looking forward to. I also enjoyed being able to view media sent in my family group chats on WhatsApp. I often would end up revisiting the photos on videos once I pulled out my phone, but being able to instantly see these messages as they came in tickled whatever part of my brain responds to instant gratification. There's some impressive tech inside those thick frames. Karissa Bell for Engadget The display also solves one of my biggest complaints with Meta's other smart glasses: that it's really difficult to frame photos. When you open the camera app on the display model, you can see a preview of the photo and even use a gesture to zoom in to properly frame your shot. Similarly, if you're on a WhatsApp video call you can see both the other person's video as well as a small preview of your own like you would on your phone's screen. It's a cool trick but the small display felt too cramped for a proper video call. People I used this with also told me that my video feed had some quality issues despite being on Wi-Fi. The glasses' live captioning and translation features are probably the best examples of Meta bringing its existing AI features into the display. I've written before about how Meta AI's translation abilities are one of my favorite features of the Ray-Ban smart glasses. Live translation on the display is even better, because it delivers a real-time text feed of what the person in front of you is saying. I tried it out with my husband, a native Spanish speaker, and it was even more natural than the non-display glasses because I didn't have to pause and wait for the audio to relay what he was saying. It still wasn't an exactly perfect translation, and there were still a few occasions when it didn't catch everything he said, but it made the process so much simpler overall. Likewise, live captions transcribes conversations in real-time into a similar text feed. I've found that it's a cool way to demo these glasses' capabilities, but I haven't yet found an occasion to use this in anything other than a demo. However, I still think it could be useful as an accessibility aid for anyone who has trouble hearing or processing audio. Another feature that's useful for travel is walking navigation. Dictate an address or location (you can say something like \"take me to the closest Starbucks\") and the glasses' display will guide you on your route. The first time I tried this was the roughly 10-minute walk from my bus stop to Yahoo's San Francisco office. The route only required two turns, but it didn't quite work. My glasses confidently navigated me to an alleyway behind the office building rather than the entrance. These kinds of mishaps happen with lots of mapping tools — Meta's maps rely on data from OpenStreetMap and Overture — but it was a good reminder that it's still early days for this product. I don't use Meta AI a ton on any of my smart glasses, but having a bit of visual feedback for these interactions was a nice change. I retain information much better from reading than listening, so seeing text-based output to my queries felt a lot more helpful. It's also nice that for longer responses from the assistant, you can stop the audio playback and swipe through informational cards instead. Meta AI on the glasses' display delivers information in a card-like interface. Meta While cooking dinner one night, I asked for a quick recipe for teriyaki salmon and Meta AI supplied what seemed like a passable recipe onto the display. The only drawback was the display goes to sleep pretty quickly unless you continue to interact with the content you're seeing, so the recipe I liked disappeared before I could actually attempt it. (You can view your Meta AI history in the Meta AI app if you really want to revisit something.) My main complaint is that I want to be able to do much more with the display. Messaging app integrations are nice, but I wish the display worked with more of the apps on my phone. When it worked best, I was happy to be able to view and dismiss messaging notifications without having to touch my phone; I just wish it worked with all my phone's notifications. There are also some frustrating limitations on sending and receiving texts. For example, there's no simple way to take a photo on your glasses and text it to a friend with the glasses. You have to wait for the glasses to send a \"preview\" of your message to your phone and then manually send the text. Or, you can opt in to Meta's cloud services and send the photo immediately as a link, but I'm not sure many of my friends would readily open a \"media.meta.com\" URL. The glasses also don't really support non-WhatsApp group chats, at least on iOS. You can receive messages sent in group chats, but there's no indication the message originated in a group thread. And, it's impossible to reply in the same thread; instead, replies are sent directly to the person who texted, which can get confusing if you're not checking your phone. It was also a little annoying that reading and even replying to texts from my glasses wouldn't mark the text as read in my phone's inbox. Meta blames all this on Apple's iOS restrictions, and says it's hoping to work with the company to improve the experience. The company tells me that group messaging should work normally for people with Android devices and that there is also a dedicated inbox for checking texts on the glasses. I haven’t tested this out yet. The band + battery life The glasses are controlled using Meta's Neural Band, which can translate subtle gestures like finger taps into actions on the display. Because the band relies on electromyography (EMG), you do need a fairly snug fit for it to work properly. I didn't find it uncomfortable, but, like the glasses, I don't love how it looks as a daily accessory. It also requires daily charging if you wear the glasses all day. But the band does work surprisingly well. In more than a week, it almost never missed a gesture, and it never falsely registered a gesture, despite my efforts to confuse it by fidgeting or rubbing my fingers together. The gestures themselves are also pretty intuitive and don't take long to get used to: double tapping your thumb and middle fingers wakes up or puts the display to sleep, single taps of your index and middle fingers allow you to select an item or go back, and swiping your thumb along the side of your index finger lets you navigate around the display. There are a few others, but those are the ones I used most often. The Meta Neural Band requires a snug fit to work properly. Karissa Bell for Engadget Each time you make a gesture, the band emits a small vibration so you get a bit of haptic feedback letting you know it registered. I've used hand tracking-based navigation in various VR, AR and mixed reality devices and I've always felt a bit goofy waving my hands around. But the neural band gestures work when your hand is by your side or in your pocket. The other major drawback of these glasses is that heavy use of the display drains the battery pretty quickly. Meta says the Ray-Ban Display’s battery can go about six hours on a single charge, but it really depends on how much you're using the display. With very limited use, l was able to stretch the battery to about seven hours, but if you're doing display-intensive tasks like video calling or live translation, it will die much, much more quickly. The Meta Ray-Ban Display glasses, charging case and neural band. Karissa Bell for Engadget The glasses do come with a charging case that can deliver a few extra charges on-the-go, but I was a bit surprised at how often I had to recharge the case. With my normal Ray-Ban Meta glasses I can go several days without topping up the charging case, but with the Meta Ray-Ban Display case, I'm charging it at least every other day. Privacy and safety Whenever I write or post on social media about a pair of Meta-branded glasses, I inevitably hear from people concerned about the privacy implications of these devices. As I wrote in my recent review of Meta's second-gen Ray-Ban glasses, I share a lot of these concerns. Meta has made subtle but meaningful changes to its glasses' privacy policy over the last year, and its track record suggests these devices will inevitably scoop up more of our data over time. In terms of privacy implications of the display-enabled glasses, there isn't a meaningful difference compared to their counterparts. Meta's policies are the same for all its wearables. I suppose you could use live translation to surreptitiously eavesdrop on a conversation you wouldn't typically understand, though that's technically possible with Meta's other glasses too. And the addition of a wrist-based controller means taking photos is a bit less obvious, but there's still an LED indicator that lights up when the camera is on. The neural band allows you to snap photos without touching the capture button or using a voice command. Karissa Bell for Engadget I have been surprised at how many people have asked me if these glasses have some kind of facial recognition abilities. I'm not sure if that's a sign of people's general distrust of Meta, or an assumption based on seeing similar glasses in sci-fi flicks, but I do think it's telling. (They don't, to be clear. Meta currently only uses facial recognition for two safety-related features on Facebook and Instagram.) Meta hasn't done much to earn people's trust when it comes to privacy, and I wish the company would use its growing wearables business to try to prove otherwise. On a more practical level, I have some safety concerns. The display didn't hinder my situational awareness while walking, but I could see how it might for others. And I'm definitely not comfortable using the display while driving. Meta does have an audio-only \"driving detection\" setting that can automatically kick in when you're traveling in a car, but the feature is optional, which seems potentially problematic. Should you buy these? In short: probably not. As much as I've been genuinely impressed with Meta's display tech, I don't think these glasses make sense for most people right now. And, at $800, the Meta Ray-Ban Display glasses are more than twice as much as the company's very good second-generation Ray-Ban glasses, which come in a wide range of much more normal-looking frame styles and colors. The Meta Ray-Ban Display glasses, on the other hand, still look very much like a first-gen product. There are some really compelling use cases for the display, but its functionality is limited. The glasses are also too thick and bulky for what's meant to be an everyday accessory. At the end of the day, most people want glasses that make them look good. There’s also the fact that right now, these glasses are somewhat difficult to actually buy. They are only available at a handful of physical retailers, which currently have a very limited supply, Meta is also requiring would-be buyers to schedule demo appointments in order to buy, though some stores — like the LensCrafters where I bought my pair — aren’t enforcing this. Still, there's a lot to be excited about. Watching people's reactions to trying these has been almost as much fun as using them myself. Meta also has a solid lineup of new features already in the works, including a standalone Reels app, a teleprompter and gesture-based handwriting for message replies. If you're already all-in on smart glasses or, like me, you've been patiently waiting for glasses with a high quality, usable display, then the Meta Ray-Ban Display glasses are worth the investment now — as long as you can accept the thick frames. Update, October 17, 2025, 3:42PM PT: Added more information about group text functionality on Android. This article originally appeared on Engadget at https://www.engadget.com/wearables/meta-ray-ban-display-review-chunky-frames-with-impressive-abilities-193127070.html?src=rss",
          "content": "I've been wearing the $800 Meta Ray-Ban Display glasses daily for ten days and I'm still a bit conflicted. On one hand, I'm still not entirely comfortable with how they look. I've worn them on the bus, at the office, on walks around my neighborhood and during hangouts with friends. Each time, I'm very aware that I probably look a bit strange. On the other hand, there's a lot I really like about using these glasses. The built-in display has helped me look at my phone less throughout the day. The neural band feels more innovative than any wrist-based device I've tried. Together, it feels like a significant milestone for smart glasses overall. But it's also very much a first-generation device with some issues that still need to be worked out. Chunky statement glasses or hideously nerdy? To once again state the obvious: The frames are extremely chunky and too wide for my face. The dark black frames I tried for this review unfortunately accentuate the extra thickness. I won't pretend it's my best look and I did feel a bit self-conscious at times wearing these in public. Meta also makes a light brown \"sand\" color that I tried at the Connect event, and I think that color is a bit more flattering, even if the frames are just as oversized. (Sidenote: Smart glasses companies, please, please make your frames available in something other than black!) But, everyone has a different face shape, skin tone and general ability to \"pull off\" what one of my friends charitably described as \"chunky statement glasses.\" What looks not-great on my face, may look good on someone else. I really wish Meta could have squeezed this tech into slightly smaller frames, but I did get more used to the look the more I wore them. Overall, I do think the size is a reasonable tradeoff for a first-generation product that's pretty clearly aimed at early adopters. Here's how they look in the lighter \"sand\" color. Karissa Bell for Engadget The reason the glasses are so thick compared with Meta's other frames is because there are a lot of extra components to power the display, including a mini projector and waveguide. And, at 69 grams, the display glasses are noticeably heavier. I didn't find it particularly uncomfortable at first, but there is a noticeable pressure after six or seven hours of wear. Plus, the extra weight and width also made them consistently slide down my nose. I'm not sure I'd feel comfortable wearing these on a bike ride or a jog as I'd worry about them falling off. While I tested these, I was very interested to get reactions from friends and family. I didn't get many positive comments about how they looked on my face, though a few particularly generous colleagues assured me I was \"pulling them off.\" But seeing people's reactions as soon as the display activated was another matter. Almost everyone has had the same initial reaction: \"whoa.\" Quality display with some limitations As I discussed in my initial impressions, these glasses have a monocular display on the right side, so it doesn't offer the kind of immersive AR I experienced with the Orion prototype last year. You have to look slightly up and to the right to focus on the full-color display. It's impressively bright and clear, but doesn't overtake your vision. At 20 degrees, the field of view is small, but it never felt like a limitation. Because the content you see isn't meant to be immersive, it never feels like what's on the display is being cut off or like you have to adjust where you're looking to properly see it. The display itself has three main menus: an app launcher, a kind of home screen where you can access Meta AI and view notifications and a settings page for adjusting brightness, volume and other preferences. The display is in the right lens. Karissa Bell for Engadget For now, there are only a handful of Meta-created \"apps\" available. You can check your Instagram, WhatsApp and Messenger inboxes and chat with Meta AI. There's also a simple maps app for walking navigation, a music/audio player, camera and live translation and captioning features. There's also a mini puzzle game called \"Hypertrail.\" One of my favorite integrations was the ability to check Instagram DMs. Not only can you quickly read and respond to messages, you can watch Reels sent by your friends. While the video quality isn't as high as what you'd see on your phone, there's something very cool about quickly watching a clip without having to pull out your phone. Meta is also working on a standalone Reels experience that I'm very much looking forward to. I also enjoyed being able to view media sent in my family group chats on WhatsApp. I often would end up revisiting the photos on videos once I pulled out my phone, but being able to instantly see these messages as they came in tickled whatever part of my brain responds to instant gratification. There's some impressive tech inside those thick frames. Karissa Bell for Engadget The display also solves one of my biggest complaints with Meta's other smart glasses: that it's really difficult to frame photos. When you open the camera app on the display model, you can see a preview of the photo and even use a gesture to zoom in to properly frame your shot. Similarly, if you're on a WhatsApp video call you can see both the other person's video as well as a small preview of your own like you would on your phone's screen. It's a cool trick but the small display felt too cramped for a proper video call. People I used this with also told me that my video feed had some quality issues despite being on Wi-Fi. The glasses' live captioning and translation features are probably the best examples of Meta bringing its existing AI features into the display. I've written before about how Meta AI's translation abilities are one of my favorite features of the Ray-Ban smart glasses. Live translation on the display is even better, because it delivers a real-time text feed of what the person in front of you is saying. I tried it out with my husband, a native Spanish speaker, and it was even more natural than the non-display glasses because I didn't have to pause and wait for the audio to relay what he was saying. It still wasn't an exactly perfect translation, and there were still a few occasions when it didn't catch everything he said, but it made the process so much simpler overall. Likewise, live captions transcribes conversations in real-time into a similar text feed. I've found that it's a cool way to demo these glasses' capabilities, but I haven't yet found an occasion to use this in anything other than a demo. However, I still think it could be useful as an accessibility aid for anyone who has trouble hearing or processing audio. Another feature that's useful for travel is walking navigation. Dictate an address or location (you can say something like \"take me to the closest Starbucks\") and the glasses' display will guide you on your route. The first time I tried this was the roughly 10-minute walk from my bus stop to Yahoo's San Francisco office. The route only required two turns, but it didn't quite work. My glasses confidently navigated me to an alleyway behind the office building rather than the entrance. These kinds of mishaps happen with lots of mapping tools — Meta's maps rely on data from OpenStreetMap and Overture — but it was a good reminder that it's still early days for this product. I don't use Meta AI a ton on any of my smart glasses, but having a bit of visual feedback for these interactions was a nice change. I retain information much better from reading than listening, so seeing text-based output to my queries felt a lot more helpful. It's also nice that for longer responses from the assistant, you can stop the audio playback and swipe through informational cards instead. Meta AI on the glasses' display delivers information in a card-like interface. Meta While cooking dinner one night, I asked for a quick recipe for teriyaki salmon and Meta AI supplied what seemed like a passable recipe onto the display. The only drawback was the display goes to sleep pretty quickly unless you continue to interact with the content you're seeing, so the recipe I liked disappeared before I could actually attempt it. (You can view your Meta AI history in the Meta AI app if you really want to revisit something.) My main complaint is that I want to be able to do much more with the display. Messaging app integrations are nice, but I wish the display worked with more of the apps on my phone. When it worked best, I was happy to be able to view and dismiss messaging notifications without having to touch my phone; I just wish it worked with all my phone's notifications. There are also some frustrating limitations on sending and receiving texts. For example, there's no simple way to take a photo on your glasses and text it to a friend with the glasses. You have to wait for the glasses to send a \"preview\" of your message to your phone and then manually send the text. Or, you can opt in to Meta's cloud services and send the photo immediately as a link, but I'm not sure many of my friends would readily open a \"media.meta.com\" URL. The glasses also don't really support non-WhatsApp group chats, at least on iOS. You can receive messages sent in group chats, but there's no indication the message originated in a group thread. And, it's impossible to reply in the same thread; instead, replies are sent directly to the person who texted, which can get confusing if you're not checking your phone. It was also a little annoying that reading and even replying to texts from my glasses wouldn't mark the text as read in my phone's inbox. Meta blames all this on Apple's iOS restrictions, and says it's hoping to work with the company to improve the experience. The company tells me that group messaging should work normally for people with Android devices and that there is also a dedicated inbox for checking texts on the glasses. I haven’t tested this out yet. The band + battery life The glasses are controlled using Meta's Neural Band, which can translate subtle gestures like finger taps into actions on the display. Because the band relies on electromyography (EMG), you do need a fairly snug fit for it to work properly. I didn't find it uncomfortable, but, like the glasses, I don't love how it looks as a daily accessory. It also requires daily charging if you wear the glasses all day. But the band does work surprisingly well. In more than a week, it almost never missed a gesture, and it never falsely registered a gesture, despite my efforts to confuse it by fidgeting or rubbing my fingers together. The gestures themselves are also pretty intuitive and don't take long to get used to: double tapping your thumb and middle fingers wakes up or puts the display to sleep, single taps of your index and middle fingers allow you to select an item or go back, and swiping your thumb along the side of your index finger lets you navigate around the display. There are a few others, but those are the ones I used most often. The Meta Neural Band requires a snug fit to work properly. Karissa Bell for Engadget Each time you make a gesture, the band emits a small vibration so you get a bit of haptic feedback letting you know it registered. I've used hand tracking-based navigation in various VR, AR and mixed reality devices and I've always felt a bit goofy waving my hands around. But the neural band gestures work when your hand is by your side or in your pocket. The other major drawback of these glasses is that heavy use of the display drains the battery pretty quickly. Meta says the Ray-Ban Display’s battery can go about six hours on a single charge, but it really depends on how much you're using the display. With very limited use, l was able to stretch the battery to about seven hours, but if you're doing display-intensive tasks like video calling or live translation, it will die much, much more quickly. The Meta Ray-Ban Display glasses, charging case and neural band. Karissa Bell for Engadget The glasses do come with a charging case that can deliver a few extra charges on-the-go, but I was a bit surprised at how often I had to recharge the case. With my normal Ray-Ban Meta glasses I can go several days without topping up the charging case, but with the Meta Ray-Ban Display case, I'm charging it at least every other day. Privacy and safety Whenever I write or post on social media about a pair of Meta-branded glasses, I inevitably hear from people concerned about the privacy implications of these devices. As I wrote in my recent review of Meta's second-gen Ray-Ban glasses, I share a lot of these concerns. Meta has made subtle but meaningful changes to its glasses' privacy policy over the last year, and its track record suggests these devices will inevitably scoop up more of our data over time. In terms of privacy implications of the display-enabled glasses, there isn't a meaningful difference compared to their counterparts. Meta's policies are the same for all its wearables. I suppose you could use live translation to surreptitiously eavesdrop on a conversation you wouldn't typically understand, though that's technically possible with Meta's other glasses too. And the addition of a wrist-based controller means taking photos is a bit less obvious, but there's still an LED indicator that lights up when the camera is on. The neural band allows you to snap photos without touching the capture button or using a voice command. Karissa Bell for Engadget I have been surprised at how many people have asked me if these glasses have some kind of facial recognition abilities. I'm not sure if that's a sign of people's general distrust of Meta, or an assumption based on seeing similar glasses in sci-fi flicks, but I do think it's telling. (They don't, to be clear. Meta currently only uses facial recognition for two safety-related features on Facebook and Instagram.) Meta hasn't done much to earn people's trust when it comes to privacy, and I wish the company would use its growing wearables business to try to prove otherwise. On a more practical level, I have some safety concerns. The display didn't hinder my situational awareness while walking, but I could see how it might for others. And I'm definitely not comfortable using the display while driving. Meta does have an audio-only \"driving detection\" setting that can automatically kick in when you're traveling in a car, but the feature is optional, which seems potentially problematic. Should you buy these? In short: probably not. As much as I've been genuinely impressed with Meta's display tech, I don't think these glasses make sense for most people right now. And, at $800, the Meta Ray-Ban Display glasses are more than twice as much as the company's very good second-generation Ray-Ban glasses, which come in a wide range of much more normal-looking frame styles and colors. The Meta Ray-Ban Display glasses, on the other hand, still look very much like a first-gen product. There are some really compelling use cases for the display, but its functionality is limited. The glasses are also too thick and bulky for what's meant to be an everyday accessory. At the end of the day, most people want glasses that make them look good. There’s also the fact that right now, these glasses are somewhat difficult to actually buy. They are only available at a handful of physical retailers, which currently have a very limited supply, Meta is also requiring would-be buyers to schedule demo appointments in order to buy, though some stores — like the LensCrafters where I bought my pair — aren’t enforcing this. Still, there's a lot to be excited about. Watching people's reactions to trying these has been almost as much fun as using them myself. Meta also has a solid lineup of new features already in the works, including a standalone Reels app, a teleprompter and gesture-based handwriting for message replies. If you're already all-in on smart glasses or, like me, you've been patiently waiting for glasses with a high quality, usable display, then the Meta Ray-Ban Display glasses are worth the investment now — as long as you can accept the thick frames. Update, October 17, 2025, 3:42PM PT: Added more information about group text functionality on Android. This article originally appeared on Engadget at https://www.engadget.com/wearables/meta-ray-ban-display-review-chunky-frames-with-impressive-abilities-193127070.html?src=rss",
          "feed_position": 12,
          "image_url": "https://d29szjachogqwa.cloudfront.net/videos/user-uploaded/meta_display_sand_on_kb.jpg"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/security/cisco-warns-enterprises-without-tapping-machine-data-your-ai-strategy-is",
          "published_at": "Fri, 17 Oct 2025 18:10:00 GMT",
          "title": "Cisco warns enterprises: Without tapping machine data, your AI strategy is incomplete",
          "standfirst": "Cisco executives make the case that the distinction between product and model companies is disappearing, and that accessing the 55% of enterprise data growth that current AI ignores will separate winners from losers.VentureBeat recently caught up with Jeetu Patel, Cisco&#x27;s President and Chief Product Officer and DJ Sampath, Senior Vice President of AI Software and Platform, to gain new insights into a compelling thesis both leaders share. They and their teams contend that every successful product company must become an AI model company to survive the next decade.When one considers how compressed product lifecycles are becoming, combined with the many advantages of digital twin technology to accelerate time-to-market of next-gen products, the thesis makes sense.The conversation revealed why this transformation is inevitable, backed by solid data points. The team contends that 55% of all data growth is machine data that current AI models don&#x27;t touch. OpenAI&#x27;s Greg Brockman estimates we need 10 billion GPUs to give every human the AI agents they&#x27;ll need, and Cisco&#x27;s open source security model, Foundation-Sec-8B, has already seen 200,000 downloads on Hugging Face.Why the model is becoming the product VentureBeat: You&#x27;ve stated that in the future, every product company will become a model company. Why is this inevitable rather than just one possible path?Jeetu Patel: In the future, there&#x27;s no distinction between model companies and product companies. Great product companies will be model companies. The close tie-in between model and product is a closed loop. To enhance the product, you enhance the model, not just a UI shim.These companies being formed right now that are a thin shim on top of a model; their days are numbered. The true moat is the model you build that drives product behavior. This requires being simultaneously good at two things: building great models in domains where you have great data, and building great product experiences powered by those models in an iterative loop where the models adapt and evolve when you have product enhancement requests.DJ Sampath: This becomes even more critical when you think about things moving to agents. Agents are going to be governed by these models. Your moat is really going to be how well your model reacts to the changes it needs to.Harnessing machine data&#x27;s growth is key VentureBeat: You mentioned that 55% of data growth is machine data, yet current models aren&#x27;t trained on it. Why does this represent such a massive opportunity?Patel: So far, models have been very good at being trained on publicly available, human-generated data freely available on the internet. But we&#x27;re done with the amount of public data you could crawl. Where else do you go next? It&#x27;s all locked up inside enterprises.55% of data growth is machine data, but models are not trained on machine data. Every company says &#x27;my data is my moat,&#x27; but most don&#x27;t have an effective way to condition that data into an organized pipeline so they can train AI with it and harness its full potential.Imagine how much log data will be generated when agents work 24/7 and every human has 100 agents. Greg Brockman from OpenAI said if you assume every human has a GPU, you&#x27;re three orders of magnitude away from where you need to be; you need 10 billion GPUs. When you think that way, if you don&#x27;t train your models with machine data effectively, you&#x27;re incomplete in your ability to harness the full potential of AI.Sampath: Most of the models are being trained on public data. The data that&#x27;s inside enterprises is mostly machine data. We&#x27;re unlocking that machine data. We give each enterprise a starting model. Think of it as a starter kit. They&#x27;ll take that model and build applications and agents fine-tuned on their proprietary data inside their enterprises. We&#x27;re going to be a model company, but we&#x27;re also going to make it incredibly easy for every single enterprise to build their own models using the infrastructure we provide.Why hardware companies have an advantageVentureBeat: Many see hardware as a liability in the software and AI era. You argue the opposite. Why?Patel: A lot of people look down on hardware. I actually think hardware is a great asset to have, because if you know how to build great hardware and great software and great AI models and tie them all together, that&#x27;s when magic starts to happen.Think about what we can do by correlating machine data from logs with our time series model. If there&#x27;s a one-degree change in your switch or router, you might predict system failure in three days, something you couldn&#x27;t correlate before. You identify the change, reroute traffic to prevent problems, and solve the issue. Get much more predictive in outages and infrastructure stability.Cisco is the critical infrastructure company for AI. This completely changes the level of stability we can generate for our infrastructure. Manufacturing is one of the top industries for the data volume generated daily. Combined with agentic AI and accumulated metadata, it completely changes the competitive nature of manufacturing or asset-intensive industries. With enough data, they can transcend disruptions around tariffs or supply chain variations, getting them out of price and availability commoditization.Cisco&#x27;s deep commitment to Open SourceVentureBeat: Why make your security models open source when that seems to give away competitive advantage?Sampath: The cat is out of the bag; attackers also have access to open source models. The next step is equipping as many defenders as possible with models that make defense stronger. That&#x27;s really what we did at RSAC 2025 when we launched our open source model, Foundation-Sec-8B.Funding for open source initiatives has stalled. There&#x27;s an increased drain in the open source community, needing sustainable, collaborative funding sources. It&#x27;s a corporate responsibility to make these models available, plus it provides access to communities to start working with AI from a defense perspective.We&#x27;ve integrated ClamAV, a widely used open source antivirus tool, with Hugging Face, which hosts over 2 million models. Every single model gets scanned for malware. You have to ensure the AI supply chain is appropriately protected, and we&#x27;re at the forefront of doing that.Patel: We launched not just the security model that&#x27;s open source, but also one on Splunk for time series data. These correlate data; time series and security incident data, to be able to find very interesting outcomes. With 200,000 downloads on Hugging Face, we&#x27;re seeing resellers starting to build applications with it.Taking the customers&#x27; pulse after Cisco LiveVentureBeat: Following Cisco Live&#x27;s product launches, how are customers responding?Patel: There are three categories. First, completely ecstatic customers: &#x27;We&#x27;ve been asking for this for a while. Hallelujah.&#x27;Second, those saying &#x27;I&#x27;m going to try this out.&#x27; DJ shows them a demo with white glove treatment, they do a POC, and they&#x27;re dumbfounded that it&#x27;s even better than what we said in three minutes on stage.Third are skeptics who verify that every announcement comes out on the exact days. That group used to be much bigger three years ago. As it&#x27;s shrunk, we&#x27;ve seen meaningful improvements in our financial results and how the market sees us.We don&#x27;t talk about things three years out, only within a six-month window. The payload is so large that we have enough to discuss for six months. Our biggest challenge, frankly, is keeping our customers up to date with the velocity of innovation we have.Obsessing over customers, not hardwareVentureBeat: How are you migrating your hardware-centric installed base without creating too much disruption?Patel: Rather than fixating on &#x27;hardware versus software,&#x27; you start from where the customer is. Your strategy can no longer be a perimeter-based firewall for network security because the market has moved. It&#x27;s hyper-distributed. But you currently have firewalls that need efficient management.We&#x27;re giving you a fully refreshed firewall lineup. If you want to look at what we&#x27;ve done with public cloud, managing egress traffic with Multicloud Defense with zero trust, not just user-to-application, but application-to-application. We&#x27;ve built Hypershield technology. We&#x27;ve built a revolutionary Smart Switch. All managed by the same Security Cloud Control with AI Canvas on top.We tell our customers they can go at their own pace. Start with firewalls, move to Multicloud Defense, add Hypershield enforcement points with Cilium for observability, and add Smart Switches. You don&#x27;t have to add more complexity because we have a true platform advantage with Security Cloud Control. Rather than saying &#x27;forget everything and move to the new thing&#x27;, creating too much cognitive load, we start where the customer is and take them through the journey.What&#x27;s next: energizing global partners to turn AI into a revenue opportunityThe interview concluded with discussions of November&#x27;s Partner Summit in San Diego, where Cisco plans significant partner activation announcements. As Patel noted, \"Sustained, consistent emphasis is needed to get the entire reseller engine moving.\" VentureBeat is convinced that a globally strong partner organization is indispensable for any cybersecurity company to attain its long-term AI vision.",
          "content": "Cisco executives make the case that the distinction between product and model companies is disappearing, and that accessing the 55% of enterprise data growth that current AI ignores will separate winners from losers.VentureBeat recently caught up with Jeetu Patel, Cisco&#x27;s President and Chief Product Officer and DJ Sampath, Senior Vice President of AI Software and Platform, to gain new insights into a compelling thesis both leaders share. They and their teams contend that every successful product company must become an AI model company to survive the next decade.When one considers how compressed product lifecycles are becoming, combined with the many advantages of digital twin technology to accelerate time-to-market of next-gen products, the thesis makes sense.The conversation revealed why this transformation is inevitable, backed by solid data points. The team contends that 55% of all data growth is machine data that current AI models don&#x27;t touch. OpenAI&#x27;s Greg Brockman estimates we need 10 billion GPUs to give every human the AI agents they&#x27;ll need, and Cisco&#x27;s open source security model, Foundation-Sec-8B, has already seen 200,000 downloads on Hugging Face.Why the model is becoming the product VentureBeat: You&#x27;ve stated that in the future, every product company will become a model company. Why is this inevitable rather than just one possible path?Jeetu Patel: In the future, there&#x27;s no distinction between model companies and product companies. Great product companies will be model companies. The close tie-in between model and product is a closed loop. To enhance the product, you enhance the model, not just a UI shim.These companies being formed right now that are a thin shim on top of a model; their days are numbered. The true moat is the model you build that drives product behavior. This requires being simultaneously good at two things: building great models in domains where you have great data, and building great product experiences powered by those models in an iterative loop where the models adapt and evolve when you have product enhancement requests.DJ Sampath: This becomes even more critical when you think about things moving to agents. Agents are going to be governed by these models. Your moat is really going to be how well your model reacts to the changes it needs to.Harnessing machine data&#x27;s growth is key VentureBeat: You mentioned that 55% of data growth is machine data, yet current models aren&#x27;t trained on it. Why does this represent such a massive opportunity?Patel: So far, models have been very good at being trained on publicly available, human-generated data freely available on the internet. But we&#x27;re done with the amount of public data you could crawl. Where else do you go next? It&#x27;s all locked up inside enterprises.55% of data growth is machine data, but models are not trained on machine data. Every company says &#x27;my data is my moat,&#x27; but most don&#x27;t have an effective way to condition that data into an organized pipeline so they can train AI with it and harness its full potential.Imagine how much log data will be generated when agents work 24/7 and every human has 100 agents. Greg Brockman from OpenAI said if you assume every human has a GPU, you&#x27;re three orders of magnitude away from where you need to be; you need 10 billion GPUs. When you think that way, if you don&#x27;t train your models with machine data effectively, you&#x27;re incomplete in your ability to harness the full potential of AI.Sampath: Most of the models are being trained on public data. The data that&#x27;s inside enterprises is mostly machine data. We&#x27;re unlocking that machine data. We give each enterprise a starting model. Think of it as a starter kit. They&#x27;ll take that model and build applications and agents fine-tuned on their proprietary data inside their enterprises. We&#x27;re going to be a model company, but we&#x27;re also going to make it incredibly easy for every single enterprise to build their own models using the infrastructure we provide.Why hardware companies have an advantageVentureBeat: Many see hardware as a liability in the software and AI era. You argue the opposite. Why?Patel: A lot of people look down on hardware. I actually think hardware is a great asset to have, because if you know how to build great hardware and great software and great AI models and tie them all together, that&#x27;s when magic starts to happen.Think about what we can do by correlating machine data from logs with our time series model. If there&#x27;s a one-degree change in your switch or router, you might predict system failure in three days, something you couldn&#x27;t correlate before. You identify the change, reroute traffic to prevent problems, and solve the issue. Get much more predictive in outages and infrastructure stability.Cisco is the critical infrastructure company for AI. This completely changes the level of stability we can generate for our infrastructure. Manufacturing is one of the top industries for the data volume generated daily. Combined with agentic AI and accumulated metadata, it completely changes the competitive nature of manufacturing or asset-intensive industries. With enough data, they can transcend disruptions around tariffs or supply chain variations, getting them out of price and availability commoditization.Cisco&#x27;s deep commitment to Open SourceVentureBeat: Why make your security models open source when that seems to give away competitive advantage?Sampath: The cat is out of the bag; attackers also have access to open source models. The next step is equipping as many defenders as possible with models that make defense stronger. That&#x27;s really what we did at RSAC 2025 when we launched our open source model, Foundation-Sec-8B.Funding for open source initiatives has stalled. There&#x27;s an increased drain in the open source community, needing sustainable, collaborative funding sources. It&#x27;s a corporate responsibility to make these models available, plus it provides access to communities to start working with AI from a defense perspective.We&#x27;ve integrated ClamAV, a widely used open source antivirus tool, with Hugging Face, which hosts over 2 million models. Every single model gets scanned for malware. You have to ensure the AI supply chain is appropriately protected, and we&#x27;re at the forefront of doing that.Patel: We launched not just the security model that&#x27;s open source, but also one on Splunk for time series data. These correlate data; time series and security incident data, to be able to find very interesting outcomes. With 200,000 downloads on Hugging Face, we&#x27;re seeing resellers starting to build applications with it.Taking the customers&#x27; pulse after Cisco LiveVentureBeat: Following Cisco Live&#x27;s product launches, how are customers responding?Patel: There are three categories. First, completely ecstatic customers: &#x27;We&#x27;ve been asking for this for a while. Hallelujah.&#x27;Second, those saying &#x27;I&#x27;m going to try this out.&#x27; DJ shows them a demo with white glove treatment, they do a POC, and they&#x27;re dumbfounded that it&#x27;s even better than what we said in three minutes on stage.Third are skeptics who verify that every announcement comes out on the exact days. That group used to be much bigger three years ago. As it&#x27;s shrunk, we&#x27;ve seen meaningful improvements in our financial results and how the market sees us.We don&#x27;t talk about things three years out, only within a six-month window. The payload is so large that we have enough to discuss for six months. Our biggest challenge, frankly, is keeping our customers up to date with the velocity of innovation we have.Obsessing over customers, not hardwareVentureBeat: How are you migrating your hardware-centric installed base without creating too much disruption?Patel: Rather than fixating on &#x27;hardware versus software,&#x27; you start from where the customer is. Your strategy can no longer be a perimeter-based firewall for network security because the market has moved. It&#x27;s hyper-distributed. But you currently have firewalls that need efficient management.We&#x27;re giving you a fully refreshed firewall lineup. If you want to look at what we&#x27;ve done with public cloud, managing egress traffic with Multicloud Defense with zero trust, not just user-to-application, but application-to-application. We&#x27;ve built Hypershield technology. We&#x27;ve built a revolutionary Smart Switch. All managed by the same Security Cloud Control with AI Canvas on top.We tell our customers they can go at their own pace. Start with firewalls, move to Multicloud Defense, add Hypershield enforcement points with Cilium for observability, and add Smart Switches. You don&#x27;t have to add more complexity because we have a true platform advantage with Security Cloud Control. Rather than saying &#x27;forget everything and move to the new thing&#x27;, creating too much cognitive load, we start where the customer is and take them through the journey.What&#x27;s next: energizing global partners to turn AI into a revenue opportunityThe interview concluded with discussions of November&#x27;s Partner Summit in San Diego, where Cisco plans significant partner activation announcements. As Patel noted, \"Sustained, consistent emphasis is needed to get the entire reseller engine moving.\" VentureBeat is convinced that a globally strong partner organization is indispensable for any cybersecurity company to attain its long-term AI vision.",
          "feed_position": 2,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/1ooehT0PzSVnkDAGGhBfos/f7f832028725f8f1ca1af654c398ae7a/hero_2.jpg"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/codev-lets-enterprises-avoid-vibe-coding-hangovers-with-a-team-of-agents",
          "published_at": "Fri, 17 Oct 2025 17:45:00 GMT",
          "title": "Codev lets enterprises avoid vibe coding hangovers with a team of agents that generate and document code",
          "standfirst": "For many software developers using generative AI, vibe coding is a double-edged sword. The process delivers rapid prototypes but often leaves a trail of brittle, undocumented code that creates significant technical debt. A new open-source platform, Codev, addresses this by proposing a fundamental shift: treating the natural language conversation with an AI as part of the actual source code. Codev is based on SP(IDE)R, a framework designed to turn vibe-coding conversations into structured, versioned, and auditable assets that become part of the code repository.What is Codev?At its core, Codev is a methodology that treats natural language context as an integral part of the development lifecycle as opposed to a disposable artifact as is the case with vanilla vibe coding. According to co-founder Waleed Kadous, the goal is to invert the typical engineering workflow. \"A key principle of Codev is that documents like the specification are the actual code of the system,\" he told VentureBeat. \"It&#x27;s almost like natural language is compiled down into Typescript by our agents.\"This approach avoids the common pitfall where documentation is created after the fact, if at all.Its flagship protocol, SP(IDE)R, provides a lightweight but formal structure for building software. The process begins with Specify, where a human and multiple AI agents collaborate to turn a high-level request into concrete acceptance criteria. Next, in the Plan stage, an AI proposes a phased implementation, which is again reviewed. For each phase, the AI enters an IDE loop: it Implements the code, Defends it against bugs and regression with comprehensive tests, and Evaluates the result against the specification. The final step is Review, where the team documents lessons learned to update and improve the SP(IDE)R protocol itself for future projects.The framework’s key differentiator is its use of multiple agents and explicit human review at different stages. Kadous notes that each agent brings unique strengths to the review process. \"Gemini is extremely good at catching security issues,\" he said, citing a critical cross-site scripting (XSS) flaw and another bug that \"would have shared an OpenAI API key with the client, which could cost thousands of dollars.\" Meanwhile, \"GPT-5 is very good at understanding how to simplify a design.\" This structured review, with a human providing final approval at each stage, prevents the kind of runaway automation that leads to flawed code.The platform’s AI-native philosophy extends to its installation. There is no complex installer; instead, a user instructs their AI agent to apply the Codev GitHub repository to set up the project. The developers \"dogfooded\" their framework, using Codev to build Codev.“The key point here is that natural language is executable now, with the agent being the interpreter,” Kadous said. “This is great because it means it&#x27;s not a ‘blind’ integration of Codev, the agent gets to choose the best way to integrate it and can intelligently make decisions.”Codev case studyTo test the framework&#x27;s effectiveness, its creators ran a direct comparison between vanilla vibe-coding and Codev. They gave Claude Opus 4.1 a request to build a modern web-based todo manager. The first attempt used a conversational, vibe-coding approach. The result was a plausible-looking demo. However, an automated analysis conducted by three independent AI agents found that it had implemented 0% of the required functionality, contained no tests, and lacked a database or API.The second attempt used the same AI model and prompt but applied the SP(IDE)R protocol. This time, the AI produced a production-ready application with 32 source files, 100% of the specified functionality, five test suites, a SQLite database, and a complete RESTful API. Throughout this process, the human developers reported they never directly edited a single line of source code. While this was a single experiment, Kadous estimates the impact is substantial. \"Subjectively, it feels like I&#x27;m about three times as productive with Codev as without,\" he says. The quality also speaks for itself. \"I used LLMs as a judge, and one of them described the output like what a well-oiled engineering team would produce. That was exactly what I was aiming for.\"While the process is powerful, it redefines the developer&#x27;s role from a hands-on coder to a system architect and reviewer. According to Kadous, the initial spec and plan stages can each take between 45 minutes to two hours of focused collaboration. This is in contrast to the impression given by many vibe-coding platforms, where a single prompt and a few minutes of processing gives you a fully functional and scalable application.\"All of the value I add is in the background knowledge I apply to the specs and plans,\" he explains. He emphasizes that the framework is designed to augment, not replace, experienced talent. \"The people who will do the best... are senior engineers and above because they know the pitfalls... It just takes the senior engineer you already have and makes them much more productive.\"A future of human and AI collaborationFrameworks like Codev signal a shift where the primary creative act of software development moves from writing code to crafting precise, machine-readable specifications and plans. For enterprise teams, this means AI-generated code can become auditable, maintainable, and reliable. By capturing the entire development conversation in version control and enforcing it with CI, the process turns ephemeral chats into durable engineering assets.Codev proposes a future where the AI acts not as a chaotic assistant, but as a disciplined collaborator in a structured, human-led workflow. However, Kadous acknowledges this shift creates new challenges for the workforce. \"Senior engineers that reject AI outright will be outpaced by senior engineers who embrace it,\" he predicts. He also expresses concern for junior developers who may not get the chance \"to build their architectural chops,\" a skill that becomes even more critical when guiding AI. This highlights a central challenge for the industry: ensuring that as AI elevates top performers, it also creates pathways to develop the next generation of talent.",
          "content": "For many software developers using generative AI, vibe coding is a double-edged sword. The process delivers rapid prototypes but often leaves a trail of brittle, undocumented code that creates significant technical debt. A new open-source platform, Codev, addresses this by proposing a fundamental shift: treating the natural language conversation with an AI as part of the actual source code. Codev is based on SP(IDE)R, a framework designed to turn vibe-coding conversations into structured, versioned, and auditable assets that become part of the code repository.What is Codev?At its core, Codev is a methodology that treats natural language context as an integral part of the development lifecycle as opposed to a disposable artifact as is the case with vanilla vibe coding. According to co-founder Waleed Kadous, the goal is to invert the typical engineering workflow. \"A key principle of Codev is that documents like the specification are the actual code of the system,\" he told VentureBeat. \"It&#x27;s almost like natural language is compiled down into Typescript by our agents.\"This approach avoids the common pitfall where documentation is created after the fact, if at all.Its flagship protocol, SP(IDE)R, provides a lightweight but formal structure for building software. The process begins with Specify, where a human and multiple AI agents collaborate to turn a high-level request into concrete acceptance criteria. Next, in the Plan stage, an AI proposes a phased implementation, which is again reviewed. For each phase, the AI enters an IDE loop: it Implements the code, Defends it against bugs and regression with comprehensive tests, and Evaluates the result against the specification. The final step is Review, where the team documents lessons learned to update and improve the SP(IDE)R protocol itself for future projects.The framework’s key differentiator is its use of multiple agents and explicit human review at different stages. Kadous notes that each agent brings unique strengths to the review process. \"Gemini is extremely good at catching security issues,\" he said, citing a critical cross-site scripting (XSS) flaw and another bug that \"would have shared an OpenAI API key with the client, which could cost thousands of dollars.\" Meanwhile, \"GPT-5 is very good at understanding how to simplify a design.\" This structured review, with a human providing final approval at each stage, prevents the kind of runaway automation that leads to flawed code.The platform’s AI-native philosophy extends to its installation. There is no complex installer; instead, a user instructs their AI agent to apply the Codev GitHub repository to set up the project. The developers \"dogfooded\" their framework, using Codev to build Codev.“The key point here is that natural language is executable now, with the agent being the interpreter,” Kadous said. “This is great because it means it&#x27;s not a ‘blind’ integration of Codev, the agent gets to choose the best way to integrate it and can intelligently make decisions.”Codev case studyTo test the framework&#x27;s effectiveness, its creators ran a direct comparison between vanilla vibe-coding and Codev. They gave Claude Opus 4.1 a request to build a modern web-based todo manager. The first attempt used a conversational, vibe-coding approach. The result was a plausible-looking demo. However, an automated analysis conducted by three independent AI agents found that it had implemented 0% of the required functionality, contained no tests, and lacked a database or API.The second attempt used the same AI model and prompt but applied the SP(IDE)R protocol. This time, the AI produced a production-ready application with 32 source files, 100% of the specified functionality, five test suites, a SQLite database, and a complete RESTful API. Throughout this process, the human developers reported they never directly edited a single line of source code. While this was a single experiment, Kadous estimates the impact is substantial. \"Subjectively, it feels like I&#x27;m about three times as productive with Codev as without,\" he says. The quality also speaks for itself. \"I used LLMs as a judge, and one of them described the output like what a well-oiled engineering team would produce. That was exactly what I was aiming for.\"While the process is powerful, it redefines the developer&#x27;s role from a hands-on coder to a system architect and reviewer. According to Kadous, the initial spec and plan stages can each take between 45 minutes to two hours of focused collaboration. This is in contrast to the impression given by many vibe-coding platforms, where a single prompt and a few minutes of processing gives you a fully functional and scalable application.\"All of the value I add is in the background knowledge I apply to the specs and plans,\" he explains. He emphasizes that the framework is designed to augment, not replace, experienced talent. \"The people who will do the best... are senior engineers and above because they know the pitfalls... It just takes the senior engineer you already have and makes them much more productive.\"A future of human and AI collaborationFrameworks like Codev signal a shift where the primary creative act of software development moves from writing code to crafting precise, machine-readable specifications and plans. For enterprise teams, this means AI-generated code can become auditable, maintainable, and reliable. By capturing the entire development conversation in version control and enforcing it with CI, the process turns ephemeral chats into durable engineering assets.Codev proposes a future where the AI acts not as a chaotic assistant, but as a disciplined collaborator in a structured, human-led workflow. However, Kadous acknowledges this shift creates new challenges for the workforce. \"Senior engineers that reject AI outright will be outpaced by senior engineers who embrace it,\" he predicts. He also expresses concern for junior developers who may not get the chance \"to build their architectural chops,\" a skill that becomes even more critical when guiding AI. This highlights a central challenge for the industry: ensuring that as AI elevates top performers, it also creates pathways to develop the next generation of talent.",
          "feed_position": 3,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/74nSfOtaxxWUY7F5d25vJp/2f602a8058b366f3bdd73cd30d00cf58/AI-human_coding.jpg"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/audio/headphones/bose-quietcomfort-ultra-headphones-2nd-gen-review-impactful-upgrades-to-a-familiar-formula-150000709.html",
          "published_at": "Fri, 17 Oct 2025 15:00:00 +0000",
          "title": "Bose QuietComfort Ultra Headphones (2nd gen) review: Impactful upgrades to a familiar formula",
          "standfirst": "Bose took a different approach with its new products in 2025. Instead of entirely redesigning its QuietComfort Ultra lineup, the company unveiled upgraded second-generation models of the flagship-level earbuds and headphones. Like the QuietComfort Ultra Earbuds that debuted earlier this year, the new Quiet Comfort Ultra headphones ($449) don’t offer a comprehensive overhaul. However, the changes provide enough performance improvements to further cement these as the best noise-canceling headphones that you can buy right now. Trust me, we’re much better off with this revamped version than we would be with a year (or longer) wait for something brand new with the 2023 model. What’s new on the QuietComfort Ultra Headphones? Bose debuted a few new features on the second-gen QuietComfort Ultra Earbuds in August that it carried over to these new headphones. First, the company improved its already stellar active noise cancelation (ANC) with tweaks to its ActiveSense technology. Specifically, the system can respond to sudden spikes in environmental noise by adapting more precisely. It’s not something you’ll notice all the time, but when you need it, you’ll be glad it's there. Otherwise, the excellent ANC performance here is just as effective as it was on the previous model. More on that in a bit. The immersive Cinema Mode that Bose added to the QC Ultra Earbuds is also available on these headphones. It’s a sound profile that enhances dialogue clarity while keeping the rest of the soundstage as wide and enveloping as possible. I like it best for movies and TV, as the name suggests, but per Bose’s suggestion I also tried it with podcasts and audiobooks. Cinema Mode is probably overkill for those types of content, unless you’re listening to shows or titles with lots of background effects. One of the biggest changes on the second-gen QC Ultra Headphones is how Bose decided to handle power management. Most importantly, the company extended battery life in all use cases. With ANC on (and Immersive Audio off), you’ll get up to 30 hours of listening time. Turn off ANC and that jumps to 45 hours. When you decide to enable both ANC and Bose’s spatial Immersive Audio, you can expect up to 23 hours on a charge. Compared to those on the first-generation model, all of these numbers are up by at least five hours, which is a significant boost. These headphones rotate flat and fold in for compact transport. Billy Steele for Engadget Like the previous QC Ultra Headphones, this model has an automatic disconnection feature after 10 minutes of standby . But the company went a step further on this version by adding a low-power mode that the headphones enter after 30 minutes of idle time. And if you want to disconnect them quickly, you can rotate the earcups and lay them on a flat surface. That’ll make them go into a deeper standby mode that Bose says can run “for months.” All of this means you can effectively turn the new QuietComfort Ultra Headphones on and off by putting them on and taking them off. If you’re using them regularly, you’ll never have to press the power button. Sound-wise, the big upgrade on these headphones is the addition of lossless audio over USB-C. Like the AirPods Max, this model can be connected with a cable to your phone, tablet, laptop or desktop to stream or play higher-quality tunes from compatible services or your library. Bose says you can expect 16-bit 44.1kHz or 48kHz audio depending on your source. It’s yet another nice-to-have feature that’s becoming standard fare on premium wireless headphones. What else is good about the QuietComfort Ultra Headphones? Like most Bose over-ear headphones, the second-gen QuietComfort Ultra Headphones are supremely comfortable. Even for long periods of time, they never become a burden, and that’s thanks in large part to the soft, pillowy ear pads. I could easily wear these for an entire trans-Atlantic flight with minimal discomfort and I’ve been wearing them for entire workdays at home. As I already mentioned, the ANC performance here is still top-tier. In fact, these QC Ultra Headphones will soon replace the first-gen model on our best noise-canceling headphones list. Both the Immersion (ANC + spatial audio) and Quiet (just ANC) modes provide robust noise blocking that surpasses those by Sony, Sennheiser and others. If you’re making your buying decision based solely on ANC performance, this is the best option. You’ll enjoy relief from constant ambient noise sources like fans and sound machines, plus the QC Ultra Headphones do a respectable job with human voices. Heck, I couldn’t even hear my dog barking at the imminent threat from falling leaves outside. Lastly, Bose’s take on spatial audio is still quite good. The company calls it Immersive Audio and the feature doesn’t rely on specialized content like other headphones. Music sounds obviously fuller and slightly louder when the sound profile is active thanks to Bose’s method for upscaling stereo content. There’s also enhanced vocal clarity and elements like percussion and synths are less compressed than usual. The headphones lend a particularly airy feel to the tracks of Ruston Kelly’s Pale, Through the Window, an acoustic-driven collection of soulful, country-tinged tunes. His vocals float atop enveloping acoustic guitars and tight, punchy drums. What’s not so good about the QC Ultra Headphones? The Bose app gives you access to controls and customization. Billy Steele for Engadget The biggest issue with the second-gen QuietComfort Ultra Headphones is the price. To be clear, the likes of Sony, Apple and others charge around the same amount for their top-of-the-line models, but $449 is still a significant investment. If that’s too steep for you, Bose has the highly capable QuietComfort Headphones in its arsenal for a slightly more palatable $359. My other gripe is that the only real design change Bose made for the updated QC Ultra Headphones is that the metal headband yokes now have a gloss finish. Depending on your personal preference, this might be a dealbreaker for you. It’s least noticeable on the black and violet colorways, since these have a tone-on-tone look. After a few weeks with the bronze and tan Driftwood Sand hue, I’m not a fan of the more stylized aesthetic. It’s flashy, for sure, but it’s a tweak I could’ve done without. Wrap-up Similar to the second-gen QC Ultra Earbuds over the summer, Bose didn’t make huge upgrades for the updated version of the QC Ultra Headphones. But what you do get here is a decent improvement over its predecessor. The company devised an intuitive setup for power management and even addressed one of my main gripes with the original by adding support for lossless audio over USB-C. Plus, the extended battery life is significant in all sound modes, and not just by an hour or two here or there. To top it all off, the QuietComfort Ultra Headphones remain the best option for pure noise-blocking ability, and that’s not likely to change any time soon. This article originally appeared on Engadget at https://www.engadget.com/audio/headphones/bose-quietcomfort-ultra-headphones-2nd-gen-review-impactful-upgrades-to-a-familiar-formula-150000709.html?src=rss",
          "content": "Bose took a different approach with its new products in 2025. Instead of entirely redesigning its QuietComfort Ultra lineup, the company unveiled upgraded second-generation models of the flagship-level earbuds and headphones. Like the QuietComfort Ultra Earbuds that debuted earlier this year, the new Quiet Comfort Ultra headphones ($449) don’t offer a comprehensive overhaul. However, the changes provide enough performance improvements to further cement these as the best noise-canceling headphones that you can buy right now. Trust me, we’re much better off with this revamped version than we would be with a year (or longer) wait for something brand new with the 2023 model. What’s new on the QuietComfort Ultra Headphones? Bose debuted a few new features on the second-gen QuietComfort Ultra Earbuds in August that it carried over to these new headphones. First, the company improved its already stellar active noise cancelation (ANC) with tweaks to its ActiveSense technology. Specifically, the system can respond to sudden spikes in environmental noise by adapting more precisely. It’s not something you’ll notice all the time, but when you need it, you’ll be glad it's there. Otherwise, the excellent ANC performance here is just as effective as it was on the previous model. More on that in a bit. The immersive Cinema Mode that Bose added to the QC Ultra Earbuds is also available on these headphones. It’s a sound profile that enhances dialogue clarity while keeping the rest of the soundstage as wide and enveloping as possible. I like it best for movies and TV, as the name suggests, but per Bose’s suggestion I also tried it with podcasts and audiobooks. Cinema Mode is probably overkill for those types of content, unless you’re listening to shows or titles with lots of background effects. One of the biggest changes on the second-gen QC Ultra Headphones is how Bose decided to handle power management. Most importantly, the company extended battery life in all use cases. With ANC on (and Immersive Audio off), you’ll get up to 30 hours of listening time. Turn off ANC and that jumps to 45 hours. When you decide to enable both ANC and Bose’s spatial Immersive Audio, you can expect up to 23 hours on a charge. Compared to those on the first-generation model, all of these numbers are up by at least five hours, which is a significant boost. These headphones rotate flat and fold in for compact transport. Billy Steele for Engadget Like the previous QC Ultra Headphones, this model has an automatic disconnection feature after 10 minutes of standby . But the company went a step further on this version by adding a low-power mode that the headphones enter after 30 minutes of idle time. And if you want to disconnect them quickly, you can rotate the earcups and lay them on a flat surface. That’ll make them go into a deeper standby mode that Bose says can run “for months.” All of this means you can effectively turn the new QuietComfort Ultra Headphones on and off by putting them on and taking them off. If you’re using them regularly, you’ll never have to press the power button. Sound-wise, the big upgrade on these headphones is the addition of lossless audio over USB-C. Like the AirPods Max, this model can be connected with a cable to your phone, tablet, laptop or desktop to stream or play higher-quality tunes from compatible services or your library. Bose says you can expect 16-bit 44.1kHz or 48kHz audio depending on your source. It’s yet another nice-to-have feature that’s becoming standard fare on premium wireless headphones. What else is good about the QuietComfort Ultra Headphones? Like most Bose over-ear headphones, the second-gen QuietComfort Ultra Headphones are supremely comfortable. Even for long periods of time, they never become a burden, and that’s thanks in large part to the soft, pillowy ear pads. I could easily wear these for an entire trans-Atlantic flight with minimal discomfort and I’ve been wearing them for entire workdays at home. As I already mentioned, the ANC performance here is still top-tier. In fact, these QC Ultra Headphones will soon replace the first-gen model on our best noise-canceling headphones list. Both the Immersion (ANC + spatial audio) and Quiet (just ANC) modes provide robust noise blocking that surpasses those by Sony, Sennheiser and others. If you’re making your buying decision based solely on ANC performance, this is the best option. You’ll enjoy relief from constant ambient noise sources like fans and sound machines, plus the QC Ultra Headphones do a respectable job with human voices. Heck, I couldn’t even hear my dog barking at the imminent threat from falling leaves outside. Lastly, Bose’s take on spatial audio is still quite good. The company calls it Immersive Audio and the feature doesn’t rely on specialized content like other headphones. Music sounds obviously fuller and slightly louder when the sound profile is active thanks to Bose’s method for upscaling stereo content. There’s also enhanced vocal clarity and elements like percussion and synths are less compressed than usual. The headphones lend a particularly airy feel to the tracks of Ruston Kelly’s Pale, Through the Window, an acoustic-driven collection of soulful, country-tinged tunes. His vocals float atop enveloping acoustic guitars and tight, punchy drums. What’s not so good about the QC Ultra Headphones? The Bose app gives you access to controls and customization. Billy Steele for Engadget The biggest issue with the second-gen QuietComfort Ultra Headphones is the price. To be clear, the likes of Sony, Apple and others charge around the same amount for their top-of-the-line models, but $449 is still a significant investment. If that’s too steep for you, Bose has the highly capable QuietComfort Headphones in its arsenal for a slightly more palatable $359. My other gripe is that the only real design change Bose made for the updated QC Ultra Headphones is that the metal headband yokes now have a gloss finish. Depending on your personal preference, this might be a dealbreaker for you. It’s least noticeable on the black and violet colorways, since these have a tone-on-tone look. After a few weeks with the bronze and tan Driftwood Sand hue, I’m not a fan of the more stylized aesthetic. It’s flashy, for sure, but it’s a tweak I could’ve done without. Wrap-up Similar to the second-gen QC Ultra Earbuds over the summer, Bose didn’t make huge upgrades for the updated version of the QC Ultra Headphones. But what you do get here is a decent improvement over its predecessor. The company devised an intuitive setup for power management and even addressed one of my main gripes with the original by adding support for lossless audio over USB-C. Plus, the extended battery life is significant in all sound modes, and not just by an hour or two here or there. To top it all off, the QuietComfort Ultra Headphones remain the best option for pure noise-blocking ability, and that’s not likely to change any time soon. This article originally appeared on Engadget at https://www.engadget.com/audio/headphones/bose-quietcomfort-ultra-headphones-2nd-gen-review-impactful-upgrades-to-a-familiar-formula-150000709.html?src=rss",
          "feed_position": 20,
          "image_url": "https://d29szjachogqwa.cloudfront.net/videos/user-uploaded/DSC_5360.jpg"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/data-infrastructure/worlds-largest-open-source-multimodal-dataset-delivers-17x-training",
          "published_at": "Fri, 17 Oct 2025 13:00:00 GMT",
          "title": "World's largest open-source multimodal dataset delivers 17x training efficiency, unlocking enterprise AI that connects documents, audio and video",
          "standfirst": "AI models are only as good as the data they&#x27;re trained on. That data generally needs to be labeled, curated and organized before models can learn from it in an effective way.One of the big missing links in the AI ecosystem has been the availability of a large high-quality open-source multimodal dataset. That changes today with the debut of the EMM-1 dataset which is comprised of 1 billion data pairs and 100M data groups across 5 modalities: text, image, video, audio and 3d point clouds. Multimodal datasets combine different types of data that AI systems can process together. This mirrors how humans perceive the world using multiple senses simultaneously. These datasets enable AI systems to make richer inferences by understanding relationships across data types, rather than processing each modality in isolation.EMM-1 is developed by data labeling platform vendor Encord. The company&#x27;s platform enables teams to curate, label and manage training data at scale using both automated and human-in-the-loop workflows. Alongside the new model, Encord developed the EBind training methodology that prioritizes data quality over raw computational scale. The approach enabled a compact 1.8 billion parameter model to match the performance of models up to 17 times larger while slashing training time from days to hours on a single GPU rather than GPU clusters.\"The big trick for us was to really focus on the data and to make the data very, very high quality,\" Encord Co-Founder and CEO Eric Landau told VentureBeat in an exclusive interview. \"We were able to get to the same level of performance as models 20 times larger, not because we were super clever on the architecture, but because we trained it with really good data overall.\"The data quality advantageEncord&#x27;s dataset is 100 times larger than the next comparable multimodal dataset, according to Landau. It operates at petabyte scale with terabytes of raw data and over 1 million human annotations.But scale alone doesn&#x27;t explain the performance gains. The technical innovation centers on addressing what Landau calls an \"under-appreciated\" problem in AI training: data leakage between training and evaluation sets.\"The leakage problem was one which we spent a lot of time on,\" Landau explained. \"In a lot of data sets, there is a kind of leakage between different subsets of the data. Leakage actually boosts your results. It makes your evaluations look better. But it&#x27;s one thing that we were quite diligent about.\"Data leakage occurs when information from test data inadvertently appears in training data, artificially inflating model performance metrics. Many benchmark datasets suffer from this contamination. Encord deployed hierarchical clustering techniques to ensure clean separation while maintaining representative distribution across data types. The company also used clustering to address bias and ensure diverse representation. How EBind boosts efficiencyThe data quality improvements work in tandem with an architectural approach designed for efficiencyEncord&#x27;s EBind extends the CLIP (Contrastive Language-Image Pre-training) approach (originally developed by OpenAI) from two modalities to five. CLIP learns to associate images and text in a shared representation space, enabling tasks like searching for images using text descriptions.Where CLIP learns to associate images and text in a shared latent space, EBind does the same across images, text, audio, 3D point clouds and video.The architectural choice prioritizes parameter efficiency. Rather than deploying separate specialized models for each modality pair, EBind uses a single base model with one encoder per modality.\"Other methodologies, what they do is they use a bunch of different models, and they route to the best model for embedding these pairs, so they tend to explode in the number of parameters,\" Landau said. \"We found we could use a single base model and just train one encoder per modality, so keeping it very simple and very parameter efficient, if we fed that overall architecture really, really good data.\"The resulting model rivals OmniBind, a much larger competitor in the multimodal space, but requires dramatically fewer computational resources for both training and inference. This makes EBind deployable in resource-constrained environments including edge devices for robotics and autonomous systems.The enterprise value of a multi-modal datasetMultimodal models enable enterprise use cases that span different data types.Most organizations store different data types in separate systems: documents in content management platforms, audio recordings in communication tools, training videos in learning management systems and structured data in databases. Multimodal models can search and retrieve across all of these simultaneously.\"Enterprises have all different types of data. They don&#x27;t just have documents. They have audio recordings, and they have training videos, and they have CSV files,\" Landau said. \"Let&#x27;s say you&#x27;re a lawyer and you have a case file that has video evidence and also documents and recordings, and it&#x27;s all scattered across a lot of silos of data. You can use EBind to pick all of the relevant data and bundle together to search and surface the right data much quicker than you would have before.\"The same principle applies across verticals. Healthcare providers can link patient imaging data to clinical notes and diagnostic audio. Financial services firms can connect transaction records to compliance call recordings and customer communications. Manufacturing operations can tie equipment sensor data to maintenance video logs and inspection reports.Beyond office environments, physical AI represents another frontier. Landau highlighted autonomous vehicles that benefit from both visual perception and audio cues like emergency sirens. In manufacturing and warehousing, robots that combine visual recognition with audio feedback and spatial awareness can operate more safely and effectively than vision-only systems.Enterprise use case: Extending computer vision with multimodal contextCaptur AI, an Encord customer, illustrates how companies are planning to use the dataset for specific business applications. The startup provides on-device image verification for mobile apps, validating photos in real-time for authenticity, compliance and quality before upload. The company works with shared mobility providers like Lime and delivery companies capturing billions of package photos.Captur AI processes over 100 million images on-device and specializes in distilling models to 6-10 megabytes so they can run on smartphones without cloud connectivity. But CEO Charlotte Bax sees multimodal capabilities as critical for expanding into higher-value use cases.\"The market for us is massive. You submit photos for returns and retails. You submit photos to insurance companies for claims. You submit photos when you&#x27;re listing something on eBay,\" Bax told VentureBeat in an exclusive interview. \"Some of those use cases are very high risk or high value if something goes wrong, like insurance, the image only captures part of the context and audio can be an important signal.\"Bax cited digital vehicle inspections as a prime example. When customers photograph vehicle damage for insurance claims, they often describe what happened verbally while capturing images. Audio context can significantly improve claim accuracy and reduce fraud.\"As you&#x27;re doing that, oftentimes the customer is actually describing what&#x27;s happened,\" Bax said. \"A few of our potential prospects in InsurTech have asked us if we can actually do audio as well, because then that adds this additional bit of context for the user who&#x27;s submitting the claim.\"The challenge lies in maintaining Captur AI&#x27;s core advantage: running models efficiently on-device rather than requiring cloud processing. The company plans to use Encord&#x27;s dataset to train compact multimodal models that preserve real-time, offline capabilities while adding audio and sequential image context.\"The most important thing you can do is try and get as much context as possible,\" Bax said. \"Can you get LLMs to be small enough to run on a device within the next three years, or can you run multimodal models on the device? Solving data quality before image upload is the interesting frontier.\"What this means for enterprisesEncord&#x27;s results challenge fundamental assumptions about AI development and suggest that the next competitive battleground may be data operations rather than infrastructure scale.Multimodal datasets unlock new capabilities. The ability to train models that understand relationships across data types opens use cases that single-modality systems cannot address.Data operations deserve equal investment with compute infrastructure. The 17x parameter efficiency gain from better data curation represents orders of magnitude in cost savings. Organizations pouring resources into GPU clusters while treating data quality as an afterthought may be optimizing the wrong variable.For enterprises building multimodal AI systems, Landau&#x27;s assessment captures the strategic shift. \"We were able to get to the same level of performance as models much larger, not because we were super clever on the architecture, but because we trained it with really good data overall,\" he said.",
          "content": "AI models are only as good as the data they&#x27;re trained on. That data generally needs to be labeled, curated and organized before models can learn from it in an effective way.One of the big missing links in the AI ecosystem has been the availability of a large high-quality open-source multimodal dataset. That changes today with the debut of the EMM-1 dataset which is comprised of 1 billion data pairs and 100M data groups across 5 modalities: text, image, video, audio and 3d point clouds. Multimodal datasets combine different types of data that AI systems can process together. This mirrors how humans perceive the world using multiple senses simultaneously. These datasets enable AI systems to make richer inferences by understanding relationships across data types, rather than processing each modality in isolation.EMM-1 is developed by data labeling platform vendor Encord. The company&#x27;s platform enables teams to curate, label and manage training data at scale using both automated and human-in-the-loop workflows. Alongside the new model, Encord developed the EBind training methodology that prioritizes data quality over raw computational scale. The approach enabled a compact 1.8 billion parameter model to match the performance of models up to 17 times larger while slashing training time from days to hours on a single GPU rather than GPU clusters.\"The big trick for us was to really focus on the data and to make the data very, very high quality,\" Encord Co-Founder and CEO Eric Landau told VentureBeat in an exclusive interview. \"We were able to get to the same level of performance as models 20 times larger, not because we were super clever on the architecture, but because we trained it with really good data overall.\"The data quality advantageEncord&#x27;s dataset is 100 times larger than the next comparable multimodal dataset, according to Landau. It operates at petabyte scale with terabytes of raw data and over 1 million human annotations.But scale alone doesn&#x27;t explain the performance gains. The technical innovation centers on addressing what Landau calls an \"under-appreciated\" problem in AI training: data leakage between training and evaluation sets.\"The leakage problem was one which we spent a lot of time on,\" Landau explained. \"In a lot of data sets, there is a kind of leakage between different subsets of the data. Leakage actually boosts your results. It makes your evaluations look better. But it&#x27;s one thing that we were quite diligent about.\"Data leakage occurs when information from test data inadvertently appears in training data, artificially inflating model performance metrics. Many benchmark datasets suffer from this contamination. Encord deployed hierarchical clustering techniques to ensure clean separation while maintaining representative distribution across data types. The company also used clustering to address bias and ensure diverse representation. How EBind boosts efficiencyThe data quality improvements work in tandem with an architectural approach designed for efficiencyEncord&#x27;s EBind extends the CLIP (Contrastive Language-Image Pre-training) approach (originally developed by OpenAI) from two modalities to five. CLIP learns to associate images and text in a shared representation space, enabling tasks like searching for images using text descriptions.Where CLIP learns to associate images and text in a shared latent space, EBind does the same across images, text, audio, 3D point clouds and video.The architectural choice prioritizes parameter efficiency. Rather than deploying separate specialized models for each modality pair, EBind uses a single base model with one encoder per modality.\"Other methodologies, what they do is they use a bunch of different models, and they route to the best model for embedding these pairs, so they tend to explode in the number of parameters,\" Landau said. \"We found we could use a single base model and just train one encoder per modality, so keeping it very simple and very parameter efficient, if we fed that overall architecture really, really good data.\"The resulting model rivals OmniBind, a much larger competitor in the multimodal space, but requires dramatically fewer computational resources for both training and inference. This makes EBind deployable in resource-constrained environments including edge devices for robotics and autonomous systems.The enterprise value of a multi-modal datasetMultimodal models enable enterprise use cases that span different data types.Most organizations store different data types in separate systems: documents in content management platforms, audio recordings in communication tools, training videos in learning management systems and structured data in databases. Multimodal models can search and retrieve across all of these simultaneously.\"Enterprises have all different types of data. They don&#x27;t just have documents. They have audio recordings, and they have training videos, and they have CSV files,\" Landau said. \"Let&#x27;s say you&#x27;re a lawyer and you have a case file that has video evidence and also documents and recordings, and it&#x27;s all scattered across a lot of silos of data. You can use EBind to pick all of the relevant data and bundle together to search and surface the right data much quicker than you would have before.\"The same principle applies across verticals. Healthcare providers can link patient imaging data to clinical notes and diagnostic audio. Financial services firms can connect transaction records to compliance call recordings and customer communications. Manufacturing operations can tie equipment sensor data to maintenance video logs and inspection reports.Beyond office environments, physical AI represents another frontier. Landau highlighted autonomous vehicles that benefit from both visual perception and audio cues like emergency sirens. In manufacturing and warehousing, robots that combine visual recognition with audio feedback and spatial awareness can operate more safely and effectively than vision-only systems.Enterprise use case: Extending computer vision with multimodal contextCaptur AI, an Encord customer, illustrates how companies are planning to use the dataset for specific business applications. The startup provides on-device image verification for mobile apps, validating photos in real-time for authenticity, compliance and quality before upload. The company works with shared mobility providers like Lime and delivery companies capturing billions of package photos.Captur AI processes over 100 million images on-device and specializes in distilling models to 6-10 megabytes so they can run on smartphones without cloud connectivity. But CEO Charlotte Bax sees multimodal capabilities as critical for expanding into higher-value use cases.\"The market for us is massive. You submit photos for returns and retails. You submit photos to insurance companies for claims. You submit photos when you&#x27;re listing something on eBay,\" Bax told VentureBeat in an exclusive interview. \"Some of those use cases are very high risk or high value if something goes wrong, like insurance, the image only captures part of the context and audio can be an important signal.\"Bax cited digital vehicle inspections as a prime example. When customers photograph vehicle damage for insurance claims, they often describe what happened verbally while capturing images. Audio context can significantly improve claim accuracy and reduce fraud.\"As you&#x27;re doing that, oftentimes the customer is actually describing what&#x27;s happened,\" Bax said. \"A few of our potential prospects in InsurTech have asked us if we can actually do audio as well, because then that adds this additional bit of context for the user who&#x27;s submitting the claim.\"The challenge lies in maintaining Captur AI&#x27;s core advantage: running models efficiently on-device rather than requiring cloud processing. The company plans to use Encord&#x27;s dataset to train compact multimodal models that preserve real-time, offline capabilities while adding audio and sequential image context.\"The most important thing you can do is try and get as much context as possible,\" Bax said. \"Can you get LLMs to be small enough to run on a device within the next three years, or can you run multimodal models on the device? Solving data quality before image upload is the interesting frontier.\"What this means for enterprisesEncord&#x27;s results challenge fundamental assumptions about AI development and suggest that the next competitive battleground may be data operations rather than infrastructure scale.Multimodal datasets unlock new capabilities. The ability to train models that understand relationships across data types opens use cases that single-modality systems cannot address.Data operations deserve equal investment with compute infrastructure. The 17x parameter efficiency gain from better data curation represents orders of magnitude in cost savings. Organizations pouring resources into GPU clusters while treating data quality as an afterthought may be optimizing the wrong variable.For enterprises building multimodal AI systems, Landau&#x27;s assessment captures the strategic shift. \"We were able to get to the same level of performance as models much larger, not because we were super clever on the architecture, but because we trained it with really good data overall,\" he said.",
          "feed_position": 4,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/79Sf3Ti0NPEcvRc7c5dqV9/695d4798edd273cafd650056133fdfe7/multimodal-dataset-smk.png"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/todays-best-ipad-deals-include-50-off-the-256gb-ipad-a16-150020379.html",
          "published_at": "Fri, 17 Oct 2025 12:33:09 +0000",
          "title": "Today's best iPad deals include $50 off the 256GB iPad A16",
          "standfirst": "We generally think Apple’s iPads are the best tablets for most people, but they usually don’t come cheap. To help those looking to grab one today get the most value possible, we’re keeping an eye on sale prices and rounding up the best iPad deals we can find each week.Unfortunately, after the barrage of discounts we saw during Prime Day and other retailer sales last week, the selection available now is back to being pretty light. Most models in the iPad, iPad Air and iPad mini families aren’t significantly discounted, and the new iPad Pros were only just announced on Wednesday, so there aren’t any deals of note for those just yet. Call it a pre-Black Friday lull. That said, a couple higher-capacity configurations of the Air and base iPad are still on sale, including the 256GB version of the iPad (A16) for $50 off. And beyond tablets, we’re also seeing healthy price drops for devices like the AirPods 4, AirTags and Mac mini. Here are all the top deals on Apple gear we could find this week. Best iPad deals Apple iPad Air (11-inch, M3, 1TB) for $949 ($150 off MSRP): The most recent iPad Air is a relatively minor update, as the only major addition is a more powerful M3 chip. However, we still recommend the Air over the base model in our iPad buying guide: Its display is laminated, more color-rich and better at fending off glare (though it's still 60Hz); its speakers are more robust; it works with Apple’s best accessories and its performance should hold up better in the years ahead. This deal is only for the maxed-out model with 1TB of storage, but it ties the lowest price we've seen all the same. Apple iPad Air (13-inch, M3, 512GB, Cellular) for $1,000 ($250 off): Engadget's Nate Ingraham gave the 13-inch iPad Air a score of 89 when it was released in March. It has a bigger and slightly brighter display than its 11-inch counterpart but is otherwise the same. If you plan to keep your iPad hooked up to a keyboard, the extra screen space is lovely for multitasking or just taking in movies. This discount is an all-time low, but it only applies to the 512GB model with built-in cellular support, so it's certainly not for everyone. Best Apple deals Apple AirTags (4-pack) for $65 ($34 off): We may see an updated model by the end of the year, but the current AirTags are the best Bluetooth trackers for iPhone owners right now thanks to their vast finding network and accurate ultra-wideband tech that makes it easy to locate nearby items. Just note that you'll need a separate AirTag holder to attach them to your keys, wallet or bag. This deal comes within a dollar of the lowest price we've seen for a four-pack. Also at Walmart. Apple Pencil Pro for $99 ($30 off): The top-end option in Apple’s confusing stylus lineup, the Pencil Pro supports pressure sensitivity, wireless charging, tilt detection, haptic feedback and Apple’s double tap and squeeze gestures, among other perks. It’s a lovely tool for more intricate sketching and note-taking, but the catch is that it’s only compatible with the M4 iPad Pro, M2 and M3 iPad Air and most recent iPad mini. We've seen this deal fairly often over the course of the year, but it's a fine discount compared to buying from Apple directly. Also at Walmart. Apple MacBook Air (13-inch, M4, 512GB) for $999 ($200 off): Apple's latest MacBook Air is the top pick in our guide to the best laptops, and it earned a score of 92 in our review. It's not a major overhaul, but the design is still exceptionally thin, light and well-built, with long battery life and a top-notch keyboard and trackpad. Now it's a bit faster. (Though we'd still love more ports and a refresh rate higher than 60Hz.) This discount ties the all-time low for the model with 16GB of RAM and a 512GB SSD. Apple Mac mini (M4) for $499 ($100 off): The newest version of Apple’s tiny desktop PC has a smaller overall footprint, a faster M4 chip, 16GB of RAM as standard (finally), two front-facing USB-C ports, an extra Thunderbolt 4 port and the ability to drive three external displays. It doesn't have any USB-A ports, however. We gave the M4 Pro model a review score of 90. This deal is for the entry-level version with a base M4 chip, 16GB of RAM and a 256GB SSD — we’ve seen it fall as low as $469 in the past, but this is still a decent savings. Also at Best Buy, Walmart and B&H. Apple iMac (M4) for $1,149 ($150 off): We like the M4 iMac as an all-in-one computer thanks to its powerful performance, standard 16GB of RAM and improved webcam. Just note that it only comes in a 24-inch screen size option. This deal on the base model isn't quite an all-time low, but it's roughly $40 lower than the desktop's usual street price and a decent savings compared to buying directly from Apple. Apple Watch Series 11 (GPS, 42mm) for $389 ($10 off): The latest flagship Apple Watch only hit store shelves last month, but Amazon is already selling it for $10 off. It doesn't show up as a percentage off, but you'll see some models listed at $389 instead of Apple's $399 MSRP. If you're new to Apple's wearables or are ready to upgrade from a Series 9 or older, this is a good model to grab. If you're coming from a Series 10, however, there's not much need to upgrade as the only major change from last year's model is a slightly larger battery and a tougher screen. Apple Watch SE 3 (GPS, 40mm) for $240 ($9 off): There's a similar stealth discount for the newest budget model, the Apple Watch SE 3, at Amazon. It normally goes for $249 — again, not a big discount, but better than nothing if you're looking to get onboard early. Apple gave this model some badly needed updates compared to its predecessor, including an always-on display, faster charging, better sensors and the same processor that you'll find in the new Apple Watch Series 11. Read more Apple coverage: The best AirPods The best Apple Watches The best MacBooks The best iPhones The best iPads Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/todays-best-ipad-deals-include-50-off-the-256gb-ipad-a16-150020379.html?src=rss",
          "content": "We generally think Apple’s iPads are the best tablets for most people, but they usually don’t come cheap. To help those looking to grab one today get the most value possible, we’re keeping an eye on sale prices and rounding up the best iPad deals we can find each week.Unfortunately, after the barrage of discounts we saw during Prime Day and other retailer sales last week, the selection available now is back to being pretty light. Most models in the iPad, iPad Air and iPad mini families aren’t significantly discounted, and the new iPad Pros were only just announced on Wednesday, so there aren’t any deals of note for those just yet. Call it a pre-Black Friday lull. That said, a couple higher-capacity configurations of the Air and base iPad are still on sale, including the 256GB version of the iPad (A16) for $50 off. And beyond tablets, we’re also seeing healthy price drops for devices like the AirPods 4, AirTags and Mac mini. Here are all the top deals on Apple gear we could find this week. Best iPad deals Apple iPad Air (11-inch, M3, 1TB) for $949 ($150 off MSRP): The most recent iPad Air is a relatively minor update, as the only major addition is a more powerful M3 chip. However, we still recommend the Air over the base model in our iPad buying guide: Its display is laminated, more color-rich and better at fending off glare (though it's still 60Hz); its speakers are more robust; it works with Apple’s best accessories and its performance should hold up better in the years ahead. This deal is only for the maxed-out model with 1TB of storage, but it ties the lowest price we've seen all the same. Apple iPad Air (13-inch, M3, 512GB, Cellular) for $1,000 ($250 off): Engadget's Nate Ingraham gave the 13-inch iPad Air a score of 89 when it was released in March. It has a bigger and slightly brighter display than its 11-inch counterpart but is otherwise the same. If you plan to keep your iPad hooked up to a keyboard, the extra screen space is lovely for multitasking or just taking in movies. This discount is an all-time low, but it only applies to the 512GB model with built-in cellular support, so it's certainly not for everyone. Best Apple deals Apple AirTags (4-pack) for $65 ($34 off): We may see an updated model by the end of the year, but the current AirTags are the best Bluetooth trackers for iPhone owners right now thanks to their vast finding network and accurate ultra-wideband tech that makes it easy to locate nearby items. Just note that you'll need a separate AirTag holder to attach them to your keys, wallet or bag. This deal comes within a dollar of the lowest price we've seen for a four-pack. Also at Walmart. Apple Pencil Pro for $99 ($30 off): The top-end option in Apple’s confusing stylus lineup, the Pencil Pro supports pressure sensitivity, wireless charging, tilt detection, haptic feedback and Apple’s double tap and squeeze gestures, among other perks. It’s a lovely tool for more intricate sketching and note-taking, but the catch is that it’s only compatible with the M4 iPad Pro, M2 and M3 iPad Air and most recent iPad mini. We've seen this deal fairly often over the course of the year, but it's a fine discount compared to buying from Apple directly. Also at Walmart. Apple MacBook Air (13-inch, M4, 512GB) for $999 ($200 off): Apple's latest MacBook Air is the top pick in our guide to the best laptops, and it earned a score of 92 in our review. It's not a major overhaul, but the design is still exceptionally thin, light and well-built, with long battery life and a top-notch keyboard and trackpad. Now it's a bit faster. (Though we'd still love more ports and a refresh rate higher than 60Hz.) This discount ties the all-time low for the model with 16GB of RAM and a 512GB SSD. Apple Mac mini (M4) for $499 ($100 off): The newest version of Apple’s tiny desktop PC has a smaller overall footprint, a faster M4 chip, 16GB of RAM as standard (finally), two front-facing USB-C ports, an extra Thunderbolt 4 port and the ability to drive three external displays. It doesn't have any USB-A ports, however. We gave the M4 Pro model a review score of 90. This deal is for the entry-level version with a base M4 chip, 16GB of RAM and a 256GB SSD — we’ve seen it fall as low as $469 in the past, but this is still a decent savings. Also at Best Buy, Walmart and B&H. Apple iMac (M4) for $1,149 ($150 off): We like the M4 iMac as an all-in-one computer thanks to its powerful performance, standard 16GB of RAM and improved webcam. Just note that it only comes in a 24-inch screen size option. This deal on the base model isn't quite an all-time low, but it's roughly $40 lower than the desktop's usual street price and a decent savings compared to buying directly from Apple. Apple Watch Series 11 (GPS, 42mm) for $389 ($10 off): The latest flagship Apple Watch only hit store shelves last month, but Amazon is already selling it for $10 off. It doesn't show up as a percentage off, but you'll see some models listed at $389 instead of Apple's $399 MSRP. If you're new to Apple's wearables or are ready to upgrade from a Series 9 or older, this is a good model to grab. If you're coming from a Series 10, however, there's not much need to upgrade as the only major change from last year's model is a slightly larger battery and a tougher screen. Apple Watch SE 3 (GPS, 40mm) for $240 ($9 off): There's a similar stealth discount for the newest budget model, the Apple Watch SE 3, at Amazon. It normally goes for $249 — again, not a big discount, but better than nothing if you're looking to get onboard early. Apple gave this model some badly needed updates compared to its predecessor, including an always-on display, faster charging, better sensors and the same processor that you'll find in the new Apple Watch Series 11. Read more Apple coverage: The best AirPods The best Apple Watches The best MacBooks The best iPhones The best iPads Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/todays-best-ipad-deals-include-50-off-the-256gb-ipad-a16-150020379.html?src=rss",
          "feed_position": 24
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/general/the-morning-after-engadget-newsletter-111523653.html",
          "published_at": "Fri, 17 Oct 2025 11:15:23 +0000",
          "title": "The Morning After: Apple adds its new M5 chip to iPads, MacBooks and even the Vision Pro",
          "standfirst": "This week, Apple announced fall hardware updates across multiple devices — pretty much every major category, besides iPhones and AirPods. Don’t get too excited: It’s not a redesign reveal, but we’re expecting a tangible performance jump for both the iPad Pro and MacBook Pro. With the new M5 chip (no Pro or Max versions so far), Apple used the same 3-nanometer fabrication process for the M5 as it did for the M4. The new chip has 10 GPU cores and 10 CPU cores, along with a 16-core Neural Engine. Apple claims the M5 has the “world’s fastest CPU core” with up to 20 percent faster multithreaded performance compared to the M4 chip of the previous MacBook Pro. Graphics performance also gets a significant boost too. The M5 MacBook Pro ($1,599), otherwise, has identical specs to its M4 predecessor, right down to the same dimensions, weight and 70-watt power adapter. Meanwhile, inside the iPad Pro, Apple claims it has more than four times the peak GPU compute performance of the M4. If you’re looking to use the new iPad Pro for video tasks, Apple says that video transcoding is six times faster than the old M1 iPad Pro from 2021. The 11-inch iPad Pro starts at $999 for the 11-inch model and $1,299 for the 13-inch model. And it seems to have a lot of M5 chips to use: The Vision Pro gets one and a seemingly more comfortable Dual Knit Band. The M5 Vision Pro should last half an hour longer than the original model, as well, according to Apple. — Mat Smith Get Engadget's newsletter delivered direct to your inbox. Subscribe right here! The news you might have missed The US Mint is putting Steve Jobs on a $1 coin Ball x Pit’s deeply satisfying grind keeps me coming back for more Apple’s M6 MacBook Pro generation will reportedly offer touchscreens The Honor ‘robot’ phone Less robot, more arm. Honor Chinese phone maker Honor says its next phone will feature a camera on a pop-out mechanical arm. Talking to CNBC, Honor said it will be a robot phone, framing it around AI innovation — something the company is throwing millions of dollars at. I enjoy that its camera arm reminds me of the ubiquitous DJI Osmo Pocket 3, beloved by bloggers, creators and tourists that get in my way. If its foldout camera can track, stabilize video footage and focus on its own, it could be a cool feature. The camera seems to fold away inside the back of the future device, but can it be used while tucked away there? We don’t know. What are these amazing future AI experiences? No idea. Questions, questions, questions. Honor said it plans to share more details at Mobile World Congress in Barcelona early next year. Continue reading. ROG Xbox Ally X handheld gaming PC review Not sure if this is an Xbox. Engadget The co-creation handheld from ASUS ROG and Xbox is here. The Ally X is arguably the best handheld console for Xbox games yet. It’s not just bigger grips and familiar button layouts, but they do help. No, the bigger evolution is how Microsoft has finessed the UI and software, making it more console-like and less like you need a mouse to navigate everything. Perhaps most importantly, when the ROG Xbox Ally X costs $1,000, the AMD Ryzen Z2 Extreme chip offers a lot of power, enough to handle rich flagship games, with some setting tinkering. If you want the best performance from the Ally X, you’ll need to plug it into the wall. Check out our detailed review.This article originally appeared on Engadget at https://www.engadget.com/general/the-morning-after-engadget-newsletter-111523653.html?src=rss",
          "content": "This week, Apple announced fall hardware updates across multiple devices — pretty much every major category, besides iPhones and AirPods. Don’t get too excited: It’s not a redesign reveal, but we’re expecting a tangible performance jump for both the iPad Pro and MacBook Pro. With the new M5 chip (no Pro or Max versions so far), Apple used the same 3-nanometer fabrication process for the M5 as it did for the M4. The new chip has 10 GPU cores and 10 CPU cores, along with a 16-core Neural Engine. Apple claims the M5 has the “world’s fastest CPU core” with up to 20 percent faster multithreaded performance compared to the M4 chip of the previous MacBook Pro. Graphics performance also gets a significant boost too. The M5 MacBook Pro ($1,599), otherwise, has identical specs to its M4 predecessor, right down to the same dimensions, weight and 70-watt power adapter. Meanwhile, inside the iPad Pro, Apple claims it has more than four times the peak GPU compute performance of the M4. If you’re looking to use the new iPad Pro for video tasks, Apple says that video transcoding is six times faster than the old M1 iPad Pro from 2021. The 11-inch iPad Pro starts at $999 for the 11-inch model and $1,299 for the 13-inch model. And it seems to have a lot of M5 chips to use: The Vision Pro gets one and a seemingly more comfortable Dual Knit Band. The M5 Vision Pro should last half an hour longer than the original model, as well, according to Apple. — Mat Smith Get Engadget's newsletter delivered direct to your inbox. Subscribe right here! The news you might have missed The US Mint is putting Steve Jobs on a $1 coin Ball x Pit’s deeply satisfying grind keeps me coming back for more Apple’s M6 MacBook Pro generation will reportedly offer touchscreens The Honor ‘robot’ phone Less robot, more arm. Honor Chinese phone maker Honor says its next phone will feature a camera on a pop-out mechanical arm. Talking to CNBC, Honor said it will be a robot phone, framing it around AI innovation — something the company is throwing millions of dollars at. I enjoy that its camera arm reminds me of the ubiquitous DJI Osmo Pocket 3, beloved by bloggers, creators and tourists that get in my way. If its foldout camera can track, stabilize video footage and focus on its own, it could be a cool feature. The camera seems to fold away inside the back of the future device, but can it be used while tucked away there? We don’t know. What are these amazing future AI experiences? No idea. Questions, questions, questions. Honor said it plans to share more details at Mobile World Congress in Barcelona early next year. Continue reading. ROG Xbox Ally X handheld gaming PC review Not sure if this is an Xbox. Engadget The co-creation handheld from ASUS ROG and Xbox is here. The Ally X is arguably the best handheld console for Xbox games yet. It’s not just bigger grips and familiar button layouts, but they do help. No, the bigger evolution is how Microsoft has finessed the UI and software, making it more console-like and less like you need a mouse to navigate everything. Perhaps most importantly, when the ROG Xbox Ally X costs $1,000, the AMD Ryzen Z2 Extreme chip offers a lot of power, enough to handle rich flagship games, with some setting tinkering. If you want the best performance from the Ally X, you’ll need to plug it into the wall. Check out our detailed review.This article originally appeared on Engadget at https://www.engadget.com/general/the-morning-after-engadget-newsletter-111523653.html?src=rss",
          "feed_position": 28,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2025-10/3f83ffb0-ab3f-11f0-bfff-d212cd1e7880"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/mobile/smartphones/best-android-phone-130030805.html",
          "published_at": "Fri, 17 Oct 2025 09:00:36 +0000",
          "title": "The best Android phones for 2025",
          "standfirst": "Choosing the best Android phone can feel overwhelming as there are so many options from so many brands, it’s hard to know where to start. Unlike Apple, which sticks to its sleek lineup of iPhones, Android offers a world of variety. Whether you're eyeing the latest flagship from Samsung, a budget-friendly smartphone from Motorola or something unique with a foldable design, there’s an Android device out there to suit your needs.The beauty of Android is its flexibility. You’ll find phones with different screen sizes, camera setups, battery life and even quirky extras like stylus support or rugged builds. Plus, Android lets you customize your device to your heart's content – something Apple fans might envy. We’ve tested and researched the top Android phones to help you find the right one for your budget, lifestyle, and tech preferences. Best Android phones for 2025 Other Android phones we tested Google Pixel 10 Pro Fold While the design and performance of the Galaxy Z Fold 7 is so good that we had to pick it as our favorite foldable of this generation, the Pixel 10 Pro Fold isn’t that far behind. Sure, it’s bigger and bulkier, but it still has the best cameras on any foldable phone along with better software and a larger battery. But perhaps most importantly, it now has a proper IP68 rating for dust and water resistance — something you won’t find on any of its rivals. This could save the phone from an early demise and prevent a lot of headaches if you frequent the beach or pretty much anywhere with little particles that could threaten the insides of your device. What to look for in a new Android phone Performance When it comes to picking our favorite Android phones, the main things we look for are pretty straightforward: good performance (both compute and AI), a nice display, solid design, sharp cameras, long battery life and a significant commitment to ongoing software support. For performance, not only do we look at benchmarks and other metrics, but we also evaluate phones based on responsiveness. Regardless of whether you’re reading, text messaging, scrolling through social media or playing a game, no one wants a gadget that feels sluggish. Display When it comes to displays, we generally prefer OLED panels that can produce rich, saturated colors with at least 600 nits of brightness, though many of our top mid-range and high-end phones can hit 1,000 nits or more. And more recently, most of our favorite devices also support screens with fast refresh rates of 90Hz or 120Hz, which adds an extra level of smoothness and fluidity. Design Now we will admit there is a bit of subjectivity when deciding which phones look the best, but there are other design aspects like dust and water resistance or screen durability that can make a big difference to long-term survival. It’s also important to consider things like support for wireless charging, power sharing (aka reverse wireless charging) and UWB connectivity, which can have an impact on how your phone interacts with your other devices. Cameras Obviously, for photos we’re looking for sharp, colorful shots in both bright and low-light conditions. And we want video clips with high dynamic range, rich audio and smooth image stabilization. Extra cameras for ultra-wide and telephoto lenses are a plus. The best cameras should also include features like dedicated night modes, support for various video recording resolutions, and additional photo modes like timelapse, slow motion and more. Battery and software Finally, in terms of longevity, we’re looking for all-day battery life on devices that also delivered great results on our local video rundown test (at least 16 hours on a charge, but more is obviously better). Wireless charging capabilities have become almost ubiquitous over the past few years, and most of our top picks have this extra perk. Fast-charging is available on some Android phones, too. Finally, with people holding onto their phones longer than ever, we like to see companies commit to at least three years of software support, upgrades and regular security updates.This article originally appeared on Engadget at https://www.engadget.com/mobile/smartphones/best-android-phone-130030805.html?src=rss",
          "content": "Choosing the best Android phone can feel overwhelming as there are so many options from so many brands, it’s hard to know where to start. Unlike Apple, which sticks to its sleek lineup of iPhones, Android offers a world of variety. Whether you're eyeing the latest flagship from Samsung, a budget-friendly smartphone from Motorola or something unique with a foldable design, there’s an Android device out there to suit your needs.The beauty of Android is its flexibility. You’ll find phones with different screen sizes, camera setups, battery life and even quirky extras like stylus support or rugged builds. Plus, Android lets you customize your device to your heart's content – something Apple fans might envy. We’ve tested and researched the top Android phones to help you find the right one for your budget, lifestyle, and tech preferences. Best Android phones for 2025 Other Android phones we tested Google Pixel 10 Pro Fold While the design and performance of the Galaxy Z Fold 7 is so good that we had to pick it as our favorite foldable of this generation, the Pixel 10 Pro Fold isn’t that far behind. Sure, it’s bigger and bulkier, but it still has the best cameras on any foldable phone along with better software and a larger battery. But perhaps most importantly, it now has a proper IP68 rating for dust and water resistance — something you won’t find on any of its rivals. This could save the phone from an early demise and prevent a lot of headaches if you frequent the beach or pretty much anywhere with little particles that could threaten the insides of your device. What to look for in a new Android phone Performance When it comes to picking our favorite Android phones, the main things we look for are pretty straightforward: good performance (both compute and AI), a nice display, solid design, sharp cameras, long battery life and a significant commitment to ongoing software support. For performance, not only do we look at benchmarks and other metrics, but we also evaluate phones based on responsiveness. Regardless of whether you’re reading, text messaging, scrolling through social media or playing a game, no one wants a gadget that feels sluggish. Display When it comes to displays, we generally prefer OLED panels that can produce rich, saturated colors with at least 600 nits of brightness, though many of our top mid-range and high-end phones can hit 1,000 nits or more. And more recently, most of our favorite devices also support screens with fast refresh rates of 90Hz or 120Hz, which adds an extra level of smoothness and fluidity. Design Now we will admit there is a bit of subjectivity when deciding which phones look the best, but there are other design aspects like dust and water resistance or screen durability that can make a big difference to long-term survival. It’s also important to consider things like support for wireless charging, power sharing (aka reverse wireless charging) and UWB connectivity, which can have an impact on how your phone interacts with your other devices. Cameras Obviously, for photos we’re looking for sharp, colorful shots in both bright and low-light conditions. And we want video clips with high dynamic range, rich audio and smooth image stabilization. Extra cameras for ultra-wide and telephoto lenses are a plus. The best cameras should also include features like dedicated night modes, support for various video recording resolutions, and additional photo modes like timelapse, slow motion and more. Battery and software Finally, in terms of longevity, we’re looking for all-day battery life on devices that also delivered great results on our local video rundown test (at least 16 hours on a charge, but more is obviously better). Wireless charging capabilities have become almost ubiquitous over the past few years, and most of our top picks have this extra perk. Fast-charging is available on some Android phones, too. Finally, with people holding onto their phones longer than ever, we like to see companies commit to at least three years of software support, upgrades and regular security updates.This article originally appeared on Engadget at https://www.engadget.com/mobile/smartphones/best-android-phone-130030805.html?src=rss",
          "feed_position": 31,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2023-02/54f98de0-a87e-11ed-afd1-54c1e8d571fc"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/researchers-find-adding-this-one-simple-sentence-to-prompts-makes-ai-models",
          "published_at": "Fri, 17 Oct 2025 02:40:00 GMT",
          "title": "Researchers find adding this one simple sentence to prompts makes AI models way more creative",
          "standfirst": "One of the coolest things about generative AI models — both large language models (LLMs) and diffusion-based image generators — is that they are \"non-deterministic.\" That is, despite their reputation among some critics as being \"fancy autocorrect,\" generative AI models actually generate their outputs by choosing from a distribution of the most probable next tokens (units of information) to fill out their response.Asking an LLM: \"What is the capital of France?\" will have it sample its probability distribution for France, capitals, cities, etc. to arrive at the answer \"Paris.\" But that answer could come in the format of \"The capital of France is Paris,\" or simply \"Paris\" or \"Paris, though it was Versailles at one point.\" Still, those of us that use these models frequently day-to-day will note that sometimes, their answers can feel annoyingly repetitive or similar. A common joke about coffee is recycled across generations of queries. Story prompts generate similar arcs. Even tasks that should yield many plausible answers—like naming U.S. states—tend to collapse into only a few. This phenomenon, known as mode collapse, arises during post-training alignment and limits the usefulness of otherwise powerful models.Especially when using LLMs to generate new creative works in writing, communications, strategy, or illustrations, we actually want their outputs to be even more varied than they already are. Now a team of researchers at Northeastern University, Stanford University and West Virginia University have come up with an ingenuously simple method to get language and image models to generate a wider variety of responses to nearly any user prompt by adding a single, simple sentence: \"Generate 5 responses with their corresponding probabilities, sampled from the full distribution.\"The method, called Verbalized Sampling (VS), helps models like GPT-4, Claude, and Gemini produce more diverse and human-like outputs—without retraining or access to internal parameters. It is described in a paper published on the open access journal arxiv.org online in early October 2025.When prompted in this way, the model no longer defaults to its safest, most typical output. Instead, it verbalizes its internal distribution over potential completions and samples across a wider spectrum of possibilities. This one-line change leads to substantial gains in output diversity across multiple domains.As Weiyan Shi, an assistant professor at Northeastern University and co-author of the paper, wrote on X: \"LLMs&#x27; potentials are not fully unlocked yet! As shown in our paper, prompt optimization can be guided by thinking about how LLMs are trained and aligned, and can be proved theoretically.\"Why Models Collapse—and How VS Reverses ItAccording to the research team, the root cause of mode collapse lies not just in algorithms like reinforcement learning from human feedback (RLHF), but in the structure of human preferences. People tend to rate more familiar or typical answers as better, which nudges LLMs toward “safe” choices over diverse ones during fine-tuning.However, this bias doesn’t erase the model’s underlying knowledge—it just suppresses it. VS works by bypassing this suppression. Instead of asking for the single most likely output, it invites the model to reveal a set of plausible responses and their relative probabilities. This distribution-level prompting restores access to the richer diversity present in the base pretraining model.Real-World Performance Across TasksThe research team tested Verbalized Sampling across several common use cases:Creative Writing: In story generation, VS increased diversity scores by up to 2.1× compared to standard prompting, while maintaining quality. One story prompt—“Without a goodbye”—produced formulaic breakup scenes under direct prompting, but yielded narratives involving cosmic events, silent emails, and music stopping mid-dance when prompted via VS.Dialogue Simulation: In persuasive dialogue tasks, VS enabled models to simulate human-like patterns, such as hesitation, resistance, and changes of mind. Donation behavior distributions under VS better aligned with real human data compared to baseline methods.Open-ended QA: When asked to enumerate valid answers (e.g., naming U.S. states), models using VS generated responses that more closely matched the diversity of real-world data. They covered a broader set of answers without sacrificing factual accuracy.Synthetic Data Generation: When used to generate math problems for model training, VS created more varied datasets. These, in turn, improved downstream performance in competitive math benchmarks, outperforming synthetic data generated via direct prompting.Tunable Diversity and Better Use of Larger ModelsA notable advantage of VS is its tunability. Users can set a probability threshold in the prompt to sample from lower-probability “tails” of the model’s distribution. Lower thresholds correspond to higher diversity. This tuning can be done via prompt text alone, without changing any decoding settings like temperature or top-p.In one test using the Gemini-2.5-Flash model, diversity in story writing increased steadily as the probability threshold dropped from 1 to 0.001. The chart accompanying the study showed VS outperforming both direct and sequence-based prompting across all thresholds.Interestingly, the method scales well with model size. Larger models like GPT-4.1 and Claude-4 showed even greater gains from VS compared to smaller ones. While smaller models benefitted, the improvement in diversity was roughly 1.5–2× stronger in larger counterparts—suggesting VS helps unlock more of the latent capabilities in advanced models.Deployment and AvailabilityThe Verbalized Sampling method is available now as a Python package:pip install verbalized-samplingThe package includes integration with LangChain and supports a simple interface for sampling from the verbalized distribution. Users can also adjust parameters like k (number of responses), thresholds, and temperature to suit their applications. A live Colab notebook and documentation are available under an enterprise friendly Apache 2.0 license on GitHub at: https://github.com/CHATS-lab/verbalized-samplingPractical Tips and Common IssuesWhile the method works across all major LLMs, some users may initially encounter refusals or errors. In these cases, the authors suggest using the system prompt version of the template or referring to alternative formats listed on the GitHub page. Some models interpret complex instructions as jailbreak attempts and refuse to comply unless the structure is clearer.For example, prompting via a system-level instruction like this improves reliability:You are a helpful assistant. For each query, generate five responses within separate tags, each with a probability below 0.10.This small change typically resolves any issues.A Lightweight Fix for a Big ProblemVerbalized Sampling represents a practical, inference-time fix to a deep limitation in how modern language models behave. It doesn’t require model retraining or internal access. It is not dependent on any one model family. And it improves not only the diversity of outputs, but their quality—as judged by both human evaluation and benchmark scores.With growing interest in tools that enhance model creativity, VS is likely to see rapid adoption in domains like writing, design, simulation, education, and synthetic data generation.For users and developers frustrated by the sameness of LLM responses, the fix may be as simple as changing the question.",
          "content": "One of the coolest things about generative AI models — both large language models (LLMs) and diffusion-based image generators — is that they are \"non-deterministic.\" That is, despite their reputation among some critics as being \"fancy autocorrect,\" generative AI models actually generate their outputs by choosing from a distribution of the most probable next tokens (units of information) to fill out their response.Asking an LLM: \"What is the capital of France?\" will have it sample its probability distribution for France, capitals, cities, etc. to arrive at the answer \"Paris.\" But that answer could come in the format of \"The capital of France is Paris,\" or simply \"Paris\" or \"Paris, though it was Versailles at one point.\" Still, those of us that use these models frequently day-to-day will note that sometimes, their answers can feel annoyingly repetitive or similar. A common joke about coffee is recycled across generations of queries. Story prompts generate similar arcs. Even tasks that should yield many plausible answers—like naming U.S. states—tend to collapse into only a few. This phenomenon, known as mode collapse, arises during post-training alignment and limits the usefulness of otherwise powerful models.Especially when using LLMs to generate new creative works in writing, communications, strategy, or illustrations, we actually want their outputs to be even more varied than they already are. Now a team of researchers at Northeastern University, Stanford University and West Virginia University have come up with an ingenuously simple method to get language and image models to generate a wider variety of responses to nearly any user prompt by adding a single, simple sentence: \"Generate 5 responses with their corresponding probabilities, sampled from the full distribution.\"The method, called Verbalized Sampling (VS), helps models like GPT-4, Claude, and Gemini produce more diverse and human-like outputs—without retraining or access to internal parameters. It is described in a paper published on the open access journal arxiv.org online in early October 2025.When prompted in this way, the model no longer defaults to its safest, most typical output. Instead, it verbalizes its internal distribution over potential completions and samples across a wider spectrum of possibilities. This one-line change leads to substantial gains in output diversity across multiple domains.As Weiyan Shi, an assistant professor at Northeastern University and co-author of the paper, wrote on X: \"LLMs&#x27; potentials are not fully unlocked yet! As shown in our paper, prompt optimization can be guided by thinking about how LLMs are trained and aligned, and can be proved theoretically.\"Why Models Collapse—and How VS Reverses ItAccording to the research team, the root cause of mode collapse lies not just in algorithms like reinforcement learning from human feedback (RLHF), but in the structure of human preferences. People tend to rate more familiar or typical answers as better, which nudges LLMs toward “safe” choices over diverse ones during fine-tuning.However, this bias doesn’t erase the model’s underlying knowledge—it just suppresses it. VS works by bypassing this suppression. Instead of asking for the single most likely output, it invites the model to reveal a set of plausible responses and their relative probabilities. This distribution-level prompting restores access to the richer diversity present in the base pretraining model.Real-World Performance Across TasksThe research team tested Verbalized Sampling across several common use cases:Creative Writing: In story generation, VS increased diversity scores by up to 2.1× compared to standard prompting, while maintaining quality. One story prompt—“Without a goodbye”—produced formulaic breakup scenes under direct prompting, but yielded narratives involving cosmic events, silent emails, and music stopping mid-dance when prompted via VS.Dialogue Simulation: In persuasive dialogue tasks, VS enabled models to simulate human-like patterns, such as hesitation, resistance, and changes of mind. Donation behavior distributions under VS better aligned with real human data compared to baseline methods.Open-ended QA: When asked to enumerate valid answers (e.g., naming U.S. states), models using VS generated responses that more closely matched the diversity of real-world data. They covered a broader set of answers without sacrificing factual accuracy.Synthetic Data Generation: When used to generate math problems for model training, VS created more varied datasets. These, in turn, improved downstream performance in competitive math benchmarks, outperforming synthetic data generated via direct prompting.Tunable Diversity and Better Use of Larger ModelsA notable advantage of VS is its tunability. Users can set a probability threshold in the prompt to sample from lower-probability “tails” of the model’s distribution. Lower thresholds correspond to higher diversity. This tuning can be done via prompt text alone, without changing any decoding settings like temperature or top-p.In one test using the Gemini-2.5-Flash model, diversity in story writing increased steadily as the probability threshold dropped from 1 to 0.001. The chart accompanying the study showed VS outperforming both direct and sequence-based prompting across all thresholds.Interestingly, the method scales well with model size. Larger models like GPT-4.1 and Claude-4 showed even greater gains from VS compared to smaller ones. While smaller models benefitted, the improvement in diversity was roughly 1.5–2× stronger in larger counterparts—suggesting VS helps unlock more of the latent capabilities in advanced models.Deployment and AvailabilityThe Verbalized Sampling method is available now as a Python package:pip install verbalized-samplingThe package includes integration with LangChain and supports a simple interface for sampling from the verbalized distribution. Users can also adjust parameters like k (number of responses), thresholds, and temperature to suit their applications. A live Colab notebook and documentation are available under an enterprise friendly Apache 2.0 license on GitHub at: https://github.com/CHATS-lab/verbalized-samplingPractical Tips and Common IssuesWhile the method works across all major LLMs, some users may initially encounter refusals or errors. In these cases, the authors suggest using the system prompt version of the template or referring to alternative formats listed on the GitHub page. Some models interpret complex instructions as jailbreak attempts and refuse to comply unless the structure is clearer.For example, prompting via a system-level instruction like this improves reliability:You are a helpful assistant. For each query, generate five responses within separate tags, each with a probability below 0.10.This small change typically resolves any issues.A Lightweight Fix for a Big ProblemVerbalized Sampling represents a practical, inference-time fix to a deep limitation in how modern language models behave. It doesn’t require model retraining or internal access. It is not dependent on any one model family. And it improves not only the diversity of outputs, but their quality—as judged by both human evaluation and benchmark scores.With growing interest in tools that enhance model creativity, VS is likely to see rapid adoption in domains like writing, design, simulation, education, and synthetic data generation.For users and developers frustrated by the sameness of LLM responses, the fix may be as simple as changing the question.",
          "feed_position": 5,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/6muOslUgOQCou8M10zFbHv/b0015ffd123a52616f65bc2a26b14b05/cfr0z3n_Simple_refined_corporate_memphis_flat_illustration_isom_73fd3864-fe47-4408-bdec-76e8596ca810.png"
        }
      ],
      "featured_image": "https://images.ctfassets.net/jdtwqhzvc2n1/2Xu1YqwqTUMJMVWnRrKy7M/23b8a69e94d864a0306f259b1cd29397/u7277289442_A_rendering_of_animated_friendly_AI_agents_with_h_9b63c83c-c489-4154-a498-3bdfdc3a5c5b_3.png",
      "popularity_score": 2007.7207858333334,
      "ai_summary": [
        "Aerial_Knight's DropShot is a skydiving first-person shooter with finger guns and dragons.",
        "Final Sentence is a battle royale game where players type sentences against each other.",
        "The Final Sentence demo features a masked figure with a revolver for losing players.",
        "Steam Next Fest allows players to try many game demos before they are released.",
        "Aerial_Knight's Never Yield is a stylish and fast-paced game from a solo developer."
      ]
    },
    {
      "id": "cluster_63",
      "coverage": 2,
      "updated_at": "2025-10-17T20:15:30-04:00",
      "title": "Facebook’s new button lets its AI look at photos you haven&#8217;t uploaded yet",
      "neutral_headline": "Facebook AI Suggests Edits for Unshared Photos",
      "items": [
        {
          "source": "The Verge",
          "url": "https://www.theverge.com/ai-artificial-intelligence/802102/meta-facebook-opt-in-ai-edits-photos-camera-roll",
          "published_at": "2025-10-17T20:15:30-04:00",
          "title": "Facebook’s new button lets its AI look at photos you haven&#8217;t uploaded yet",
          "standfirst": "Meta has rolled out an opt-in AI feature to its US and Canadian Facebook users that claims to make their photos and videos more &#8220;shareworthy.&#8221; The only catch is that the feature is designed for your phone’s camera roll — not the media you’ve already uploaded to Facebook. If you opt in, Meta’s AI will [&#8230;]",
          "content": "Meta has rolled out an opt-in AI feature to its US and Canadian Facebook users that claims to make their photos and videos more &#8220;shareworthy.&#8221; The only catch is that the feature is designed for your phone’s camera roll — not the media you’ve already uploaded to Facebook. If you opt in, Meta’s AI will comb through your camera roll, upload your unpublished photos to Meta’s cloud, and surface “hidden gems” that are “lost among screenshots, receipts, and random snaps,” the company says. Users will be able to save or share the suggested edits and collages. If Facebook wanting to look at your unpublished photos sounds familiar, it might be because we wrote about an early test in June. At that time, the company claimed unposted, private photos were not being used to train Meta’s AI, but it declined to rule out whether it would do so in the future. Well, the future is now, and it sure sounds like Meta wants to train its AI on your photos — under certain conditions. In the Friday announcement of the feature, Meta says, “We don’t use media from your camera roll to improve AI at Meta, unless you choose to edit this media with our AI tools, or share.” The Verge asked Meta to confirm: Meta will use your camera roll to train its AI if you choose to use this feature, right? We also asked for clarification on when Meta begins using your unpublished photos to train its AI. Does it happen when you opt into the new feature? After you choose to edit something with the tool? Or only after you choose to share the resulting creation? Meta spokesperson Mari Melguizo sent us the following clarification: “This means the camera roll media uploaded by this feature to make suggestions won’t be used to improve AI at Meta. Only if you edit the suggestions with our AI tools or publish those suggestions to Facebook, improvements to AI at Meta may be made.” So, Meta will collect and store your photos in the cloud and Meta’s AI will get to look at them, but the company won’t use them to train their AI unless you take an additional action — at least for now, according to Meta. Today, the feature says it will “select media from your camera roll and upload it to our cloud on an ongoing basis”; in June, Meta told us that it might hold onto some of that data for longer than 30 days. The company claims your media “won’t be used for ad targeting.” Last year, Meta acknowledged that it had already quietly trained its AI models on all public photos and text posted to Facebook and Instagram by adult users since 2007. Facebook’s blog today shows that users will be asked if they want to “allow cloud processing to get creative ideas made for you from your camera roll.” It’s not yet clear if that prompt will also warn users that the feature may train Meta’s AI on your photos. The company says the feature is meant to help users who enjoy snapping pics but want to improve their photos before posting, or who don&#8217;t have time to “create something special.” Facebook says it’ll roll out the feature in the coming months.",
          "feed_position": 7
        },
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2025/10/17/facebooks-ai-can-now-suggest-edits-to-the-photos-still-on-your-phone/",
          "published_at": "Fri, 17 Oct 2025 17:29:47 +0000",
          "title": "Facebook&#8217;s AI can now suggest edits to the photos still on your phone",
          "standfirst": "Facebook is rolling out a new Meta AI photo suggestion feature across the U.S. and Canada that lets its AI recommend edits to images stored on users’ camera rolls — even if they haven’t been shared. The feature is currently opt-in only.",
          "content": "Facebook is rolling out a new Meta AI photo suggestion feature across the U.S. and Canada that lets its AI recommend edits to images stored on users’ camera rolls — even if they haven’t been shared. The feature is currently opt-in only.",
          "feed_position": 12
        }
      ],
      "popularity_score": 2000,
      "ai_summary": [
        "Meta is rolling out an AI feature to US and Canadian Facebook users.",
        "The feature suggests edits to images stored on users' camera rolls.",
        "The AI feature is designed for photos not yet uploaded to Facebook.",
        "Users must opt in to allow the AI to analyze their camera roll.",
        "The AI aims to make photos and videos more \"shareworthy\" for users."
      ]
    },
    {
      "id": "cluster_32",
      "coverage": 1,
      "updated_at": "Sat, 18 Oct 2025 12:15:59 +0000",
      "title": "Roberta Williams’ The Colonel’s Bequest was a different type of adventure game",
      "neutral_headline": "The Colonel's Bequest: A Different Adventure Game",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gaming/2025/10/roberta-williams-the-colonels-bequest-was-a-different-type-of-adventure-game/",
          "published_at": "Sat, 18 Oct 2025 12:15:59 +0000",
          "title": "Roberta Williams’ The Colonel’s Bequest was a different type of adventure game",
          "standfirst": "What if point-and-click games weren't about the puzzles?",
          "content": "Even in my youth, I always loved the idea of point-and-click adventure games more than I did the reality. I appreciated how they transported me to other worlds, each with its own rules, histories, and interesting characters. However, like many people, I often ran up against the harsh reality of solving bizarre and obtuse puzzles in a time before Internet walkthroughs. I almost never actually finished point-and-click adventure games for that reason—but there is one major exception: I completed Roberta Williams’ The Colonel’s Bequest several times. One of the last Sierra adventure games to still use a text parser, The Colonel’s Bequest follows a young woman named Laura Bow as she visits a mansion in the Southern US belonging to her college friend’s grandfather, Colonel Henri Dijon. While she’s there, a dispute breaks out over the colonel’s will, and it becomes clear a murderer is on the loose.Read full article Comments",
          "feed_position": 0,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/Colonels-Bequest-1-1152x648-1760651707.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/Colonels-Bequest-1-1152x648-1760651707.jpg",
      "popularity_score": 341.9871747222222,
      "ai_summary": [
        "The article discusses Roberta Williams' game The Colonel's Bequest.",
        "The game is a point-and-click adventure game.",
        "The article questions if the game is about puzzles.",
        "The game is a different type of adventure game.",
        "The article is about the game's unique approach."
      ]
    },
    {
      "id": "cluster_67",
      "coverage": 1,
      "updated_at": "Fri, 17 Oct 2025 22:14:59 +0000",
      "title": "With deadline looming, 4 of 9 universities reject Trump’s “compact” to remake higher ed",
      "neutral_headline": "Universities Reject Trump's Higher Education Compact",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/culture/2025/10/with-deadline-looming-4-of-9-universities-reject-trumps-compact-to-remake-higher-ed/",
          "published_at": "Fri, 17 Oct 2025 22:14:59 +0000",
          "title": "With deadline looming, 4 of 9 universities reject Trump’s “compact” to remake higher ed",
          "standfirst": "But Trump is pressuring the other five.",
          "content": "Earlier this month, the Trump administration made nine elite universities an offer they couldn’t refuse: bring in more conservatives while shutting down “institutional units that purposefully punish, belittle, and even spark violence against conservative ideas,” give up control of admissions and hiring decisions, agree to “biological” definitions of sex and gender, don’t raise tuition for five years, clamp down on student protests, and stay institutionally “neutral” on current events. Do this and you won’t be cut off from “federal benefits,” which could include research funding, student loans, federal contracts, and even student and faculty immigration visas. Instead, you may gain “substantial and meaningful federal grants.” But the universities are refusing. With the initial deadline of October 20 approaching, four of the nine universities—the University of Pennsylvania, Brown, University of Southern California, and MIT—that received the federal “compact” have announced that they will not sign it. In addition, the American Council on Education, which represents more than 1,600 colleges and universities, today issued a statement calling for the compact to be completely withdrawn.Read full article Comments",
          "feed_position": 1,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/05/GettyImages-588709290-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/05/GettyImages-588709290-1152x648.jpg",
      "popularity_score": 323,
      "ai_summary": [
        "Four of nine universities rejected Trump's \"compact\" to remake higher education.",
        "The deadline for the compact is approaching.",
        "Trump is pressuring the remaining five universities to accept.",
        "The compact aims to reshape the landscape of higher education.",
        "The article highlights the resistance to Trump's proposed changes."
      ]
    },
    {
      "id": "cluster_68",
      "coverage": 1,
      "updated_at": "Fri, 17 Oct 2025 22:02:54 +0000",
      "title": "Vaginal condition treatment update: Men should get treated, too",
      "neutral_headline": "Bacterial Vaginosis Treatment: Men Should Be Treated",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/health/2025/10/vaginal-condition-treatment-update-men-should-get-treated-too/",
          "published_at": "Fri, 17 Oct 2025 22:02:54 +0000",
          "title": "Vaginal condition treatment update: Men should get treated, too",
          "standfirst": "For bacterial vaginosis, partners are part of the problem—and the solution.",
          "content": "For some cases of bacterial vaginosis, treatment should include a package deal, doctors now say. The American College of Obstetricians & Gynecologists (ACOG) updated its clinical guidance Friday to fit with recent data indicating that treatment for recurring bacterial vaginosis (BV) in women is significantly more effective if their male partners are also treated at the same time—with both an oral antibiotic and an antibiotic cream directly onto the potentially offending member. “Partner therapy offers us another avenue for hopefully preventing recurrence and helping people feel better faster,” Christopher Zahn, chief of clinical practice and health equity and quality at ACOG, said in a statement.Read full article Comments",
          "feed_position": 2,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2185753669-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2185753669-1152x648.jpg",
      "popularity_score": 313,
      "ai_summary": [
        "The article discusses the treatment of bacterial vaginosis.",
        "Partners are part of the problem and the solution.",
        "The article emphasizes the importance of treating both partners.",
        "Bacterial vaginosis is a vaginal condition.",
        "The article provides an update on treatment strategies."
      ]
    },
    {
      "id": "cluster_69",
      "coverage": 1,
      "updated_at": "Fri, 17 Oct 2025 21:50:10 +0000",
      "title": "Ring cameras are about to get increasingly chummy with law enforcement",
      "neutral_headline": "Ring Cameras and Law Enforcement Partnerships",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2025/10/ring-cameras-are-about-to-get-increasingly-chummy-with-law-enforcement/",
          "published_at": "Fri, 17 Oct 2025 21:50:10 +0000",
          "title": "Ring cameras are about to get increasingly chummy with law enforcement",
          "standfirst": "Amazon's Ring partners with company whose tech has reportedly been used by ICE.",
          "content": "Law enforcement agencies will soon have easier access to footage captured by Amazon’s Ring smart cameras. In a partnership announced this week, Amazon will allow approximately 5,000 local law enforcement agencies to request access to Ring camera footage via surveillance platforms from Flock Safety. Ring cooperating with law enforcement and the reported use of Flock technologies by federal agencies, including US Immigration and Customs Enforcement (ICE), has resurfaced privacy concerns that have followed the devices for years. According to Flock’s announcement, its Ring partnership allows local law enforcement members to use Flock software “to send a direct post in the Ring Neighbors app with details about the investigation and request voluntary assistance.” Requests must include “specific location and timeframe of the incident, a unique investigation code, and details about what is being investigated,” and users can look at the requests anonymously, Flock said. “Any footage a Ring customer chooses to submit will be securely packaged by Flock and shared directly with the requesting local public safety agency through the FlockOS or Flock Nova platform,” the announcement reads.Read full article Comments",
          "feed_position": 3,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/20250611-lifestyle-outdoorcampro-plugin-home-exterior-wall-wht-rgb-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/20250611-lifestyle-outdoorcampro-plugin-home-exterior-wall-wht-rgb-1152x648.jpg",
      "popularity_score": 303,
      "ai_summary": [
        "Amazon's Ring partners with a company.",
        "The company's tech has reportedly been used by ICE.",
        "The partnership raises concerns about law enforcement access.",
        "Ring cameras are becoming increasingly integrated with law enforcement.",
        "The article discusses the implications of this partnership."
      ]
    },
    {
      "id": "cluster_84",
      "coverage": 1,
      "updated_at": "Fri, 17 Oct 2025 18:36:18 +0000",
      "title": "Dead Ends is a fun, macabre medical history for kids",
      "neutral_headline": "Dead Ends: A Macabre Medical History for Kids",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2025/10/dead-ends-is-a-fun-macabre-medical-history-for-kids/",
          "published_at": "Fri, 17 Oct 2025 18:36:18 +0000",
          "title": "Dead Ends is a fun, macabre medical history for kids",
          "standfirst": "Ars chats with co-authors Lindsey Fitzharris and Adrian Teal about their delightful new children's book.",
          "content": "In 1890, a German scientist named Robert Koch thought he’d invented a cure for tuberculosis, a substance derived from the infecting bacterium itself that he dubbed Tuberculin. His substance didn’t actually cure anyone, but it was eventually widely used as a diagnostic skin test. Koch’s successful failure is just one of the many colorful cases featured in Dead Ends! Flukes, Flops, and Failures that Sparked Medical Marvels, a new nonfiction illustrated children’s book by science historian Lindsey Fitzharris and her husband, cartoonist Adrian Teal. A noted science communicator with a fondness for the medically macabre, Fitzharris published a biography of surgical pioneer Joseph Lister, The Butchering Art, in 2017—a great, if occasionally grisly, read. She followed up with 2022’s The Facemaker: A Visionary Surgeon’s Battle to Mend the Disfigured Soldiers of World War I, about a WWI surgeon named Harold Gillies who rebuilt the faces of injured soldiers. And in 2020, she hosted a documentary for the Smithsonian Channel, The Curious Life and Death Of…, exploring famous deaths, ranging from drug lord Pablo Escobar to magician Harry Houdini. Fitzharris performed virtual autopsies, experimented with blood samples, interviewed witnesses, and conducted real-time demonstrations in hopes of gleaning fresh insights. For his part, Teal is a well-known caricaturist and illustrator, best known for his work on the British TV series Spitting Image. His work has also appeared in The Guardian and the Sunday Telegraph, among other outlets.Read full article Comments",
          "feed_position": 4,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/09/deadends4-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/09/deadends4-1152x648.jpg",
      "popularity_score": 293,
      "ai_summary": [
        "The article discusses the children's book Dead Ends.",
        "The book is a medical history for children.",
        "The book is described as fun and macabre.",
        "The authors are Lindsey Fitzharris and Adrian Teal.",
        "The article is an interview with the co-authors."
      ]
    },
    {
      "id": "cluster_89",
      "coverage": 1,
      "updated_at": "Fri, 17 Oct 2025 17:55:19 +0000",
      "title": "Big Tech sues Texas, says age-verification law is “broad censorship regime”",
      "neutral_headline": "Big Tech Sues Texas Over Age-Verification Law",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/10/big-tech-sues-texas-says-age-verification-law-is-broad-censorship-regime/",
          "published_at": "Fri, 17 Oct 2025 17:55:19 +0000",
          "title": "Big Tech sues Texas, says age-verification law is “broad censorship regime”",
          "standfirst": "Texas app law compared to checking IDs at bookstores and shopping malls.",
          "content": "Texas is being sued by a Big Tech lobby group over the state’s new law that will require app stores to verify users’ ages and impose restrictions on users under 18. “The Texas App Store Accountability Act imposes a broad censorship regime on the entire universe of mobile apps,” the Computer & Communications Industry Association (CCIA) said yesterday in a lawsuit. “In a misguided attempt to protect minors, Texas has decided to require proof of age before anyone with a smartphone or tablet can download an app. Anyone under 18 must obtain parental consent for every app and in-app purchase they try to download—from ebooks to email to entertainment.” The CCIA said in a press release that the law violates the First Amendment by imposing “a sweeping age-verification, parental consent, and compelled speech regime on both app stores and app developers.” When app stores determine that a user is under 18, “the law prohibits them from downloading virtually all apps and software programs and from making any in-app purchases unless their parent consents and is given control over the minor’s account,” the CCIA said. “Minors who are unable to link their accounts with a parent’s or guardian’s, or who do not receive permission, would be prohibited from accessing app store content.”Read full article Comments",
          "feed_position": 5,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/boy-with-phone-1152x648-1760722441.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/boy-with-phone-1152x648-1760722441.jpg",
      "popularity_score": 283,
      "ai_summary": [
        "Big Tech companies are suing Texas.",
        "The lawsuit concerns an age-verification law.",
        "The law is described as a \"broad censorship regime.\"",
        "The law is compared to checking IDs in stores.",
        "The article discusses the legal challenges to the law."
      ]
    },
    {
      "id": "cluster_90",
      "coverage": 1,
      "updated_at": "Fri, 17 Oct 2025 17:14:03 +0000",
      "title": "Teen sues to destroy the nudify app that left her in constant fear",
      "neutral_headline": "Teen Sues to Destroy Nudify App",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/10/teen-haunted-by-fake-nudes-sues-to-kill-nudify-app-block-telegram-bots/",
          "published_at": "Fri, 17 Oct 2025 17:14:03 +0000",
          "title": "Teen sues to destroy the nudify app that left her in constant fear",
          "standfirst": "Lawsuit accuses nudify apps of training on teen victims' images.",
          "content": "One of the earliest teen victims bullied by fake nudes has sued to destroy the app she said left her living in “constant fear.” In her complaint, the teen—who was granted anonymity as a 17-year-old minor—accused ClothOff of intentionally making it easy to generate and distribute child sexual abuse materials (CSAM), as well as nonconsensual intimate images (NCII) of adults. She also alleged that the social media network Telegram helps promote ClothOff through automated bots that have attracted hundreds of thousands of subscribers. ClothOff’s operation, the teen alleged, goes beyond promoting a single app, which can be used for free to turn an ordinary Instagram photo into CSAM or NCII in “three clicks.”Read full article Comments",
          "feed_position": 6,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-1204474749-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-1204474749-1152x648.jpg",
      "popularity_score": 273,
      "ai_summary": [
        "A teen is suing to destroy the nudify app.",
        "The app left the teen in constant fear.",
        "The lawsuit accuses the app of training on teen images.",
        "The article discusses the legal action against the app.",
        "The lawsuit addresses the harmful effects of the app."
      ]
    },
    {
      "id": "cluster_91",
      "coverage": 1,
      "updated_at": "Fri, 17 Oct 2025 16:54:32 +0000",
      "title": "NASA’s next Moonship reaches last stop before launch pad",
      "neutral_headline": "NASA's Moonship Reaches Final Stop",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2025/10/nasas-next-moonship-reaches-last-stop-before-launch-pad/",
          "published_at": "Fri, 17 Oct 2025 16:54:32 +0000",
          "title": "NASA’s next Moonship reaches last stop before launch pad",
          "standfirst": "Preparations for the Artemis II mission continue despite the federal government shutdown.",
          "content": "The Orion spacecraft, which will fly four people around the Moon, arrived inside the cavernous Vehicle Assembly Building at NASA’s Kennedy Space Center in Florida late Thursday night, ready to be stacked on top of its rocket for launch early next year. The late-night transfer covered about 6 miles (10 kilometers) from one facility to another at the Florida spaceport. NASA and its contractors are continuing preparations for the Artemis II mission after the White House approved the program as an exception to work through the ongoing government shutdown, which began on October 1. The sustained work could set up Artemis II for a launch opportunity as soon as February 5 of next year. Astronauts Reid Wiseman, Victor Glover, Christina Koch, and Jeremy Hansen will be the first humans to fly on the Orion spacecraft, a vehicle that has been in development for nearly two decades. The Artemis II crew will make history on their 10-day flight by becoming the first people to travel to the vicinity of the Moon since 1972.Read full article Comments",
          "feed_position": 7,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/orion_transfer_artii-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/orion_transfer_artii-1152x648.jpg",
      "popularity_score": 263,
      "ai_summary": [
        "NASA's next Moonship is at its last stop.",
        "The mission is called Artemis II.",
        "Preparations continue despite the government shutdown.",
        "The launch pad is the final destination before launch.",
        "The article provides an update on the mission's progress."
      ]
    },
    {
      "id": "cluster_96",
      "coverage": 1,
      "updated_at": "Fri, 17 Oct 2025 16:27:02 +0000",
      "title": "12 years of HDD analysis brings insight to the bathtub curve’s reliability",
      "neutral_headline": "HDD Analysis Reveals Insights on Reliability",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2025/10/backblaze-owner-of-317230-hdds-says-hdds-are-lasting-longer/",
          "published_at": "Fri, 17 Oct 2025 16:27:02 +0000",
          "title": "12 years of HDD analysis brings insight to the bathtub curve’s reliability",
          "standfirst": "Backup firm brings a unique, informed perspective to HDD failure rates.",
          "content": "Backblaze is a backup and cloud storage company that has been tracking the annualized failure rates (AFRs) of the hard drives in its datacenter since 2013. As you can imagine, that’s netted the firm a lot of data. And that data has led the company to conclude that HDDs “are lasting longer” and showing fewer errors. That conclusion came from a blog post this week by Stephanie Doyle, Backblaze’s writer and blog operations specialist, and Pat Patterson, Backblaze’s chief technical evangelist. The authors compared the AFRs for the approximately 317,230 drives in Backblaze’s datacenter to the AFRs the company recorded when examining the 21,195 drives it had in 2013 and 206,928 drives in 2021. Doyle and Patterson said they identified “a pretty solid deviation in both age of drive failure and the high point of AFR from the last two times we’ve run the analyses.” Credit: Backblaze Read full article Comments",
          "feed_position": 8,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2020/08/GettyImages-462699436-1024x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2020/08/GettyImages-462699436-1024x648.jpg",
      "popularity_score": 253,
      "ai_summary": [
        "A backup firm analyzed HDDs for twelve years.",
        "The analysis provides insight into the bathtub curve.",
        "The bathtub curve describes HDD failure rates.",
        "The firm offers a unique perspective on failures.",
        "The article discusses HDD failure rates over time."
      ]
    },
    {
      "id": "cluster_103",
      "coverage": 1,
      "updated_at": "Fri, 17 Oct 2025 15:40:15 +0000",
      "title": "Lead poisoning has been a feature of our evolution",
      "neutral_headline": "Lead Poisoning in Human Evolution",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2025/10/hominins-suffered-lead-poisoning-starting-at-least-2-million-years-ago/",
          "published_at": "Fri, 17 Oct 2025 15:40:15 +0000",
          "title": "Lead poisoning has been a feature of our evolution",
          "standfirst": "A recent study found lead in teeth from 2 million-year-old hominin fossils.",
          "content": "Our hominid ancestors faced a Pleistocene world full of dangers—and apparently one of those dangers was lead poisoning. Lead exposure sounds like a modern problem, at least if you define “modern” the way a paleoanthropologist might: a time that started a few thousand years ago with ancient Roman silver smelting and lead pipes. According to a recent study, however, lead is a much more ancient nemesis, one that predates not just the Romans but the existence of our genus Homo. Paleoanthropologist Renaud Joannes-Boyau of Australia’s Southern Cross University and his colleagues found evidence of exposure to dangerous amounts of lead in the teeth of fossil apes and hominins dating back almost 2 million years. And somewhat controversially, they suggest that the toxic element’s pervasiveness may have helped shape our evolutionary history. The skull of an early hominid. Credit: Einsamer Schütze / Wikimedia The Romans didn’t invent lead poisoning Joannes-Boyau and his colleagues took tiny samples of preserved enamel and dentin from the teeth of 51 fossils. In most of those teeth, the paleoanthropologists found evidence that these apes and hominins had been exposed to lead—sometimes in dangerous quantities—fairly often during their early years.Read full article Comments",
          "feed_position": 9,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-1285261327-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-1285261327-1152x648.jpg",
      "popularity_score": 246,
      "ai_summary": [
        "A recent study found lead in teeth.",
        "The teeth were from 2 million-year-old fossils.",
        "The fossils are hominin fossils.",
        "Lead poisoning has been a feature of evolution.",
        "The article discusses the findings of the study."
      ]
    },
    {
      "id": "cluster_111",
      "coverage": 1,
      "updated_at": "Fri, 17 Oct 2025 15:07:01 +0000",
      "title": "Apple pays $750 million for US Formula 1 streaming coverage",
      "neutral_headline": "Apple Pays for Formula 1 Streaming Coverage",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/cars/2025/10/apple-pays-750-million-for-us-formula-1-streaming-coverage/",
          "published_at": "Fri, 17 Oct 2025 15:07:01 +0000",
          "title": "Apple pays $750 million for US Formula 1 streaming coverage",
          "standfirst": "Some races will be free as F1 TV moves from standalone streaming to Apple TV.",
          "content": "The United States Grand Prix takes place this weekend at the Circuit of the Americas in Texas, and this morning, Formula 1 used the occasion to announce a new broadcast deal for the sport in the US. Starting next year, F1 will no longer be broadcast on ESPN—it’s moving to Apple TV in a five-year, $750 million deal. Apple boss Tim Cook has been seen at F1 races in the past, and earlier this year, Apple released F1: The Movie, starring Brad Pitt as a 50-something racing driver who improbably gets a second bite at the cherry 30 years after a brutal crash seemingly ended his F1 career. But securing the rights to the sport itself means Apple has snagged a very fast-growing series, with races almost every other week—currently, the sport has expanded to 24 races a year.Read full article Comments",
          "feed_position": 10,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/Apple-exclusive-F1-partner-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/Apple-exclusive-F1-partner-1152x648.jpg",
      "popularity_score": 145,
      "ai_summary": [
        "Apple is paying $750 million for coverage.",
        "The coverage is for US Formula 1 streaming.",
        "Some races will be free to watch.",
        "F1 TV is moving to Apple TV.",
        "The article discusses the deal's financial details."
      ]
    },
    {
      "id": "cluster_122",
      "coverage": 1,
      "updated_at": "Fri, 17 Oct 2025 14:16:58 +0000",
      "title": "Teachers get an F on AI-generated lesson plans",
      "neutral_headline": "AI-Generated Lesson Plans Fail Teachers",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/ai/2025/10/teachers-get-an-f-on-ai-generated-lesson-plans/",
          "published_at": "Fri, 17 Oct 2025 14:16:58 +0000",
          "title": "Teachers get an F on AI-generated lesson plans",
          "standfirst": "AI-generated lesson plans fall short on inspiring students and promoting critical thinking.",
          "content": "When teachers rely on commonly used artificial intelligence chatbots to devise lesson plans, it does not result in more engaging, immersive, or effective learning experiences compared with existing techniques, we found in our recent study. The AI-generated civics lesson plans we analyzed also left out opportunities for students to explore the stories and experiences of traditionally marginalized people. The allure of generative AI as a teaching aid has caught the attention of educators. A Gallup survey from September 2025 found that 60 percent of K-12 teachers are already using AI in their work, with the most common reported use being teaching preparation and lesson planning. Without the assistance of AI, teachers might spend hours every week crafting lessons for their students. With AI, time-stretched teachers can generate detailed lesson plans featuring learning objectives, materials, activities, assessments, extension activities, and homework tasks in a matter of seconds.Read full article Comments",
          "feed_position": 12,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/09/GettyImages-1287663546-1152x648-1758570568.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/09/GettyImages-1287663546-1152x648-1758570568.jpg",
      "popularity_score": 145,
      "ai_summary": [
        "AI-generated lesson plans are being assessed.",
        "The plans fall short on inspiring students.",
        "The plans do not promote critical thinking.",
        "Teachers are giving AI-generated plans a failing grade.",
        "The article discusses the shortcomings of AI lesson plans."
      ]
    },
    {
      "id": "cluster_137",
      "coverage": 1,
      "updated_at": "Fri, 17 Oct 2025 10:30:59 +0000",
      "title": "Yes, everything online sucks now—but it doesn’t have to",
      "neutral_headline": "Yes, everything online sucks now—but it doesn’t have to",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2025/10/yes-everything-online-sucks-now-but-it-doesnt-have-to/",
          "published_at": "Fri, 17 Oct 2025 10:30:59 +0000",
          "title": "Yes, everything online sucks now—but it doesn’t have to",
          "standfirst": "Ars chats with Cory Doctorow about his new book Enshittification.",
          "content": "We all feel it: Our once-happy digital spaces have become increasingly less user-friendly and more toxic, cluttered with extras nobody asked for and hardly anybody wants. There’s even a word for it: “enshittification,” named 2023 Word of the Year by the American Dialect Society. The term was coined by tech journalist/science fiction author Cory Doctorow, a longtime advocate of digital rights. Doctorow has spun his analysis of what’s been ailing the tech industry into an eminently readable new book, Enshittification: Why Everything Suddenly Got Worse and What To Do About It. As Doctorow tells it, he was on vacation in Puerto Rico, staying in a remote cabin nestled in a cloud forest with microwave Internet service—i.e., very bad Internet service, since microwave signals struggle to penetrate through clouds. It was a 90-minute drive to town, but when they tried to consult TripAdvisor for good local places to have dinner one night, they couldn’t get the site to load. “All you would get is the little TripAdvisor logo as an SVG filling your whole tab and nothing else,” Doctorow told Ars. “So I tweeted, ‘Has anyone at TripAdvisor ever been on a trip? This is the most enshittified website I’ve ever used.'” Initially, he just got a few “haha, that’s a funny word” responses. “It was when I married that to this technical critique, at a moment when things were quite visibly bad to a much larger group of people, that made it take off,” Doctorow said. “I didn’t deliberately set out to do it. I bought a million lottery tickets and one of them won the lottery. It only took two decades.”Read full article Comments",
          "feed_position": 14,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/cory1-1152x648-1759765718.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/cory1-1152x648-1759765718.jpg",
      "popularity_score": 139,
      "ai_summary": [
        "The article discusses the current state of the internet.",
        "The article is an interview with Cory Doctorow.",
        "Doctorow is the author of the book Enshittification.",
        "The article explores the concept of enshittification.",
        "The article suggests that the internet can improve."
      ]
    },
    {
      "id": "cluster_116",
      "coverage": 1,
      "updated_at": "Fri, 17 Oct 2025 14:57:07 +0000",
      "title": "3 years, 4 championships, but 0 Le Mans wins: Assessing the Porsche 963",
      "neutral_headline": "Porsche 963 Faces Complex Situation in Racing",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/cars/2025/10/3-years-4-championships-but-0-le-mans-wins-assessing-the-porsche-963/",
          "published_at": "Fri, 17 Oct 2025 14:57:07 +0000",
          "title": "3 years, 4 championships, but 0 Le Mans wins: Assessing the Porsche 963",
          "standfirst": "Riding high in IMSA but pulling out of WEC paints a complicated picture for the factory team.",
          "content": "Porsche provided flights from Washington to Atlanta and accommodation so Ars could attend Petit Le Mans. Ars does not accept paid editorial content. The car world has long had a thing about numbers. Engine outputs. Top speeds. Zero-to-60 times. Displacement. But the numbers go beyond bench racing specs. Some cars have numbers for names, and few more memorably than Porsche. Its most famous model shares its appellation with the emergency services here in North America; although the car should accurately be “nine-11,” you call it “nine-one-one.” Some numbers are less well-known, but perhaps more special to Porsche’s fans, especially those who like racing. 908. 917. 956. 962. 919. But how about 963? That’s Porsche’s current sports prototype, a 670-hp (500 kW) hybrid that for the last three years has battled against rivals in what is starting to look like, if not a golden era for endurance racing, then at least a very purple patch. And the 963 has done well, racing here in IMSA’s WeatherTech Sportscar Championship and around the globe in the FIA World Endurance Championship.Read full article Comments",
          "feed_position": 11,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/IMSA2025PetitLeMans_JT19117-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/IMSA2025PetitLeMans_JT19117-1152x648.jpg",
      "popularity_score": 133,
      "ai_summary": [
        "Porsche 963 has achieved success in IMSA racing, securing four championships in three years.",
        "The team is withdrawing from the World Endurance Championship (WEC) for the 2024 season.",
        "This decision creates a complicated picture for the factory team's future in endurance racing.",
        "The team's focus will shift to other racing series, potentially impacting its overall strategy.",
        "The reasons behind the WEC withdrawal are not explicitly stated in the provided information."
      ]
    },
    {
      "id": "cluster_135",
      "coverage": 1,
      "updated_at": "Fri, 17 Oct 2025 11:00:49 +0000",
      "title": "Rocket Report: China launches with no advance warning; Europe’s drone ship",
      "neutral_headline": "Rocket Report: China, Starlink, and US Military Launches",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2025/10/rocket-report-a-nearly-perfect-flight-for-starship-chinas-surprise-launch/",
          "published_at": "Fri, 17 Oct 2025 11:00:49 +0000",
          "title": "Rocket Report: China launches with no advance warning; Europe’s drone ship",
          "standfirst": "Starlink, Kuiper, and the US military all saw additions to their mega-constellations this week.",
          "content": "Welcome to Edition 8.15 of the Rocket Report! This year has been, at best, one of mixed results for SpaceX’s Starship program. There have been important steps forward, including the successful reuse of the rocket’s massive Super Heavy booster. Clearly, SpaceX is getting really good at launching and recovering the 33-engine booster stage. But Starship itself, part spacecraft and part upper stage, hasn’t fared as well—at least it hadn’t until the last couple of months. After four Starships were destroyed in flight and on the ground in the first half of 2025, the last two missions ended with pinpoint splashdowns in the Indian Ocean. The most recent mission this week was arguably the most successful yet for Starship, which returned to Earth with little damage, suggesting SpaceX’s improvements to the heat shield are working. As always, we welcome reader submissions. If you don’t want to miss an issue, please subscribe using the box below (the form will not appear on AMP-enabled versions of the site). Each report will include information on small-, medium-, and heavy-lift rockets, as well as a quick look ahead at the next three launches on the calendar. SpaceX vet will fly with Blue Origin. Hans Koenigsmann is one of SpaceX’s earliest, longest-tenured, and most-revered employees. He worked at Elon Musk’s space company for nearly two decades, rising to the role of vice president for mission assurance and safety before leaving SpaceX in 2021. He led the investigations into every Falcon rocket failure, mentored young engineers, and became a public face for SpaceX through numerous presentations and press conferences. And now he has announced he is going to space on a future suborbital flight on Blue Origin’s New Shepard vehicle, Ars reports.Read full article Comments",
          "feed_position": 13,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/starshipflight11-presplash-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/starshipflight11-presplash-1152x648.jpg",
      "popularity_score": 133,
      "ai_summary": [
        "China launched a rocket without providing any advance warning to the public.",
        "Starlink, Kuiper, and the US military all added satellites to their mega-constellations.",
        "The report covers multiple space-related activities from various entities.",
        "Details about the specific launches and their purposes are included.",
        "The report highlights the ongoing expansion of satellite constellations."
      ]
    }
  ]
}