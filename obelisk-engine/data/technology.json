{
  "updated_at": "2025-11-04T03:41:59.881Z",
  "clusters": [
    {
      "id": "cluster_12",
      "coverage": 2,
      "updated_at": "Mon, 03 Nov 2025 23:36:24 +0000",
      "title": "YouTube TV blackout with Disney: How to watch ESPN, ABC and more as a YouTube TV subscriber",
      "neutral_headline": "YouTube TV blackout with Disney: How to watch ESPN, ABC and more as a YouTube TV subscriber",
      "items": [
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/entertainment/streaming/youtube-tv-blackout-with-disney-how-to-watch-espn-abc-and-more-as-a-youtube-tv-subscriber-173330635.html",
          "published_at": "Mon, 03 Nov 2025 23:36:24 +0000",
          "title": "YouTube TV blackout with Disney: How to watch ESPN, ABC and more as a YouTube TV subscriber",
          "standfirst": "SOPA Images via Getty Images It doesn't look like Disney-owned channels including ABC and ESPN will be returning to YouTube TV anytime soon. The Walt Disney Co. pulled its channels from YouTube TV as of midnight on Oct. 30 after the two companies failed to reach new terms on their latest carriage agreement. While big sporting events are often where the rubber meets the road on these channel blackouts, YouTube TV subscribers were unable to see any college football games on ABC or ESPN all weekend, and it looks like anyone hoping to watch tonight's Monday Night Football game between the Arizona Cardinals and the Dallas Cowboys will suffer the same fate: YouTube TV management has officially rebuffed Disney's request for a 24-hour restoration of its channels in a blog post — ostensibly to offer coverage of Tuesday's elections — proposing instead that Disney reactivate the feeds for ABC and ESPN while negotiations continue. YouTube TV had previously stated that if Disney’s channels remain off the platform for an extended period, customers will receive a $20 monthly credit. That's all fine and good, but if you're looking to watch tonight's game or your favorite shows — including Abbott Elementary, Grey's Anatomy and Dancing with the Stars, or Wednesday's NBA games — you'll need to seek out alternative viewing methods. And unfortunately for YouTube TV's negotiating position, there are plenty of options. One of the cheapest ways to watch ESPN is with a Sling Day Pass — for just $5/day, you can tune into any and all ESPN programming, including Monday Night Football, with no other commitments. If you want a full switch from YouTube TV, there's Hulu + Live TV, DirecTV, or Fubo, where you can watch all the Disney-owned channels. (Remember, unlike a lot of cable plans, you can easily pause or cancel YouTube TV or any of these alternatives, so long as you have month-to-month subscriptions.) If you're looking for a workaround to watch ESPN, the Disney Channel, ABC and more, here's are the best options so you won't miss a moment of sports, news, or entertainment, all pulled from our list of best live TV streaming services to cut cable. Grab an ESPN bundle so you won't miss the NFL, NBA or any other games Get Hulu + Live TV at a great price Try Fubo free for a week and get $30 your first month Try DirecTV free for 5 days, and get $30 off your first month What about Sling \"day passes\"? You may have heard that Sling offers day, weekend and week passes to its streaming programming for as little as $5 per day. That is an option if you're looking for just some of the ESPN channels (the Sling Orange tier), but ABC isn't included. (If you're just looking to catch one of this week's big games, like Monday Night Football on ESPN, it's a great short-term solution.) If you want a longer-term solution, you can get both ESPN and ABC with Sling's Orange and Blue package ($30 a month to start, $61 thereafter), but you'll need to add on the Sports Extra package for ESPNU, which requires an additional charge. Get your local Disney/ABC programming for free Need your local ABC programming? Your station may have its own free local streaming news channel (many do), you can see if The Roku Channel carries your local station's news, or download your local news station app if it's a Nexstar channel. The other alternative — if you're within the broadcast radius of a local ABC affiliate — is to get an over-the-air antenna. You can plug in your ZIP code at antennaweb.org to see what channels are in your area. This off-brand unit has worked very well in our initial testing — it's under $30, and the channels are truly free. What games are on ESPN/ABC this week? If you're wondering what games you might miss as a result of the YouTubeTV/Disney blackout, here's a list of some upcoming sports you may not want to miss: Monday, Nov. 3 Monday Night Football: Arizona Cardinals vs. Dallas Cowboys, 8:15 p.m. ET (ESPN/ABC) Wednesday, Nov. 5 NBA: Minnesota Timberwolves vs. New York Knicks, 7:30 p.m. ET (ESPN) NBA: San Antonio Spurs vs. Los Angeles Lakers, 10 p.m. ET (ESPN) Update Nov. 3 2025, 6:36PM ET: This story has been updated to include YouTube TV's latest response to Disney's request to restore its channels for just 24 hours.This article originally appeared on Engadget at https://www.engadget.com/entertainment/streaming/youtube-tv-blackout-with-disney-how-to-watch-espn-abc-and-more-as-a-youtube-tv-subscriber-173330635.html?src=rss",
          "content": "SOPA Images via Getty Images It doesn't look like Disney-owned channels including ABC and ESPN will be returning to YouTube TV anytime soon. The Walt Disney Co. pulled its channels from YouTube TV as of midnight on Oct. 30 after the two companies failed to reach new terms on their latest carriage agreement. While big sporting events are often where the rubber meets the road on these channel blackouts, YouTube TV subscribers were unable to see any college football games on ABC or ESPN all weekend, and it looks like anyone hoping to watch tonight's Monday Night Football game between the Arizona Cardinals and the Dallas Cowboys will suffer the same fate: YouTube TV management has officially rebuffed Disney's request for a 24-hour restoration of its channels in a blog post — ostensibly to offer coverage of Tuesday's elections — proposing instead that Disney reactivate the feeds for ABC and ESPN while negotiations continue. YouTube TV had previously stated that if Disney’s channels remain off the platform for an extended period, customers will receive a $20 monthly credit. That's all fine and good, but if you're looking to watch tonight's game or your favorite shows — including Abbott Elementary, Grey's Anatomy and Dancing with the Stars, or Wednesday's NBA games — you'll need to seek out alternative viewing methods. And unfortunately for YouTube TV's negotiating position, there are plenty of options. One of the cheapest ways to watch ESPN is with a Sling Day Pass — for just $5/day, you can tune into any and all ESPN programming, including Monday Night Football, with no other commitments. If you want a full switch from YouTube TV, there's Hulu + Live TV, DirecTV, or Fubo, where you can watch all the Disney-owned channels. (Remember, unlike a lot of cable plans, you can easily pause or cancel YouTube TV or any of these alternatives, so long as you have month-to-month subscriptions.) If you're looking for a workaround to watch ESPN, the Disney Channel, ABC and more, here's are the best options so you won't miss a moment of sports, news, or entertainment, all pulled from our list of best live TV streaming services to cut cable. Grab an ESPN bundle so you won't miss the NFL, NBA or any other games Get Hulu + Live TV at a great price Try Fubo free for a week and get $30 your first month Try DirecTV free for 5 days, and get $30 off your first month What about Sling \"day passes\"? You may have heard that Sling offers day, weekend and week passes to its streaming programming for as little as $5 per day. That is an option if you're looking for just some of the ESPN channels (the Sling Orange tier), but ABC isn't included. (If you're just looking to catch one of this week's big games, like Monday Night Football on ESPN, it's a great short-term solution.) If you want a longer-term solution, you can get both ESPN and ABC with Sling's Orange and Blue package ($30 a month to start, $61 thereafter), but you'll need to add on the Sports Extra package for ESPNU, which requires an additional charge. Get your local Disney/ABC programming for free Need your local ABC programming? Your station may have its own free local streaming news channel (many do), you can see if The Roku Channel carries your local station's news, or download your local news station app if it's a Nexstar channel. The other alternative — if you're within the broadcast radius of a local ABC affiliate — is to get an over-the-air antenna. You can plug in your ZIP code at antennaweb.org to see what channels are in your area. This off-brand unit has worked very well in our initial testing — it's under $30, and the channels are truly free. What games are on ESPN/ABC this week? If you're wondering what games you might miss as a result of the YouTubeTV/Disney blackout, here's a list of some upcoming sports you may not want to miss: Monday, Nov. 3 Monday Night Football: Arizona Cardinals vs. Dallas Cowboys, 8:15 p.m. ET (ESPN/ABC) Wednesday, Nov. 5 NBA: Minnesota Timberwolves vs. New York Knicks, 7:30 p.m. ET (ESPN) NBA: San Antonio Spurs vs. Los Angeles Lakers, 10 p.m. ET (ESPN) Update Nov. 3 2025, 6:36PM ET: This story has been updated to include YouTube TV's latest response to Disney's request to restore its channels for just 24 hours.This article originally appeared on Engadget at https://www.engadget.com/entertainment/streaming/youtube-tv-blackout-with-disney-how-to-watch-espn-abc-and-more-as-a-youtube-tv-subscriber-173330635.html?src=rss",
          "feed_position": 2,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2025-02/f18a9a20-ea15-11ef-b5d3-3ae5ab47679d"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/big-tech/ios-ipados-and-macos-now-let-you-frost-apples-liquid-glass-225513425.html",
          "published_at": "Mon, 03 Nov 2025 22:55:13 +0000",
          "title": "iOS, iPadOS and macOS now let you frost Apple's Liquid Glass",
          "standfirst": "As expected, iOS 26.1 is out now for all Apple smartphone users today, and it includes what is sure to be a popular feature from the beta. Once installed, this update lets people opt to give the Liquid Glass look a frostier, more opaque appearance. You can find the option to tint the screen behind notifications and tab bars within the Settings menu. It's under Display & Brightness, then the Liquid Glass section. The feature is also present in iPadOS 26.1 and macOS 26.1, both of which also dropped today.Ever since Apple unveiled the Liquid Glass design it had planned for the next versions of iOS, the aesthetic has been divisive. (We at Engadget have been pretty well split down the middle about it from the start.) The tinting of the newest operating systems joins a growing roster of accessibility and visibility options to customize how Liquid Glass looks, from the full-on transparent mode to a higher-contrast and higher-opacity approach. One other standout from the 26.1 OS releases is for the iPad users. Those of you who wanted the return of Slide Over for multitasking can breathe a sigh of relief: after appearing in the beta last month, the feature is back. Many iPad owners appreciated how Slide Over let them control screen real estate without constant rearranging of windows. The feature has been reimagined for the tablet's current capabilities, essentially letting you pin a window to the top of your screen and hide it when you want. This window can also be resized and given your aspect ratio of choice.This article originally appeared on Engadget at https://www.engadget.com/big-tech/ios-ipados-and-macos-now-let-you-frost-apples-liquid-glass-225513425.html?src=rss",
          "content": "As expected, iOS 26.1 is out now for all Apple smartphone users today, and it includes what is sure to be a popular feature from the beta. Once installed, this update lets people opt to give the Liquid Glass look a frostier, more opaque appearance. You can find the option to tint the screen behind notifications and tab bars within the Settings menu. It's under Display & Brightness, then the Liquid Glass section. The feature is also present in iPadOS 26.1 and macOS 26.1, both of which also dropped today.Ever since Apple unveiled the Liquid Glass design it had planned for the next versions of iOS, the aesthetic has been divisive. (We at Engadget have been pretty well split down the middle about it from the start.) The tinting of the newest operating systems joins a growing roster of accessibility and visibility options to customize how Liquid Glass looks, from the full-on transparent mode to a higher-contrast and higher-opacity approach. One other standout from the 26.1 OS releases is for the iPad users. Those of you who wanted the return of Slide Over for multitasking can breathe a sigh of relief: after appearing in the beta last month, the feature is back. Many iPad owners appreciated how Slide Over let them control screen real estate without constant rearranging of windows. The feature has been reimagined for the tablet's current capabilities, essentially letting you pin a window to the top of your screen and hide it when you want. This window can also be resized and given your aspect ratio of choice.This article originally appeared on Engadget at https://www.engadget.com/big-tech/ios-ipados-and-macos-now-let-you-frost-apples-liquid-glass-225513425.html?src=rss",
          "feed_position": 3
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/ai/google-removes-ai-model-after-it-allegedly-accused-a-senator-of-sexual-assault-170235679.html",
          "published_at": "Mon, 03 Nov 2025 17:02:35 +0000",
          "title": "Google removes AI model after it allegedly accused a senator of sexual assault",
          "standfirst": "Google has pulled the AI model Gemma from its Studio platform after a Republican senator said it \"fabricated serious criminal allegations\" against her, as reported by The Verge. Senator Marsha Blackburn (R-TN) sent a letter to CEO Sundar Pichai to accuse the company of defamation after the model allegedly created a story about her committing sexual assault. The model was reportedly asked if Blackburn had ever \"been accused of rape\" and it reportedly answered in the affirmative, going so far as to provide a list of fake news articles to support the accusation. The chatbot said the senator “was accused of having a sexual relationship with a state trooper” during a campaign for state senate. This officer reportedly said “she pressured him to obtain prescription drugs for her and that the relationship involved non-consensual acts.” Gemma is available via an API and was also available via AI Studio, which is a developer tool (in fact to use it you need to attest you're a developer). We’ve now seen reports of non-developers trying to use Gemma in AI Studio and ask it factual questions. We never intended this…— News from Google (@NewsFromGoogle) November 1, 2025 None of this happened, of course. The chatbot said this transgression occurred during Blackburn's 1987 campaign, but she didn't run for state senate until 1998. She has never been accused of anything like that. \"The links lead to error pages and unrelated news articles. There has never been such an accusation, there is no such individual and there are no such news stories. This is not a harmless 'hallucination.' It is an act of defamation produced and distributed by a Google-owned AI model,\" she wrote to Pichai. There's one major caveat here. The chatbot in question, Gemma, is designed for developers and not for mass market queries. There are Gemma variants for medical use, coding and more. Google says it was never meant as a consumer tool or to be used to answer factual questions. It's still pulling the model from AI Studio to \"prevent this confusion.\" It'll still be available to developers through the API. Google has reportedly removed Gemma from its AI studio after I demanded the company take it down for smearing conservatives with manufactured criminal allegations. Google owes the American people answers, and I will be eagerly awaiting their response to my letter. pic.twitter.com/n8ye5ZKBu1— Sen. Marsha Blackburn (@MarshaBlackburn) November 3, 2025 Blackburn went a step further, accusing Google's AI platform of engaging in a \"consistent pattern of bias against conservative figures.\" I encounter multiple hallucinations every day. Chatbots have lied about all kinds of stuff about my life and what I write about online. AI chatbots are famous for making stuff up, conservative or not. Not everything is a political witch hunt. Sometimes tech just does what tech does.This article originally appeared on Engadget at https://www.engadget.com/ai/google-removes-ai-model-after-it-allegedly-accused-a-senator-of-sexual-assault-170235679.html?src=rss",
          "content": "Google has pulled the AI model Gemma from its Studio platform after a Republican senator said it \"fabricated serious criminal allegations\" against her, as reported by The Verge. Senator Marsha Blackburn (R-TN) sent a letter to CEO Sundar Pichai to accuse the company of defamation after the model allegedly created a story about her committing sexual assault. The model was reportedly asked if Blackburn had ever \"been accused of rape\" and it reportedly answered in the affirmative, going so far as to provide a list of fake news articles to support the accusation. The chatbot said the senator “was accused of having a sexual relationship with a state trooper” during a campaign for state senate. This officer reportedly said “she pressured him to obtain prescription drugs for her and that the relationship involved non-consensual acts.” Gemma is available via an API and was also available via AI Studio, which is a developer tool (in fact to use it you need to attest you're a developer). We’ve now seen reports of non-developers trying to use Gemma in AI Studio and ask it factual questions. We never intended this…— News from Google (@NewsFromGoogle) November 1, 2025 None of this happened, of course. The chatbot said this transgression occurred during Blackburn's 1987 campaign, but she didn't run for state senate until 1998. She has never been accused of anything like that. \"The links lead to error pages and unrelated news articles. There has never been such an accusation, there is no such individual and there are no such news stories. This is not a harmless 'hallucination.' It is an act of defamation produced and distributed by a Google-owned AI model,\" she wrote to Pichai. There's one major caveat here. The chatbot in question, Gemma, is designed for developers and not for mass market queries. There are Gemma variants for medical use, coding and more. Google says it was never meant as a consumer tool or to be used to answer factual questions. It's still pulling the model from AI Studio to \"prevent this confusion.\" It'll still be available to developers through the API. Google has reportedly removed Gemma from its AI studio after I demanded the company take it down for smearing conservatives with manufactured criminal allegations. Google owes the American people answers, and I will be eagerly awaiting their response to my letter. pic.twitter.com/n8ye5ZKBu1— Sen. Marsha Blackburn (@MarshaBlackburn) November 3, 2025 Blackburn went a step further, accusing Google's AI platform of engaging in a \"consistent pattern of bias against conservative figures.\" I encounter multiple hallucinations every day. Chatbots have lied about all kinds of stuff about my life and what I write about online. AI chatbots are famous for making stuff up, conservative or not. Not everything is a political witch hunt. Sometimes tech just does what tech does.This article originally appeared on Engadget at https://www.engadget.com/ai/google-removes-ai-model-after-it-allegedly-accused-a-senator-of-sexual-assault-170235679.html?src=rss",
          "feed_position": 9
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/black-friday-2025-the-best-early-tech-deals-on-apple-shark-lego-and-other-gear-plus-what-to-expect-during-the-sale-100052983.html",
          "published_at": "Mon, 03 Nov 2025 15:46:28 +0000",
          "title": "Black Friday 2025: The best early tech deals on Apple, Shark, Lego and other gear, plus what to expect during the sale",
          "standfirst": "Black Friday has become the time to buy the hottest tech of the year. Whether you're shopping for yourself or stocking up on gifts for the holidays, Black Friday deals are sure to bring the best prices of the year to things like headphones, game consoles, robot vacuums, phone accessories and everything in between. You don't even have to wait until Black Friday proper to save a ton of money. Over the past few years, we've seen Black Friday tech deals start earlier and earlier — to the point where the entire month of November is packed with discounts.If you're on the hunt for solid tech deals, Engadget has you covered. We've collected the best Black Friday deals on tech you can get right now, and we'll continue to update this post as we get closer to the big day at the end of November. Note that you probably have the best chance of snagging record-low prices when we get to about one week before Thanksgiving, but these deals available now are worth considering. Black Friday deals to shop now Apple Watch SE 3 for $200 ($50 off): The SE has been our top pick for the best Apple Watch for those on a budget, and the latest model only solidifies that further. It has the same chipset found in the latest flagship Apple Watches, fast-charging capabilities, an always-on display and most of the same activity-tracking features you'll find in more expensive model. Jisulife Life 7 handheld fan for $25 (14 percent off, Prime exclusive): This handy little fan is a must-have if you life in a warm climate or have a tropical vacation planned anytime soon. It can be used as a table or handheld fan and even be worn around the neck so you don't have to hold it at all. Its 5,000 mAh battery allows it to last hours on a single charge, and the small display in the middle of the fan's blades show its remaining battery level. Lego Disney advent calendar 2025 43253 for $39 (14 percent off): I probably don't need to tell you 'tis the season for advent calendars. Lego has a bunch, and this one in particular is on a good deal right now. Like most Lego advent calendars, this one includes a number of bricks and minifigs that, when put together, make a coherent, themed scene. Leebin 2025 electric spin scrubber for $40 (43 percent off, Prime exclusive): This weird little scrubber makes cleaning my bathroom and shower much less of a pain. Just choose the brush head you need for your job and the rotating head takes care of most of the hard work. I love the adjustable handle, which extends from 12 to 50 inches so you can get into hard-to-reach places without breaking a sweat. Anker MagGo power bank with kickstand (Qi2, 15W, 10K) for $57 (37 percent off): One of our favorite power banks, this brick from Anker gives you 10K capacity in a magnetic, relatively slim package. The built-in kickstand makes it easy to watch videos or make FaceTime calls while powering up, and the small display on the bank's edge will show you how much power is flowing to your phone. SanDisk microSD Express card (256GB) for $60 (12 percent off): If you have a Switch 2, no regular microSD card will do if you want to expand the console's storage. You need a newer microSD Express card, and currently there are only a handful on the market. We did some testing to find the best microSD Express card for the Switch 2 and found that performance was, in general, very similar amongst all the readily available cards. We recommend getting whichever fits within your budget at the capacity you want. Google TV Streamer 4K for $75 ($25 off): Our top pick for the best streaming device right now, the latest version of Google's streamer supports 4K video and an excellent, easy-to-use interface that will feel familiar to anyone who's seen a set with the Google TV technology built in. It provides access to all of the major streaming services including Netflix, Disney+, HBO Max, YouTube and more, plus it has a handy on-screen pop up that lets you control all compatible smart home devices right from your TV. Cosori 9-in-1 air fryer for $90 (25 percent off): I personally have this air fryer, one of our top picks, in my house and I've used it for over a year with no issues. I love that it makes good use of vertical space so it doesn't take up too much space on my counter, and its rounded-square shape allows me to cook more food than you'd think in one go in the basket. It crisps all kinds of foods up well and generally takes a lot of the guess work (and time) out of making a good meal. Shark Steam and Scrub Steamer Mop for $110 (31 percent off): This is a slightly more affordable version of this Shark steamer I bought earlier this year and has become one of my favorite things. The one on sale has almost an identical design and arguably the most important feature: rotating dual pads on the bottom that actually lift up the dirt and stains on your floor without too much extra elbow grease needed on your part. It also has three different cleaning modes to choose from, depending on how stubborn the mess is that you're trying to clean up. Shark AI Ultra robot vacuum with 60-day self-emptying base for $300 (50 percent off): This is a version of one of our top picks for the best robot vacuums. We generally like Shark machines because they do a good job cleaning all types of flooring, produce accurate home maps and the companion app is pretty easy to use. This one in particularly comes with a self-emptying base that can hold up to 60 days worth of debris. Black Friday FAQs When is Black Friday 2025? Black Friday 2025 lands on November 28. Which stores have Black Friday deals? Many physical retail stores have Black Friday deals including Walmart, Target, Best Buy and others. Even more retailers have online Black Friday deals, including Amazon, GameStop, Costco and others. When do Black Friday sales start? Gone are the times when Black Friday sales were one-day-only affairs. Now, Black Friday deals are often available starting on Thanksgiving, or even earlier. Last year, we saw Black Friday deals online begin the week before Black Friday proper. When do Black Friday sales end? Black Friday and Cyber Monday have blended a lot over the past few years. Now, you can expect to see a good portion of Black Friday deals extend through the weekend and into Cyber Monday. It's not uncommon for Black Friday deals to expire at the end of Cyber Monday. Which retailers have the best Black Friday tech deals? The best Black Friday tech deals are typically available online at retailers like Amazon, Walmart, Best Buy and Target. It's also a good idea to check the store websites of the companies that make the products you want — for example, if you're looking for a Sonos speaker, check the Sonos website on Black Friday. Most of the time, you'll find the best Black Friday tech deals are matched at multiple retailers. Does Apple have Black Friday sales? No, you will usually not find Black Friday sales at Apple stores or on Apple's website. However, you can find Black Friday deals on Apple devices elsewhere; we recommend checking Amazon, Best Buy and other big retailers for discounts on iPads, Apple Watches and more on Black Friday. Does Amazon have Black Friday sales? Yes, Amazon has Black Friday sales. The online retailer's site will look similar to Prime Day on Black Friday, with discounts on all sorts of items from household essentials to fashion to tech.This article originally appeared on Engadget at https://www.engadget.com/deals/black-friday-2025-the-best-early-tech-deals-on-apple-shark-lego-and-other-gear-plus-what-to-expect-during-the-sale-100052983.html?src=rss",
          "content": "Black Friday has become the time to buy the hottest tech of the year. Whether you're shopping for yourself or stocking up on gifts for the holidays, Black Friday deals are sure to bring the best prices of the year to things like headphones, game consoles, robot vacuums, phone accessories and everything in between. You don't even have to wait until Black Friday proper to save a ton of money. Over the past few years, we've seen Black Friday tech deals start earlier and earlier — to the point where the entire month of November is packed with discounts.If you're on the hunt for solid tech deals, Engadget has you covered. We've collected the best Black Friday deals on tech you can get right now, and we'll continue to update this post as we get closer to the big day at the end of November. Note that you probably have the best chance of snagging record-low prices when we get to about one week before Thanksgiving, but these deals available now are worth considering. Black Friday deals to shop now Apple Watch SE 3 for $200 ($50 off): The SE has been our top pick for the best Apple Watch for those on a budget, and the latest model only solidifies that further. It has the same chipset found in the latest flagship Apple Watches, fast-charging capabilities, an always-on display and most of the same activity-tracking features you'll find in more expensive model. Jisulife Life 7 handheld fan for $25 (14 percent off, Prime exclusive): This handy little fan is a must-have if you life in a warm climate or have a tropical vacation planned anytime soon. It can be used as a table or handheld fan and even be worn around the neck so you don't have to hold it at all. Its 5,000 mAh battery allows it to last hours on a single charge, and the small display in the middle of the fan's blades show its remaining battery level. Lego Disney advent calendar 2025 43253 for $39 (14 percent off): I probably don't need to tell you 'tis the season for advent calendars. Lego has a bunch, and this one in particular is on a good deal right now. Like most Lego advent calendars, this one includes a number of bricks and minifigs that, when put together, make a coherent, themed scene. Leebin 2025 electric spin scrubber for $40 (43 percent off, Prime exclusive): This weird little scrubber makes cleaning my bathroom and shower much less of a pain. Just choose the brush head you need for your job and the rotating head takes care of most of the hard work. I love the adjustable handle, which extends from 12 to 50 inches so you can get into hard-to-reach places without breaking a sweat. Anker MagGo power bank with kickstand (Qi2, 15W, 10K) for $57 (37 percent off): One of our favorite power banks, this brick from Anker gives you 10K capacity in a magnetic, relatively slim package. The built-in kickstand makes it easy to watch videos or make FaceTime calls while powering up, and the small display on the bank's edge will show you how much power is flowing to your phone. SanDisk microSD Express card (256GB) for $60 (12 percent off): If you have a Switch 2, no regular microSD card will do if you want to expand the console's storage. You need a newer microSD Express card, and currently there are only a handful on the market. We did some testing to find the best microSD Express card for the Switch 2 and found that performance was, in general, very similar amongst all the readily available cards. We recommend getting whichever fits within your budget at the capacity you want. Google TV Streamer 4K for $75 ($25 off): Our top pick for the best streaming device right now, the latest version of Google's streamer supports 4K video and an excellent, easy-to-use interface that will feel familiar to anyone who's seen a set with the Google TV technology built in. It provides access to all of the major streaming services including Netflix, Disney+, HBO Max, YouTube and more, plus it has a handy on-screen pop up that lets you control all compatible smart home devices right from your TV. Cosori 9-in-1 air fryer for $90 (25 percent off): I personally have this air fryer, one of our top picks, in my house and I've used it for over a year with no issues. I love that it makes good use of vertical space so it doesn't take up too much space on my counter, and its rounded-square shape allows me to cook more food than you'd think in one go in the basket. It crisps all kinds of foods up well and generally takes a lot of the guess work (and time) out of making a good meal. Shark Steam and Scrub Steamer Mop for $110 (31 percent off): This is a slightly more affordable version of this Shark steamer I bought earlier this year and has become one of my favorite things. The one on sale has almost an identical design and arguably the most important feature: rotating dual pads on the bottom that actually lift up the dirt and stains on your floor without too much extra elbow grease needed on your part. It also has three different cleaning modes to choose from, depending on how stubborn the mess is that you're trying to clean up. Shark AI Ultra robot vacuum with 60-day self-emptying base for $300 (50 percent off): This is a version of one of our top picks for the best robot vacuums. We generally like Shark machines because they do a good job cleaning all types of flooring, produce accurate home maps and the companion app is pretty easy to use. This one in particularly comes with a self-emptying base that can hold up to 60 days worth of debris. Black Friday FAQs When is Black Friday 2025? Black Friday 2025 lands on November 28. Which stores have Black Friday deals? Many physical retail stores have Black Friday deals including Walmart, Target, Best Buy and others. Even more retailers have online Black Friday deals, including Amazon, GameStop, Costco and others. When do Black Friday sales start? Gone are the times when Black Friday sales were one-day-only affairs. Now, Black Friday deals are often available starting on Thanksgiving, or even earlier. Last year, we saw Black Friday deals online begin the week before Black Friday proper. When do Black Friday sales end? Black Friday and Cyber Monday have blended a lot over the past few years. Now, you can expect to see a good portion of Black Friday deals extend through the weekend and into Cyber Monday. It's not uncommon for Black Friday deals to expire at the end of Cyber Monday. Which retailers have the best Black Friday tech deals? The best Black Friday tech deals are typically available online at retailers like Amazon, Walmart, Best Buy and Target. It's also a good idea to check the store websites of the companies that make the products you want — for example, if you're looking for a Sonos speaker, check the Sonos website on Black Friday. Most of the time, you'll find the best Black Friday tech deals are matched at multiple retailers. Does Apple have Black Friday sales? No, you will usually not find Black Friday sales at Apple stores or on Apple's website. However, you can find Black Friday deals on Apple devices elsewhere; we recommend checking Amazon, Best Buy and other big retailers for discounts on iPads, Apple Watches and more on Black Friday. Does Amazon have Black Friday sales? Yes, Amazon has Black Friday sales. The online retailer's site will look similar to Prime Day on Black Friday, with discounts on all sorts of items from household essentials to fashion to tech.This article originally appeared on Engadget at https://www.engadget.com/deals/black-friday-2025-the-best-early-tech-deals-on-apple-shark-lego-and-other-gear-plus-what-to-expect-during-the-sale-100052983.html?src=rss",
          "feed_position": 10
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/data-infrastructure/ai-coding-transforms-data-engineering-how-dlthubs-open-source-python-library",
          "published_at": "Mon, 03 Nov 2025 15:00:00 GMT",
          "title": "AI coding transforms data engineering: How dltHub's open-source Python library helps developers create data pipelines for AI in minutes",
          "standfirst": "A quiet revolution is reshaping enterprise data engineering. Python developers are building production data pipelines in minutes using tools that would have required entire specialized teams just months ago.The catalyst is dlt, an open-source Python library that automates complex data engineering tasks. The tool has reached 3 million monthly downloads and powers data workflows for over 5,000 companies across regulated industries including finance, healthcare and manufacturing. That technology is getting another solid vote of confidence today as dltHub, the Berlin-based company behind the open-source dlt library, is raising $8 million in seed funding led by Bessemer Venture Partners. What makes this significant isn&#x27;t just adoption numbers. It&#x27;s how developers are using the tool in combination with AI coding assistants to accomplish tasks that previously required infrastructure engineers, DevOps specialists and on-call personnel.The company is building a cloud-hosted platform that extends their open-source library into a complete end-to-end solution. The platform will allow developers to deploy pipelines, transformations and notebooks with a single command without worrying about infrastructure. This represents a fundamental shift from data engineering requiring specialized teams to becoming accessible to any Python developer.\"Any Python developer should be able to bring their business users closer to fresh, reliable data,\" Matthaus Krzykowski, dltHub&#x27;s co-founder and CEO told VentureBeat in an exclusive interview. \"Our mission is to make data engineering as accessible, collaborative and frictionless as writing Python itself.\"From SQL to Python-native data engineeringThe problem the company set out to solve emerged from real-world frustrations.One core set of frustrations comes from a fundamental clash between how different generations of developers work with data. Krzykowski noted that there is a generation of developers that are grounded in SQL and relational database technology. On the other hand is a generation of developers building AI agents with Python.This divide reflects deeper technical challenges. SQL-based data engineering locks teams into specific platforms and requires extensive infrastructure knowledge. Python developers working on AI need lightweight, platform-agnostic tools that work in notebooks and integrate with LLM coding assistants.The dlt library changes this equation by automating complex data engineering tasks in simple Python code. \"If you know what a function in Python is, what a list is, a source and resource, then you can write this very declarative, very simple code,\" Krzykowski explained.The key technical breakthrough addresses schema evolution automatically. When data sources change their output format, traditional pipelines break. \"DLT has mechanisms to automatically resolve these issues,\" Thierry Jean, founding engineer at dltHub told VentureBeat. \"So it will push data, and you can say, alert me if things change upstream, or just make it flexible enough and change the data and the destination in a way to accommodate these things.\"Real-world developer experienceHoyt Emerson, Data Consultant and Content Creator at The Full Data Stack, recently adopted the tool for a job where he had a challenge to solve.He needed to move data from Google Cloud Storage to multiple destinations including Amazon S3 and a data warehouse. Traditional approaches would require platform-specific knowledge for each destination. Emerson told VentureBeat that what he really wanted was a much more lightweight, platform agnostic way to send data from one spot to another. \"That&#x27;s when DLT gave me the aha moment,\" Emerson said.He completed the entire pipeline in five minutes using the library&#x27;s documentation which made it easy to get up and running quickly and without issue..The process gets even more powerful when combined with AI coding assistants. Emerson noted that he&#x27;s using agentic AI coding principles and realized that the dlt documentation could be sent as context to an LLM to accelerate and automate his data work. With the documentation as context, Emerson was able to create reusable templates for future projects and used AI assistants to generate deployment configurations.\"It&#x27;s extremely LLM friendly because it&#x27;s very well documented,\" he said.The LLM-Native development patternThis combination of well-documented tools and AI assistance represents a new development pattern. The company has optimized specifically for what they call \"YOLO mode\" development where developers copy error messages and paste them into AI coding assistants.\"A lot of these people are literally just copying and pasting error messages and are trying the code editors to figure it out,\" Krzykowski said. The company takes this behavior seriously enough that they fix issues specifically for AI-assisted workflows.The results speak to the approach&#x27;s effectiveness. In September alone, users created over 50,000 custom connectors using the library. That represents a 20x increase since January, driven largely by LLM-assisted development.Technical architecture for enterprise scaleThe dlt design philosophy prioritizes interoperability over platform lock-in. The tool can deploy anywhere from AWS Lambda to existing enterprise data stacks. It integrates with platforms like Snowflake while maintaining the flexibility to work with any destination.\"We always believe that DLT needs to be interoperable and modular,\" Krzykowski explained. \"It can be deployed anywhere. It can be on Lambda. It often becomes part of other people&#x27;s data infrastructures.\"Key technical capabilities include:Automatic Schema Evolution: Handles upstream data changes without breaking pipelines or requiring manual intervention.Incremental Loading: Processes only new or changed records, reducing computational overhead and costs.Platform Agnostic Deployment: Works across cloud providers and on-premises infrastructure without modification.LLM-Optimized Documentation: Structured specifically for AI assistant consumption, enabling rapid problem-solving and template generation.The platform currently supports over 4,600 REST API data sources with continuous expansion driven by user-generated connectors.Competing against ETL giants with a code-first approachThe data engineering landscape splits into distinct camps, each serving different enterprise needs and developer preferences. Traditional ETL platforms like Informatica and Talend dominate enterprise environments with GUI-based tools that require specialized training but offer comprehensive governance features.Newer SaaS platforms like Fivetran have gained traction by emphasizing pre-built connectors and managed infrastructure, reducing operational overhead but creating vendor dependency.The open-source dlt library occupies a fundamentally different position as code-first, LLM-native infrastructure that developers can extend and customize. \"We always believe that DLT needs to be interoperable and modular,\" Krzykowski explained. \"It can be deployed anywhere. It can be on Lambda. It often becomes part of other people&#x27;s data infrastructures.\"This positioning reflects the broader shift toward what the industry calls the composable data stack where enterprises build infrastructure from interoperable components rather than monolithic platforms.More importantly, the intersection with AI creates new market dynamics. \"LLMs aren&#x27;t replacing data engineers,\" Krzykowski said. \"But they radically expand their reach and productivity.\"What this means for enterprise data leadersFor enterprises looking to lead in AI-driven operations, this development represents an opportunity to fundamentally rethink data engineering strategies.The immediate tactical advantages are clear. Organizations can leverage existing Python developers instead of hiring specialized data engineering teams. Organizations that adapt their tooling and hiking approaches to leverage this trend may find significant cost and agility advantages over competitors still dependent on traditional, team-intensive data engineering.The question isn&#x27;t whether this shift toward democratized data engineering will occur. It&#x27;s how quickly enterprises adapt to capitalize on it.",
          "content": "A quiet revolution is reshaping enterprise data engineering. Python developers are building production data pipelines in minutes using tools that would have required entire specialized teams just months ago.The catalyst is dlt, an open-source Python library that automates complex data engineering tasks. The tool has reached 3 million monthly downloads and powers data workflows for over 5,000 companies across regulated industries including finance, healthcare and manufacturing. That technology is getting another solid vote of confidence today as dltHub, the Berlin-based company behind the open-source dlt library, is raising $8 million in seed funding led by Bessemer Venture Partners. What makes this significant isn&#x27;t just adoption numbers. It&#x27;s how developers are using the tool in combination with AI coding assistants to accomplish tasks that previously required infrastructure engineers, DevOps specialists and on-call personnel.The company is building a cloud-hosted platform that extends their open-source library into a complete end-to-end solution. The platform will allow developers to deploy pipelines, transformations and notebooks with a single command without worrying about infrastructure. This represents a fundamental shift from data engineering requiring specialized teams to becoming accessible to any Python developer.\"Any Python developer should be able to bring their business users closer to fresh, reliable data,\" Matthaus Krzykowski, dltHub&#x27;s co-founder and CEO told VentureBeat in an exclusive interview. \"Our mission is to make data engineering as accessible, collaborative and frictionless as writing Python itself.\"From SQL to Python-native data engineeringThe problem the company set out to solve emerged from real-world frustrations.One core set of frustrations comes from a fundamental clash between how different generations of developers work with data. Krzykowski noted that there is a generation of developers that are grounded in SQL and relational database technology. On the other hand is a generation of developers building AI agents with Python.This divide reflects deeper technical challenges. SQL-based data engineering locks teams into specific platforms and requires extensive infrastructure knowledge. Python developers working on AI need lightweight, platform-agnostic tools that work in notebooks and integrate with LLM coding assistants.The dlt library changes this equation by automating complex data engineering tasks in simple Python code. \"If you know what a function in Python is, what a list is, a source and resource, then you can write this very declarative, very simple code,\" Krzykowski explained.The key technical breakthrough addresses schema evolution automatically. When data sources change their output format, traditional pipelines break. \"DLT has mechanisms to automatically resolve these issues,\" Thierry Jean, founding engineer at dltHub told VentureBeat. \"So it will push data, and you can say, alert me if things change upstream, or just make it flexible enough and change the data and the destination in a way to accommodate these things.\"Real-world developer experienceHoyt Emerson, Data Consultant and Content Creator at The Full Data Stack, recently adopted the tool for a job where he had a challenge to solve.He needed to move data from Google Cloud Storage to multiple destinations including Amazon S3 and a data warehouse. Traditional approaches would require platform-specific knowledge for each destination. Emerson told VentureBeat that what he really wanted was a much more lightweight, platform agnostic way to send data from one spot to another. \"That&#x27;s when DLT gave me the aha moment,\" Emerson said.He completed the entire pipeline in five minutes using the library&#x27;s documentation which made it easy to get up and running quickly and without issue..The process gets even more powerful when combined with AI coding assistants. Emerson noted that he&#x27;s using agentic AI coding principles and realized that the dlt documentation could be sent as context to an LLM to accelerate and automate his data work. With the documentation as context, Emerson was able to create reusable templates for future projects and used AI assistants to generate deployment configurations.\"It&#x27;s extremely LLM friendly because it&#x27;s very well documented,\" he said.The LLM-Native development patternThis combination of well-documented tools and AI assistance represents a new development pattern. The company has optimized specifically for what they call \"YOLO mode\" development where developers copy error messages and paste them into AI coding assistants.\"A lot of these people are literally just copying and pasting error messages and are trying the code editors to figure it out,\" Krzykowski said. The company takes this behavior seriously enough that they fix issues specifically for AI-assisted workflows.The results speak to the approach&#x27;s effectiveness. In September alone, users created over 50,000 custom connectors using the library. That represents a 20x increase since January, driven largely by LLM-assisted development.Technical architecture for enterprise scaleThe dlt design philosophy prioritizes interoperability over platform lock-in. The tool can deploy anywhere from AWS Lambda to existing enterprise data stacks. It integrates with platforms like Snowflake while maintaining the flexibility to work with any destination.\"We always believe that DLT needs to be interoperable and modular,\" Krzykowski explained. \"It can be deployed anywhere. It can be on Lambda. It often becomes part of other people&#x27;s data infrastructures.\"Key technical capabilities include:Automatic Schema Evolution: Handles upstream data changes without breaking pipelines or requiring manual intervention.Incremental Loading: Processes only new or changed records, reducing computational overhead and costs.Platform Agnostic Deployment: Works across cloud providers and on-premises infrastructure without modification.LLM-Optimized Documentation: Structured specifically for AI assistant consumption, enabling rapid problem-solving and template generation.The platform currently supports over 4,600 REST API data sources with continuous expansion driven by user-generated connectors.Competing against ETL giants with a code-first approachThe data engineering landscape splits into distinct camps, each serving different enterprise needs and developer preferences. Traditional ETL platforms like Informatica and Talend dominate enterprise environments with GUI-based tools that require specialized training but offer comprehensive governance features.Newer SaaS platforms like Fivetran have gained traction by emphasizing pre-built connectors and managed infrastructure, reducing operational overhead but creating vendor dependency.The open-source dlt library occupies a fundamentally different position as code-first, LLM-native infrastructure that developers can extend and customize. \"We always believe that DLT needs to be interoperable and modular,\" Krzykowski explained. \"It can be deployed anywhere. It can be on Lambda. It often becomes part of other people&#x27;s data infrastructures.\"This positioning reflects the broader shift toward what the industry calls the composable data stack where enterprises build infrastructure from interoperable components rather than monolithic platforms.More importantly, the intersection with AI creates new market dynamics. \"LLMs aren&#x27;t replacing data engineers,\" Krzykowski said. \"But they radically expand their reach and productivity.\"What this means for enterprise data leadersFor enterprises looking to lead in AI-driven operations, this development represents an opportunity to fundamentally rethink data engineering strategies.The immediate tactical advantages are clear. Organizations can leverage existing Python developers instead of hiring specialized data engineering teams. Organizations that adapt their tooling and hiking approaches to leverage this trend may find significant cost and agility advantages over competitors still dependent on traditional, team-intensive data engineering.The question isn&#x27;t whether this shift toward democratized data engineering will occur. It&#x27;s how quickly enterprises adapt to capitalize on it.",
          "feed_position": 0,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/4s24Zr8moHSwWGw0JHGi7D/77a891f4e1193fe3fdce8423ff27befc/python-dev-smk1.jpg?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/strengthening-our-core-welcoming-karyne-levy-as-venturebeats-new-managing",
          "published_at": "Mon, 03 Nov 2025 15:00:00 GMT",
          "title": "Strengthening Our Core: Welcoming Karyne Levy as VentureBeat’s New Managing Editor",
          "standfirst": "I’m thrilled to announce a fantastic new addition to our leadership team: Karyne Levy is joining VentureBeat as our new Managing Editor. Today is her first day.Many of you may know Karyne from her most recent role as Deputy Managing Editor at TechCrunch, but her career is a highlight reel of veteran tech journalism. Her resume includes pivotal roles at Protocol, NerdWallet, Business Insider, and CNET, giving her a deep understanding of this industry from every angle.Hiring Karyne is a significant step forward for VentureBeat. As we’ve sharpened our focus on serving you – the enterprise technical decision-maker navigating the complexities of AI and data – I’ve been looking for a very specific kind of leader.The \"Organizer&#x27;s Dopamine Hit\"In the past, a managing editor was often the final backstop for copy. Today, at a modern, data-focused media company like ours, the role is infinitely more dynamic. It’s the central hub of the entire content operation.During my search, I found myself talking a lot about the two types of \"dopamine hits\" in our business. There’s the writer’s hit – seeing your name on a great story. And then there’s the organizer’s hit – the satisfaction that comes from building, tuning, and running the complex machine that allows a dozen different parts of the company to move in a single, powerful direction.We were looking for the organizer.When I spoke with Karyne, I explained this vision: a leader who thrives on creating workflows, who loves being the liaison between editorial, our data and survey team, our events, and our marketing operations.Her response confirmed she was the one: \"Everything you said is exactly my dopamine hit.\"Karyne’s passion is making the entire operation hum. She has a proven track record of managing people, running newsrooms, and interfacing with all parts of a business to ensure everyone is aligned. That operational rigor is precisely what we need for our next chapter.Why This Matters for Our Strategy (and for You)As I’ve written about before, VentureBeat is on a mission to evolve. In an age where experts and companies can publish directly, it’s not enough to be a secondary source. Our goal is to become a primary source for you.How? By leveraging our relationship with our community of millions of technical leaders. We are increasingly surveying you directly to generate proprietary insights you can’t get anywhere else. We want to be the first to tell you which vector stores your peers are actually implementing, what governance challenges are most pressing for data scientists, or how your counterparts are budgeting for generative AI.This is an ambitious strategy. It requires a tight-knit team where our editorial content, our research surveys and reports, our newsletters, and our VB Transform events are all working from the same playbook.Karyne is the leader who will help us execute that vision. Her experience at Protocol, which was also dedicated to serving technical and business decision-makers, means she fundamentally understands our audience. She is ideally suited to manage our newsroom and ensure that every piece of content we produce helps you do your job better. She’ll be working alongside Carl Franzen, our executive editor, who continues to drive news decision-making.This is a fantastic hire for VentureBeat. It’s another sign of our commitment to building the most focused, expert team in enterprise AI and data.Please join me in welcoming Karyne to the team.",
          "content": "I’m thrilled to announce a fantastic new addition to our leadership team: Karyne Levy is joining VentureBeat as our new Managing Editor. Today is her first day.Many of you may know Karyne from her most recent role as Deputy Managing Editor at TechCrunch, but her career is a highlight reel of veteran tech journalism. Her resume includes pivotal roles at Protocol, NerdWallet, Business Insider, and CNET, giving her a deep understanding of this industry from every angle.Hiring Karyne is a significant step forward for VentureBeat. As we’ve sharpened our focus on serving you – the enterprise technical decision-maker navigating the complexities of AI and data – I’ve been looking for a very specific kind of leader.The \"Organizer&#x27;s Dopamine Hit\"In the past, a managing editor was often the final backstop for copy. Today, at a modern, data-focused media company like ours, the role is infinitely more dynamic. It’s the central hub of the entire content operation.During my search, I found myself talking a lot about the two types of \"dopamine hits\" in our business. There’s the writer’s hit – seeing your name on a great story. And then there’s the organizer’s hit – the satisfaction that comes from building, tuning, and running the complex machine that allows a dozen different parts of the company to move in a single, powerful direction.We were looking for the organizer.When I spoke with Karyne, I explained this vision: a leader who thrives on creating workflows, who loves being the liaison between editorial, our data and survey team, our events, and our marketing operations.Her response confirmed she was the one: \"Everything you said is exactly my dopamine hit.\"Karyne’s passion is making the entire operation hum. She has a proven track record of managing people, running newsrooms, and interfacing with all parts of a business to ensure everyone is aligned. That operational rigor is precisely what we need for our next chapter.Why This Matters for Our Strategy (and for You)As I’ve written about before, VentureBeat is on a mission to evolve. In an age where experts and companies can publish directly, it’s not enough to be a secondary source. Our goal is to become a primary source for you.How? By leveraging our relationship with our community of millions of technical leaders. We are increasingly surveying you directly to generate proprietary insights you can’t get anywhere else. We want to be the first to tell you which vector stores your peers are actually implementing, what governance challenges are most pressing for data scientists, or how your counterparts are budgeting for generative AI.This is an ambitious strategy. It requires a tight-knit team where our editorial content, our research surveys and reports, our newsletters, and our VB Transform events are all working from the same playbook.Karyne is the leader who will help us execute that vision. Her experience at Protocol, which was also dedicated to serving technical and business decision-makers, means she fundamentally understands our audience. She is ideally suited to manage our newsroom and ensure that every piece of content we produce helps you do your job better. She’ll be working alongside Carl Franzen, our executive editor, who continues to drive news decision-making.This is a fantastic hire for VentureBeat. It’s another sign of our commitment to building the most focused, expert team in enterprise AI and data.Please join me in welcoming Karyne to the team.",
          "feed_position": 1,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/3M8UYGsdxlIyYdOeQxU2Eo/3703cc8b9b2839a667c762773d65b725/Gemini_Generated_Image_n6phqnn6phqnn6ph.png?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/computing/laptops/acer-predator-triton-14-ai-review-a-true-ultraportable-gaming-laptop-145300067.html",
          "published_at": "Mon, 03 Nov 2025 14:53:00 +0000",
          "title": "Acer Predator Triton 14 AI review: A true ultraportable gaming laptop",
          "standfirst": "When I review products, I try to take other perspectives and use cases into account as much as possible. I'm very aware that I'm not the target audience for every device. But once in a while I run into something that seems like it was designed specifically for me and it just hits different. With the Acer Predator Triton 14 AI, that's pretty much the situation. While it isn't the flashiest or most powerful gaming laptop on the market, it has pretty much everything I look for in a portable system that lets me play games on the go — and then some. Design and display The term ultraportable is typically reserved for more traditional thin-and-light productivity machines, but I think it definitely applies to the Triton 14 AI. At just 3.5 pounds and 0.71 inches thick, Acer's rig is actually a touch lighter and just as thin as a Dell 14 Premium (3.7 pounds and 0.71 inches), despite featuring a much beefier GPU. And even compared to rivals like the Razer Blade 14 (3.6 pounds and 0.64 inches thick), the Triton 14 AI isn't losing much ground there either. Furthermore, while some gaming notebooks go overboard with edgy aesthetics and an abundance of RGB lighting, the Triton 14 AI looks refreshingly understated. Sure, it still has customizable LEDs behind the Predator logo on its lid and per-key lighting on its keyboard. But aside from that, the laptop feels like an exercise in restraint for a category that often favors excess.The other small design flourish is a pixelated Predator logo (that looks like it was made from a tiny dot matrix display) to the right of the touchpad.. I think it's a clever touch that hints at the notebook's gaming focus without hitting you over the head with it. Despite its size, the Triton 14 AI also has excellent connectivity. You get two USB-C ports (one on either side), with Thunderbolt 4 support on the right while the other is used for power and USB 4 data speeds (both can be used for charging). There are also two USB-A 3.2 jacks, 3.5mm audio, a full-size HDMI 2.1 connector and even a microSD card reader. That means you can easily hook it up to an external monitor (which you really ought to have when fragging at home). Alternatively, when you're not gaming, it can be a great mobile editing station because offloading photos and videos from a camera via microSD is a cinch. The Acer Predator Triton 14 AI's right side features a microSD card reader, two USB ports (one Type-C and one Type-A) and a full-size HDMI jack. Sam Rutherford for Engadget Acer didn't cut corners with the Triton 14 AI's display either. Sure, its 120Hz refresh rate could be a touch faster or it could have gone with a slightly higher 3.2K display like on the Dell 14 Premium, but those are real nitpicks. The OLED panel produces rich colors and in my testing, the display on my review unit actually exceeded Acer's stated 340-nit brightness by a few percent. While Acer included six speakers that get plenty loud, my one small gripe is that they aren't located in the best spots to maximize audio quality. There are two drivers hidden behind tiny grilles on each edge of the laptop and four more located on the bottom. This means unless the laptop is sitting on a hard reflective surface like a desk (without something like a desk mat in between), audio often sounds muffled or dampened. It's not a dealbreaker and I understand that the Triton 14 AI's petite dimensions didn't leave much room for up-firing drivers, but I wish Acer had found an arrangement that sounds slightly better. Keyboard, touchpad and an unusual special feature The Acer Predator Triton 14 AI features a keyboard with per-key mini LED lighting and a touchpad with built-in stylus support. Sam Rutherford for Engadget In addition to per-key lighting and a pleasantly bouncy typing experience, Acer added a few extra features to the Triton 14 AI's mouse and keyboard that you don't normally see on gaming laptops. On the left above the function row, there's a physical button that makes it fast and easy to switch between various performance modes with a single press. There's also a dedicated Predator key that acts as a shortcut to Acer's app, where you can do things like tweak settings or adjust the laptop's lighting. Down below, the Triton 14 AI features a large seamless touchpad made from Gorilla Glass, similar to what you get on a Dell 14 Premium. However, to address the issue of you not knowing where the trackpad ends and the rest of the notebook's deck begins, Acer added two light strips on either side. It’s a simple and elegant solution that looks nice too. Not only does the Predator Triton 14 AI's touchpad feature stylus support, Acer included an active pen in the box, so you won't need to buy one separately. Sam Rutherford for Engadget However, the Triton's real party trick is that it also supports stylus input (via MPP 2.0) with 4,096 levels of pressure sensitivity. This means you can use it like a small built-in Wacom tablet. On top of that, the laptop ships with an active pen, so you don't need to shell out extra money for one. And because Windows recognizes the stylus out of the box, there's no extra setup required. So while this isn't something I will use all the time, it's nice to have for times when I feel like taking notes, sketching or just need to sign a document electronically. Performance Our $2,500 review unit features an Intel Core Ultra 9 CPU with 32GB of RAM and a 1TB PCIe Gen 4 SSD along with an NVIDIA RTX 5070 GPU. Notably, this is as big a graphics card as the Triton 14 AI can handle, but considering similarly-sized rivals like the Razer Blade 14 have the same limitation, it's hard to be upset. More importantly, even without the option for an RTX 5080 or 5090, Acer's tiny gaming laptop still boasts respectable performance. The Acer Predator Triton 14 AI features a vivid 14.5-inch OLED panel with a WQXGA+ (2880 x 1800) resolution. Sam Rutherford for Engadget In Cyberpunk 2077 at 1080p and Ultra RT settings, the Triton 14 AI hit 55 fps, which is a notch above the 45 fps I got from the Radeon 8060S in the ROG Z Flow 13. It also means that with just a tiny bit of tweaking, it's easy to push framerates above 60 while keeping almost all of the graphics settings maxed out. Meanwhile, in Returnal at 1080p on Epic, the Triton 14 AI fared even better, hitting 115 fps. That falls short of what I saw on the Alienware 16 Area-51 (154 fps), but considering that's a larger system with an RTX 5080, the difference between the two machines is understandable. As for cooling, Acer went beyond simply using a built-in vapor chamber. Instead of the paste or liquid metal used by the competition, the company says this is the first time a graphene-based thermal interface material has been used inside a gaming laptop. This makes a difference, especially on a notebook this thin, because it means for less demanding games like Teamfight Tactics, if you adjust its performance mode you can actually play them on your lap without worrying about scorching your legs. That said, you still have to watch out because there are two largish fans on the bottom as well, so for more serious titles you'll still want to switch to a table or desk. Battery life The Acer Predator Triton 14 AI's stay relatively cool in normal use thanks to a vapor chamber and a graphene-based thermal interface material. However, under heavy loads, it will still get a bit toasty. Sam Rutherford for Engadget Longevity is often a concern for small, power-hungry gaming laptops like this. But somehow, Acer managed to fit a more than adequate 76Whr battery inside. On PCMark 10's Modern Office rundown test, the Triton 14 AI lasted seven hours and 26 minutes. That's three hours better than larger systems like the Alienware 16 Area-51 (4:13) and half an hour better than smaller rivals like the ASUS ROG Z Flow 13 (6:54). And even though it fell short by an hour when compared to a traditional ultraportable like the Dell 14 Premium (8:30), that's still very solid when you consider the Triton’s more powerful graphics. Wrap-up If you're in the market for a more powerful and sedentary type of gaming laptop that might only get moved around a couple of times a month (if that), the Triton 14 AI might not be for you. But as someone who prefers gaming laptops that are, you know, actually portable, this thing is pretty much my ideal notebook. Even though it's a gaming laptop, the Acer Predator Triton 14 AI's design is refreshingly understated. Sam Rutherford for Engadget For $2,500 as tested, the Predator Triton 14 AI has a vivid OLED display, solid performance, surprisingly good battery life and an incredibly sleek chassis that begs you to take this thing everywhere. It's a bit pricey, but considering a similarly-specced Blade 14 costs $2,700 (before sales or discounts), you might even say it's a bit of a bargain. What puts this thing over the top though, is that Acer could have stopped there and no one would have complained. But then it added extra features like ample ports, powerful cooling and built-in stylus support (not to mention the included pen). In a lot of ways, this isn't just a travel-friendly gaming machine, it's a true do-everything ultraportable. This article originally appeared on Engadget at https://www.engadget.com/computing/laptops/acer-predator-triton-14-ai-review-a-true-ultraportable-gaming-laptop-145300067.html?src=rss",
          "content": "When I review products, I try to take other perspectives and use cases into account as much as possible. I'm very aware that I'm not the target audience for every device. But once in a while I run into something that seems like it was designed specifically for me and it just hits different. With the Acer Predator Triton 14 AI, that's pretty much the situation. While it isn't the flashiest or most powerful gaming laptop on the market, it has pretty much everything I look for in a portable system that lets me play games on the go — and then some. Design and display The term ultraportable is typically reserved for more traditional thin-and-light productivity machines, but I think it definitely applies to the Triton 14 AI. At just 3.5 pounds and 0.71 inches thick, Acer's rig is actually a touch lighter and just as thin as a Dell 14 Premium (3.7 pounds and 0.71 inches), despite featuring a much beefier GPU. And even compared to rivals like the Razer Blade 14 (3.6 pounds and 0.64 inches thick), the Triton 14 AI isn't losing much ground there either. Furthermore, while some gaming notebooks go overboard with edgy aesthetics and an abundance of RGB lighting, the Triton 14 AI looks refreshingly understated. Sure, it still has customizable LEDs behind the Predator logo on its lid and per-key lighting on its keyboard. But aside from that, the laptop feels like an exercise in restraint for a category that often favors excess.The other small design flourish is a pixelated Predator logo (that looks like it was made from a tiny dot matrix display) to the right of the touchpad.. I think it's a clever touch that hints at the notebook's gaming focus without hitting you over the head with it. Despite its size, the Triton 14 AI also has excellent connectivity. You get two USB-C ports (one on either side), with Thunderbolt 4 support on the right while the other is used for power and USB 4 data speeds (both can be used for charging). There are also two USB-A 3.2 jacks, 3.5mm audio, a full-size HDMI 2.1 connector and even a microSD card reader. That means you can easily hook it up to an external monitor (which you really ought to have when fragging at home). Alternatively, when you're not gaming, it can be a great mobile editing station because offloading photos and videos from a camera via microSD is a cinch. The Acer Predator Triton 14 AI's right side features a microSD card reader, two USB ports (one Type-C and one Type-A) and a full-size HDMI jack. Sam Rutherford for Engadget Acer didn't cut corners with the Triton 14 AI's display either. Sure, its 120Hz refresh rate could be a touch faster or it could have gone with a slightly higher 3.2K display like on the Dell 14 Premium, but those are real nitpicks. The OLED panel produces rich colors and in my testing, the display on my review unit actually exceeded Acer's stated 340-nit brightness by a few percent. While Acer included six speakers that get plenty loud, my one small gripe is that they aren't located in the best spots to maximize audio quality. There are two drivers hidden behind tiny grilles on each edge of the laptop and four more located on the bottom. This means unless the laptop is sitting on a hard reflective surface like a desk (without something like a desk mat in between), audio often sounds muffled or dampened. It's not a dealbreaker and I understand that the Triton 14 AI's petite dimensions didn't leave much room for up-firing drivers, but I wish Acer had found an arrangement that sounds slightly better. Keyboard, touchpad and an unusual special feature The Acer Predator Triton 14 AI features a keyboard with per-key mini LED lighting and a touchpad with built-in stylus support. Sam Rutherford for Engadget In addition to per-key lighting and a pleasantly bouncy typing experience, Acer added a few extra features to the Triton 14 AI's mouse and keyboard that you don't normally see on gaming laptops. On the left above the function row, there's a physical button that makes it fast and easy to switch between various performance modes with a single press. There's also a dedicated Predator key that acts as a shortcut to Acer's app, where you can do things like tweak settings or adjust the laptop's lighting. Down below, the Triton 14 AI features a large seamless touchpad made from Gorilla Glass, similar to what you get on a Dell 14 Premium. However, to address the issue of you not knowing where the trackpad ends and the rest of the notebook's deck begins, Acer added two light strips on either side. It’s a simple and elegant solution that looks nice too. Not only does the Predator Triton 14 AI's touchpad feature stylus support, Acer included an active pen in the box, so you won't need to buy one separately. Sam Rutherford for Engadget However, the Triton's real party trick is that it also supports stylus input (via MPP 2.0) with 4,096 levels of pressure sensitivity. This means you can use it like a small built-in Wacom tablet. On top of that, the laptop ships with an active pen, so you don't need to shell out extra money for one. And because Windows recognizes the stylus out of the box, there's no extra setup required. So while this isn't something I will use all the time, it's nice to have for times when I feel like taking notes, sketching or just need to sign a document electronically. Performance Our $2,500 review unit features an Intel Core Ultra 9 CPU with 32GB of RAM and a 1TB PCIe Gen 4 SSD along with an NVIDIA RTX 5070 GPU. Notably, this is as big a graphics card as the Triton 14 AI can handle, but considering similarly-sized rivals like the Razer Blade 14 have the same limitation, it's hard to be upset. More importantly, even without the option for an RTX 5080 or 5090, Acer's tiny gaming laptop still boasts respectable performance. The Acer Predator Triton 14 AI features a vivid 14.5-inch OLED panel with a WQXGA+ (2880 x 1800) resolution. Sam Rutherford for Engadget In Cyberpunk 2077 at 1080p and Ultra RT settings, the Triton 14 AI hit 55 fps, which is a notch above the 45 fps I got from the Radeon 8060S in the ROG Z Flow 13. It also means that with just a tiny bit of tweaking, it's easy to push framerates above 60 while keeping almost all of the graphics settings maxed out. Meanwhile, in Returnal at 1080p on Epic, the Triton 14 AI fared even better, hitting 115 fps. That falls short of what I saw on the Alienware 16 Area-51 (154 fps), but considering that's a larger system with an RTX 5080, the difference between the two machines is understandable. As for cooling, Acer went beyond simply using a built-in vapor chamber. Instead of the paste or liquid metal used by the competition, the company says this is the first time a graphene-based thermal interface material has been used inside a gaming laptop. This makes a difference, especially on a notebook this thin, because it means for less demanding games like Teamfight Tactics, if you adjust its performance mode you can actually play them on your lap without worrying about scorching your legs. That said, you still have to watch out because there are two largish fans on the bottom as well, so for more serious titles you'll still want to switch to a table or desk. Battery life The Acer Predator Triton 14 AI's stay relatively cool in normal use thanks to a vapor chamber and a graphene-based thermal interface material. However, under heavy loads, it will still get a bit toasty. Sam Rutherford for Engadget Longevity is often a concern for small, power-hungry gaming laptops like this. But somehow, Acer managed to fit a more than adequate 76Whr battery inside. On PCMark 10's Modern Office rundown test, the Triton 14 AI lasted seven hours and 26 minutes. That's three hours better than larger systems like the Alienware 16 Area-51 (4:13) and half an hour better than smaller rivals like the ASUS ROG Z Flow 13 (6:54). And even though it fell short by an hour when compared to a traditional ultraportable like the Dell 14 Premium (8:30), that's still very solid when you consider the Triton’s more powerful graphics. Wrap-up If you're in the market for a more powerful and sedentary type of gaming laptop that might only get moved around a couple of times a month (if that), the Triton 14 AI might not be for you. But as someone who prefers gaming laptops that are, you know, actually portable, this thing is pretty much my ideal notebook. Even though it's a gaming laptop, the Acer Predator Triton 14 AI's design is refreshingly understated. Sam Rutherford for Engadget For $2,500 as tested, the Predator Triton 14 AI has a vivid OLED display, solid performance, surprisingly good battery life and an incredibly sleek chassis that begs you to take this thing everywhere. It's a bit pricey, but considering a similarly-specced Blade 14 costs $2,700 (before sales or discounts), you might even say it's a bit of a bargain. What puts this thing over the top though, is that Acer could have stopped there and no one would have complained. But then it added extra features like ample ports, powerful cooling and built-in stylus support (not to mention the included pen). In a lot of ways, this isn't just a travel-friendly gaming machine, it's a true do-everything ultraportable. This article originally appeared on Engadget at https://www.engadget.com/computing/laptops/acer-predator-triton-14-ai-review-a-true-ultraportable-gaming-laptop-145300067.html?src=rss",
          "feed_position": 15,
          "image_url": "https://d29szjachogqwa.cloudfront.net/images/user-uploaded/triton-14-right-side.jpg"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/the-beginning-of-the-end-of-the-transformer-era-neuro-symbolic-ai-startup",
          "published_at": "Mon, 03 Nov 2025 14:00:00 GMT",
          "title": "The beginning of the end of the transformer era? Neuro-symbolic AI startup AUI announces new funding at $750M valuation",
          "standfirst": "The buzzed-about but still stealthy New York City startup Augmented Intelligence Inc (AUI), which seeks to go beyond the popular \"transformer\" architecture used by most of today&#x27;s LLMs such as ChatGPT and Gemini, has raised $20 million in a bridge SAFE round at a $750 million valuation cap, bringing its total funding to nearly $60 million, VentureBeat can exclusively reveal.The round, completed in under a week, comes amid heightened interest in deterministic conversational AI and precedes a larger raise now in advanced stages.AUI relies on a fusion of the transformer tech and a newer technology called \"neuro-symbolic AI,\" described in greater detail below. \"We realize that you can combine the brilliance of LLMs in linguistic capabilities with the guarantees of symbolic AI,\" said Ohad Elhelo, AUI co-founder and CEO in a recent interview with VentureBeat. Elhelo launched the company in 2017 alongside co-founder and Chief Product Officer Ori Cohen.The new financing includes participation from eGateway Ventures, New Era Capital Partners, existing shareholders, and other strategic investors. It follows a $10 million raise in September 2024 at a $350 million valuation cap, coinciding with the company’s announced go-to-market partnership with Google in October 2024. Early investors include Vertex Pharmaceuticals founder Joshua Boger, UKG Chairman Aron Ain, and former IBM President Jim Whitehurst.According to the company, the bridge round is a precursor to a significantly larger raise already in advanced stages.AUI is the company behind Apollo-1, a new foundation model built for task-oriented dialog, which it describes as the \"economic half\" of conversational AI — distinct from the open-ended dialog handled by LLMs like ChatGPT and Gemini. The firm argues that existing LLMs lack the determinism, policy enforcement, and operational certainty required by enterprises, especially in regulated sectors.Chris Varelas, co-founder of Redwood Capital and an advisor to AUI, said in a press release provided to VentureBeat: “I’ve seen some of today’s top AI leaders walk away with their heads spinning after interacting with Apollo-1.”A Distinctive Neuro-Symbolic ArchitectureApollo-1’s core innovation is its neuro-symbolic architecture, which separates linguistic fluency from task reasoning. Instead of using the most common technology underpinning most LLMs and conversational AI systems today — the vaunted transformer architecture described in the seminal 2017 Google paper \"Attention Is All You Need\" — AUI&#x27;s system integrates two layers:Neural modules, powered by LLMs, handle perception: encoding user inputs and generating natural language responses.A symbolic reasoning engine, developed over several years, interprets structured task elements such as intents, entities, and parameters. This symbolic state engine determines the appropriate next actions using deterministic logic.This hybrid architecture allows Apollo-1 to maintain state continuity, enforce organizational policies, and reliably trigger tool or API calls — capabilities that transformer-only agents lack.Elhelo said this design emerged from a multi-year data collection effort: “We built a consumer service and recorded millions of human-agent interactions across 60,000 live agents. From that, we abstracted a symbolic language that defines the structure of task-based dialogs, separate from their domain-specific content.”However, enterprises that have already built systems built around transformer LLMs needn&#x27;t worry. AUI wants to make adopting its new technology just as easy. \"Apollo-1 deploys like any modern foundation model,\" Elhelo told VentureBeat in a text last night. \"It doesn’t require dedicated or proprietary clusters to run. It operates across standard cloud and hybrid environments, leveraging both GPUs and CPUs, and is significantly more cost-efficient to deploy than frontier reasoning models. Apollo-1 can also be deployed across all major clouds in a separated environment for increased security.\"Generalization and Domain FlexibilityApollo-1 is described as a foundation model for task-oriented dialog, meaning it is domain-agnostic and generalizable across verticals like healthcare, travel, insurance, and retail.Unlike consulting-heavy AI platforms that require building bespoke logic per client, Apollo-1 allows enterprises to define behaviors and tools within a shared symbolic language. This approach supports faster onboarding and reduces long-term maintenance. According to the team, an enterprise can launch a working agent in under a day.Crucially, procedural rules are encoded at the symbolic layer — not learned from examples. This enables deterministic execution for sensitive or regulated tasks. For instance, a system can block cancellation of a Basic Economy flight not by guessing intent but by applying hard-coded logic to a symbolic representation of the booking class.As Elhelo explained to VentureBeat, LLMs are \"not a good mechanism when you’re looking for certainty. It’s better if you know what you’re going to send [to an AI model] and always send it, and you know, always, what’s going to come back [to the user] and how to handle that.”Availability and Developer AccessApollo-1 is already in active use within Fortune 500 enterprises in a closed beta, and a broader general availability release is expected before the end of 2025, according to a previous report by The Information, which broke the initial news on the startup.Enterprises can integrate with Apollo-1 either via:A developer playground, where business users and technical teams jointly configure policies, rules, and behaviors; orA standard API, using OpenAI-compatible formats.The model supports policy enforcement, rule-based customization, and steering via guardrails. Symbolic rules allow businesses to dictate fixed behaviors, while LLM modules handle open-text interpretation and user interaction.Enterprise Fit: When Reliability Beats FluencyWhile LLMs have advanced general-purpose dialog and creativity, they remain probabilistic — a barrier to enterprise deployment in finance, healthcare, and customer service. Apollo-1 targets this gap by offering a system where policy adherence and deterministic task completion are first-class design goals.Elhelo puts it plainly: “If your use case is task-oriented dialog, you have to use us, even if you are ChatGPT.”",
          "content": "The buzzed-about but still stealthy New York City startup Augmented Intelligence Inc (AUI), which seeks to go beyond the popular \"transformer\" architecture used by most of today&#x27;s LLMs such as ChatGPT and Gemini, has raised $20 million in a bridge SAFE round at a $750 million valuation cap, bringing its total funding to nearly $60 million, VentureBeat can exclusively reveal.The round, completed in under a week, comes amid heightened interest in deterministic conversational AI and precedes a larger raise now in advanced stages.AUI relies on a fusion of the transformer tech and a newer technology called \"neuro-symbolic AI,\" described in greater detail below. \"We realize that you can combine the brilliance of LLMs in linguistic capabilities with the guarantees of symbolic AI,\" said Ohad Elhelo, AUI co-founder and CEO in a recent interview with VentureBeat. Elhelo launched the company in 2017 alongside co-founder and Chief Product Officer Ori Cohen.The new financing includes participation from eGateway Ventures, New Era Capital Partners, existing shareholders, and other strategic investors. It follows a $10 million raise in September 2024 at a $350 million valuation cap, coinciding with the company’s announced go-to-market partnership with Google in October 2024. Early investors include Vertex Pharmaceuticals founder Joshua Boger, UKG Chairman Aron Ain, and former IBM President Jim Whitehurst.According to the company, the bridge round is a precursor to a significantly larger raise already in advanced stages.AUI is the company behind Apollo-1, a new foundation model built for task-oriented dialog, which it describes as the \"economic half\" of conversational AI — distinct from the open-ended dialog handled by LLMs like ChatGPT and Gemini. The firm argues that existing LLMs lack the determinism, policy enforcement, and operational certainty required by enterprises, especially in regulated sectors.Chris Varelas, co-founder of Redwood Capital and an advisor to AUI, said in a press release provided to VentureBeat: “I’ve seen some of today’s top AI leaders walk away with their heads spinning after interacting with Apollo-1.”A Distinctive Neuro-Symbolic ArchitectureApollo-1’s core innovation is its neuro-symbolic architecture, which separates linguistic fluency from task reasoning. Instead of using the most common technology underpinning most LLMs and conversational AI systems today — the vaunted transformer architecture described in the seminal 2017 Google paper \"Attention Is All You Need\" — AUI&#x27;s system integrates two layers:Neural modules, powered by LLMs, handle perception: encoding user inputs and generating natural language responses.A symbolic reasoning engine, developed over several years, interprets structured task elements such as intents, entities, and parameters. This symbolic state engine determines the appropriate next actions using deterministic logic.This hybrid architecture allows Apollo-1 to maintain state continuity, enforce organizational policies, and reliably trigger tool or API calls — capabilities that transformer-only agents lack.Elhelo said this design emerged from a multi-year data collection effort: “We built a consumer service and recorded millions of human-agent interactions across 60,000 live agents. From that, we abstracted a symbolic language that defines the structure of task-based dialogs, separate from their domain-specific content.”However, enterprises that have already built systems built around transformer LLMs needn&#x27;t worry. AUI wants to make adopting its new technology just as easy. \"Apollo-1 deploys like any modern foundation model,\" Elhelo told VentureBeat in a text last night. \"It doesn’t require dedicated or proprietary clusters to run. It operates across standard cloud and hybrid environments, leveraging both GPUs and CPUs, and is significantly more cost-efficient to deploy than frontier reasoning models. Apollo-1 can also be deployed across all major clouds in a separated environment for increased security.\"Generalization and Domain FlexibilityApollo-1 is described as a foundation model for task-oriented dialog, meaning it is domain-agnostic and generalizable across verticals like healthcare, travel, insurance, and retail.Unlike consulting-heavy AI platforms that require building bespoke logic per client, Apollo-1 allows enterprises to define behaviors and tools within a shared symbolic language. This approach supports faster onboarding and reduces long-term maintenance. According to the team, an enterprise can launch a working agent in under a day.Crucially, procedural rules are encoded at the symbolic layer — not learned from examples. This enables deterministic execution for sensitive or regulated tasks. For instance, a system can block cancellation of a Basic Economy flight not by guessing intent but by applying hard-coded logic to a symbolic representation of the booking class.As Elhelo explained to VentureBeat, LLMs are \"not a good mechanism when you’re looking for certainty. It’s better if you know what you’re going to send [to an AI model] and always send it, and you know, always, what’s going to come back [to the user] and how to handle that.”Availability and Developer AccessApollo-1 is already in active use within Fortune 500 enterprises in a closed beta, and a broader general availability release is expected before the end of 2025, according to a previous report by The Information, which broke the initial news on the startup.Enterprises can integrate with Apollo-1 either via:A developer playground, where business users and technical teams jointly configure policies, rules, and behaviors; orA standard API, using OpenAI-compatible formats.The model supports policy enforcement, rule-based customization, and steering via guardrails. Symbolic rules allow businesses to dictate fixed behaviors, while LLM modules handle open-text interpretation and user interaction.Enterprise Fit: When Reliability Beats FluencyWhile LLMs have advanced general-purpose dialog and creativity, they remain probabilistic — a barrier to enterprise deployment in finance, healthcare, and customer service. Apollo-1 targets this gap by offering a system where policy adherence and deterministic task completion are first-class design goals.Elhelo puts it plainly: “If your use case is task-oriented dialog, you have to use us, even if you are ChatGPT.”",
          "feed_position": 2,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/4igdWans9zhjcEbMEhNvi2/df1b7697d86e37a001cfc65dbe5524ab/cfr0z3n_no_bracelets_or_watches_remove_and_keep_unadorned_--cha_9327ad90-cbcb-46f3-9695-65a002bd7142.png?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/ar-vr/apple-vision-pro-m5-review-a-better-beta-is-still-a-beta-130000284.html",
          "published_at": "Mon, 03 Nov 2025 13:00:00 +0000",
          "title": "Apple Vision Pro M5 review: A better beta is still a beta",
          "standfirst": "Everything new about the revamped Apple Vision Pro can fit in a single sentence: It has a far faster and more efficient M5 chip, it comes with a more comfortable Dual Knit Band and its display looks slightly sharper and faster. Beyond that, the Vision Pro is still basically a $3,500 developer kit that really isn't meant for consumers. Still, the Vision Pro fascinates me. It's a bold swing from a company that's become increasingly risk-averse over the last decade. And now that we have its first refresh, it's clear that Apple isn't ready to give up on the concept of spatial computing yet. For the niche audience of mixed reality developers and hardcore Apple fanatics who haven't already picked up a Vision Pro, the new model is more compelling than the original with its aging M2 chip. If you've already got one, though, you can just pick up the Dual Knit Band for $99 to get a more comfortable fit. What's the point of the M5 Apple Vision Pro? While the Vision Pro launched with a huge splash in 2023, it was released early last year with the 2022-era M2 chip. Now that we're three generations beyond that Apple silicon, it's high time it received an upgrade. With the M5 chip, the Vision Pro is up to 50 percent faster at rendering your Persona avatar and creating spatial scenes from photos, according to Apple. Both of those experiences were noticeably faster during my testing, but they also never felt too sluggish on the original Vision Pro. As I mentioned above, having new hardware is a sign that Apple hasn't completely forgotten about the Vision Pro. It's not being almost immediately ignored like the original HomePod. Instead, this new model aims to fix some of the biggest annoyances from the first. The Dual Knit Band alone makes the Vision Pro feel more comfortable, since it relies on a rear and top strap to balance the device on your head. The original Solo Knit Band only had a rear strap, which clamped the Vision Pro on your head and left much of its weight resting on your forehead and nose. While the original Vision Pro also included a Dual Loop Band in the box, that was rarely featured in the marketing for the Vision Pro, likely because it made the device appear to be more like a traditional VR headset. The new Dual Knit Band almost feels like an apology for Apple's previous bands — it's as if the company is admitting that it was more concerned with how the Vision Pro appeared in ads, instead of choosing a more comfortable default head strap. In addition to making the headset feel easier to wear, the Dual Knit Band is also ingeniously simple to adjust. Twisting its small tightening knob adjusts the horizontal straps, and then you just have to pop that knob out to customize the over-head straps. That's a major upgrade over most VR headsets, which typically rely on velcro to tighten straps over your head. Apple Vision Pro M5 adjustment knob. Devindra Hardawar for Engadget Aside from the Dual Knit Band, the Vision Pro sports the exact same design as the original model, so I'd recommend reading my initial review for more hardware specifics. \"In typical Apple fashion, the Vision Pro looks far more handsome than any VR headset I've seen,\" I wrote last year. \"That mostly comes down to materials: Whereas the competition is almost entirely made of plastic cases, Apple's device is built out of smooth glass, polished metal and designer fabrics.\" While the new Vision Pro is using the same micro OLED displays as before, the headset can render 10 percent more pixels thanks to the M5 chip. I couldn't really see a difference when I swapped between the two headsets, but that's to be expected with a small resolution bump. What's more important is that the M5 Vision Pro still has some of the most impressive screens I've ever seen. It can scale up 4K video into enormous 300-inch windows while still looking sharp, and it's easy to read text in the browser, or while working on a virtual Mac display. MacBook Mirroring on Apple Vision Pro M5 Devindra Hardawar for Engadget The M5 chip also allows the Vision Pro to reach up to a 120Hz refresh rate, instead of being limited to refresh rates between 90Hz and 100Hz. Again, I didn't see a huge difference with the new model, but theoretically the higher refresh rate should allow for smoother performance while scrolling through windows and documents. It also means the Vision Pro can run games at up to 120 fps, which could be helpful if you're trying to play Overwatch over GeForce Now streaming. In addition to being more powerful than the M2, the M5 chip is also more efficient. I was able to use the new Vision Pro for more than two and a half hours while swapping between videos, visionOS apps and Macbook mirroring. The same workflow typically drained the original model's battery in around two hours. Apple Vision Pro M5 Devindra Hardawar for Engadget How has the Vision Pro ecosystem changed over the last year? It's not too often Apple has to build an entirely new operating system with fresh input mechanisms, but that's basically what we got with visionOS. Its interface hovers in front of you, like a holographic iPad home screen. And instead of a keyboard and mouse, you interact with it mainly using finger gestures and eye tracking. I found visionOS to be surprisingly easy to use on the original Vision Pro -- flicking through floating windows quickly made me feel like Tom Cruise in Minority Report -- and it's only gotten more refined over time. For one, Apple added Spatial Personas, which are virtual avatars that can float around your space during FaceTime calls with other Vision Pro users. That feature made the headset feel like a \"telepresence dream\" when I first tested it out, and it's only gotten better with visionOS 26, which has more realistic Spatial Personas. During several group FaceTime calls, I felt like I was sitting beside people in the real world, even though I was just looking at rendered faces, shoulders and hands floating in the air. The sense of true presence was uncanny: Spatial Personas can walk freely around your room, and with the flick of a button you can also collaborate together on documents, view 3D models or watch videos together in virtual space. Apple Vision Pro M5 viewed from the side. Devindra Hardawar for Engadget Apple's Immersive Videos — 8K 3D 180-degree footage shot using its custom cameras — were one of the highlights of the original Vision Pro, and they still look great on the new model. I was most impressed by \"Hill Climb,\" an episode of the Adventure series focused on Laura Hayes, a driver attempting to make a new record racing to the top of Pikes Peak. Expansive overhead shots (which feel incredibly life-like in 3D) did a fine job of showing the scale of her drive, and footage from beside and inside her car delivered a thrilling sense of speed. All of the Immersive Videos I've seen are miles ahead of the blurry, low-res 360-degree VR footage we've been seeing for years. Apple's 8K 3D content is more focused on trying to recreate reality right in front of you. Looking ahead, the company also plans to broadcast live NBA games in Immersive Video and more Vision Pro content is coming from Red Bull, CNN, BBC and others. Speaking of immersive content, Apple also added support for the PS VR2 Sense controllers in visionOS 26, which gives Vision Pro the ability to support true VR experiences. When I tried the What If?... Vision Pro experience last year, it was clear that hand gestures weren’t precise enough to handle VR gaming. I’ve only been able to try the PS VR2 controllers in the pickleball game Pickle Pro, but they were instantly impressive, allowing me to realistically angle and swing my paddle. Apple Vision Pro M5 lenses. Devindra Hardawar for Engadget Wrap-up: Still very much a beta I’m astounded by the Vision Pro every time I put it on. The displays look fantastic, and they’re versatile enough to handle everything from watching movies, immersing myself in 3D content and diving into productivity work by mirroring my MacBook Pro. But, when I take off the headset, reality sets in. It’s still wildly expensive at $3,499, and there isn’t nearly enough spatial computing content to make that price worth it. Once again, the Vision Pro feels like a proof of concept — a symbol of what Apple can do when it’s not constrained by traditional screens. But the company’s dream of spatial computing won’t go anywhere until it can deliver cheaper devices. As I’ve argued, Apple should just take a cue from Xreal and shove visionOS into a pair of display glasses. That would allow the company to produce a much more accessible device, and it would also put Apple in a better position to compete with Android XR hardware like Samsung’s $1,800 Galaxy XR. Until Apple can open up visionOS to more users, it will still be just beta testing the future. This article originally appeared on Engadget at https://www.engadget.com/ar-vr/apple-vision-pro-m5-review-a-better-beta-is-still-a-beta-130000284.html?src=rss",
          "content": "Everything new about the revamped Apple Vision Pro can fit in a single sentence: It has a far faster and more efficient M5 chip, it comes with a more comfortable Dual Knit Band and its display looks slightly sharper and faster. Beyond that, the Vision Pro is still basically a $3,500 developer kit that really isn't meant for consumers. Still, the Vision Pro fascinates me. It's a bold swing from a company that's become increasingly risk-averse over the last decade. And now that we have its first refresh, it's clear that Apple isn't ready to give up on the concept of spatial computing yet. For the niche audience of mixed reality developers and hardcore Apple fanatics who haven't already picked up a Vision Pro, the new model is more compelling than the original with its aging M2 chip. If you've already got one, though, you can just pick up the Dual Knit Band for $99 to get a more comfortable fit. What's the point of the M5 Apple Vision Pro? While the Vision Pro launched with a huge splash in 2023, it was released early last year with the 2022-era M2 chip. Now that we're three generations beyond that Apple silicon, it's high time it received an upgrade. With the M5 chip, the Vision Pro is up to 50 percent faster at rendering your Persona avatar and creating spatial scenes from photos, according to Apple. Both of those experiences were noticeably faster during my testing, but they also never felt too sluggish on the original Vision Pro. As I mentioned above, having new hardware is a sign that Apple hasn't completely forgotten about the Vision Pro. It's not being almost immediately ignored like the original HomePod. Instead, this new model aims to fix some of the biggest annoyances from the first. The Dual Knit Band alone makes the Vision Pro feel more comfortable, since it relies on a rear and top strap to balance the device on your head. The original Solo Knit Band only had a rear strap, which clamped the Vision Pro on your head and left much of its weight resting on your forehead and nose. While the original Vision Pro also included a Dual Loop Band in the box, that was rarely featured in the marketing for the Vision Pro, likely because it made the device appear to be more like a traditional VR headset. The new Dual Knit Band almost feels like an apology for Apple's previous bands — it's as if the company is admitting that it was more concerned with how the Vision Pro appeared in ads, instead of choosing a more comfortable default head strap. In addition to making the headset feel easier to wear, the Dual Knit Band is also ingeniously simple to adjust. Twisting its small tightening knob adjusts the horizontal straps, and then you just have to pop that knob out to customize the over-head straps. That's a major upgrade over most VR headsets, which typically rely on velcro to tighten straps over your head. Apple Vision Pro M5 adjustment knob. Devindra Hardawar for Engadget Aside from the Dual Knit Band, the Vision Pro sports the exact same design as the original model, so I'd recommend reading my initial review for more hardware specifics. \"In typical Apple fashion, the Vision Pro looks far more handsome than any VR headset I've seen,\" I wrote last year. \"That mostly comes down to materials: Whereas the competition is almost entirely made of plastic cases, Apple's device is built out of smooth glass, polished metal and designer fabrics.\" While the new Vision Pro is using the same micro OLED displays as before, the headset can render 10 percent more pixels thanks to the M5 chip. I couldn't really see a difference when I swapped between the two headsets, but that's to be expected with a small resolution bump. What's more important is that the M5 Vision Pro still has some of the most impressive screens I've ever seen. It can scale up 4K video into enormous 300-inch windows while still looking sharp, and it's easy to read text in the browser, or while working on a virtual Mac display. MacBook Mirroring on Apple Vision Pro M5 Devindra Hardawar for Engadget The M5 chip also allows the Vision Pro to reach up to a 120Hz refresh rate, instead of being limited to refresh rates between 90Hz and 100Hz. Again, I didn't see a huge difference with the new model, but theoretically the higher refresh rate should allow for smoother performance while scrolling through windows and documents. It also means the Vision Pro can run games at up to 120 fps, which could be helpful if you're trying to play Overwatch over GeForce Now streaming. In addition to being more powerful than the M2, the M5 chip is also more efficient. I was able to use the new Vision Pro for more than two and a half hours while swapping between videos, visionOS apps and Macbook mirroring. The same workflow typically drained the original model's battery in around two hours. Apple Vision Pro M5 Devindra Hardawar for Engadget How has the Vision Pro ecosystem changed over the last year? It's not too often Apple has to build an entirely new operating system with fresh input mechanisms, but that's basically what we got with visionOS. Its interface hovers in front of you, like a holographic iPad home screen. And instead of a keyboard and mouse, you interact with it mainly using finger gestures and eye tracking. I found visionOS to be surprisingly easy to use on the original Vision Pro -- flicking through floating windows quickly made me feel like Tom Cruise in Minority Report -- and it's only gotten more refined over time. For one, Apple added Spatial Personas, which are virtual avatars that can float around your space during FaceTime calls with other Vision Pro users. That feature made the headset feel like a \"telepresence dream\" when I first tested it out, and it's only gotten better with visionOS 26, which has more realistic Spatial Personas. During several group FaceTime calls, I felt like I was sitting beside people in the real world, even though I was just looking at rendered faces, shoulders and hands floating in the air. The sense of true presence was uncanny: Spatial Personas can walk freely around your room, and with the flick of a button you can also collaborate together on documents, view 3D models or watch videos together in virtual space. Apple Vision Pro M5 viewed from the side. Devindra Hardawar for Engadget Apple's Immersive Videos — 8K 3D 180-degree footage shot using its custom cameras — were one of the highlights of the original Vision Pro, and they still look great on the new model. I was most impressed by \"Hill Climb,\" an episode of the Adventure series focused on Laura Hayes, a driver attempting to make a new record racing to the top of Pikes Peak. Expansive overhead shots (which feel incredibly life-like in 3D) did a fine job of showing the scale of her drive, and footage from beside and inside her car delivered a thrilling sense of speed. All of the Immersive Videos I've seen are miles ahead of the blurry, low-res 360-degree VR footage we've been seeing for years. Apple's 8K 3D content is more focused on trying to recreate reality right in front of you. Looking ahead, the company also plans to broadcast live NBA games in Immersive Video and more Vision Pro content is coming from Red Bull, CNN, BBC and others. Speaking of immersive content, Apple also added support for the PS VR2 Sense controllers in visionOS 26, which gives Vision Pro the ability to support true VR experiences. When I tried the What If?... Vision Pro experience last year, it was clear that hand gestures weren’t precise enough to handle VR gaming. I’ve only been able to try the PS VR2 controllers in the pickleball game Pickle Pro, but they were instantly impressive, allowing me to realistically angle and swing my paddle. Apple Vision Pro M5 lenses. Devindra Hardawar for Engadget Wrap-up: Still very much a beta I’m astounded by the Vision Pro every time I put it on. The displays look fantastic, and they’re versatile enough to handle everything from watching movies, immersing myself in 3D content and diving into productivity work by mirroring my MacBook Pro. But, when I take off the headset, reality sets in. It’s still wildly expensive at $3,499, and there isn’t nearly enough spatial computing content to make that price worth it. Once again, the Vision Pro feels like a proof of concept — a symbol of what Apple can do when it’s not constrained by traditional screens. But the company’s dream of spatial computing won’t go anywhere until it can deliver cheaper devices. As I’ve argued, Apple should just take a cue from Xreal and shove visionOS into a pair of display glasses. That would allow the company to produce a much more accessible device, and it would also put Apple in a better position to compete with Android XR hardware like Samsung’s $1,800 Galaxy XR. Until Apple can open up visionOS to more users, it will still be just beta testing the future. This article originally appeared on Engadget at https://www.engadget.com/ar-vr/apple-vision-pro-m5-review-a-better-beta-is-still-a-beta-130000284.html?src=rss",
          "feed_position": 20,
          "image_url": "https://d29szjachogqwa.cloudfront.net/images/user-uploaded/Vision_Pro_M5-3.jpg"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/meet-denario-the-ai-research-assistant-that-is-already-getting-its-own",
          "published_at": "Mon, 03 Nov 2025 09:40:00 GMT",
          "title": "Meet Denario, the AI ‘research assistant’ that is already getting its own papers published",
          "standfirst": "An international team of researchers has released an artificial intelligence system capable of autonomously conducting scientific research across multiple disciplines — generating papers from initial concept to publication-ready manuscript in approximately 30 minutes for about $4 each.The system, called Denario, can formulate research ideas, review existing literature, develop methodologies, write and execute code, create visualizations, and draft complete academic papers. In a demonstration of its versatility, the team used Denario to generate papers spanning astrophysics, biology, chemistry, medicine, neuroscience, and other fields, with one AI-generated paper already accepted for publication at an academic conference.\"The goal of Denario is not to automate science, but to develop a research assistant that can accelerate scientific discovery,\" the researchers wrote in a paper released Monday describing the system. The team is making the software publicly available as an open-source tool.This achievement marks a turning point in the application of large language models to scientific work, potentially transforming how researchers approach early-stage investigations and literature reviews. However, the research also highlights substantial limitations and raises pressing questions about validation, authorship, and the changing nature of scientific labor.From data to draft: how AI agents collaborate to conduct researchAt its core, Denario operates not as a single AI brain but as a digital research department where specialized AI agents collaborate to push a project from conception to completion. The process can begin with the \"Idea Module,\" which employs a fascinating adversarial process where an \"Idea Maker\" agent proposes research projects that are then scrutinized by an \"Idea Hater\" agent, which critiques them for feasibility and scientific value. This iterative loop refines raw concepts into robust research directions.Once a hypothesis is solidified, a \"Literature Module\" scours academic databases like Semantic Scholar to check the idea&#x27;s novelty, followed by a \"Methodology Module\" that lays out a detailed, step-by-step research plan. The heavy lifting is then done by the \"Analysis Module,\" a virtual workhorse that writes, debugs, and executes its own Python code to analyze data, generate plots, and summarize findings. Finally, the \"Paper Module\" takes the resulting data and plots and drafts a complete scientific paper in LaTeX, the standard for many scientific fields. In a final, recursive step, a \"Review Module\" can even act as an AI peer-reviewer, providing a critical report on the generated paper&#x27;s strengths and weaknesses.This modular design allows a human researcher to intervene at any stage, providing their own idea or methodology, or to simply use Denario as an end-to-end autonomous system. \"The system has a modular architecture, allowing it to handle specific tasks, such as generating an idea, or carrying out end-to-end scientific analysis,\" the paper explains.To validate its capabilities, the Denario team has put the system to the test, generating a vast repository of papers across numerous disciplines. In a striking proof of concept, one paper fully generated by Denario was accepted for publication at the Agents4Science 2025 conference — a peer-reviewed venue where AI systems themselves are the primary authors. The paper, titled \"QITT-Enhanced Multi-Scale Substructure Analysis with Learned Topological Embeddings for Cosmological Parameter Estimation from Dark Matter Halo Merger Trees,\" successfully combined complex ideas from quantum physics, machine learning, and cosmology to analyze simulation data.The ghost in the machine: AI’s ‘vacuous’ results and ethical alarmsWhile the successes are notable, the research paper is refreshingly candid about Denario&#x27;s significant limitations and failure modes. The authors stress that the system currently \"behaves more like a good undergraduate or early graduate student rather than a full professor in terms of big picture, connecting results...etc.\" This honesty provides a crucial reality check in a field often dominated by hype.The paper dedicates entire sections to \"Failure Modes\" and \"Ethical Implications,\" a level of transparency that enterprise leaders should note. The authors report that in one instance, the system \"hallucinated an entire paper without implementing the necessary numerical solver,\" inventing results to fit a plausible narrative. In another test on a pure mathematics problem, the AI produced text that had the form of a mathematical proof but was, in the authors&#x27; words, \"mathematically vacuous.\"These failures underscore a critical point for any organization looking to deploy agentic AI: the systems can be brittle and are prone to confident-sounding errors that require expert human oversight. The Denario paper serves as a vital case study in the importance of keeping a human in the loop for validation and critical assessment.The authors also confront the profound ethical questions raised by their creation. They warn that \"AI agents could be used to quickly flood the scientific literature with claims driven by a particular political agenda or specific commercial or economic interests.\" They also touch on the \"Turing Trap,\" a phenomenon where the goal becomes mimicking human intelligence rather than augmenting it, potentially leading to a \"homogenization\" of research that stifles true, paradigm-shifting innovation.An open-source co-pilot for the world&#x27;s labsDenario is not just a theoretical exercise locked away in an academic lab. The entire system is open-source under a GPL-3.0 license and is accessible to the broader community. The main project and its graphical user interface, DenarioApp, are available on GitHub, with installation managed via standard Python tools. For enterprise environments focused on reproducibility and scalability, the project also provides official Docker images. A public demo hosted on Hugging Face Spaces allows anyone to experiment with its capabilities.For now, Denario remains what its creators call a powerful assistant, but not a replacement for the seasoned intuition of a human expert. This framing is deliberate. The Denario project is less about creating an automated scientist and more about building the ultimate co-pilot, one designed to handle the tedious and time-consuming aspects of modern research.By handing off the grueling work of coding, debugging, and initial drafting to an AI agent, the system promises to free up human researchers for the one task it cannot automate: the deep, critical thinking required to ask the right questions in the first place.",
          "content": "An international team of researchers has released an artificial intelligence system capable of autonomously conducting scientific research across multiple disciplines — generating papers from initial concept to publication-ready manuscript in approximately 30 minutes for about $4 each.The system, called Denario, can formulate research ideas, review existing literature, develop methodologies, write and execute code, create visualizations, and draft complete academic papers. In a demonstration of its versatility, the team used Denario to generate papers spanning astrophysics, biology, chemistry, medicine, neuroscience, and other fields, with one AI-generated paper already accepted for publication at an academic conference.\"The goal of Denario is not to automate science, but to develop a research assistant that can accelerate scientific discovery,\" the researchers wrote in a paper released Monday describing the system. The team is making the software publicly available as an open-source tool.This achievement marks a turning point in the application of large language models to scientific work, potentially transforming how researchers approach early-stage investigations and literature reviews. However, the research also highlights substantial limitations and raises pressing questions about validation, authorship, and the changing nature of scientific labor.From data to draft: how AI agents collaborate to conduct researchAt its core, Denario operates not as a single AI brain but as a digital research department where specialized AI agents collaborate to push a project from conception to completion. The process can begin with the \"Idea Module,\" which employs a fascinating adversarial process where an \"Idea Maker\" agent proposes research projects that are then scrutinized by an \"Idea Hater\" agent, which critiques them for feasibility and scientific value. This iterative loop refines raw concepts into robust research directions.Once a hypothesis is solidified, a \"Literature Module\" scours academic databases like Semantic Scholar to check the idea&#x27;s novelty, followed by a \"Methodology Module\" that lays out a detailed, step-by-step research plan. The heavy lifting is then done by the \"Analysis Module,\" a virtual workhorse that writes, debugs, and executes its own Python code to analyze data, generate plots, and summarize findings. Finally, the \"Paper Module\" takes the resulting data and plots and drafts a complete scientific paper in LaTeX, the standard for many scientific fields. In a final, recursive step, a \"Review Module\" can even act as an AI peer-reviewer, providing a critical report on the generated paper&#x27;s strengths and weaknesses.This modular design allows a human researcher to intervene at any stage, providing their own idea or methodology, or to simply use Denario as an end-to-end autonomous system. \"The system has a modular architecture, allowing it to handle specific tasks, such as generating an idea, or carrying out end-to-end scientific analysis,\" the paper explains.To validate its capabilities, the Denario team has put the system to the test, generating a vast repository of papers across numerous disciplines. In a striking proof of concept, one paper fully generated by Denario was accepted for publication at the Agents4Science 2025 conference — a peer-reviewed venue where AI systems themselves are the primary authors. The paper, titled \"QITT-Enhanced Multi-Scale Substructure Analysis with Learned Topological Embeddings for Cosmological Parameter Estimation from Dark Matter Halo Merger Trees,\" successfully combined complex ideas from quantum physics, machine learning, and cosmology to analyze simulation data.The ghost in the machine: AI’s ‘vacuous’ results and ethical alarmsWhile the successes are notable, the research paper is refreshingly candid about Denario&#x27;s significant limitations and failure modes. The authors stress that the system currently \"behaves more like a good undergraduate or early graduate student rather than a full professor in terms of big picture, connecting results...etc.\" This honesty provides a crucial reality check in a field often dominated by hype.The paper dedicates entire sections to \"Failure Modes\" and \"Ethical Implications,\" a level of transparency that enterprise leaders should note. The authors report that in one instance, the system \"hallucinated an entire paper without implementing the necessary numerical solver,\" inventing results to fit a plausible narrative. In another test on a pure mathematics problem, the AI produced text that had the form of a mathematical proof but was, in the authors&#x27; words, \"mathematically vacuous.\"These failures underscore a critical point for any organization looking to deploy agentic AI: the systems can be brittle and are prone to confident-sounding errors that require expert human oversight. The Denario paper serves as a vital case study in the importance of keeping a human in the loop for validation and critical assessment.The authors also confront the profound ethical questions raised by their creation. They warn that \"AI agents could be used to quickly flood the scientific literature with claims driven by a particular political agenda or specific commercial or economic interests.\" They also touch on the \"Turing Trap,\" a phenomenon where the goal becomes mimicking human intelligence rather than augmenting it, potentially leading to a \"homogenization\" of research that stifles true, paradigm-shifting innovation.An open-source co-pilot for the world&#x27;s labsDenario is not just a theoretical exercise locked away in an academic lab. The entire system is open-source under a GPL-3.0 license and is accessible to the broader community. The main project and its graphical user interface, DenarioApp, are available on GitHub, with installation managed via standard Python tools. For enterprise environments focused on reproducibility and scalability, the project also provides official Docker images. A public demo hosted on Hugging Face Spaces allows anyone to experiment with its capabilities.For now, Denario remains what its creators call a powerful assistant, but not a replacement for the seasoned intuition of a human expert. This framing is deliberate. The Denario project is less about creating an automated scientist and more about building the ultimate co-pilot, one designed to handle the tedious and time-consuming aspects of modern research.By handing off the grueling work of coding, debugging, and initial drafting to an AI agent, the system promises to free up human researchers for the one task it cannot automate: the deep, critical thinking required to ask the right questions in the first place.",
          "feed_position": 3,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/3vIbj1xm8lwmfZQ7qEASzr/9f0690cb8087927276c290678be0e26c/nuneybits_Vector_art_of_data_flowing_into_a_book_c0036116-c0b9-48bb-9dbf-7986efe147d1.webp?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/gaming/nintendo/best-microsd-cards-for-nintendo-switch-2-160052947.html",
          "published_at": "Mon, 03 Nov 2025 08:00:38 +0000",
          "title": "The best microSD cards for the Nintendo Switch 2",
          "standfirst": "The Nintendo Switch 2 is here, and that means lots of people will have to buy a new microSD card. While the device comes with 256GB of storage built in, some new games chew up a ton of space. Cyberpunk 2077 is a 60GB download, for instance, while Split Fiction checks in at 69GB. Other titles aren’t nearly as big, but it’s easy to see how you could wind up deleting and redownloading games within a couple months.When you do want to add room, you’ll need a microSD Express card. The “Express” part is important: These are not the same as the old reliable microSD cards you may have bought for the original Switch or other gaming handhelds — they’re newer, faster and significantly more expensive. They’re also your only choice. If you’re looking for the best microSD card for the Switch 2 today, we’ve broken down what you should know before you buy. The best microSD cards for the Switch 2 (aren't all that important) Jeff Dunn for Engadget The Switch 2 is the first mainstream device to require microSD Express for storage expansion, so there aren’t many options available to buy just yet. Of the handful of compatible models released thus far, we’ve tested six: the SanDisk microSD Express Card (aka the “SanDisk GamePlay microSD Express Card” at Walmart), the Lexar Play Pro, the Samsung microSD Card for Nintendo Switch 2, the PNY microSD Express Card, the GameStop Express microSD Card for Nintendo Switch 2 and the Onn microSD Express Card. The first four are made by genuine storage manufacturers, while the GameStop and Onn cards appear to be rebadged versions of other models. We used the 256GB version of every card except for Lexar Play Pro, which was 1TB. After timing these microSD Express cards across a range of Switch 2 games, our advice is simple: Get whichever one is in stock for the lowest price in the capacity you want. They aren’t identically fast, especially if you want to move a game to the card from the console’s internal storage (or vice versa). But the differences in load times and overall performance within actual games are tough to notice unless you have a stopwatch handy. All five cards loaded up the digital version of Mario Kart World, for instance, between 18 and 20 seconds. Each loaded the first Grand Prix race in about 6.5 seconds. Getting to the start screen of Cyberpunk 2077 took about 38 or 39 seconds in each case. Loading a save in a particularly asset-heavy area (Jig-Jig Street) then took between 26 and 29 seconds, depending on the card. (The one exception was with the Onn card, which averaged closer to 31 seconds with that particular task, though that’s also the cheapest choice.) With Fast Fusion, a smaller native Switch 2 game, the initial load always took six to seven seconds, while each card loaded the first championship race in roughly 4 seconds. It was a similar situation with the Switch 2 upgrade for The Legend of Zelda: Breath of the Wild (using a Switch 1 cartridge): Each card took just over six seconds to get to the start screen, between 19 and 20 seconds to load a save just before the final boss, about 16 seconds to fast travel between Kakariko Village and Korok Forest, and so on. We saw no significant issues with in-game loads when playing each game, either. The SanDisk microSD Express Card and Lexar Play Pro. Jeff Dunn for Engadget All of this suggests that the Switch 2 has a relatively specific target for these cards to hit, and that there may not be much room for one model to leap too far out in front of the others. We’ll also note that the console’s built-in storage was consistently faster than any external option: The gap wasn’t always big, but no card truly outpaced it in any of our tests. Loading that demanding area in Cyberpunk, for example, took about 22.5 seconds on average. So if you want the absolute fastest load times, don’t put your game on a card at all. If you need the mental comfort of knowing you technically have the best card available, get the SanDisk microSD Express Card. It had no outliers across our many game loading tests, and it was consistently right near the top when it came to moving games to and from system storage, which means it offers strong sequential read and write performance. Benchmark testing on PC with tools like CrystalDiskMark backed this up, as noted in our broader microSD card buying guide. Putting Mario Kart (a 21.9GB file) on that card took four minutes and 39 seconds on average, which was only second to the Lexar Play Pro by six seconds. It was the fastest to write Fast Fusion (3.5GB), taking an average of 27 seconds across three runs. Only PNY’s card was faster to move games back to the console’s storage, but that one was far slower at writing games to the card — getting Mario Kart on there took seven minutes and 11 seconds on average. Just note that the 128GB version of SanDisk’s card has slower sequential writes than the larger versions, including much slower sustained write speeds (100 MB/s vs 210-220 MB/s). So transferring a game to that particular model will take much longer. The Onn card was also slower to move games back to system storage, taking about 50 seconds more than the SanDisk with Mario Kart and nearly three minutes longer with Cyberpunk Practically speaking, though, speed differences aren’t as important in this case as having lots of space to hold games at a price you can live with. To make things easy, we’ve listed every Express card we’ve seen at retailers at the time of writing below. Remember: You want microSD Express, not “Extreme,” like the branding SanDisk uses for some conventional microSD cards. A microSD Express card will have a big “EX” logo printed on it. 128GB SanDisk microSD Express Card ($64 MSRP) PNY microSD Express Card ($45 MSRP) 256GB Samsung microSD Express Card for Nintendo Switch 2 ($60 MSRP) SanDisk microSD Express Card ($70 MSRP) Lexar Play Pro ($60 MSRP) PNY microSD Express Card ($62 MSRP) GameStop Express microSD Card for Nintendo Switch 2 ($60 MSRP) Onn microSD Express Card ($47 MSRP) 512GB SanDisk microSD Express Card ($120 MSRP) Lexar Play Pro ($120 MSRP) PNY microSD Express Card ($120 MSRP) GameStop Express microSD Card for Nintendo Switch 2 ($100 MSRP) Onn microSD Express Card ($85 MSRP) 1TB Lexar Play Pro ($220 MSRP) GameStop Express microSD Card for Nintendo Switch 2 ($190 MSRP) All microSD Express cards will have this \"EX\" logo printed on them. Nintendo/Engadget As you can see, while the SanDisk card is fast, it’s also the most expensive of an already-pricey bunch. Is it worth an extra $10-20 to shave a couple seconds off certain loads in certain games, or a couple minutes when moving a game to external storage? Probably not for most people. But stock for all of these cards has been a bit patchy since the Switch 2 landed, especially for the Walmart Onn model, which is by far the cheapest choice. If only one card is actually available by the time you read this — and you must have it today — it’s safe to just get it. You won’t lose or gain all that much when it comes to real-world performance. Ultimately, though, we still advise holding off on buying a microSD Express card for as long as you can. President Trump’s ongoing tariff shenanigans could spike prices a little higher in the short term, but we've only just started to see a few small discounts in recent weeks, and in general, all of these cards should only fall further over time. And compared to traditional microSD options, they are still pricey: The Samsung Pro Plus, for example, costs $17 for 128GB, $27 for 256GB, $50 for 512GB and $95 for 1TB as of this writing. The Switch 2 is extremely popular, so more microSD Express cards will need to be made and prices will (eventually) come down. Ideally, we’ll see more high-capacity options as well: Nintendo says the Switch 2 technically supports cards up to 2TB, but so far only a couple even go up to 1TB. All of this means you should try to use all 256GB baked into the Switch 2 first, even if it means having to delete a game or two. But if you absolutely need more space right away, the cards above should be fine. What are microSD Express cards? A microSD Express card like the one on the right has a second row of pins on the back. Jeff Dunn for Engadget Most microSD cards are based on a standard called Ultra High Speed (UHS), of which there are three versions: UHS-I, UHS-II and UHS-III. The vast majority of cards you may have bought in the past utilize UHS-I. These have one row of pins in the back and a theoretical maximum data transfer speed of 104 megabytes per second (MB/s). (Though many cards are able to surpass that limit with proprietary tech and card readers.) The original Switch has a UHS-I microSD slot, as do most other gaming handhelds like Valve’s Steam Deck. UHS-II cards add a second row of pins and can reach up to 312 MB/s. These are pricier and much less common than cards based on UHS-I, but they’re supported by some cameras and higher-power handhelds like the ASUS ROG Xbox Ally X. UHS-III, meanwhile, is twice as fast as UHS-II in theory (624 MB/s), but no microSD cards have actually used it. UHS-I cards have held on over the years because they’re cheap, widely supported and fast enough for the things most people need them to do: record 4K video, stash photos and so on. But with the Switch 2, Nintendo needs more. The new console is dramatically more powerful, which allows it to run demanding games that may have originally been built for stronger hardware like the PlayStation 5, Xbox Series X or gaming PCs. The device also uses UFS 3.1 storage internally, which is much speedier than the eMMC storage used by the original Switch. (A custom file decompression engine helps improve load times as well.) So if the Switch 2 is going to accept microSD cards, it needs ones that won’t bring a serious drop-off in performance and can hold up with modern games. The Nintendo Switch 2. Sam Rutherford for Engadget Hence, SD Express. This standard has technically been around since 2018 but mostly went nowhere until the Switch 2 came along. It also uses a second row of pins, but it lets microSD cards take advantage of the PCI Express (PCIe)/NVMe interface, which is the same underlying tech used by modern SSDs. As a result, it can produce considerably faster read and write speeds, with a current theoretical maximum of 985 MB/s. As noted above, real-world performance won’t be quite that fast. Even if it was, the best microSD Express cards would still be much slower than the NVMe SSDs used by the PS5 and Xbox. (Sony recommends SSDs with sequential read speeds of at least 5,500 MB/s.) And they’ll fall well below their peak speeds under sustained loads: SanDisk, for instance, says sustained write speeds for its 128GB Express card can drop as low as 100 MB/s. But they’re still a marked improvement over old UHS-I cards, and in theory, they should be quicker than some older SATA-based SSDs when it comes loading game levels, asset streaming, retrieving saves or copying games to external storage. Whereas SanDisk’s microSD Express card can produce sequential read speeds around 900 MB/s, Lexar’s Professional Silver Plus — the top UHS-I pick in our general microSD card guide — topped out just over 200 MB/s, and that’s with a proprietary reader. (On the first Switch, it’d be closer to 100 MB/s.) Sequential writes and random speeds were three to four times better as well, and sometimes even more depending on the benchmark we used. It remains to be seen how well these Express cards will hold up with years of use, and there’s no way to know exactly when their sky-high prices will drop. Non-Switch 2 devices that support microSD Express are still exceedingly rare, and the standard itself isn’t backwards compatible with UHS-II, so you’ll be limited to UHS-I speeds if you want to use your card with another device (unless you buy a pricey external reader). Still, while the increased costs and limited selection are annoying, the tech itself is worthy of a next-gen Switch. How we test microSD Express cards Jeff Dunn for Engadget We put our microSD Express cards through a series of tests meant to simulate how people would use each card on the Switch 2 in the real world. We mainly worked with four games: a mid-sized title in Mario Kart World, a small one in Fast Fusion, a relatively large one in Cyberpunk 2077 and a hybrid in The Legend of Zelda: Breath of the Wild, which ran off a Switch 1 cartridge but used a roughly 10GB Switch 2 upgrade pack that was downloaded and installed digitally. We first timed how long it took to move each game from the system’s internal storage to the card in question, and vice versa. We then timed how long it took to load each game when installed to a given card. After that, we measured how quickly the cards could load certain in-game scenarios: the first Grand Prix race in Mario Kart; the first championship race in Fast Fusion; fast traveling between the Jig-Jig Street, Embers and Downtown Central areas in Cyberpunk and fast traveling between the Kakariko Village, Korok Forest and the Hyrule Castle Town Ruins areas in Zelda. (We chose those places in the latter two games because they’re more taxing than other regions.) With Cyberpunk and Zelda, we also timed how long it took to load up different save files in those locations. With each test, we completed three to five runs to account for any irregularities and marked down the average time taken between them. We did each test in airplane mode, with Wi-Fi and Bluetooth off, to minimize any performance drain that could arise from background downloads. Between each test, we also spent at least an hour playing the games off each card to ensure there were no significant drop-offs compared to the console’s built-in storage. Recent updates November 2025: We’ve tested the 256GB version of Walmart’s Onn microSD Express card and edited our guide accordingly. It’s generally slower than most of the other options we’ve tried, especially when moving games to and from system storage, and in synthetic benchmark tests on a PC. But the performance drop-off isn’t particularly noticeable in actual Switch 2 games, so if you see it in stock and just want to pay as little as possible, it’s a decent buy. September 2025: We’ve taken another pass through this guide to confirm our advice is still accurate. We’ve also noted a new 512GB version of PNY’s microSD Express card and confirmed that a “SanDisk GamePlay” Express card sold at Walmart has the same performance as the standard SanDisk model we recommend above, just with a different name.This article originally appeared on Engadget at https://www.engadget.com/gaming/nintendo/best-microsd-cards-for-nintendo-switch-2-160052947.html?src=rss",
          "content": "The Nintendo Switch 2 is here, and that means lots of people will have to buy a new microSD card. While the device comes with 256GB of storage built in, some new games chew up a ton of space. Cyberpunk 2077 is a 60GB download, for instance, while Split Fiction checks in at 69GB. Other titles aren’t nearly as big, but it’s easy to see how you could wind up deleting and redownloading games within a couple months.When you do want to add room, you’ll need a microSD Express card. The “Express” part is important: These are not the same as the old reliable microSD cards you may have bought for the original Switch or other gaming handhelds — they’re newer, faster and significantly more expensive. They’re also your only choice. If you’re looking for the best microSD card for the Switch 2 today, we’ve broken down what you should know before you buy. The best microSD cards for the Switch 2 (aren't all that important) Jeff Dunn for Engadget The Switch 2 is the first mainstream device to require microSD Express for storage expansion, so there aren’t many options available to buy just yet. Of the handful of compatible models released thus far, we’ve tested six: the SanDisk microSD Express Card (aka the “SanDisk GamePlay microSD Express Card” at Walmart), the Lexar Play Pro, the Samsung microSD Card for Nintendo Switch 2, the PNY microSD Express Card, the GameStop Express microSD Card for Nintendo Switch 2 and the Onn microSD Express Card. The first four are made by genuine storage manufacturers, while the GameStop and Onn cards appear to be rebadged versions of other models. We used the 256GB version of every card except for Lexar Play Pro, which was 1TB. After timing these microSD Express cards across a range of Switch 2 games, our advice is simple: Get whichever one is in stock for the lowest price in the capacity you want. They aren’t identically fast, especially if you want to move a game to the card from the console’s internal storage (or vice versa). But the differences in load times and overall performance within actual games are tough to notice unless you have a stopwatch handy. All five cards loaded up the digital version of Mario Kart World, for instance, between 18 and 20 seconds. Each loaded the first Grand Prix race in about 6.5 seconds. Getting to the start screen of Cyberpunk 2077 took about 38 or 39 seconds in each case. Loading a save in a particularly asset-heavy area (Jig-Jig Street) then took between 26 and 29 seconds, depending on the card. (The one exception was with the Onn card, which averaged closer to 31 seconds with that particular task, though that’s also the cheapest choice.) With Fast Fusion, a smaller native Switch 2 game, the initial load always took six to seven seconds, while each card loaded the first championship race in roughly 4 seconds. It was a similar situation with the Switch 2 upgrade for The Legend of Zelda: Breath of the Wild (using a Switch 1 cartridge): Each card took just over six seconds to get to the start screen, between 19 and 20 seconds to load a save just before the final boss, about 16 seconds to fast travel between Kakariko Village and Korok Forest, and so on. We saw no significant issues with in-game loads when playing each game, either. The SanDisk microSD Express Card and Lexar Play Pro. Jeff Dunn for Engadget All of this suggests that the Switch 2 has a relatively specific target for these cards to hit, and that there may not be much room for one model to leap too far out in front of the others. We’ll also note that the console’s built-in storage was consistently faster than any external option: The gap wasn’t always big, but no card truly outpaced it in any of our tests. Loading that demanding area in Cyberpunk, for example, took about 22.5 seconds on average. So if you want the absolute fastest load times, don’t put your game on a card at all. If you need the mental comfort of knowing you technically have the best card available, get the SanDisk microSD Express Card. It had no outliers across our many game loading tests, and it was consistently right near the top when it came to moving games to and from system storage, which means it offers strong sequential read and write performance. Benchmark testing on PC with tools like CrystalDiskMark backed this up, as noted in our broader microSD card buying guide. Putting Mario Kart (a 21.9GB file) on that card took four minutes and 39 seconds on average, which was only second to the Lexar Play Pro by six seconds. It was the fastest to write Fast Fusion (3.5GB), taking an average of 27 seconds across three runs. Only PNY’s card was faster to move games back to the console’s storage, but that one was far slower at writing games to the card — getting Mario Kart on there took seven minutes and 11 seconds on average. Just note that the 128GB version of SanDisk’s card has slower sequential writes than the larger versions, including much slower sustained write speeds (100 MB/s vs 210-220 MB/s). So transferring a game to that particular model will take much longer. The Onn card was also slower to move games back to system storage, taking about 50 seconds more than the SanDisk with Mario Kart and nearly three minutes longer with Cyberpunk Practically speaking, though, speed differences aren’t as important in this case as having lots of space to hold games at a price you can live with. To make things easy, we’ve listed every Express card we’ve seen at retailers at the time of writing below. Remember: You want microSD Express, not “Extreme,” like the branding SanDisk uses for some conventional microSD cards. A microSD Express card will have a big “EX” logo printed on it. 128GB SanDisk microSD Express Card ($64 MSRP) PNY microSD Express Card ($45 MSRP) 256GB Samsung microSD Express Card for Nintendo Switch 2 ($60 MSRP) SanDisk microSD Express Card ($70 MSRP) Lexar Play Pro ($60 MSRP) PNY microSD Express Card ($62 MSRP) GameStop Express microSD Card for Nintendo Switch 2 ($60 MSRP) Onn microSD Express Card ($47 MSRP) 512GB SanDisk microSD Express Card ($120 MSRP) Lexar Play Pro ($120 MSRP) PNY microSD Express Card ($120 MSRP) GameStop Express microSD Card for Nintendo Switch 2 ($100 MSRP) Onn microSD Express Card ($85 MSRP) 1TB Lexar Play Pro ($220 MSRP) GameStop Express microSD Card for Nintendo Switch 2 ($190 MSRP) All microSD Express cards will have this \"EX\" logo printed on them. Nintendo/Engadget As you can see, while the SanDisk card is fast, it’s also the most expensive of an already-pricey bunch. Is it worth an extra $10-20 to shave a couple seconds off certain loads in certain games, or a couple minutes when moving a game to external storage? Probably not for most people. But stock for all of these cards has been a bit patchy since the Switch 2 landed, especially for the Walmart Onn model, which is by far the cheapest choice. If only one card is actually available by the time you read this — and you must have it today — it’s safe to just get it. You won’t lose or gain all that much when it comes to real-world performance. Ultimately, though, we still advise holding off on buying a microSD Express card for as long as you can. President Trump’s ongoing tariff shenanigans could spike prices a little higher in the short term, but we've only just started to see a few small discounts in recent weeks, and in general, all of these cards should only fall further over time. And compared to traditional microSD options, they are still pricey: The Samsung Pro Plus, for example, costs $17 for 128GB, $27 for 256GB, $50 for 512GB and $95 for 1TB as of this writing. The Switch 2 is extremely popular, so more microSD Express cards will need to be made and prices will (eventually) come down. Ideally, we’ll see more high-capacity options as well: Nintendo says the Switch 2 technically supports cards up to 2TB, but so far only a couple even go up to 1TB. All of this means you should try to use all 256GB baked into the Switch 2 first, even if it means having to delete a game or two. But if you absolutely need more space right away, the cards above should be fine. What are microSD Express cards? A microSD Express card like the one on the right has a second row of pins on the back. Jeff Dunn for Engadget Most microSD cards are based on a standard called Ultra High Speed (UHS), of which there are three versions: UHS-I, UHS-II and UHS-III. The vast majority of cards you may have bought in the past utilize UHS-I. These have one row of pins in the back and a theoretical maximum data transfer speed of 104 megabytes per second (MB/s). (Though many cards are able to surpass that limit with proprietary tech and card readers.) The original Switch has a UHS-I microSD slot, as do most other gaming handhelds like Valve’s Steam Deck. UHS-II cards add a second row of pins and can reach up to 312 MB/s. These are pricier and much less common than cards based on UHS-I, but they’re supported by some cameras and higher-power handhelds like the ASUS ROG Xbox Ally X. UHS-III, meanwhile, is twice as fast as UHS-II in theory (624 MB/s), but no microSD cards have actually used it. UHS-I cards have held on over the years because they’re cheap, widely supported and fast enough for the things most people need them to do: record 4K video, stash photos and so on. But with the Switch 2, Nintendo needs more. The new console is dramatically more powerful, which allows it to run demanding games that may have originally been built for stronger hardware like the PlayStation 5, Xbox Series X or gaming PCs. The device also uses UFS 3.1 storage internally, which is much speedier than the eMMC storage used by the original Switch. (A custom file decompression engine helps improve load times as well.) So if the Switch 2 is going to accept microSD cards, it needs ones that won’t bring a serious drop-off in performance and can hold up with modern games. The Nintendo Switch 2. Sam Rutherford for Engadget Hence, SD Express. This standard has technically been around since 2018 but mostly went nowhere until the Switch 2 came along. It also uses a second row of pins, but it lets microSD cards take advantage of the PCI Express (PCIe)/NVMe interface, which is the same underlying tech used by modern SSDs. As a result, it can produce considerably faster read and write speeds, with a current theoretical maximum of 985 MB/s. As noted above, real-world performance won’t be quite that fast. Even if it was, the best microSD Express cards would still be much slower than the NVMe SSDs used by the PS5 and Xbox. (Sony recommends SSDs with sequential read speeds of at least 5,500 MB/s.) And they’ll fall well below their peak speeds under sustained loads: SanDisk, for instance, says sustained write speeds for its 128GB Express card can drop as low as 100 MB/s. But they’re still a marked improvement over old UHS-I cards, and in theory, they should be quicker than some older SATA-based SSDs when it comes loading game levels, asset streaming, retrieving saves or copying games to external storage. Whereas SanDisk’s microSD Express card can produce sequential read speeds around 900 MB/s, Lexar’s Professional Silver Plus — the top UHS-I pick in our general microSD card guide — topped out just over 200 MB/s, and that’s with a proprietary reader. (On the first Switch, it’d be closer to 100 MB/s.) Sequential writes and random speeds were three to four times better as well, and sometimes even more depending on the benchmark we used. It remains to be seen how well these Express cards will hold up with years of use, and there’s no way to know exactly when their sky-high prices will drop. Non-Switch 2 devices that support microSD Express are still exceedingly rare, and the standard itself isn’t backwards compatible with UHS-II, so you’ll be limited to UHS-I speeds if you want to use your card with another device (unless you buy a pricey external reader). Still, while the increased costs and limited selection are annoying, the tech itself is worthy of a next-gen Switch. How we test microSD Express cards Jeff Dunn for Engadget We put our microSD Express cards through a series of tests meant to simulate how people would use each card on the Switch 2 in the real world. We mainly worked with four games: a mid-sized title in Mario Kart World, a small one in Fast Fusion, a relatively large one in Cyberpunk 2077 and a hybrid in The Legend of Zelda: Breath of the Wild, which ran off a Switch 1 cartridge but used a roughly 10GB Switch 2 upgrade pack that was downloaded and installed digitally. We first timed how long it took to move each game from the system’s internal storage to the card in question, and vice versa. We then timed how long it took to load each game when installed to a given card. After that, we measured how quickly the cards could load certain in-game scenarios: the first Grand Prix race in Mario Kart; the first championship race in Fast Fusion; fast traveling between the Jig-Jig Street, Embers and Downtown Central areas in Cyberpunk and fast traveling between the Kakariko Village, Korok Forest and the Hyrule Castle Town Ruins areas in Zelda. (We chose those places in the latter two games because they’re more taxing than other regions.) With Cyberpunk and Zelda, we also timed how long it took to load up different save files in those locations. With each test, we completed three to five runs to account for any irregularities and marked down the average time taken between them. We did each test in airplane mode, with Wi-Fi and Bluetooth off, to minimize any performance drain that could arise from background downloads. Between each test, we also spent at least an hour playing the games off each card to ensure there were no significant drop-offs compared to the console’s built-in storage. Recent updates November 2025: We’ve tested the 256GB version of Walmart’s Onn microSD Express card and edited our guide accordingly. It’s generally slower than most of the other options we’ve tried, especially when moving games to and from system storage, and in synthetic benchmark tests on a PC. But the performance drop-off isn’t particularly noticeable in actual Switch 2 games, so if you see it in stock and just want to pay as little as possible, it’s a decent buy. September 2025: We’ve taken another pass through this guide to confirm our advice is still accurate. We’ve also noted a new 512GB version of PNY’s microSD Express card and confirmed that a “SanDisk GamePlay” Express card sold at Walmart has the same performance as the standard SanDisk model we recommend above, just with a different name.This article originally appeared on Engadget at https://www.engadget.com/gaming/nintendo/best-microsd-cards-for-nintendo-switch-2-160052947.html?src=rss",
          "feed_position": 24,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2025-08/f2871a40-7217-11f0-9abf-6d1045de6d8d"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/developers-beware-googles-gemma-model-controversy-exposes-model-lifecycle",
          "published_at": "Mon, 03 Nov 2025 05:00:00 GMT",
          "title": "Developers beware: Google’s Gemma model controversy exposes model lifecycle risks",
          "standfirst": "The recent controversy surrounding Google’s Gemma model has once again highlighted the dangers of using developer test models and the fleeting nature of model availability. Google pulled its Gemma 3 model from AI Studio following a statement from Senator Marsha Blackburn (R-Tenn.) that the Gemma model willfully hallucinated falsehoods about her. Blackburn said the model fabricated news stories about her that go beyond “harmless hallucination” and function as a defamatory act. In response, Google posted on X on October 31 that it will remove Gemma from AI Studio, stating that this is “to prevent confusion.” Gemma remains available via API. It is also available via AI Studio, which, the company described, is \"a developer tool (in fact, to use it you need to attest you&#x27;re a developer). We’ve now seen reports of non-developers trying to use Gemma in AI Studio and ask it factual questions. We never intended this to be a consumer tool or model, or to be used this way. To prevent this confusion, access to Gemma is no longer available on AI Studio.\"To be clear, Google has the right to remove its model from its platform, especially if people have found hallucinations and falsehoods that could proliferate. It also underscores the danger of relying mainly on experimental models and why enterprise developers need to save projects before AI models are sunsetted or removed. Technology companies like Google continue to face political controversies, which often influence their deployments. VentureBeat reached out to Google for additional information and was pointed to their October 31 posts. We also contacted the office of Sen. Blackburn, who reiterated her stance outlined in a statement that AI companies should “shut [models] down until you can control it.\"Developer experimentsThe Gemma family of models, which includes a 270M parameter version, is best suited for small, quick apps and tasks that can run on devices such as smartphones and laptops. Google said the Gemma models were “built specifically for the developer and research community. They are not meant for factual assistance or for consumers to use.”Nevertheless, non-developers could still access Gemma because it is on the AI Studio platform, a more beginner-friendly space for developers to play around with Google AI models compared to Vertex AI. So even if Google never intended Gemma and AI Studio to be accessible to, say, Congressional staffers, these situations can still occur. It also shows that as models continue to improve, these models still produce inaccurate and potentially harmful information. Enterprises must continually weigh the benefits of using models like Gemma against their potential inaccuracies. Project continuity Another concern is the control that AI companies have over their models. The adage “you don’t own anything on the internet” remains true. If you don’t own a physical or local copy of software, it’s easy for you to lose access to it if the company that owns it decides to take it away. Google did not clarify with VentureBeat if current projects on AI Studio powered by Gemma are saved. Similarly, OpenAI users were disappointed when the company announced that it would remove popular older models on ChatGPT. Even after walking back his statement and reinstating GPT-4o back to ChatGPT, OpenAI CEO Sam Altman continues to field questions around keeping and supporting the model. AI companies can, and should, remove their models if they create harmful outputs. AI models, no matter how mature, remain works in progress and are constantly evolving and improving. But, since they are experimental in nature, models can easily become tools that technology companies and lawmakers can wield as leverage. Enterprise developers must ensure that their work can be saved before models are removed from platforms.",
          "content": "The recent controversy surrounding Google’s Gemma model has once again highlighted the dangers of using developer test models and the fleeting nature of model availability. Google pulled its Gemma 3 model from AI Studio following a statement from Senator Marsha Blackburn (R-Tenn.) that the Gemma model willfully hallucinated falsehoods about her. Blackburn said the model fabricated news stories about her that go beyond “harmless hallucination” and function as a defamatory act. In response, Google posted on X on October 31 that it will remove Gemma from AI Studio, stating that this is “to prevent confusion.” Gemma remains available via API. It is also available via AI Studio, which, the company described, is \"a developer tool (in fact, to use it you need to attest you&#x27;re a developer). We’ve now seen reports of non-developers trying to use Gemma in AI Studio and ask it factual questions. We never intended this to be a consumer tool or model, or to be used this way. To prevent this confusion, access to Gemma is no longer available on AI Studio.\"To be clear, Google has the right to remove its model from its platform, especially if people have found hallucinations and falsehoods that could proliferate. It also underscores the danger of relying mainly on experimental models and why enterprise developers need to save projects before AI models are sunsetted or removed. Technology companies like Google continue to face political controversies, which often influence their deployments. VentureBeat reached out to Google for additional information and was pointed to their October 31 posts. We also contacted the office of Sen. Blackburn, who reiterated her stance outlined in a statement that AI companies should “shut [models] down until you can control it.\"Developer experimentsThe Gemma family of models, which includes a 270M parameter version, is best suited for small, quick apps and tasks that can run on devices such as smartphones and laptops. Google said the Gemma models were “built specifically for the developer and research community. They are not meant for factual assistance or for consumers to use.”Nevertheless, non-developers could still access Gemma because it is on the AI Studio platform, a more beginner-friendly space for developers to play around with Google AI models compared to Vertex AI. So even if Google never intended Gemma and AI Studio to be accessible to, say, Congressional staffers, these situations can still occur. It also shows that as models continue to improve, these models still produce inaccurate and potentially harmful information. Enterprises must continually weigh the benefits of using models like Gemma against their potential inaccuracies. Project continuity Another concern is the control that AI companies have over their models. The adage “you don’t own anything on the internet” remains true. If you don’t own a physical or local copy of software, it’s easy for you to lose access to it if the company that owns it decides to take it away. Google did not clarify with VentureBeat if current projects on AI Studio powered by Gemma are saved. Similarly, OpenAI users were disappointed when the company announced that it would remove popular older models on ChatGPT. Even after walking back his statement and reinstating GPT-4o back to ChatGPT, OpenAI CEO Sam Altman continues to field questions around keeping and supporting the model. AI companies can, and should, remove their models if they create harmful outputs. AI models, no matter how mature, remain works in progress and are constantly evolving and improving. But, since they are experimental in nature, models can easily become tools that technology companies and lawmakers can wield as leverage. Enterprise developers must ensure that their work can be saved before models are removed from platforms.",
          "feed_position": 4,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/5qNV2RpIAeBQOQANXVwft6/ba2fb53327b104d5d3b6fe084f427e4d/gemma-3-270m.png?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/mobile/smartphones/how-to-use-call-screening-on-an-iphone-130041744.html",
          "published_at": "Sun, 02 Nov 2025 13:00:41 +0000",
          "title": "How to use call screening on an iPhone",
          "standfirst": "Spam and scam calls are some of the most annoying things we all have to deal with when it comes to our phones. Apple’s iOS 26 update takes a firm step toward solving this problem with Call Screening, a new feature designed to filter unknown numbers before you ever pick up the phone. Once it’s activated, your iPhone can automatically answer calls from numbers not saved in your contacts, ask the caller to identify themselves and display a real-time transcription of their response on your screen. You can then decide whether the call is worth taking, effectively cutting off spam before it reaches you. This feature blends convenience and privacy by keeping all processing on the device rather than sending data to the cloud. It’s a logical evolution from earlier tools like Silence Unknown Callers, but this time it allows for a more conversational gatekeeping system that mirrors the experience of live voicemail. Here’s how to turn on Call Screening on your iPhone. Checking compatibility and updating to iOS 26 Call Screening is available to anyone with an iPhone capable of running iOS 26, which includes models from the iPhone 11 onward. To access it, you first need to make sure your device is running the latest version of the software. You can check this by opening Settings, selecting General and tapping Software Update. If you see that iOS 26 is available, download and install it before continuing. This step is essential because Call Screening is only available within the redesigned Phone app introduced in iOS 26. Once your device is updated, the feature becomes accessible through the app’s settings menu, ready to be switched on. How to turn on Call Screening To enable Call Screening, start by opening the Settings app on your iPhone. Scroll down until you see Apps then tap to open the list and select Phone. Within the Phone settings, look for the section labeled Screen Unknown Callers. This is where you’ll find the new options introduced with iOS 26. Apple gives you a few choices here. If you select Never, every call will come through as usual, even from numbers that aren’t saved to your contacts. The Ask Reason for Calling option activates the new screening feature, prompting unknown callers to state who they are before you see the transcription on screen. The Silence option, meanwhile, sends unknown numbers directly to voicemail without ringing your phone at all. To use Call Screening, tap Ask Reason for Calling. Once you back out of the Settings app, the feature will be active. From that point forward, any call from an unfamiliar number will automatically be filtered through Apple’s new screening system. What Call Screening actually does When Call Screening is turned on, your iPhone will automatically step in whenever a call arrives from a number it doesn’t recognize. Instead of the phone ringing immediately, the caller hears an automated message that asks them to state their name and reason for calling. Their response is converted into text in real time, which appears on your screen while the call is still active. You can read their answer, assess whether the call looks legitimate and choose whether to pick up or ignore it. If the call comes from someone in your contacts list, the process doesn’t activate at all, meaning known callers and recent outgoing numbers will continue to ring normally. The feature simply adds a layer of defense between you and unwanted interruptions, allowing genuine callers through while stopping random or suspicious ones before they take your attention.This article originally appeared on Engadget at https://www.engadget.com/mobile/smartphones/how-to-use-call-screening-on-an-iphone-130041744.html?src=rss",
          "content": "Spam and scam calls are some of the most annoying things we all have to deal with when it comes to our phones. Apple’s iOS 26 update takes a firm step toward solving this problem with Call Screening, a new feature designed to filter unknown numbers before you ever pick up the phone. Once it’s activated, your iPhone can automatically answer calls from numbers not saved in your contacts, ask the caller to identify themselves and display a real-time transcription of their response on your screen. You can then decide whether the call is worth taking, effectively cutting off spam before it reaches you. This feature blends convenience and privacy by keeping all processing on the device rather than sending data to the cloud. It’s a logical evolution from earlier tools like Silence Unknown Callers, but this time it allows for a more conversational gatekeeping system that mirrors the experience of live voicemail. Here’s how to turn on Call Screening on your iPhone. Checking compatibility and updating to iOS 26 Call Screening is available to anyone with an iPhone capable of running iOS 26, which includes models from the iPhone 11 onward. To access it, you first need to make sure your device is running the latest version of the software. You can check this by opening Settings, selecting General and tapping Software Update. If you see that iOS 26 is available, download and install it before continuing. This step is essential because Call Screening is only available within the redesigned Phone app introduced in iOS 26. Once your device is updated, the feature becomes accessible through the app’s settings menu, ready to be switched on. How to turn on Call Screening To enable Call Screening, start by opening the Settings app on your iPhone. Scroll down until you see Apps then tap to open the list and select Phone. Within the Phone settings, look for the section labeled Screen Unknown Callers. This is where you’ll find the new options introduced with iOS 26. Apple gives you a few choices here. If you select Never, every call will come through as usual, even from numbers that aren’t saved to your contacts. The Ask Reason for Calling option activates the new screening feature, prompting unknown callers to state who they are before you see the transcription on screen. The Silence option, meanwhile, sends unknown numbers directly to voicemail without ringing your phone at all. To use Call Screening, tap Ask Reason for Calling. Once you back out of the Settings app, the feature will be active. From that point forward, any call from an unfamiliar number will automatically be filtered through Apple’s new screening system. What Call Screening actually does When Call Screening is turned on, your iPhone will automatically step in whenever a call arrives from a number it doesn’t recognize. Instead of the phone ringing immediately, the caller hears an automated message that asks them to state their name and reason for calling. Their response is converted into text in real time, which appears on your screen while the call is still active. You can read their answer, assess whether the call looks legitimate and choose whether to pick up or ignore it. If the call comes from someone in your contacts list, the process doesn’t activate at all, meaning known callers and recent outgoing numbers will continue to ring normally. The feature simply adds a layer of defense between you and unwanted interruptions, allowing genuine callers through while stopping random or suspicious ones before they take your attention.This article originally appeared on Engadget at https://www.engadget.com/mobile/smartphones/how-to-use-call-screening-on-an-iphone-130041744.html?src=rss",
          "feed_position": 30
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/moving-past-speculation-how-deterministic-cpus-deliver-predictable-ai",
          "published_at": "Sun, 02 Nov 2025 05:00:00 GMT",
          "title": "Moving past speculation: How deterministic CPUs deliver predictable AI performance",
          "standfirst": "For more than three decades, modern CPUs have relied on speculative execution to keep pipelines full. When it emerged in the 1990s, speculation was hailed as a breakthrough — just as pipelining and superscalar execution had been in earlier decades. Each marked a generational leap in microarchitecture. By predicting the outcomes of branches and memory loads, processors could avoid stalls and keep execution units busy. But this architectural shift came at a cost: Wasted energy when predictions failed, increased complexity and vulnerabilities such as Spectre and Meltdown. These challenges set the stage for an alternative: A deterministic, time-based execution model. As David Patterson observed in 1980, “A RISC potentially gains in speed merely from a simpler design.” Patterson’s principle of simplicity underpins a new alternative to speculation: A deterministic, time-based execution model.\"For the first time since speculative execution became the dominant paradigm, a fundamentally new approach has been invented. This breakthrough is embodied in a series of six recently issued U.S. patents, sailing through the U.S. Patent and Trademark Office (USPTO). Together, they introduce a radically different instruction execution model. Departing sharply from conventional speculative techniques, this novel deterministic framework replaces guesswork with a time-based, latency-tolerant mechanism. Each instruction is assigned a precise execution slot within the pipeline, resulting in a rigorously ordered and predictable flow of execution. This reimagined model redefines how modern processors can handle latency and concurrency with greater efficiency and reliability. A simple time counter is used to deterministically set the exact time of when instructions should be executed in the future. Each instruction is dispatched to an execution queue with a preset execution time based on resolving its data dependencies and availability of resources — read buses, execution units and the write bus to the register file. Each instruction remains queued until its scheduled execution slot arrives. This new deterministic approach may represent the first major architectural challenge to speculation since it became the standard.The architecture extends naturally into matrix computation, with a RISC-V instruction set proposal under community review. Configurable general matrix multiply (GEMM) units, ranging from 8×8 to 64×64, can operate using either register-based or direct-memory acceess (DMA)-fed operands. This flexibility supports a wide range of AI and high-performance computing (HPC) workloads. Early analysis suggests scalability that rivals Google’s TPU cores, while maintaining significantly lower cost and power requirements. Rather than a direct comparison with general-purpose CPUs, the more accurate reference point is vector and matrix engines: Traditional CPUs still depend on speculation and branch prediction, whereas this design applies deterministic scheduling directly to GEMM and vector units. This efficiency stems not only from the configurable GEMM blocks but also from the time-based execution model, where instructions are decoded and assigned precise execution slots based on operand readiness and resource availability. Execution is never a random or heuristic choice among many candidates, but a predictable, pre-planned flow that keeps compute resources continuously busy. Planned matrix benchmarks will provide direct comparisons with TPU GEMM implementations, highlighting the ability to deliver datacenter-class performance without datacenter-class overhead.Critics may argue that static scheduling introduces latency into instruction execution. In reality, the latency already exists — waiting on data dependencies or memory fetches. Conventional CPUs attempt to hide it with speculation, but when predictions fail, the resulting pipeline flush introduces delay and wastes power. The time-counter approach acknowledges this latency and fills it deterministically with useful work, avoiding rollbacks. As the first patent notes, instructions retain out-of-order efficiency: “A microprocessor with a time counter for statically dispatching instructions enables execution based on predicted timing rather than speculative issue and recovery,\" with preset execution times but without the overhead of register renaming or speculative comparators.Why speculation stalledSpeculative execution boosts performance by predicting outcomes before they’re known — executing instructions ahead of time and discarding them if the guess was wrong. While this approach can accelerate workloads, it also introduces unpredictability and power inefficiency. Mispredictions inject “No Ops” into the pipeline, stalling progress and wasting energy on work that never completes. These issues are magnified in modern AI and machine learning (ML) workloads, where vector and matrix operations dominate and memory access patterns are irregular. Long fetches, non-cacheable loads and misaligned vectors frequently trigger pipeline flushes in speculative architectures.The result is performance cliffs that vary wildly across datasets and problem sizes, making consistent tuning nearly impossible. Worse still, speculative side effects have exposed vulnerabilities that led to high-profile security exploits. As data intensity grows and memory systems strain, speculation struggles to keep pace — undermining its original promise of seamless acceleration.Time-based execution and deterministic schedulingAt the core of this invention is a vector coprocessor with a time counter for statically dispatching instructions. Rather than relying on speculation, instructions are issued only when data dependencies and latency windows are fully known. This eliminates guesswork and costly pipeline flushes while preserving the throughput advantages of out-of-order execution. Architectures built on this patented framework feature deep pipelines — typically spanning 12 stages — combined with wide front ends supporting up to 8-way decode and large reorder buffers exceeding 250 entriesAs illustrated in Figure 1, the architecture mirrors a conventional RISC-V processor at the top level, with instruction fetch and decode stages feeding into execution units. The innovation emerges in the integration of a time counter and register scoreboard, strategically positioned between fetch/decode and the vector execution units. Instead of relying on speculative comparators or register renaming, they utilize a Register Scoreboard and Time Resource Matrix (TRM) to deterministically schedule instructions based on operand readiness and resource availability. Figure 1: High-level block diagram of deterministic processor. A time counter and scoreboard sit between fetch/decode and vector execution units, ensuring instructions issue only when operands are ready.A typical program running on the deterministic processor begins much like it does on any conventional RISC-V system: Instructions are fetched from memory and decoded to determine whether they are scalar, vector, matrix or custom extensions. The difference emerges at the point of dispatch. Instead of issuing instructions speculatively, the processor employs a cycle-accurate time counter, working with a register scoreboard, to decide exactly when each instruction can be executed. This mechanism provides a deterministic execution contract, ensuring instructions complete at predictable cycles and reducing wasted issue slots.In conjunction with a register scoreboard, the time-resource matrix associates instructions with execution cycles, allowing the processor to plan dispatch deterministically across available resources. The scoreboard tracks operand readiness and hazard information, enabling scheduling without register renaming or speculative comparators. By monitoring dependencies such as read-after-write (RAW) and write-after-read, it ensures hazards are resolved without costly pipeline flushes. As noted in the patent, “in a multi-threaded microprocessor, the time counter and scoreboard permit rescheduling around cache misses, branch flushes, and RAW hazards without speculative rollback.”Once operands are ready, the instruction is dispatched to the appropriate execution unit. Scalar operations use standard artithmetic logic units (ALUs), while vector and matrix instructions execute in wide execution units connected to a large vector register file. Because instructions launch only when conditions are safe, these units stay highly utilized without the wasted work or recovery cycles caused by mis-predicted speculation. The key enabler of this approach is a simple time counter that orchestrates execution according to data readiness and resource availability, ensuring instructions advance only when operands are ready and resources available. The same principle applies to memory operations: The interface predicts latency windows for loads and stores, allowing the processor to fill those slots with independent instructions and keep execution flowing.Programming model differencesFrom the programmer’s perspective, the flow remains familiar — RISC-V code compiles and executes in the usual way. The crucial difference lies in the execution contract: Rather than relying on dynamic speculation to hide latency, the processor guarantees predictable dispatch and completion times. This eliminates the performance cliffs and wasted energy of speculation while still providing the throughput benefits of out-of-order execution. This perspective underscores how deterministic execution preserves the familiar RISC-V programming model while eliminating the unpredictability and wasted effort of speculation. As John Hennessy put it: \"It’s stupid to do work in run time that you can do in compile time”— a remark reflecting the foundations of RISC and its forward-looking design philosophy.The RISC-V ISA provides opcodes for custom and extension instructions, including floating-point, DSP, and vector operations. The result is a processor that executes instructions deterministically while retaining the benefits of out-of-order performance. By eliminating speculation, the design simplifies hardware, reduces power consumption and avoids pipeline flushes. These efficiency gains grow even more significant in vector and matrix operations, where wide execution units require consistent utilization to reach peak performance. Vector extensions require wide register files and large execution units, which in speculative processors necessitate expensive register renaming to recover from branch mispredictions. In the deterministic design, vector instructions are executed only after commit, eliminating the need for renaming.Each instruction is scheduled against a cycle-accurate time counter: “The time counter provides a deterministic execution contract, ensuring instructions complete at predictable cycles and reducing wasted issue slots.” The vector register scoreboard resolves data dependency before issuing instructions to execution pipeline. Instructions are dispatched in a known order at the correct cycle, making execution both predictable and efficient.Vector execution units (integer and floating point) connect directly to a large vector register file. Because instructions are never flushed, there is no renaming overhead. The scoreboard ensures safe access, while the time counter aligns execution with memory readiness. A dedicated memory block predicts the return cycle of loads. Instead of stalling or speculating, the processor schedules independent instructions into latency slots, keeping execution units busy. “A vector coprocessor with a time counter for statically dispatching instructions ensures high utilization of wide execution units while avoiding misprediction penalties.”In today’s CPUs, compilers and programmers write code assuming the hardware will dynamically reorder instructions and speculatively execute branches. The hardware handles hazards with register renaming, branch prediction and recovery mechanisms. Programmers benefit from performance, but at the cost of unpredictability and power consumption.In the deterministic time-based architecture, instructions are dispatched only when the time counter indicates their operands will be ready. This means the compiler (or runtime system) doesn’t need to insert guard code for misprediction recovery. Instead, compiler scheduling becomes simpler, as instructions are guaranteed to issue at the correct cycle without rollbacks. For programmers, the ISA remains RISC-V compatible, but deterministic extensions reduce reliance on speculative safety nets.Application in AI and MLIn AI/ML kernels, vector loads and matrix operations often dominate runtime. On a speculative CPU, misaligned or non-cacheable loads can trigger stalls or flushes, starving wide vector and matrix units and wasting energy on discarded work. A deterministic design instead issues these operations with cycle-accurate timing, ensuring high utilization and steady throughput. For programmers, this means fewer performance cliffs and more predictable scaling across problem sizes. And because the patents extend the RISC-V ISA rather than replace it, deterministic processors remain fully compatible with the RVA23 profile and mainstream toolchains such as GCC, LLVM, FreeRTOS, and Zephyr.In practice, the deterministic model doesn’t change how code is written — it remains RISC-V assembly or high-level languages compiled to RISC-V instructions. What changes is the execution contract: Rather than relying on speculative guesswork, programmers can expect predictable latency behavior and higher efficiency without tuning code around microarchitectural quirks.The industry is at an inflection point. AI/ML workloads are dominated by vector and matrix math, where GPUs and TPUs excel — but only by consuming massive power and adding architectural complexity. In contrast, general-purpose CPUs, still tied to speculative execution models, lag behind.A deterministic processor delivers predictable performance across a wide range of workloads, ensuring consistent behavior regardless of task complexity. Eliminating speculative execution enhances energy efficiency and avoids unnecessary computational overhead. Furthermore, deterministic design scales naturally to vector and matrix operations, making it especially well-suited for AI workloads that rely on high-throughput parallelism. This new deterministic approach may represent the next such leap: The first major architectural challenge to speculation since speculation itself became the standard.Will deterministic CPUs replace speculation in mainstream computing? That remains to be seen. But with issued patents, proven novelty and growing pressure from AI workloads, the timing is right for a paradigm shift. Taken together, these advances signal deterministic execution as the next architectural leap — redefining performance and efficiency just as speculation once did.Speculation marked the last revolution in CPU design; determinism may well represent the next.Thang Tran is the founder and CTO of Simplex Micro.Read more from our guest writers. Or, consider submitting a post of your own! See our guidelines here.",
          "content": "For more than three decades, modern CPUs have relied on speculative execution to keep pipelines full. When it emerged in the 1990s, speculation was hailed as a breakthrough — just as pipelining and superscalar execution had been in earlier decades. Each marked a generational leap in microarchitecture. By predicting the outcomes of branches and memory loads, processors could avoid stalls and keep execution units busy. But this architectural shift came at a cost: Wasted energy when predictions failed, increased complexity and vulnerabilities such as Spectre and Meltdown. These challenges set the stage for an alternative: A deterministic, time-based execution model. As David Patterson observed in 1980, “A RISC potentially gains in speed merely from a simpler design.” Patterson’s principle of simplicity underpins a new alternative to speculation: A deterministic, time-based execution model.\"For the first time since speculative execution became the dominant paradigm, a fundamentally new approach has been invented. This breakthrough is embodied in a series of six recently issued U.S. patents, sailing through the U.S. Patent and Trademark Office (USPTO). Together, they introduce a radically different instruction execution model. Departing sharply from conventional speculative techniques, this novel deterministic framework replaces guesswork with a time-based, latency-tolerant mechanism. Each instruction is assigned a precise execution slot within the pipeline, resulting in a rigorously ordered and predictable flow of execution. This reimagined model redefines how modern processors can handle latency and concurrency with greater efficiency and reliability. A simple time counter is used to deterministically set the exact time of when instructions should be executed in the future. Each instruction is dispatched to an execution queue with a preset execution time based on resolving its data dependencies and availability of resources — read buses, execution units and the write bus to the register file. Each instruction remains queued until its scheduled execution slot arrives. This new deterministic approach may represent the first major architectural challenge to speculation since it became the standard.The architecture extends naturally into matrix computation, with a RISC-V instruction set proposal under community review. Configurable general matrix multiply (GEMM) units, ranging from 8×8 to 64×64, can operate using either register-based or direct-memory acceess (DMA)-fed operands. This flexibility supports a wide range of AI and high-performance computing (HPC) workloads. Early analysis suggests scalability that rivals Google’s TPU cores, while maintaining significantly lower cost and power requirements. Rather than a direct comparison with general-purpose CPUs, the more accurate reference point is vector and matrix engines: Traditional CPUs still depend on speculation and branch prediction, whereas this design applies deterministic scheduling directly to GEMM and vector units. This efficiency stems not only from the configurable GEMM blocks but also from the time-based execution model, where instructions are decoded and assigned precise execution slots based on operand readiness and resource availability. Execution is never a random or heuristic choice among many candidates, but a predictable, pre-planned flow that keeps compute resources continuously busy. Planned matrix benchmarks will provide direct comparisons with TPU GEMM implementations, highlighting the ability to deliver datacenter-class performance without datacenter-class overhead.Critics may argue that static scheduling introduces latency into instruction execution. In reality, the latency already exists — waiting on data dependencies or memory fetches. Conventional CPUs attempt to hide it with speculation, but when predictions fail, the resulting pipeline flush introduces delay and wastes power. The time-counter approach acknowledges this latency and fills it deterministically with useful work, avoiding rollbacks. As the first patent notes, instructions retain out-of-order efficiency: “A microprocessor with a time counter for statically dispatching instructions enables execution based on predicted timing rather than speculative issue and recovery,\" with preset execution times but without the overhead of register renaming or speculative comparators.Why speculation stalledSpeculative execution boosts performance by predicting outcomes before they’re known — executing instructions ahead of time and discarding them if the guess was wrong. While this approach can accelerate workloads, it also introduces unpredictability and power inefficiency. Mispredictions inject “No Ops” into the pipeline, stalling progress and wasting energy on work that never completes. These issues are magnified in modern AI and machine learning (ML) workloads, where vector and matrix operations dominate and memory access patterns are irregular. Long fetches, non-cacheable loads and misaligned vectors frequently trigger pipeline flushes in speculative architectures.The result is performance cliffs that vary wildly across datasets and problem sizes, making consistent tuning nearly impossible. Worse still, speculative side effects have exposed vulnerabilities that led to high-profile security exploits. As data intensity grows and memory systems strain, speculation struggles to keep pace — undermining its original promise of seamless acceleration.Time-based execution and deterministic schedulingAt the core of this invention is a vector coprocessor with a time counter for statically dispatching instructions. Rather than relying on speculation, instructions are issued only when data dependencies and latency windows are fully known. This eliminates guesswork and costly pipeline flushes while preserving the throughput advantages of out-of-order execution. Architectures built on this patented framework feature deep pipelines — typically spanning 12 stages — combined with wide front ends supporting up to 8-way decode and large reorder buffers exceeding 250 entriesAs illustrated in Figure 1, the architecture mirrors a conventional RISC-V processor at the top level, with instruction fetch and decode stages feeding into execution units. The innovation emerges in the integration of a time counter and register scoreboard, strategically positioned between fetch/decode and the vector execution units. Instead of relying on speculative comparators or register renaming, they utilize a Register Scoreboard and Time Resource Matrix (TRM) to deterministically schedule instructions based on operand readiness and resource availability. Figure 1: High-level block diagram of deterministic processor. A time counter and scoreboard sit between fetch/decode and vector execution units, ensuring instructions issue only when operands are ready.A typical program running on the deterministic processor begins much like it does on any conventional RISC-V system: Instructions are fetched from memory and decoded to determine whether they are scalar, vector, matrix or custom extensions. The difference emerges at the point of dispatch. Instead of issuing instructions speculatively, the processor employs a cycle-accurate time counter, working with a register scoreboard, to decide exactly when each instruction can be executed. This mechanism provides a deterministic execution contract, ensuring instructions complete at predictable cycles and reducing wasted issue slots.In conjunction with a register scoreboard, the time-resource matrix associates instructions with execution cycles, allowing the processor to plan dispatch deterministically across available resources. The scoreboard tracks operand readiness and hazard information, enabling scheduling without register renaming or speculative comparators. By monitoring dependencies such as read-after-write (RAW) and write-after-read, it ensures hazards are resolved without costly pipeline flushes. As noted in the patent, “in a multi-threaded microprocessor, the time counter and scoreboard permit rescheduling around cache misses, branch flushes, and RAW hazards without speculative rollback.”Once operands are ready, the instruction is dispatched to the appropriate execution unit. Scalar operations use standard artithmetic logic units (ALUs), while vector and matrix instructions execute in wide execution units connected to a large vector register file. Because instructions launch only when conditions are safe, these units stay highly utilized without the wasted work or recovery cycles caused by mis-predicted speculation. The key enabler of this approach is a simple time counter that orchestrates execution according to data readiness and resource availability, ensuring instructions advance only when operands are ready and resources available. The same principle applies to memory operations: The interface predicts latency windows for loads and stores, allowing the processor to fill those slots with independent instructions and keep execution flowing.Programming model differencesFrom the programmer’s perspective, the flow remains familiar — RISC-V code compiles and executes in the usual way. The crucial difference lies in the execution contract: Rather than relying on dynamic speculation to hide latency, the processor guarantees predictable dispatch and completion times. This eliminates the performance cliffs and wasted energy of speculation while still providing the throughput benefits of out-of-order execution. This perspective underscores how deterministic execution preserves the familiar RISC-V programming model while eliminating the unpredictability and wasted effort of speculation. As John Hennessy put it: \"It’s stupid to do work in run time that you can do in compile time”— a remark reflecting the foundations of RISC and its forward-looking design philosophy.The RISC-V ISA provides opcodes for custom and extension instructions, including floating-point, DSP, and vector operations. The result is a processor that executes instructions deterministically while retaining the benefits of out-of-order performance. By eliminating speculation, the design simplifies hardware, reduces power consumption and avoids pipeline flushes. These efficiency gains grow even more significant in vector and matrix operations, where wide execution units require consistent utilization to reach peak performance. Vector extensions require wide register files and large execution units, which in speculative processors necessitate expensive register renaming to recover from branch mispredictions. In the deterministic design, vector instructions are executed only after commit, eliminating the need for renaming.Each instruction is scheduled against a cycle-accurate time counter: “The time counter provides a deterministic execution contract, ensuring instructions complete at predictable cycles and reducing wasted issue slots.” The vector register scoreboard resolves data dependency before issuing instructions to execution pipeline. Instructions are dispatched in a known order at the correct cycle, making execution both predictable and efficient.Vector execution units (integer and floating point) connect directly to a large vector register file. Because instructions are never flushed, there is no renaming overhead. The scoreboard ensures safe access, while the time counter aligns execution with memory readiness. A dedicated memory block predicts the return cycle of loads. Instead of stalling or speculating, the processor schedules independent instructions into latency slots, keeping execution units busy. “A vector coprocessor with a time counter for statically dispatching instructions ensures high utilization of wide execution units while avoiding misprediction penalties.”In today’s CPUs, compilers and programmers write code assuming the hardware will dynamically reorder instructions and speculatively execute branches. The hardware handles hazards with register renaming, branch prediction and recovery mechanisms. Programmers benefit from performance, but at the cost of unpredictability and power consumption.In the deterministic time-based architecture, instructions are dispatched only when the time counter indicates their operands will be ready. This means the compiler (or runtime system) doesn’t need to insert guard code for misprediction recovery. Instead, compiler scheduling becomes simpler, as instructions are guaranteed to issue at the correct cycle without rollbacks. For programmers, the ISA remains RISC-V compatible, but deterministic extensions reduce reliance on speculative safety nets.Application in AI and MLIn AI/ML kernels, vector loads and matrix operations often dominate runtime. On a speculative CPU, misaligned or non-cacheable loads can trigger stalls or flushes, starving wide vector and matrix units and wasting energy on discarded work. A deterministic design instead issues these operations with cycle-accurate timing, ensuring high utilization and steady throughput. For programmers, this means fewer performance cliffs and more predictable scaling across problem sizes. And because the patents extend the RISC-V ISA rather than replace it, deterministic processors remain fully compatible with the RVA23 profile and mainstream toolchains such as GCC, LLVM, FreeRTOS, and Zephyr.In practice, the deterministic model doesn’t change how code is written — it remains RISC-V assembly or high-level languages compiled to RISC-V instructions. What changes is the execution contract: Rather than relying on speculative guesswork, programmers can expect predictable latency behavior and higher efficiency without tuning code around microarchitectural quirks.The industry is at an inflection point. AI/ML workloads are dominated by vector and matrix math, where GPUs and TPUs excel — but only by consuming massive power and adding architectural complexity. In contrast, general-purpose CPUs, still tied to speculative execution models, lag behind.A deterministic processor delivers predictable performance across a wide range of workloads, ensuring consistent behavior regardless of task complexity. Eliminating speculative execution enhances energy efficiency and avoids unnecessary computational overhead. Furthermore, deterministic design scales naturally to vector and matrix operations, making it especially well-suited for AI workloads that rely on high-throughput parallelism. This new deterministic approach may represent the next such leap: The first major architectural challenge to speculation since speculation itself became the standard.Will deterministic CPUs replace speculation in mainstream computing? That remains to be seen. But with issued patents, proven novelty and growing pressure from AI workloads, the timing is right for a paradigm shift. Taken together, these advances signal deterministic execution as the next architectural leap — redefining performance and efficiency just as speculation once did.Speculation marked the last revolution in CPU design; determinism may well represent the next.Thang Tran is the founder and CTO of Simplex Micro.Read more from our guest writers. Or, consider submitting a post of your own! See our guidelines here.",
          "feed_position": 5,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/7xDQkK3UJNPCfHY5rmlmLW/4b5264fdd5c1156b8c4863814660dadd/Modern_data_infrastructure.png?w=300&q=30"
        }
      ],
      "featured_image": "https://s.yimg.com/os/creatr-uploaded-images/2025-02/f18a9a20-ea15-11ef-b5d3-3ae5ab47679d",
      "popularity_score": 2015.9067,
      "ai_summary": [
        "Disney-owned channels, including ABC and ESPN, are unavailable on YouTube TV.",
        "The blackout began after Disney and YouTube TV failed to agree on new terms.",
        "YouTube TV subscribers missed college football games and \"Monday Night Football.\"",
        "YouTube TV offered a $20 monthly credit if the channels remain unavailable.",
        "Alternative viewing options include Sling Day Pass, Hulu + Live TV, and others."
      ]
    },
    {
      "id": "cluster_5",
      "coverage": 1,
      "updated_at": "Tue, 04 Nov 2025 02:08:26 +0000",
      "title": "Apple releases iOS 26.1, macOS 26.1, other updates with Liquid Glass controls and more",
      "neutral_headline": "Apple releases iOS 26.1, macOS 26.1, other updates",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2025/11/apple-releases-ios-26-1-macos-26-1-other-updates-with-liquid-glass-controls-and-more/",
          "published_at": "Tue, 04 Nov 2025 02:08:26 +0000",
          "title": "Apple releases iOS 26.1, macOS 26.1, other updates with Liquid Glass controls and more",
          "standfirst": "New updates include a small but noteworthy batch of fixes and features.",
          "content": "After several weeks of testing, Apple has released the final versions of the 26.1 update to its various operating systems. Those include iOS, iPadOS, macOS, watchOS, tvOS, visionOS, and the HomePod operating system, all of which switched to a new unified year-based version numbering system this fall. This isn’t the first update that these operating systems have gotten since they were released in September, but it is the first to add significant changes and tweaks to existing features, addressing the early complaints and bugs that inevitably come with any major operating system update. One of the biggest changes across most of the platforms is a new translucency control for Liquid Glass that tones it down without totally disabling the effect. Users can stay with the default Clear look to see the clearer, glassier look that allows more of the contents underneath Liquid Glass to show through, or the new Tinted look to get a more opaque background that shows only vague shapes and colors to improve readability.Read full article Comments",
          "feed_position": 0,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/06/apple-os-beta-26-2025-1152x648-1750706565.jpeg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/06/apple-os-beta-26-2025-1152x648-1750706565.jpeg",
      "popularity_score": 363.4405888888889,
      "ai_summary": [
        "Apple has released iOS 26.1 and macOS 26.1 updates.",
        "The updates include a collection of fixes and new features.",
        "The updates are considered small but noteworthy.",
        "The updates include controls for Liquid Glass.",
        "The updates are available for various Apple devices."
      ]
    },
    {
      "id": "cluster_24",
      "coverage": 1,
      "updated_at": "Mon, 03 Nov 2025 22:13:53 +0000",
      "title": "A commercial space station startup now has a foothold in space",
      "neutral_headline": "Commercial space station startup secures foothold in space",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2025/11/a-commercial-space-station-startup-now-has-a-foothold-in-space/",
          "published_at": "Mon, 03 Nov 2025 22:13:53 +0000",
          "title": "A commercial space station startup now has a foothold in space",
          "standfirst": "Vast differs from its space station cohorts by flying a series of progressively more complex demos.",
          "content": "A pathfinder mission for Vast’s privately owned space station launched into orbit Sunday and promptly extended its solar panel, kicking off a shakedown cruise to prove the company’s designs can meet the demands of spaceflight. Vast’s Haven Demo mission lifted off just after midnight Sunday from Cape Canaveral Space Force Station, Florida, and rode a SpaceX Falcon 9 rocket into orbit. Haven Demo was one of 18 satellites sharing a ride on SpaceX’s Bandwagon 4 mission, launching alongside a South Korean spy satellite and a small testbed for Starcloud, a startup working with Nvidia to build an orbital data center. After release from the Falcon 9, the half-ton Haven Demo spacecraft stabilized itself and extended its power-generating solar array. The satellite captured 4K video of the solar array deployment, and Vast shared the beauty shot on social media.Read full article Comments",
          "feed_position": 1,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/haven_demo_array-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/haven_demo_array-1152x648.jpg",
      "popularity_score": 345.5314222222222,
      "ai_summary": [
        "A commercial space station startup has achieved a presence in space.",
        "The startup is named Vast.",
        "Vast is different from other space station companies.",
        "Vast is flying a series of progressively complex demos.",
        "The company is working on developing a space station."
      ]
    },
    {
      "id": "cluster_34",
      "coverage": 1,
      "updated_at": "Mon, 03 Nov 2025 21:01:21 +0000",
      "title": "Real humans don’t stream Drake songs 23 hours a day, rapper suing Spotify says",
      "neutral_headline": "Rapper sues Spotify, alleging streaming fraud by Drake",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/11/real-humans-dont-stream-drake-songs-23-hours-a-day-rapper-suing-spotify-says/",
          "published_at": "Mon, 03 Nov 2025 21:01:21 +0000",
          "title": "Real humans don’t stream Drake songs 23 hours a day, rapper suing Spotify says",
          "standfirst": "Proposed class action may force Spotify to pay back artists harmed by streaming fraud.",
          "content": "Spotify profits off fake Drake streams that rob other artists of perhaps hundreds of millions in revenue shares, a lawsuit filed Sunday alleged—hoping to force Spotify to reimburse every artist impacted. The lawsuit was filed by an American rapper known as RBX, who may be best known for cameos on two of the 1990s’ biggest hip-hop records, Dr. Dre’s The Chronic and Snoop Dogg’s Doggystyle. The problem goes beyond Drake, RBX’s lawsuit alleged. It claims Spotify ignores “billions of fraudulent streams” each month, selfishly benefiting from bot networks that artificially inflate user numbers to help Spotify attract significantly higher ad revenue.Read full article Comments",
          "feed_position": 2,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-2182712179-1152x648-1762197922.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-2182712179-1152x648-1762197922.jpg",
      "popularity_score": 326.32253333333335,
      "ai_summary": [
        "A rapper is suing Spotify, claiming streaming fraud.",
        "The lawsuit alleges that Drake songs are streamed excessively.",
        "The lawsuit suggests that real humans do not stream Drake's songs.",
        "The lawsuit may force Spotify to pay artists for fraud.",
        "The lawsuit is a proposed class action."
      ]
    },
    {
      "id": "cluster_41",
      "coverage": 1,
      "updated_at": "Mon, 03 Nov 2025 20:21:36 +0000",
      "title": "After confusing driver release, AMD says old GPUs are still actively supported",
      "neutral_headline": "AMD says old GPUs still actively supported after driver release",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2025/11/amd-says-that-its-not-pulling-driver-support-for-older-radeon-gpus-afterall/",
          "published_at": "Mon, 03 Nov 2025 20:21:36 +0000",
          "title": "After confusing driver release, AMD says old GPUs are still actively supported",
          "standfirst": "Re-using old silicon means that dropping \"old\" GPUs can affect \"new\" products.",
          "content": "Last week, AMD released version 25.10.2 of its Adrenalin driver package for Radeon GPUs. It seemed like a relatively routine driver release with a typical list of bug fixes and game performance improvements, except for one accompanying announcement: AMD said at the time that it would be moving support for Radeon RX 5000-series and 6000-series GPUs (and their RDNA 1 and RDNA 2 architectures) to “maintenance mode.” That meant that a bunch of GPUs, including some dedicated graphics cards launched as recently as 2022, would no longer get fresh fixes and performance optimizations for newly launched games. As reported by Tom’s Hardware, AMD released several clarifying statements to address the ensuing backlash, saying that these older GPUs would still get “new features, bug fixes, and game optimizations” based on “market needs.” That must not have quieted the complaints, because AMD then made an entirely separate post to confirm that the 25.10.2 driver release “is not the end of support for RDNA 1 and RDNA 2,” and that integrated and dedicated GPUs based on these architectures would continue to receive “game support for new releases,” “stability and game optimizations,” and “security and bug fixes.” AMD confirmed that these older GPU architectures had been moved to a separate driver path, but the company says this is meant to keep fixes and features intended for newer RDNA 3 and RDNA 4-based GPUs from inadvertently breaking things for RDNA 1 and RDNA 2 GPUs.Read full article Comments",
          "feed_position": 3,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/lisa-su-6900xt-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/lisa-su-6900xt-1152x648.jpg",
      "popularity_score": 315.66003333333333,
      "ai_summary": [
        "AMD confirms continued support for older GPUs.",
        "This announcement follows a confusing driver release.",
        "Re-using old silicon can affect new products.",
        "Dropping older GPUs can impact newer products.",
        "AMD is committed to supporting older hardware."
      ]
    },
    {
      "id": "cluster_67",
      "coverage": 1,
      "updated_at": "Mon, 03 Nov 2025 18:28:22 +0000",
      "title": "Google removes Gemma models from AI Studio after GOP senator’s complaint",
      "neutral_headline": "Google removes Gemma models from AI Studio after senator's complaint",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/google/2025/11/google-removes-gemma-models-from-ai-studio-after-gop-senators-complaint/",
          "published_at": "Mon, 03 Nov 2025 18:28:22 +0000",
          "title": "Google removes Gemma models from AI Studio after GOP senator’s complaint",
          "standfirst": "Sen. Marsha Blackburn says Gemma concocted sexual misconduct allegations against her.",
          "content": "You may be disappointed if you go looking for Google’s open Gemma AI model in AI Studio today. Google announced late on Friday that it was pulling Gemma from the platform, but it was vague about the reasoning. The abrupt change appears to be tied to a letter from Sen. Marsha Blackburn (R-Tenn.), who claims the Gemma model generated false accusations of sexual misconduct against her. Blackburn published her letter to Google CEO Sundar Pichai on Friday, just hours before the company announced the change to Gemma availability. She demanded Google explain how the model could fail in this way, tying the situation to ongoing hearings that accuse Google and others of creating bots that defame conservatives. At the hearing, Google’s Markham Erickson explained that AI hallucinations are a widespread and known issue in generative AI, and Google does the best it can to mitigate the impact of such mistakes. Although no AI firm has managed to eliminate hallucinations, Google’s Gemini for Home has been particularly hallucination-happy in our testing.Read full article Comments",
          "feed_position": 6,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/Gemma-social-share.width-1300-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/Gemma-social-share.width-1300-1152x648.jpg",
      "popularity_score": 310.7728111111111,
      "ai_summary": [
        "Google removed Gemma models from AI Studio.",
        "The removal followed a complaint from Senator Marsha Blackburn.",
        "Senator Blackburn claimed Gemma made false allegations.",
        "The allegations involved sexual misconduct against her.",
        "The senator raised concerns about the AI's output."
      ]
    },
    {
      "id": "cluster_45",
      "coverage": 1,
      "updated_at": "Mon, 03 Nov 2025 20:08:40 +0000",
      "title": "LLMs show a “highly unreliable” capacity to describe their own internal processes",
      "neutral_headline": "LLMs show unreliable capacity to describe internal processes",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/ai/2025/11/llms-show-a-highly-unreliable-capacity-to-describe-their-own-internal-processes/",
          "published_at": "Mon, 03 Nov 2025 20:08:40 +0000",
          "title": "LLMs show a “highly unreliable” capacity to describe their own internal processes",
          "standfirst": "Anthropic finds some LLM \"self-awareness,\" but \"failures of introspection remain the norm.\"",
          "content": "If you ask an LLM to explain its own reasoning process, it may well simply confabulate a plausible-sounding explanation for its actions based on text found in its training data. To get around this problem, Anthropic is expanding on its previous research into AI interpretability with a new study that aims to measure LLMs’ actual so-called “introspective awareness” of their own inference processes. The full paper on “Emergent Introspective Awareness in Large Language Models” uses some interesting methods to separate out the metaphorical “thought process” represented by an LLM’s artificial neurons from simple text output that purports to represent that process. In the end, though, the research finds that current AI models are “highly unreliable” at describing their own inner workings and that “failures of introspection remain the norm.” Inception, but for AI Anthropic’s new research is centered on a process it calls “concept injection.” The method starts by comparing the model’s internal activation states following both a control prompt and an experimental prompt (e.g. an “ALL CAPS” prompt versus the same prompt in lower case). Calculating the differences between those activations across billions of internal neurons creates what Anthropic calls a “vector” that in some sense represents how that concept is modeled in the LLM’s internal state.Read full article Comments",
          "feed_position": 4,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-514162112-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-514162112-1152x648.jpg",
      "popularity_score": 305.44447777777776,
      "ai_summary": [
        "Large language models (LLMs) struggle to describe their processes.",
        "LLMs demonstrate a \"highly unreliable\" capacity for self-awareness.",
        "Anthropic found some LLM \"self-awareness.\"",
        "\"Failures of introspection remain the norm\" for LLMs.",
        "LLMs often cannot explain how they work internally."
      ]
    },
    {
      "id": "cluster_56",
      "coverage": 1,
      "updated_at": "Mon, 03 Nov 2025 19:15:21 +0000",
      "title": "Trump on why he pardoned Binance CEO: “Are you ready? I don’t know who he is.”",
      "neutral_headline": "Trump on why he pardoned Binance CEO: “I don’t know who he is”",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/11/trump-on-why-he-pardoned-binance-ceo-are-you-ready-i-dont-know-who-he-is/",
          "published_at": "Mon, 03 Nov 2025 19:15:21 +0000",
          "title": "Trump on why he pardoned Binance CEO: “Are you ready? I don’t know who he is.”",
          "standfirst": "Trump family business could benefit from pardon of crypto ex-con Changpeng Zhao.",
          "content": "President Trump says he still doesn’t know who Binance founder and former CEO Changpeng Zhao is, despite having pardoned Zhao last month. CBS correspondent Norah O’Donnell asked Trump about the pardon in a 60 Minutes interview that aired yesterday, noting that Zhao pleaded guilty to violating anti-money laundering laws. “The government at the time said that C.Z. had caused ‘significant harm to US national security,’ essentially by allowing terrorist groups like Hamas to move millions of dollars around. Why did you pardon him?” O’Donnell asked. “Okay, are you ready? I don’t know who he is. I know he got a four-month sentence or something like that. And I heard it was a Biden witch hunt,” answered Trump, who has criticized his predecessor for signing pardons with an autopen.Read full article Comments",
          "feed_position": 5,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2024/04/Changpeng-Zhao-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2024/04/Changpeng-Zhao-1152x648.jpg",
      "popularity_score": 294.55586666666665,
      "ai_summary": [
        "Donald Trump pardoned Binance CEO Changpeng Zhao.",
        "Trump stated he did not know who Zhao was.",
        "The Trump family business could benefit from the pardon.",
        "Zhao is a former crypto ex-con.",
        "The pardon has generated controversy."
      ]
    },
    {
      "id": "cluster_76",
      "coverage": 1,
      "updated_at": "Mon, 03 Nov 2025 17:23:11 +0000",
      "title": "OpenAI signs massive AI compute deal with Amazon",
      "neutral_headline": "OpenAI signs massive AI compute deal with Amazon",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/ai/2025/11/openai-signs-massive-ai-compute-deal-with-amazon/",
          "published_at": "Mon, 03 Nov 2025 17:23:11 +0000",
          "title": "OpenAI signs massive AI compute deal with Amazon",
          "standfirst": "Deal will provide access to hundreds of thousands of Nvidia chips that power ChatGPT.",
          "content": "On Monday, OpenAI announced it has signed a seven-year, $38 billion deal to buy cloud services from Amazon Web Services to power products like ChatGPT and Sora. It’s the company’s first big computing deal after a fundamental restructuring last week that gave OpenAI more operational and financial freedom from Microsoft. The agreement gives OpenAI access to hundreds of thousands of Nvidia graphics processors to train and run its AI models. “Scaling frontier AI requires massive, reliable compute,” OpenAI CEO Sam Altman said in a statement. “Our partnership with AWS strengthens the broad compute ecosystem that will power this next era and bring advanced AI to everyone.” OpenAI will reportedly use Amazon Web Services immediately, with all planned capacity set to come online by the end of 2026 and room to expand further in 2027 and beyond. Amazon plans to roll out hundreds of thousands of chips, including Nvidia’s GB200 and GB300 AI accelerators, in data clusters built to power ChatGPT’s responses, generate AI videos, and train OpenAI’s next wave of models.Read full article Comments",
          "feed_position": 7,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2023/08/hiding_hero_1-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2023/08/hiding_hero_1-1152x648.jpg",
      "popularity_score": 284.68642222222223,
      "ai_summary": [
        "OpenAI has signed a large AI compute deal with Amazon.",
        "The deal provides access to Nvidia chips.",
        "The chips power ChatGPT.",
        "The deal involves hundreds of thousands of Nvidia chips.",
        "The deal will help OpenAI expand its AI capabilities."
      ]
    },
    {
      "id": "cluster_81",
      "coverage": 1,
      "updated_at": "Mon, 03 Nov 2025 16:53:45 +0000",
      "title": "Capitol Hill is abuzz with talk of the “Athena” plan for NASA",
      "neutral_headline": "Capitol Hill discusses \"Athena\" plan for NASA",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2025/11/capitol-hill-is-abuzz-with-talk-of-the-athena-plan-for-nasa/",
          "published_at": "Mon, 03 Nov 2025 16:53:45 +0000",
          "title": "Capitol Hill is abuzz with talk of the “Athena” plan for NASA",
          "standfirst": "The Athena plan lays out a blueprint for Isaacman's tenure at NASA.",
          "content": "In recent weeks, copies of an intriguing policy document have started to spread among space lobbyists on Capitol Hill in Washington, DC. The document bears the title “Athena,” and it purports to summarize the actions that private astronaut Jared Isaacman would have taken, were his nomination to become NASA administrator confirmed. The 62-page plan is notable both for the ideas to remake NASA that it espouses as well as the manner in which it has been leaked to the space community. After receiving a copy of this plan from an industry official, I spoke with multiple sources over the weekend to understand what is happening. Based upon this reporting there are clearly multiple layers to the story, which I want to unpack.Read full article Comments",
          "feed_position": 8,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/08/news-080625d-lg-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/08/news-080625d-lg-1152x648.jpg",
      "popularity_score": 262.1958666666667,
      "ai_summary": [
        "Capitol Hill is discussing the \"Athena\" plan for NASA.",
        "The plan is related to Isaacman's tenure at NASA.",
        "The Athena plan is a blueprint for the future.",
        "The plan is generating buzz on Capitol Hill.",
        "The plan is a NASA initiative."
      ]
    },
    {
      "id": "cluster_88",
      "coverage": 1,
      "updated_at": "Mon, 03 Nov 2025 14:45:11 +0000",
      "title": "Disruption to science will last longer than the US government shutdown",
      "neutral_headline": "Government shutdown impacts science research and funding",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2025/11/disruption-to-science-will-last-longer-than-the-us-government-shutdown/",
          "published_at": "Mon, 03 Nov 2025 14:45:11 +0000",
          "title": "Disruption to science will last longer than the US government shutdown",
          "standfirst": "The consequences of the shutdown extend far beyond a lapse in funding.",
          "content": "US science always suffers during government shutdowns. Funding lapses send government scientists home without pay. Federal agencies suspend new grant opportunities, place expert review panels on hold, and stop collecting and analyzing critical public datasets that tell us about the economy, the environment, and public health. In 2025, the stakes are higher than in past shutdowns. This shutdown arrives at a time of massive upheaval to American science and innovation driven by President Donald Trump’s ongoing attempts to extend executive power and assert political control of scientific institutions.Read full article Comments",
          "feed_position": 9,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2018/12/getty-us-capitol-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2018/12/getty-us-capitol-1152x648.jpg",
      "popularity_score": 250.05308861111112,
      "ai_summary": [
        "The US government shutdown will have lasting effects on scientific research.",
        "Funding lapses will cause delays and disruptions to ongoing projects.",
        "Researchers face uncertainty regarding future grant applications and renewals.",
        "Data collection and analysis may be hindered by lack of resources.",
        "The shutdown's impact extends beyond immediate financial constraints."
      ]
    },
    {
      "id": "cluster_99",
      "coverage": 1,
      "updated_at": "Mon, 03 Nov 2025 12:00:51 +0000",
      "title": "Internet Archive’s legal fights are over, but its founder mourns what was lost",
      "neutral_headline": "Internet Archive founder reflects on legal battles and loss",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/11/the-internet-archive-survived-major-copyright-losses-whats-next/",
          "published_at": "Mon, 03 Nov 2025 12:00:51 +0000",
          "title": "Internet Archive’s legal fights are over, but its founder mourns what was lost",
          "standfirst": "\"We survived, but it wiped out the library,\" Internet Archive's founder says.",
          "content": "Last month, the Internet Archive’s Wayback Machine archived its trillionth webpage, and the nonprofit invited its more than 1,200 library partners and 800,000 daily users to join a celebration of the moment. To honor “three decades of safeguarding the world’s online heritage,” the city of San Francisco declared October 22 to be “Internet Archive Day.” The Archive was also recently designated a federal depository library by Sen. Alex Padilla (D-Calif.), who proclaimed the organization a “perfect fit” to expand “access to federal government publications amid an increasingly digital landscape.” The Internet Archive might sound like a thriving organization, but it only recently emerged from years of bruising copyright battles that threatened to bankrupt the beloved library project. In the end, the fight led to more than 500,000 books being removed from the Archive’s “Open Library.” “We survived,” Internet Archive founder Brewster Kahle told Ars. “But it wiped out the Library.”Read full article Comments",
          "feed_position": 10,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/2025-10-22-Internet-Archive-1-Trillion-web-pages-375-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/2025-10-22-Internet-Archive-1-Trillion-web-pages-375-1152x648.jpg",
      "popularity_score": 137.3141997222222,
      "ai_summary": [
        "The Internet Archive has concluded its legal battles successfully.",
        "The founder expresses sadness over the loss of the library's content.",
        "Legal challenges significantly impacted the archive's operations.",
        "The archive faced threats to its mission of preserving information.",
        "The founder acknowledges the challenges of digital preservation."
      ]
    }
  ]
}