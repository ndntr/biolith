{
  "updated_at": "2025-10-14T07:19:45.383Z",
  "clusters": [
    {
      "id": "cluster_13",
      "coverage": 2,
      "updated_at": "Mon, 13 Oct 2025 23:30:01 -0400",
      "title": "Researchers used an off-the-shelf system to compile a vast collection of private data, including T-Mobile users' calls and texts, sent by satellites unencrypted (Wired)",
      "neutral_headline": "Researchers used an off-the-shelf system to compile a vast collection of private data, including T-Mobile users' calls and texts, sent by satellites unencrypted (Wired)",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/251013/p50#a251013p50",
          "published_at": "Mon, 13 Oct 2025 23:30:01 -0400",
          "title": "Researchers used an off-the-shelf system to compile a vast collection of private data, including T-Mobile users' calls and texts, sent by satellites unencrypted (Wired)",
          "standfirst": "Wired: Researchers used an off-the-shelf system to compile a vast collection of private data, including T-Mobile users' calls and texts, sent by satellites unencrypted &mdash; With just $800 in basic equipment, researchers found a stunning variety of data&mdash;including thousands of T-Mobile users' calls &hellip;",
          "content": "Wired: Researchers used an off-the-shelf system to compile a vast collection of private data, including T-Mobile users' calls and texts, sent by satellites unencrypted &mdash; With just $800 in basic equipment, researchers found a stunning variety of data&mdash;including thousands of T-Mobile users' calls &hellip;",
          "feed_position": 4,
          "image_url": "http://www.techmeme.com/251013/i50.jpg"
        },
        {
          "source": "Wired Tech",
          "url": "https://www.wired.com/story/satellites-are-leaking-the-worlds-secrets-calls-texts-military-and-corporate-data/",
          "published_at": "Tue, 14 Oct 2025 01:00:00 +0000",
          "title": "Satellites Are Leaking the World’s Secrets: Calls, Texts, Military and Corporate Data",
          "standfirst": "With just $800 in basic equipment, researchers found a stunning variety of data—including thousands of T-Mobile users’ calls and texts and even US military communications—sent by satellites unencrypted.",
          "content": "With just $800 in basic equipment, researchers found a stunning variety of data—including thousands of T-Mobile users’ calls and texts and even US military communications—sent by satellites unencrypted.",
          "feed_position": 0,
          "image_url": "https://media.wired.com/photos/68e961a8dc203222538eb046/master/pass/security_satellites_leak_data.jpg"
        }
      ],
      "featured_image": "http://www.techmeme.com/251013/i50.jpg",
      "popularity_score": 2016.1710047222223,
      "ai_summary": [
        "Researchers used basic equipment to collect data from unencrypted satellites.",
        "They found a variety of data, including T-Mobile users' calls and texts.",
        "The data also included US military communications.",
        "The equipment cost approximately $800.",
        "The findings highlight vulnerabilities in satellite data security."
      ]
    },
    {
      "id": "cluster_30",
      "coverage": 2,
      "updated_at": "Mon, 13 Oct 2025 19:00:01 -0400",
      "title": "Microsoft unveils MAI-Image-1, its first text-to-image AI model developed in house, and says it excels at photorealistic imagery, like lighting and landscapes (Andrew J. Hawkins/The Verge)",
      "neutral_headline": "Microsoft unveils MAI-Image-1, its first text-to-image AI model developed in house, and says it excels at photorealistic imagery, like lighting and landscapes (Andrew J. Hawkins/The Verge)",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/251013/p40#a251013p40",
          "published_at": "Mon, 13 Oct 2025 19:00:01 -0400",
          "title": "Microsoft unveils MAI-Image-1, its first text-to-image AI model developed in house, and says it excels at photorealistic imagery, like lighting and landscapes (Andrew J. Hawkins/The Verge)",
          "standfirst": "Andrew J. Hawkins / The Verge: Microsoft unveils MAI-Image-1, its first text-to-image AI model developed in house, and says it excels at photorealistic imagery, like lighting and landscapes &mdash; The model has already secured a spot in the top 10 of LMArena. &hellip; Microsoft AI just announced its first text-to-image generator &hellip;",
          "content": "Andrew J. Hawkins / The Verge: Microsoft unveils MAI-Image-1, its first text-to-image AI model developed in house, and says it excels at photorealistic imagery, like lighting and landscapes &mdash; The model has already secured a spot in the top 10 of LMArena. &hellip; Microsoft AI just announced its first text-to-image generator &hellip;",
          "feed_position": 14,
          "image_url": "http://www.techmeme.com/251013/i40.jpg"
        },
        {
          "source": "The Verge",
          "url": "https://www.theverge.com/news/798923/microsoft-ai-image-generator-in-house",
          "published_at": "2025-10-13T17:24:05-04:00",
          "title": "Microsoft AI announces first image generator created in-house",
          "standfirst": "Microsoft AI just announced its first text-to-image generator, MAI-Image-1, designed and developed in-house. The tech giant, which recently announced its first in-house Microsoft AI models, called the new image generator “the next step on our journey.” Microsoft says it sought feedback from creative professionals in order to avoid “repetitive or generically-stylized outputs.” MAI-Image-1 “excels” at [&#8230;]",
          "content": "Microsoft AI just announced its first text-to-image generator, MAI-Image-1, designed and developed in-house. The tech giant, which recently announced its first in-house Microsoft AI models, called the new image generator “the next step on our journey.” Microsoft says it sought feedback from creative professionals in order to avoid “repetitive or generically-stylized outputs.” MAI-Image-1 “excels” at photorealistic imagery like lightning, landscapes, and more, the company claims. And it can process requests and produce images faster than “larger, slower models.” The model has already secured a spot in the top 10 of LMArena, the AI benchmark site where humans compare outputs from different systems and vote on the best one. MAI-Image-1 joins Microsoft’s other AI products, voice generator MAI-Voice-1 AI and chatbot MAI-1-preview. The company was an early funder of OpenAI, but the two companies’ relationship has grown increasingly complicated. Microsoft has also started to use Anthropic’s AI models for some features in Microsoft 365 recently, and is making “significant investments” in training its own AI models like MAI-Image-1. We haven’t had a chance to test Microsoft’s new image generator, but once we have we’ll report back on its safety guardrails. The company says it’s committed to “ensuring safe and responsible outcomes.”",
          "feed_position": 2
        }
      ],
      "featured_image": "http://www.techmeme.com/251013/i40.jpg",
      "popularity_score": 2011.6710047222223,
      "ai_summary": [
        "Microsoft announced its first in-house text-to-image AI model, MAI-Image-1.",
        "The model is designed to excel at photorealistic imagery.",
        "Microsoft sought feedback from creative professionals.",
        "The model has already secured a spot in the top 10 of LMArena.",
        "MAI-Image-1 is the next step in Microsoft's AI journey."
      ]
    },
    {
      "id": "cluster_31",
      "coverage": 2,
      "updated_at": "Mon, 13 Oct 2025 22:51:00 GMT",
      "title": "Self-improving language models are becoming reality with MIT's updated SEAL technique",
      "neutral_headline": "Self-improving language models are becoming reality with MIT's updated SEAL technique",
      "items": [
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/self-improving-language-models-are-becoming-reality-with-mits-updated-seal",
          "published_at": "Mon, 13 Oct 2025 22:51:00 GMT",
          "title": "Self-improving language models are becoming reality with MIT's updated SEAL technique",
          "standfirst": "Researchers at the Massachusetts Institute of Technology (MIT) are gaining renewed attention for developing and open sourcing a technique that allows large language models (LLMs) — like those underpinning ChatGPT and most modern AI chatbots — to improve themselves by generating synthetic data to fine-tune upon. The technique, known as SEAL (Self-Adapting LLMs), was first described in a paper published back in June and covered by VentureBeat at the time.A significantly expanded and updated version of the paper was released last month, as well as open source code posted on Github (under an MIT License, allowing for commercial and enterprise usage), and is making new waves among AI power users on the social network X this week.SEAL allows LLMs to autonomously generate and apply their own fine-tuning strategies. Unlike conventional models that rely on fixed external data and human-crafted optimization pipelines, SEAL enables models to evolve by producing their own synthetic training data and corresponding optimization directives.The development comes from a team affiliated with MIT’s Improbable AI Lab, including Adam Zweiger, Jyothish Pari, Han Guo, Ekin Akyürek, Yoon Kim, and Pulkit Agrawal. Their research was recently presented at the 39th Conference on Neural Information Processing Systems (NeurIPS 2025).Background: From “Beyond Static AI” to Self-Adaptive SystemsEarlier this year, VentureBeat first reported on SEAL as an early-stage framework that allowed language models to generate and train on their own synthetic data — a potential remedy for the stagnation of pretrained models once deployed. At that stage, SEAL was framed as a proof-of-concept that could let enterprise AI agents continuously learn in dynamic environments without manual retraining.Since then, the research has advanced considerably. The new version expands on the prior framework by demonstrating that SEAL’s self-adaptation ability scales with model size, integrates reinforcement learning more effectively to reduce catastrophic forgetting, and formalizes SEAL’s dual-loop structure (inner supervised fine-tuning and outer reinforcement optimization) for reproducibility. The updated paper also introduces evaluations across different prompting formats, improved stability during learning cycles, and a discussion of practical deployment challenges at inference time.Addressing the Limitations of Static ModelsWhile LLMs have demonstrated remarkable capabilities in text generation and understanding, their adaptation to new tasks or knowledge is often manual, brittle, or dependent on context. SEAL challenges this status quo by equipping models with the ability to generate what the authors call “self-edits” — natural language outputs that specify how the model should update its weights.These self-edits may take the form of reformulated information, logical implications, or tool configurations for augmentation and training. Once generated, the model fine-tunes itself based on these edits. The process is guided by reinforcement learning, where the reward signal comes from improved performance on a downstream task.The design mimics how human learners might rephrase or reorganize study materials to better internalize information. This restructuring of knowledge before assimilation serves as a key advantage over models that passively consume new data “as-is.”Performance Across TasksSEAL has been tested across two main domains: knowledge incorporation and few-shot learning.In the knowledge incorporation setting, the researchers evaluated how well a model could internalize new factual content from passages similar to those in the SQuAD dataset, a benchmark reading comprehension dataset introduced by Stanford University in 2016, consisting of over 100,000 crowd-sourced question–answer pairs based on Wikipedia articles (Rajpurkar et al., 2016). Rather than fine-tuning directly on passage text, the model generated synthetic implications of the passage and then fine-tuned on them. After two rounds of reinforcement learning, the model improved question-answering accuracy from 33.5% to 47.0% on a no-context version of SQuAD — surpassing results obtained using synthetic data generated by GPT-4.1.In the few-shot learning setting, SEAL was evaluated using a subset of the ARC benchmark, where tasks require reasoning from only a few examples. Here, SEAL generated self-edits specifying data augmentations and hyperparameters. After reinforcement learning, the success rate in correctly solving held-out tasks jumped to 72.5%, up from 20% using self-edits generated without reinforcement learning. Models that relied solely on in-context learning without any adaptation scored 0%.Technical FrameworkSEAL operates using a two-loop structure: an inner loop performs supervised fine-tuning based on the self-edit, while an outer loop uses reinforcement learning to refine the policy that generates those self-edits.The reinforcement learning algorithm used is based on ReSTEM, which combines sampling with filtered behavior cloning. During training, only self-edits that lead to performance improvements are reinforced. This approach effectively teaches the model which kinds of edits are most beneficial for learning.For efficiency, SEAL applies LoRA-based fine-tuning rather than full parameter updates, enabling rapid experimentation and low-cost adaptation.Strengths and LimitationsThe researchers report that SEAL can produce high-utility training data with minimal supervision, outperforming even large external models like GPT-4.1 in specific tasks. They also demonstrate that SEAL generalizes beyond its original setup: it continues to perform well when scaling from single-pass updates to multi-document continued pretraining scenarios.However, the framework is not without limitations. One issue is catastrophic forgetting, where updates to incorporate new information can degrade performance on previously learned tasks. In response to this concern, co-author Jyo Pari told VentureBeat via email that reinforcement learning (RL) appears to mitigate forgetting more effectively than standard supervised fine-tuning (SFT), citing a recent paper on the topic. He added that combining this insight with SEAL could lead to new variants where SEAL learns not just training data, but reward functions.Another challenge is computational overhead: evaluating each self-edit requires fine-tuning and performance testing, which can take 30–45 seconds per edit — significantly more than standard reinforcement learning tasks. As Jyo explained, “Training SEAL is non-trivial because it requires 2 loops of optimization, an outer RL one and an inner SFT one. At inference time, updating model weights will also require new systems infrastructure.” He emphasized the need for future research into deployment systems as a critical path to making SEAL practical.Additionally, SEAL’s current design assumes the presence of paired tasks and reference answers for every context, limiting its direct applicability to unlabeled corpora. However, Jyo clarified that as long as there is a downstream task with a computable reward, SEAL can be trained to adapt accordingly—even in safety-critical domains. In principle, a SEAL-trained model could learn to avoid training on harmful or malicious inputs if guided by the appropriate reward signal.AI Community ReactionsThe AI research and builder community has reacted with a mix of excitement and speculation to the SEAL paper. On X, formerly Twitter, several prominent AI-focused accounts weighed in on the potential impact.User @VraserX, a self-described educator and AI enthusiast, called SEAL “the birth of continuous self-learning AI” and predicted that models like OpenAI&#x27;s GPT-6 could adopt similar architecture. In their words, SEAL represents “the end of the frozen-weights era,” ushering in systems that evolve as the world around them changes. They highlighted SEAL&#x27;s ability to form persistent memories, repair knowledge, and learn from real-time data, comparing it to a foundational step toward models that don’t just use information but absorb it.Meanwhile, @alex_prompter, co-founder of an AI-powered marketing venture, framed SEAL as a leap toward models that literally rewrite themselves. “MIT just built an AI that can rewrite its own code to get smarter,” he wrote. Citing the paper’s key results — a 40% boost in factual recall and outperforming GPT-4.1 using self-generated data — he described the findings as confirmation that “LLMs that finetune themselves are no longer sci-fi.”The enthusiasm reflects a broader appetite in the AI space for models that can evolve without constant retraining or human oversight — particularly in rapidly changing domains or personalized use cases.Future Directions and Open QuestionsIn response to questions about scaling SEAL to larger models and tasks, Jyo pointed to experiments (Appendix B.7) showing that as model size increases, so does their self-adaptation ability. He compared this to students improving their study techniques over time — larger models are simply better at generating useful self-edits.When asked whether SEAL generalizes to new prompting styles, he confirmed it does, citing Table 10 in the paper. However, he also acknowledged that the team has not yet tested SEAL’s ability to transfer across entirely new domains or model architectures. “SEAL is an initial work showcasing the possibilities,” he said. “But it requires much more testing.” He added that generalization may improve as SEAL is trained on a broader distribution of tasks.Interestingly, the team found that only a few reinforcement learning steps already led to measurable performance gains. “This is exciting,” Jyo noted, “because it means that with more compute, we could hopefully get even more improvements.” He suggested future experiments could explore more advanced reinforcement learning methods beyond ReSTEM, such as Group Relative Policy Optimization (GRPO).Toward More Adaptive and Agentic ModelsSEAL represents a step toward models that can autonomously improve over time, both by integrating new knowledge and by reconfiguring how they learn. The authors envision future extensions where SEAL could assist in self-pretraining, continual learning, and the development of agentic systems — models that interact with evolving environments and adapt incrementally.In such settings, a model could use SEAL to synthesize weight updates after each interaction, gradually internalizing behaviors or insights. This could reduce the need for repeated supervision and manual intervention, particularly in data-constrained or specialized domains.As public web text becomes saturated and further scaling of LLMs becomes bottlenecked by data availability, self-directed approaches like SEAL could play a critical role in pushing the boundaries of what LLMs can achieve.You can access the SEAL project, including code and further documentation, at: https://jyopari.github.io/posts/seal",
          "content": "Researchers at the Massachusetts Institute of Technology (MIT) are gaining renewed attention for developing and open sourcing a technique that allows large language models (LLMs) — like those underpinning ChatGPT and most modern AI chatbots — to improve themselves by generating synthetic data to fine-tune upon. The technique, known as SEAL (Self-Adapting LLMs), was first described in a paper published back in June and covered by VentureBeat at the time.A significantly expanded and updated version of the paper was released last month, as well as open source code posted on Github (under an MIT License, allowing for commercial and enterprise usage), and is making new waves among AI power users on the social network X this week.SEAL allows LLMs to autonomously generate and apply their own fine-tuning strategies. Unlike conventional models that rely on fixed external data and human-crafted optimization pipelines, SEAL enables models to evolve by producing their own synthetic training data and corresponding optimization directives.The development comes from a team affiliated with MIT’s Improbable AI Lab, including Adam Zweiger, Jyothish Pari, Han Guo, Ekin Akyürek, Yoon Kim, and Pulkit Agrawal. Their research was recently presented at the 39th Conference on Neural Information Processing Systems (NeurIPS 2025).Background: From “Beyond Static AI” to Self-Adaptive SystemsEarlier this year, VentureBeat first reported on SEAL as an early-stage framework that allowed language models to generate and train on their own synthetic data — a potential remedy for the stagnation of pretrained models once deployed. At that stage, SEAL was framed as a proof-of-concept that could let enterprise AI agents continuously learn in dynamic environments without manual retraining.Since then, the research has advanced considerably. The new version expands on the prior framework by demonstrating that SEAL’s self-adaptation ability scales with model size, integrates reinforcement learning more effectively to reduce catastrophic forgetting, and formalizes SEAL’s dual-loop structure (inner supervised fine-tuning and outer reinforcement optimization) for reproducibility. The updated paper also introduces evaluations across different prompting formats, improved stability during learning cycles, and a discussion of practical deployment challenges at inference time.Addressing the Limitations of Static ModelsWhile LLMs have demonstrated remarkable capabilities in text generation and understanding, their adaptation to new tasks or knowledge is often manual, brittle, or dependent on context. SEAL challenges this status quo by equipping models with the ability to generate what the authors call “self-edits” — natural language outputs that specify how the model should update its weights.These self-edits may take the form of reformulated information, logical implications, or tool configurations for augmentation and training. Once generated, the model fine-tunes itself based on these edits. The process is guided by reinforcement learning, where the reward signal comes from improved performance on a downstream task.The design mimics how human learners might rephrase or reorganize study materials to better internalize information. This restructuring of knowledge before assimilation serves as a key advantage over models that passively consume new data “as-is.”Performance Across TasksSEAL has been tested across two main domains: knowledge incorporation and few-shot learning.In the knowledge incorporation setting, the researchers evaluated how well a model could internalize new factual content from passages similar to those in the SQuAD dataset, a benchmark reading comprehension dataset introduced by Stanford University in 2016, consisting of over 100,000 crowd-sourced question–answer pairs based on Wikipedia articles (Rajpurkar et al., 2016). Rather than fine-tuning directly on passage text, the model generated synthetic implications of the passage and then fine-tuned on them. After two rounds of reinforcement learning, the model improved question-answering accuracy from 33.5% to 47.0% on a no-context version of SQuAD — surpassing results obtained using synthetic data generated by GPT-4.1.In the few-shot learning setting, SEAL was evaluated using a subset of the ARC benchmark, where tasks require reasoning from only a few examples. Here, SEAL generated self-edits specifying data augmentations and hyperparameters. After reinforcement learning, the success rate in correctly solving held-out tasks jumped to 72.5%, up from 20% using self-edits generated without reinforcement learning. Models that relied solely on in-context learning without any adaptation scored 0%.Technical FrameworkSEAL operates using a two-loop structure: an inner loop performs supervised fine-tuning based on the self-edit, while an outer loop uses reinforcement learning to refine the policy that generates those self-edits.The reinforcement learning algorithm used is based on ReSTEM, which combines sampling with filtered behavior cloning. During training, only self-edits that lead to performance improvements are reinforced. This approach effectively teaches the model which kinds of edits are most beneficial for learning.For efficiency, SEAL applies LoRA-based fine-tuning rather than full parameter updates, enabling rapid experimentation and low-cost adaptation.Strengths and LimitationsThe researchers report that SEAL can produce high-utility training data with minimal supervision, outperforming even large external models like GPT-4.1 in specific tasks. They also demonstrate that SEAL generalizes beyond its original setup: it continues to perform well when scaling from single-pass updates to multi-document continued pretraining scenarios.However, the framework is not without limitations. One issue is catastrophic forgetting, where updates to incorporate new information can degrade performance on previously learned tasks. In response to this concern, co-author Jyo Pari told VentureBeat via email that reinforcement learning (RL) appears to mitigate forgetting more effectively than standard supervised fine-tuning (SFT), citing a recent paper on the topic. He added that combining this insight with SEAL could lead to new variants where SEAL learns not just training data, but reward functions.Another challenge is computational overhead: evaluating each self-edit requires fine-tuning and performance testing, which can take 30–45 seconds per edit — significantly more than standard reinforcement learning tasks. As Jyo explained, “Training SEAL is non-trivial because it requires 2 loops of optimization, an outer RL one and an inner SFT one. At inference time, updating model weights will also require new systems infrastructure.” He emphasized the need for future research into deployment systems as a critical path to making SEAL practical.Additionally, SEAL’s current design assumes the presence of paired tasks and reference answers for every context, limiting its direct applicability to unlabeled corpora. However, Jyo clarified that as long as there is a downstream task with a computable reward, SEAL can be trained to adapt accordingly—even in safety-critical domains. In principle, a SEAL-trained model could learn to avoid training on harmful or malicious inputs if guided by the appropriate reward signal.AI Community ReactionsThe AI research and builder community has reacted with a mix of excitement and speculation to the SEAL paper. On X, formerly Twitter, several prominent AI-focused accounts weighed in on the potential impact.User @VraserX, a self-described educator and AI enthusiast, called SEAL “the birth of continuous self-learning AI” and predicted that models like OpenAI&#x27;s GPT-6 could adopt similar architecture. In their words, SEAL represents “the end of the frozen-weights era,” ushering in systems that evolve as the world around them changes. They highlighted SEAL&#x27;s ability to form persistent memories, repair knowledge, and learn from real-time data, comparing it to a foundational step toward models that don’t just use information but absorb it.Meanwhile, @alex_prompter, co-founder of an AI-powered marketing venture, framed SEAL as a leap toward models that literally rewrite themselves. “MIT just built an AI that can rewrite its own code to get smarter,” he wrote. Citing the paper’s key results — a 40% boost in factual recall and outperforming GPT-4.1 using self-generated data — he described the findings as confirmation that “LLMs that finetune themselves are no longer sci-fi.”The enthusiasm reflects a broader appetite in the AI space for models that can evolve without constant retraining or human oversight — particularly in rapidly changing domains or personalized use cases.Future Directions and Open QuestionsIn response to questions about scaling SEAL to larger models and tasks, Jyo pointed to experiments (Appendix B.7) showing that as model size increases, so does their self-adaptation ability. He compared this to students improving their study techniques over time — larger models are simply better at generating useful self-edits.When asked whether SEAL generalizes to new prompting styles, he confirmed it does, citing Table 10 in the paper. However, he also acknowledged that the team has not yet tested SEAL’s ability to transfer across entirely new domains or model architectures. “SEAL is an initial work showcasing the possibilities,” he said. “But it requires much more testing.” He added that generalization may improve as SEAL is trained on a broader distribution of tasks.Interestingly, the team found that only a few reinforcement learning steps already led to measurable performance gains. “This is exciting,” Jyo noted, “because it means that with more compute, we could hopefully get even more improvements.” He suggested future experiments could explore more advanced reinforcement learning methods beyond ReSTEM, such as Group Relative Policy Optimization (GRPO).Toward More Adaptive and Agentic ModelsSEAL represents a step toward models that can autonomously improve over time, both by integrating new knowledge and by reconfiguring how they learn. The authors envision future extensions where SEAL could assist in self-pretraining, continual learning, and the development of agentic systems — models that interact with evolving environments and adapt incrementally.In such settings, a model could use SEAL to synthesize weight updates after each interaction, gradually internalizing behaviors or insights. This could reduce the need for repeated supervision and manual intervention, particularly in data-constrained or specialized domains.As public web text becomes saturated and further scaling of LLMs becomes bottlenecked by data availability, self-directed approaches like SEAL could play a critical role in pushing the boundaries of what LLMs can achieve.You can access the SEAL project, including code and further documentation, at: https://jyopari.github.io/posts/seal",
          "feed_position": 0,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/4816Y0YfsXKIENLGFsuaG6/a4620bd99d25c8fe32ab054bd16ff390/cfr0z3n_a_cybernetic_seal_looks_up_with_cute_alert_eyes_under_a_a6f43d56-7792-4d4f-bc1e-18b6dd2f5e4e.png"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/researchers-find-that-retraining-only-small-parts-of-ai-models-can-cut-costs",
          "published_at": "Mon, 13 Oct 2025 22:39:00 GMT",
          "title": "Researchers find that retraining only small parts of AI models can cut costs and prevent forgetting",
          "standfirst": "Enterprises often find that when they fine-tune models, one effective approach to making a large language model (LLM) fit for purpose and grounded in data is to have the model lose some of its abilities. After fine-tuning, some models “forget” how to perform certain tasks or other tasks they already learned. Research from the University of Illinois Urbana-Champaign proposes a new method for retraining models that avoids “catastrophic forgetting,” in which the model loses some of its prior knowledge. The paper focuses on two specific LLMs that generate responses from images: LLaVA and Qwen 2.5-VL.The approach encourages enterprises to retrain only narrow parts of an LLM to avoid retraining the entire model and incurring a significant increase in compute costs. The team claims that catastrophic forgetting isn’t true memory loss, but rather a side effect of bias drift. “Training a new LMM can cost millions of dollars, weeks of time, and emit hundreds of tons of CO2, so finding ways to more efficiently and effectively update existing models is a pressing concern,” the team wrote in the paper. “Guided by this result, we explore tuning recipes that preserve learning while limiting output shift.”The researchers focused on a multi-layer perceptron (MLP), the model&#x27;s internal decision-making component. Catastrophic forgetting The researchers wanted first to verify the existence and the cause of catastrophic forgetting in models. To do this, they created a set of target tasks for the models to complete. The models were then fine-tuned and evaluated to determine whether they led to substantial forgetting. But as the process went on, the researchers found that the models were recovering some of their abilities. “We also noticed a surprising result, that the model performance would drop significantly in held out benchmarks after training on the counting task, it would mostly recover on PathVQA, another specialized task that is not well represented in the benchmarks,” they said. “Meanwhile, while performing the forgetting mitigation experiments, we also tried separately tuning only the self-attention projection (SA Proj) or MLP layers, motivated by the finding that tuning only the LLM was generally better than tuning the full model. This led to another very surprising result – that tuning only self-attention projection layers led to very good learning of the target tasks with no drop in performance in held out tasks, even after training all five target tasks in a sequence.”The researchers said they believe that “what looks like forgetting or interference after fine-tuning on a narrow target task is actually bias in the output distribution due to the task distribution shift.”Narrow retrainingThat finding turned out to be the key to the experiment. The researchers noted that tuning the MLP increases the likelihood of “outputting numeric tokens and a highly correlated drop in held out task accuracy.” What it showed is that a model forgetting some of its knowledge is only temporary and not a long-term matter. “To avoid biasing the output distribution, we tune the MLP up/gating projections while keeping the down projection frozen, and find that it achieves similar learning to full MLP tuning with little forgetting,” the researchers said. This allows for a more straightforward and more reproducible method for fine-tuning a model. By focusing on a narrow segment of the model, rather than a wholesale retraining, enterprises can cut compute costs. It also allows better control of output drift. However, the research focuses only on two models, specifically those dealing with vision and language. The researchers noted that due to limited resources, they are unable to try the experiment with other models.Their findings, however, can be extended to other LLMs, especially for different modalities.",
          "content": "Enterprises often find that when they fine-tune models, one effective approach to making a large language model (LLM) fit for purpose and grounded in data is to have the model lose some of its abilities. After fine-tuning, some models “forget” how to perform certain tasks or other tasks they already learned. Research from the University of Illinois Urbana-Champaign proposes a new method for retraining models that avoids “catastrophic forgetting,” in which the model loses some of its prior knowledge. The paper focuses on two specific LLMs that generate responses from images: LLaVA and Qwen 2.5-VL.The approach encourages enterprises to retrain only narrow parts of an LLM to avoid retraining the entire model and incurring a significant increase in compute costs. The team claims that catastrophic forgetting isn’t true memory loss, but rather a side effect of bias drift. “Training a new LMM can cost millions of dollars, weeks of time, and emit hundreds of tons of CO2, so finding ways to more efficiently and effectively update existing models is a pressing concern,” the team wrote in the paper. “Guided by this result, we explore tuning recipes that preserve learning while limiting output shift.”The researchers focused on a multi-layer perceptron (MLP), the model&#x27;s internal decision-making component. Catastrophic forgetting The researchers wanted first to verify the existence and the cause of catastrophic forgetting in models. To do this, they created a set of target tasks for the models to complete. The models were then fine-tuned and evaluated to determine whether they led to substantial forgetting. But as the process went on, the researchers found that the models were recovering some of their abilities. “We also noticed a surprising result, that the model performance would drop significantly in held out benchmarks after training on the counting task, it would mostly recover on PathVQA, another specialized task that is not well represented in the benchmarks,” they said. “Meanwhile, while performing the forgetting mitigation experiments, we also tried separately tuning only the self-attention projection (SA Proj) or MLP layers, motivated by the finding that tuning only the LLM was generally better than tuning the full model. This led to another very surprising result – that tuning only self-attention projection layers led to very good learning of the target tasks with no drop in performance in held out tasks, even after training all five target tasks in a sequence.”The researchers said they believe that “what looks like forgetting or interference after fine-tuning on a narrow target task is actually bias in the output distribution due to the task distribution shift.”Narrow retrainingThat finding turned out to be the key to the experiment. The researchers noted that tuning the MLP increases the likelihood of “outputting numeric tokens and a highly correlated drop in held out task accuracy.” What it showed is that a model forgetting some of its knowledge is only temporary and not a long-term matter. “To avoid biasing the output distribution, we tune the MLP up/gating projections while keeping the down projection frozen, and find that it achieves similar learning to full MLP tuning with little forgetting,” the researchers said. This allows for a more straightforward and more reproducible method for fine-tuning a model. By focusing on a narrow segment of the model, rather than a wholesale retraining, enterprises can cut compute costs. It also allows better control of output drift. However, the research focuses only on two models, specifically those dealing with vision and language. The researchers noted that due to limited resources, they are unable to try the experiment with other models.Their findings, however, can be extended to other LLMs, especially for different modalities.",
          "feed_position": 1,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/wvhlzgUGzNikEzmtJxvNB/ed69fb909b090e1e7e6b81ff61abf8b0/crimedy7_illustration_of_a_sculptor_creating_a_robot_from_a_p_501bf165-0b44-4bb1-9608-1025a42400b7_1.png"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/computing/windows-10-support-ends-tomorrow-but-heres-how-to-get-an-extra-year-for-free-125118505.html",
          "published_at": "Mon, 13 Oct 2025 14:25:30 +0000",
          "title": "Windows 10 support ends tomorrow, but here's how to get an extra year for free",
          "standfirst": "You'll get access to Windows 10 a little longer by doing this. (Getty Images) Are you still running Windows 10 on your PC? Starting tomorrow, October 14, Microsoft is moving the software to \"end of life\" status. What does that mean for you? While Windows 10 PCs will continue to work after that date, they'll stop getting important security updates by default. That leaves you with three choices if you want to make sure your computer remains secure: You can choose to upgrade to Windows 11 for free if your computer is compatible. You can buy a new PC that already has Windows 11 pre-installed (or opt for an alternative, like a Mac or a Chromebook). Forget about Windows 11 right now and sign up for the Extended Security Updates (ESU), which lets you kick the can down the road for a year. The last one is easy — and can now be done for free in many cases — so we'll focus on that one here. We'll walk you through the steps of keeping Windows 10 on your PC… for now, at least. How to sign up for Windows 10 Extended Security Updates on your computer We can question Microsoft's motives for killing off Windows 10, even though it works perfectly well on most older PCs. But without those periodic security updates, your PC will become increasingly susceptible to malware with each passing week. To that end, enrolling in Extended Security Updates (ESU) will give you another year of using Windows 10 securely. At one point, Microsoft suggested the 12-month extension would require a $30 fee. While that's still an option, there's now a free path for Windows 10 users in the US. Here's how to make it happen. Step 1: Make sure your PC is up to date You can find out if your computer is up-to-date by going into your Settings > System > About, then scroll down to see what version you're running. If not, you'll want to make sure you also install all the Windows 10 updates available. Step 2: Make sure you're using an administrator account If you share a computer with multiple people in your household, make sure you're signed in to the administrator account. Typically, it's the first account created on the computer. You'll know it's the right one when you see \"Administrator\" under the name. (You can double-check under Settings > Your Info.) Step 3: Verify if your PC is eligible to upgrade to Windows 11 (or not) If you see an option to upgrade to Windows 11, just do that. It's free and it keeps you in the Windows loop. Otherwise, continue following the steps below so you can keep your computer safe with security updates. Step 4: Enroll in Extended Security Updates Sign up for ESU by selecting Update & Security from the Settings menu. Click the \"Enroll Now\" sign-up link, as pictured below. Again, you may see an option to download Windows 11 if your computer meets the requirements (again, definitely do that if you see it). Find out if you need to update your computer. (Screenshot/Engadget) If you're not seeing the \"Enroll now\" link, you probably need to update and install the latest Windows 10 updates (as noted above). By enrolling in Extended Security Updates, you'll have another year before you need to upgrade to Windows 11. (Screenshots/Engadget) Step 5: Choose your upgrade method Next up is choosing how you want to enroll, and you have a few options. The easiest way is to back up your PC settings. It's free, but it takes a little bit of time since you'll need to back up your data. Again, you'll need to be using your administrator account to get started. Back up your PC before you enroll in ESU. (ExplainingComputers via YouTube) That said, the free option here comes with two catches, at least for users in the US. (European users will get the free option with no strings attached.) The first is that you'll be linking your Windows login to Microsoft's cloud-based online service. Most users have likely already done this (if they're using CoPilot, Office 365, GamePass, OneDrive or one of Microsoft's other various online services). But if you've specifically opted for a local login to Windows, the price you're paying for this \"free\" extension is joining the cloud-connected Microsoft universe. The other potential issue is that the free backup only applies to the first 5 GB of storage. Anything more, and you’ll need to pay up for Microsoft's OneDrive services. But thankfully, you can turn off anything you don't want to back up by going to Settings > OneDrive and toggling off options like Documents, Pictures and Videos to get in under the free threshold to start. Once you're signed in, a window will pop up that says \"Add this device to receive Extended Security Updates.\" Click Add Device to enroll it. Click Done. A note: Thanks to YouTube's Explaining Computers channel, where we grabbed the screenshot above (since our test PC was already signed up for cloud backups, and didn't provide the splash screen to choose options). You can watch their full video if you'd like a deeper dive into the process. That's it, you're done! (Until next year) You've got 12 more months to figure out an alternative upgrade path to Windows 11. If anything changes next year, we'll update this story with what your next steps are. You did it right if you see this window. (Screenshot/Engadget) This article originally appeared on Engadget at https://www.engadget.com/computing/windows-10-support-ends-tomorrow-but-heres-how-to-get-an-extra-year-for-free-125118505.html?src=rss",
          "content": "You'll get access to Windows 10 a little longer by doing this. (Getty Images) Are you still running Windows 10 on your PC? Starting tomorrow, October 14, Microsoft is moving the software to \"end of life\" status. What does that mean for you? While Windows 10 PCs will continue to work after that date, they'll stop getting important security updates by default. That leaves you with three choices if you want to make sure your computer remains secure: You can choose to upgrade to Windows 11 for free if your computer is compatible. You can buy a new PC that already has Windows 11 pre-installed (or opt for an alternative, like a Mac or a Chromebook). Forget about Windows 11 right now and sign up for the Extended Security Updates (ESU), which lets you kick the can down the road for a year. The last one is easy — and can now be done for free in many cases — so we'll focus on that one here. We'll walk you through the steps of keeping Windows 10 on your PC… for now, at least. How to sign up for Windows 10 Extended Security Updates on your computer We can question Microsoft's motives for killing off Windows 10, even though it works perfectly well on most older PCs. But without those periodic security updates, your PC will become increasingly susceptible to malware with each passing week. To that end, enrolling in Extended Security Updates (ESU) will give you another year of using Windows 10 securely. At one point, Microsoft suggested the 12-month extension would require a $30 fee. While that's still an option, there's now a free path for Windows 10 users in the US. Here's how to make it happen. Step 1: Make sure your PC is up to date You can find out if your computer is up-to-date by going into your Settings > System > About, then scroll down to see what version you're running. If not, you'll want to make sure you also install all the Windows 10 updates available. Step 2: Make sure you're using an administrator account If you share a computer with multiple people in your household, make sure you're signed in to the administrator account. Typically, it's the first account created on the computer. You'll know it's the right one when you see \"Administrator\" under the name. (You can double-check under Settings > Your Info.) Step 3: Verify if your PC is eligible to upgrade to Windows 11 (or not) If you see an option to upgrade to Windows 11, just do that. It's free and it keeps you in the Windows loop. Otherwise, continue following the steps below so you can keep your computer safe with security updates. Step 4: Enroll in Extended Security Updates Sign up for ESU by selecting Update & Security from the Settings menu. Click the \"Enroll Now\" sign-up link, as pictured below. Again, you may see an option to download Windows 11 if your computer meets the requirements (again, definitely do that if you see it). Find out if you need to update your computer. (Screenshot/Engadget) If you're not seeing the \"Enroll now\" link, you probably need to update and install the latest Windows 10 updates (as noted above). By enrolling in Extended Security Updates, you'll have another year before you need to upgrade to Windows 11. (Screenshots/Engadget) Step 5: Choose your upgrade method Next up is choosing how you want to enroll, and you have a few options. The easiest way is to back up your PC settings. It's free, but it takes a little bit of time since you'll need to back up your data. Again, you'll need to be using your administrator account to get started. Back up your PC before you enroll in ESU. (ExplainingComputers via YouTube) That said, the free option here comes with two catches, at least for users in the US. (European users will get the free option with no strings attached.) The first is that you'll be linking your Windows login to Microsoft's cloud-based online service. Most users have likely already done this (if they're using CoPilot, Office 365, GamePass, OneDrive or one of Microsoft's other various online services). But if you've specifically opted for a local login to Windows, the price you're paying for this \"free\" extension is joining the cloud-connected Microsoft universe. The other potential issue is that the free backup only applies to the first 5 GB of storage. Anything more, and you’ll need to pay up for Microsoft's OneDrive services. But thankfully, you can turn off anything you don't want to back up by going to Settings > OneDrive and toggling off options like Documents, Pictures and Videos to get in under the free threshold to start. Once you're signed in, a window will pop up that says \"Add this device to receive Extended Security Updates.\" Click Add Device to enroll it. Click Done. A note: Thanks to YouTube's Explaining Computers channel, where we grabbed the screenshot above (since our test PC was already signed up for cloud backups, and didn't provide the splash screen to choose options). You can watch their full video if you'd like a deeper dive into the process. That's it, you're done! (Until next year) You've got 12 more months to figure out an alternative upgrade path to Windows 11. If anything changes next year, we'll update this story with what your next steps are. You did it right if you see this window. (Screenshot/Engadget) This article originally appeared on Engadget at https://www.engadget.com/computing/windows-10-support-ends-tomorrow-but-heres-how-to-get-an-extra-year-for-free-125118505.html?src=rss",
          "feed_position": 11,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2025-10/c836b6e0-a60d-11f0-aff0-71a091f199fd"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/this-new-ai-technique-creates-digital-twin-consumers-and-it-could-kill-the",
          "published_at": "Mon, 13 Oct 2025 13:00:00 GMT",
          "title": "This new AI technique creates ‘digital twin’ consumers, and it could kill the traditional survey industry",
          "standfirst": "A new research paper quietly published last week outlines a breakthrough method that allows large language models (LLMs) to simulate human consumer behavior with startling accuracy, a development that could reshape the multi-billion-dollar market research industry. The technique promises to create armies of synthetic consumers who can provide not just realistic product ratings, but also the qualitative reasoning behind them, at a scale and speed currently unattainable.For years, companies have sought to use AI for market research, but have been stymied by a fundamental flaw: when asked to provide a numerical rating on a scale of 1 to 5, LLMs produce unrealistic and poorly distributed responses. A new paper, \"LLMs Reproduce Human Purchase Intent via Semantic Similarity Elicitation of Likert Ratings,\" submitted to the pre-print server arXiv on October 9th proposes an elegant solution that sidesteps this problem entirely.The international team of researchers, led by Benjamin F. Maier, developed a method they call semantic similarity rating (SSR). Instead of asking an LLM for a number, SSR prompts the model for a rich, textual opinion on a product. This text is then converted into a numerical vector — an \"embedding\" — and its similarity is measured against a set of pre-defined reference statements. For example, a response of \"I would absolutely buy this, it&#x27;s exactly what I&#x27;m looking for\" would be semantically closer to the reference statement for a \"5\" rating than to the statement for a \"1.\"The results are striking. Tested against a massive real-world dataset from a leading personal care corporation — comprising 57 product surveys and 9,300 human responses — the SSR method achieved 90% of human test-retest reliability. Crucially, the distribution of AI-generated ratings was statistically almost indistinguishable from the human panel. The authors state, \"This framework enables scalable consumer research simulations while preserving traditional survey metrics and interpretability.\"A timely solution as AI threatens survey integrityThis development arrives at a critical time, as the integrity of traditional online survey panels is increasingly under threat from AI. A 2024 analysis from the Stanford Graduate School of Business highlighted a growing problem of human survey-takers using chatbots to generate their answers. These AI-generated responses were found to be \"suspiciously nice,\" overly verbose, and lacking the \"snark\" and authenticity of genuine human feedback, leading to what researchers called a \"homogenization\" of data that could mask serious issues like discrimination or product flaws.Maier&#x27;s research offers a starkly different approach: instead of fighting to purge contaminated data, it creates a controlled environment for generating high-fidelity synthetic data from the ground up.\"What we&#x27;re seeing is a pivot from defense to offense,\" said one analyst not affiliated with the study. \"The Stanford paper showed the chaos of uncontrolled AI polluting human datasets. This new paper shows the order and utility of controlled AI creating its own datasets. For a Chief Data Officer, this is the difference between cleaning a contaminated well and tapping into a fresh spring.\"From text to intent: The technical leap behind the synthetic consumerThe technical validity of the new method hinges on the quality of the text embeddings, a concept explored in a 2022 paper in EPJ Data Science. That research argued for a rigorous \"construct validity\" framework to ensure that text embeddings — the numerical representations of text — truly \"measure what they are supposed to.\" The success of the SSR method suggests its embeddings effectively capture the nuances of purchase intent. For this new technique to be widely adopted, enterprises will need to be confident that the underlying models are not just generating plausible text, but are mapping that text to scores in a way that is robust and meaningful.The approach also represents a significant leap from prior research, which has largely focused on using text embeddings to analyze and predict ratings from existing online reviews. A 2022 study, for example, evaluated the performance of models like BERT and word2vec in predicting review scores on retail sites, finding that newer models like BERT performed better for general use. The new research moves beyond analyzing existing data to generating novel, predictive insights before a product even hits the market.The dawn of the digital focus groupFor technical decision-makers, the implications are profound. The ability to spin up a \"digital twin\" of a target consumer segment and test product concepts, ad copy, or packaging variations in a matter of hours could drastically accelerate innovation cycles. As the paper notes, these synthetic respondents also provide \"rich qualitative feedback explaining their ratings,\" offering a treasure trove of data for product development that is both scalable and interpretable. While the era of human-only focus groups is far from over, this research provides the most compelling evidence yet that their synthetic counterparts are ready for business.But the business case extends beyond speed and scale. Consider the economics: a traditional survey panel for a national product launch might cost tens of thousands of dollars and take weeks to field. An SSR-based simulation could deliver comparable insights in a fraction of the time, at a fraction of the cost, and with the ability to iterate instantly based on findings. For companies in fast-moving consumer goods categories — where the window between concept and shelf can determine market leadership — this velocity advantage could be decisive.There are, of course, caveats. The method was validated on personal care products; its performance on complex B2B purchasing decisions, luxury goods, or culturally specific products remains unproven. And while the paper demonstrates that SSR can replicate aggregate human behavior, it does not claim to predict individual consumer choices. The technique works at the population level, not the person level — a distinction that matters greatly for applications like personalized marketing.Yet even with these limitations, the research is a watershed. While the era of human-only focus groups is far from over, this paper provides the most compelling evidence yet that their synthetic counterparts are ready for business. The question is no longer whether AI can simulate consumer sentiment, but whether enterprises can move fast enough to capitalize on it before their competitors do.",
          "content": "A new research paper quietly published last week outlines a breakthrough method that allows large language models (LLMs) to simulate human consumer behavior with startling accuracy, a development that could reshape the multi-billion-dollar market research industry. The technique promises to create armies of synthetic consumers who can provide not just realistic product ratings, but also the qualitative reasoning behind them, at a scale and speed currently unattainable.For years, companies have sought to use AI for market research, but have been stymied by a fundamental flaw: when asked to provide a numerical rating on a scale of 1 to 5, LLMs produce unrealistic and poorly distributed responses. A new paper, \"LLMs Reproduce Human Purchase Intent via Semantic Similarity Elicitation of Likert Ratings,\" submitted to the pre-print server arXiv on October 9th proposes an elegant solution that sidesteps this problem entirely.The international team of researchers, led by Benjamin F. Maier, developed a method they call semantic similarity rating (SSR). Instead of asking an LLM for a number, SSR prompts the model for a rich, textual opinion on a product. This text is then converted into a numerical vector — an \"embedding\" — and its similarity is measured against a set of pre-defined reference statements. For example, a response of \"I would absolutely buy this, it&#x27;s exactly what I&#x27;m looking for\" would be semantically closer to the reference statement for a \"5\" rating than to the statement for a \"1.\"The results are striking. Tested against a massive real-world dataset from a leading personal care corporation — comprising 57 product surveys and 9,300 human responses — the SSR method achieved 90% of human test-retest reliability. Crucially, the distribution of AI-generated ratings was statistically almost indistinguishable from the human panel. The authors state, \"This framework enables scalable consumer research simulations while preserving traditional survey metrics and interpretability.\"A timely solution as AI threatens survey integrityThis development arrives at a critical time, as the integrity of traditional online survey panels is increasingly under threat from AI. A 2024 analysis from the Stanford Graduate School of Business highlighted a growing problem of human survey-takers using chatbots to generate their answers. These AI-generated responses were found to be \"suspiciously nice,\" overly verbose, and lacking the \"snark\" and authenticity of genuine human feedback, leading to what researchers called a \"homogenization\" of data that could mask serious issues like discrimination or product flaws.Maier&#x27;s research offers a starkly different approach: instead of fighting to purge contaminated data, it creates a controlled environment for generating high-fidelity synthetic data from the ground up.\"What we&#x27;re seeing is a pivot from defense to offense,\" said one analyst not affiliated with the study. \"The Stanford paper showed the chaos of uncontrolled AI polluting human datasets. This new paper shows the order and utility of controlled AI creating its own datasets. For a Chief Data Officer, this is the difference between cleaning a contaminated well and tapping into a fresh spring.\"From text to intent: The technical leap behind the synthetic consumerThe technical validity of the new method hinges on the quality of the text embeddings, a concept explored in a 2022 paper in EPJ Data Science. That research argued for a rigorous \"construct validity\" framework to ensure that text embeddings — the numerical representations of text — truly \"measure what they are supposed to.\" The success of the SSR method suggests its embeddings effectively capture the nuances of purchase intent. For this new technique to be widely adopted, enterprises will need to be confident that the underlying models are not just generating plausible text, but are mapping that text to scores in a way that is robust and meaningful.The approach also represents a significant leap from prior research, which has largely focused on using text embeddings to analyze and predict ratings from existing online reviews. A 2022 study, for example, evaluated the performance of models like BERT and word2vec in predicting review scores on retail sites, finding that newer models like BERT performed better for general use. The new research moves beyond analyzing existing data to generating novel, predictive insights before a product even hits the market.The dawn of the digital focus groupFor technical decision-makers, the implications are profound. The ability to spin up a \"digital twin\" of a target consumer segment and test product concepts, ad copy, or packaging variations in a matter of hours could drastically accelerate innovation cycles. As the paper notes, these synthetic respondents also provide \"rich qualitative feedback explaining their ratings,\" offering a treasure trove of data for product development that is both scalable and interpretable. While the era of human-only focus groups is far from over, this research provides the most compelling evidence yet that their synthetic counterparts are ready for business.But the business case extends beyond speed and scale. Consider the economics: a traditional survey panel for a national product launch might cost tens of thousands of dollars and take weeks to field. An SSR-based simulation could deliver comparable insights in a fraction of the time, at a fraction of the cost, and with the ability to iterate instantly based on findings. For companies in fast-moving consumer goods categories — where the window between concept and shelf can determine market leadership — this velocity advantage could be decisive.There are, of course, caveats. The method was validated on personal care products; its performance on complex B2B purchasing decisions, luxury goods, or culturally specific products remains unproven. And while the paper demonstrates that SSR can replicate aggregate human behavior, it does not claim to predict individual consumer choices. The technique works at the population level, not the person level — a distinction that matters greatly for applications like personalized marketing.Yet even with these limitations, the research is a watershed. While the era of human-only focus groups is far from over, this paper provides the most compelling evidence yet that their synthetic counterparts are ready for business. The question is no longer whether AI can simulate consumer sentiment, but whether enterprises can move fast enough to capitalize on it before their competitors do.",
          "feed_position": 2,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/51ORdJ4l6mg6ZSTkJpGFOw/85fa4483c879359d1872778236f4fc20/nuneybits_Vector_art_of_AI_shopper_silhouette_e82a806b-f70d-4b1d-9418-71b2ffd65ea4.webp"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/salesforce-bets-on-ai-agents-to-fix-what-it-calls-a-usd7-billion-problem-in",
          "published_at": "Mon, 13 Oct 2025 12:00:00 GMT",
          "title": "Salesforce bets on AI 'agents' to fix what it calls a $7 billion problem in enterprise software",
          "standfirst": "As 50,000 attendees descend on Salesforce&#x27;s Dreamforce conference this week, the enterprise software giant is making its most aggressive bet yet on artificial intelligence agents, positioning itself as the antidote to what it calls an industry-wide \"pilot purgatory\" where 95% of enterprise AI projects never reach production.The company on Monday launched Agentforce 360, a sweeping reimagination of its entire product portfolio designed to transform businesses into what it calls \"agentic enterprises\" — organizations where AI agents work alongside humans to handle up to 40% of work across sales, service, marketing, and operations.\"We are truly in the agentic AI era, and I think it&#x27;s probably the biggest revolution, the biggest transition in technology I&#x27;ve ever experienced in my career,\" said Parker Harris, Salesforce&#x27;s co-founder and chief technology officer, during a recent press briefing. \"In the future, 40% of the work in the Fortune 1000 is probably going to be done by AI, and it&#x27;s going to be humans and AI actually working together.\"The announcement comes at a pivotal moment for Salesforce, which has deployed more than 12,000 AI agent implementations over the past year while building what Harris called a \"$7 billion business\" around its AI platform. Yet the launch also arrives amid unusual turbulence, as CEO Marc Benioff faces fierce backlash for recent comments supporting President Trump and suggesting National Guard troops should patrol San Francisco streets.Why 95% of enterprise AI projects never launchThe stakes are enormous. While companies have rushed to experiment with AI following ChatGPT&#x27;s emergence two years ago, most enterprise deployments have stalled before reaching production, according to recent MIT research that Salesforce executives cited extensively.\"Customers have invested a lot in AI, but they&#x27;re not getting the value,\" said Srini Tallapragada, Salesforce&#x27;s president and chief engineering and customer success officer. \"95% of enterprise AI pilots fail before production. It&#x27;s not because of lack of intent. People want to do this. Everybody understands the power of the technology. But why is it so hard?\"The answer, according to Tallapragada, is that AI tools remain disconnected from enterprise workflows, data, and governance systems. \"You&#x27;re writing prompts, prompts, you&#x27;re getting frustrated because the context is not there,\" he said, describing what he called a \"prompt doom loop.\"Salesforce&#x27;s solution is a deeply integrated platform connecting what it calls four ingredients: the Agentforce 360 agent platform, Data 360 for unified data access, Customer 360 apps containing business logic, and Slack as the \"conversational interface\" where humans and agents collaborate.Slack becomes the front door to SalesforcePerhaps the most significant strategic shift is the elevation of Slack — acquired by Salesforce in 2019 for $27.7 billion — as the primary interface for Salesforce itself. The company is effectively reimagining its traditional Lightning interface around Slack channels, where sales deals, service cases, and data insights will surface conversationally rather than through forms and dashboards.\"Imagine that you maybe don&#x27;t log into Salesforce, you don&#x27;t see Salesforce, but it&#x27;s there. It&#x27;s coming to you in Slack, because that&#x27;s where you&#x27;re getting your work done,\" Harris explained.The strategy includes embedding Salesforce&#x27;s Agentforce agents for sales, IT service, HR service, and analytics directly into Slack, alongside a completely rebuilt Slackbot that acts as a personal AI companion. The company is also launching \"Channel Expert,\" an always-on agent that provides instant answers from channel conversations.To enable third-party AI tools to access Slack&#x27;s conversational data, Salesforce is releasing a Real-Time Search API and Model Context Protocol server. Partners including OpenAI, Anthropic, Google, Perplexity, Writer, Dropbox, Notion, and Cursor are building agents that will live natively in Slack.\"The best way to see the power of the platform is through the AI apps and agents already being built,\" Rob Seaman, a Salesforce executive, said during a technical briefing, citing examples of startups \"achieving tens of thousands of customers that have it installed in 120 days or less.\"Voice and IT service take aim at new marketsBeyond Slack integration, Salesforce announced major expansions into voice-based interactions and employee service. Agentforce Voice, now generally available, transforms traditional IVR systems into natural conversations that can update CRM records, trigger workflows, and seamlessly hand off to human agents.The IT Service offering represents Salesforce&#x27;s most direct challenge to ServiceNow, the market leader. Mudhu Sudhakar, who joined Salesforce two months ago as senior vice president for IT and HR Service, positioned the product as a fundamental reimagining of employee support.\"Legacy IT service management is very portals, forms, tickets focused, manual process,\" Sudhakar said. \"What we had a few key tenets: conversation first and agent first, really focused on having a conversational experience for the people requesting the support and for the people providing the support.\"The IT Service platform includes what Salesforce describes as 25+ specialized agents and 100+ pre-built workflows and connectors that can handle everything from password resets to complex incident management.Early customers report dramatic efficiency gainsCustomer results suggest the approach is gaining traction. Reddit reduced average support resolution time from 8.9 minutes to 1.4 minutes — an 84% improvement — while deflecting 46% of cases entirely to AI agents. \"This efficiency has allowed us to provide on-demand help for complex tasks and boost advertiser satisfaction scores by 20%,\" said John Thompson, Reddit&#x27;s VP of sales strategy and operations, in a statement.Engine, a travel management company, reduced average handle time by 15%, saving over $2 million annually. OpenTable resolved 70% of restaurant and diner inquiries autonomously. And 1-800Accountant achieved a 90% case deflection rate during the critical tax week period.Salesforce&#x27;s own internal deployments may be most telling. Tallapragada&#x27;s customer success organization now handles 1.8 million AI-powered conversations weekly, with metrics published at help.salesforce.com showing how many agents answer versus escalating to humans.Even more significantly, Salesforce has deployed AI-powered sales development representatives to follow up on leads that would previously have gone uncontacted due to cost constraints. \"Now, Agentforce has an SDR which is doing thousands of leads following up,\" Tallapragada explained. The company also increased proactive customer outreach by 40% by shifting staff from reactive support.The trust layer problem enterprises can&#x27;t ignoreGiven enterprise concerns about AI reliability, Salesforce has invested heavily in what it calls the \"trust layer\" — audit trails, compliance checks, and observability tools that let organizations monitor agent behavior at scale.\"You should think of an agent as a human. Digital labor. You need to manage performance just like a human. And you need these audit trails,\" Tallapragada explained.The company encountered this challenge firsthand when its own agent deployment scaled. \"When we started at Agentforce at Salesforce, we would track every message, which is great until 1,000, 3,000,\" Tallapragada said. \"Once you have a million chats, there&#x27;s no human, we cannot do it.\"The platform now includes \"Agentforce Grid\" for searching across millions of conversations to identify and fix problematic patterns. The company also introduced Agent Script, a new scripting language that allows developers to define precise guardrails and deterministic controls for agent behavior.Data infrastructure gets a major upgradeUnderlying the agent capabilities is significant infrastructure investment. Salesforce&#x27;s Data 360 includes \"Intelligent Context,\" which automatically extracts structured information from unstructured content like PDFs, diagrams, and flowcharts using what the company describes as \"AI-powered unstructured data pipelines.\"The company is also collaborating with Databricks, dbt Labs, and Snowflake on the \"Universal Semantic Interchange,\" an attempt to standardize how different platforms define business metrics. The pending $8 billion acquisition of Informatica, expected to close soon, will expand metadata management capabilities across the enterprise.The competitive landscape keeps intensifyingSalesforce&#x27;s aggressive AI agent push comes as virtually every major enterprise software vendor pursues similar strategies. Microsoft has embedded Copilot across its product line, Google offers agent capabilities through Vertex AI and Gemini, and ServiceNow has launched its own agentic offerings.When asked how Salesforce&#x27;s announcement compared to OpenAI&#x27;s recent releases, Tallapragada emphasized that customers will use multiple AI tools simultaneously. \"Most of the time I&#x27;m seeing they&#x27;re using OpenAI, they&#x27;re using Gemini, they&#x27;re using Anthropic, just like Salesforce, we use all three,\" he said.The real differentiation, executives argued, lies not in the AI models but in the integration with business processes and data. Harris framed the competition in terms familiar from Salesforce&#x27;s founding: \"26 years ago, we just said, let&#x27;s make Salesforce automation as easy as buying a book on Amazon.com. We&#x27;re doing that same thing. We want to make agentic AI as easy as buying a book on Amazon.\"The company&#x27;s customer success stories are impressive but remain a small fraction of its customer base. With 150,000 Salesforce customers and one million Slack customers, the 12,000 Agentforce deployments represent roughly 8% penetration — strong for a one-year-old product line, but hardly ubiquitous.The company&#x27;s stock, down roughly 28% year to date with a Relative Strength rating of just 15, suggests investors remain skeptical. This week&#x27;s Dreamforce demonstrations — and the months of customer deployments that follow — will begin to provide answers to whether Salesforce can finally move enterprise AI from pilots to production at scale, or whether the \"$7 billion business\" remains more aspiration than reality.",
          "content": "As 50,000 attendees descend on Salesforce&#x27;s Dreamforce conference this week, the enterprise software giant is making its most aggressive bet yet on artificial intelligence agents, positioning itself as the antidote to what it calls an industry-wide \"pilot purgatory\" where 95% of enterprise AI projects never reach production.The company on Monday launched Agentforce 360, a sweeping reimagination of its entire product portfolio designed to transform businesses into what it calls \"agentic enterprises\" — organizations where AI agents work alongside humans to handle up to 40% of work across sales, service, marketing, and operations.\"We are truly in the agentic AI era, and I think it&#x27;s probably the biggest revolution, the biggest transition in technology I&#x27;ve ever experienced in my career,\" said Parker Harris, Salesforce&#x27;s co-founder and chief technology officer, during a recent press briefing. \"In the future, 40% of the work in the Fortune 1000 is probably going to be done by AI, and it&#x27;s going to be humans and AI actually working together.\"The announcement comes at a pivotal moment for Salesforce, which has deployed more than 12,000 AI agent implementations over the past year while building what Harris called a \"$7 billion business\" around its AI platform. Yet the launch also arrives amid unusual turbulence, as CEO Marc Benioff faces fierce backlash for recent comments supporting President Trump and suggesting National Guard troops should patrol San Francisco streets.Why 95% of enterprise AI projects never launchThe stakes are enormous. While companies have rushed to experiment with AI following ChatGPT&#x27;s emergence two years ago, most enterprise deployments have stalled before reaching production, according to recent MIT research that Salesforce executives cited extensively.\"Customers have invested a lot in AI, but they&#x27;re not getting the value,\" said Srini Tallapragada, Salesforce&#x27;s president and chief engineering and customer success officer. \"95% of enterprise AI pilots fail before production. It&#x27;s not because of lack of intent. People want to do this. Everybody understands the power of the technology. But why is it so hard?\"The answer, according to Tallapragada, is that AI tools remain disconnected from enterprise workflows, data, and governance systems. \"You&#x27;re writing prompts, prompts, you&#x27;re getting frustrated because the context is not there,\" he said, describing what he called a \"prompt doom loop.\"Salesforce&#x27;s solution is a deeply integrated platform connecting what it calls four ingredients: the Agentforce 360 agent platform, Data 360 for unified data access, Customer 360 apps containing business logic, and Slack as the \"conversational interface\" where humans and agents collaborate.Slack becomes the front door to SalesforcePerhaps the most significant strategic shift is the elevation of Slack — acquired by Salesforce in 2019 for $27.7 billion — as the primary interface for Salesforce itself. The company is effectively reimagining its traditional Lightning interface around Slack channels, where sales deals, service cases, and data insights will surface conversationally rather than through forms and dashboards.\"Imagine that you maybe don&#x27;t log into Salesforce, you don&#x27;t see Salesforce, but it&#x27;s there. It&#x27;s coming to you in Slack, because that&#x27;s where you&#x27;re getting your work done,\" Harris explained.The strategy includes embedding Salesforce&#x27;s Agentforce agents for sales, IT service, HR service, and analytics directly into Slack, alongside a completely rebuilt Slackbot that acts as a personal AI companion. The company is also launching \"Channel Expert,\" an always-on agent that provides instant answers from channel conversations.To enable third-party AI tools to access Slack&#x27;s conversational data, Salesforce is releasing a Real-Time Search API and Model Context Protocol server. Partners including OpenAI, Anthropic, Google, Perplexity, Writer, Dropbox, Notion, and Cursor are building agents that will live natively in Slack.\"The best way to see the power of the platform is through the AI apps and agents already being built,\" Rob Seaman, a Salesforce executive, said during a technical briefing, citing examples of startups \"achieving tens of thousands of customers that have it installed in 120 days or less.\"Voice and IT service take aim at new marketsBeyond Slack integration, Salesforce announced major expansions into voice-based interactions and employee service. Agentforce Voice, now generally available, transforms traditional IVR systems into natural conversations that can update CRM records, trigger workflows, and seamlessly hand off to human agents.The IT Service offering represents Salesforce&#x27;s most direct challenge to ServiceNow, the market leader. Mudhu Sudhakar, who joined Salesforce two months ago as senior vice president for IT and HR Service, positioned the product as a fundamental reimagining of employee support.\"Legacy IT service management is very portals, forms, tickets focused, manual process,\" Sudhakar said. \"What we had a few key tenets: conversation first and agent first, really focused on having a conversational experience for the people requesting the support and for the people providing the support.\"The IT Service platform includes what Salesforce describes as 25+ specialized agents and 100+ pre-built workflows and connectors that can handle everything from password resets to complex incident management.Early customers report dramatic efficiency gainsCustomer results suggest the approach is gaining traction. Reddit reduced average support resolution time from 8.9 minutes to 1.4 minutes — an 84% improvement — while deflecting 46% of cases entirely to AI agents. \"This efficiency has allowed us to provide on-demand help for complex tasks and boost advertiser satisfaction scores by 20%,\" said John Thompson, Reddit&#x27;s VP of sales strategy and operations, in a statement.Engine, a travel management company, reduced average handle time by 15%, saving over $2 million annually. OpenTable resolved 70% of restaurant and diner inquiries autonomously. And 1-800Accountant achieved a 90% case deflection rate during the critical tax week period.Salesforce&#x27;s own internal deployments may be most telling. Tallapragada&#x27;s customer success organization now handles 1.8 million AI-powered conversations weekly, with metrics published at help.salesforce.com showing how many agents answer versus escalating to humans.Even more significantly, Salesforce has deployed AI-powered sales development representatives to follow up on leads that would previously have gone uncontacted due to cost constraints. \"Now, Agentforce has an SDR which is doing thousands of leads following up,\" Tallapragada explained. The company also increased proactive customer outreach by 40% by shifting staff from reactive support.The trust layer problem enterprises can&#x27;t ignoreGiven enterprise concerns about AI reliability, Salesforce has invested heavily in what it calls the \"trust layer\" — audit trails, compliance checks, and observability tools that let organizations monitor agent behavior at scale.\"You should think of an agent as a human. Digital labor. You need to manage performance just like a human. And you need these audit trails,\" Tallapragada explained.The company encountered this challenge firsthand when its own agent deployment scaled. \"When we started at Agentforce at Salesforce, we would track every message, which is great until 1,000, 3,000,\" Tallapragada said. \"Once you have a million chats, there&#x27;s no human, we cannot do it.\"The platform now includes \"Agentforce Grid\" for searching across millions of conversations to identify and fix problematic patterns. The company also introduced Agent Script, a new scripting language that allows developers to define precise guardrails and deterministic controls for agent behavior.Data infrastructure gets a major upgradeUnderlying the agent capabilities is significant infrastructure investment. Salesforce&#x27;s Data 360 includes \"Intelligent Context,\" which automatically extracts structured information from unstructured content like PDFs, diagrams, and flowcharts using what the company describes as \"AI-powered unstructured data pipelines.\"The company is also collaborating with Databricks, dbt Labs, and Snowflake on the \"Universal Semantic Interchange,\" an attempt to standardize how different platforms define business metrics. The pending $8 billion acquisition of Informatica, expected to close soon, will expand metadata management capabilities across the enterprise.The competitive landscape keeps intensifyingSalesforce&#x27;s aggressive AI agent push comes as virtually every major enterprise software vendor pursues similar strategies. Microsoft has embedded Copilot across its product line, Google offers agent capabilities through Vertex AI and Gemini, and ServiceNow has launched its own agentic offerings.When asked how Salesforce&#x27;s announcement compared to OpenAI&#x27;s recent releases, Tallapragada emphasized that customers will use multiple AI tools simultaneously. \"Most of the time I&#x27;m seeing they&#x27;re using OpenAI, they&#x27;re using Gemini, they&#x27;re using Anthropic, just like Salesforce, we use all three,\" he said.The real differentiation, executives argued, lies not in the AI models but in the integration with business processes and data. Harris framed the competition in terms familiar from Salesforce&#x27;s founding: \"26 years ago, we just said, let&#x27;s make Salesforce automation as easy as buying a book on Amazon.com. We&#x27;re doing that same thing. We want to make agentic AI as easy as buying a book on Amazon.\"The company&#x27;s customer success stories are impressive but remain a small fraction of its customer base. With 150,000 Salesforce customers and one million Slack customers, the 12,000 Agentforce deployments represent roughly 8% penetration — strong for a one-year-old product line, but hardly ubiquitous.The company&#x27;s stock, down roughly 28% year to date with a Relative Strength rating of just 15, suggests investors remain skeptical. This week&#x27;s Dreamforce demonstrations — and the months of customer deployments that follow — will begin to provide answers to whether Salesforce can finally move enterprise AI from pilots to production at scale, or whether the \"$7 billion business\" remains more aspiration than reality.",
          "feed_position": 3,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/4ElJnVoqNF5EdTdQ2qbc94/6c61cde31557a66a34fdfbbfaa9e57ed/nuneybits_Vector_art_of_businessperson_guiding_robot_f1c9b0fc-59ad-455c-b635-5551c598aa7b.webp"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/home/smart-home/best-smart-led-light-bulbs-143022856.html",
          "published_at": "Mon, 13 Oct 2025 09:00:36 +0000",
          "title": "The best smart LED light bulbs for 2025",
          "standfirst": "Smart LED light bulbs are one of the easiest ways to get into the IoT space. These smart lighting solutions let you control your home’s illumination from your phone and other connected devices, and in addition to that practicality, they also inject some fun into your space. Color-changing bulbs have a plethora of RGB options for you to customize the lighting mood for your next movie night, date night or game day, or you can opt for cozy warm white light when you need to unwind at the end of a long day.It goes without saying that many of these smart LED light bulbs work with Amazon’s Alexa and the Google Assistant, so if you already have a smart home setup in the works, you can find one that fits into your chosen ecosystem. And arguably the best thing about these devices is that they can fit into any budget; affordable and advanced options have flooded the space over the past few years. We’ve tested out a bunch of smart lights over the years, and these are our current favorites. Table of contents Best smart lights for 2025 Other smart bulbs we tested What to look for in smart light bulbs Smart light bulb FAQs Best smart lights for 2025 Other smart bulbs we’ve tested Nanoleaf Smarter Kit While we’ve recommended Nanoleaf’s Smarter Kits in guides in the past, they’re a bit more niche than other smart lights on this list. They’re best for adding flare to your living room or game-streaming setup as they come in different shapes like hexagons and triangles and can sync with music. In addition to different colors, light animations and schedules, Nanoleaf’s Smart Kits also support Amazon Alexa and Google Assistant voice commands. What to look for in smart light bulbs Connectivity (To hub or not to hub) One of the biggest appeals of smart lighting solutions is being able to control them from your phone. Most of them are able to do so by connecting to it via Wi-Fi or Bluetooth, or via an external hub, which handles the connection for them. Bluetooth connectivity limits the range in which you’ll be able to control the light, so it’s only best for a limited number of bulbs and ones you don’t expect to control when you’re away. Wi-Fi color-changing bulbs are easy to set up and can be cheaper overall since they don’t require a hub to connect them. However, having something like a central Zigbee hub can make your whole system more reliable since its connection is separate from your home’s network. For that reason, hub-based bulbs tend to be more expandable, so we mainly recommend those if you want to eventually have dozens of smart lights around your home. White or color? Most color-changing bulbs you’ll find today are “white and color” bulbs, meaning they can glow in vibrant RGB color-options like blues, pinks, greens and everything in between, as well as shine with different temperatures of white. But there are some white-only bulbs out there, and they are often a bit more affordable than their color counterparts. While we recommend springing for the white-and-color devices, if you’d prefer white only, make sure you’re getting a bulb that can span the color temperature spectrum (typically from about 2000 to 5000 Kelvin), offering you various levels of cool and warm white light. App features One of the perks of smart lighting solutions is the amount of control you have over them thanks to their various app-control capabilities. Most companion apps let you do things like set lighting schedules and timers, group individual lights into room designations and create your own custom light “scenes” with different RGB options. But we have seen other features that aren’t as ubiquitous like vacation mode for automatically turning lights on and off to enhance your home security, and sync with media, which changes the colors of lights depending on the music you’re listening to or the game you’re currently live-streaming. Smart home compatibility If you use a smart assistant like Amazon’s Alexa or the Google Assistant regularly, make sure the smart lights or smart switches work with your favorite. All of the bulbs we tested supported both Amazon’s and Google’s virtual assistants, allowing you to use voice commands to turn lights on and off, dim them with a virtual dimmer and more. The wildcard here is Siri and Apple’s HomeKit; while numerous smart bulbs have added HomeKit support, not all lights are compatible with Apple’s smart home system. Expandability We alluded to this above, but you’ll want to consider how many smart lights you eventually want in your home. Some brands and lighting systems are easier to expand than others, and we generally recommend going for hub-based bulbs if you plan on putting smart lights in every room in your home. If you’re only looking to deck out your home office or living room with some fancy color-changing bulbs, Wi-Fi options should serve you well. Thankfully, these are some of the most affordable smart home devices you can get, so even if you don’t have a clear answer to this question now, you can reconsider your options down the line if you do decide to outfit your home with multiple smart bulbs. Smart light bulb FAQs What’s the best smart light bulb for Alexa? There is no best smart light bulb for Alexa. Amazon doesn’t make its own smart bulbs (like it does for smart plugs and thermostats), but rather there are dozens of smart lights made by third-parties that work with Alexa — including all of the ones we tested. Before picking the best smart light bulb for you, make sure to check the voice assistants that the contenders support. You’ll find that most smart light bulbs available today work with Amazon’s Alexa and the Google Assistant, and plenty of them also have support for Apple’s Siri and HomeKit. Can you put a smart bulb in any lamp? Smart light bulbs can go into most modern light fixtures — but just like regular bulbs, they need to be the right shape/size for the fixture. A standard A19 smart light bulb should work properly in most table, floor and other lamps. If you have a fixture that takes a specific type of bulb, look for smart bulbs that will fit properly. Do smart light bulbs use electricity when off? Smart light bulbs do use a negligible amount of electricity when their fixtures are turned off. This is due to the fact that the smart bulb needs to stay in constant contact with your home’s internet connection or Bluetooth in order to work properly. However, their energy-saving benefits usually outweigh the small amount of power they consume even while turned off.This article originally appeared on Engadget at https://www.engadget.com/home/smart-home/best-smart-led-light-bulbs-143022856.html?src=rss",
          "content": "Smart LED light bulbs are one of the easiest ways to get into the IoT space. These smart lighting solutions let you control your home’s illumination from your phone and other connected devices, and in addition to that practicality, they also inject some fun into your space. Color-changing bulbs have a plethora of RGB options for you to customize the lighting mood for your next movie night, date night or game day, or you can opt for cozy warm white light when you need to unwind at the end of a long day.It goes without saying that many of these smart LED light bulbs work with Amazon’s Alexa and the Google Assistant, so if you already have a smart home setup in the works, you can find one that fits into your chosen ecosystem. And arguably the best thing about these devices is that they can fit into any budget; affordable and advanced options have flooded the space over the past few years. We’ve tested out a bunch of smart lights over the years, and these are our current favorites. Table of contents Best smart lights for 2025 Other smart bulbs we tested What to look for in smart light bulbs Smart light bulb FAQs Best smart lights for 2025 Other smart bulbs we’ve tested Nanoleaf Smarter Kit While we’ve recommended Nanoleaf’s Smarter Kits in guides in the past, they’re a bit more niche than other smart lights on this list. They’re best for adding flare to your living room or game-streaming setup as they come in different shapes like hexagons and triangles and can sync with music. In addition to different colors, light animations and schedules, Nanoleaf’s Smart Kits also support Amazon Alexa and Google Assistant voice commands. What to look for in smart light bulbs Connectivity (To hub or not to hub) One of the biggest appeals of smart lighting solutions is being able to control them from your phone. Most of them are able to do so by connecting to it via Wi-Fi or Bluetooth, or via an external hub, which handles the connection for them. Bluetooth connectivity limits the range in which you’ll be able to control the light, so it’s only best for a limited number of bulbs and ones you don’t expect to control when you’re away. Wi-Fi color-changing bulbs are easy to set up and can be cheaper overall since they don’t require a hub to connect them. However, having something like a central Zigbee hub can make your whole system more reliable since its connection is separate from your home’s network. For that reason, hub-based bulbs tend to be more expandable, so we mainly recommend those if you want to eventually have dozens of smart lights around your home. White or color? Most color-changing bulbs you’ll find today are “white and color” bulbs, meaning they can glow in vibrant RGB color-options like blues, pinks, greens and everything in between, as well as shine with different temperatures of white. But there are some white-only bulbs out there, and they are often a bit more affordable than their color counterparts. While we recommend springing for the white-and-color devices, if you’d prefer white only, make sure you’re getting a bulb that can span the color temperature spectrum (typically from about 2000 to 5000 Kelvin), offering you various levels of cool and warm white light. App features One of the perks of smart lighting solutions is the amount of control you have over them thanks to their various app-control capabilities. Most companion apps let you do things like set lighting schedules and timers, group individual lights into room designations and create your own custom light “scenes” with different RGB options. But we have seen other features that aren’t as ubiquitous like vacation mode for automatically turning lights on and off to enhance your home security, and sync with media, which changes the colors of lights depending on the music you’re listening to or the game you’re currently live-streaming. Smart home compatibility If you use a smart assistant like Amazon’s Alexa or the Google Assistant regularly, make sure the smart lights or smart switches work with your favorite. All of the bulbs we tested supported both Amazon’s and Google’s virtual assistants, allowing you to use voice commands to turn lights on and off, dim them with a virtual dimmer and more. The wildcard here is Siri and Apple’s HomeKit; while numerous smart bulbs have added HomeKit support, not all lights are compatible with Apple’s smart home system. Expandability We alluded to this above, but you’ll want to consider how many smart lights you eventually want in your home. Some brands and lighting systems are easier to expand than others, and we generally recommend going for hub-based bulbs if you plan on putting smart lights in every room in your home. If you’re only looking to deck out your home office or living room with some fancy color-changing bulbs, Wi-Fi options should serve you well. Thankfully, these are some of the most affordable smart home devices you can get, so even if you don’t have a clear answer to this question now, you can reconsider your options down the line if you do decide to outfit your home with multiple smart bulbs. Smart light bulb FAQs What’s the best smart light bulb for Alexa? There is no best smart light bulb for Alexa. Amazon doesn’t make its own smart bulbs (like it does for smart plugs and thermostats), but rather there are dozens of smart lights made by third-parties that work with Alexa — including all of the ones we tested. Before picking the best smart light bulb for you, make sure to check the voice assistants that the contenders support. You’ll find that most smart light bulbs available today work with Amazon’s Alexa and the Google Assistant, and plenty of them also have support for Apple’s Siri and HomeKit. Can you put a smart bulb in any lamp? Smart light bulbs can go into most modern light fixtures — but just like regular bulbs, they need to be the right shape/size for the fixture. A standard A19 smart light bulb should work properly in most table, floor and other lamps. If you have a fixture that takes a specific type of bulb, look for smart bulbs that will fit properly. Do smart light bulbs use electricity when off? Smart light bulbs do use a negligible amount of electricity when their fixtures are turned off. This is due to the fact that the smart bulb needs to stay in constant contact with your home’s internet connection or Bluetooth in order to work properly. However, their energy-saving benefits usually outweigh the small amount of power they consume even while turned off.This article originally appeared on Engadget at https://www.engadget.com/home/smart-home/best-smart-led-light-bulbs-143022856.html?src=rss",
          "feed_position": 16
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/breaking-the-bottleneck-why-ai-demands-an-ssd-first-future",
          "published_at": "Mon, 13 Oct 2025 04:00:00 GMT",
          "title": "Breaking the bottleneck: Why AI demands an SSD-first future",
          "standfirst": "Presented by SolidigmAs AI adoption surges, data centers face a critical bottleneck in storage — and traditional HDDs are at the center of it. Data that once sat idle as cold archives is now being pulled into frequent use to build more accurate models and deliver better inference results. This shift from cold data to warm data demands low-latency, high-throughput storage that can handle parallel computations. HDDs will remain the workhorse for low-cost cold storage, but without rethinking their role, the high-capacity storage layer risks becoming the weakest link in the AI factory.\"Modern AI workloads, combined with data center constraints, have created new challenges for HDDs,\" says Jeff Janukowicz, research vice president at IDC. \"While HDD suppliers are addressing data storage growth by offering larger drives, this often comes at the expense of slower performance. As a result, the concept of &#x27;nearline SSDs&#x27; is becoming an increasingly relevant topic of discussion within the industry.\" Today, AI operators need to maximize GPU utilization, manage network-attached storage efficiently, and scale compute — all while cutting costs on increasingly scarce power and space. In an environment where every watt and every square inch counts, says Roger Corell, senior director of AI and leadership marketing at Solidigm, success requires more than a technical refresh. It calls for a deeper realignment.“It speaks to the tectonic shift in the value of data for AI,” Corell says. “That’s where high-capacity SSDs come into play. Along with capacity, they bring performance and efficiency -- enabling exabyte-scale storage pipelines to keep pace with the relentless pace of data set size. All of that consumes power and space, so we need to do it as efficiently as possible to enable more GPU scale in this constrained environment.”High-capacity SSDs aren’t just displacing HDDs — they’re removing one of the biggest bottlenecks on the AI factory floor. By delivering massive gains in performance, efficiency, and density, SSDs free up the power and space needed to push GPU scale further. It’s less a storage upgrade than a structural shift in how data infrastructure is designed for the AI era.HDDs vs. SDDs: More than just a hardware refreshHDDs have impressive mechanical designs, but they&#x27;re made up of many moving parts that at scale use more energy, take up more space, and fail at a higher rate than solid state drives. The reliance on spinning platters and mechanical read/write heads inherently limits Input/Output Operations Per Second (IOPS), creating bottlenecks for AI workloads that demand low latency, high concurrency, and sustained throughput. HDDs also struggle with latency-sensitive tasks, as the physical act of seeking data introduces mechanical delays unsuited for real-time AI inference and training. Moreover, their power and cooling requirements increase significantly under frequent and intensive data access, reducing efficiency as data scales and warms.In contrast, the SSD-based VAST storage solution reduces energy usage by ~$1M a year, and in an AI environment where every watt matters, this is a huge advantage for SSDs. To demonstrate, Solidigm and VAST Data completed a study examining the economics of data storage at exabyte scale — a quadrillion bytes, or a billion gigabytes, with an analysis of storage power consumption versus HDDs over a 10-year period. As a starting reference point, you’d need four 30TB HDDs to equal the capacity of a single 122TB Solidigm SSD. After factoring in VAST’s data reduction techniques made possible by the superior performance of SSDs, the exabyte solution comprises 3,738 Solidigm SSDs vs over 40,000 high-capacity HDDs. The study found that the SSD-based VAST solution consumes 77% less storage energy. Minimizing data center footprints\"We’re shipping 122-terabyte drives to some of the top OEMs and leading AI cloud service providers in the world,\" Corell says. \"When you compare an all-122TB SSD to hybrid HDD + TLC SSD configuration, they&#x27;re getting a nine-to-one savings in data center footprint. And yes, it’s important in these massive data centers that are building their own nuclear reactors and signing hefty power purchase agreements with renewable energy providers, but it’s increasingly important as you get to the regional data centers, the local data centers, and all the way out to your edge deployments where space can come at a premium.\"That nine-to-one savings goes beyond space and power — it lets organizations fit infrastructure into previously unavailable spaces, expand GPU scale, or build smaller footprints.\"If you’re given X amount of land and Y amount of power, you’re going to use it. You’re AI\" Corell explains, “where every watt and square inch counts, so why not use it in the most efficient way? Get the most efficient storage possible on the planet and enable greater GPU scale within that envelope that you have to fit in. On an ongoing basis, it’s going to save you operational cost as well. You have 90 percent fewer storage bays to maintain, and the cost associated with that is gone.\"Another often-overlooked element, the (much) larger physical footprint of data stored on mechanical HDDs results in a greater construction materials footprint. Collectively, concrete and steel production accounts for over 15% of global greenhouse gas emissions. By reducing the physical footprint of storage, high-capacity SSDs can help reduce embodied concrete and steel-based emissions by more than 80% compared to HDDs. And in the last phase of the sustainability life cycle, which is drive end-of-life, there will be 90% percent fewer drives to disposition. . Reshaping cold and archival storage strategiesThe move to SDD isn&#x27;t just a storage upgrade; it&#x27;s a fundamental realignment of data infrastructure strategy in the AI era, and it&#x27;s picking up speed.\"Big hyperscalers are looking to wring the most out of their existing infrastructure, doing unnatural acts, if you will, with HDDs like overprovisioning them to near 90% to try to wring out as many IOPS per terabyte as possible, but they’re beginning to come around,\" Corell says. \"Once they turn to a modern all high-capacity storage infrastructure, the industry at large will be on that trajectory. Plus, we&#x27;re starting to see these lessons learned on the value of modern storage in AI applied to other segments as well, such as big data analytics, HPC, and many more.\"While all-flash solutions are being embraced almost universally, there will always be a place for HDDs, he adds. HDDs will persist in usages like archival, cold storage, and scenarios where pure cost per gigabyte concerns outweigh the need for real-time access. But as the token economy heats up and enterprises realize value in monetizing data, the warm and warming data segments will continue to grow. Solving power challenges of the futureNow in its 4th generation, with more than 122 cumulative exabytes shipped to date, Solidigm’s QLC (Quad-Level Cell) technology has led the industry in balancing higher drive capacities with cost efficiency.\"We don’t think of storage as just storing bits and bytes. We think about how we can develop these amazing drives that are able to deliver benefits at a solution level,\" Corell says. \"The shining star on that is our recently launched, E1.S, designed specifically for dense and efficient storage in direct attach storage configurations for the next-generation fanless GPU server.\"The Solidigm D7-PS1010 E1.S is a breakthrough, the industry’s first eSSD with single-sided direct-to-chip liquid cooling technology. Solidigm worked with NVIDIA to address the dual challenges of heat management and cost efficiency, while delivering the high performance required for demanding AI workloads. \"We’re rapidly moving to an environment where all critical IT components will be direct-to-chip liquid-cooled on the direct attach side,\" he says. \"I think the market needs to be looking at their approach to cooling, because power limitations, power challenges are not going to abate in my lifetime, at least. They need to be applying a neocloud mindset to how they’re architecting the most efficient infrastructure.\"Increasingly complex inference is pushing against a memory wall, which makes storage architecture a front-line design challenge, not an afterthought. High-capacity SSDs, paired with liquid cooling and efficient design, are emerging as the only path to meet AI’s escalating demands. The mandate now is to build infrastructure not just for efficiency, but for storage that can efficiently scale as data grows. The organizations that realign storage now will be the ones able to scale AI tomorrow.Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.",
          "content": "Presented by SolidigmAs AI adoption surges, data centers face a critical bottleneck in storage — and traditional HDDs are at the center of it. Data that once sat idle as cold archives is now being pulled into frequent use to build more accurate models and deliver better inference results. This shift from cold data to warm data demands low-latency, high-throughput storage that can handle parallel computations. HDDs will remain the workhorse for low-cost cold storage, but without rethinking their role, the high-capacity storage layer risks becoming the weakest link in the AI factory.\"Modern AI workloads, combined with data center constraints, have created new challenges for HDDs,\" says Jeff Janukowicz, research vice president at IDC. \"While HDD suppliers are addressing data storage growth by offering larger drives, this often comes at the expense of slower performance. As a result, the concept of &#x27;nearline SSDs&#x27; is becoming an increasingly relevant topic of discussion within the industry.\" Today, AI operators need to maximize GPU utilization, manage network-attached storage efficiently, and scale compute — all while cutting costs on increasingly scarce power and space. In an environment where every watt and every square inch counts, says Roger Corell, senior director of AI and leadership marketing at Solidigm, success requires more than a technical refresh. It calls for a deeper realignment.“It speaks to the tectonic shift in the value of data for AI,” Corell says. “That’s where high-capacity SSDs come into play. Along with capacity, they bring performance and efficiency -- enabling exabyte-scale storage pipelines to keep pace with the relentless pace of data set size. All of that consumes power and space, so we need to do it as efficiently as possible to enable more GPU scale in this constrained environment.”High-capacity SSDs aren’t just displacing HDDs — they’re removing one of the biggest bottlenecks on the AI factory floor. By delivering massive gains in performance, efficiency, and density, SSDs free up the power and space needed to push GPU scale further. It’s less a storage upgrade than a structural shift in how data infrastructure is designed for the AI era.HDDs vs. SDDs: More than just a hardware refreshHDDs have impressive mechanical designs, but they&#x27;re made up of many moving parts that at scale use more energy, take up more space, and fail at a higher rate than solid state drives. The reliance on spinning platters and mechanical read/write heads inherently limits Input/Output Operations Per Second (IOPS), creating bottlenecks for AI workloads that demand low latency, high concurrency, and sustained throughput. HDDs also struggle with latency-sensitive tasks, as the physical act of seeking data introduces mechanical delays unsuited for real-time AI inference and training. Moreover, their power and cooling requirements increase significantly under frequent and intensive data access, reducing efficiency as data scales and warms.In contrast, the SSD-based VAST storage solution reduces energy usage by ~$1M a year, and in an AI environment where every watt matters, this is a huge advantage for SSDs. To demonstrate, Solidigm and VAST Data completed a study examining the economics of data storage at exabyte scale — a quadrillion bytes, or a billion gigabytes, with an analysis of storage power consumption versus HDDs over a 10-year period. As a starting reference point, you’d need four 30TB HDDs to equal the capacity of a single 122TB Solidigm SSD. After factoring in VAST’s data reduction techniques made possible by the superior performance of SSDs, the exabyte solution comprises 3,738 Solidigm SSDs vs over 40,000 high-capacity HDDs. The study found that the SSD-based VAST solution consumes 77% less storage energy. Minimizing data center footprints\"We’re shipping 122-terabyte drives to some of the top OEMs and leading AI cloud service providers in the world,\" Corell says. \"When you compare an all-122TB SSD to hybrid HDD + TLC SSD configuration, they&#x27;re getting a nine-to-one savings in data center footprint. And yes, it’s important in these massive data centers that are building their own nuclear reactors and signing hefty power purchase agreements with renewable energy providers, but it’s increasingly important as you get to the regional data centers, the local data centers, and all the way out to your edge deployments where space can come at a premium.\"That nine-to-one savings goes beyond space and power — it lets organizations fit infrastructure into previously unavailable spaces, expand GPU scale, or build smaller footprints.\"If you’re given X amount of land and Y amount of power, you’re going to use it. You’re AI\" Corell explains, “where every watt and square inch counts, so why not use it in the most efficient way? Get the most efficient storage possible on the planet and enable greater GPU scale within that envelope that you have to fit in. On an ongoing basis, it’s going to save you operational cost as well. You have 90 percent fewer storage bays to maintain, and the cost associated with that is gone.\"Another often-overlooked element, the (much) larger physical footprint of data stored on mechanical HDDs results in a greater construction materials footprint. Collectively, concrete and steel production accounts for over 15% of global greenhouse gas emissions. By reducing the physical footprint of storage, high-capacity SSDs can help reduce embodied concrete and steel-based emissions by more than 80% compared to HDDs. And in the last phase of the sustainability life cycle, which is drive end-of-life, there will be 90% percent fewer drives to disposition. . Reshaping cold and archival storage strategiesThe move to SDD isn&#x27;t just a storage upgrade; it&#x27;s a fundamental realignment of data infrastructure strategy in the AI era, and it&#x27;s picking up speed.\"Big hyperscalers are looking to wring the most out of their existing infrastructure, doing unnatural acts, if you will, with HDDs like overprovisioning them to near 90% to try to wring out as many IOPS per terabyte as possible, but they’re beginning to come around,\" Corell says. \"Once they turn to a modern all high-capacity storage infrastructure, the industry at large will be on that trajectory. Plus, we&#x27;re starting to see these lessons learned on the value of modern storage in AI applied to other segments as well, such as big data analytics, HPC, and many more.\"While all-flash solutions are being embraced almost universally, there will always be a place for HDDs, he adds. HDDs will persist in usages like archival, cold storage, and scenarios where pure cost per gigabyte concerns outweigh the need for real-time access. But as the token economy heats up and enterprises realize value in monetizing data, the warm and warming data segments will continue to grow. Solving power challenges of the futureNow in its 4th generation, with more than 122 cumulative exabytes shipped to date, Solidigm’s QLC (Quad-Level Cell) technology has led the industry in balancing higher drive capacities with cost efficiency.\"We don’t think of storage as just storing bits and bytes. We think about how we can develop these amazing drives that are able to deliver benefits at a solution level,\" Corell says. \"The shining star on that is our recently launched, E1.S, designed specifically for dense and efficient storage in direct attach storage configurations for the next-generation fanless GPU server.\"The Solidigm D7-PS1010 E1.S is a breakthrough, the industry’s first eSSD with single-sided direct-to-chip liquid cooling technology. Solidigm worked with NVIDIA to address the dual challenges of heat management and cost efficiency, while delivering the high performance required for demanding AI workloads. \"We’re rapidly moving to an environment where all critical IT components will be direct-to-chip liquid-cooled on the direct attach side,\" he says. \"I think the market needs to be looking at their approach to cooling, because power limitations, power challenges are not going to abate in my lifetime, at least. They need to be applying a neocloud mindset to how they’re architecting the most efficient infrastructure.\"Increasingly complex inference is pushing against a memory wall, which makes storage architecture a front-line design challenge, not an afterthought. High-capacity SSDs, paired with liquid cooling and efficient design, are emerging as the only path to meet AI’s escalating demands. The mandate now is to build infrastructure not just for efficiency, but for storage that can efficiently scale as data grows. The organizations that realign storage now will be the ones able to scale AI tomorrow.Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.",
          "feed_position": 4,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/4rRjfWe1S9u2XgvoKASwHM/ddafa77e43acdade763516ae3aa9a5fe/Solidigm_2.png"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/we-keep-talking-about-ai-agents-but-do-we-ever-know-what-they-are",
          "published_at": "Sun, 12 Oct 2025 19:00:00 GMT",
          "title": "We keep talking about AI agents, but do we ever know what they are?",
          "standfirst": "Imagine you do two things on a Monday morning.First, you ask a chatbot to summarize your new emails. Next, you ask an AI tool to figure out why your top competitor grew so fast last quarter. The AI silently gets to work. It scours financial reports, news articles and social media sentiment. It cross-references that data with your internal sales numbers, drafts a strategy outlining three potential reasons for the competitor&#x27;s success and schedules a 30-minute meeting with your team to present its findings.We&#x27;re calling both of these \"AI agents,\" but they represent worlds of difference in intelligence, capability and the level of trust we place in them. This ambiguity creates a fog that makes it difficult to build, evaluate, and safely govern these powerful new tools. If we can&#x27;t agree on what we&#x27;re building, how can we know when we&#x27;ve succeeded?This post won&#x27;t try to sell you on yet another definitive framework. Instead, think of it as a survey of the current landscape of agent autonomy, a map to help us all navigate the terrain together.What are we even talking about? Defining an \"AI agent\"Before we can measure an agent&#x27;s autonomy, we need to agree on what an \"agent\" actually is. The most widely accepted starting point comes from the foundational textbook on AI, Stuart Russell and Peter Norvig’s “Artificial Intelligence: A Modern Approach.” They define an agent as anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators. A thermostat is a simple agent: Its sensor perceives the room temperature, and its actuator acts by turning the heat on or off.ReAct Model for AI Agents (Credit: Confluent) That classic definition provides a solid mental model. For today&#x27;s technology, we can translate it into four key components that make up a modern AI agent:Perception (the \"senses\"): This is how an agent takes in information about its digital or physical environment. It&#x27;s the input stream that allows the agent to understand the current state of the world relevant to its task.Reasoning engine (the \"brain\"): This is the core logic that processes the perceptions and decides what to do next. For modern agents, this is typically powered by a large language model (LLM). The engine is responsible for planning, breaking down large goals into smaller steps, handling errors and choosing the right tools for the job.Action (the \"hands\"): This is how an agent affects its environment to move closer to its goal. The ability to take action via tools is what gives an agent its power.Goal/objective: This is the overarching task or purpose that guides all of the agent&#x27;s actions. It is the \"why\" that turns a collection of tools into a purposeful system. The goal can be simple (\"Find the best price for this book\") or complex (\"Launch the marketing campaign for our new product\")Putting it all together, a true agent is a full-body system. The reasoning engine is the brain, but it’s useless without the senses (perception) to understand the world and the hands (actions) to change it. This complete system, all guided by a central goal, is what creates genuine agency.With these components in mind, the distinction we made earlier becomes clear. A standard chatbot isn&#x27;t a true agent. It perceives your question and acts by providing an answer, but it lacks an overarching goal and the ability to use external tools to accomplish it.An agent, on the other hand, is software that has agency. It has the capacity to act independently and dynamically toward a goal. And it&#x27;s this capacity that makes a discussion about the levels of autonomy so important.Learning from the past: How we learned to classify autonomyThe dizzying pace of AI can make it feel like we&#x27;re navigating uncharted territory. But when it comes to classifying autonomy, we’re not starting from scratch. Other industries have been working on this problem for decades, and their playbooks offer powerful lessons for the world of AI agents.The core challenge is always the same: How do you create a clear, shared language for the gradual handover of responsibility from a human to a machine?SAE levels of driving automationPerhaps the most successful framework comes from the automotive industry. The SAE J3016 standard defines six levels of driving automation, from Level 0 (fully manual) to Level 5 (fully autonomous).The SAE J3016 Levels of Driving Automation (Credit: SAE International) What makes this model so effective isn&#x27;t its technical detail, but its focus on two simple concepts:Dynamic driving task (DDT): This is everything involved in the real-time act of driving: steering, braking, accelerating and monitoring the road.Operational design domain (ODD): These are the specific conditions under which the system is designed to work. For example, \"only on divided highways\" or \"only in clear weather during the daytime.\"The question for each level is simple: Who is doing the DDT, and what is the ODD? At Level 2, the human must supervise at all times. At Level 3, the car handles the DDT within its ODD, but the human must be ready to take over. At Level 4, the car can handle everything within its ODD, and if it encounters a problem, it can safely pull over on its own.The key insight for AI agents: A robust framework isn&#x27;t about the sophistication of the AI \"brain.\" It&#x27;s about clearly defining the division of responsibility between human and machine under specific, well-defined conditions.Aviation&#x27;s 10 Levels of AutomationWhile the SAE’s six levels are great for broad classification, aviation offers a more granular model for systems designed for close human-machine collaboration. The Parasuraman, Sheridan, and Wickens model proposes a detailed 10-level spectrum of automation.Levels of Automation of Decision and Action Selection for Aviation (Credit: The MITRE Corporation)This framework is less about full autonomy and more about the nuances of interaction. For example:At Level 3, the computer \"narrows the selection down to a few\" for the human to choose from.At Level 6, the computer \"allows the human a restricted time to veto before it executes\" an action.At Level 9, the computer \"informs the human only if it, the computer, decides to.\"The key insight for AI agents: This model is perfect for describing the collaborative \"centaur\" systems we&#x27;re seeing today. Most AI agents won&#x27;t be fully autonomous (Level 10) but will exist somewhere on this spectrum, acting as a co-pilot that suggests, executes with approval or acts with a veto window.Robotics and unmanned systemsFinally, the world of robotics brings in another critical dimension: context. The National Institute of Standards and Technology&#x27;s (NIST) Autonomy Levels for Unmanned Systems (ALFUS) framework was designed for systems like drones and industrial robots.The Three-Axis Model for ALFUS (Credit: NIST) Its main contribution is adding context to the definition of autonomy, assessing it along three axes:Human independence: How much human supervision is required?Mission complexity: How difficult or unstructured is the task?Environmental complexity: How predictable and stable is the environment in which the agent operates?The key insight for AI agents: This framework reminds us that autonomy isn&#x27;t a single number. An agent performing a simple task in a stable, predictable digital environment (like sorting files in a single folder) is fundamentally less autonomous than an agent performing a complex task across the chaotic, unpredictable environment of the open internet, even if the level of human supervision is the same.The emerging frameworks for AI agentsHaving looked at the lessons from automotive, aviation and robotics, we can now examine the emerging frameworks designed for AI agents. While the field is still new and no single standard has won out, most proposals fall into three distinct, but often overlapping, categories based on the primary question they seek to answer.Category 1: The \"What can it do?\" frameworks (capability-focused)These frameworks classify agents based on their underlying technical architecture and what they are capable of achieving. They provide a roadmap for developers, outlining a progression of increasingly sophisticated technical milestones that often correspond directly to code patterns.A prime example of this developer-centric approach comes from Hugging Face. Their framework uses a star rating to show the gradual shift in control from human to AI:Five Levels of AI Agent Autonomy, as proposed by HuggingFace (Credit: Hugging Face) Zero stars (simple processor): The AI has no impact on the program&#x27;s flow. It simply processes information and its output is displayed, like a print statement. The human is in complete control.One star (router): The AI makes a basic decision that directs program flow, like choosing between two predefined paths (if/else). The human still defines how everything is done.Two stars (tool call): The AI chooses which predefined tool to use and what arguments to use with it. The human has defined the available tools, but the AI decides how to execute them.Three stars (multi-step agent): The AI now controls the iteration loop. It decides which tool to use, when to use it and whether to continue working on the task.Four stars (fully autonomous): The AI can generate and execute entirely new code to accomplish a goal, going beyond the predefined tools it was given.Strengths: This model is excellent for engineers. It&#x27;s concrete, maps directly to code and clearly benchmarks the transfer of executive control to the AI. Weaknesses: It is highly technical and less intuitive for non-developers trying to understand an agent&#x27;s real-world impact.Category 2: The \"How do we work together?\" frameworks (interaction-focused)This second category defines autonomy not by the agent’s internal skills, but by the nature of its relationship with the human user. The central question is: Who is in control, and how do we collaborate?This approach often mirrors the nuance we saw in the aviation models. For instance, a framework detailed in the paper Levels of Autonomy for AI Agents defines levels based on the user&#x27;s role:L1 - user as an operator: The human is in direct control (like a person using Photoshop with AI-assist features).L4 - user as an approver: The agent proposes a full plan or action, and the human must give a simple \"yes\" or \"no\" before it proceeds.L5 - user as an observer: The agent has full autonomy to pursue a goal and simply reports its progress and results back to the human.Levels of Autonomy for AI AgentsStrengths: These frameworks are highly intuitive and user-centric. They directly address the critical issues of control, trust, and oversight.Weaknesses: An agent with simple capabilities and one with highly advanced reasoning could both fall into the \"Approver\" level, so this approach can sometimes obscure the underlying technical sophistication.Category 3: The \"Who is responsible?\" frameworks (governance-focused)The final category is less concerned with how an agent works and more with what happens when it fails. These frameworks are designed to help answer crucial questions about law, safety and ethics.Think tanks like Germany&#x27;s Stiftung Neue VTrantwortung have analyzed AI agents through the lens of legal liability. Their work aims to classify agents in a way that helps regulators determine who is responsible for an agent&#x27;s actions: The user who deployed it, the developer who built it or the company that owns the platform it runs on?This perspective is essential for navigating complex regulations like the EU&#x27;s Artificial Intelligence Act, which will treat AI systems differently based on the level of risk they pose.Strengths: This approach is absolutely essential for real-world deployment. It forces the difficult but necessary conversations about accountability that build public trust.Weaknesses: It&#x27;s more of a legal or policy guide than a technical roadmap for developers.A comprehensive understanding requires looking at all three questions at once: An agent&#x27;s capabilities, how we interact with it and who is responsible for the outcome..Identifying the gaps and challengesLooking at the landscape of autonomy frameworks shows us that no single model is sufficient because the true challenges lie in the gaps between them, in areas that are incredibly difficult to define and measure.What is the \"Road\" for a digital agent?The SAE framework for self-driving cars gave us the powerful concept of an ODD, the specific conditions under which a system can operate safely. For a car, that might be \"divided highways, in clear weather, during the day.\" This is a great solution for a physical environment, but what’s the ODD for a digital agent?The \"road\" for an agent is the entire internet. An infinite, chaotic and constantly changing environment. Websites get redesigned overnight, APIs are deprecated and social norms in online communities shift. How do we define a \"safe\" operational boundary for an agent that can browse websites, access databases and interact with third-party services? Answering this is one of the biggest unsolved problems. Without a clear digital ODD, we can&#x27;t make the same safety guarantees that are becoming standard in the automotive world.This is why, for now, the most effective and reliable agents operate within well-defined, closed-world scenarios. As I argued in a recent VentureBeat article, forgetting the open-world fantasies and focusing on \"bounded problems\" is the key to real-world success. This means defining a clear, limited set of tools, data sources and potential actions. Beyond simple tool useToday&#x27;s agents are getting very good at executing straightforward plans. If you tell one to \"find the price of this item using Tool A, then book a meeting with Tool B,\" it can often succeed. But true autonomy requires much more. Many systems today hit a technical wall when faced with tasks that require:Long-term reasoning and planning: Agents struggle to create and adapt complex, multi-step plans in the face of uncertainty. They can follow a recipe, but they can&#x27;t yet invent one from scratch when things go wrong.Robust self-correction: What happens when an API call fails or a website returns an unexpected error? A truly autonomous agent needs the resilience to diagnose the problem, form a new hypothesis and try a different approach, all without a human stepping in.Composability: The future likely involves not one agent, but a team of specialized agents working together. Getting them to collaborate reliably, to pass information back and forth, delegate tasks and resolve conflicts is a monumental software engineering challenge that we are just beginning to tackle.The elephant in the room: Alignment and controlThis is the most critical challenge of all, because it&#x27;s not just technical, it&#x27;s deeply human. Alignment is the problem of ensuring an agent&#x27;s goals and actions are consistent with our intentions and values, even when those values are complex, unstated or nuanced.Imagine you give an agent the seemingly harmless goal of \"maximizing customer engagement for our new product.\" The agent might correctly determine that the most effective strategy is to send a dozen notifications a day to every user. The agent has achieved its literal goal perfectly, but it has violated the unstated, common-sense goal of \"don&#x27;t be incredibly annoying.\"This is a failure of alignment.The core difficulty, which organizations like the AI Alignment Forum are dedicated to studying, is that it is incredibly hard to specify fuzzy, complex human preferences in the precise, literal language of code. As agents become more powerful, ensuring they are not just capable but also safe, predictable and aligned with our true intent becomes the most important challenge we face.The future is agentic (and collaborative)The path forward for AI agents is not a single leap to a god-like super-intelligence, but a more practical and collaborative journey. The immense challenges of open-world reasoning and perfect alignment mean that the future is a team effort.We will see less of the single, all-powerful agent and more of an \"agentic mesh\" — a network of specialized agents, each operating within a bounded domain, working together to tackle complex problems. More importantly, they will work with us. The most valuable and safest applications will keep a human on the loop, casting them as a co-pilot or strategist to augment our intellect with the speed of machine execution. This \"centaur\" model will be the most effective and responsible path forward.The frameworks we&#x27;ve explored aren’t just theoretical. They’re practical tools for building trust, assigning responsibility and setting clear expectations. They help developers define limits and leaders shape vision, laying the groundwork for AI to become a dependable partner in our work and lives.Sean Falconer is Confluent&#x27;s AI entrepreneur in residence.",
          "content": "Imagine you do two things on a Monday morning.First, you ask a chatbot to summarize your new emails. Next, you ask an AI tool to figure out why your top competitor grew so fast last quarter. The AI silently gets to work. It scours financial reports, news articles and social media sentiment. It cross-references that data with your internal sales numbers, drafts a strategy outlining three potential reasons for the competitor&#x27;s success and schedules a 30-minute meeting with your team to present its findings.We&#x27;re calling both of these \"AI agents,\" but they represent worlds of difference in intelligence, capability and the level of trust we place in them. This ambiguity creates a fog that makes it difficult to build, evaluate, and safely govern these powerful new tools. If we can&#x27;t agree on what we&#x27;re building, how can we know when we&#x27;ve succeeded?This post won&#x27;t try to sell you on yet another definitive framework. Instead, think of it as a survey of the current landscape of agent autonomy, a map to help us all navigate the terrain together.What are we even talking about? Defining an \"AI agent\"Before we can measure an agent&#x27;s autonomy, we need to agree on what an \"agent\" actually is. The most widely accepted starting point comes from the foundational textbook on AI, Stuart Russell and Peter Norvig’s “Artificial Intelligence: A Modern Approach.” They define an agent as anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators. A thermostat is a simple agent: Its sensor perceives the room temperature, and its actuator acts by turning the heat on or off.ReAct Model for AI Agents (Credit: Confluent) That classic definition provides a solid mental model. For today&#x27;s technology, we can translate it into four key components that make up a modern AI agent:Perception (the \"senses\"): This is how an agent takes in information about its digital or physical environment. It&#x27;s the input stream that allows the agent to understand the current state of the world relevant to its task.Reasoning engine (the \"brain\"): This is the core logic that processes the perceptions and decides what to do next. For modern agents, this is typically powered by a large language model (LLM). The engine is responsible for planning, breaking down large goals into smaller steps, handling errors and choosing the right tools for the job.Action (the \"hands\"): This is how an agent affects its environment to move closer to its goal. The ability to take action via tools is what gives an agent its power.Goal/objective: This is the overarching task or purpose that guides all of the agent&#x27;s actions. It is the \"why\" that turns a collection of tools into a purposeful system. The goal can be simple (\"Find the best price for this book\") or complex (\"Launch the marketing campaign for our new product\")Putting it all together, a true agent is a full-body system. The reasoning engine is the brain, but it’s useless without the senses (perception) to understand the world and the hands (actions) to change it. This complete system, all guided by a central goal, is what creates genuine agency.With these components in mind, the distinction we made earlier becomes clear. A standard chatbot isn&#x27;t a true agent. It perceives your question and acts by providing an answer, but it lacks an overarching goal and the ability to use external tools to accomplish it.An agent, on the other hand, is software that has agency. It has the capacity to act independently and dynamically toward a goal. And it&#x27;s this capacity that makes a discussion about the levels of autonomy so important.Learning from the past: How we learned to classify autonomyThe dizzying pace of AI can make it feel like we&#x27;re navigating uncharted territory. But when it comes to classifying autonomy, we’re not starting from scratch. Other industries have been working on this problem for decades, and their playbooks offer powerful lessons for the world of AI agents.The core challenge is always the same: How do you create a clear, shared language for the gradual handover of responsibility from a human to a machine?SAE levels of driving automationPerhaps the most successful framework comes from the automotive industry. The SAE J3016 standard defines six levels of driving automation, from Level 0 (fully manual) to Level 5 (fully autonomous).The SAE J3016 Levels of Driving Automation (Credit: SAE International) What makes this model so effective isn&#x27;t its technical detail, but its focus on two simple concepts:Dynamic driving task (DDT): This is everything involved in the real-time act of driving: steering, braking, accelerating and monitoring the road.Operational design domain (ODD): These are the specific conditions under which the system is designed to work. For example, \"only on divided highways\" or \"only in clear weather during the daytime.\"The question for each level is simple: Who is doing the DDT, and what is the ODD? At Level 2, the human must supervise at all times. At Level 3, the car handles the DDT within its ODD, but the human must be ready to take over. At Level 4, the car can handle everything within its ODD, and if it encounters a problem, it can safely pull over on its own.The key insight for AI agents: A robust framework isn&#x27;t about the sophistication of the AI \"brain.\" It&#x27;s about clearly defining the division of responsibility between human and machine under specific, well-defined conditions.Aviation&#x27;s 10 Levels of AutomationWhile the SAE’s six levels are great for broad classification, aviation offers a more granular model for systems designed for close human-machine collaboration. The Parasuraman, Sheridan, and Wickens model proposes a detailed 10-level spectrum of automation.Levels of Automation of Decision and Action Selection for Aviation (Credit: The MITRE Corporation)This framework is less about full autonomy and more about the nuances of interaction. For example:At Level 3, the computer \"narrows the selection down to a few\" for the human to choose from.At Level 6, the computer \"allows the human a restricted time to veto before it executes\" an action.At Level 9, the computer \"informs the human only if it, the computer, decides to.\"The key insight for AI agents: This model is perfect for describing the collaborative \"centaur\" systems we&#x27;re seeing today. Most AI agents won&#x27;t be fully autonomous (Level 10) but will exist somewhere on this spectrum, acting as a co-pilot that suggests, executes with approval or acts with a veto window.Robotics and unmanned systemsFinally, the world of robotics brings in another critical dimension: context. The National Institute of Standards and Technology&#x27;s (NIST) Autonomy Levels for Unmanned Systems (ALFUS) framework was designed for systems like drones and industrial robots.The Three-Axis Model for ALFUS (Credit: NIST) Its main contribution is adding context to the definition of autonomy, assessing it along three axes:Human independence: How much human supervision is required?Mission complexity: How difficult or unstructured is the task?Environmental complexity: How predictable and stable is the environment in which the agent operates?The key insight for AI agents: This framework reminds us that autonomy isn&#x27;t a single number. An agent performing a simple task in a stable, predictable digital environment (like sorting files in a single folder) is fundamentally less autonomous than an agent performing a complex task across the chaotic, unpredictable environment of the open internet, even if the level of human supervision is the same.The emerging frameworks for AI agentsHaving looked at the lessons from automotive, aviation and robotics, we can now examine the emerging frameworks designed for AI agents. While the field is still new and no single standard has won out, most proposals fall into three distinct, but often overlapping, categories based on the primary question they seek to answer.Category 1: The \"What can it do?\" frameworks (capability-focused)These frameworks classify agents based on their underlying technical architecture and what they are capable of achieving. They provide a roadmap for developers, outlining a progression of increasingly sophisticated technical milestones that often correspond directly to code patterns.A prime example of this developer-centric approach comes from Hugging Face. Their framework uses a star rating to show the gradual shift in control from human to AI:Five Levels of AI Agent Autonomy, as proposed by HuggingFace (Credit: Hugging Face) Zero stars (simple processor): The AI has no impact on the program&#x27;s flow. It simply processes information and its output is displayed, like a print statement. The human is in complete control.One star (router): The AI makes a basic decision that directs program flow, like choosing between two predefined paths (if/else). The human still defines how everything is done.Two stars (tool call): The AI chooses which predefined tool to use and what arguments to use with it. The human has defined the available tools, but the AI decides how to execute them.Three stars (multi-step agent): The AI now controls the iteration loop. It decides which tool to use, when to use it and whether to continue working on the task.Four stars (fully autonomous): The AI can generate and execute entirely new code to accomplish a goal, going beyond the predefined tools it was given.Strengths: This model is excellent for engineers. It&#x27;s concrete, maps directly to code and clearly benchmarks the transfer of executive control to the AI. Weaknesses: It is highly technical and less intuitive for non-developers trying to understand an agent&#x27;s real-world impact.Category 2: The \"How do we work together?\" frameworks (interaction-focused)This second category defines autonomy not by the agent’s internal skills, but by the nature of its relationship with the human user. The central question is: Who is in control, and how do we collaborate?This approach often mirrors the nuance we saw in the aviation models. For instance, a framework detailed in the paper Levels of Autonomy for AI Agents defines levels based on the user&#x27;s role:L1 - user as an operator: The human is in direct control (like a person using Photoshop with AI-assist features).L4 - user as an approver: The agent proposes a full plan or action, and the human must give a simple \"yes\" or \"no\" before it proceeds.L5 - user as an observer: The agent has full autonomy to pursue a goal and simply reports its progress and results back to the human.Levels of Autonomy for AI AgentsStrengths: These frameworks are highly intuitive and user-centric. They directly address the critical issues of control, trust, and oversight.Weaknesses: An agent with simple capabilities and one with highly advanced reasoning could both fall into the \"Approver\" level, so this approach can sometimes obscure the underlying technical sophistication.Category 3: The \"Who is responsible?\" frameworks (governance-focused)The final category is less concerned with how an agent works and more with what happens when it fails. These frameworks are designed to help answer crucial questions about law, safety and ethics.Think tanks like Germany&#x27;s Stiftung Neue VTrantwortung have analyzed AI agents through the lens of legal liability. Their work aims to classify agents in a way that helps regulators determine who is responsible for an agent&#x27;s actions: The user who deployed it, the developer who built it or the company that owns the platform it runs on?This perspective is essential for navigating complex regulations like the EU&#x27;s Artificial Intelligence Act, which will treat AI systems differently based on the level of risk they pose.Strengths: This approach is absolutely essential for real-world deployment. It forces the difficult but necessary conversations about accountability that build public trust.Weaknesses: It&#x27;s more of a legal or policy guide than a technical roadmap for developers.A comprehensive understanding requires looking at all three questions at once: An agent&#x27;s capabilities, how we interact with it and who is responsible for the outcome..Identifying the gaps and challengesLooking at the landscape of autonomy frameworks shows us that no single model is sufficient because the true challenges lie in the gaps between them, in areas that are incredibly difficult to define and measure.What is the \"Road\" for a digital agent?The SAE framework for self-driving cars gave us the powerful concept of an ODD, the specific conditions under which a system can operate safely. For a car, that might be \"divided highways, in clear weather, during the day.\" This is a great solution for a physical environment, but what’s the ODD for a digital agent?The \"road\" for an agent is the entire internet. An infinite, chaotic and constantly changing environment. Websites get redesigned overnight, APIs are deprecated and social norms in online communities shift. How do we define a \"safe\" operational boundary for an agent that can browse websites, access databases and interact with third-party services? Answering this is one of the biggest unsolved problems. Without a clear digital ODD, we can&#x27;t make the same safety guarantees that are becoming standard in the automotive world.This is why, for now, the most effective and reliable agents operate within well-defined, closed-world scenarios. As I argued in a recent VentureBeat article, forgetting the open-world fantasies and focusing on \"bounded problems\" is the key to real-world success. This means defining a clear, limited set of tools, data sources and potential actions. Beyond simple tool useToday&#x27;s agents are getting very good at executing straightforward plans. If you tell one to \"find the price of this item using Tool A, then book a meeting with Tool B,\" it can often succeed. But true autonomy requires much more. Many systems today hit a technical wall when faced with tasks that require:Long-term reasoning and planning: Agents struggle to create and adapt complex, multi-step plans in the face of uncertainty. They can follow a recipe, but they can&#x27;t yet invent one from scratch when things go wrong.Robust self-correction: What happens when an API call fails or a website returns an unexpected error? A truly autonomous agent needs the resilience to diagnose the problem, form a new hypothesis and try a different approach, all without a human stepping in.Composability: The future likely involves not one agent, but a team of specialized agents working together. Getting them to collaborate reliably, to pass information back and forth, delegate tasks and resolve conflicts is a monumental software engineering challenge that we are just beginning to tackle.The elephant in the room: Alignment and controlThis is the most critical challenge of all, because it&#x27;s not just technical, it&#x27;s deeply human. Alignment is the problem of ensuring an agent&#x27;s goals and actions are consistent with our intentions and values, even when those values are complex, unstated or nuanced.Imagine you give an agent the seemingly harmless goal of \"maximizing customer engagement for our new product.\" The agent might correctly determine that the most effective strategy is to send a dozen notifications a day to every user. The agent has achieved its literal goal perfectly, but it has violated the unstated, common-sense goal of \"don&#x27;t be incredibly annoying.\"This is a failure of alignment.The core difficulty, which organizations like the AI Alignment Forum are dedicated to studying, is that it is incredibly hard to specify fuzzy, complex human preferences in the precise, literal language of code. As agents become more powerful, ensuring they are not just capable but also safe, predictable and aligned with our true intent becomes the most important challenge we face.The future is agentic (and collaborative)The path forward for AI agents is not a single leap to a god-like super-intelligence, but a more practical and collaborative journey. The immense challenges of open-world reasoning and perfect alignment mean that the future is a team effort.We will see less of the single, all-powerful agent and more of an \"agentic mesh\" — a network of specialized agents, each operating within a bounded domain, working together to tackle complex problems. More importantly, they will work with us. The most valuable and safest applications will keep a human on the loop, casting them as a co-pilot or strategist to augment our intellect with the speed of machine execution. This \"centaur\" model will be the most effective and responsible path forward.The frameworks we&#x27;ve explored aren’t just theoretical. They’re practical tools for building trust, assigning responsibility and setting clear expectations. They help developers define limits and leaders shape vision, laying the groundwork for AI to become a dependable partner in our work and lives.Sean Falconer is Confluent&#x27;s AI entrepreneur in residence.",
          "feed_position": 5,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/5b0hTPUKh7PB9D7CfWwO22/3b31fdc3f95c670ce5096758d8760ccd/Agent_autonomy.png"
        }
      ],
      "featured_image": "https://images.ctfassets.net/jdtwqhzvc2n1/4816Y0YfsXKIENLGFsuaG6/a4620bd99d25c8fe32ab054bd16ff390/cfr0z3n_a_cybernetic_seal_looks_up_with_cute_alert_eyes_under_a_a6f43d56-7792-4d4f-bc1e-18b6dd2f5e4e.png",
      "popularity_score": 2011.5207269444445,
      "ai_summary": [
        "MIT researchers are developing a technique for self-improving language models.",
        "The technique, SEAL, allows models to generate synthetic data for fine-tuning.",
        "SEAL enables models to evolve by producing their own training data.",
        "The research was presented at the 39th Conference on Neural Information Processing Systems.",
        "The updated version expands on the prior framework."
      ]
    },
    {
      "id": "cluster_0",
      "coverage": 1,
      "updated_at": "Tue, 14 Oct 2025 07:05:16 +0000",
      "title": "After year of hardships, SpaceX’s Starship finally flirts with perfection",
      "neutral_headline": "After year of hardships, SpaceX’s Starship finally flirts with perfection",
      "items": [
        {
          "source": "Ars Technica Main",
          "url": "https://arstechnica.com/space/2025/10/after-year-of-hardships-spacexs-starship-finally-flirts-with-perfection/",
          "published_at": "Tue, 14 Oct 2025 07:05:16 +0000",
          "title": "After year of hardships, SpaceX’s Starship finally flirts with perfection",
          "standfirst": "This was the last flight of SpaceX's second-gen Starship design. Version 3 arrives next year.",
          "content": "SpaceX closed a troubled but instructive chapter in its Starship rocket program Monday with a near-perfect test flight that carried the stainless steel spacecraft halfway around the world from South Texas to the Indian Ocean. The rocket's 33 methane-fueled Raptor engines roared to life at 6:23 pm CDT (7:23 pm EDT; 23:23 UTC), throttling up to generate some 16.7 million pounds of thrust, by large measure more powerful than any rocket before Starship. Moments later, the 404-foot-tall (123.1-meter) rocket began a vertical climb away from SpaceX's test site in Starbase, Texas, near the US-Mexico border. From then on, the rocket executed its flight plan like clockwork. This was arguably SpaceX's most successful Starship test flight to date. The only flight with a similar claim occurred one year ago Monday, when the company caught the rocket's Super Heavy booster back at the launch pad after soaring to the uppermost fringes of the atmosphere. But that flight didn't accomplish as much in space.Read full article Comments",
          "feed_position": 0,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/starshipflight11recap1-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/starshipflight11recap1-1152x648.jpg",
      "popularity_score": 360.7585047222222,
      "ai_summary": [
        "SpaceX's Starship is nearing the end of its second-gen design.",
        "This was the last flight of SpaceX's second-gen Starship design.",
        "Version 3 of the Starship design is expected next year.",
        "The flight represents progress after a year of challenges.",
        "The focus is now shifting to the next iteration of Starship."
      ]
    },
    {
      "id": "cluster_36",
      "coverage": 1,
      "updated_at": "Mon, 13 Oct 2025 21:36:35 +0000",
      "title": "Hackers can steal 2FA codes and private messages from Android phones",
      "neutral_headline": "Hackers can steal 2FA codes and private messages from Android phones",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/security/2025/10/no-fix-yet-for-attack-that-lets-hackers-pluck-2fa-codes-from-android-phones/",
          "published_at": "Mon, 13 Oct 2025 21:36:35 +0000",
          "title": "Hackers can steal 2FA codes and private messages from Android phones",
          "standfirst": "Malicious app required to make \"Pixnapping\" attack work requires no permissions.",
          "content": "Android devices are vulnerable to a new attack that can covertly steal 2FA codes, location timelines, and other private data in less than 30 seconds. The new attack, named Pixnapping by the team of academic researchers who devised it, requires a victim to first install a malicious app on an Android phone or tablet. The app, which requires no system permissions, can then effectively read data that any other installed app displays on the screen. Pixnapping has been demonstrated on Google Pixel phones and the Samsung Galaxy S25 phone and likely could be modified to work on other models with additional work. Google released mitigations last month, but the researchers said a modified version of the attack works even when the update is installed. Like taking a screenshot Pixnapping attacks begin with the malicious app invoking Android programming interfaces that cause the authenticator or other targeted apps to send sensitive information to the device screen. The malicious app then runs graphical operations on individual pixels of interest to the attacker. Pixnapping then exploits a side channel that allows the malicious app to map the pixels at those coordinates to letters, numbers, or shapes.Read full article Comments",
          "feed_position": 0,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/01/008-family-galaxy-s25ultra-titaniumsilverblue-s25plus-navy-s25-icyblue-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/01/008-family-galaxy-s25ultra-titaniumsilverblue-s25plus-navy-s25-icyblue-1152x648.jpg",
      "popularity_score": 353.28044916666664,
      "ai_summary": [
        "Hackers can steal 2FA codes and private messages from Android phones.",
        "The \"Pixnapping\" attack requires a malicious app.",
        "The malicious app requires no special permissions.",
        "The attack exploits vulnerabilities in Android security.",
        "Users are advised to be cautious about app downloads."
      ]
    },
    {
      "id": "cluster_45",
      "coverage": 1,
      "updated_at": "Mon, 13 Oct 2025 19:52:23 +0000",
      "title": "Google’s Photoshop-killer AI model is coming to search, Photos, and NotebookLM",
      "neutral_headline": "Google’s Photoshop-killer AI model is coming to search, Photos, and NotebookLM",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/google/2025/10/googles-nano-banana-ai-image-editor-is-coming-to-search-photos-and-notebooklm/",
          "published_at": "Mon, 13 Oct 2025 19:52:23 +0000",
          "title": "Google’s Photoshop-killer AI model is coming to search, Photos, and NotebookLM",
          "standfirst": "After more than 5 billion AI image edits, Nano Banana is expanding.",
          "content": "Google began experimenting with conversational image editing earlier this year in the dev-focused AI studio, but the feature didn't remain experimental for long. Over the summer, Google rolled out the \"Nano Banana\" image-editing model in Gemini 2.5 Flash. You can use this feature to modify images with just a prompt, and now you don't even need to go to Gemini to use it. Google says Nano Banana is now coming to search, Google Photos, and NotebookLM. The AI image editor is coming to search via Lens and AI Mode. For Lens, you can simply open the app (iOS and Android) and snap a photo to get started. When the rollout is complete, you'll see a \"Create\" button at the bottom, with a banana icon. Tap that to enter a prompt, telling the AI how you'd like the photo changed. When you begin an edit in Lens, the Google app will display the results and offer the chance for follow-up edits in the AI Mode interface. Google is always looking for more ways to get people plugged into its conversational search bot, so there's also a separate way to access Nano Banana there. Simply select the \"Create image\" tool and enter your prompt to create an image. You can then continue the conversation to have Nano Banana change the image.Read full article Comments",
          "feed_position": 2,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/BananaLens-2096x1182.width-2200.format-webp-copy-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/BananaLens-2096x1182.width-2200.format-webp-copy-1152x648.jpg",
      "popularity_score": 348.5437825,
      "ai_summary": [
        "Google's AI image editing model is expanding its reach.",
        "The model has been used for over 5 billion AI image edits.",
        "It will be integrated into Google Search, Photos, and NotebookLM.",
        "The expansion aims to make the model more accessible.",
        "The model is designed to compete with Photoshop."
      ]
    },
    {
      "id": "cluster_41",
      "coverage": 1,
      "updated_at": "Mon, 13 Oct 2025 20:48:21 +0000",
      "title": "Measles outbreak in SC sends 150 unvaccinated kids into 21-day quarantine",
      "neutral_headline": "Measles outbreak in SC sends 150 unvaccinated kids into 21-day quarantine",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/health/2025/10/over-150-unvaccinated-kids-quarantined-for-21-days-in-sc-measles-outbreak/",
          "published_at": "Mon, 13 Oct 2025 20:48:21 +0000",
          "title": "Measles outbreak in SC sends 150 unvaccinated kids into 21-day quarantine",
          "standfirst": "The outbreak includes at least seven confirmed cases, but some are clearly being missed.",
          "content": "Health officials in South Carolina are warning that the highly infectious measles virus is spreading undetected in communities in the northern part of the state, specifically Spartanburg and Greenville counties. Last week, officials in Greenville identified an eighth measles case that is potentially linked to the outbreak. Seven outbreak cases had been confirmed since September 25 in neighboring Spartanburg, where transmission was identified in two schools: Fairforest Elementary and Global Academy, a public charter school. Across those two schools, at least 153 unvaccinated children were exposed to the virus and have been put in a 21-day quarantine, during which they are barred from attending school, state officials said in a press conference. Twenty-one days is the maximum incubation period, spanning from when a person is exposed to when they would develop a rash if infected.Read full article Comments",
          "feed_position": 1,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/02/GettyImages-2152300024-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/02/GettyImages-2152300024-1152x648.jpg",
      "popularity_score": 332.47656027777776,
      "ai_summary": [
        "A measles outbreak occurred in South Carolina.",
        "At least seven confirmed cases have been identified.",
        "150 unvaccinated children are in quarantine for 21 days.",
        "Some cases are likely being missed.",
        "The outbreak highlights the importance of vaccination."
      ]
    },
    {
      "id": "cluster_47",
      "coverage": 1,
      "updated_at": "Mon, 13 Oct 2025 19:27:10 +0000",
      "title": "Starship’s elementary era ends today with mega-rocket’s 11th test flight",
      "neutral_headline": "Starship’s elementary era ends today with mega-rocket’s 11th test flight",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2025/10/starships-elementary-era-ends-today-with-mega-rockets-11th-test-flight/",
          "published_at": "Mon, 13 Oct 2025 19:27:10 +0000",
          "title": "Starship’s elementary era ends today with mega-rocket’s 11th test flight",
          "standfirst": "\"The final phase of Starship’s trajectory on Flight 11 includes a dynamic banking maneuver.\"",
          "content": "SpaceX is set to launch the 11th full-scale test flight of the company's Starship rocket Monday evening, with hopes of capping a tumultuous year with a successful one-hour voyage from South Texas to the Indian Ocean. Liftoff of the Super Heavy booster with the Starship upper stage is scheduled for 6:15 pm CDT (7:15 pm EDT; 23:15 UTC). SpaceX has a 75-minute window to launch Monday. You can watch a livestream of the flight here. SpaceX's control team, positioned a couple of miles away from the launch pad at Starbase, Texas, will oversee the loading of more than 10.5 million pounds of super-cold methane and liquid oxygen into the two-stage rocket beginning about an hour before liftoff. In the final minutes of the countdown, the world's largest rocket will undergo a steering check, and the launch director will give a final \"go\" for launch.Read full article Comments",
          "feed_position": 3,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/starshipflight11pre1-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/starshipflight11pre1-1152x648.jpg",
      "popularity_score": 316.1235047222222,
      "ai_summary": [
        "Starship's 11th test flight marks the end of its elementary era.",
        "The final phase of the flight includes a dynamic banking maneuver.",
        "The flight is a significant step in Starship's development.",
        "The test flight is part of the mega-rocket's development.",
        "The flight is a key milestone for SpaceX."
      ]
    },
    {
      "id": "cluster_61",
      "coverage": 1,
      "updated_at": "Mon, 13 Oct 2025 17:16:56 +0000",
      "title": "Apple’s streaming service gets harder to tell apart from its streaming app, box",
      "neutral_headline": "Apple’s streaming service gets harder to tell apart from its streaming app, box",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/apple/2025/10/apple-tv-streaming-service-is-renamed-to-just-apple-tv/",
          "published_at": "Mon, 13 Oct 2025 17:16:56 +0000",
          "title": "Apple’s streaming service gets harder to tell apart from its streaming app, box",
          "standfirst": "Apple unifies its remaining streaming offerings by dropping the \"+\".",
          "content": "Apple has lightly rebranded its video-on-demand streaming service. The Netflix rival that has brought us critically acclaimed shows and movies like Slow Horses and The Lost Bus has gone from Apple TV+ to Apple TV. Apple announced the name change today in a press release that was primarily about the film F1: The Movie coming to its streaming service on December 12. Unlike previous announcements, however, today’s release referred to the streaming service as Apple TV, instead of Apple TV+. The announcement reads: Apple TV+ is now simply Apple TV, with a vibrant new identity. Apple didn’t specify how its streaming service’s “identity” has changed at all. As of this writing, accessing Apple’s streaming service via a browser or smart TV app still shows the original Apple TV+ branding.Read full article Comments",
          "feed_position": 5,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2024/12/sever2-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2024/12/sever2-1152x648.jpg",
      "popularity_score": 300.95294916666666,
      "ai_summary": [
        "Apple is unifying its streaming offerings.",
        "The company is dropping the \"+\" from its streaming service.",
        "The change aims to simplify the user experience.",
        "The move makes the service harder to distinguish.",
        "The change is part of Apple's streaming strategy."
      ]
    },
    {
      "id": "cluster_53",
      "coverage": 1,
      "updated_at": "Mon, 13 Oct 2025 18:52:06 +0000",
      "title": "To shield kids, California hikes fake nude fines to $250K max",
      "neutral_headline": "California Increases Penalties for AI-Generated Child Exploitation",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/10/to-shield-kids-california-hikes-fake-nude-fines-to-250k-max/",
          "published_at": "Mon, 13 Oct 2025 18:52:06 +0000",
          "title": "To shield kids, California hikes fake nude fines to $250K max",
          "standfirst": "California cracks down on AI as child safety concerns grow.",
          "content": "California is cracking down on AI technology deemed too harmful for kids, attacking two increasingly notorious child safety fronts: companion bots and deepfake pornography. On Monday, Governor Gavin Newsom signed the first-ever US law regulating companion bots after several teen suicides sparked lawsuits. Moving forward, California will require any companion bot platforms—including ChatGPT, Grok, Character.AI, and the like—to create and make public \"protocols to identify and address users’ suicidal ideation or expressions of self-harm.\"Read full article Comments",
          "feed_position": 4,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-1262115351-2-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-1262115351-2-1152x648.jpg",
      "popularity_score": 300.53906027777776,
      "ai_summary": [
        "California will increase fines for creating or distributing AI-generated child sexual abuse material.",
        "The maximum fine will increase to $250,000, reflecting growing child safety concerns.",
        "This action is part of California's broader efforts to regulate artificial intelligence.",
        "The state aims to protect children from the potential harms of AI technology.",
        "The legislation addresses the misuse of AI in creating harmful content."
      ]
    },
    {
      "id": "cluster_67",
      "coverage": 1,
      "updated_at": "Mon, 13 Oct 2025 16:15:53 +0000",
      "title": "Why Signal’s post-quantum makeover is an amazing engineering achievement",
      "neutral_headline": "Signal's Post-Quantum Encryption Design Is Significant",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/security/2025/10/why-signals-post-quantum-makeover-is-an-amazing-engineering-achievement/",
          "published_at": "Mon, 13 Oct 2025 16:15:53 +0000",
          "title": "Why Signal’s post-quantum makeover is an amazing engineering achievement",
          "standfirst": "New design sets a high standard for post-quantum readiness.",
          "content": "The encryption protecting communications against criminal and nation-state snooping is under threat. As private industry and governments get closer to building useful quantum computers, the algorithms protecting Bitcoin wallets, encrypted web visits, and other sensitive secrets will be useless. No one doubts the day will come, but as the now-common joke in cryptography circles observes, experts have been forecasting this cryptocalypse will arrive in the next 15 to 30 years for the past 30 years. The uncertainty has created something of an existential dilemma: Should network architects spend the billions of dollars required to wean themselves off quantum-vulnerable algorithms now, or should they prioritize their limited security budgets fighting more immediate threats such as ransomware and espionage attacks? Given the expense and no clear deadline, it’s little wonder that less than half of all TLS connections made inside the Cloudflare network and only 18 percent of Fortune 500 networks support quantum-resistant TLS connections. It's all but certain that many fewer organizations still are supporting quantum-ready encryption in less prominent protocols. Triumph of the cypherpunks One exception to the industry-wide lethargy is the engineering team that designs the Signal Protocol, the open source engine that powers the world’s most robust and resilient form of end-to-end encryption for multiple private chat apps, most notably the Signal Messenger. Eleven days ago, the nonprofit entity that develops the protocol, Signal Messenger LLC, published a 5,900-word write-up describing its latest updates that bring Signal a significant step toward being fully quantum-resistant.Read full article Comments",
          "feed_position": 6,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/signal-quantum-security-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/signal-quantum-security-1152x648.jpg",
      "popularity_score": 283.9354491666667,
      "ai_summary": [
        "Signal is implementing post-quantum cryptography to enhance user privacy.",
        "The new design is considered a major engineering achievement in the field.",
        "It sets a high standard for other messaging apps to follow.",
        "The update aims to protect against future quantum computing threats.",
        "Signal's approach is designed to be both secure and user-friendly."
      ]
    },
    {
      "id": "cluster_69",
      "coverage": 1,
      "updated_at": "Mon, 13 Oct 2025 15:33:42 +0000",
      "title": "Hans Koenigsmann, who investigated all of SpaceX’s rocket failures, is going to space",
      "neutral_headline": "SpaceX Rocket Failure Investigator Will Experience Spaceflight",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2025/10/one-of-spacexs-earliest-employees-is-going-to-space-via-blue-origin/",
          "published_at": "Mon, 13 Oct 2025 15:33:42 +0000",
          "title": "Hans Koenigsmann, who investigated all of SpaceX’s rocket failures, is going to space",
          "standfirst": "\"I've always been interested to experience Max Q from the inside.\"",
          "content": "Hans Koenigsmann is one of SpaceX's earliest, longest-tenured, and most-revered employees. When Elon Musk started the company in 2002, he was joined by two other \"founding\" employees, Tom Mueller in propulsion and Chris Thompson in structures. Koenigsmann was the next hire, brought on to develop avionics for the Falcon 1 rocket. Koenigsmann remained at the company for two decades before leaving SpaceX in late 2021. During that time, he transitioned from avionics to lead mission assurance and safety while also spearheading every major failure investigation of the Falcon 9 rocket. He was a beloved leader and mentor for his employees within the company's demanding culture.Read full article Comments",
          "feed_position": 8,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-621034912-1024x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-621034912-1024x648.jpg",
      "popularity_score": 280.2323936111111,
      "ai_summary": [
        "Hans Koenigsmann, a SpaceX veteran, will travel to space on a future mission.",
        "He investigated all SpaceX rocket failures, contributing to their success.",
        "Koenigsmann expressed interest in experiencing the Max Q phase.",
        "His experience provides a unique perspective on spaceflight.",
        "This marks a significant personal milestone for the SpaceX engineer."
      ]
    },
    {
      "id": "cluster_68",
      "coverage": 1,
      "updated_at": "Mon, 13 Oct 2025 15:49:12 +0000",
      "title": "4chan fined $26K for refusing to assess risks under UK Online Safety Act",
      "neutral_headline": "4chan Fined for Noncompliance with UK Online Safety Act",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/10/4chan-fined-26k-for-refusing-to-assess-risks-under-uk-online-safety-act/",
          "published_at": "Mon, 13 Oct 2025 15:49:12 +0000",
          "title": "4chan fined $26K for refusing to assess risks under UK Online Safety Act",
          "standfirst": "4chan fine may test if US will intervene to block UK’s Online Safety Act.",
          "content": "A battle over the United Kingdom's Online Safety Act (OSA) heated up Monday as UK regulator Ofcom fined the notorious image-hosting board 4chan about $26,000 for failing to provide a risk assessment detailing the potential harms of illegal content hosted on its forum. In a press release provided to Ars, Ofcom said 4chan refused to respond to two requests for information that the regulator considered \"routine.\" The first asked for the risk assessment and the second for 4chan's \"qualifying worldwide revenue.\" 4chan was anticipating the Monday fine, noting in a lawsuit—which was jointly filed with the online trolling forum Kiwi Farms in August and seeks to permanently enjoin Ofcom from enforcing OSA—that Ofcom had made it clear that because 4chan ignored Ofcom's emails, the fine was coming.Read full article Comments",
          "feed_position": 7,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-1814351886.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-1814351886.jpg",
      "popularity_score": 267.4907269444444,
      "ai_summary": [
        "4chan received a fine of $26,000 for failing to assess risks.",
        "The fine relates to the UK's Online Safety Act.",
        "The situation may test US intervention regarding the UK law.",
        "The act aims to regulate online content and protect users.",
        "This case highlights international tensions over internet regulation."
      ]
    },
    {
      "id": "cluster_71",
      "coverage": 1,
      "updated_at": "Mon, 13 Oct 2025 15:14:44 +0000",
      "title": "Layoffs, a “coding error,” chaos: Trump admin ravages the health dept.",
      "neutral_headline": "Trump Administration's Actions Impacted Health Department",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/health/2025/10/mass-firings-at-us-health-dept-partially-reversed-but-still-devastating/",
          "published_at": "Mon, 13 Oct 2025 15:14:44 +0000",
          "title": "Layoffs, a “coding error,” chaos: Trump admin ravages the health dept.",
          "standfirst": "Reports suggest the hardest hit is the CDC, which is already struggling to function.",
          "content": "Federal health agencies are reeling from mass layoffs on Friday that appear to have particularly devastated the Centers for Disease Control and Prevention, despite some terminations being rescinded on Saturday. Numbers are still sketchy, but reports from Friday indicate that more than 4,000 federal workers overall were initially targeted for layoffs. The Trump administration linked the firings to the ongoing government shutdown, which legal experts have suggested is illegal. Unions representing federal workers have already filed a lawsuit challenging the move. Of the reported 4,000 terminations, about 1,100 to 1,200 were among employees in the Department of Health and Human Services (HHS). HHS is a massive department that houses critical federal agencies, including the Centers for Disease Control and Prevention, the National Institutes of Health, the Food and Drug Administration, and the Centers for Medicare & Medicaid Services, among others. Before Trump's second term, the HHS workforce was about 82,000, but that was slashed to about 62,000 earlier this year amid initial cuts and efforts to push civil servants out.Read full article Comments",
          "feed_position": 9,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/08/GettyImages-2228551722-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/08/GettyImages-2228551722-1152x648.jpg",
      "popularity_score": 246.9162825,
      "ai_summary": [
        "The Trump administration's actions reportedly caused chaos within the health department.",
        "Layoffs and a \"coding error\" are cited as contributing factors.",
        "The Centers for Disease Control and Prevention (CDC) was particularly affected.",
        "The CDC is reportedly struggling to maintain its operational capacity.",
        "The reports suggest significant disruption to public health functions."
      ]
    },
    {
      "id": "cluster_101",
      "coverage": 1,
      "updated_at": "Sun, 12 Oct 2025 20:50:35 +0000",
      "title": "New Starfleet Academy trailer debuts at NYCC",
      "neutral_headline": "New Starfleet Academy Trailer Debuts at New York Comic Con",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/culture/2025/10/new-starfleet-academy-trailer-debuts-at-nycc/",
          "published_at": "Sun, 12 Oct 2025 20:50:35 +0000",
          "title": "New Starfleet Academy trailer debuts at NYCC",
          "standfirst": "Also: Our first look at S4 of Star Trek: Strange New Worlds.",
          "content": "The Star Trek universe panel at New York Comic Con (NYCC) this weekend concluded with a brand-new trailer for Star Trek: Starfleet Academy. The 10-episode series will follow the exploits of the first new crop of cadets in a century. Per the official premise: Star Trek: Starfleet Academy introduces viewers to a young group of cadets who come together to pursue a common dream of hope and optimism. Under the watchful and demanding eyes of their instructors, they discover what it takes to become Starfleet officers as they navigate blossoming friendships, explosive rivalries, first loves and a new enemy that threatens both the Academy and the Federation itself. The new cadets include Sandro Rosta as human orphan Caleb Mir; Karim Diané as a Klingon cadet, Jay-Den Kraag; Kerrice Brooks as Sam (Series Acclimation Mil), the first Kasquain to attend the Academy; George Hawkins as Darem Reymi, a Khionian cadet who wants to be a captain; Bella Shepard as Genesis Lythe, a Dar-Sha cadet; and Zoë Steiner as Tarima Sadal, daughter of the president of Betazed. In addition, Holly Hunter plays Academy chancellor Captain Nahla Ake; Gina Yashere plays Cadet Master Lura Thok; Becky Lynch plays a member of Starfleet bridge crew; and Paul Giamatti plays a half-Klingon, half-Tellarite character named Nus Braka, the series' chief villain.Read full article Comments",
          "feed_position": 12,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/academy1-1152x648-1760302457.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/academy1-1152x648-1760302457.jpg",
      "popularity_score": 145,
      "ai_summary": [
        "A new trailer for Starfleet Academy was revealed at NYCC.",
        "The event also featured the first look at Star Trek: Strange New Worlds Season 4.",
        "Fans received updates on upcoming Star Trek content.",
        "The trailer provided a preview of the new series.",
        "The presentation generated excitement among Star Trek enthusiasts."
      ]
    },
    {
      "id": "cluster_76",
      "coverage": 1,
      "updated_at": "Mon, 13 Oct 2025 14:57:01 +0000",
      "title": "Keep losing your key fob? Ford’s new “Truckle” is the answer.",
      "neutral_headline": "Ford Introduces \"Truckle\" to Prevent Key Fob Loss",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/cars/2025/10/keep-losing-your-key-fob-fords-new-truckle-is-the-answer/",
          "published_at": "Mon, 13 Oct 2025 14:57:01 +0000",
          "title": "Keep losing your key fob? Ford’s new “Truckle” is the answer.",
          "standfirst": "Keep your pants up and your F-150 unlocked at the same time.",
          "content": "I came across possibly one of the weirdest official automotive accessories this morning, courtesy of a friend's social media feed. It's called the \"Truckle,\" and it's a hand-crafted silver and bronze belt buckle that might be the envy of every other cowboy out there, since this one has a place to keep your F-150's key fob without ruining the lines of your jeans. The Truckle was designed by Utah-based A Cut Above Buckles, with a hand-engraved F-150 on the bump in the front. Behind the truck? Storage space for a Ford truck key fob, which should fit any F-150 from model year 2018 onward. \"You can put your key fob in the buckle—all your remote features work while it’s in the buckle,\" designer Andy Andrews told the Detroit Free Press. \"Once you have it in there, you're not going to lose that key fob. You’re not going to be scratching your head (wondering) where it’s at. It's right there with you in the Truckle.\"Read full article Comments",
          "feed_position": 10,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/Truckle_7-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/Truckle_7-1152x648.jpg",
      "popularity_score": 136.6210047222222,
      "ai_summary": [
        "Ford is introducing a new device called \"Truckle.\"",
        "The device is designed to prevent key fob loss.",
        "It aims to keep pants up and the F-150 unlocked.",
        "The Truckle offers a solution for key fob management.",
        "The product is intended for F-150 truck owners."
      ]
    },
    {
      "id": "cluster_80",
      "coverage": 1,
      "updated_at": "Mon, 13 Oct 2025 14:10:25 +0000",
      "title": "Software update bricks some Jeep 4xe hybrids over the weekend",
      "neutral_headline": "Software Update Bricks Some Jeep 4xe Hybrids",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/cars/2025/10/software-update-bricks-some-jeep-4xe-hybrids-over-the-weekend/",
          "published_at": "Mon, 13 Oct 2025 14:10:25 +0000",
          "title": "Software update bricks some Jeep 4xe hybrids over the weekend",
          "standfirst": "Jeep has pulled the update; owners are advised to ignore it if it already downloaded.",
          "content": "Owners of some Jeep Wrangler 4xe hybrids have been left stranded after installing an over-the-air software update this weekend. The automaker pushed out a telematics update for the Uconnect infotainment system that evidently wasn't ready, resulting in cars losing power while driving and then becoming stranded. Stranded Jeep owners have been detailing their experiences in forum and Reddit posts, as well as on YouTube. The buggy update doesn't appear to brick the car immediately. Instead, the failure appears to occur while driving—a far more serious problem. For some, this happened close to home and at low speed, but others claim to have experienced a powertrain failure at highway speeds. Jeep pulled the update after reports of problems, but the software had already downloaded to many owners' cars by then. A member of Stellantis' social engagement team told 4xe owners at a Jeep forum to ignore the update pop-up if they haven't installed it yet.Read full article Comments",
          "feed_position": 11,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/2024Wrangler4xe-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/2024Wrangler4xe-1152x648.jpg",
      "popularity_score": 135.84433805555557,
      "ai_summary": [
        "A software update caused issues for some Jeep 4xe hybrid owners.",
        "The update caused vehicles to become inoperable.",
        "Jeep has pulled the update to prevent further problems.",
        "Owners are advised to avoid the update if already downloaded.",
        "The issue highlights the risks of software updates."
      ]
    }
  ]
}