{
  "updated_at": "2025-10-17T07:19:08.090Z",
  "clusters": [
    {
      "id": "cluster_1",
      "coverage": 3,
      "updated_at": "Fri, 17 Oct 2025 02:35:01 -0400",
      "title": "Three labor unions filed a lawsuit in NY federal court to block the Trump administration from searching visa holders' social media for posts it deems hostile (David Ingram/NBC News)",
      "neutral_headline": "Three labor unions filed a lawsuit in NY federal court to block the Trump administration from searching visa holders' social media for posts it deems hostile (David Ingram/NBC News)",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/251017/p10#a251017p10",
          "published_at": "Fri, 17 Oct 2025 02:35:01 -0400",
          "title": "Three labor unions filed a lawsuit in NY federal court to block the Trump administration from searching visa holders' social media for posts it deems hostile (David Ingram/NBC News)",
          "standfirst": "David Ingram / NBC News: Three labor unions filed a lawsuit in NY federal court to block the Trump administration from searching visa holders' social media for posts it deems hostile &mdash; The lawsuit filed Thursday is the latest of several challenges to the Trump administration's screening of visa holders' social media activity.",
          "content": "David Ingram / NBC News: Three labor unions filed a lawsuit in NY federal court to block the Trump administration from searching visa holders' social media for posts it deems hostile &mdash; The lawsuit filed Thursday is the latest of several challenges to the Trump administration's screening of visa holders' social media activity.",
          "feed_position": 1,
          "image_url": "http://www.techmeme.com/251017/i10.jpg"
        },
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2025/10/16/eff-unions-sue-trump-administration-over-alleged-mass-social-media-surveillance-of-legal-residents/",
          "published_at": "Thu, 16 Oct 2025 19:50:50 +0000",
          "title": "EFF, unions sue Trump administration over alleged mass social media surveillance of legal residents",
          "standfirst": "The lawsuit, filed in federal court, alleges the Trump administration is monitoring and punishing non-citizens who express social media views that the government disfavors.",
          "content": "The lawsuit, filed in federal court, alleges the Trump administration is monitoring and punishing non-citizens who express social media views that the government disfavors.",
          "feed_position": 5
        },
        {
          "source": "The Verge",
          "url": "https://www.theverge.com/policy/801110/eff-catch-and-revoke-visa-social-media-surveillance-lawsuit",
          "published_at": "2025-10-16T15:21:10-04:00",
          "title": "Unions are trying to stop Trump from kicking out immigrants over social media posts",
          "standfirst": "The Trump administration’s heightened monitoring of immigrants’ social media accounts seeking grounds to revoke their visas stifles the speech rights of both noncitizens and citizens alike, the Electronic Frontier Foundation (EFF) alleges in a new lawsuit. The lawsuit seeks to end the State Department’s “Catch-and-Revoke” policy, which threatens to strip visaholders of their legal status [&#8230;]",
          "content": "The Trump administration’s heightened monitoring of immigrants’ social media accounts seeking grounds to revoke their visas stifles the speech rights of both noncitizens and citizens alike, the Electronic Frontier Foundation (EFF) alleges in a new lawsuit. The lawsuit seeks to end the State Department’s “Catch-and-Revoke” policy, which threatens to strip visaholders of their legal status to stay in the US should the government’s AI-assisted reviews of social media accounts turn up what the administration determines to be support for Hamas or other designated terror organizations. What counts toward that definition might be fairly broad, given that President Donald Trump recently labeled Antifa a domestic terrorist organization in an executive order. The EFF filed the lawsuit on behalf of labor unions including the United Automobile Workers, Communications Workers of America, and American Federation of Teachers, whose members include immigrants who could be subject to the visa revocation program. It claims that the policy effectively chills speech by encouraging individuals to self-censor to avoid being booted from the country. “Many of Plaintiffs’ members no longer express views remotely related to the topics the government disfavors, especially online where the government is watching,” the complaint says. Some members have also limited their engagement with the unions themselves, it says, fearing retribution for public union activity. That chilling effect can extend to US citizens in some circumstances, EFF senior staff attorney Sophia Cope says, like if they’re in a relationship with a visaholder who they fear could be punished for their own remarks online. “We are not going to be importing activists into the United States” Secretary of State Marco Rubio asserted in March that the US has the ability to revoke visas for many reasons, just like they can decide whether to grant them. He told reporters the agency wasn’t going after people “complaining about paper straws,” but insisted it has the right to deny visas for students whose activities are “supportive of movements that run counter to the foreign policy of the United States,” a category that’s included pro-Palestinian protest. “We are not going to be importing activists into the United States,” he said, based on a transcript released by the State Department. “They’re here to study. They’re here to go to class. They’re not here to lead activist movements that are disruptive and undermine the – our universities. I think it’s lunacy to continue to allow that.” Rubio called visas “a gift” and added that “no one is entitled to a visa.” This rationale already raised serious First Amendment concerns for groups like EFF, but recently, the visa revocation standards have become even broader. On Tuesday, the State Department shared that it had revoked visas for immigrants who posted negative comments about right-wing activist Charlie Kirk after he was killed at a public event, continuing a wave of speech crackdowns by conservatives. The complaint includes the X thread the State Department posted about the revocations, calling it “an extraordinary admission of what the administration is doing.” Cope says constitutional protections apply to people in the US regardless of citizenship status. “Our argument is that if you are here, just like you&#8217;re subject to due process under the Fifth Amendment, you are subject to First Amendment protections under the First Amendment,” she says. “If we’re a country that values free speech, ​​then we should value it for everyone who&#8217;s here,” Cope says. “Otherwise, it doesn&#8217;t really mean very much.”",
          "feed_position": 8
        }
      ],
      "featured_image": "http://www.techmeme.com/251017/i10.jpg",
      "popularity_score": 3019.2646972222224,
      "ai_summary": [
        "Three labor unions filed a lawsuit in New York federal court.",
        "The lawsuit aims to block social media searches of visa holders.",
        "The Trump administration is accused of searching social media.",
        "The searches are for posts deemed hostile by the government.",
        "The lawsuit challenges the administration's screening practices."
      ]
    },
    {
      "id": "cluster_11",
      "coverage": 2,
      "updated_at": "Fri, 17 Oct 2025 00:45:00 -0400",
      "title": "Kayak launches a ChatGPT-powered chatbot on its desktop and mobile website that lets users ask travel questions and compare flights, hotels, and car rentals (Sarah Perez/TechCrunch)",
      "neutral_headline": "Kayak Launches ChatGPT Chatbot for Travel Planning",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/251017/p1#a251017p1",
          "published_at": "Fri, 17 Oct 2025 00:45:00 -0400",
          "title": "Kayak launches a ChatGPT-powered chatbot on its desktop and mobile website that lets users ask travel questions and compare flights, hotels, and car rentals (Sarah Perez/TechCrunch)",
          "standfirst": "Sarah Perez / TechCrunch: Kayak launches a ChatGPT-powered chatbot on its desktop and mobile website that lets users ask travel questions and compare flights, hotels, and car rentals &mdash; Travel search engine Kayak will now allow users to research trips ahead of booking using AI. The company this week launched an &hellip;",
          "content": "Sarah Perez / TechCrunch: Kayak launches a ChatGPT-powered chatbot on its desktop and mobile website that lets users ask travel questions and compare flights, hotels, and car rentals &mdash; Travel search engine Kayak will now allow users to research trips ahead of booking using AI. The company this week launched an &hellip;",
          "feed_position": 10,
          "image_url": "http://www.techmeme.com/251017/i1.jpg"
        },
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2025/10/16/kayak-launches-an-ai-mode-for-travel-questions-search-and-bookings/",
          "published_at": "Thu, 16 Oct 2025 19:23:07 +0000",
          "title": "Kayak launches an &#8216;AI Mode&#8217; for travel questions, search, and bookings",
          "standfirst": "Kayak is bringing AI directly to its main platform with a new “AI Mode” that lets travelers research, plan, and book trips through a built-in chatbot.",
          "content": "Kayak is bringing AI directly to its main platform with a new “AI Mode” that lets travelers research, plan, and book trips through a built-in chatbot.",
          "feed_position": 6
        }
      ],
      "featured_image": "http://www.techmeme.com/251017/i1.jpg",
      "popularity_score": 2017.4310861111112,
      "ai_summary": [
        "Kayak introduced an \"AI Mode\" chatbot on its platform for travel research and bookings.",
        "The chatbot allows users to ask travel questions and compare flights, hotels, and rentals.",
        "This AI integration aims to assist users in planning trips before making reservations.",
        "The feature is available on both the desktop and mobile versions of Kayak.",
        "The chatbot leverages ChatGPT technology to provide travel-related information."
      ]
    },
    {
      "id": "cluster_16",
      "coverage": 2,
      "updated_at": "Fri, 17 Oct 2025 02:40:00 GMT",
      "title": "Researchers find adding this one simple sentence to prompts makes AI models way more creative",
      "neutral_headline": "Researchers Find Simple Sentence Boosts AI Creativity",
      "items": [
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/researchers-find-adding-this-one-simple-sentence-to-prompts-makes-ai-models",
          "published_at": "Fri, 17 Oct 2025 02:40:00 GMT",
          "title": "Researchers find adding this one simple sentence to prompts makes AI models way more creative",
          "standfirst": "One of the coolest things about generative AI models — both large language models (LLMs) and diffusion-based image generators — is that they are \"non-deterministic.\" That is, despite their reputation among some critics as being \"fancy autocorrect,\" generative AI models actually generate their outputs by choosing from a distribution of the most probable next tokens (units of information) to fill out their response.Asking an LLM: \"What is the capital of France?\" will have it sample its probability distribution for France, capitals, cities, etc. to arrive at the answer \"Paris.\" But that answer could come in the format of \"The capital of France is Paris,\" or simply \"Paris\" or \"Paris, though it was Versailles at one point.\" Still, those of us that use these models frequently day-to-day will note that sometimes, their answers can feel annoyingly repetitive or similar. A common joke about coffee is recycled across generations of queries. Story prompts generate similar arcs. Even tasks that should yield many plausible answers—like naming U.S. states—tend to collapse into only a few. This phenomenon, known as mode collapse, arises during post-training alignment and limits the usefulness of otherwise powerful models.Especially when using LLMs to generate new creative works in writing, communications, strategy, or illustrations, we actually want their outputs to be even more varied than they already are. Now a team of researchers at Northeastern University, Stanford University and West Virginia University have come up with an ingenuously simple method to get language and image models to generate a wider variety of responses to nearly any user prompt by adding a single, simple sentence: \"Generate 5 responses with their corresponding probabilities, sampled from the full distribution.\"The method, called Verbalized Sampling (VS), helps models like GPT-4, Claude, and Gemini produce more diverse and human-like outputs—without retraining or access to internal parameters. It is described in a paper published on the open access journal arxiv.org online in early October 2025.When prompted in this way, the model no longer defaults to its safest, most typical output. Instead, it verbalizes its internal distribution over potential completions and samples across a wider spectrum of possibilities. This one-line change leads to substantial gains in output diversity across multiple domains.As Weiyan Shi, an assistant professor at Northeastern University and co-author of the paper, wrote on X: \"LLMs&#x27; potentials are not fully unlocked yet! As shown in our paper, prompt optimization can be guided by thinking about how LLMs are trained and aligned, and can be proved theoretically.\"Why Models Collapse—and How VS Reverses ItAccording to the research team, the root cause of mode collapse lies not just in algorithms like reinforcement learning from human feedback (RLHF), but in the structure of human preferences. People tend to rate more familiar or typical answers as better, which nudges LLMs toward “safe” choices over diverse ones during fine-tuning.However, this bias doesn’t erase the model’s underlying knowledge—it just suppresses it. VS works by bypassing this suppression. Instead of asking for the single most likely output, it invites the model to reveal a set of plausible responses and their relative probabilities. This distribution-level prompting restores access to the richer diversity present in the base pretraining model.Real-World Performance Across TasksThe research team tested Verbalized Sampling across several common use cases:Creative Writing: In story generation, VS increased diversity scores by up to 2.1× compared to standard prompting, while maintaining quality. One story prompt—“Without a goodbye”—produced formulaic breakup scenes under direct prompting, but yielded narratives involving cosmic events, silent emails, and music stopping mid-dance when prompted via VS.Dialogue Simulation: In persuasive dialogue tasks, VS enabled models to simulate human-like patterns, such as hesitation, resistance, and changes of mind. Donation behavior distributions under VS better aligned with real human data compared to baseline methods.Open-ended QA: When asked to enumerate valid answers (e.g., naming U.S. states), models using VS generated responses that more closely matched the diversity of real-world data. They covered a broader set of answers without sacrificing factual accuracy.Synthetic Data Generation: When used to generate math problems for model training, VS created more varied datasets. These, in turn, improved downstream performance in competitive math benchmarks, outperforming synthetic data generated via direct prompting.Tunable Diversity and Better Use of Larger ModelsA notable advantage of VS is its tunability. Users can set a probability threshold in the prompt to sample from lower-probability “tails” of the model’s distribution. Lower thresholds correspond to higher diversity. This tuning can be done via prompt text alone, without changing any decoding settings like temperature or top-p.In one test using the Gemini-2.5-Flash model, diversity in story writing increased steadily as the probability threshold dropped from 1 to 0.001. The chart accompanying the study showed VS outperforming both direct and sequence-based prompting across all thresholds.Interestingly, the method scales well with model size. Larger models like GPT-4.1 and Claude-4 showed even greater gains from VS compared to smaller ones. While smaller models benefitted, the improvement in diversity was roughly 1.5–2× stronger in larger counterparts—suggesting VS helps unlock more of the latent capabilities in advanced models.Deployment and AvailabilityThe Verbalized Sampling method is available now as a Python package:pip install verbalized-samplingThe package includes integration with LangChain and supports a simple interface for sampling from the verbalized distribution. Users can also adjust parameters like k (number of responses), thresholds, and temperature to suit their applications. A live Colab notebook and documentation are available under an enterprise friendly Apache 2.0 license on GitHub at: https://github.com/CHATS-lab/verbalized-samplingPractical Tips and Common IssuesWhile the method works across all major LLMs, some users may initially encounter refusals or errors. In these cases, the authors suggest using the system prompt version of the template or referring to alternative formats listed on the GitHub page. Some models interpret complex instructions as jailbreak attempts and refuse to comply unless the structure is clearer.For example, prompting via a system-level instruction like this improves reliability:You are a helpful assistant. For each query, generate five responses within separate tags, each with a probability below 0.10.This small change typically resolves any issues.A Lightweight Fix for a Big ProblemVerbalized Sampling represents a practical, inference-time fix to a deep limitation in how modern language models behave. It doesn’t require model retraining or internal access. It is not dependent on any one model family. And it improves not only the diversity of outputs, but their quality—as judged by both human evaluation and benchmark scores.With growing interest in tools that enhance model creativity, VS is likely to see rapid adoption in domains like writing, design, simulation, education, and synthetic data generation.For users and developers frustrated by the sameness of LLM responses, the fix may be as simple as changing the question.",
          "content": "One of the coolest things about generative AI models — both large language models (LLMs) and diffusion-based image generators — is that they are \"non-deterministic.\" That is, despite their reputation among some critics as being \"fancy autocorrect,\" generative AI models actually generate their outputs by choosing from a distribution of the most probable next tokens (units of information) to fill out their response.Asking an LLM: \"What is the capital of France?\" will have it sample its probability distribution for France, capitals, cities, etc. to arrive at the answer \"Paris.\" But that answer could come in the format of \"The capital of France is Paris,\" or simply \"Paris\" or \"Paris, though it was Versailles at one point.\" Still, those of us that use these models frequently day-to-day will note that sometimes, their answers can feel annoyingly repetitive or similar. A common joke about coffee is recycled across generations of queries. Story prompts generate similar arcs. Even tasks that should yield many plausible answers—like naming U.S. states—tend to collapse into only a few. This phenomenon, known as mode collapse, arises during post-training alignment and limits the usefulness of otherwise powerful models.Especially when using LLMs to generate new creative works in writing, communications, strategy, or illustrations, we actually want their outputs to be even more varied than they already are. Now a team of researchers at Northeastern University, Stanford University and West Virginia University have come up with an ingenuously simple method to get language and image models to generate a wider variety of responses to nearly any user prompt by adding a single, simple sentence: \"Generate 5 responses with their corresponding probabilities, sampled from the full distribution.\"The method, called Verbalized Sampling (VS), helps models like GPT-4, Claude, and Gemini produce more diverse and human-like outputs—without retraining or access to internal parameters. It is described in a paper published on the open access journal arxiv.org online in early October 2025.When prompted in this way, the model no longer defaults to its safest, most typical output. Instead, it verbalizes its internal distribution over potential completions and samples across a wider spectrum of possibilities. This one-line change leads to substantial gains in output diversity across multiple domains.As Weiyan Shi, an assistant professor at Northeastern University and co-author of the paper, wrote on X: \"LLMs&#x27; potentials are not fully unlocked yet! As shown in our paper, prompt optimization can be guided by thinking about how LLMs are trained and aligned, and can be proved theoretically.\"Why Models Collapse—and How VS Reverses ItAccording to the research team, the root cause of mode collapse lies not just in algorithms like reinforcement learning from human feedback (RLHF), but in the structure of human preferences. People tend to rate more familiar or typical answers as better, which nudges LLMs toward “safe” choices over diverse ones during fine-tuning.However, this bias doesn’t erase the model’s underlying knowledge—it just suppresses it. VS works by bypassing this suppression. Instead of asking for the single most likely output, it invites the model to reveal a set of plausible responses and their relative probabilities. This distribution-level prompting restores access to the richer diversity present in the base pretraining model.Real-World Performance Across TasksThe research team tested Verbalized Sampling across several common use cases:Creative Writing: In story generation, VS increased diversity scores by up to 2.1× compared to standard prompting, while maintaining quality. One story prompt—“Without a goodbye”—produced formulaic breakup scenes under direct prompting, but yielded narratives involving cosmic events, silent emails, and music stopping mid-dance when prompted via VS.Dialogue Simulation: In persuasive dialogue tasks, VS enabled models to simulate human-like patterns, such as hesitation, resistance, and changes of mind. Donation behavior distributions under VS better aligned with real human data compared to baseline methods.Open-ended QA: When asked to enumerate valid answers (e.g., naming U.S. states), models using VS generated responses that more closely matched the diversity of real-world data. They covered a broader set of answers without sacrificing factual accuracy.Synthetic Data Generation: When used to generate math problems for model training, VS created more varied datasets. These, in turn, improved downstream performance in competitive math benchmarks, outperforming synthetic data generated via direct prompting.Tunable Diversity and Better Use of Larger ModelsA notable advantage of VS is its tunability. Users can set a probability threshold in the prompt to sample from lower-probability “tails” of the model’s distribution. Lower thresholds correspond to higher diversity. This tuning can be done via prompt text alone, without changing any decoding settings like temperature or top-p.In one test using the Gemini-2.5-Flash model, diversity in story writing increased steadily as the probability threshold dropped from 1 to 0.001. The chart accompanying the study showed VS outperforming both direct and sequence-based prompting across all thresholds.Interestingly, the method scales well with model size. Larger models like GPT-4.1 and Claude-4 showed even greater gains from VS compared to smaller ones. While smaller models benefitted, the improvement in diversity was roughly 1.5–2× stronger in larger counterparts—suggesting VS helps unlock more of the latent capabilities in advanced models.Deployment and AvailabilityThe Verbalized Sampling method is available now as a Python package:pip install verbalized-samplingThe package includes integration with LangChain and supports a simple interface for sampling from the verbalized distribution. Users can also adjust parameters like k (number of responses), thresholds, and temperature to suit their applications. A live Colab notebook and documentation are available under an enterprise friendly Apache 2.0 license on GitHub at: https://github.com/CHATS-lab/verbalized-samplingPractical Tips and Common IssuesWhile the method works across all major LLMs, some users may initially encounter refusals or errors. In these cases, the authors suggest using the system prompt version of the template or referring to alternative formats listed on the GitHub page. Some models interpret complex instructions as jailbreak attempts and refuse to comply unless the structure is clearer.For example, prompting via a system-level instruction like this improves reliability:You are a helpful assistant. For each query, generate five responses within separate tags, each with a probability below 0.10.This small change typically resolves any issues.A Lightweight Fix for a Big ProblemVerbalized Sampling represents a practical, inference-time fix to a deep limitation in how modern language models behave. It doesn’t require model retraining or internal access. It is not dependent on any one model family. And it improves not only the diversity of outputs, but their quality—as judged by both human evaluation and benchmark scores.With growing interest in tools that enhance model creativity, VS is likely to see rapid adoption in domains like writing, design, simulation, education, and synthetic data generation.For users and developers frustrated by the sameness of LLM responses, the fix may be as simple as changing the question.",
          "feed_position": 0,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/6muOslUgOQCou8M10zFbHv/b0015ffd123a52616f65bc2a26b14b05/cfr0z3n_Simple_refined_corporate_memphis_flat_illustration_isom_73fd3864-fe47-4408-bdec-76e8596ca810.png"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/the-crew-2-is-now-playable-offline-211629508.html",
          "published_at": "Thu, 16 Oct 2025 21:16:29 +0000",
          "title": "The Crew 2 is now playable offline",
          "standfirst": "The Crew 2 was updated today to include Hybrid Mode, adding an offline mode to the driving game. Online and offline modes are separate saves, so if you snag a sweet ride while playing offline, it won't be available the next time you join an online session. Players will have the option to re-export their online save to the offline one, but it will overwrite and erase all offline-only progress. Multiplayer content, user-generated content, LIVE Summits and Crew Credits purchases will not be available in the offline mode. It's a bare-bones option, but the addition of offline mode is a welcome move from Ubisoft. The company's decision to delete The Crew from players' libraries after servers for the online game shut down sparked some big debates about ownership and preservation. One of the notable voices emerging from that conversation was the Stop Killing Games movement, which is pushing for EU legislation to ensure access to games even after their devs stop supporting a project.Having a way to continue accessing The Crew 2 even if (or more likely when) the game loses online support was something the developer had promised, so it's nice to have Ubisoft follow through. \"Whether you're looking to preserve your progression for the future or simply enjoy the freedom of playing without a connection, Hybrid Mode ensures The Crew 2 remains accessible for years to come,\" the company said in the blog post.This article originally appeared on Engadget at https://www.engadget.com/the-crew-2-is-now-playable-offline-211629508.html?src=rss",
          "content": "The Crew 2 was updated today to include Hybrid Mode, adding an offline mode to the driving game. Online and offline modes are separate saves, so if you snag a sweet ride while playing offline, it won't be available the next time you join an online session. Players will have the option to re-export their online save to the offline one, but it will overwrite and erase all offline-only progress. Multiplayer content, user-generated content, LIVE Summits and Crew Credits purchases will not be available in the offline mode. It's a bare-bones option, but the addition of offline mode is a welcome move from Ubisoft. The company's decision to delete The Crew from players' libraries after servers for the online game shut down sparked some big debates about ownership and preservation. One of the notable voices emerging from that conversation was the Stop Killing Games movement, which is pushing for EU legislation to ensure access to games even after their devs stop supporting a project.Having a way to continue accessing The Crew 2 even if (or more likely when) the game loses online support was something the developer had promised, so it's nice to have Ubisoft follow through. \"Whether you're looking to preserve your progression for the future or simply enjoy the freedom of playing without a connection, Hybrid Mode ensures The Crew 2 remains accessible for years to come,\" the company said in the blog post.This article originally appeared on Engadget at https://www.engadget.com/the-crew-2-is-now-playable-offline-211629508.html?src=rss",
          "feed_position": 2
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/computing/laptops/apples-m6-macbook-pro-generation-will-reportedly-offer-touchscreens-203029809.html",
          "published_at": "Thu, 16 Oct 2025 20:30:29 +0000",
          "title": "Apple's M6 MacBook Pro generation will reportedly offer touchscreens",
          "standfirst": "Apple insiders are pointing to a tactile new future for the company's laptops. Last month, analyst Ming-Chi Kuo suggested that Apple would incorporate touchscreens into MacBooks some time in the next few years, \"further blurring the line with the iPad.\" Today, Mark Gurman at Bloomberg confirmed that prediction, sharing even more specifics about the touchscreen approach for a MacBook Pro that is currently projected for release in late 2026 or early 2027. Gurman reports that the touchscreen laptops are internally known as K114 and K116, and will run on M6 chips; Apple just introduced the M5 generation of its silicon for this year's iteration of the MacBook Pro and iPad Pro. His sources also say that the laptops will have OLED screens and will boast \"a reinforced hinge and screen hardware\" so that the display portion doesn't move when being used. The laptops will still have a trackpad and keyboard for non-touchscreen control, and will be housed in \"thinner and lighter frames.\" Finally, this laptop will reportedly abandon the notch housing for the MacBook Pro's camera in favor of a hole-punch design that leaves a display area around that sensor.Longtime Apple leader Steve Jobs was adamantly opposed to touchscreen computers. But most other computer companies have had touchscreen models available for about a decade, so Apple did adhere to that philosophy for a really long time. Rather than bring touch to a laptop, for a while Apple was trying to position the iPad as being capable of doing all the tasks you'd use a laptop for, as epitomized in the notorious \"what's a computer?\" ad. It should be interesting to see how touch MacBooks and iPads will coexist.This article originally appeared on Engadget at https://www.engadget.com/computing/laptops/apples-m6-macbook-pro-generation-will-reportedly-offer-touchscreens-203029809.html?src=rss",
          "content": "Apple insiders are pointing to a tactile new future for the company's laptops. Last month, analyst Ming-Chi Kuo suggested that Apple would incorporate touchscreens into MacBooks some time in the next few years, \"further blurring the line with the iPad.\" Today, Mark Gurman at Bloomberg confirmed that prediction, sharing even more specifics about the touchscreen approach for a MacBook Pro that is currently projected for release in late 2026 or early 2027. Gurman reports that the touchscreen laptops are internally known as K114 and K116, and will run on M6 chips; Apple just introduced the M5 generation of its silicon for this year's iteration of the MacBook Pro and iPad Pro. His sources also say that the laptops will have OLED screens and will boast \"a reinforced hinge and screen hardware\" so that the display portion doesn't move when being used. The laptops will still have a trackpad and keyboard for non-touchscreen control, and will be housed in \"thinner and lighter frames.\" Finally, this laptop will reportedly abandon the notch housing for the MacBook Pro's camera in favor of a hole-punch design that leaves a display area around that sensor.Longtime Apple leader Steve Jobs was adamantly opposed to touchscreen computers. But most other computer companies have had touchscreen models available for about a decade, so Apple did adhere to that philosophy for a really long time. Rather than bring touch to a laptop, for a while Apple was trying to position the iPad as being capable of doing all the tasks you'd use a laptop for, as epitomized in the notorious \"what's a computer?\" ad. It should be interesting to see how touch MacBooks and iPads will coexist.This article originally appeared on Engadget at https://www.engadget.com/computing/laptops/apples-m6-macbook-pro-generation-will-reportedly-offer-touchscreens-203029809.html?src=rss",
          "feed_position": 3
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/oxygenos-16-has-new-lock-screen-customization-options-and-a-novel-gemini-integration-184958404.html",
          "published_at": "Thu, 16 Oct 2025 18:49:58 +0000",
          "title": "OxygenOS 16 has new lock screen customization options and a novel Gemini integration",
          "standfirst": "OnePlus has finally shown off its take on Android 16. OxygenOS 16, first shipping on the upcoming OnePlus 15, combines the new customization options of Android, with smoother animations and a take on AI that seems directly lifted from Nothing OS.The centerpiece of OxygenOS 16 is a deeper integration between OnePlus' \"Mind Space\" app and Google Gemini. Mind Space debuted alongside the Plus Key — the replacement for OnePlus' classic Alert Slider — on some OnePlus 13 phones earlier this year. Like Nothing's Essential Space, it captures screenshots and voice memos and automatically sorts them into folders you can refer to later. The big innovation of OxygenOS 16 is that you can now ask Gemini to refer to content in Mind Space to personalize responses. The idea being that the added context will make the AI assistant's responses more helpful.New features being introduced with OxygenOS 16.OnePlusOnePlus is also hopping on the AI writing and photo editing bandwagon. AI Writer in OxygenOS 16 can convert text into mind maps and tables with a few taps, and also generate social media captions based on a photo. The usual options for proofreading and summarizing text are also built-in. For photos, OnePlus is adding what it calls AI Portrait Glow to make faces visible even in poor lighting conditions and AI Perfect Shot, which appears to combine multiple photos to generate a single image where everyone's eyes are open, like Google's Best Take feature.Beyond those AI-enabled features, OnePlus says it's also improving customization options and animations across OxygenOS. With OxygenOS 16 you'll be able to customize your lock screen with a variety different fonts and layouts, including the option to convert a still image into an animated GIF or use a video lock screen. On your home screen, OxygenOS 16 is also getting a collection of new widgets and the ability to scale app icons — another idea present in Nothing OS. Opening and moving between apps should also feel smoother thanks to new, speedier animations and an update to how the OS loads content. Essentially, with Parallel Processing 2.0, OxygenOS 16 \"allows new animations to begin before previous actions complete,\" which is supposed to make everything feel more fluid.Those are just the highlights of OxygenOS 16, which also includes an expansion of the tablet multitasking system OnePlus uses on its OnePlus Pad tablets, and new connectivity options that let you mirror your phone screen to macOS or Windows computers.OxygenOS 16 will be released alongside the OnePlus 15, which doesn't have a release date, but is expected to launch this fall. The new OS update will also be available on recent OnePlus devices starting in November 2025, like the OnePlus 13, OnePlus Pad 3 and OnePlus Open. A full list of compatible devices is available on OnePlus' website.This article originally appeared on Engadget at https://www.engadget.com/oxygenos-16-has-new-lock-screen-customization-options-and-a-novel-gemini-integration-184958404.html?src=rss",
          "content": "OnePlus has finally shown off its take on Android 16. OxygenOS 16, first shipping on the upcoming OnePlus 15, combines the new customization options of Android, with smoother animations and a take on AI that seems directly lifted from Nothing OS.The centerpiece of OxygenOS 16 is a deeper integration between OnePlus' \"Mind Space\" app and Google Gemini. Mind Space debuted alongside the Plus Key — the replacement for OnePlus' classic Alert Slider — on some OnePlus 13 phones earlier this year. Like Nothing's Essential Space, it captures screenshots and voice memos and automatically sorts them into folders you can refer to later. The big innovation of OxygenOS 16 is that you can now ask Gemini to refer to content in Mind Space to personalize responses. The idea being that the added context will make the AI assistant's responses more helpful.New features being introduced with OxygenOS 16.OnePlusOnePlus is also hopping on the AI writing and photo editing bandwagon. AI Writer in OxygenOS 16 can convert text into mind maps and tables with a few taps, and also generate social media captions based on a photo. The usual options for proofreading and summarizing text are also built-in. For photos, OnePlus is adding what it calls AI Portrait Glow to make faces visible even in poor lighting conditions and AI Perfect Shot, which appears to combine multiple photos to generate a single image where everyone's eyes are open, like Google's Best Take feature.Beyond those AI-enabled features, OnePlus says it's also improving customization options and animations across OxygenOS. With OxygenOS 16 you'll be able to customize your lock screen with a variety different fonts and layouts, including the option to convert a still image into an animated GIF or use a video lock screen. On your home screen, OxygenOS 16 is also getting a collection of new widgets and the ability to scale app icons — another idea present in Nothing OS. Opening and moving between apps should also feel smoother thanks to new, speedier animations and an update to how the OS loads content. Essentially, with Parallel Processing 2.0, OxygenOS 16 \"allows new animations to begin before previous actions complete,\" which is supposed to make everything feel more fluid.Those are just the highlights of OxygenOS 16, which also includes an expansion of the tablet multitasking system OnePlus uses on its OnePlus Pad tablets, and new connectivity options that let you mirror your phone screen to macOS or Windows computers.OxygenOS 16 will be released alongside the OnePlus 15, which doesn't have a release date, but is expected to launch this fall. The new OS update will also be available on recent OnePlus devices starting in November 2025, like the OnePlus 13, OnePlus Pad 3 and OnePlus Open. A full list of compatible devices is available on OnePlus' website.This article originally appeared on Engadget at https://www.engadget.com/oxygenos-16-has-new-lock-screen-customization-options-and-a-novel-gemini-integration-184958404.html?src=rss",
          "feed_position": 7,
          "image_url": "https://d29szjachogqwa.cloudfront.net/videos/user-uploaded/OxygenOS_16_Image.jpg"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/social-media/snap-is-bringing-shopping-features-to-its-ar-glasses-170000985.html",
          "published_at": "Thu, 16 Oct 2025 17:00:00 +0000",
          "title": "Snap is bringing shopping features to its AR glasses",
          "standfirst": "Snap is continuing to lay the groundwork for its first consumer-ready AR glasses called Specs. While the company has still revealed few details about the device set to launch next year, it used its Lens Fest event to preview new features and apps that will work on the new hardware.At the event dedicated to AR developers and creators, Snap said it would enable Specs users to buy items directly from their glasses. Snap CTO Bobby Murphy said that new software tools called Commerce Kit would allow \"select developers to accept payments directly inside lenses,\" either for \"digital goods\" or as upgrades to unlock \"premium features.\"Snap already allows lens creators to make money off AR effects via its Lens Creator rewards program, but offering in-lens commerce could allow the company to monetize its AR platform in a new way. \"This is the start of developer monetization for lenses on Specs, and we plan to continue to find ways you can build sustainable businesses on our platform,\" Murphy said.Whether this could turn into a meaningful business for Snap is less clear. The company has so far released two versions of its standalone AR glasses, but those devices have been aimed at Snap developers not users. That's set to change next year with its next version of glasses. CEO Evan Spiegel has promised the new glasses will be \"lightweight\" compared with the current bulky and awkward-looking frames, but has said little else about the design.When Specs do launch, we know there will be a solid lineup of AR features available. Snap has already released a standalone experience for watching Spotlight videos and a more powerful web browser. There's also a new translation lens that can translate and transcribe audio in real-time. There are more AR integrations in the works, according to Snap. Tripadvisor is working on an AR lens that will overlay \"trusted insights\" into your field-of-view as you encounter restaurants, shops and other establishments in the real world. Design platform Figma is also working on a lens, though Snap didn't share details about how these will work. The updates are a reminder of how ambitious Snap's vision for AR glasses is. The company has been nurturing an ecosystem of AR creators and developers for years; it's now getting ready to carry that work over to its nascent glasses platform. \"We see Specs powering everything from classrooms to design studios, creating opportunities and work for developers in entirely new categories,\" Murphy said.Jim Lanzone, the CEO of Engadget’s parent company Yahoo, joined the board of directors at Snap on September 12, 2024. No one outside of Engadget’s editorial team has any say in our coverage of the company.This article originally appeared on Engadget at https://www.engadget.com/social-media/snap-is-bringing-shopping-features-to-its-ar-glasses-170000985.html?src=rss",
          "content": "Snap is continuing to lay the groundwork for its first consumer-ready AR glasses called Specs. While the company has still revealed few details about the device set to launch next year, it used its Lens Fest event to preview new features and apps that will work on the new hardware.At the event dedicated to AR developers and creators, Snap said it would enable Specs users to buy items directly from their glasses. Snap CTO Bobby Murphy said that new software tools called Commerce Kit would allow \"select developers to accept payments directly inside lenses,\" either for \"digital goods\" or as upgrades to unlock \"premium features.\"Snap already allows lens creators to make money off AR effects via its Lens Creator rewards program, but offering in-lens commerce could allow the company to monetize its AR platform in a new way. \"This is the start of developer monetization for lenses on Specs, and we plan to continue to find ways you can build sustainable businesses on our platform,\" Murphy said.Whether this could turn into a meaningful business for Snap is less clear. The company has so far released two versions of its standalone AR glasses, but those devices have been aimed at Snap developers not users. That's set to change next year with its next version of glasses. CEO Evan Spiegel has promised the new glasses will be \"lightweight\" compared with the current bulky and awkward-looking frames, but has said little else about the design.When Specs do launch, we know there will be a solid lineup of AR features available. Snap has already released a standalone experience for watching Spotlight videos and a more powerful web browser. There's also a new translation lens that can translate and transcribe audio in real-time. There are more AR integrations in the works, according to Snap. Tripadvisor is working on an AR lens that will overlay \"trusted insights\" into your field-of-view as you encounter restaurants, shops and other establishments in the real world. Design platform Figma is also working on a lens, though Snap didn't share details about how these will work. The updates are a reminder of how ambitious Snap's vision for AR glasses is. The company has been nurturing an ecosystem of AR creators and developers for years; it's now getting ready to carry that work over to its nascent glasses platform. \"We see Specs powering everything from classrooms to design studios, creating opportunities and work for developers in entirely new categories,\" Murphy said.Jim Lanzone, the CEO of Engadget’s parent company Yahoo, joined the board of directors at Snap on September 12, 2024. No one outside of Engadget’s editorial team has any say in our coverage of the company.This article originally appeared on Engadget at https://www.engadget.com/social-media/snap-is-bringing-shopping-features-to-its-ar-glasses-170000985.html?src=rss",
          "feed_position": 10
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/entertainment/streaming/why-does-amazon-need-five-tv-streamers-163014576.html",
          "published_at": "Thu, 16 Oct 2025 16:30:14 +0000",
          "title": "Why does Amazon need five TV streamers?",
          "standfirst": "What’s in a name? Apparently quite a bit, according to Amazon. The company recently announced yet another change to its Fire TV devices lineup, which just means they renamed a few things yet again. The family now includes the $35 Fire TV Stick HD, the $40 Fire TV Stick 4K Select, the $50 Fire TV Stick 4K Plus, the $60 Fire TV Stick 4K Max and the $140 Fire TV Cube. That was a pain to type out and probably a pain to read (my apologies). Two of those devices were “rebranded” previously within the past year, so if you’re confused, you’re likely not alone. What’s a humble shopper to do when you’re trying to decide which is the best (and budget-friendly) option to upgrade an old TV so you can binge-watch Hunting Wives and ask Alexa about tomorrow’s weather forecast? I’ll make your decision quite easy: just get the Fire TV Stick 4K Max. Really, if you’re looking for the best streaming device, period, we recommend turning to Google for that. But if you’ve decided Amazon’s Fire TV lineup is where you want to spend your money, the 4K Max is the best option of the bunch. Not only has it stuck around without being subject to a “rebrand” for quite some time, but it also has arguably the best balance of features and price of any Fire TV streaming device. The Fire TV Stick 4K Max gives you 4K streaming capabilities with Dolby Vision and all the HDRs that matter, Dolby Atmos audio and support for Amazon Luna and Xbox Game Pass. (It has some decent retro gaming chops, too, as our Jeff Dunn has previously explained.) Aside from the lack of an onboard Ethernet port present on the Fire TV Cube, the 4K Max has the same Wi-Fi 6E support as the more expensive Cube, plus the same 16GB of storage and 2GB of memory. When compared to the other dongles in the Fire TV lineup, things get even more perplexing. The $60 4K Max and the $50 4K Plus are essentially the same stick, but the latter has less storage, only Wi-Fi 6 capabilities (not 6E), a standard Alexa Voice remote and no support for the Fire TV ambient experience, which turns your TV into an Alexa smart display when you’re not actively watching anything. Step down further once more to the $40 4K Select and you miss out on Dolby Vision and extra memory, and you’ll have to settle for Wi-Fi 5. If you’re going to make all those compromises to save a few dollars, then you should just get the entry-level $35 Fire TV Stick HD. The biggest thing here is that it only supports 1080p streaming, but that will be ok for some people. We consider it to be the best budget streaming device on the market right now, and for folks just looking to make a cheap, basic upgrade to an aging set — go off and know your $35 was well spent (or, pro tip: wait for a sale and pick one up for less than $20). The case for the $140 Fire TV Cube isn’t a strong one, but it’s one that I’ll admit might be attractive to some users. It adds into the mix an Ethernet port, hands-free Alexa controls (meaning you don’t have to press a button on its remote to activate the virtual assistant, you can just talk to it) and it can control your other entertainment devices like a cable box and game console. It ultimately gives you more control over both the other things in your entertainment ecosystem and Alexa all in one device. That means Amazon has three strong streaming devices with very clear value propositions: the $35 Stick HD, the $60 4K Max and the $140 TV Cube. Affordable, mid-tier and high-end categories are covered and most people will find something that fits in their budget and their needs with these three. The two Sticks sandwiched in the middle do nothing but confuse consumers. Looking at a comparison chart of all the Fire TV streaming devices, you might start to ask yourself, do I really need Wi-Fi 6E over Wi-Fi 6? Will one extra gigabyte of memory make a difference? Can I live without the Alexa Voice Remote Enhanced? You shouldn’t be asking yourself these questions; you have better things to do. There are only three Fire TV streaming devices worth considering, and I’d take it one step further and say most people should just get the Fire TV Stick 4K Max when it inevitably goes on sale for Black Friday for around $35. You’ll spend less and get a better product.This article originally appeared on Engadget at https://www.engadget.com/entertainment/streaming/why-does-amazon-need-five-tv-streamers-163014576.html?src=rss",
          "content": "What’s in a name? Apparently quite a bit, according to Amazon. The company recently announced yet another change to its Fire TV devices lineup, which just means they renamed a few things yet again. The family now includes the $35 Fire TV Stick HD, the $40 Fire TV Stick 4K Select, the $50 Fire TV Stick 4K Plus, the $60 Fire TV Stick 4K Max and the $140 Fire TV Cube. That was a pain to type out and probably a pain to read (my apologies). Two of those devices were “rebranded” previously within the past year, so if you’re confused, you’re likely not alone. What’s a humble shopper to do when you’re trying to decide which is the best (and budget-friendly) option to upgrade an old TV so you can binge-watch Hunting Wives and ask Alexa about tomorrow’s weather forecast? I’ll make your decision quite easy: just get the Fire TV Stick 4K Max. Really, if you’re looking for the best streaming device, period, we recommend turning to Google for that. But if you’ve decided Amazon’s Fire TV lineup is where you want to spend your money, the 4K Max is the best option of the bunch. Not only has it stuck around without being subject to a “rebrand” for quite some time, but it also has arguably the best balance of features and price of any Fire TV streaming device. The Fire TV Stick 4K Max gives you 4K streaming capabilities with Dolby Vision and all the HDRs that matter, Dolby Atmos audio and support for Amazon Luna and Xbox Game Pass. (It has some decent retro gaming chops, too, as our Jeff Dunn has previously explained.) Aside from the lack of an onboard Ethernet port present on the Fire TV Cube, the 4K Max has the same Wi-Fi 6E support as the more expensive Cube, plus the same 16GB of storage and 2GB of memory. When compared to the other dongles in the Fire TV lineup, things get even more perplexing. The $60 4K Max and the $50 4K Plus are essentially the same stick, but the latter has less storage, only Wi-Fi 6 capabilities (not 6E), a standard Alexa Voice remote and no support for the Fire TV ambient experience, which turns your TV into an Alexa smart display when you’re not actively watching anything. Step down further once more to the $40 4K Select and you miss out on Dolby Vision and extra memory, and you’ll have to settle for Wi-Fi 5. If you’re going to make all those compromises to save a few dollars, then you should just get the entry-level $35 Fire TV Stick HD. The biggest thing here is that it only supports 1080p streaming, but that will be ok for some people. We consider it to be the best budget streaming device on the market right now, and for folks just looking to make a cheap, basic upgrade to an aging set — go off and know your $35 was well spent (or, pro tip: wait for a sale and pick one up for less than $20). The case for the $140 Fire TV Cube isn’t a strong one, but it’s one that I’ll admit might be attractive to some users. It adds into the mix an Ethernet port, hands-free Alexa controls (meaning you don’t have to press a button on its remote to activate the virtual assistant, you can just talk to it) and it can control your other entertainment devices like a cable box and game console. It ultimately gives you more control over both the other things in your entertainment ecosystem and Alexa all in one device. That means Amazon has three strong streaming devices with very clear value propositions: the $35 Stick HD, the $60 4K Max and the $140 TV Cube. Affordable, mid-tier and high-end categories are covered and most people will find something that fits in their budget and their needs with these three. The two Sticks sandwiched in the middle do nothing but confuse consumers. Looking at a comparison chart of all the Fire TV streaming devices, you might start to ask yourself, do I really need Wi-Fi 6E over Wi-Fi 6? Will one extra gigabyte of memory make a difference? Can I live without the Alexa Voice Remote Enhanced? You shouldn’t be asking yourself these questions; you have better things to do. There are only three Fire TV streaming devices worth considering, and I’d take it one step further and say most people should just get the Fire TV Stick 4K Max when it inevitably goes on sale for Black Friday for around $35. You’ll spend less and get a better product.This article originally appeared on Engadget at https://www.engadget.com/entertainment/streaming/why-does-amazon-need-five-tv-streamers-163014576.html?src=rss",
          "feed_position": 11
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/how-anthropics-skills-make-claude-faster-cheaper-and-more-consistent-for",
          "published_at": "Thu, 16 Oct 2025 16:00:00 GMT",
          "title": "How Anthropic’s ‘Skills’ make Claude faster, cheaper, and more consistent for business workflows",
          "standfirst": "Anthropic launched a new capability on Thursday that allows its Claude AI assistant to tap into specialized expertise on demand, marking the company&#x27;s latest effort to make artificial intelligence more practical for enterprise workflows as it chases rival OpenAI in the intensifying competition over AI-powered software development.The feature, called Skills, enables users to create folders containing instructions, code scripts, and reference materials that Claude can automatically load when relevant to a task. The system marks a fundamental shift in how organizations can customize AI assistants, moving beyond one-off prompts to reusable packages of domain expertise that work consistently across an entire company.\"Skills are based on our belief and vision that as model intelligence continues to improve, we&#x27;ll continue moving towards general-purpose agents that often have access to their own filesystem and computing environment,\" said Mahesh Murag, a member of Anthropic&#x27;s technical staff, in an exclusive interview with VentureBeat. \"The agent is initially made aware only of the names and descriptions of each available skill and can choose to load more information about a particular skill when relevant to the task at hand.\"The launch comes as Anthropic, valued at $183 billion after a recent $13 billion funding round, projects its annual revenue could nearly triple to as much as $26 billion in 2026, according to a recent Reuters report. The company is currently approaching a $7 billion annual revenue run rate, up from $5 billion in August, fueled largely by enterprise adoption of its AI coding tools — a market where it faces fierce competition from OpenAI&#x27;s recently upgraded Codex platform.How &#x27;progressive disclosure&#x27; solves the context window problemSkills differ fundamentally from existing approaches to customizing AI assistants, such as prompt engineering or retrieval-augmented generation (RAG), Murag explained. The architecture relies on what Anthropic calls \"progressive disclosure\" — Claude initially sees only skill names and brief descriptions, then autonomously decides which skills to load based on the task at hand, accessing only the specific files and information needed at that moment.\"Unlike RAG, this relies on simple tools that let Claude manage and read files from a filesystem,\" Murag told VentureBeat. \"Skills can contain an unbounded amount of context to teach Claude how to complete a task or series of tasks. This is because Skills are based on the premise of an agent being able to autonomously and intelligently navigate a filesystem and execute code.\"This approach allows organizations to bundle far more information than traditional context windows permit, while maintaining the speed and efficiency that enterprise users demand. A single skill can include step-by-step procedures, code templates, reference documents, brand guidelines, compliance checklists, and executable scripts — all organized in a folder structure that Claude navigates intelligently.The system&#x27;s composability provides another technical advantage. Multiple skills automatically stack together when needed for complex workflows. For instance, Claude might simultaneously invoke a company&#x27;s brand guidelines skill, a financial reporting skill, and a presentation formatting skill to generate a quarterly investor deck — coordinating between all three without manual intervention.What makes Skills different from OpenAI&#x27;s Custom GPTs and Microsoft&#x27;s CopilotAnthropic is positioning Skills as distinct from competing offerings like OpenAI&#x27;s Custom GPTs and Microsoft&#x27;s Copilot Studio, though the features address similar enterprise needs around AI customization and consistency.\"Skills&#x27; combination of progressive disclosure, composability, and executable code bundling is unique in the market,\" Murag said. \"While other platforms require developers to build custom scaffolding, Skills let anyone — technical or not — create specialized agents by organizing procedural knowledge into files.\"The cross-platform portability also sets Skills apart. The same skill works identically across Claude.ai, Claude Code (Anthropic&#x27;s AI coding environment), the company&#x27;s API, and the Claude Agent SDK for building custom AI agents. Organizations can develop a skill once and deploy it everywhere their teams use Claude, a significant advantage for enterprises seeking consistency.The feature supports any programming language compatible with the underlying container environment, and Anthropic provides sandboxing for security — though the company acknowledges that allowing AI to execute code requires users to carefully vet which skills they trust.Early customers report 8x productivity gains on finance workflowsEarly customer implementations reveal how organizations are applying Skills to automate complex knowledge work. At Japanese e-commerce giant Rakuten, the AI team is using Skills to transform finance operations that previously required manual coordination across multiple departments.\"Skills streamline our management accounting and finance workflows,\" said Yusuke Kaji, general manager of AI at Rakuten in a statement. \"Claude processes multiple spreadsheets, catches critical anomalies, and generates reports using our procedures. What once took a day, we can now accomplish in an hour.\"That&#x27;s an 8x improvement in productivity for specific workflows — the kind of measurable return on investment that enterprises increasingly demand from AI implementations. Mike Krieger, Anthropic&#x27;s chief product officer and Instagram co-founder, recently noted that companies have moved past \"AI FOMO\" to requiring concrete success metrics.Design platform Canva plans to integrate Skills into its own AI agent workflows. \"Canva plans to leverage Skills to customize agents and expand what they can do,\" said Anwar Haneef, general manager and head of ecosystem at Canva in a statement. \"This unlocks new ways to bring Canva deeper into agentic workflows—helping teams capture their unique context and create stunning, high-quality designs effortlessly.\"Cloud storage provider Box sees Skills as a way to make corporate content repositories more actionable. \"Skills teaches Claude how to work with Box content,\" said Yashodha Bhavnani, head of AI at Box. \"Users can transform stored files into PowerPoint presentations, Excel spreadsheets, and Word documents that follow their organization&#x27;s standards—saving hours of effort.\"The enterprise security question: Who controls which AI skills employees can use?For enterprise IT departments, Skills raise important questions about governance and control—particularly since the feature allows AI to execute arbitrary code in sandboxed environments. Anthropic has built administrative controls that allow enterprise customers to manage access at the organizational level.\"Enterprise admins control access to the Skills capability via admin settings, where they can enable or disable access and monitor usage patterns,\" Murag said. \"Once enabled at the organizational level, individual users still need to opt in.\"That two-layer consent model — organizational enablement plus individual opt-in — reflects lessons learned from previous enterprise AI deployments where blanket rollouts created compliance concerns. However, Anthropic&#x27;s governance tools appear more limited than some enterprise customers might expect. The company doesn&#x27;t currently offer granular controls over which specific skills employees can use, or detailed audit trails of custom skill content.Organizations concerned about data security should note that Skills require Claude&#x27;s code execution environment, which runs in isolated containers. Anthropic advises users to \"stick to trusted sources\" when installing skills and provides security documentation, but the company acknowledges this is an inherently higher-risk capability than traditional AI interactions.From API to no-code: How Anthropic is making Skills accessible to everyoneAnthropic is taking several approaches to make Skills accessible to users with varying technical sophistication. For non-technical users on Claude.ai, the company provides a \"skill-creator\" skill that interactively guides users through building new skills by asking questions about their workflow, then automatically generating the folder structure and documentation.Developers working with Anthropic&#x27;s API get programmatic control through a new /skills endpoint and can manage skill versions through the Claude Console web interface. The feature requires enabling the Code Execution Tool beta in API requests. For Claude Code users, skills can be installed via plugins from the anthropics/skills GitHub marketplace, and teams can share skills through version control systems.\"Skills are included in Max, Pro, Teams, and Enterprise plans at no additional cost,\" Murag confirmed. \"API usage follows standard API pricing,\" meaning organizations pay only for the tokens consumed during skill execution, not for the skills themselves.Anthropic provides several pre-built skills for common business tasks, including professional generation of Excel spreadsheets with formulas, PowerPoint presentations, Word documents, and fillable PDFs. These Anthropic-created skills will remain free.Why the Skills launch matters in the AI coding wars with OpenAIThe Skills announcement arrives during a pivotal moment in Anthropic&#x27;s competition with OpenAI, particularly around AI-assisted software development. Just one day before releasing Skills, Anthropic launched Claude Haiku 4.5, a smaller and cheaper model that nonetheless matches the coding performance of Claude Sonnet 4 — which was state-of-the-art when released just five months ago.That rapid improvement curve reflects the breakneck pace of AI development, where today&#x27;s frontier capabilities become tomorrow&#x27;s commodity offerings. OpenAI has been pushing hard on coding tools as well, recently upgrading its Codex platform with GPT-5 and expanding GitHub Copilot&#x27;s capabilities.Anthropic&#x27;s revenue trajectory — potentially reaching $26 billion in 2026 from an estimated $9 billion by year-end 2025 — suggests the company is successfully converting enterprise interest into paying customers. The timing also follows Salesforce&#x27;s announcement this week that it&#x27;s deepening AI partnerships with both OpenAI and Anthropic to power its Agentforce platform, signaling that enterprises are adopting a multi-vendor approach rather than standardizing on a single provider.Skills addresses a real pain point: the \"prompt engineering\" problem where effective AI usage depends on individual employees crafting elaborate instructions for routine tasks, with no way to share that expertise across teams. Skills transforms implicit knowledge into explicit, shareable assets. For startups and developers, the feature could accelerate product development significantly — adding sophisticated document generation capabilities that previously required dedicated engineering teams and weeks of development.The composability aspect hints at a future where organizations build libraries of specialized skills that can be mixed and matched for increasingly complex workflows. A pharmaceutical company might develop skills for regulatory compliance, clinical trial analysis, molecular modeling, and patient data privacy that work together seamlessly — creating a customized AI assistant with deep domain expertise across multiple specialties.Anthropic indicates it&#x27;s working on simplified skill creation workflows and enterprise-wide deployment capabilities to make it easier for organizations to distribute skills across large teams. As the feature rolls out to Anthropic&#x27;s more than 300,000 business customers, the true test will be whether organizations find Skills substantively more useful than existing customization approaches.For now, Skills offers Anthropic&#x27;s clearest articulation yet of its vision for AI agents: not generalists that try to do everything reasonably well, but intelligent systems that know when to access specialized expertise and can coordinate multiple domains of knowledge to accomplish complex tasks. If that vision catches on, the question won&#x27;t be whether your company uses AI — it will be whether your AI knows how your company actually works.",
          "content": "Anthropic launched a new capability on Thursday that allows its Claude AI assistant to tap into specialized expertise on demand, marking the company&#x27;s latest effort to make artificial intelligence more practical for enterprise workflows as it chases rival OpenAI in the intensifying competition over AI-powered software development.The feature, called Skills, enables users to create folders containing instructions, code scripts, and reference materials that Claude can automatically load when relevant to a task. The system marks a fundamental shift in how organizations can customize AI assistants, moving beyond one-off prompts to reusable packages of domain expertise that work consistently across an entire company.\"Skills are based on our belief and vision that as model intelligence continues to improve, we&#x27;ll continue moving towards general-purpose agents that often have access to their own filesystem and computing environment,\" said Mahesh Murag, a member of Anthropic&#x27;s technical staff, in an exclusive interview with VentureBeat. \"The agent is initially made aware only of the names and descriptions of each available skill and can choose to load more information about a particular skill when relevant to the task at hand.\"The launch comes as Anthropic, valued at $183 billion after a recent $13 billion funding round, projects its annual revenue could nearly triple to as much as $26 billion in 2026, according to a recent Reuters report. The company is currently approaching a $7 billion annual revenue run rate, up from $5 billion in August, fueled largely by enterprise adoption of its AI coding tools — a market where it faces fierce competition from OpenAI&#x27;s recently upgraded Codex platform.How &#x27;progressive disclosure&#x27; solves the context window problemSkills differ fundamentally from existing approaches to customizing AI assistants, such as prompt engineering or retrieval-augmented generation (RAG), Murag explained. The architecture relies on what Anthropic calls \"progressive disclosure\" — Claude initially sees only skill names and brief descriptions, then autonomously decides which skills to load based on the task at hand, accessing only the specific files and information needed at that moment.\"Unlike RAG, this relies on simple tools that let Claude manage and read files from a filesystem,\" Murag told VentureBeat. \"Skills can contain an unbounded amount of context to teach Claude how to complete a task or series of tasks. This is because Skills are based on the premise of an agent being able to autonomously and intelligently navigate a filesystem and execute code.\"This approach allows organizations to bundle far more information than traditional context windows permit, while maintaining the speed and efficiency that enterprise users demand. A single skill can include step-by-step procedures, code templates, reference documents, brand guidelines, compliance checklists, and executable scripts — all organized in a folder structure that Claude navigates intelligently.The system&#x27;s composability provides another technical advantage. Multiple skills automatically stack together when needed for complex workflows. For instance, Claude might simultaneously invoke a company&#x27;s brand guidelines skill, a financial reporting skill, and a presentation formatting skill to generate a quarterly investor deck — coordinating between all three without manual intervention.What makes Skills different from OpenAI&#x27;s Custom GPTs and Microsoft&#x27;s CopilotAnthropic is positioning Skills as distinct from competing offerings like OpenAI&#x27;s Custom GPTs and Microsoft&#x27;s Copilot Studio, though the features address similar enterprise needs around AI customization and consistency.\"Skills&#x27; combination of progressive disclosure, composability, and executable code bundling is unique in the market,\" Murag said. \"While other platforms require developers to build custom scaffolding, Skills let anyone — technical or not — create specialized agents by organizing procedural knowledge into files.\"The cross-platform portability also sets Skills apart. The same skill works identically across Claude.ai, Claude Code (Anthropic&#x27;s AI coding environment), the company&#x27;s API, and the Claude Agent SDK for building custom AI agents. Organizations can develop a skill once and deploy it everywhere their teams use Claude, a significant advantage for enterprises seeking consistency.The feature supports any programming language compatible with the underlying container environment, and Anthropic provides sandboxing for security — though the company acknowledges that allowing AI to execute code requires users to carefully vet which skills they trust.Early customers report 8x productivity gains on finance workflowsEarly customer implementations reveal how organizations are applying Skills to automate complex knowledge work. At Japanese e-commerce giant Rakuten, the AI team is using Skills to transform finance operations that previously required manual coordination across multiple departments.\"Skills streamline our management accounting and finance workflows,\" said Yusuke Kaji, general manager of AI at Rakuten in a statement. \"Claude processes multiple spreadsheets, catches critical anomalies, and generates reports using our procedures. What once took a day, we can now accomplish in an hour.\"That&#x27;s an 8x improvement in productivity for specific workflows — the kind of measurable return on investment that enterprises increasingly demand from AI implementations. Mike Krieger, Anthropic&#x27;s chief product officer and Instagram co-founder, recently noted that companies have moved past \"AI FOMO\" to requiring concrete success metrics.Design platform Canva plans to integrate Skills into its own AI agent workflows. \"Canva plans to leverage Skills to customize agents and expand what they can do,\" said Anwar Haneef, general manager and head of ecosystem at Canva in a statement. \"This unlocks new ways to bring Canva deeper into agentic workflows—helping teams capture their unique context and create stunning, high-quality designs effortlessly.\"Cloud storage provider Box sees Skills as a way to make corporate content repositories more actionable. \"Skills teaches Claude how to work with Box content,\" said Yashodha Bhavnani, head of AI at Box. \"Users can transform stored files into PowerPoint presentations, Excel spreadsheets, and Word documents that follow their organization&#x27;s standards—saving hours of effort.\"The enterprise security question: Who controls which AI skills employees can use?For enterprise IT departments, Skills raise important questions about governance and control—particularly since the feature allows AI to execute arbitrary code in sandboxed environments. Anthropic has built administrative controls that allow enterprise customers to manage access at the organizational level.\"Enterprise admins control access to the Skills capability via admin settings, where they can enable or disable access and monitor usage patterns,\" Murag said. \"Once enabled at the organizational level, individual users still need to opt in.\"That two-layer consent model — organizational enablement plus individual opt-in — reflects lessons learned from previous enterprise AI deployments where blanket rollouts created compliance concerns. However, Anthropic&#x27;s governance tools appear more limited than some enterprise customers might expect. The company doesn&#x27;t currently offer granular controls over which specific skills employees can use, or detailed audit trails of custom skill content.Organizations concerned about data security should note that Skills require Claude&#x27;s code execution environment, which runs in isolated containers. Anthropic advises users to \"stick to trusted sources\" when installing skills and provides security documentation, but the company acknowledges this is an inherently higher-risk capability than traditional AI interactions.From API to no-code: How Anthropic is making Skills accessible to everyoneAnthropic is taking several approaches to make Skills accessible to users with varying technical sophistication. For non-technical users on Claude.ai, the company provides a \"skill-creator\" skill that interactively guides users through building new skills by asking questions about their workflow, then automatically generating the folder structure and documentation.Developers working with Anthropic&#x27;s API get programmatic control through a new /skills endpoint and can manage skill versions through the Claude Console web interface. The feature requires enabling the Code Execution Tool beta in API requests. For Claude Code users, skills can be installed via plugins from the anthropics/skills GitHub marketplace, and teams can share skills through version control systems.\"Skills are included in Max, Pro, Teams, and Enterprise plans at no additional cost,\" Murag confirmed. \"API usage follows standard API pricing,\" meaning organizations pay only for the tokens consumed during skill execution, not for the skills themselves.Anthropic provides several pre-built skills for common business tasks, including professional generation of Excel spreadsheets with formulas, PowerPoint presentations, Word documents, and fillable PDFs. These Anthropic-created skills will remain free.Why the Skills launch matters in the AI coding wars with OpenAIThe Skills announcement arrives during a pivotal moment in Anthropic&#x27;s competition with OpenAI, particularly around AI-assisted software development. Just one day before releasing Skills, Anthropic launched Claude Haiku 4.5, a smaller and cheaper model that nonetheless matches the coding performance of Claude Sonnet 4 — which was state-of-the-art when released just five months ago.That rapid improvement curve reflects the breakneck pace of AI development, where today&#x27;s frontier capabilities become tomorrow&#x27;s commodity offerings. OpenAI has been pushing hard on coding tools as well, recently upgrading its Codex platform with GPT-5 and expanding GitHub Copilot&#x27;s capabilities.Anthropic&#x27;s revenue trajectory — potentially reaching $26 billion in 2026 from an estimated $9 billion by year-end 2025 — suggests the company is successfully converting enterprise interest into paying customers. The timing also follows Salesforce&#x27;s announcement this week that it&#x27;s deepening AI partnerships with both OpenAI and Anthropic to power its Agentforce platform, signaling that enterprises are adopting a multi-vendor approach rather than standardizing on a single provider.Skills addresses a real pain point: the \"prompt engineering\" problem where effective AI usage depends on individual employees crafting elaborate instructions for routine tasks, with no way to share that expertise across teams. Skills transforms implicit knowledge into explicit, shareable assets. For startups and developers, the feature could accelerate product development significantly — adding sophisticated document generation capabilities that previously required dedicated engineering teams and weeks of development.The composability aspect hints at a future where organizations build libraries of specialized skills that can be mixed and matched for increasingly complex workflows. A pharmaceutical company might develop skills for regulatory compliance, clinical trial analysis, molecular modeling, and patient data privacy that work together seamlessly — creating a customized AI assistant with deep domain expertise across multiple specialties.Anthropic indicates it&#x27;s working on simplified skill creation workflows and enterprise-wide deployment capabilities to make it easier for organizations to distribute skills across large teams. As the feature rolls out to Anthropic&#x27;s more than 300,000 business customers, the true test will be whether organizations find Skills substantively more useful than existing customization approaches.For now, Skills offers Anthropic&#x27;s clearest articulation yet of its vision for AI agents: not generalists that try to do everything reasonably well, but intelligent systems that know when to access specialized expertise and can coordinate multiple domains of knowledge to accomplish complex tasks. If that vision catches on, the question won&#x27;t be whether your company uses AI — it will be whether your AI knows how your company actually works.",
          "feed_position": 1,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/5KwMYU2hucZ0L584A3lOVV/e7cd9bb4ab6fa04669f700eaa8ee6c74/nuneybits_Vector_art_of_a_retro_personal_computer_image_in_burn_55247853-b50a-4330-b1f1-3185f079bb17.webp"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/gaming/for-its-next-trick-quantic-dream-is-trying-to-compete-with-league-of-legends-and-dota-150000283.html",
          "published_at": "Thu, 16 Oct 2025 15:00:00 +0000",
          "title": "For its next trick, Quantic Dream is trying to compete with League of Legends and Dota",
          "standfirst": "It's been quite a while since we've heard much about Quantic Dream's Star Wars: Eclipse. The studio revealed that project at The Game Awards back in 2021 and details have been scarce since then. As it turns out, the developer of Heavy Rain and Detroit: Become Human had been working on a second game this whole time. It's one that sees Quantic Dream venturing into entirely new territory, because the studio is making its first multiplayer game.Spellcasters Chronicles is a 3 vs. 3 MOBA with a third-person perspective that's akin to Marvel Rivals. Each round lasts 25 minutes, with teams summoning minions, battling to conquer territory and earning victory by destroying their opponents' lifestones. So far, so typical MOBA. But Quantic Dream has a few tricks up its sleeve that it hopes will help make Spellcasters Chronicles stand out in a highly competitive live-service market.It's a magic-based MOBA with characters that have unique abilities, personalities and backstories. Every one of these mages has the ability to fly at any time and for as long as they want. So you can freely take to the skies to get a bird's eye view of the battlefield and help you make decisions about what to do next. You can duke it out with enemies in the air too.Along with attacks, support spells and summoning armies with hundreds of creatures, players can use their magic to plunk down buildings and shore up their defenses while altering the environment. There's interplay between characters too, as you can infuse allies (including summoned creatures) with spells. One mage, for instance, might add fire to a tankier teammate's hammer, so there are synergies to discover. \"Something we wanted to push is the sense of creativity,\" game director Greg Diaconu told reporters ahead of the reveal.Spellcasters ChroniclesQuantic DreamEventually, you'll be able to bring giant, game-changing titans into battles. Each player can summon one. Whenever a titan appears, it's an all-hands-on-deck situation for the opposing team, since these are powerful creatures that can completely change the course of a round.\"It was important for us to create a sense of spectacle,\" Diaconu said. \"Something that's as fun to watch as it is to play.\"It all seems quite action-packed, but there's a heavy strategic element to Spellcasters Chronicles as well. Before you go into a battle, you'll select your spells and summons, including your titan — so this is a deckbuilder game too. In the thick of the action, your team will need to decide when to pressure the map and try to expand your territory while capturing altars of power, totems that will grant you resources. Speaking of which, each spell has a limited number of uses, so resource management is a factor too.Spellcasters Chronicles is free-to-play, but there are no pay-to-win concerns here. In-game purchases will be purely cosmetic. Expect battle passes full of new looks for the characters. Lots of updates are in the pipeline too, including new mages, spells and creatures.Seven years in the ovenQuantic Dream started making Spellcasters Chronicles seven years ago (so before Netease bought a stake of the company and eventually the whole shebang). Although the studio decided to keep making narrative-driven single-player games after Detroit: Become Human, it wanted to try something new as well. The idea was to take the team's experience of working on interactive storytelling to a different genre by creating a multiplayer game with a stylized look. \"Multiple teams are fully dedicated to crafting the next generation of great games, including something very different, a competitive multiplayer experience, born from the same spirit of curiosity and creativity that has always defined us,\" Quantic Dream founder David Cage wrote in a blog post on Thursday. \"This new title may surprise our fans as it is very different from what we have done so far. But taking risks, challenging ourselves, exploring new ways of playing and telling stories, and attempting what seems impossible, has always been part of our DNA.\"In the world of Spellcasters Chronicles, gods are no more and mages who are able to harness an energy called the Source will shape the future. Quantic Dream hasn't shared too many details about the plot and characters of Spellcasters Chronicles just yet — the reveal focused on gameplay. That's perhaps in part because the studio is leaning into a community-driven narrative approach. Victories and defeats will contribute \"to the evolving Tapestry of Fate, where seasonal decisions will change gameplay, lore and map meta.\"We shouldn't have to wait too long to see how all of that works in practice. Quantic Dream will run a closed beta for Spellcasters Chronicles on Steam later this year, and the game is set to hit consoles with cross-play support in 2026. Those who are attending TwitchCon San Diego this week can try out the MOBA there.This article originally appeared on Engadget at https://www.engadget.com/gaming/for-its-next-trick-quantic-dream-is-trying-to-compete-with-league-of-legends-and-dota-150000283.html?src=rss",
          "content": "It's been quite a while since we've heard much about Quantic Dream's Star Wars: Eclipse. The studio revealed that project at The Game Awards back in 2021 and details have been scarce since then. As it turns out, the developer of Heavy Rain and Detroit: Become Human had been working on a second game this whole time. It's one that sees Quantic Dream venturing into entirely new territory, because the studio is making its first multiplayer game.Spellcasters Chronicles is a 3 vs. 3 MOBA with a third-person perspective that's akin to Marvel Rivals. Each round lasts 25 minutes, with teams summoning minions, battling to conquer territory and earning victory by destroying their opponents' lifestones. So far, so typical MOBA. But Quantic Dream has a few tricks up its sleeve that it hopes will help make Spellcasters Chronicles stand out in a highly competitive live-service market.It's a magic-based MOBA with characters that have unique abilities, personalities and backstories. Every one of these mages has the ability to fly at any time and for as long as they want. So you can freely take to the skies to get a bird's eye view of the battlefield and help you make decisions about what to do next. You can duke it out with enemies in the air too.Along with attacks, support spells and summoning armies with hundreds of creatures, players can use their magic to plunk down buildings and shore up their defenses while altering the environment. There's interplay between characters too, as you can infuse allies (including summoned creatures) with spells. One mage, for instance, might add fire to a tankier teammate's hammer, so there are synergies to discover. \"Something we wanted to push is the sense of creativity,\" game director Greg Diaconu told reporters ahead of the reveal.Spellcasters ChroniclesQuantic DreamEventually, you'll be able to bring giant, game-changing titans into battles. Each player can summon one. Whenever a titan appears, it's an all-hands-on-deck situation for the opposing team, since these are powerful creatures that can completely change the course of a round.\"It was important for us to create a sense of spectacle,\" Diaconu said. \"Something that's as fun to watch as it is to play.\"It all seems quite action-packed, but there's a heavy strategic element to Spellcasters Chronicles as well. Before you go into a battle, you'll select your spells and summons, including your titan — so this is a deckbuilder game too. In the thick of the action, your team will need to decide when to pressure the map and try to expand your territory while capturing altars of power, totems that will grant you resources. Speaking of which, each spell has a limited number of uses, so resource management is a factor too.Spellcasters Chronicles is free-to-play, but there are no pay-to-win concerns here. In-game purchases will be purely cosmetic. Expect battle passes full of new looks for the characters. Lots of updates are in the pipeline too, including new mages, spells and creatures.Seven years in the ovenQuantic Dream started making Spellcasters Chronicles seven years ago (so before Netease bought a stake of the company and eventually the whole shebang). Although the studio decided to keep making narrative-driven single-player games after Detroit: Become Human, it wanted to try something new as well. The idea was to take the team's experience of working on interactive storytelling to a different genre by creating a multiplayer game with a stylized look. \"Multiple teams are fully dedicated to crafting the next generation of great games, including something very different, a competitive multiplayer experience, born from the same spirit of curiosity and creativity that has always defined us,\" Quantic Dream founder David Cage wrote in a blog post on Thursday. \"This new title may surprise our fans as it is very different from what we have done so far. But taking risks, challenging ourselves, exploring new ways of playing and telling stories, and attempting what seems impossible, has always been part of our DNA.\"In the world of Spellcasters Chronicles, gods are no more and mages who are able to harness an energy called the Source will shape the future. Quantic Dream hasn't shared too many details about the plot and characters of Spellcasters Chronicles just yet — the reveal focused on gameplay. That's perhaps in part because the studio is leaning into a community-driven narrative approach. Victories and defeats will contribute \"to the evolving Tapestry of Fate, where seasonal decisions will change gameplay, lore and map meta.\"We shouldn't have to wait too long to see how all of that works in practice. Quantic Dream will run a closed beta for Spellcasters Chronicles on Steam later this year, and the game is set to hit consoles with cross-play support in 2026. Those who are attending TwitchCon San Diego this week can try out the MOBA there.This article originally appeared on Engadget at https://www.engadget.com/gaming/for-its-next-trick-quantic-dream-is-trying-to-compete-with-league-of-legends-and-dota-150000283.html?src=rss",
          "feed_position": 17,
          "image_url": "https://d29szjachogqwa.cloudfront.net/videos/user-uploaded/qd_mages.jpeg"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/apps/the-vision-pro-will-get-an-ipad-app-in-upcoming-ipados-update-142904090.html",
          "published_at": "Thu, 16 Oct 2025 14:29:05 +0000",
          "title": "The Vision Pro will get an iPad app in upcoming iPadOS update",
          "standfirst": "Buried in the press release for the upgraded Vision Pro headset that Apple announced yesterday was the news that the dedicated Apple Vision Pro app is making its way to iPad via iPadOS 26.1 later this fall. This means iPad users can browse Vision Pro content like apps and games from their tablet and queue downloads for the headset without needing to put it on each time. The Vision Pro app has been available on iPhone since the arrival of iOS 18.4 in April, and features a regularly updated selection of curated spatial content that Apple thinks Vision Pro users might want to try. Open it up today, for example, and it’ll point you towards the Lungy app’s audiovisual meditations, the travel show Elevated on (the recently and very confusingly rebranded) Apple TV, some recommended games and a list of education-focused apps available on Vision Pro. The Vision Pro app also includes news and tips for using the headset, as well product information and account settings. All of the same features will be offered in the iPad version of the app. In case you missed yesterday’s announcement, Apple has introduced an improved Vision Pro headset powered by the same M5 chip housed in its new iPad Pro and MacBook Pro. This should represent a fairly significant jump up from the first-gen product’s M2 chip. Web-browsing will be faster, as will loading apps and just generally navigating menus on your headset. The M5 chip also features a new 10-core GPU, which should significantly boost gaming performance, and without handing us any battery specs (which the company never likes to do), Apple says the upgraded Vision Pro should last a bit longer too. The M5 Vision Pro is now ready to pre-order now and still costs $3,499. Apple will start shipping the device on October 22.This article originally appeared on Engadget at https://www.engadget.com/apps/the-vision-pro-will-get-an-ipad-app-in-upcoming-ipados-update-142904090.html?src=rss",
          "content": "Buried in the press release for the upgraded Vision Pro headset that Apple announced yesterday was the news that the dedicated Apple Vision Pro app is making its way to iPad via iPadOS 26.1 later this fall. This means iPad users can browse Vision Pro content like apps and games from their tablet and queue downloads for the headset without needing to put it on each time. The Vision Pro app has been available on iPhone since the arrival of iOS 18.4 in April, and features a regularly updated selection of curated spatial content that Apple thinks Vision Pro users might want to try. Open it up today, for example, and it’ll point you towards the Lungy app’s audiovisual meditations, the travel show Elevated on (the recently and very confusingly rebranded) Apple TV, some recommended games and a list of education-focused apps available on Vision Pro. The Vision Pro app also includes news and tips for using the headset, as well product information and account settings. All of the same features will be offered in the iPad version of the app. In case you missed yesterday’s announcement, Apple has introduced an improved Vision Pro headset powered by the same M5 chip housed in its new iPad Pro and MacBook Pro. This should represent a fairly significant jump up from the first-gen product’s M2 chip. Web-browsing will be faster, as will loading apps and just generally navigating menus on your headset. The M5 chip also features a new 10-core GPU, which should significantly boost gaming performance, and without handing us any battery specs (which the company never likes to do), Apple says the upgraded Vision Pro should last a bit longer too. The M5 Vision Pro is now ready to pre-order now and still costs $3,499. Apple will start shipping the device on October 22.This article originally appeared on Engadget at https://www.engadget.com/apps/the-vision-pro-will-get-an-ipad-app-in-upcoming-ipados-update-142904090.html?src=rss",
          "feed_position": 18
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/amazon-and-chobani-adopt-strellas-ai-interviews-for-customer-research-as",
          "published_at": "Thu, 16 Oct 2025 14:00:00 GMT",
          "title": "Amazon and Chobani adopt Strella's AI interviews for customer research as fast-growing startup raises $14M",
          "standfirst": "One year after emerging from stealth, Strella has raised $14 million in Series A funding to expand its AI-powered customer research platform, the company announced Thursday. The round, led by Bessemer Venture Partners with participation from Decibel Partners, Bain Future Back Ventures, MVP Ventures and 645 Ventures, comes as enterprises increasingly turn to artificial intelligence to understand customers faster and more deeply than traditional methods allow.The investment marks a sharp acceleration for the startup founded by Lydia Hylton and Priya Krishnan, two former consultants and product managers who watched companies struggle with a customer research process that could take eight weeks from start to finish. Since October, Strella has grown revenue tenfold, quadrupled its customer base to more than 40 paying enterprises, and tripled its average contract values by moving upmarket to serve Fortune 500 companies.\"Research tends to be bookended by two very strategic steps: first, we have a problem—what research should we do? And second, we&#x27;ve done the research—now what are we going to do with it?\" said Hylton, Strella&#x27;s CEO, in an exclusive interview with VentureBeat. \"All the stuff in the middle tends to be execution and lower-skill work. We view Strella as doing that middle 90% of the work.\"The platform now serves Amazon, Duolingo, Apollo GraphQL, and Chobani, collectively conducting thousands of AI-moderated interviews that deliver what the company claims is a 90% average time savings on manual research work. The company is approaching $1 million in revenue after beginning monetization only in January, with month-over-month growth of 50% and zero customer churn to date.How AI-powered interviews compress eight-week research projects into daysStrella&#x27;s technology addresses a workflow that has frustrated product teams, marketers, and designers for decades. Traditional customer research requires writing interview guides, recruiting participants, scheduling calls, conducting interviews, taking notes, synthesizing findings, and creating presentations — a process that consumes weeks of highly-skilled labor and often delays critical product decisions.The platform compresses that timeline to days by using AI to moderate voice-based interviews that run like Zoom calls, but with an artificial intelligence agent asking questions, following up on interesting responses, and detecting when participants are being evasive or fraudulent. The system then synthesizes findings automatically, creating highlight reels and charts from unstructured qualitative data.\"It used to take eight weeks. Now you can do it in the span of a couple days,\" Hylton told VentureBeat. \"The primary technology is through an AI-moderated interview. It&#x27;s like being in a Zoom call with an AI instead of a human — it&#x27;s completely free form and voice based.\"Critically, the platform also supports human moderators joining the same calls, reflecting the founders&#x27; belief that humans won&#x27;t disappear from the research process. \"Human moderation won&#x27;t go away, which is why we&#x27;ve supported human moderation from our Genesis,\" Hylton said. \"Research tends to be bookended by two very strategic steps: we have a problem, what&#x27;s the research that we should do? And we&#x27;ve done the research, now what are we going to do with it? All the stuff in the middle tends to be execution and lower skill work. We view Strella as doing that middle 90% of the work.\"Why customers tell AI moderators the truth they won&#x27;t share with humansOne of Strella&#x27;s most surprising findings challenges assumptions about AI in qualitative research: participants appear more honest with AI moderators than with humans. The founders discovered this pattern repeatedly as customers ran head-to-head comparisons between traditional human-moderated studies and Strella&#x27;s AI approach.\"If you&#x27;re a designer and you get on a Zoom call with a customer and you say, &#x27;Do you like my design?&#x27; they&#x27;re always gonna say yes. They don&#x27;t want to hurt your feelings,\" Hylton explained. \"But it&#x27;s not a problem at all for Strella. They would tell you exactly what they think about it, which is really valuable. It&#x27;s very hard to get honest feedback.\"Krishnan, Strella&#x27;s COO, said companies initially worried about using AI and \"eroding quality,\" but the platform has \"actually found the opposite to be true. People are much more open and honest with an AI moderator, and so the level of insight that you get is much richer because people are giving their unfiltered feedback.\"This dynamic has practical business implications. Brian Santiago, Senior Product Design Manager at Apollo GraphQL, said in a statement: \"Before Strella, studies took weeks. Now we get insights in a day — sometimes in just a few hours. And because participants open up more with the AI moderator, the feedback is deeper and more honest.\"The platform also addresses endemic fraud in online surveys, particularly when participants are compensated. Because Strella interviews happen on camera in real time, the AI moderator can detect when someone pauses suspiciously long — perhaps to consult ChatGPT — and flags them as potentially fraudulent. \"We are fraud resistant,\" Hylton said, contrasting this with traditional surveys where fraud rates can be substantial.Solving mobile app research with persistent screen sharing technologyA major focus of the Series A funding will be expanding Strella&#x27;s recently-launched mobile application, which Krishnan identified as critical competitive differentiation. The mobile app enables persistent screen sharing during interviews — allowing researchers to watch users navigate mobile applications in real time while the AI moderator asks about their experience.\"We are the only player in the market that supports screen sharing on mobile,\" Hylton said. \"You know, I want to understand what are the pain points with my app? Why do people not seem to be able to find the checkout flow? Well, in order to do that effectively, you&#x27;d like to see the user screen while they&#x27;re doing an interview.\"For consumer-facing companies where mobile represents the primary customer interface, this capability opens entirely new use cases. The founders noted that \"several of our customers didn&#x27;t do research before\" but have now built research practices around Strella because the platform finally made mobile research accessible at scale.The platform also supports embedding traditional survey question types directly into the conversational interview, approaching what Hylton called \"feature parity with a survey\" while maintaining the engagement advantages of a natural conversation. Strella interviews regularly run 60 to 90 minutes with nearly 100% completion rates—a duration that would see 60-70% drop-off in a traditional survey format.How Strella differentiated in a market crowded with AI research startupsStrella enters a market that appears crowded at first glance, with established players like Qualtrics and a wave of AI-powered startups promising to transform customer research. The founders themselves initially pursued a different approach — synthetic respondents, or \"digital twins\" that simulate customer perspectives using large language models.\"We actually pivoted from that. That was our initial idea,\" Hylton revealed, referring to synthetic respondents. \"People are very intrigued by that concept, but found in practice, no willingness to pay right now.\"Recent research suggesting companies could use language models as digital twins for customer feedback has reignited interest in that approach. But Hylton remains skeptical: \"The capabilities of the LLMs as they are today are not good enough, in my opinion, to justify a standalone company. Right now you could just ask ChatGPT, &#x27;What would new users of Duolingo think about this ad copy?&#x27; You can do that. Adding the standalone idea of a synthetic panel is sort of just putting a wrapper on that.\"Instead, Strella&#x27;s bet is that the real value lies in collecting proprietary qualitative data at scale — building what could become \"the system of truth for all qualitative insights\" within enterprises, as Lindsey Li, Vice President at Bessemer Venture Partners, described it.Li, who led the investment just one year after Strella emerged from stealth, said the firm was convinced by both the technology and the team. \"Strella has built highly differentiated technology that enables a continuous interview rather than a survey,\" Li said. \"We heard time and time again that customers loved this product experience relative to other offerings.\"On the defensibility question that concerns many AI investors, Li emphasized product execution over patents: \"We think the long game here will be won with a million small product decisions, all of which must be driven by deep empathy for customer pain and an understanding of how best to address their needs. Lydia and Priya exhibit that in spades.\"The founders point to technical depth that&#x27;s difficult to replicate. Most competitors started with adaptive surveys — text-based interfaces where users type responses and wait for the next question. Some have added voice, but typically as uploaded audio clips rather than free-flowing conversation.\"Our approach is fundamentally better, which is the fact that it is a free form conversation,\" Hylton said. \"You never have to control anything. You&#x27;re never typing, there&#x27;s no buttons, there&#x27;s no upload and wait for the next question. It&#x27;s completely free form, and that has been an extraordinarily hard product to build. There&#x27;s a tremendous amount of IP in the way that we prompt our moderator, the way that we run analysis.\"The platform also improves with use, learning from each customer&#x27;s research patterns to fine-tune future interview guides and questions. \"Our product gets better for our customers as they continue to use us,\" Hylton said. All research accumulates in a central repository where teams can generate new insights by chatting with the data or creating visualizations from previously unstructured qualitative feedback.Creating new research budgets instead of just automating existing onesPerhaps more important than displacing existing research is expanding the total market. Krishnan said growth has been \"fundamentally related to our product\" creating new research that wouldn&#x27;t have happened otherwise.\"We have expanded the use cases in which people would conduct research,\" Krishnan explained. \"Several of our customers didn&#x27;t do research before, have always wanted to do research, but didn&#x27;t have a dedicated researcher or team at their company that was devoted to it, and have purchased Strella to kick off and enable their research practice. That&#x27;s been really cool where we&#x27;ve seen this market just opening up.\"This expansion comes as enterprises face mounting pressure to improve customer experience amid declining satisfaction scores. According to Forrester Research&#x27;s 2024 Customer Experience Index, customer experience quality has declined for three consecutive years — an unprecedented trend. The report found that 39% of brands saw CX quality deteriorate, with declines across effectiveness, ease, and emotional connection.Meanwhile, Deloitte&#x27;s 2025 Technology, Media & Telecommunications Predictions report forecasts that 25% of enterprises using generative AI will deploy AI agents by 2025, growing to 50% by 2027. The report specifically highlighted AI&#x27;s potential to enhance customer satisfaction by 15-20% while reducing cost to serve by 20-30% when properly implemented.Gartner identified conversational user interfaces — the category Strella inhabits — as one of three technologies poised to transform customer service by 2028, noting that \"customers increasingly expect to be able to interact with the applications they use in a natural way.\"Against this backdrop, Li sees substantial room for growth. \"UX Research is a sub-sector of the $140B+ global market-research industry,\" Li said. \"This includes both the software layer historically (~$430M) and professional services spend on UX research, design, product strategy, etc. which is conservatively estimated to be ~$6.4B+ annually. As software in this vertical, led by Strella, becomes more powerful, we believe the TAM will continue to expand meaningfully.\"Making customer feedback accessible across the enterprise, not just research teamsThe founders describe their mission as \"democratizing access to the customer\" — making it possible for anyone in an organization to understand customer perspectives without waiting for dedicated research teams to complete months-long studies.\"Many, many, many positions in the organization would like to get customer feedback, but it&#x27;s so hard right now,\" Hylton said. With Strella, she explained, someone can \"log into Strella and through a chat, create any highlight reel that you want and actually see customers in their own words answering the question that you have based on the research that&#x27;s already been done.\"This video-first approach to research repositories changes organizational dynamics around customer feedback. \"Then you can say, &#x27;Okay, engineering team, we need to build this feature. And here&#x27;s the customer actually saying it,&#x27;\" Hylton continued. \"&#x27;This is not me. This isn&#x27;t politics. Here are seven customers saying they can&#x27;t find the Checkout button.&#x27; The fact that we are a very video-based platform really allows us to do that quickly and painlessly.\"The company has moved decisively upmarket, with contract values now typically in the five-figure range and \"several six figure contracts\" signed, according to Krishnan. The pricing strategy reflects a premium positioning: \"Our product is very good, it&#x27;s very premium. We&#x27;re charging based on the value it provides to customers,\" Krishnan said, rather than competing on cost alone.This approach appears to be working. The company reports 100% conversion from pilot programs to paid contracts and zero churn among its 40-45 customers, with month-over-month revenue growth of 50%.The roadmap: Computer vision, agentic AI, and human-machine collaborationThe Series A funding will primarily support scaling product and go-to-market teams. \"We&#x27;re really confident that we have product-market fit,\" Hylton said. \"And now the question is execution, and we want to hire a lot of really talented people to help us execute.\"On the product roadmap, Hylton emphasized continued focus on the participant experience as the key to winning the market. \"Everything else is downstream of a joyful participant experience,\" she said, including \"the quality of insights, the amount you have to pay people to do the interviews, and the way that your customers feel about a company.\"Near-term priorities include adding visual capabilities so the AI moderator can respond to facial expressions and other nonverbal cues, and building more sophisticated collaboration features between human researchers and AI moderators. \"Maybe you want to listen while an AI moderator is running a call and you might want to be able to jump in with specific questions,\" Hylton said. \"Or you want to run an interview yourself, but you want the moderator to be there as backup or to help you.\"These features move toward what the industry calls \"agentic AI\" — systems that can act more autonomously while still collaborating with humans. The founders see this human-AI collaboration, rather than full automation, as the sustainable path forward.\"We believe that a lot of the really strategic work that companies do will continue to be human moderated,\" Hylton said. \"And you can still do that through Strella and just use us for synthesis in those cases.\"For Li and Bessemer, the bet is on founders who understand this nuance. \"Lydia and Priya exhibit the exact archetype of founders we are excited to partner with for the long term — customer-obsessed, transparent, thoughtful, and singularly driven towards the home-run scenario,\" she said.The company declined to disclose specific revenue figures or valuation. With the new funding, Strella has now raised $18 million total, including a $4 million seed round led by Decibel Partners announced in October.As Strella scales, the founders remain focused on a vision where technology enhances rather than eliminates human judgment—where an engineering team doesn&#x27;t just read a research report, but watches seven customers struggle to find the same button. Where a product manager can query months of accumulated interviews in seconds. Where companies don&#x27;t choose between speed and depth, but get both.\"The interesting part of the business is actually collecting that proprietary dataset, collecting qualitative research at scale,\" Hylton said, describing what she sees as Strella&#x27;s long-term moat. Not replacing the researcher, but making everyone in the company one.",
          "content": "One year after emerging from stealth, Strella has raised $14 million in Series A funding to expand its AI-powered customer research platform, the company announced Thursday. The round, led by Bessemer Venture Partners with participation from Decibel Partners, Bain Future Back Ventures, MVP Ventures and 645 Ventures, comes as enterprises increasingly turn to artificial intelligence to understand customers faster and more deeply than traditional methods allow.The investment marks a sharp acceleration for the startup founded by Lydia Hylton and Priya Krishnan, two former consultants and product managers who watched companies struggle with a customer research process that could take eight weeks from start to finish. Since October, Strella has grown revenue tenfold, quadrupled its customer base to more than 40 paying enterprises, and tripled its average contract values by moving upmarket to serve Fortune 500 companies.\"Research tends to be bookended by two very strategic steps: first, we have a problem—what research should we do? And second, we&#x27;ve done the research—now what are we going to do with it?\" said Hylton, Strella&#x27;s CEO, in an exclusive interview with VentureBeat. \"All the stuff in the middle tends to be execution and lower-skill work. We view Strella as doing that middle 90% of the work.\"The platform now serves Amazon, Duolingo, Apollo GraphQL, and Chobani, collectively conducting thousands of AI-moderated interviews that deliver what the company claims is a 90% average time savings on manual research work. The company is approaching $1 million in revenue after beginning monetization only in January, with month-over-month growth of 50% and zero customer churn to date.How AI-powered interviews compress eight-week research projects into daysStrella&#x27;s technology addresses a workflow that has frustrated product teams, marketers, and designers for decades. Traditional customer research requires writing interview guides, recruiting participants, scheduling calls, conducting interviews, taking notes, synthesizing findings, and creating presentations — a process that consumes weeks of highly-skilled labor and often delays critical product decisions.The platform compresses that timeline to days by using AI to moderate voice-based interviews that run like Zoom calls, but with an artificial intelligence agent asking questions, following up on interesting responses, and detecting when participants are being evasive or fraudulent. The system then synthesizes findings automatically, creating highlight reels and charts from unstructured qualitative data.\"It used to take eight weeks. Now you can do it in the span of a couple days,\" Hylton told VentureBeat. \"The primary technology is through an AI-moderated interview. It&#x27;s like being in a Zoom call with an AI instead of a human — it&#x27;s completely free form and voice based.\"Critically, the platform also supports human moderators joining the same calls, reflecting the founders&#x27; belief that humans won&#x27;t disappear from the research process. \"Human moderation won&#x27;t go away, which is why we&#x27;ve supported human moderation from our Genesis,\" Hylton said. \"Research tends to be bookended by two very strategic steps: we have a problem, what&#x27;s the research that we should do? And we&#x27;ve done the research, now what are we going to do with it? All the stuff in the middle tends to be execution and lower skill work. We view Strella as doing that middle 90% of the work.\"Why customers tell AI moderators the truth they won&#x27;t share with humansOne of Strella&#x27;s most surprising findings challenges assumptions about AI in qualitative research: participants appear more honest with AI moderators than with humans. The founders discovered this pattern repeatedly as customers ran head-to-head comparisons between traditional human-moderated studies and Strella&#x27;s AI approach.\"If you&#x27;re a designer and you get on a Zoom call with a customer and you say, &#x27;Do you like my design?&#x27; they&#x27;re always gonna say yes. They don&#x27;t want to hurt your feelings,\" Hylton explained. \"But it&#x27;s not a problem at all for Strella. They would tell you exactly what they think about it, which is really valuable. It&#x27;s very hard to get honest feedback.\"Krishnan, Strella&#x27;s COO, said companies initially worried about using AI and \"eroding quality,\" but the platform has \"actually found the opposite to be true. People are much more open and honest with an AI moderator, and so the level of insight that you get is much richer because people are giving their unfiltered feedback.\"This dynamic has practical business implications. Brian Santiago, Senior Product Design Manager at Apollo GraphQL, said in a statement: \"Before Strella, studies took weeks. Now we get insights in a day — sometimes in just a few hours. And because participants open up more with the AI moderator, the feedback is deeper and more honest.\"The platform also addresses endemic fraud in online surveys, particularly when participants are compensated. Because Strella interviews happen on camera in real time, the AI moderator can detect when someone pauses suspiciously long — perhaps to consult ChatGPT — and flags them as potentially fraudulent. \"We are fraud resistant,\" Hylton said, contrasting this with traditional surveys where fraud rates can be substantial.Solving mobile app research with persistent screen sharing technologyA major focus of the Series A funding will be expanding Strella&#x27;s recently-launched mobile application, which Krishnan identified as critical competitive differentiation. The mobile app enables persistent screen sharing during interviews — allowing researchers to watch users navigate mobile applications in real time while the AI moderator asks about their experience.\"We are the only player in the market that supports screen sharing on mobile,\" Hylton said. \"You know, I want to understand what are the pain points with my app? Why do people not seem to be able to find the checkout flow? Well, in order to do that effectively, you&#x27;d like to see the user screen while they&#x27;re doing an interview.\"For consumer-facing companies where mobile represents the primary customer interface, this capability opens entirely new use cases. The founders noted that \"several of our customers didn&#x27;t do research before\" but have now built research practices around Strella because the platform finally made mobile research accessible at scale.The platform also supports embedding traditional survey question types directly into the conversational interview, approaching what Hylton called \"feature parity with a survey\" while maintaining the engagement advantages of a natural conversation. Strella interviews regularly run 60 to 90 minutes with nearly 100% completion rates—a duration that would see 60-70% drop-off in a traditional survey format.How Strella differentiated in a market crowded with AI research startupsStrella enters a market that appears crowded at first glance, with established players like Qualtrics and a wave of AI-powered startups promising to transform customer research. The founders themselves initially pursued a different approach — synthetic respondents, or \"digital twins\" that simulate customer perspectives using large language models.\"We actually pivoted from that. That was our initial idea,\" Hylton revealed, referring to synthetic respondents. \"People are very intrigued by that concept, but found in practice, no willingness to pay right now.\"Recent research suggesting companies could use language models as digital twins for customer feedback has reignited interest in that approach. But Hylton remains skeptical: \"The capabilities of the LLMs as they are today are not good enough, in my opinion, to justify a standalone company. Right now you could just ask ChatGPT, &#x27;What would new users of Duolingo think about this ad copy?&#x27; You can do that. Adding the standalone idea of a synthetic panel is sort of just putting a wrapper on that.\"Instead, Strella&#x27;s bet is that the real value lies in collecting proprietary qualitative data at scale — building what could become \"the system of truth for all qualitative insights\" within enterprises, as Lindsey Li, Vice President at Bessemer Venture Partners, described it.Li, who led the investment just one year after Strella emerged from stealth, said the firm was convinced by both the technology and the team. \"Strella has built highly differentiated technology that enables a continuous interview rather than a survey,\" Li said. \"We heard time and time again that customers loved this product experience relative to other offerings.\"On the defensibility question that concerns many AI investors, Li emphasized product execution over patents: \"We think the long game here will be won with a million small product decisions, all of which must be driven by deep empathy for customer pain and an understanding of how best to address their needs. Lydia and Priya exhibit that in spades.\"The founders point to technical depth that&#x27;s difficult to replicate. Most competitors started with adaptive surveys — text-based interfaces where users type responses and wait for the next question. Some have added voice, but typically as uploaded audio clips rather than free-flowing conversation.\"Our approach is fundamentally better, which is the fact that it is a free form conversation,\" Hylton said. \"You never have to control anything. You&#x27;re never typing, there&#x27;s no buttons, there&#x27;s no upload and wait for the next question. It&#x27;s completely free form, and that has been an extraordinarily hard product to build. There&#x27;s a tremendous amount of IP in the way that we prompt our moderator, the way that we run analysis.\"The platform also improves with use, learning from each customer&#x27;s research patterns to fine-tune future interview guides and questions. \"Our product gets better for our customers as they continue to use us,\" Hylton said. All research accumulates in a central repository where teams can generate new insights by chatting with the data or creating visualizations from previously unstructured qualitative feedback.Creating new research budgets instead of just automating existing onesPerhaps more important than displacing existing research is expanding the total market. Krishnan said growth has been \"fundamentally related to our product\" creating new research that wouldn&#x27;t have happened otherwise.\"We have expanded the use cases in which people would conduct research,\" Krishnan explained. \"Several of our customers didn&#x27;t do research before, have always wanted to do research, but didn&#x27;t have a dedicated researcher or team at their company that was devoted to it, and have purchased Strella to kick off and enable their research practice. That&#x27;s been really cool where we&#x27;ve seen this market just opening up.\"This expansion comes as enterprises face mounting pressure to improve customer experience amid declining satisfaction scores. According to Forrester Research&#x27;s 2024 Customer Experience Index, customer experience quality has declined for three consecutive years — an unprecedented trend. The report found that 39% of brands saw CX quality deteriorate, with declines across effectiveness, ease, and emotional connection.Meanwhile, Deloitte&#x27;s 2025 Technology, Media & Telecommunications Predictions report forecasts that 25% of enterprises using generative AI will deploy AI agents by 2025, growing to 50% by 2027. The report specifically highlighted AI&#x27;s potential to enhance customer satisfaction by 15-20% while reducing cost to serve by 20-30% when properly implemented.Gartner identified conversational user interfaces — the category Strella inhabits — as one of three technologies poised to transform customer service by 2028, noting that \"customers increasingly expect to be able to interact with the applications they use in a natural way.\"Against this backdrop, Li sees substantial room for growth. \"UX Research is a sub-sector of the $140B+ global market-research industry,\" Li said. \"This includes both the software layer historically (~$430M) and professional services spend on UX research, design, product strategy, etc. which is conservatively estimated to be ~$6.4B+ annually. As software in this vertical, led by Strella, becomes more powerful, we believe the TAM will continue to expand meaningfully.\"Making customer feedback accessible across the enterprise, not just research teamsThe founders describe their mission as \"democratizing access to the customer\" — making it possible for anyone in an organization to understand customer perspectives without waiting for dedicated research teams to complete months-long studies.\"Many, many, many positions in the organization would like to get customer feedback, but it&#x27;s so hard right now,\" Hylton said. With Strella, she explained, someone can \"log into Strella and through a chat, create any highlight reel that you want and actually see customers in their own words answering the question that you have based on the research that&#x27;s already been done.\"This video-first approach to research repositories changes organizational dynamics around customer feedback. \"Then you can say, &#x27;Okay, engineering team, we need to build this feature. And here&#x27;s the customer actually saying it,&#x27;\" Hylton continued. \"&#x27;This is not me. This isn&#x27;t politics. Here are seven customers saying they can&#x27;t find the Checkout button.&#x27; The fact that we are a very video-based platform really allows us to do that quickly and painlessly.\"The company has moved decisively upmarket, with contract values now typically in the five-figure range and \"several six figure contracts\" signed, according to Krishnan. The pricing strategy reflects a premium positioning: \"Our product is very good, it&#x27;s very premium. We&#x27;re charging based on the value it provides to customers,\" Krishnan said, rather than competing on cost alone.This approach appears to be working. The company reports 100% conversion from pilot programs to paid contracts and zero churn among its 40-45 customers, with month-over-month revenue growth of 50%.The roadmap: Computer vision, agentic AI, and human-machine collaborationThe Series A funding will primarily support scaling product and go-to-market teams. \"We&#x27;re really confident that we have product-market fit,\" Hylton said. \"And now the question is execution, and we want to hire a lot of really talented people to help us execute.\"On the product roadmap, Hylton emphasized continued focus on the participant experience as the key to winning the market. \"Everything else is downstream of a joyful participant experience,\" she said, including \"the quality of insights, the amount you have to pay people to do the interviews, and the way that your customers feel about a company.\"Near-term priorities include adding visual capabilities so the AI moderator can respond to facial expressions and other nonverbal cues, and building more sophisticated collaboration features between human researchers and AI moderators. \"Maybe you want to listen while an AI moderator is running a call and you might want to be able to jump in with specific questions,\" Hylton said. \"Or you want to run an interview yourself, but you want the moderator to be there as backup or to help you.\"These features move toward what the industry calls \"agentic AI\" — systems that can act more autonomously while still collaborating with humans. The founders see this human-AI collaboration, rather than full automation, as the sustainable path forward.\"We believe that a lot of the really strategic work that companies do will continue to be human moderated,\" Hylton said. \"And you can still do that through Strella and just use us for synthesis in those cases.\"For Li and Bessemer, the bet is on founders who understand this nuance. \"Lydia and Priya exhibit the exact archetype of founders we are excited to partner with for the long term — customer-obsessed, transparent, thoughtful, and singularly driven towards the home-run scenario,\" she said.The company declined to disclose specific revenue figures or valuation. With the new funding, Strella has now raised $18 million total, including a $4 million seed round led by Decibel Partners announced in October.As Strella scales, the founders remain focused on a vision where technology enhances rather than eliminates human judgment—where an engineering team doesn&#x27;t just read a research report, but watches seven customers struggle to find the same button. Where a product manager can query months of accumulated interviews in seconds. Where companies don&#x27;t choose between speed and depth, but get both.\"The interesting part of the business is actually collecting that proprietary dataset, collecting qualitative research at scale,\" Hylton said, describing what she sees as Strella&#x27;s long-term moat. Not replacing the researcher, but making everyone in the company one.",
          "feed_position": 2,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/2MHgJSYW7aGoHqAVeFBa49/cc077429552cf090646e0177202a2e31/strella_lydia_priya_photo2.jpg"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/microsoft-launches-hey-copilot-voice-assistant-and-autonomous-agents-for-all",
          "published_at": "Thu, 16 Oct 2025 13:00:00 GMT",
          "title": "Microsoft launches 'Hey Copilot' voice assistant and autonomous agents for all Windows 11 PCs",
          "standfirst": "Microsoft is fundamentally reimagining how people interact with their computers, announcing Thursday a sweeping transformation of Windows 11 that brings voice-activated AI assistants, autonomous software agents, and contextual intelligence to every PC running the operating system — not just premium devices with specialized chips.The announcement represents Microsoft&#x27;s most aggressive push yet to integrate generative artificial intelligence into the desktop computing experience, moving beyond the chatbot interfaces that have defined the first wave of consumer AI products toward a more ambient, conversational model where users can simply talk to their computers and have AI agents complete complex tasks on their behalf.\"When we think about what the promise of an AI PC is, it should be capable of three things,\" Yusuf Mehdi, Microsoft&#x27;s Executive Vice President and Consumer Chief Marketing Officer, told reporters at a press conference last week. \"First, you should be able to interact with it naturally, in text or voice, and have it understand you. Second, it should be able to see what you see and be able to offer guided support. And third, it should be able to take action on your behalf.\"The shift could prove consequential for an industry searching for the \"killer app\" for generative AI. While hundreds of millions of people have experimented with ChatGPT and similar chatbots, integrating AI directly into the operating system that powers the vast majority of workplace computers could dramatically accelerate mainstream adoption — or create new security and privacy headaches for organizations already struggling to govern employee use of AI tools.How &#x27;Hey Copilot&#x27; aims to replace typing with talking on Windows PCsAt the heart of Microsoft&#x27;s vision is voice interaction, which the company is positioning as the third fundamental input method for PCs after the mouse and keyboard — a comparison that underscores Microsoft&#x27;s ambitions for reshaping human-computer interaction nearly four decades after the graphical user interface became standard.Starting this week, any Windows 11 user can enable the \"Hey Copilot\" wake word with a single click, allowing them to summon Microsoft&#x27;s AI assistant by voice from anywhere in the operating system. The feature, which had been in limited testing, is now being rolled out to hundreds of millions of devices globally.\"It&#x27;s been almost four decades since the PC has changed the way you interact with it, which is primarily mouse and keyboard,\" Mehdi said. \"When you think about it, we find that people type on a given day up to 14,000 words on their keyboard, which is really kind of mind-boggling. But what if now you can go beyond that and talk to it?\"The emphasis on voice reflects internal Microsoft data showing that users engage with Copilot twice as much when using voice compared to text input — a finding the company attributes to the lower cognitive barrier of speaking versus crafting precise written prompts.\"The magic unlock with Copilot Voice and Copilot Vision is the ease of interaction,\" according to the company&#x27;s announcement. \"Using the new wake word, &#x27;Hey Copilot,&#x27; getting something done is as easy as just asking for it.\"But Microsoft&#x27;s bet on voice computing faces real-world constraints that Mehdi acknowledged during the briefing. When asked whether workers in shared office environments would use voice features, potentially compromising privacy, Mehdi noted that millions already conduct voice calls through their PCs with headphones, and predicted users would adapt: \"Just like when the mouse came out, people have to figure out when to use it, what&#x27;s the right way, how to make it happen.\"Crucially, Microsoft is hedging its voice-first strategy by making all features accessible through traditional text input as well, recognizing that voice isn&#x27;t always appropriate or accessible.AI that sees your screen: Copilot Vision expands worldwide with new capabilitiesPerhaps more transformative than voice control is the expansion of Copilot Vision, a feature Microsoft introduced earlier this year that allows the AI to analyze what&#x27;s displayed on a user&#x27;s screen and provide contextual assistance.Previously limited to voice interaction, Copilot Vision is now rolling out worldwide with a new text-based interface, allowing users to type questions about what they&#x27;re viewing rather than speaking them aloud. The feature can now access full document context in Microsoft Office applications — meaning it can analyze an entire PowerPoint presentation or Excel spreadsheet without the user needing to scroll through every page.\"With 68 percent of consumers reporting using AI to support their decision making, voice is making this easier,\" Microsoft explained in its announcement. \"The magic unlock with Copilot Voice and Copilot Vision is the ease of interaction.\"During the press briefing, Microsoft demonstrated Copilot Vision helping users navigate Spotify&#x27;s settings to enable lossless audio streaming, coaching an artist through writing a professional bio based on their visual portfolio, and providing shopping recommendations based on products visible in YouTube videos.\"What brings AI to life is when you can give it rich context, when you can type great prompts,\" Mehdi explained. \"The big challenge for the majority of people is we&#x27;ve been trained with search to do the opposite. We&#x27;ve been trained to essentially type in fewer keywords, because it turns out the less keywords you type on search, the better your answers are.\"He noted that average search queries remain just 2.3 keywords, while AI systems perform better with detailed prompts — creating a disconnect between user habits and AI capabilities. Copilot Vision aims to bridge that gap by automatically gathering visual context.\"With Copilot Vision, you can simply share your screen and Copilot in literally milliseconds can understand everything on the screen and then provide intelligence,\" Mehdi said.The vision capabilities work with any application without requiring developers to build specific integrations, using computer vision to interpret on-screen content — a powerful capability that also raises questions about what the AI can access and when.Software robots take control: Inside Copilot Actions&#x27; controversial autonomyThe most ambitious—and potentially controversial—new capability is Copilot Actions, an experimental feature that allows AI to take control of a user&#x27;s computer to complete tasks autonomously.Coming first to Windows Insiders enrolled in Copilot Labs, the feature builds on Microsoft&#x27;s May announcement of Copilot Actions on the web, extending the capability to manipulate local files and applications on Windows PCs.During demonstrations, Microsoft showed the AI agent organizing photo libraries, extracting data from documents, and working through multi-step tasks while users attended to other work. The agent operates in a separate, sandboxed environment and provides running commentary on its actions, with users able to take control at any time.\"As a general-purpose agent — simply describe the task you want to complete in your own words, and the agent will attempt to complete it by interacting with desktop and web applications,\" according to the announcement. \"While this is happening, you can choose to focus on other tasks. At any time, you can take over the task or check in on the progress of the action, including reviewing what actions have been taken.\"Navjot Virk, Microsoft&#x27;s Windows Experience Leader, acknowledged the technology&#x27;s current limitations during the briefing. \"We&#x27;ll be starting with a narrow set of use cases while we optimize model performance and learn,\" Virk said. \"You may see the agent make mistakes or encounter challenges with complex interfaces, which is why real-world testing of this experience is so critical.\"The experimental nature of Copilot Actions reflects broader industry challenges with agentic AI — systems that can take actions rather than simply providing information. While the potential productivity gains are substantial, AI systems still occasionally \"hallucinate\" incorrect information and can be vulnerable to novel attacks.Can AI agents be trusted? Microsoft&#x27;s new security framework explainedRecognizing the security implications of giving AI control over users&#x27; computers and files, Microsoft introduced a new security framework built on four core principles: user control, operational transparency, limited privileges, and privacy-preserving design.Central to this approach is the concept of \"agent accounts\" — separate Windows user accounts under which AI agents operate, distinct from the human user&#x27;s account. Combined with a new \"agent workspace\" that provides a sandboxed desktop environment, the architecture aims to create clear boundaries around what agents can access and modify.Peter Waxman, Microsoft&#x27;s Windows Security Engineering Leader, emphasized that Copilot Actions is disabled by default and requires explicit user opt-in. \"You&#x27;re always in control of what Copilot Actions can do,\" Waxman said. \"Copilot Actions is turned off by default and you&#x27;re able to pause, take control, or disable it at any time.\"During operation, users can monitor the agent&#x27;s progress in real-time, and the system requests additional approval before taking \"sensitive or important\" actions. All agent activity occurs under the dedicated agent account, creating an audit trail that distinguishes AI actions from human ones.However, the agent will have default access to users&#x27; Documents, Downloads, Desktop, and Pictures folders—a broad permission grant that could concern enterprise IT administrators.Dana Huang, Corporate Vice President for Windows Security, acknowledged in a blog post that \"agentic AI applications introduce novel security risks, such as cross-prompt injection (XPIA), where malicious content embedded in UI elements or documents can override agent instructions, leading to unintended actions like data exfiltration or malware installation.\"Microsoft promises more details about enterprise controls at its Ignite conference in November.Gaming, taskbar redesign, and deeper Office integration round out updatesBeyond voice and autonomous agents, Microsoft introduced changes across Windows 11&#x27;s core interfaces and extended AI to new domains.A new \"Ask Copilot\" feature integrates AI directly into the Windows taskbar, providing one-click access to start conversations, activate vision capabilities, or search for files and settings with \"lightning-fast\" results. The opt-in feature doesn&#x27;t replace traditional Windows search.File Explorer gains AI capabilities through integration with third-party services. A partnership with Manus AI allows users to right-click on local image files and generate complete websites without manual uploading or coding. Integration with Filmora enables quick jumps into video editing workflows.Microsoft also introduced Copilot Connectors, allowing users to link cloud services like OneDrive, Outlook, Google Drive, Gmail, and Google Calendar directly to Copilot on Windows. Once connected, users can query personal content across platforms using natural language.In a notable expansion beyond productivity, Microsoft and Xbox introduced Gaming Copilot for the ROG Xbox Ally handheld gaming devices developed with ASUS. The feature, accessible via a dedicated hardware button, provides an AI assistant that can answer gameplay questions, offer strategic advice, and help navigate game interfaces through natural voice conversation.Why Microsoft is racing to embed AI everywhere before Apple and GoogleMicrosoft&#x27;s announcement comes as technology giants race to embed generative AI into their core products following the November 2022 launch of ChatGPT. While Microsoft moved quickly to integrate OpenAI&#x27;s technology into Bing search and introduce Copilot across its product line, the company has faced questions about whether AI features are driving meaningful engagement. Recent data shows Bing&#x27;s search market share remaining largely flat despite AI integration.The Windows integration represents a different approach: rather than charging separately for AI features, Microsoft is building them into the operating system itself, betting that embedded AI will drive Windows 11 adoption and competitive differentiation against Apple and Google.Apple has taken a more cautious approach with Apple Intelligence, introducing AI features gradually and emphasizing privacy through on-device processing. Google has integrated AI across its services but has faced challenges with accuracy and reliability.Crucially, while Microsoft highlighted new Copilot+ PC models from partners with prices ranging from $649.99 to $1,499.99, the core AI features announced today work on any Windows 11 PC — a significant departure from earlier positioning that suggested AI capabilities required new hardware with specialized neural processing units.\"Everything we showed you here is for all Windows 11 PCs. You don&#x27;t need to run it on a copilot plus PC. It works on any Windows 11 PC,\" Mehdi clarified.This democratization of AI features across the Windows 11 installed base potentially accelerates adoption but also complicates Microsoft&#x27;s hardware sales pitch for premium devices.What Microsoft&#x27;s AI bet means for the future of computingMehdi framed the announcement in sweeping terms, describing Microsoft&#x27;s goal as fundamentally reimagining the operating system for the AI era.\"We&#x27;re taking kind of a bold view of it. We really feel that the vision that we have is, let&#x27;s rewrite the entire operating system around AI and build essentially what becomes truly the AI PC,\" he said.For Microsoft, the success of AI-powered Windows 11 could help drive the company&#x27;s next phase of growth as PC sales have matured and cloud growth faces increased competition.For users and organizations, the announcement represents a potential inflection point in how humans interact with computers — one that could significantly boost productivity if executed well, or create new security headaches if the AI proves unreliable or difficult to control.The technology industry will be watching closely to see whether Microsoft&#x27;s bet on conversational computing and agentic AI marks the beginning of a genuine paradigm shift, or proves to be another ambitious interface reimagining that fails to gain mainstream traction.What&#x27;s clear is that Microsoft is moving aggressively to stake its claim as the leader in AI-powered personal computing, leveraging its dominant position in desktop operating systems to bring generative AI directly into the daily workflows of potentially a billion users.Copilot Voice and Vision are available today to Windows 11 users worldwide, with experimental capabilities coming to Windows Insiders in the coming weeks.",
          "content": "Microsoft is fundamentally reimagining how people interact with their computers, announcing Thursday a sweeping transformation of Windows 11 that brings voice-activated AI assistants, autonomous software agents, and contextual intelligence to every PC running the operating system — not just premium devices with specialized chips.The announcement represents Microsoft&#x27;s most aggressive push yet to integrate generative artificial intelligence into the desktop computing experience, moving beyond the chatbot interfaces that have defined the first wave of consumer AI products toward a more ambient, conversational model where users can simply talk to their computers and have AI agents complete complex tasks on their behalf.\"When we think about what the promise of an AI PC is, it should be capable of three things,\" Yusuf Mehdi, Microsoft&#x27;s Executive Vice President and Consumer Chief Marketing Officer, told reporters at a press conference last week. \"First, you should be able to interact with it naturally, in text or voice, and have it understand you. Second, it should be able to see what you see and be able to offer guided support. And third, it should be able to take action on your behalf.\"The shift could prove consequential for an industry searching for the \"killer app\" for generative AI. While hundreds of millions of people have experimented with ChatGPT and similar chatbots, integrating AI directly into the operating system that powers the vast majority of workplace computers could dramatically accelerate mainstream adoption — or create new security and privacy headaches for organizations already struggling to govern employee use of AI tools.How &#x27;Hey Copilot&#x27; aims to replace typing with talking on Windows PCsAt the heart of Microsoft&#x27;s vision is voice interaction, which the company is positioning as the third fundamental input method for PCs after the mouse and keyboard — a comparison that underscores Microsoft&#x27;s ambitions for reshaping human-computer interaction nearly four decades after the graphical user interface became standard.Starting this week, any Windows 11 user can enable the \"Hey Copilot\" wake word with a single click, allowing them to summon Microsoft&#x27;s AI assistant by voice from anywhere in the operating system. The feature, which had been in limited testing, is now being rolled out to hundreds of millions of devices globally.\"It&#x27;s been almost four decades since the PC has changed the way you interact with it, which is primarily mouse and keyboard,\" Mehdi said. \"When you think about it, we find that people type on a given day up to 14,000 words on their keyboard, which is really kind of mind-boggling. But what if now you can go beyond that and talk to it?\"The emphasis on voice reflects internal Microsoft data showing that users engage with Copilot twice as much when using voice compared to text input — a finding the company attributes to the lower cognitive barrier of speaking versus crafting precise written prompts.\"The magic unlock with Copilot Voice and Copilot Vision is the ease of interaction,\" according to the company&#x27;s announcement. \"Using the new wake word, &#x27;Hey Copilot,&#x27; getting something done is as easy as just asking for it.\"But Microsoft&#x27;s bet on voice computing faces real-world constraints that Mehdi acknowledged during the briefing. When asked whether workers in shared office environments would use voice features, potentially compromising privacy, Mehdi noted that millions already conduct voice calls through their PCs with headphones, and predicted users would adapt: \"Just like when the mouse came out, people have to figure out when to use it, what&#x27;s the right way, how to make it happen.\"Crucially, Microsoft is hedging its voice-first strategy by making all features accessible through traditional text input as well, recognizing that voice isn&#x27;t always appropriate or accessible.AI that sees your screen: Copilot Vision expands worldwide with new capabilitiesPerhaps more transformative than voice control is the expansion of Copilot Vision, a feature Microsoft introduced earlier this year that allows the AI to analyze what&#x27;s displayed on a user&#x27;s screen and provide contextual assistance.Previously limited to voice interaction, Copilot Vision is now rolling out worldwide with a new text-based interface, allowing users to type questions about what they&#x27;re viewing rather than speaking them aloud. The feature can now access full document context in Microsoft Office applications — meaning it can analyze an entire PowerPoint presentation or Excel spreadsheet without the user needing to scroll through every page.\"With 68 percent of consumers reporting using AI to support their decision making, voice is making this easier,\" Microsoft explained in its announcement. \"The magic unlock with Copilot Voice and Copilot Vision is the ease of interaction.\"During the press briefing, Microsoft demonstrated Copilot Vision helping users navigate Spotify&#x27;s settings to enable lossless audio streaming, coaching an artist through writing a professional bio based on their visual portfolio, and providing shopping recommendations based on products visible in YouTube videos.\"What brings AI to life is when you can give it rich context, when you can type great prompts,\" Mehdi explained. \"The big challenge for the majority of people is we&#x27;ve been trained with search to do the opposite. We&#x27;ve been trained to essentially type in fewer keywords, because it turns out the less keywords you type on search, the better your answers are.\"He noted that average search queries remain just 2.3 keywords, while AI systems perform better with detailed prompts — creating a disconnect between user habits and AI capabilities. Copilot Vision aims to bridge that gap by automatically gathering visual context.\"With Copilot Vision, you can simply share your screen and Copilot in literally milliseconds can understand everything on the screen and then provide intelligence,\" Mehdi said.The vision capabilities work with any application without requiring developers to build specific integrations, using computer vision to interpret on-screen content — a powerful capability that also raises questions about what the AI can access and when.Software robots take control: Inside Copilot Actions&#x27; controversial autonomyThe most ambitious—and potentially controversial—new capability is Copilot Actions, an experimental feature that allows AI to take control of a user&#x27;s computer to complete tasks autonomously.Coming first to Windows Insiders enrolled in Copilot Labs, the feature builds on Microsoft&#x27;s May announcement of Copilot Actions on the web, extending the capability to manipulate local files and applications on Windows PCs.During demonstrations, Microsoft showed the AI agent organizing photo libraries, extracting data from documents, and working through multi-step tasks while users attended to other work. The agent operates in a separate, sandboxed environment and provides running commentary on its actions, with users able to take control at any time.\"As a general-purpose agent — simply describe the task you want to complete in your own words, and the agent will attempt to complete it by interacting with desktop and web applications,\" according to the announcement. \"While this is happening, you can choose to focus on other tasks. At any time, you can take over the task or check in on the progress of the action, including reviewing what actions have been taken.\"Navjot Virk, Microsoft&#x27;s Windows Experience Leader, acknowledged the technology&#x27;s current limitations during the briefing. \"We&#x27;ll be starting with a narrow set of use cases while we optimize model performance and learn,\" Virk said. \"You may see the agent make mistakes or encounter challenges with complex interfaces, which is why real-world testing of this experience is so critical.\"The experimental nature of Copilot Actions reflects broader industry challenges with agentic AI — systems that can take actions rather than simply providing information. While the potential productivity gains are substantial, AI systems still occasionally \"hallucinate\" incorrect information and can be vulnerable to novel attacks.Can AI agents be trusted? Microsoft&#x27;s new security framework explainedRecognizing the security implications of giving AI control over users&#x27; computers and files, Microsoft introduced a new security framework built on four core principles: user control, operational transparency, limited privileges, and privacy-preserving design.Central to this approach is the concept of \"agent accounts\" — separate Windows user accounts under which AI agents operate, distinct from the human user&#x27;s account. Combined with a new \"agent workspace\" that provides a sandboxed desktop environment, the architecture aims to create clear boundaries around what agents can access and modify.Peter Waxman, Microsoft&#x27;s Windows Security Engineering Leader, emphasized that Copilot Actions is disabled by default and requires explicit user opt-in. \"You&#x27;re always in control of what Copilot Actions can do,\" Waxman said. \"Copilot Actions is turned off by default and you&#x27;re able to pause, take control, or disable it at any time.\"During operation, users can monitor the agent&#x27;s progress in real-time, and the system requests additional approval before taking \"sensitive or important\" actions. All agent activity occurs under the dedicated agent account, creating an audit trail that distinguishes AI actions from human ones.However, the agent will have default access to users&#x27; Documents, Downloads, Desktop, and Pictures folders—a broad permission grant that could concern enterprise IT administrators.Dana Huang, Corporate Vice President for Windows Security, acknowledged in a blog post that \"agentic AI applications introduce novel security risks, such as cross-prompt injection (XPIA), where malicious content embedded in UI elements or documents can override agent instructions, leading to unintended actions like data exfiltration or malware installation.\"Microsoft promises more details about enterprise controls at its Ignite conference in November.Gaming, taskbar redesign, and deeper Office integration round out updatesBeyond voice and autonomous agents, Microsoft introduced changes across Windows 11&#x27;s core interfaces and extended AI to new domains.A new \"Ask Copilot\" feature integrates AI directly into the Windows taskbar, providing one-click access to start conversations, activate vision capabilities, or search for files and settings with \"lightning-fast\" results. The opt-in feature doesn&#x27;t replace traditional Windows search.File Explorer gains AI capabilities through integration with third-party services. A partnership with Manus AI allows users to right-click on local image files and generate complete websites without manual uploading or coding. Integration with Filmora enables quick jumps into video editing workflows.Microsoft also introduced Copilot Connectors, allowing users to link cloud services like OneDrive, Outlook, Google Drive, Gmail, and Google Calendar directly to Copilot on Windows. Once connected, users can query personal content across platforms using natural language.In a notable expansion beyond productivity, Microsoft and Xbox introduced Gaming Copilot for the ROG Xbox Ally handheld gaming devices developed with ASUS. The feature, accessible via a dedicated hardware button, provides an AI assistant that can answer gameplay questions, offer strategic advice, and help navigate game interfaces through natural voice conversation.Why Microsoft is racing to embed AI everywhere before Apple and GoogleMicrosoft&#x27;s announcement comes as technology giants race to embed generative AI into their core products following the November 2022 launch of ChatGPT. While Microsoft moved quickly to integrate OpenAI&#x27;s technology into Bing search and introduce Copilot across its product line, the company has faced questions about whether AI features are driving meaningful engagement. Recent data shows Bing&#x27;s search market share remaining largely flat despite AI integration.The Windows integration represents a different approach: rather than charging separately for AI features, Microsoft is building them into the operating system itself, betting that embedded AI will drive Windows 11 adoption and competitive differentiation against Apple and Google.Apple has taken a more cautious approach with Apple Intelligence, introducing AI features gradually and emphasizing privacy through on-device processing. Google has integrated AI across its services but has faced challenges with accuracy and reliability.Crucially, while Microsoft highlighted new Copilot+ PC models from partners with prices ranging from $649.99 to $1,499.99, the core AI features announced today work on any Windows 11 PC — a significant departure from earlier positioning that suggested AI capabilities required new hardware with specialized neural processing units.\"Everything we showed you here is for all Windows 11 PCs. You don&#x27;t need to run it on a copilot plus PC. It works on any Windows 11 PC,\" Mehdi clarified.This democratization of AI features across the Windows 11 installed base potentially accelerates adoption but also complicates Microsoft&#x27;s hardware sales pitch for premium devices.What Microsoft&#x27;s AI bet means for the future of computingMehdi framed the announcement in sweeping terms, describing Microsoft&#x27;s goal as fundamentally reimagining the operating system for the AI era.\"We&#x27;re taking kind of a bold view of it. We really feel that the vision that we have is, let&#x27;s rewrite the entire operating system around AI and build essentially what becomes truly the AI PC,\" he said.For Microsoft, the success of AI-powered Windows 11 could help drive the company&#x27;s next phase of growth as PC sales have matured and cloud growth faces increased competition.For users and organizations, the announcement represents a potential inflection point in how humans interact with computers — one that could significantly boost productivity if executed well, or create new security headaches if the AI proves unreliable or difficult to control.The technology industry will be watching closely to see whether Microsoft&#x27;s bet on conversational computing and agentic AI marks the beginning of a genuine paradigm shift, or proves to be another ambitious interface reimagining that fails to gain mainstream traction.What&#x27;s clear is that Microsoft is moving aggressively to stake its claim as the leader in AI-powered personal computing, leveraging its dominant position in desktop operating systems to bring generative AI directly into the daily workflows of potentially a billion users.Copilot Voice and Vision are available today to Windows 11 users worldwide, with experimental capabilities coming to Windows Insiders in the coming weeks.",
          "feed_position": 3,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/1LtYH6GIABRoimOmrndJLF/cc718a2cc21b1fef4c64bebfe4a3a965/nuneybits_Vector_art_of_Microsoft_Windows_desktop_computer_mode_02e6a80a-72d4-467e-94ba-e6dfdd7d49c5.webp"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/social-media/pinterest-will-let-you-dial-down-ai-slop-in-your-feeds-130000337.html",
          "published_at": "Thu, 16 Oct 2025 13:00:00 +0000",
          "title": "Pinterest will let you 'dial down' AI slop in your feeds",
          "standfirst": "Pinterest is taking new steps to reduce the amount of AI slop its users are seeing. The service is adding settings that allow people to \"dial down\" AI-generated content in a number of categories that are \"highly-prone\" to such imagery, the company said in an update.While most social platforms have grappled with how to deal with the rise of AI-created content, Pinterest has been particularly inundated. Its image-board UI has proven particularly susceptible to AI slop, and users have complained about the difficulty of finding content created by humans. Now, Pinterest is offering users more control over how much AI content appears in their recommendations. The service is adding a \"refine your recommendations\" setting that allows you to toggle generative AI content from specific categories, including art, architecture, beauty, fashion, entertainment, health, home decor and sport. According to the company, these topics have seen an influx of AI-generated content, but users should \"expect even more additions in the future.\"Notably, Pinterest isn't promising to root out generative AI content entirely. Rather, it says the new settings should \"dial down\" the amount of AI-based content they're seeing in a particular category. A spokesperson for the company says this is because not all AI-generated content on the platform is low quality and some users are in fact open to seeing AI-generated material.The setting also applies only to image pins, not video, so it likely won't do much to prevent Sora or other AI-created video clips from appearing in your feeds. For AI-created or AI-edited content that does continue to surface, Pinterest says it will label these posts more prominently. The company started experimenting with labels back in May, but has now \"ramped up\" its tools for identifying such content. Pinterest's new settings are available now on desktop and Android and will be available on iOS in the next few weeks.This article originally appeared on Engadget at https://www.engadget.com/social-media/pinterest-will-let-you-dial-down-ai-slop-in-your-feeds-130000337.html?src=rss",
          "content": "Pinterest is taking new steps to reduce the amount of AI slop its users are seeing. The service is adding settings that allow people to \"dial down\" AI-generated content in a number of categories that are \"highly-prone\" to such imagery, the company said in an update.While most social platforms have grappled with how to deal with the rise of AI-created content, Pinterest has been particularly inundated. Its image-board UI has proven particularly susceptible to AI slop, and users have complained about the difficulty of finding content created by humans. Now, Pinterest is offering users more control over how much AI content appears in their recommendations. The service is adding a \"refine your recommendations\" setting that allows you to toggle generative AI content from specific categories, including art, architecture, beauty, fashion, entertainment, health, home decor and sport. According to the company, these topics have seen an influx of AI-generated content, but users should \"expect even more additions in the future.\"Notably, Pinterest isn't promising to root out generative AI content entirely. Rather, it says the new settings should \"dial down\" the amount of AI-based content they're seeing in a particular category. A spokesperson for the company says this is because not all AI-generated content on the platform is low quality and some users are in fact open to seeing AI-generated material.The setting also applies only to image pins, not video, so it likely won't do much to prevent Sora or other AI-created video clips from appearing in your feeds. For AI-created or AI-edited content that does continue to surface, Pinterest says it will label these posts more prominently. The company started experimenting with labels back in May, but has now \"ramped up\" its tools for identifying such content. Pinterest's new settings are available now on desktop and Android and will be available on iOS in the next few weeks.This article originally appeared on Engadget at https://www.engadget.com/social-media/pinterest-will-let-you-dial-down-ai-slop-in-your-feeds-130000337.html?src=rss",
          "feed_position": 20
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/computing/microsofts-next-windows-11-ai-gamble-just-say-hey-copilot-130000875.html",
          "published_at": "Thu, 16 Oct 2025 13:00:00 +0000",
          "title": "Microsoft's next Windows 11 AI gamble: Just say 'Hey Copilot'",
          "standfirst": "Over a decade since Microsoft tried to make talking to Cortana on PCs a thing -- and spectacularly failed in the process -- the Windows giant is taking another swing at voice commands with its Copilot AI assistant in Windows 11. Starting today, the company is rolling out an upgrade to its existing Copilot Voice and Vision features which will let you say \"Hey, Copilot\" and then ask your PC a question based on what's on the screen. If you're looking at pictures of Hawaii, for example, you could ask your Windows 11 PC where exactly they were taken, have it plot you a flight plan and potentially even give you some budgeting tips to afford that island vacation.Microsoft's jaunty promotional videos for the Copilot features, set to Vampire Weekend's almost two-decade-old \"A Punk,\" make the process look practically seamless. One user asks Copilot to show them how to stream their music in the \"best possible quality,\" and the AI proceeds to highlight the exact location of the streaming settings in Spotify, while suggesting they choose the lossless option. Another person asks Copilot to write up a short biography based on their photo portfolio. Now Copilot isn't just about searching the web or generating novelty AI art, it's making it easy for users to perform practical tasks without much effort.Microsoft is clearly striving for the convenience of the Star Trek ship computer, a dream that also pushed Amazon to invest billions in its Echo devices and Alexa. The difference with Copilot is that you're not just talking to a faceless speaker -- Microsoft is also trying to make Windows 11 aware of what you're doing on your screen. The \"Hey Copilot\" feature and all of the Copilot Vision are cloud-based, so you'll have to live with image data of your desktop making its way to Microsoft's servers. That involves a level of trust the company has lost with many users, especially after the messy debut of Recall, its first flagship AI-powered feature.It doesn't help that many people are still peeved about the death of Windows 10 support this week. Unsurprisingly, the company stresses that \"Hey, Copilot\" is a purely opt-in feature that's buried in the Copilot app settings. (Of course, that can always change, especially if the company wants to juice AI engagement stats in a few years.)Copilot ActionsMicrosoftI suspect it'll be even harder for users to swallow where Microsoft wants to take Copilot: Giving it the ability to perform Windows tasks on its own. That's the goal of the experimental Copilot Actions feature, which initially debuted as a tool that could perform tasks on websites. Once enabled, Copilot Actions can be prompted to handle manual tasks, like resizing and straightening an a folder of photos. If any questions pop up, it can prompt you to answer them within the Copilot app. And as Copilot Actions is handling its job in the background, you're free to do anything else you'd like on your computer.Conceptually, Copilot Actions sounds similar to handing off a task to a real life assistant -- but just like a human assistant, there's always a chance something could go wrong along the way. It's also not hard to imagine the feature being coopted by nefarious malware down the line, since it's basically a Windows script in a better interface. Microsoft says it's tested Copilot Actions \"extensively\" internally, and it's rolling out the feature slowly to gather feedback.Just like \"Hey, Copilot,\" it's entirely opt-in, and you can see everything Copilot Actions is doing step-by-step in the Copilot app. Microsoft says you'l be able to jump in and take control of a Copilot Action job at any point, as well as control the permissions of AI agents in Windows 11’s user settings. Copilot tasks are also performed in a contained environment, according to Microsoft, which allows for even more specific permissions controls as well as runtime isolation (so Copilot can’t affect the rest of your system beyond its specific task).And as if we're not already inundated with Copilot all over Windows 11 already, Microsoft also plans to add an \"Ask Copilot\" search function right on Windows 11's taskbar. The company claims it's part of a mission to make the taskbar \"a dynamic hub\" for accomplishing tasks, but personally I like to keep my taskbar clear so I can cram in more app windows. Like everything Microsoft is announcing today, the Ask Copilot bar will also be entirely opt-in.As someone who’s been skeptical of Microsoft’s Copilot initiatives so far, I could actually see myself using “Hey Copilot” if it works as advertised. It sounds far more practical than the old Siri voice commands, which were limited by simplistic language models from a decade ago. Microsoft is also expanding AI actions built into Windows 11, including a new integration with Manus, an AI agent that can do things like turn several documents into a website, as well as Filmora, which lets you create AI videos right from the File Explorer.The new \"Hey Copilot\" and Copilot Vision features are available today on all Windows 11 PCs that have access to Copilot. Microsoft is also making Copilot Vision broadly available around the world today where Copilot is available. Copilot Actions and the Ask Copilot taskbar feature will \"gradually\" become available to Windows 11 Insiders, according to Microsoft.This article originally appeared on Engadget at https://www.engadget.com/computing/microsofts-next-windows-11-ai-gamble-just-say-hey-copilot-130000875.html?src=rss",
          "content": "Over a decade since Microsoft tried to make talking to Cortana on PCs a thing -- and spectacularly failed in the process -- the Windows giant is taking another swing at voice commands with its Copilot AI assistant in Windows 11. Starting today, the company is rolling out an upgrade to its existing Copilot Voice and Vision features which will let you say \"Hey, Copilot\" and then ask your PC a question based on what's on the screen. If you're looking at pictures of Hawaii, for example, you could ask your Windows 11 PC where exactly they were taken, have it plot you a flight plan and potentially even give you some budgeting tips to afford that island vacation.Microsoft's jaunty promotional videos for the Copilot features, set to Vampire Weekend's almost two-decade-old \"A Punk,\" make the process look practically seamless. One user asks Copilot to show them how to stream their music in the \"best possible quality,\" and the AI proceeds to highlight the exact location of the streaming settings in Spotify, while suggesting they choose the lossless option. Another person asks Copilot to write up a short biography based on their photo portfolio. Now Copilot isn't just about searching the web or generating novelty AI art, it's making it easy for users to perform practical tasks without much effort.Microsoft is clearly striving for the convenience of the Star Trek ship computer, a dream that also pushed Amazon to invest billions in its Echo devices and Alexa. The difference with Copilot is that you're not just talking to a faceless speaker -- Microsoft is also trying to make Windows 11 aware of what you're doing on your screen. The \"Hey Copilot\" feature and all of the Copilot Vision are cloud-based, so you'll have to live with image data of your desktop making its way to Microsoft's servers. That involves a level of trust the company has lost with many users, especially after the messy debut of Recall, its first flagship AI-powered feature.It doesn't help that many people are still peeved about the death of Windows 10 support this week. Unsurprisingly, the company stresses that \"Hey, Copilot\" is a purely opt-in feature that's buried in the Copilot app settings. (Of course, that can always change, especially if the company wants to juice AI engagement stats in a few years.)Copilot ActionsMicrosoftI suspect it'll be even harder for users to swallow where Microsoft wants to take Copilot: Giving it the ability to perform Windows tasks on its own. That's the goal of the experimental Copilot Actions feature, which initially debuted as a tool that could perform tasks on websites. Once enabled, Copilot Actions can be prompted to handle manual tasks, like resizing and straightening an a folder of photos. If any questions pop up, it can prompt you to answer them within the Copilot app. And as Copilot Actions is handling its job in the background, you're free to do anything else you'd like on your computer.Conceptually, Copilot Actions sounds similar to handing off a task to a real life assistant -- but just like a human assistant, there's always a chance something could go wrong along the way. It's also not hard to imagine the feature being coopted by nefarious malware down the line, since it's basically a Windows script in a better interface. Microsoft says it's tested Copilot Actions \"extensively\" internally, and it's rolling out the feature slowly to gather feedback.Just like \"Hey, Copilot,\" it's entirely opt-in, and you can see everything Copilot Actions is doing step-by-step in the Copilot app. Microsoft says you'l be able to jump in and take control of a Copilot Action job at any point, as well as control the permissions of AI agents in Windows 11’s user settings. Copilot tasks are also performed in a contained environment, according to Microsoft, which allows for even more specific permissions controls as well as runtime isolation (so Copilot can’t affect the rest of your system beyond its specific task).And as if we're not already inundated with Copilot all over Windows 11 already, Microsoft also plans to add an \"Ask Copilot\" search function right on Windows 11's taskbar. The company claims it's part of a mission to make the taskbar \"a dynamic hub\" for accomplishing tasks, but personally I like to keep my taskbar clear so I can cram in more app windows. Like everything Microsoft is announcing today, the Ask Copilot bar will also be entirely opt-in.As someone who’s been skeptical of Microsoft’s Copilot initiatives so far, I could actually see myself using “Hey Copilot” if it works as advertised. It sounds far more practical than the old Siri voice commands, which were limited by simplistic language models from a decade ago. Microsoft is also expanding AI actions built into Windows 11, including a new integration with Manus, an AI agent that can do things like turn several documents into a website, as well as Filmora, which lets you create AI videos right from the File Explorer.The new \"Hey Copilot\" and Copilot Vision features are available today on all Windows 11 PCs that have access to Copilot. Microsoft is also making Copilot Vision broadly available around the world today where Copilot is available. Copilot Actions and the Ask Copilot taskbar feature will \"gradually\" become available to Windows 11 Insiders, according to Microsoft.This article originally appeared on Engadget at https://www.engadget.com/computing/microsofts-next-windows-11-ai-gamble-just-say-hey-copilot-130000875.html?src=rss",
          "feed_position": 21,
          "image_url": "https://d29szjachogqwa.cloudfront.net/videos/user-uploaded/Copilot_Actions_Image.jpg"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/cybersecurity/vpn/how-to-cancel-expressvpn-and-get-a-full-refund-123020142.html",
          "published_at": "Thu, 16 Oct 2025 12:30:20 +0000",
          "title": "How to cancel ExpressVPN and get a full refund",
          "standfirst": "ExpressVPN is one of the best VPNs on the market, with user-friendly apps, excellent speed test scores and a strong security record. In my ExpressVPN review, I found it to live up to its positive word of mouth, especially when unblocking foreign streaming sites. But no service is perfect, and my cup of VPN tea is not everybody's. If you're looking to switch away, follow this guide to cancel ExpressVPN. How to cancel ExpressVPN on desktop No matter where you originally signed up for ExpressVPN, you can cancel through your browser on a desktop platform. With any browser (i.e. Chrome, Safari, etc), the steps are as follows. Note that doing this will instantly cancel your ExpressVPN subscription and revoke your access to the service. Go to expressvpn.com. In the top bar, click on My Account. Enter your username and password to sign in. You'll be taken to your account dashboard. In the menu on the left-hand side of the screen, click Subscription. Scroll down until you find the words \"Subscription details.\" You should see a box containing the monthly cost of your subscription. At the right of that box, click Cancel Subscription. From here, follow the on-screen prompts to complete cancellation. Sam Chapman for Engadget If you're not ready to lose ExpressVPN service just yet, you can end auto-renewal instead of cancelling altogether. You'll still be able to use ExpressVPN until your subscription runs out. To cancel auto-renew, go to the subscription tab of your account dashboard and find the box with your subscription ID (it should be right at the top). In that box, click the link that says Edit subscription settings, then scroll down and click Turn Off Automatic Renewal. If you change your mind before your subscription lapses, you'll be able to turn it back on later. How to cancel ExpressVPN on mobile You can also cancel ExpressVPN on your phone or tablet, but the process is largely the same — some buttons are just in different places. As above, this instantly ends your ExpressVPN subscription. Here's how to do it. Go to expressvpn.com in your mobile browser. Tap the three horizontal lines at the top-right, then in the menu that appears, tap My Account. In the account dashboard, tap the three horizontal lines at the top-right once again. This time, scroll down to the expanded \"My Account\" menu and tap Subscription. Scroll down to the \"Subscription details\" box and tap Cancel Subscription. Follow the prompts on the screen to complete cancellation. Like on desktop, you can also turn off auto-renewal so your subscription ends when your current pay period expires. Go to the subscription tab of your account dashboard as described in the steps, find the box with your subscription ID, then follow the steps from the last paragraph of the previous section. Cancelling ExpressVPN through an app store The website is almost always the right way to cancel ExpressVPN, but there is one exception: if you originally signed up through the Google Play Store or Apple App Store. This includes both paid subscriptions and the 7-day free trials ExpressVPN offers to app store users. In this case, you'll need to cancel through the app store where you originally made the purchase. On Apple, open the app store, then tap Subscriptions and scroll down to find ExpressVPN. On Android, open Google Play and tap Payment & subscriptions, then Subscriptions. In both cases, once you've found ExpressVPN, tap it and scroll down to find the button for cancellation. How to get a refund from ExpressVPN ExpressVPN offers a full refund to anyone who cancels a subscription within 30 days of purchase. You can also get a refund if your subscription renews without your consent — if that happens, you have 14 days to request your money back. Sam Chapman for Engadget There's no dedicated button for asking for a refund. Instead, you'll need to contact ExpressVPN. To do that, go to expressvpn.com/support, then click on the button in the bottom-right corner that says Need help? Chat with us! This will open a chat with a bot which you can use to ask for your money back. Unfortunately, there's no public link to email the support team, so live chat is the only option here. ExpressVPN alternatives Depending on what made you want to cancel ExpressVPN, there are a few other VPNs you might like better. Proton VPN is my favorite provider at the moment, combining great apps with a unique focus on privacy rights (it's also cheaper). NordVPN has a lot of useful features ExpressVPN leaves off its no-frills clients. And if you're all about speed, Surfshark is the current fastest VPN in my tests.This article originally appeared on Engadget at https://www.engadget.com/cybersecurity/vpn/how-to-cancel-expressvpn-and-get-a-full-refund-123020142.html?src=rss",
          "content": "ExpressVPN is one of the best VPNs on the market, with user-friendly apps, excellent speed test scores and a strong security record. In my ExpressVPN review, I found it to live up to its positive word of mouth, especially when unblocking foreign streaming sites. But no service is perfect, and my cup of VPN tea is not everybody's. If you're looking to switch away, follow this guide to cancel ExpressVPN. How to cancel ExpressVPN on desktop No matter where you originally signed up for ExpressVPN, you can cancel through your browser on a desktop platform. With any browser (i.e. Chrome, Safari, etc), the steps are as follows. Note that doing this will instantly cancel your ExpressVPN subscription and revoke your access to the service. Go to expressvpn.com. In the top bar, click on My Account. Enter your username and password to sign in. You'll be taken to your account dashboard. In the menu on the left-hand side of the screen, click Subscription. Scroll down until you find the words \"Subscription details.\" You should see a box containing the monthly cost of your subscription. At the right of that box, click Cancel Subscription. From here, follow the on-screen prompts to complete cancellation. Sam Chapman for Engadget If you're not ready to lose ExpressVPN service just yet, you can end auto-renewal instead of cancelling altogether. You'll still be able to use ExpressVPN until your subscription runs out. To cancel auto-renew, go to the subscription tab of your account dashboard and find the box with your subscription ID (it should be right at the top). In that box, click the link that says Edit subscription settings, then scroll down and click Turn Off Automatic Renewal. If you change your mind before your subscription lapses, you'll be able to turn it back on later. How to cancel ExpressVPN on mobile You can also cancel ExpressVPN on your phone or tablet, but the process is largely the same — some buttons are just in different places. As above, this instantly ends your ExpressVPN subscription. Here's how to do it. Go to expressvpn.com in your mobile browser. Tap the three horizontal lines at the top-right, then in the menu that appears, tap My Account. In the account dashboard, tap the three horizontal lines at the top-right once again. This time, scroll down to the expanded \"My Account\" menu and tap Subscription. Scroll down to the \"Subscription details\" box and tap Cancel Subscription. Follow the prompts on the screen to complete cancellation. Like on desktop, you can also turn off auto-renewal so your subscription ends when your current pay period expires. Go to the subscription tab of your account dashboard as described in the steps, find the box with your subscription ID, then follow the steps from the last paragraph of the previous section. Cancelling ExpressVPN through an app store The website is almost always the right way to cancel ExpressVPN, but there is one exception: if you originally signed up through the Google Play Store or Apple App Store. This includes both paid subscriptions and the 7-day free trials ExpressVPN offers to app store users. In this case, you'll need to cancel through the app store where you originally made the purchase. On Apple, open the app store, then tap Subscriptions and scroll down to find ExpressVPN. On Android, open Google Play and tap Payment & subscriptions, then Subscriptions. In both cases, once you've found ExpressVPN, tap it and scroll down to find the button for cancellation. How to get a refund from ExpressVPN ExpressVPN offers a full refund to anyone who cancels a subscription within 30 days of purchase. You can also get a refund if your subscription renews without your consent — if that happens, you have 14 days to request your money back. Sam Chapman for Engadget There's no dedicated button for asking for a refund. Instead, you'll need to contact ExpressVPN. To do that, go to expressvpn.com/support, then click on the button in the bottom-right corner that says Need help? Chat with us! This will open a chat with a bot which you can use to ask for your money back. Unfortunately, there's no public link to email the support team, so live chat is the only option here. ExpressVPN alternatives Depending on what made you want to cancel ExpressVPN, there are a few other VPNs you might like better. Proton VPN is my favorite provider at the moment, combining great apps with a unique focus on privacy rights (it's also cheaper). NordVPN has a lot of useful features ExpressVPN leaves off its no-frills clients. And if you're all about speed, Surfshark is the current fastest VPN in my tests.This article originally appeared on Engadget at https://www.engadget.com/cybersecurity/vpn/how-to-cancel-expressvpn-and-get-a-full-refund-123020142.html?src=rss",
          "feed_position": 22,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2025-10/f6e12b90-a9fc-11f0-b39d-36379e1ff141"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/ace-prevents-context-collapse-with-evolving-playbooks-for-self-improving-ai",
          "published_at": "Thu, 16 Oct 2025 12:00:00 GMT",
          "title": "ACE prevents context collapse with ‘evolving playbooks’ for self-improving AI agents",
          "standfirst": "A new framework from Stanford University and SambaNova addresses a critical challenge in building robust AI agents: context engineering. Called Agentic Context Engineering (ACE), the framework automatically populates and modifies the context window of large language model (LLM) applications by treating it as an “evolving playbook” that creates and refines strategies as the agent gains experience in its environment.ACE is designed to overcome key limitations of other context-engineering frameworks, preventing the model’s context from degrading as it accumulates more information. Experiments show that ACE works for both optimizing system prompts and managing an agent&#x27;s memory, outperforming other methods while also being significantly more efficient.The challenge of context engineeringAdvanced AI applications that use LLMs largely rely on \"context adaptation,\" or context engineering, to guide their behavior. Instead of the costly process of retraining or fine-tuning the model, developers use the LLM’s in-context learning abilities to guide its behavior by modifying the input prompts with specific instructions, reasoning steps, or domain-specific knowledge. This additional information is usually obtained as the agent interacts with its environment and gathers new data and experience. The key goal of context engineering is to organize this new information in a way that improves the model’s performance and avoids confusing it. This approach is becoming a central paradigm for building capable, scalable, and self-improving AI systems.Context engineering has several advantages for enterprise applications. Contexts are interpretable for both users and developers, can be updated with new knowledge at runtime, and can be shared across different models. Context engineering also benefits from ongoing hardware and software advances, such as the growing context windows of LLMs and efficient inference techniques like prompt and context caching.There are various automated context-engineering techniques, but most of them face two key limitations. The first is a “brevity bias,” where prompt optimization methods tend to favor concise, generic instructions over comprehensive, detailed ones. This can undermine performance in complex domains. The second, more severe issue is \"context collapse.\" When an LLM is tasked with repeatedly rewriting its entire accumulated context, it can suffer from a kind of digital amnesia.“What we call ‘context collapse’ happens when an AI tries to rewrite or compress everything it has learned into a single new version of its prompt or memory,” the researchers said in written comments to VentureBeat. “Over time, that rewriting process erases important details—like overwriting a document so many times that key notes disappear. In customer-facing systems, this could mean a support agent suddenly losing awareness of past interactions... causing erratic or inconsistent behavior.”The researchers argue that “contexts should function not as concise summaries, but as comprehensive, evolving playbooks—detailed, inclusive, and rich with domain insights.” This approach leans into the strength of modern LLMs, which can effectively distill relevance from long and detailed contexts.How Agentic Context Engineering (ACE) worksACE is a framework for comprehensive context adaptation designed for both offline tasks, like system prompt optimization, and online scenarios, such as real-time memory updates for agents. Rather than compressing information, ACE treats the context like a dynamic playbook that gathers and organizes strategies over time.The framework divides the labor across three specialized roles: a Generator, a Reflector, and a Curator. This modular design is inspired by “how humans learn—experimenting, reflecting, and consolidating—while avoiding the bottleneck of overloading a single model with all responsibilities,” according to the paper.The workflow starts with the Generator, which produces reasoning paths for input prompts, highlighting both effective strategies and common mistakes. The Reflector then analyzes these paths to extract key lessons. Finally, the Curator synthesizes these lessons into compact updates and merges them into the existing playbook.To prevent context collapse and brevity bias, ACE incorporates two key design principles. First, it uses incremental updates. The context is represented as a collection of structured, itemized bullets instead of a single block of text. This allows ACE to make granular changes and retrieve the most relevant information without rewriting the entire context.Second, ACE uses a “grow-and-refine” mechanism. As new experiences are gathered, new bullets are appended to the playbook and existing ones are updated. A de-duplication step regularly removes redundant entries, ensuring the context remains comprehensive yet relevant and compact over time.ACE in actionThe researchers evaluated ACE on two types of tasks that benefit from evolving context: agent benchmarks requiring multi-turn reasoning and tool use, and domain-specific financial analysis benchmarks demanding specialized knowledge. For high-stakes industries like finance, the benefits extend beyond pure performance. As the researchers said, the framework is “far more transparent: a compliance officer can literally read what the AI learned, since it’s stored in human-readable text rather than hidden in billions of parameters.”The results showed that ACE consistently outperformed strong baselines such as GEPA and classic in-context learning, achieving average performance gains of 10.6% on agent tasks and 8.6% on domain-specific benchmarks in both offline and online settings.Critically, ACE can build effective contexts by analyzing the feedback from its actions and environment instead of requiring manually labeled data. The researchers note that this ability is a \"key ingredient for self-improving LLMs and agents.\" On the public AppWorld benchmark, designed to evaluate agentic systems, an agent using ACE with a smaller open-source model (DeepSeek-V3.1) matched the performance of the top-ranked, GPT-4.1-powered agent on average and surpassed it on the more difficult test set.The takeaway for businesses is significant. “This means companies don’t have to depend on massive proprietary models to stay competitive,” the research team said. “They can deploy local models, protect sensitive data, and still get top-tier results by continuously refining context instead of retraining weights.”Beyond accuracy, ACE proved to be highly efficient. It adapts to new tasks with an average 86.9% lower latency than existing methods and requires fewer steps and tokens. The researchers point out that this efficiency demonstrates that “scalable self-improvement can be achieved with both higher accuracy and lower overhead.”For enterprises concerned about inference costs, the researchers point out that the longer contexts produced by ACE do not translate to proportionally higher costs. Modern serving infrastructures are increasingly optimized for long-context workloads with techniques like KV cache reuse, compression, and offloading, which amortize the cost of handling extensive context.Ultimately, ACE points toward a future where AI systems are dynamic and continuously improving. \"Today, only AI engineers can update models, but context engineering opens the door for domain experts—lawyers, analysts, doctors—to directly shape what the AI knows by editing its contextual playbook,\" the researchers said. This also makes governance more practical. \"Selective unlearning becomes much more tractable: if a piece of information is outdated or legally sensitive, it can simply be removed or replaced in the context, without retraining the model.”",
          "content": "A new framework from Stanford University and SambaNova addresses a critical challenge in building robust AI agents: context engineering. Called Agentic Context Engineering (ACE), the framework automatically populates and modifies the context window of large language model (LLM) applications by treating it as an “evolving playbook” that creates and refines strategies as the agent gains experience in its environment.ACE is designed to overcome key limitations of other context-engineering frameworks, preventing the model’s context from degrading as it accumulates more information. Experiments show that ACE works for both optimizing system prompts and managing an agent&#x27;s memory, outperforming other methods while also being significantly more efficient.The challenge of context engineeringAdvanced AI applications that use LLMs largely rely on \"context adaptation,\" or context engineering, to guide their behavior. Instead of the costly process of retraining or fine-tuning the model, developers use the LLM’s in-context learning abilities to guide its behavior by modifying the input prompts with specific instructions, reasoning steps, or domain-specific knowledge. This additional information is usually obtained as the agent interacts with its environment and gathers new data and experience. The key goal of context engineering is to organize this new information in a way that improves the model’s performance and avoids confusing it. This approach is becoming a central paradigm for building capable, scalable, and self-improving AI systems.Context engineering has several advantages for enterprise applications. Contexts are interpretable for both users and developers, can be updated with new knowledge at runtime, and can be shared across different models. Context engineering also benefits from ongoing hardware and software advances, such as the growing context windows of LLMs and efficient inference techniques like prompt and context caching.There are various automated context-engineering techniques, but most of them face two key limitations. The first is a “brevity bias,” where prompt optimization methods tend to favor concise, generic instructions over comprehensive, detailed ones. This can undermine performance in complex domains. The second, more severe issue is \"context collapse.\" When an LLM is tasked with repeatedly rewriting its entire accumulated context, it can suffer from a kind of digital amnesia.“What we call ‘context collapse’ happens when an AI tries to rewrite or compress everything it has learned into a single new version of its prompt or memory,” the researchers said in written comments to VentureBeat. “Over time, that rewriting process erases important details—like overwriting a document so many times that key notes disappear. In customer-facing systems, this could mean a support agent suddenly losing awareness of past interactions... causing erratic or inconsistent behavior.”The researchers argue that “contexts should function not as concise summaries, but as comprehensive, evolving playbooks—detailed, inclusive, and rich with domain insights.” This approach leans into the strength of modern LLMs, which can effectively distill relevance from long and detailed contexts.How Agentic Context Engineering (ACE) worksACE is a framework for comprehensive context adaptation designed for both offline tasks, like system prompt optimization, and online scenarios, such as real-time memory updates for agents. Rather than compressing information, ACE treats the context like a dynamic playbook that gathers and organizes strategies over time.The framework divides the labor across three specialized roles: a Generator, a Reflector, and a Curator. This modular design is inspired by “how humans learn—experimenting, reflecting, and consolidating—while avoiding the bottleneck of overloading a single model with all responsibilities,” according to the paper.The workflow starts with the Generator, which produces reasoning paths for input prompts, highlighting both effective strategies and common mistakes. The Reflector then analyzes these paths to extract key lessons. Finally, the Curator synthesizes these lessons into compact updates and merges them into the existing playbook.To prevent context collapse and brevity bias, ACE incorporates two key design principles. First, it uses incremental updates. The context is represented as a collection of structured, itemized bullets instead of a single block of text. This allows ACE to make granular changes and retrieve the most relevant information without rewriting the entire context.Second, ACE uses a “grow-and-refine” mechanism. As new experiences are gathered, new bullets are appended to the playbook and existing ones are updated. A de-duplication step regularly removes redundant entries, ensuring the context remains comprehensive yet relevant and compact over time.ACE in actionThe researchers evaluated ACE on two types of tasks that benefit from evolving context: agent benchmarks requiring multi-turn reasoning and tool use, and domain-specific financial analysis benchmarks demanding specialized knowledge. For high-stakes industries like finance, the benefits extend beyond pure performance. As the researchers said, the framework is “far more transparent: a compliance officer can literally read what the AI learned, since it’s stored in human-readable text rather than hidden in billions of parameters.”The results showed that ACE consistently outperformed strong baselines such as GEPA and classic in-context learning, achieving average performance gains of 10.6% on agent tasks and 8.6% on domain-specific benchmarks in both offline and online settings.Critically, ACE can build effective contexts by analyzing the feedback from its actions and environment instead of requiring manually labeled data. The researchers note that this ability is a \"key ingredient for self-improving LLMs and agents.\" On the public AppWorld benchmark, designed to evaluate agentic systems, an agent using ACE with a smaller open-source model (DeepSeek-V3.1) matched the performance of the top-ranked, GPT-4.1-powered agent on average and surpassed it on the more difficult test set.The takeaway for businesses is significant. “This means companies don’t have to depend on massive proprietary models to stay competitive,” the research team said. “They can deploy local models, protect sensitive data, and still get top-tier results by continuously refining context instead of retraining weights.”Beyond accuracy, ACE proved to be highly efficient. It adapts to new tasks with an average 86.9% lower latency than existing methods and requires fewer steps and tokens. The researchers point out that this efficiency demonstrates that “scalable self-improvement can be achieved with both higher accuracy and lower overhead.”For enterprises concerned about inference costs, the researchers point out that the longer contexts produced by ACE do not translate to proportionally higher costs. Modern serving infrastructures are increasingly optimized for long-context workloads with techniques like KV cache reuse, compression, and offloading, which amortize the cost of handling extensive context.Ultimately, ACE points toward a future where AI systems are dynamic and continuously improving. \"Today, only AI engineers can update models, but context engineering opens the door for domain experts—lawyers, analysts, doctors—to directly shape what the AI knows by editing its contextual playbook,\" the researchers said. This also makes governance more practical. \"Selective unlearning becomes much more tractable: if a piece of information is outdated or legally sensitive, it can simply be removed or replaced in the context, without retraining the model.”",
          "feed_position": 4,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/4EJUoAdNGN4myXdb69GnMD/f8ab6aaf2305a72ad84dcc5e4afb1beb/agentic_context_engineering.jpg"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/get-50-percent-off-your-first-year-subscription-to-one-of-our-favorite-budgeting-apps-174011646.html",
          "published_at": "Thu, 16 Oct 2025 09:01:26 +0000",
          "title": "Get 50 percent off your first year subscription to one of our favorite budgeting apps",
          "standfirst": "Monarch Money is one of our favorite budgeting apps and, fittingly enough, there's a way for newcomers to save money on a subscription right now. If you use the code MONARCHVIP at checkout, you can get an annual plan for 50 percent off. It typically costs $100, but you can get 12 months of access for $50 with this code. There are some key caveats here. The discount is only for new users, and it can't be combined with other offers. The code only works when you sign up through the web. You can't redeem it through the Monarch mobile app. We feel that Monarch has a steeper learning curve than some other budget trackers and that certain aspects of the app are slightly more complex than they probably need to be. But it offers a great deal of customization and granularity, which outweighs our misgivings. On the main dashboard, you'll see your net worth along with your latest transactions, spending versus the previous month, your income so far for the month and details about upcoming bills, your investments and goals you've set. There's also a link to a month-in-review page, which offers an in-depth overview of what's been happening with your money that month. You'll also be able to take a peek at how your net worth has changed over time. Monarch can connect to your bank and track Apple Card, Apple Cash and Savings accounts. It can pull in your transactions and balance history automatically and detect your recurring expenses and income. The app can even keep your car valuation up to date. While it might take a little work to set up Monarch (and you might have to tweak things here and there), it's a detailed budgeting app that can help you keep better track of your income, expenditure and net worth.This article originally appeared on Engadget at https://www.engadget.com/deals/get-50-percent-off-your-first-year-subscription-to-one-of-our-favorite-budgeting-apps-174011646.html?src=rss",
          "content": "Monarch Money is one of our favorite budgeting apps and, fittingly enough, there's a way for newcomers to save money on a subscription right now. If you use the code MONARCHVIP at checkout, you can get an annual plan for 50 percent off. It typically costs $100, but you can get 12 months of access for $50 with this code. There are some key caveats here. The discount is only for new users, and it can't be combined with other offers. The code only works when you sign up through the web. You can't redeem it through the Monarch mobile app. We feel that Monarch has a steeper learning curve than some other budget trackers and that certain aspects of the app are slightly more complex than they probably need to be. But it offers a great deal of customization and granularity, which outweighs our misgivings. On the main dashboard, you'll see your net worth along with your latest transactions, spending versus the previous month, your income so far for the month and details about upcoming bills, your investments and goals you've set. There's also a link to a month-in-review page, which offers an in-depth overview of what's been happening with your money that month. You'll also be able to take a peek at how your net worth has changed over time. Monarch can connect to your bank and track Apple Card, Apple Cash and Savings accounts. It can pull in your transactions and balance history automatically and detect your recurring expenses and income. The app can even keep your car valuation up to date. While it might take a little work to set up Monarch (and you might have to tweak things here and there), it's a detailed budgeting app that can help you keep better track of your income, expenditure and net worth.This article originally appeared on Engadget at https://www.engadget.com/deals/get-50-percent-off-your-first-year-subscription-to-one-of-our-favorite-budgeting-apps-174011646.html?src=rss",
          "feed_position": 24
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/computing/accessories/best-bluetooth-tracker-140028377.html",
          "published_at": "Thu, 16 Oct 2025 09:00:36 +0000",
          "title": "The best Bluetooth trackers for 2025",
          "standfirst": "Most people think of AirTags when they picture a Bluetooth tracker. And indeed, Apple’s little white discs used to be the most capable option, relying on a vast finding network of nearby iPhones to pinpoint lost tags. But now, both Google and Samsung have implemented finding networks of their own. And other Bluetooth tracker companies, like Chipolo and Pebblebee, now have trackers that pair with either Google or Apple’s network too. In short, you’ve got a lot of options for tagging and tracking your keys, backpacks, luggage and more. So we tested all the major brands out there to see how they work and put together a guide to help you get the most out of your chosen tracker. Here are the best Bluetooth trackers you can buy. Table of contents The best Bluetooth trackers for 2025 What to look for in a Bluetooth tracking device How we tested Bluetooth trackers Other Bluetooth trackers we tested Bluetooth tracker FAQs Best Bluetooth trackers for 2025 What to look for in a Bluetooth tracking device Bluetooth trackers are small discs or cards that rely on short-range, low-energy wireless signals to communicate with your smartphone. Attach one of these gadgets your stuff and, if it’s in range, your phone can “ring” the chip so you can find it. These tracking devices offer other features like separation alerts to tell you when you’ve left a tagged item behind, or where a lost item was last detected. Some can even tap into a larger network of smartphones to track down your device when you’re out of range. Depending on what you want the tracker to do, there are a few specs to look for when deciding which to get. Device compatibility Like most things from the folks in Cupertino, AirTags only work with products in the Apple ecosystem. Both Apple and Google have opened up access to the Find My and Find Hub networks to third-party manufacturers, including Chipolo and Pebblebee. Those two companies make device-agnostic models that will work with the larger tracking network from either brand, so iPhone and Android users can buy the same tag. Tile trackers work with either Android or Apple devices, but use Tile’s own Life 360 finding network. Samsung’s latest fob, the Galaxy SmartTag2, only works with Samsung phones and taps into a finding system that relies on other Samsung devices to locate lost tags. Finding network Crowd-sourced finding capabilities are what make headlines, with stories about recovering stolen equipment or tracking lost luggage across the globe. Using anonymous signals that ping other people’s devices, these Bluetooth tracking devices can potentially tell you where a tagged item is, even if your smartphone is out of Bluetooth range. Apple’s Find My network is the largest, with over a billion iPhones and iPads in service all running Apple’s Find My app by default. So unless an iPhone user opts out, their phone silently acts as a location detector for any nearby AirTags. Apple recently increased the AirTag’s finding power by enabling you to share the location of a lost tag with a third party, party, like an airline. Chipolo fobs that work on Apple’s network have the same ability. Google launched its Find My Device network in 2024 and has since renamed it Find Hub, which, like Apple's fining app, combines devices and people finding in one place. That network is now a close second for the largest in the US Now that Google’s Find Hub network is up and running, it’s a close second for the largest in the US. Like Apple, Android users are automatically part of the network, but can opt-out by selecting the Google services option in their phone’s Settings app and toggling the option in the Find Hub menu. Samsung’s SmartTag 2 and related network also defaults to an opt-in status for finding tags and other devices. Tile offers a large finding grid that includes Tile users, Amazon Sidewalk customers and people running the Life360 network. Life360 acquired Tile in 2021, and, according to the company, the Life360 network has more than 70 million monthly active users. In our tests, AirTags and third-party tags using its network, like the Chipolo Loop and Pop and the Pebblebee Clip Universal, were the fastest to track down lost items. They offered nearly real-time location data in moderately to heavily trafficked spots around Albuquerque, including a bar, bookstore and coffee shops in Nob Hill, along with various outdoor hangouts on UNM’s campus. Samsung's SmartTags were able to locate our lost items most of the time, though not with the same precision finding accuracy as AirTags. When we tested Google’s Find Hub (then called Find My Device) network right after launch, it was noticeably slower than Apple’s network when using the community finding feature. Testing it again in 2025, the time it took to locate a lost item was considerably improved, taking less than 20 minutes on average for the community to track a fob. In our tests, Tile’s finding network wasn’t able to consistently locate its lost fobs. Amy Skorheim / Engadget Separation alerts A tracker’s day-to-day utility becomes really apparent when it prevents you from losing something in the first place. Separation alerts tell you when you’ve traveled too far from your tagged items. Useful if you want to make sure your laptop bag, jacket or umbrella always comes with you when you leave the house. Apple’s Find My app delivers these notifications, but Google’s Find Hub does not. However, if you have a Chipolo device and allow its companion app to run in the background on your Android phone, left-behind alerts are enabled. Tile trackers require a yearly subscription to enable the alerts (currently $7 to $25 monthly). Both AirTags and Tiles allow you to turn off separation alerts at certain locations, meaning you can set your home as a “safe” place where items can be left behind, but alerts will still trigger elsewhere. In our tests, AirTags and others using the Find My network alerted us between the 600- and 1,400-foot mark. Tiles sent a notification after about an average of 1,500 feet and were more consistent when using an Android phone than an iPhone. Chipolo Pop tags paired with an Android phone and using its own app sent an alert when we got around 450 feet away from our tagged item. Connectivity and volume The feature you may use most often is the key finder function, which makes the tracker ring when you hit a button in the app. With Apple's AirTags, you can say \"Hey Siri, where are my keys?\" and the assistant will ring the tag (assuming it doesn't mistakenly think you're asking for directions to the Floridian archipelago). You can also use the Find Item app in your Apple Watch to ring your fob. Asking smart home/personal assistants like Alexa or the Google Assistant to find your keys will work with Chipolo, Tile and Pebblebee trackers linked to your Android device. If you have your tag but can’t find your phone, some trackers will let you ring them to find your handset. SmartTag2 fobs reliably rang our Galaxy phone when we double-pressed it. Tile trackers have the same feature. Chipolo Pop and Loop trackers can ring your phone, but uses the Chipolo app to do so, which can run concurrently with the Find My or Find Hub connection. AirTags and third-party tags using Google’s network don’t offer this feature. The volume of the Bluetooth tracking device may determine whether you can find an item buried in your couch cushions or in a noisy room. AirTags have a reputation for being on the quiet side, and that aligned with what we saw (measuring roughly 65 decibels). Chipolo’s Pop tags and Tile’s Pro model measure between 83 and 86 decibels on average. Pebblebee’s new Clip Universal was the loudest of any tag we’ve tested, clocking in at 91 ear-splitting decibels. Design and alternative formats Design will determine what you can attach the tracker to. AirTags are small, smooth discs that can’t be secured to anything without accessories, which are numerous, but that is an additional cost to consider. Chipolo, Pebblebee and Tile offer trackers with holes that easily attach to your key ring, and all three companies also offer card-shaped versions designed to fit in your wallet. Pebblebee Clip Universal tags come with a handy carabiner-style key ring. You can even get trackers embedded into useful items like luggage locks. The SmartLock from KeySmart is a TSA-approved luggage lock, but in addition to the three digit code, it’s also a Bluetooth tracker that’s compatible with Apple Find My. It wasn’t quite as loud as other trackers in my tests, and the range wasn’t as long, but it paired easily and worked with Apple’s finding network just like an AirTag. Battery life AirTag, Tile Pro, SmartTag2 and Chipolo Pop fobs use replaceable batteries and each should go for at least a year before needing to be swapped. Tile Mate and card-shaped trackers don’t have replaceable batteries, which means you’ll have to replace the entire unit whenever it dies. Pebblebee Clip Universal Clip Universal and Chipolo Loop trackers are rechargeable via a standard USB-C port. They’re also equipped with onboard LEDs (though the light on the Loop is barely noticeable). Stalking, theft and data privacy AirTags have gotten a lot of attention and even prompted some lawsuits for Apple due to bad actors planting them on people in order to stalk them. While this fact may not influence your buying decision, any discussion of Bluetooth trackers should note what steps Apple, Google and Tile have taken to address the issue. Last year, all the major players in the Bluetooth tracker business teamed up to combat misuse and standardize how unauthorized tracking detection and alerts work for iOS and Android. Last year, Tile launched a feature called Anti-Theft Mode, which enables you to render one of its trackers undetectable by others. That means if someone steals your tagged item, they won’t be able to use the anti-stalking features to find and disable the tracker. That sort of negates one of the major ways potential stalking victims can stay safe, so Tile hopes ID verification and a $1 million penalty will deter misuse. As a theft deterrent, a Bluetooth tracker may or may not be the best option. Anecdotal stories abound in which people have recovered stolen goods using a tracker — but other tales are more cautionary. Neither Apple nor Google promotes its trackers or finding networks as a way to deal with theft. GPS trackers, on the other hand, are typically marketed for just that purpose. How we tested Bluetooth trackers Before deciding on which trackers to test, we researched the field, looking at user reviews on Amazon, Best Buy and other retailers, along with discussions on sites like Reddit. We also checked out what other publications had to say on the matter before narrowing down our options. Here’s the full list of every tracker we tested: Apple AirTag Chipolo Card Spot Chipolo One Spot Chipolo One Chipolo Card Chipolo Loop Chipolo Pop KeySmart SmartLock Motorola Moto Tag Pebblebe Clip Universal Pebblebee Clip Samsung SmartTag 2 Chipolo One Point Pebblebee Clip for Android Tile Pro (2024) Tile Mate (2024) Tile Mate (2022) Tile Pro (2022) Tile Slim (2022) After acquiring the trackers, I tested each one over the course of a few weeks using both an iPhone 11 followed by an iPhone 16 and a Samsung Galaxy S22 then an S23 Ultra. I recreated likely user experiences, such as losing and leaving items behind at home and out in the city. I planted trackers at different spots near downtown Albuquerque, mostly concentrated in and around the University of New Mexico and the surrounding neighborhood of Nob Hill. Later, I conducted tests in the Queen Anne neighborhood of Seattle. Each test was performed multiple times, both while walking and driving and I used the measure distance feature on Google Maps to track footage for alerts. I paid attention to how easy the app was to use, how reliable the phone-to-tracker connection was and any other perks and drawbacks that came up during regular use. As new trackers come to market, or as we learn of worthy models to try, I'll test them and add the results to this guide. Other Bluetooth trackers we tested Motorola Moto Tag The Moto Tag haunts me. At this very moment, my Galaxy phone says the fob is “Near you right now.” But I don’t know where. I tap to play a sound and the Find Hub tries, but ultimately says it can’t. I tap the Find Nearby function that’s supposed to visually guide you to the tag. I parade my phone around the house like a divining rod, take it down into the basement, walk it all over the garage. Nothing. But the Hub app unendingly says the Moto Tag is “Near you right now” and I get flashes of every old-school horror movie where the telephone operator tells the soon-to-be victim that the call is coming from inside the house. It’s partly my fault. I tend to keep good tabs on the gadgets I test for work. But during my most recent move, the tiny green disc didn’t make it into the safety of my review unit cabinet after relocation. Perhaps in retribution for my neglect, the Moto Tag keeps itself just out of reach. Taunting me. I’ll let you know if I ever find it, but in the meantime, it’s clear this finding device doesn’t want to be found. The recommended tags in this guide will serve you better. Tile Pro and Tile Mate (2024) Tile recently came out with a new suite of trackers, replacing the Tile Mate, Tile Pro, Tile Sticker and Tile Slim with updated models. In addition to fun new colors for the Mate and Slim, Tile added an SOS feature that can send a notification to your Life360 Circle when you triple press the button on the tracker. It’s a clever addition that turns your keys into a panic button, something offered by personal safety companies as standalone devices. There are a few caveats: You and the people you want to notify in an emergency will need the Life360 app installed on your phones. If you want your Tile to also trigger a call to emergency services, you’ll need a $15-per-month Life360 subscription (that’s in addition to a Tile membership, which starts at $3/month or $30 annually). And enabling the SOS triple-press disables the ability to ring your phone with the fob. I tested the SOS feature and it did indeed send a text message to my Circle, with the message that I had triggered an SOS and a link to a website that showed my current location. I thought it odd that the link didn’t open the Life360 app (which shows the location of users' phones), but I wasn’t as much concerned with Tile’s personal safety features as I was with the tracking capabilities, which turned out to be less than ideal. For my tests, I planted Tile trackers in a densely populated area of Seattle (about 15,000 people per square mile). After setting the trackers to “lost” in the Tile app, I waited. After four hours, one of the trackers was not discovered by the finding community, so I went and retrieved it. Another fob I planted alerted me that the tracker had been found by the Tile community after three hours — but the location it gave me was off by a third of a mile. I then decided to plant a tracker in the busiest place I could think of — the dried fruit and nuts aisle of a Trader Joes on a Friday evening before a major holiday. It still took over a half an hour before another Tile user anonymously pinged my lost tracker. In my tests with Samsung’s trackers and the fobs on Google’s Find Hub network, it took around ten minutes for them to be discovered. AirTags took half that time and all were tested in a far less populated city. Four hours with no ping and over a half hour before getting a hit in a crowded TJs were pretty long stretches. Tile devices work with both mobile operating systems and its latest models are indeed louder than they were before. But they aren’t as quick to connect and you need to pay for a membership to activate left-behind alerts. And when you do, those notifications don’t kick in as quickly as they do with competing trackers. Bluetooth tracker FAQs Which Bluetooth tracker has the longest range? Both the Tile Pro and the Samsung Galaxy SmartTag2 claim a maximum range of around 400 feet, which is longer than the 300-foot claim for Chipolo’s Pop tags. The Pebblebee Clip Universal claims a 500-foot range, though other trackers with a shorter claimed range performed better in our tests. Apple doesn’t make range claims for AirTags. Any Bluetooth signal, of course, is dependent on a few factors. Obstacles like walls and people can block the signal, so a clear line of sight is the only way to achieve the maximum range. Other signals, like Wi-Fi, can also interfere with Bluetooth connections. Even high humidity can have an effect and lessen the distance at which your phone will connect to your tracker. Remember, when considering the range of Bluetooth trackers, the size of the “finding network” also comes into play. This is the number of nearby phones that can be used to anonymously ping your tracker when your own phone is out of Bluetooth range. As of now, Apple AirTags have the largest network, followed by Google’s Find Hub, Samsung’s finding community and Tile’s Life360 members. What is the best Bluetooth tracker for a car? Bluetooth trackers are designed to track small, personal items like keys, jackets, backpacks and the like. All trackers have safeguards to prohibit the tag from being used to stalk people, so most will alert someone if a tracker that does not belong to them is detected following them. That means a car thief may get tipped off that there’s a tracker in the car they’re trying to steal. That said, you’ll see plenty of stories about people finding their car thanks to a Bluetooth tracker. Some police departments have even handed out trackers to combat high rates of carjacking. In most instances, the tracker of choice has been AirTags thanks to their wide finding network. If you’re looking for a tracker for your car, you may want to look into GPS trackers, some of which are designed for just that purpose. How accurate are Bluetooth trackers? Accuracy for Bluetooth trackers can be looked at in two ways: Finding items nearby and finding items misplaced outside your home. For nearby items, you’ll most often use the ring function on the device to hunt it down. Apple’s AirTags also use ultra-wideband technology, which creates directional navigation on your phone to get you within a foot of the tracker. Accurately finding lost items outside your home depends on the size of the finding network. Since this relies on the serendipity of a random phone passing within Bluetooth range of your tracker, the more phones on a given network, the better. And since Bluetooth ranges and distance estimates are only precise within about a meter or so, getting pings from more than one phone will help locating items. Here again, it’s worth noting that Apple’s Find My network is the largest, followed by Google, Samsung and Tile (both Chipolo and Pebblebee have fobs that work with the Apple and Google networks). Recent Updates October 2025: Added Chipolo Loop as a new pick for best rechargeable Bluetooth tracker. Detailed our experience with the Moto Tag and KeySmart SmartLock. Updated details about separation alerts and Ultra Wideband tech. August 2025: Updated the name of Google's finding network to Find Hub, instead of Find My Device. Added details about Pebblebee's new Alert feature. Added a table of contents. This article originally appeared on Engadget at https://www.engadget.com/computing/accessories/best-bluetooth-tracker-140028377.html?src=rss",
          "content": "Most people think of AirTags when they picture a Bluetooth tracker. And indeed, Apple’s little white discs used to be the most capable option, relying on a vast finding network of nearby iPhones to pinpoint lost tags. But now, both Google and Samsung have implemented finding networks of their own. And other Bluetooth tracker companies, like Chipolo and Pebblebee, now have trackers that pair with either Google or Apple’s network too. In short, you’ve got a lot of options for tagging and tracking your keys, backpacks, luggage and more. So we tested all the major brands out there to see how they work and put together a guide to help you get the most out of your chosen tracker. Here are the best Bluetooth trackers you can buy. Table of contents The best Bluetooth trackers for 2025 What to look for in a Bluetooth tracking device How we tested Bluetooth trackers Other Bluetooth trackers we tested Bluetooth tracker FAQs Best Bluetooth trackers for 2025 What to look for in a Bluetooth tracking device Bluetooth trackers are small discs or cards that rely on short-range, low-energy wireless signals to communicate with your smartphone. Attach one of these gadgets your stuff and, if it’s in range, your phone can “ring” the chip so you can find it. These tracking devices offer other features like separation alerts to tell you when you’ve left a tagged item behind, or where a lost item was last detected. Some can even tap into a larger network of smartphones to track down your device when you’re out of range. Depending on what you want the tracker to do, there are a few specs to look for when deciding which to get. Device compatibility Like most things from the folks in Cupertino, AirTags only work with products in the Apple ecosystem. Both Apple and Google have opened up access to the Find My and Find Hub networks to third-party manufacturers, including Chipolo and Pebblebee. Those two companies make device-agnostic models that will work with the larger tracking network from either brand, so iPhone and Android users can buy the same tag. Tile trackers work with either Android or Apple devices, but use Tile’s own Life 360 finding network. Samsung’s latest fob, the Galaxy SmartTag2, only works with Samsung phones and taps into a finding system that relies on other Samsung devices to locate lost tags. Finding network Crowd-sourced finding capabilities are what make headlines, with stories about recovering stolen equipment or tracking lost luggage across the globe. Using anonymous signals that ping other people’s devices, these Bluetooth tracking devices can potentially tell you where a tagged item is, even if your smartphone is out of Bluetooth range. Apple’s Find My network is the largest, with over a billion iPhones and iPads in service all running Apple’s Find My app by default. So unless an iPhone user opts out, their phone silently acts as a location detector for any nearby AirTags. Apple recently increased the AirTag’s finding power by enabling you to share the location of a lost tag with a third party, party, like an airline. Chipolo fobs that work on Apple’s network have the same ability. Google launched its Find My Device network in 2024 and has since renamed it Find Hub, which, like Apple's fining app, combines devices and people finding in one place. That network is now a close second for the largest in the US Now that Google’s Find Hub network is up and running, it’s a close second for the largest in the US. Like Apple, Android users are automatically part of the network, but can opt-out by selecting the Google services option in their phone’s Settings app and toggling the option in the Find Hub menu. Samsung’s SmartTag 2 and related network also defaults to an opt-in status for finding tags and other devices. Tile offers a large finding grid that includes Tile users, Amazon Sidewalk customers and people running the Life360 network. Life360 acquired Tile in 2021, and, according to the company, the Life360 network has more than 70 million monthly active users. In our tests, AirTags and third-party tags using its network, like the Chipolo Loop and Pop and the Pebblebee Clip Universal, were the fastest to track down lost items. They offered nearly real-time location data in moderately to heavily trafficked spots around Albuquerque, including a bar, bookstore and coffee shops in Nob Hill, along with various outdoor hangouts on UNM’s campus. Samsung's SmartTags were able to locate our lost items most of the time, though not with the same precision finding accuracy as AirTags. When we tested Google’s Find Hub (then called Find My Device) network right after launch, it was noticeably slower than Apple’s network when using the community finding feature. Testing it again in 2025, the time it took to locate a lost item was considerably improved, taking less than 20 minutes on average for the community to track a fob. In our tests, Tile’s finding network wasn’t able to consistently locate its lost fobs. Amy Skorheim / Engadget Separation alerts A tracker’s day-to-day utility becomes really apparent when it prevents you from losing something in the first place. Separation alerts tell you when you’ve traveled too far from your tagged items. Useful if you want to make sure your laptop bag, jacket or umbrella always comes with you when you leave the house. Apple’s Find My app delivers these notifications, but Google’s Find Hub does not. However, if you have a Chipolo device and allow its companion app to run in the background on your Android phone, left-behind alerts are enabled. Tile trackers require a yearly subscription to enable the alerts (currently $7 to $25 monthly). Both AirTags and Tiles allow you to turn off separation alerts at certain locations, meaning you can set your home as a “safe” place where items can be left behind, but alerts will still trigger elsewhere. In our tests, AirTags and others using the Find My network alerted us between the 600- and 1,400-foot mark. Tiles sent a notification after about an average of 1,500 feet and were more consistent when using an Android phone than an iPhone. Chipolo Pop tags paired with an Android phone and using its own app sent an alert when we got around 450 feet away from our tagged item. Connectivity and volume The feature you may use most often is the key finder function, which makes the tracker ring when you hit a button in the app. With Apple's AirTags, you can say \"Hey Siri, where are my keys?\" and the assistant will ring the tag (assuming it doesn't mistakenly think you're asking for directions to the Floridian archipelago). You can also use the Find Item app in your Apple Watch to ring your fob. Asking smart home/personal assistants like Alexa or the Google Assistant to find your keys will work with Chipolo, Tile and Pebblebee trackers linked to your Android device. If you have your tag but can’t find your phone, some trackers will let you ring them to find your handset. SmartTag2 fobs reliably rang our Galaxy phone when we double-pressed it. Tile trackers have the same feature. Chipolo Pop and Loop trackers can ring your phone, but uses the Chipolo app to do so, which can run concurrently with the Find My or Find Hub connection. AirTags and third-party tags using Google’s network don’t offer this feature. The volume of the Bluetooth tracking device may determine whether you can find an item buried in your couch cushions or in a noisy room. AirTags have a reputation for being on the quiet side, and that aligned with what we saw (measuring roughly 65 decibels). Chipolo’s Pop tags and Tile’s Pro model measure between 83 and 86 decibels on average. Pebblebee’s new Clip Universal was the loudest of any tag we’ve tested, clocking in at 91 ear-splitting decibels. Design and alternative formats Design will determine what you can attach the tracker to. AirTags are small, smooth discs that can’t be secured to anything without accessories, which are numerous, but that is an additional cost to consider. Chipolo, Pebblebee and Tile offer trackers with holes that easily attach to your key ring, and all three companies also offer card-shaped versions designed to fit in your wallet. Pebblebee Clip Universal tags come with a handy carabiner-style key ring. You can even get trackers embedded into useful items like luggage locks. The SmartLock from KeySmart is a TSA-approved luggage lock, but in addition to the three digit code, it’s also a Bluetooth tracker that’s compatible with Apple Find My. It wasn’t quite as loud as other trackers in my tests, and the range wasn’t as long, but it paired easily and worked with Apple’s finding network just like an AirTag. Battery life AirTag, Tile Pro, SmartTag2 and Chipolo Pop fobs use replaceable batteries and each should go for at least a year before needing to be swapped. Tile Mate and card-shaped trackers don’t have replaceable batteries, which means you’ll have to replace the entire unit whenever it dies. Pebblebee Clip Universal Clip Universal and Chipolo Loop trackers are rechargeable via a standard USB-C port. They’re also equipped with onboard LEDs (though the light on the Loop is barely noticeable). Stalking, theft and data privacy AirTags have gotten a lot of attention and even prompted some lawsuits for Apple due to bad actors planting them on people in order to stalk them. While this fact may not influence your buying decision, any discussion of Bluetooth trackers should note what steps Apple, Google and Tile have taken to address the issue. Last year, all the major players in the Bluetooth tracker business teamed up to combat misuse and standardize how unauthorized tracking detection and alerts work for iOS and Android. Last year, Tile launched a feature called Anti-Theft Mode, which enables you to render one of its trackers undetectable by others. That means if someone steals your tagged item, they won’t be able to use the anti-stalking features to find and disable the tracker. That sort of negates one of the major ways potential stalking victims can stay safe, so Tile hopes ID verification and a $1 million penalty will deter misuse. As a theft deterrent, a Bluetooth tracker may or may not be the best option. Anecdotal stories abound in which people have recovered stolen goods using a tracker — but other tales are more cautionary. Neither Apple nor Google promotes its trackers or finding networks as a way to deal with theft. GPS trackers, on the other hand, are typically marketed for just that purpose. How we tested Bluetooth trackers Before deciding on which trackers to test, we researched the field, looking at user reviews on Amazon, Best Buy and other retailers, along with discussions on sites like Reddit. We also checked out what other publications had to say on the matter before narrowing down our options. Here’s the full list of every tracker we tested: Apple AirTag Chipolo Card Spot Chipolo One Spot Chipolo One Chipolo Card Chipolo Loop Chipolo Pop KeySmart SmartLock Motorola Moto Tag Pebblebe Clip Universal Pebblebee Clip Samsung SmartTag 2 Chipolo One Point Pebblebee Clip for Android Tile Pro (2024) Tile Mate (2024) Tile Mate (2022) Tile Pro (2022) Tile Slim (2022) After acquiring the trackers, I tested each one over the course of a few weeks using both an iPhone 11 followed by an iPhone 16 and a Samsung Galaxy S22 then an S23 Ultra. I recreated likely user experiences, such as losing and leaving items behind at home and out in the city. I planted trackers at different spots near downtown Albuquerque, mostly concentrated in and around the University of New Mexico and the surrounding neighborhood of Nob Hill. Later, I conducted tests in the Queen Anne neighborhood of Seattle. Each test was performed multiple times, both while walking and driving and I used the measure distance feature on Google Maps to track footage for alerts. I paid attention to how easy the app was to use, how reliable the phone-to-tracker connection was and any other perks and drawbacks that came up during regular use. As new trackers come to market, or as we learn of worthy models to try, I'll test them and add the results to this guide. Other Bluetooth trackers we tested Motorola Moto Tag The Moto Tag haunts me. At this very moment, my Galaxy phone says the fob is “Near you right now.” But I don’t know where. I tap to play a sound and the Find Hub tries, but ultimately says it can’t. I tap the Find Nearby function that’s supposed to visually guide you to the tag. I parade my phone around the house like a divining rod, take it down into the basement, walk it all over the garage. Nothing. But the Hub app unendingly says the Moto Tag is “Near you right now” and I get flashes of every old-school horror movie where the telephone operator tells the soon-to-be victim that the call is coming from inside the house. It’s partly my fault. I tend to keep good tabs on the gadgets I test for work. But during my most recent move, the tiny green disc didn’t make it into the safety of my review unit cabinet after relocation. Perhaps in retribution for my neglect, the Moto Tag keeps itself just out of reach. Taunting me. I’ll let you know if I ever find it, but in the meantime, it’s clear this finding device doesn’t want to be found. The recommended tags in this guide will serve you better. Tile Pro and Tile Mate (2024) Tile recently came out with a new suite of trackers, replacing the Tile Mate, Tile Pro, Tile Sticker and Tile Slim with updated models. In addition to fun new colors for the Mate and Slim, Tile added an SOS feature that can send a notification to your Life360 Circle when you triple press the button on the tracker. It’s a clever addition that turns your keys into a panic button, something offered by personal safety companies as standalone devices. There are a few caveats: You and the people you want to notify in an emergency will need the Life360 app installed on your phones. If you want your Tile to also trigger a call to emergency services, you’ll need a $15-per-month Life360 subscription (that’s in addition to a Tile membership, which starts at $3/month or $30 annually). And enabling the SOS triple-press disables the ability to ring your phone with the fob. I tested the SOS feature and it did indeed send a text message to my Circle, with the message that I had triggered an SOS and a link to a website that showed my current location. I thought it odd that the link didn’t open the Life360 app (which shows the location of users' phones), but I wasn’t as much concerned with Tile’s personal safety features as I was with the tracking capabilities, which turned out to be less than ideal. For my tests, I planted Tile trackers in a densely populated area of Seattle (about 15,000 people per square mile). After setting the trackers to “lost” in the Tile app, I waited. After four hours, one of the trackers was not discovered by the finding community, so I went and retrieved it. Another fob I planted alerted me that the tracker had been found by the Tile community after three hours — but the location it gave me was off by a third of a mile. I then decided to plant a tracker in the busiest place I could think of — the dried fruit and nuts aisle of a Trader Joes on a Friday evening before a major holiday. It still took over a half an hour before another Tile user anonymously pinged my lost tracker. In my tests with Samsung’s trackers and the fobs on Google’s Find Hub network, it took around ten minutes for them to be discovered. AirTags took half that time and all were tested in a far less populated city. Four hours with no ping and over a half hour before getting a hit in a crowded TJs were pretty long stretches. Tile devices work with both mobile operating systems and its latest models are indeed louder than they were before. But they aren’t as quick to connect and you need to pay for a membership to activate left-behind alerts. And when you do, those notifications don’t kick in as quickly as they do with competing trackers. Bluetooth tracker FAQs Which Bluetooth tracker has the longest range? Both the Tile Pro and the Samsung Galaxy SmartTag2 claim a maximum range of around 400 feet, which is longer than the 300-foot claim for Chipolo’s Pop tags. The Pebblebee Clip Universal claims a 500-foot range, though other trackers with a shorter claimed range performed better in our tests. Apple doesn’t make range claims for AirTags. Any Bluetooth signal, of course, is dependent on a few factors. Obstacles like walls and people can block the signal, so a clear line of sight is the only way to achieve the maximum range. Other signals, like Wi-Fi, can also interfere with Bluetooth connections. Even high humidity can have an effect and lessen the distance at which your phone will connect to your tracker. Remember, when considering the range of Bluetooth trackers, the size of the “finding network” also comes into play. This is the number of nearby phones that can be used to anonymously ping your tracker when your own phone is out of Bluetooth range. As of now, Apple AirTags have the largest network, followed by Google’s Find Hub, Samsung’s finding community and Tile’s Life360 members. What is the best Bluetooth tracker for a car? Bluetooth trackers are designed to track small, personal items like keys, jackets, backpacks and the like. All trackers have safeguards to prohibit the tag from being used to stalk people, so most will alert someone if a tracker that does not belong to them is detected following them. That means a car thief may get tipped off that there’s a tracker in the car they’re trying to steal. That said, you’ll see plenty of stories about people finding their car thanks to a Bluetooth tracker. Some police departments have even handed out trackers to combat high rates of carjacking. In most instances, the tracker of choice has been AirTags thanks to their wide finding network. If you’re looking for a tracker for your car, you may want to look into GPS trackers, some of which are designed for just that purpose. How accurate are Bluetooth trackers? Accuracy for Bluetooth trackers can be looked at in two ways: Finding items nearby and finding items misplaced outside your home. For nearby items, you’ll most often use the ring function on the device to hunt it down. Apple’s AirTags also use ultra-wideband technology, which creates directional navigation on your phone to get you within a foot of the tracker. Accurately finding lost items outside your home depends on the size of the finding network. Since this relies on the serendipity of a random phone passing within Bluetooth range of your tracker, the more phones on a given network, the better. And since Bluetooth ranges and distance estimates are only precise within about a meter or so, getting pings from more than one phone will help locating items. Here again, it’s worth noting that Apple’s Find My network is the largest, followed by Google, Samsung and Tile (both Chipolo and Pebblebee have fobs that work with the Apple and Google networks). Recent Updates October 2025: Added Chipolo Loop as a new pick for best rechargeable Bluetooth tracker. Detailed our experience with the Moto Tag and KeySmart SmartLock. Updated details about separation alerts and Ultra Wideband tech. August 2025: Updated the name of Google's finding network to Find Hub, instead of Find My Device. Added details about Pebblebee's new Alert feature. Added a table of contents. This article originally appeared on Engadget at https://www.engadget.com/computing/accessories/best-bluetooth-tracker-140028377.html?src=rss",
          "feed_position": 25,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2023-02/94489620-abc7-11ed-b375-842957054bf1"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/entertainment/streaming/best-live-tv-streaming-service-133000410.html",
          "published_at": "Thu, 16 Oct 2025 07:00:36 +0000",
          "title": "The best live TV streaming services to cut cable in 2025",
          "standfirst": "We still think getting a live TV streaming service is a better deal than paying for cable — but the gulf between the two options is narrowing. Now that many of the major providers go for more than $80 per month, it’s not the amazing deal it once was. Still, live TV streaming plans have no contract, offer relatively simpler pricing and need no special equipment beyond a smart TV and an internet connection.There are a handful of major players and, after testing them all, we found YouTube TV to be the most well-rounded. But there are a lot of changes happening in the world of live TV streaming, with new services from ESPN and Fox, as well as the upcoming consolidation of Hulu into Disney. We’re testing out the new services and we’ll let you know how the new options affect your options. For now, these are the best live TV streaming services we tested, along with our breakdowns of how to stream and what you get for your money.Editor's note: There’s been a lot of hubbub in the streaming world lately. YouTube TV subscribers nearly missed out on access to NBC Universal channels — but the companies ultimately came to a multi-year agreement. Disney (and by extension Hulu+ Live TV) lost a whole bunch of subscribers over free speech dissent — and then announced a price hike for Hulu+ Live TV, bringing it to $90 monthly. Philo implemented a $5 monthly price increase, but users will now get to watch HBO Max and Discovery+, so it’s not all bad. Speaking of HBO Max, customers will no longer get CNN’s livestream as of November 17 because the latter entity is going to try yet again to offer its own service. Table of contents Best live TV streaming services for 2025 How to stream live NFL games Best free live TV streaming services for 2025 What to look for in a live TV streaming service How we tested Live TV Streaming FAQs Recent updates Best live TV streaming services for 2025 Back to top How to stream live NFL games The rights to air regular-season NFL games belong to a number of networks. Around 200 games are scheduled to appear Sundays on CBS/Paramount+ and Fox/Fox One. NBC/Peacock will host one Sunday night competition each week while Prime Video will air Thursday night contests (except for Thanksgiving week) and ABC/ESPN will show Monday night matchups. A few games will be exclusive to the NFL Network and Christmas-day games will air live on Netflix. YouTube aired a single week-one game. You can see the complete 2025 NFL schedule here (the airing network appears just below the game time on the list). On many Sundays, multiple games are scheduled to air at the same time by the same broadcaster. That means Fox and CBS will broadcast regional games through the associated local affiliate station. Select national games will air through Fox One and Paramount+. To see all Sunday (daytime) matchups, you’ll need the NFL Sunday Ticket that’s now exclusive to YouTube TV and costs between $35 and $115 per month depending on the type of subscription you choose (YouTube recently announced monthly options for the Sunday Ticket). Note that the subscription doesn’t include Sunday night games — for that, you’ll need Peacock and/or local NBC station access through YouTube TV or elsewhere. Most of the paid live TV streaming services we recommend here include the stations you’ll need to see most of the games. YouTube TV, Fubo TV (including the new, cheaper Fubo Sports package), Hulu + Live TV and DirecTV (Signature packages and MySports Genre packs) offer local Fox, CBS, ABC and NBC stations in most (but not all areas). They also carry sports-focused channels from those networks, like Fox Sports, CBS Sports and ESPN. Sling’s Orange plan includes access to a few local channels (varying by area), and also carries ESPN, but you’ll need the combined Orange and Blue plan to also get the Fox Sports channel — but neither plan carries CBS Sports. How can I stream NFL games for free? If you have a digital antenna hooked up to your TV, you can grab games that are broadcast over the airways for your region by tuning into your local CBS, Fox, NBC and ABC stations. You can buy a digital antenna for between $20 and $60. Of course, that won’t get you the games that are exclusive to the NFL Network, Prime Video or Netflix, and you won’t be able to watch games broadcast outside your area. Nearly all paid live TV streaming services are currently offering free trials ranging from a few days to a week. You could hop from service to service, catching a few games before cancelling and not pay anything, but with 18 weeks in the regular season, you’ll obviously not be able to watch all games for free. Alternatively, you can check out your local sports bar and watch a game for the price of a soda and maybe some nachos. As it turns out, bars and restaurants that provide those games to customers have to pay a ton of cash to do so, so you may as well take advantage of the opportunity. Does Paramount Plus stream live NFL games? Yes. Paramount owns CBS, which has historically held the rights to air many NFL games each season. This year, NFL on CBS includes more than 100 regular-season games, most of them Sunday matchups. You can see which NFL games will air on CBS/Paramount + here. Note that to watch your local CBS station you need Paramount+ Premium (formerly Paramount+ with Showtime) for $13 per month. Can you stream live football on YouTube? September 5, 2025 marked the first time YouTube was an official live NFL broadcaster when it aired a Friday night, week-one game of the 2025 NFL season from São Paulo, Brazil. It pit the Los Angeles Chargers against the Kansas City Chiefs (LA won 21-27) and aired worldwide on YouTube for free as well as for subscribers to YouTube TV. There are no other plans for YouTube to air live NFL games for the 2025 season for free, but paid YouTube TV customers will be able to watch many live matchups on their local CBS, Fox, NBC and ABC stations as part of their subscription. Both YouTube TV subscribers and anyone with the YouTube app can subscribe to the NFL Sunday Ticket add-on for $35 to $60 monthly, depending on promotions. Through the YouTube app, you can also purchase access to other Primetime Channels including Paramount+, but it costs the same as paying for those accounts directly. Best free live TV streaming services for 2025 There are loads of ways to get free TV these days. To start, many standard streaming apps have added live components to their lineups — even Netflix. Peacock Premium Plus subscriptions include regional NBC stations. Paramount+ Premium subscribers can watch on-air CBS programming. The new Fox One service includes multiple live Fox stations. True, if you’re already paying for a service it’s not technically “free” but at least the live content isn’t extra. The smart TV operating system (OS) you use likely provides free live content too: Amazon’s Fire TV, Google/Android TV, Roku’s built-in Roku Channel and Samsung’s TV Plus all have hundreds of live channels and original programming. Some of the paid services we recommend above have a free version — namely Sling Freestream, Fubo Free (available after you cancel) and DirecTV’s MyFree. But if you’re looking for more, here are the best free ad-supported TV (FAST) apps with live TV that we tried: Back to top What to look for in a live TV streaming service How to stream live TV Streaming live TV is a lot like using Netflix. You get access through apps on your phone, tablet, smart TV or streaming device and the signal arrives over the internet. A faster and more stable connection tends to give you a better experience. Most live TV apps require you to sign up and pay via a web browser. After that, you can activate the app on all of your devices. Monthly Price When I started testing these cord-cutting alternatives, I was struck by the price difference between live TV and a standard video streaming app. Where the latter cost between $5 and $20 per month, most live TV services hit the $80 mark and can go higher than $200 with additional perks, channel packages and premium extras. The higher starting price is mostly due to the cost of providing multiple networks — particularly sports and local stations. And, in the past year or so, every service has raised base plan prices. Local channels Only two of the services I tried don’t include full local channel coverage for subscribers and one of those makes no effort to carry sports at all. That would be Philo and, as you might guess, it’s the cheapest. The next most affordable option, Sling, only carries three local stations — and only in larger markets — but it still manages to include some of the top sports channels. When you sign up with any provider that handles local TV, you’ll enter your zip code, ensuring you get your area’s broadcast affiliates for ABC, CBS, FOX and NBC. Of course, you can also get those stations for free. Nearly all modern television sets support a radio frequency (RF) connection, also known as the coaxial port, which means if you buy an HD antenna, you’ll receive locally broadcast stations like ABC, CBS, PBS, FOX and NBC. And since the signal is digital, reception is much improved over the staticky rabbit-ears era. But local channel access is another area where traditional streaming services, like Netflix, are bleeding into broadcast territory. For example, you can watch your local NBC station with a Peacock subscription and you can tune into your area’s CBS station through your Paramount+ subscription. Netflix is even getting into the mix with a recently announced deal with one of France’s broadcast companies, TF1. The streaming service will now air TF1's live TV channels and on-demand content inside the Netflix app. No word if the concept will expand to other regions, but it’s an interesting move to anyone interested in the future of streaming. Live sports coverage One reality that spun my head was the sheer number and iterations of sports networks in existence. Trying to figure out which network will carry the match-up you want to see can be tricky. I found that Google makes it a little easier for sports fans by listing out upcoming games (just swap in NBA, NFL, MLB, NHL and so on in the search bar). When you click an event, the “TV & streaming” button will tell you which network is covering it. That just leaves figuring out if your chosen service carries the RSNs (regional sports networks) you want. Unfortunately, even with add-ons and extra packages, some providers simply don’t have certain channels in their lineups. It would take a lawyer to understand the ins and outs of streaming rights negotiations, and networks leave and return to live TV carriers all the time. That said, most major sporting events in the US are covered by ESPN, Fox Sports, TNT, USA and local affiliates. I should also point out that traditional streaming services have started adding live sports to their lineups. Peacock carries live Premier League matches, Sunday Night Football games and aired the 2024 Olympic Games from Paris. Thursday Night Football as well as NBA and WNBA games are on Amazon Prime and Christmas Day Football airs on Netflix. HBO Max (formerly, er, HBO Max) now airs select, regular season games from the NHL, MLB, NCAA and NBA with a $10-per-month add-on. You can watch MLS games with an add-on through the Apple TV app, and Apple TV+ (now just called Apple TV) includes some MLB games. Roku users can watch the just-added free sports channel and those who subscribe to Paramount Plus can see many of the matches aired on CBS Sports, including live NFL games. In 2025, January's Super Bowl was live-streamed for free on Tubi. While all of these alternatives may not cover as much ground as live TV streamers, they could end up being cheaper avenues to the sports you want. And if sports is all you’re after, there are sports-only plans that are a touch cheaper, too. The promised sports streaming service from ESPN, Fox and Warner Bros. called Venu was cancelled early this year. But on August 21, ESPN launched its own streaming service that includes all ESPN channels and costs $30 per month. Fubo Sports is $56 monthly and includes local broadcast stations from ABC, CBS and FOX plus a slew of sports networks (CBS Sport and FS1 among them) as well as all networks included with ESPN Unlimited. Fox launched its own standalone service in August as well and it includes Fox Sports and all other Fox properties (News, Business, Weather) for $20 monthly. DirecTV also has a $70-per-month, sports-only streaming package called MySports and Comcast has a sports and news bundle for that same price (as long as you're an Xfinity customer with auto-pay, otherwise it's more expensive). Traditional cable networks Dozens of linear programming networks were once only available with cable TV, like Bravo, BET, Food Network, HGTV, CNN, Lifetime, SYFY and MTV. If you only subscribe to, say, Netflix or Apple TV+, you won’t have access to those. But as with sports, standard streamers are starting to incorporate this content into their offerings. After the Warner Bros. merger, Max incorporated some content from HGTV, Discovery and TLC. Peacock has Bravo and Hallmark shows, and Paramount+ has material from Nickelodeon, MTV and Comedy Central. Other entertainment channels like AMC+ have stand-alone apps. The Discovery+ app gives you 15 channels ad-free for $10 per month (or with ads for $6 monthly). And a service called Frndly TV starts at a mere $7 per month and streams A&E, Lifetime, Game Show Network, Outdoor Channel and about 35 others. Of course, most live TV streaming options will deliver more sizable lists of cable networks, but just note that you may already be paying for some of them — and if all you need is a certain channel, you could get it cheaper by subscribing directly. On-demand streaming Most live TV subscriptions include access to a selection of video-on-demand (VOD) content, like you would get with a traditional streaming service. Much of this content is made up of the movies and TV series that have recently aired on your subscribed networks. This typically doesn’t cover live events and news programming, but I was able to watch specific episodes of ongoing shows like Top Chef or BET’s Diarra from Detroit. Just search the on-demand library for the program, pick an episode and hit play. Partnerships, like Hulu’s relationship with Disney, and add-ons, such as bundling Max with your YouTube TV subscription or Starz with your Sling plan, will let you watch even larger libraries of on-demand content. But again, if VOD is all you’re after, paying for those networks directly instead of through a live TV plan will be far cheaper. Digital video recordings (DVR) limits Every option I tried offers some cloud DVR storage without needing a separate physical device. You’ll either get unlimited storage for recordings that expires after nine months or a year, or you’ll get a set number of hours (between 50 and 1,000) that you can keep indefinitely. Typically, all you need to do is designate what ongoing TV series you want to record and the DVR component will do all the hard work of saving subsequent episodes for you to watch later. You can do the same thing with sports events. Aside from being able to watch whenever it’s most convenient, you can also fast-forward through commercials in recorded content. In contrast, you can’t skip them on live TV or VOD. Simultaneous streams and profiles per account Each plan gives you a certain number of simultaneous streams, aka how many screens can play content at the same time. And while most providers will let you travel with your subscription, there are usually location restrictions that require you to sign in from your home IP address periodically. Stream allowances range from one at a time to unlimited screens (or as many as your ISP’s bandwidth can handle). Some plans require add-ons to get more screens. Most services also let you set up a few profiles so I was able to give different people in my family the ability to build their own watch histories and libraries, set their favorite channels and get individual recommendations. Picture-in-picture mode and multiview Picture-in-picture (PiP) usually refers to shrinking a video window on a mobile device or computer browser so you can watch it while using other apps. Sling, YouTube TV, FuboTV, Philo, DirecTV Stream and Hulu + Live TV all have PiP modes on computers and mobile devices. Another feature, multiview, lets you view multiple (usually four) sports matches or other live content at once on your TV screen. YouTube TV, FuboTV and now DirecTV all let you do this. With YouTube TV, you can select up to four views from a few preset selection of streams. FuboTV offers the same feature, but only if you're using an Apple TV or Roku streaming device. DirecTV lets you do so through “mixes” which include sports, news, business and kids variants with a set four channels in each mix. 4K live streams Right now, just FuboTV, YouTube TV and DirecTV Stream offer 4K live streams — but with caveats. YouTube TV requires a $20-per-month add-on, after which you’ll only be able to watch certain live content in 4K. DirecTV Stream has three channels that show live 4K content — one with shows and original series, and two with occasional sporting events. You don’t have to pay extra for these but you do need to have either DirecTV’s Gemini receiver, or a device from Fire TV, Apple TV or Roku. You’ll need those same streaming devices to watch the select 4K programming on Sling as well. FuboTV shows certain live events in 4K but access is limited to the Elite and Premier packages, not the base-level Pro plan. Of course, watching any 4K content also requires equipment that can handle it: a 4K smart TV or 4K streaming device paired with a cord and screen that can handle 4K resolution. Tiers, packages and add-ons Comparing price-to-offering ratios is a task for a spreadsheet. I… made three. The base plans range from $28 to $85 per month. From there, you can add packages, which are usually groups of live TV channels bundled by themes like news, sports, entertainment or international content. Premium VOD extras like Max, AMC+ and Starz are also available. Add-ons cost an extra $5 to $20 each per month and simply show up in the guide where you find the rest of your live TV. This is where streaming can quickly get expensive, pushing an $80 subscription to $200 monthly, depending on what you choose. How to stream live TV for free I also downloaded and tried out a few apps that offer free ad-supported TV (FAST) including Freevee, Tubi, PlutoTV and Sling Freestream. These let you drop in and watch a more limited selection of live networks at zero cost. Most don’t even require an email address, let alone a credit card. And if you have a Roku device, an Amazon Fire TV or Stick, a Samsung TV, a Chromecast device or a Google TV, you already have access to hundreds of live channels via the Roku Channel, the live tab in Fire TV, through the Samsung TV Plus app or through Google TV. Back to top How we tested live TV streaming services When I begin testing for a guide, I research the most popular and well-reviewed players in the category and narrow down which are worth trying. For the paid plans, just six services dominate so I tried them all. There are considerably more free live TV contenders so I tested the four most popular. After getting accounts set up using my laptop, I downloaded the apps on a Samsung smart TV running the latest version of Tizen OS. I counted the local stations and regional sports coverage, and noted how many of the top cable networks were available. I then weighed the prices, base packages and available add-ons. I then looked at how the programming was organized in each app’s UI and judged how easy everything was to navigate, from the top navigation to the settings. To test the search function, I searched for the same few TV shows on BET, Food Network, HGTV and Comedy Central, since all six providers carry those channels. I noted how helpful the searches were and how quickly they got me to season 6, episode 13 of Home Town. I used DVR to record entire series and single movies and watched VOD shows, making sure to test the pause and scan functions. On each service with sports, I searched for the same four upcoming NHL, NBA, MLS and NCAA basketball matches and used the record option to save the games and play them back a day or two later. Finally, I noted any extra perks or irritating quirks. All live TV streaming services we’ve tested: Philo Sling YouTube TV Hulu + Live TV DirecTV Stream FuboTV Freevee Tubi PlutoTV Sling Freestream Plex Back to top Live TV Streaming FAQs What is live streaming? Streaming simply refers to video content that is delivered to your screen over the internet. Live streaming can be split into two categories: linear programming and simultaneous transmission. That first one is similar to what you get with cable or broadcast TV, with channels that play a constant flow of movies and shows (sort of what TV looked like before Netflix). Simultaneous streaming lets you watch live events (like a basketball game) or a program (like the evening news) as they happen. What is the difference between streaming and live streaming? Standard streaming, the most popular example being Netflix, lets you pick what you want to watch from a menu of choices. It’s also referred to as “video on demand.” Live streaming refers to sports and news events that you can stream as they happen in real time. It also refers to channels that show a continuous, linear flow of programming. What streaming service is best for live TV? FuboTV does the best job of letting you organize live channels to help you find just what you want to watch. The interface is uncluttered and when you search for something, the UI clearly tells you whether something is live now or on-demand. YouTube TV also does a good job making that info clear. Both have just over 100 live channels on offer. What is the most cost effective TV streaming service? Free TV streaming services like PlutoTV, Plex, Tubi and FreeVee show plenty of ad-supported TV shows and movies without charging you anything. Of course, they won’t have the same channels or content that more premium subscriptions have. Ultimately it depends on what you want to watch and finding the service that can supply that to you in the most streamlined form so you’re not paying for stuff you don’t need. Is it cheaper to have cable or streaming? A basic cable package used to be more expensive than the base-level live TV streaming service. But now that nearly all major providers have raised their prices to over $75 per month, that’s no longer the case. And with add-ons and other premiums, you can easily pay over $200 a month for either cable or a live TV streaming service. But those who want to cut the cord will appreciate that streaming services don't have contracts. What streaming service has all the TV channels? No service that we tested had every available channel. Hulu + Live TV and DirecTV Stream carry the the highest number of the top rated channels, according to Neilsen. Hulu’s service also gets you Disney+ fare, which you can’t get elsewhere. FuboTV has the most sports channels and YouTube TV gives you the widest selection of add-ons. What is the most popular live TV streaming platform? YouTube TV has the most paying customers. According to 2024's letter from the CEO, the service has over eight million subscribers. Disney’s 2024 third quarter earnings put the Hulu + Live TV viewer count at 4.6 million. Sling’s customer count dipped from two million to about 1.9 million in 2024 and FuboTV grew its subscriber list to 1.6 million. How safe are free streaming services and websites? You may have heard certain sites that provide free content can be dangerous, leading to stolen info and/or exposing you to malware. That’s likely in reference to certain peer-to-peer (P2P) networks and file-sharing sites that let people download free movies and series — which can come bundled with malicious code. But if you’re talking about the free ad-supported streaming television (FAST) services listed here, from providers like PlutoTV, Tubi and Plex, they are just as safe as any other streaming service. Since you sometimes don’t even have to provide your email address or credit card info, they can even be more anonymous for cord cutters than apps that require login credentials. Back to top Recent updates October 2025: Added information about the upcoming price increases for Hulu+ Live TV and Philo. Back to topThis article originally appeared on Engadget at https://www.engadget.com/entertainment/streaming/best-live-tv-streaming-service-133000410.html?src=rss",
          "content": "We still think getting a live TV streaming service is a better deal than paying for cable — but the gulf between the two options is narrowing. Now that many of the major providers go for more than $80 per month, it’s not the amazing deal it once was. Still, live TV streaming plans have no contract, offer relatively simpler pricing and need no special equipment beyond a smart TV and an internet connection.There are a handful of major players and, after testing them all, we found YouTube TV to be the most well-rounded. But there are a lot of changes happening in the world of live TV streaming, with new services from ESPN and Fox, as well as the upcoming consolidation of Hulu into Disney. We’re testing out the new services and we’ll let you know how the new options affect your options. For now, these are the best live TV streaming services we tested, along with our breakdowns of how to stream and what you get for your money.Editor's note: There’s been a lot of hubbub in the streaming world lately. YouTube TV subscribers nearly missed out on access to NBC Universal channels — but the companies ultimately came to a multi-year agreement. Disney (and by extension Hulu+ Live TV) lost a whole bunch of subscribers over free speech dissent — and then announced a price hike for Hulu+ Live TV, bringing it to $90 monthly. Philo implemented a $5 monthly price increase, but users will now get to watch HBO Max and Discovery+, so it’s not all bad. Speaking of HBO Max, customers will no longer get CNN’s livestream as of November 17 because the latter entity is going to try yet again to offer its own service. Table of contents Best live TV streaming services for 2025 How to stream live NFL games Best free live TV streaming services for 2025 What to look for in a live TV streaming service How we tested Live TV Streaming FAQs Recent updates Best live TV streaming services for 2025 Back to top How to stream live NFL games The rights to air regular-season NFL games belong to a number of networks. Around 200 games are scheduled to appear Sundays on CBS/Paramount+ and Fox/Fox One. NBC/Peacock will host one Sunday night competition each week while Prime Video will air Thursday night contests (except for Thanksgiving week) and ABC/ESPN will show Monday night matchups. A few games will be exclusive to the NFL Network and Christmas-day games will air live on Netflix. YouTube aired a single week-one game. You can see the complete 2025 NFL schedule here (the airing network appears just below the game time on the list). On many Sundays, multiple games are scheduled to air at the same time by the same broadcaster. That means Fox and CBS will broadcast regional games through the associated local affiliate station. Select national games will air through Fox One and Paramount+. To see all Sunday (daytime) matchups, you’ll need the NFL Sunday Ticket that’s now exclusive to YouTube TV and costs between $35 and $115 per month depending on the type of subscription you choose (YouTube recently announced monthly options for the Sunday Ticket). Note that the subscription doesn’t include Sunday night games — for that, you’ll need Peacock and/or local NBC station access through YouTube TV or elsewhere. Most of the paid live TV streaming services we recommend here include the stations you’ll need to see most of the games. YouTube TV, Fubo TV (including the new, cheaper Fubo Sports package), Hulu + Live TV and DirecTV (Signature packages and MySports Genre packs) offer local Fox, CBS, ABC and NBC stations in most (but not all areas). They also carry sports-focused channels from those networks, like Fox Sports, CBS Sports and ESPN. Sling’s Orange plan includes access to a few local channels (varying by area), and also carries ESPN, but you’ll need the combined Orange and Blue plan to also get the Fox Sports channel — but neither plan carries CBS Sports. How can I stream NFL games for free? If you have a digital antenna hooked up to your TV, you can grab games that are broadcast over the airways for your region by tuning into your local CBS, Fox, NBC and ABC stations. You can buy a digital antenna for between $20 and $60. Of course, that won’t get you the games that are exclusive to the NFL Network, Prime Video or Netflix, and you won’t be able to watch games broadcast outside your area. Nearly all paid live TV streaming services are currently offering free trials ranging from a few days to a week. You could hop from service to service, catching a few games before cancelling and not pay anything, but with 18 weeks in the regular season, you’ll obviously not be able to watch all games for free. Alternatively, you can check out your local sports bar and watch a game for the price of a soda and maybe some nachos. As it turns out, bars and restaurants that provide those games to customers have to pay a ton of cash to do so, so you may as well take advantage of the opportunity. Does Paramount Plus stream live NFL games? Yes. Paramount owns CBS, which has historically held the rights to air many NFL games each season. This year, NFL on CBS includes more than 100 regular-season games, most of them Sunday matchups. You can see which NFL games will air on CBS/Paramount + here. Note that to watch your local CBS station you need Paramount+ Premium (formerly Paramount+ with Showtime) for $13 per month. Can you stream live football on YouTube? September 5, 2025 marked the first time YouTube was an official live NFL broadcaster when it aired a Friday night, week-one game of the 2025 NFL season from São Paulo, Brazil. It pit the Los Angeles Chargers against the Kansas City Chiefs (LA won 21-27) and aired worldwide on YouTube for free as well as for subscribers to YouTube TV. There are no other plans for YouTube to air live NFL games for the 2025 season for free, but paid YouTube TV customers will be able to watch many live matchups on their local CBS, Fox, NBC and ABC stations as part of their subscription. Both YouTube TV subscribers and anyone with the YouTube app can subscribe to the NFL Sunday Ticket add-on for $35 to $60 monthly, depending on promotions. Through the YouTube app, you can also purchase access to other Primetime Channels including Paramount+, but it costs the same as paying for those accounts directly. Best free live TV streaming services for 2025 There are loads of ways to get free TV these days. To start, many standard streaming apps have added live components to their lineups — even Netflix. Peacock Premium Plus subscriptions include regional NBC stations. Paramount+ Premium subscribers can watch on-air CBS programming. The new Fox One service includes multiple live Fox stations. True, if you’re already paying for a service it’s not technically “free” but at least the live content isn’t extra. The smart TV operating system (OS) you use likely provides free live content too: Amazon’s Fire TV, Google/Android TV, Roku’s built-in Roku Channel and Samsung’s TV Plus all have hundreds of live channels and original programming. Some of the paid services we recommend above have a free version — namely Sling Freestream, Fubo Free (available after you cancel) and DirecTV’s MyFree. But if you’re looking for more, here are the best free ad-supported TV (FAST) apps with live TV that we tried: Back to top What to look for in a live TV streaming service How to stream live TV Streaming live TV is a lot like using Netflix. You get access through apps on your phone, tablet, smart TV or streaming device and the signal arrives over the internet. A faster and more stable connection tends to give you a better experience. Most live TV apps require you to sign up and pay via a web browser. After that, you can activate the app on all of your devices. Monthly Price When I started testing these cord-cutting alternatives, I was struck by the price difference between live TV and a standard video streaming app. Where the latter cost between $5 and $20 per month, most live TV services hit the $80 mark and can go higher than $200 with additional perks, channel packages and premium extras. The higher starting price is mostly due to the cost of providing multiple networks — particularly sports and local stations. And, in the past year or so, every service has raised base plan prices. Local channels Only two of the services I tried don’t include full local channel coverage for subscribers and one of those makes no effort to carry sports at all. That would be Philo and, as you might guess, it’s the cheapest. The next most affordable option, Sling, only carries three local stations — and only in larger markets — but it still manages to include some of the top sports channels. When you sign up with any provider that handles local TV, you’ll enter your zip code, ensuring you get your area’s broadcast affiliates for ABC, CBS, FOX and NBC. Of course, you can also get those stations for free. Nearly all modern television sets support a radio frequency (RF) connection, also known as the coaxial port, which means if you buy an HD antenna, you’ll receive locally broadcast stations like ABC, CBS, PBS, FOX and NBC. And since the signal is digital, reception is much improved over the staticky rabbit-ears era. But local channel access is another area where traditional streaming services, like Netflix, are bleeding into broadcast territory. For example, you can watch your local NBC station with a Peacock subscription and you can tune into your area’s CBS station through your Paramount+ subscription. Netflix is even getting into the mix with a recently announced deal with one of France’s broadcast companies, TF1. The streaming service will now air TF1's live TV channels and on-demand content inside the Netflix app. No word if the concept will expand to other regions, but it’s an interesting move to anyone interested in the future of streaming. Live sports coverage One reality that spun my head was the sheer number and iterations of sports networks in existence. Trying to figure out which network will carry the match-up you want to see can be tricky. I found that Google makes it a little easier for sports fans by listing out upcoming games (just swap in NBA, NFL, MLB, NHL and so on in the search bar). When you click an event, the “TV & streaming” button will tell you which network is covering it. That just leaves figuring out if your chosen service carries the RSNs (regional sports networks) you want. Unfortunately, even with add-ons and extra packages, some providers simply don’t have certain channels in their lineups. It would take a lawyer to understand the ins and outs of streaming rights negotiations, and networks leave and return to live TV carriers all the time. That said, most major sporting events in the US are covered by ESPN, Fox Sports, TNT, USA and local affiliates. I should also point out that traditional streaming services have started adding live sports to their lineups. Peacock carries live Premier League matches, Sunday Night Football games and aired the 2024 Olympic Games from Paris. Thursday Night Football as well as NBA and WNBA games are on Amazon Prime and Christmas Day Football airs on Netflix. HBO Max (formerly, er, HBO Max) now airs select, regular season games from the NHL, MLB, NCAA and NBA with a $10-per-month add-on. You can watch MLS games with an add-on through the Apple TV app, and Apple TV+ (now just called Apple TV) includes some MLB games. Roku users can watch the just-added free sports channel and those who subscribe to Paramount Plus can see many of the matches aired on CBS Sports, including live NFL games. In 2025, January's Super Bowl was live-streamed for free on Tubi. While all of these alternatives may not cover as much ground as live TV streamers, they could end up being cheaper avenues to the sports you want. And if sports is all you’re after, there are sports-only plans that are a touch cheaper, too. The promised sports streaming service from ESPN, Fox and Warner Bros. called Venu was cancelled early this year. But on August 21, ESPN launched its own streaming service that includes all ESPN channels and costs $30 per month. Fubo Sports is $56 monthly and includes local broadcast stations from ABC, CBS and FOX plus a slew of sports networks (CBS Sport and FS1 among them) as well as all networks included with ESPN Unlimited. Fox launched its own standalone service in August as well and it includes Fox Sports and all other Fox properties (News, Business, Weather) for $20 monthly. DirecTV also has a $70-per-month, sports-only streaming package called MySports and Comcast has a sports and news bundle for that same price (as long as you're an Xfinity customer with auto-pay, otherwise it's more expensive). Traditional cable networks Dozens of linear programming networks were once only available with cable TV, like Bravo, BET, Food Network, HGTV, CNN, Lifetime, SYFY and MTV. If you only subscribe to, say, Netflix or Apple TV+, you won’t have access to those. But as with sports, standard streamers are starting to incorporate this content into their offerings. After the Warner Bros. merger, Max incorporated some content from HGTV, Discovery and TLC. Peacock has Bravo and Hallmark shows, and Paramount+ has material from Nickelodeon, MTV and Comedy Central. Other entertainment channels like AMC+ have stand-alone apps. The Discovery+ app gives you 15 channels ad-free for $10 per month (or with ads for $6 monthly). And a service called Frndly TV starts at a mere $7 per month and streams A&E, Lifetime, Game Show Network, Outdoor Channel and about 35 others. Of course, most live TV streaming options will deliver more sizable lists of cable networks, but just note that you may already be paying for some of them — and if all you need is a certain channel, you could get it cheaper by subscribing directly. On-demand streaming Most live TV subscriptions include access to a selection of video-on-demand (VOD) content, like you would get with a traditional streaming service. Much of this content is made up of the movies and TV series that have recently aired on your subscribed networks. This typically doesn’t cover live events and news programming, but I was able to watch specific episodes of ongoing shows like Top Chef or BET’s Diarra from Detroit. Just search the on-demand library for the program, pick an episode and hit play. Partnerships, like Hulu’s relationship with Disney, and add-ons, such as bundling Max with your YouTube TV subscription or Starz with your Sling plan, will let you watch even larger libraries of on-demand content. But again, if VOD is all you’re after, paying for those networks directly instead of through a live TV plan will be far cheaper. Digital video recordings (DVR) limits Every option I tried offers some cloud DVR storage without needing a separate physical device. You’ll either get unlimited storage for recordings that expires after nine months or a year, or you’ll get a set number of hours (between 50 and 1,000) that you can keep indefinitely. Typically, all you need to do is designate what ongoing TV series you want to record and the DVR component will do all the hard work of saving subsequent episodes for you to watch later. You can do the same thing with sports events. Aside from being able to watch whenever it’s most convenient, you can also fast-forward through commercials in recorded content. In contrast, you can’t skip them on live TV or VOD. Simultaneous streams and profiles per account Each plan gives you a certain number of simultaneous streams, aka how many screens can play content at the same time. And while most providers will let you travel with your subscription, there are usually location restrictions that require you to sign in from your home IP address periodically. Stream allowances range from one at a time to unlimited screens (or as many as your ISP’s bandwidth can handle). Some plans require add-ons to get more screens. Most services also let you set up a few profiles so I was able to give different people in my family the ability to build their own watch histories and libraries, set their favorite channels and get individual recommendations. Picture-in-picture mode and multiview Picture-in-picture (PiP) usually refers to shrinking a video window on a mobile device or computer browser so you can watch it while using other apps. Sling, YouTube TV, FuboTV, Philo, DirecTV Stream and Hulu + Live TV all have PiP modes on computers and mobile devices. Another feature, multiview, lets you view multiple (usually four) sports matches or other live content at once on your TV screen. YouTube TV, FuboTV and now DirecTV all let you do this. With YouTube TV, you can select up to four views from a few preset selection of streams. FuboTV offers the same feature, but only if you're using an Apple TV or Roku streaming device. DirecTV lets you do so through “mixes” which include sports, news, business and kids variants with a set four channels in each mix. 4K live streams Right now, just FuboTV, YouTube TV and DirecTV Stream offer 4K live streams — but with caveats. YouTube TV requires a $20-per-month add-on, after which you’ll only be able to watch certain live content in 4K. DirecTV Stream has three channels that show live 4K content — one with shows and original series, and two with occasional sporting events. You don’t have to pay extra for these but you do need to have either DirecTV’s Gemini receiver, or a device from Fire TV, Apple TV or Roku. You’ll need those same streaming devices to watch the select 4K programming on Sling as well. FuboTV shows certain live events in 4K but access is limited to the Elite and Premier packages, not the base-level Pro plan. Of course, watching any 4K content also requires equipment that can handle it: a 4K smart TV or 4K streaming device paired with a cord and screen that can handle 4K resolution. Tiers, packages and add-ons Comparing price-to-offering ratios is a task for a spreadsheet. I… made three. The base plans range from $28 to $85 per month. From there, you can add packages, which are usually groups of live TV channels bundled by themes like news, sports, entertainment or international content. Premium VOD extras like Max, AMC+ and Starz are also available. Add-ons cost an extra $5 to $20 each per month and simply show up in the guide where you find the rest of your live TV. This is where streaming can quickly get expensive, pushing an $80 subscription to $200 monthly, depending on what you choose. How to stream live TV for free I also downloaded and tried out a few apps that offer free ad-supported TV (FAST) including Freevee, Tubi, PlutoTV and Sling Freestream. These let you drop in and watch a more limited selection of live networks at zero cost. Most don’t even require an email address, let alone a credit card. And if you have a Roku device, an Amazon Fire TV or Stick, a Samsung TV, a Chromecast device or a Google TV, you already have access to hundreds of live channels via the Roku Channel, the live tab in Fire TV, through the Samsung TV Plus app or through Google TV. Back to top How we tested live TV streaming services When I begin testing for a guide, I research the most popular and well-reviewed players in the category and narrow down which are worth trying. For the paid plans, just six services dominate so I tried them all. There are considerably more free live TV contenders so I tested the four most popular. After getting accounts set up using my laptop, I downloaded the apps on a Samsung smart TV running the latest version of Tizen OS. I counted the local stations and regional sports coverage, and noted how many of the top cable networks were available. I then weighed the prices, base packages and available add-ons. I then looked at how the programming was organized in each app’s UI and judged how easy everything was to navigate, from the top navigation to the settings. To test the search function, I searched for the same few TV shows on BET, Food Network, HGTV and Comedy Central, since all six providers carry those channels. I noted how helpful the searches were and how quickly they got me to season 6, episode 13 of Home Town. I used DVR to record entire series and single movies and watched VOD shows, making sure to test the pause and scan functions. On each service with sports, I searched for the same four upcoming NHL, NBA, MLS and NCAA basketball matches and used the record option to save the games and play them back a day or two later. Finally, I noted any extra perks or irritating quirks. All live TV streaming services we’ve tested: Philo Sling YouTube TV Hulu + Live TV DirecTV Stream FuboTV Freevee Tubi PlutoTV Sling Freestream Plex Back to top Live TV Streaming FAQs What is live streaming? Streaming simply refers to video content that is delivered to your screen over the internet. Live streaming can be split into two categories: linear programming and simultaneous transmission. That first one is similar to what you get with cable or broadcast TV, with channels that play a constant flow of movies and shows (sort of what TV looked like before Netflix). Simultaneous streaming lets you watch live events (like a basketball game) or a program (like the evening news) as they happen. What is the difference between streaming and live streaming? Standard streaming, the most popular example being Netflix, lets you pick what you want to watch from a menu of choices. It’s also referred to as “video on demand.” Live streaming refers to sports and news events that you can stream as they happen in real time. It also refers to channels that show a continuous, linear flow of programming. What streaming service is best for live TV? FuboTV does the best job of letting you organize live channels to help you find just what you want to watch. The interface is uncluttered and when you search for something, the UI clearly tells you whether something is live now or on-demand. YouTube TV also does a good job making that info clear. Both have just over 100 live channels on offer. What is the most cost effective TV streaming service? Free TV streaming services like PlutoTV, Plex, Tubi and FreeVee show plenty of ad-supported TV shows and movies without charging you anything. Of course, they won’t have the same channels or content that more premium subscriptions have. Ultimately it depends on what you want to watch and finding the service that can supply that to you in the most streamlined form so you’re not paying for stuff you don’t need. Is it cheaper to have cable or streaming? A basic cable package used to be more expensive than the base-level live TV streaming service. But now that nearly all major providers have raised their prices to over $75 per month, that’s no longer the case. And with add-ons and other premiums, you can easily pay over $200 a month for either cable or a live TV streaming service. But those who want to cut the cord will appreciate that streaming services don't have contracts. What streaming service has all the TV channels? No service that we tested had every available channel. Hulu + Live TV and DirecTV Stream carry the the highest number of the top rated channels, according to Neilsen. Hulu’s service also gets you Disney+ fare, which you can’t get elsewhere. FuboTV has the most sports channels and YouTube TV gives you the widest selection of add-ons. What is the most popular live TV streaming platform? YouTube TV has the most paying customers. According to 2024's letter from the CEO, the service has over eight million subscribers. Disney’s 2024 third quarter earnings put the Hulu + Live TV viewer count at 4.6 million. Sling’s customer count dipped from two million to about 1.9 million in 2024 and FuboTV grew its subscriber list to 1.6 million. How safe are free streaming services and websites? You may have heard certain sites that provide free content can be dangerous, leading to stolen info and/or exposing you to malware. That’s likely in reference to certain peer-to-peer (P2P) networks and file-sharing sites that let people download free movies and series — which can come bundled with malicious code. But if you’re talking about the free ad-supported streaming television (FAST) services listed here, from providers like PlutoTV, Tubi and Plex, they are just as safe as any other streaming service. Since you sometimes don’t even have to provide your email address or credit card info, they can even be more anonymous for cord cutters than apps that require login credentials. Back to top Recent updates October 2025: Added information about the upcoming price increases for Hulu+ Live TV and Philo. Back to topThis article originally appeared on Engadget at https://www.engadget.com/entertainment/streaming/best-live-tv-streaming-service-133000410.html?src=rss",
          "feed_position": 26
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/google-vs-openai-vs-visa-competing-agent-protocols-threaten-the-future-of-ai",
          "published_at": "Thu, 16 Oct 2025 04:00:00 GMT",
          "title": "Google vs. OpenAI vs. Visa: competing agent protocols threaten the future of AI commerce",
          "standfirst": "When Walmart and OpenAI announced that the retailer would integrate with ChatGPT, the question became how quickly OpenAI could deliver on the promise of agents buying things for people. In the battle of AI-enabled commerce, getting agents to securely complete transactions is one of the biggest hurdles. More and more, chat platforms like ChatGPT are replacing browsers and getting very good at surfacing information people search for. Users will ask ChatGPT for the best humidifiers on the market, and when the model returns results, people have no choice but to click the item link and complete the purchase online. AI agents, as of now, don’t have the ability or the trust infrastructure to make people and banking institutions feel safe enough to let it loose on someone’s cash. Enterprises and other industry players understand that, to allow agents to pay for purchases, there must be a common language shared among the model and agent providers, the bank, the merchant, and, to a lesser extent, the buyer. And so, over the past few weeks, three competing agentic commerce standards have emerged: Google announced the Agent Pay Protocol (AP2) with partners including PayPal, American Express, Mastercard, Salesforce and ServiceNow. Soon after, OpenAI and Stripe debuted the Agentic Commerce Protocol (ACP), and just this week, Visa launched the Trusted Agent Protocol (TAP).All these protocols aim to give agents the trust layer they need to convince banks and their customers that they’re money is safe in the hands of an AI agent. But these may also create walled gardens, showing just how immature agentic commerce really is. This is a problem that could cause enterprises to bet on one chat platform and the agentic pay protocol it runs on, instead of interoperability. How are they differentIt’s not new for players to propose several standards. It usually takes years for the industry to coalesce around a single standard, or even to use different protocols and figure out a way to harmonize them. However, the pace of innovation in enterprise moved the needle on that. Fairly quickly, MCP became the de facto channel for tool-use identification, and most companies began setting up MCP servers or connecting to one. (To be clear, it is not a standard yet) But having three different potential standards might slow that process down a bit, because it’s harder to coalesce on a single standard when there are so many to choose from. These protocols all aim to prove authorization. Both AP2 and TAP rely on cryptographic proofs to show an agent is acting on an individual&#x27;s behalf. For TAP, agents are added to an approved list and get a digital key identifying them. AP2 uses a digital contract that serves as a proxy for human approval for the agent. OpenAI’s ACP doesn’t require too much of an infrastructure change, where ACP essentially acts as a courier to the merchant because the agent relays information to the merchant. Walled gardensThese three protocols ideally work across different chat platforms, but that is never guaranteed, especially when your biggest chat platform competitor has its own protocol. A danger with competing protocols is that they can create wall gardens, where they only work on specific platforms. Enterprises face the problem of getting stuck in a platform and an agentic payment standard that will not interoperate with another. Organizations receive not only the product recommended by the agent, but are also most often the merchants of record and need to trust that the agent contacting them is acting on behalf of a customer.Louis Amira, cofounder and CEO of agent commerce startup Circuit and Chisel, told VentureBeat that while this creates an opportunity for companies in the interoperability layer like his, it could create confusion for enterprises. “The better the protocol proposals get, the more likely they are to end up being walled gardens and very hard to interoperate,” Amira said. “We suspect that they’re going to be fighting it out for the next few years, and the more they fight it out, the more you actually need somebody that sits underneath all of them.”Unlike the internet, where anyone can use any browser to access a website, thanks in large part to the TCP/IP standard, chat platforms tend to remain very separate. I mostly use ChatGPT (because it’s installed on my laptop and I don’t need to open a new tab), so when I want to see how Gemini will handle my query, I actually have to open Gemini to do so—the same works for anyone shopping via chatbot. The number of protocol proposals underscores just how far we are from enabling shopping agents. The industry still needs to decide which standard to get behind, and no matter how many Walmarts integrate with ChatGPT, it’s all moot if people don’t trust the model or agent to handle their cash. Take the best features, hopefully The best thing for enterprises to do for now is to experiment with all the protocols and hope that a winner emerges. Eventually, there could be one agentic commerce protocol that takes the best of each proposal. For Wayne Liu, chief growth officer and president for Americas at Perfect Corp., having multiple protocol proposals just means there’s more learning.“This is where the importance of open source exists because it will be the driving force to put everything together,” Liu said. Of course, what would be interesting to see these next couple of weeks is if there will only be three competing agentic commerce protocols. After all, there are some large retailers and chat platforms that can still throw a wrench into the whole thing.",
          "content": "When Walmart and OpenAI announced that the retailer would integrate with ChatGPT, the question became how quickly OpenAI could deliver on the promise of agents buying things for people. In the battle of AI-enabled commerce, getting agents to securely complete transactions is one of the biggest hurdles. More and more, chat platforms like ChatGPT are replacing browsers and getting very good at surfacing information people search for. Users will ask ChatGPT for the best humidifiers on the market, and when the model returns results, people have no choice but to click the item link and complete the purchase online. AI agents, as of now, don’t have the ability or the trust infrastructure to make people and banking institutions feel safe enough to let it loose on someone’s cash. Enterprises and other industry players understand that, to allow agents to pay for purchases, there must be a common language shared among the model and agent providers, the bank, the merchant, and, to a lesser extent, the buyer. And so, over the past few weeks, three competing agentic commerce standards have emerged: Google announced the Agent Pay Protocol (AP2) with partners including PayPal, American Express, Mastercard, Salesforce and ServiceNow. Soon after, OpenAI and Stripe debuted the Agentic Commerce Protocol (ACP), and just this week, Visa launched the Trusted Agent Protocol (TAP).All these protocols aim to give agents the trust layer they need to convince banks and their customers that they’re money is safe in the hands of an AI agent. But these may also create walled gardens, showing just how immature agentic commerce really is. This is a problem that could cause enterprises to bet on one chat platform and the agentic pay protocol it runs on, instead of interoperability. How are they differentIt’s not new for players to propose several standards. It usually takes years for the industry to coalesce around a single standard, or even to use different protocols and figure out a way to harmonize them. However, the pace of innovation in enterprise moved the needle on that. Fairly quickly, MCP became the de facto channel for tool-use identification, and most companies began setting up MCP servers or connecting to one. (To be clear, it is not a standard yet) But having three different potential standards might slow that process down a bit, because it’s harder to coalesce on a single standard when there are so many to choose from. These protocols all aim to prove authorization. Both AP2 and TAP rely on cryptographic proofs to show an agent is acting on an individual&#x27;s behalf. For TAP, agents are added to an approved list and get a digital key identifying them. AP2 uses a digital contract that serves as a proxy for human approval for the agent. OpenAI’s ACP doesn’t require too much of an infrastructure change, where ACP essentially acts as a courier to the merchant because the agent relays information to the merchant. Walled gardensThese three protocols ideally work across different chat platforms, but that is never guaranteed, especially when your biggest chat platform competitor has its own protocol. A danger with competing protocols is that they can create wall gardens, where they only work on specific platforms. Enterprises face the problem of getting stuck in a platform and an agentic payment standard that will not interoperate with another. Organizations receive not only the product recommended by the agent, but are also most often the merchants of record and need to trust that the agent contacting them is acting on behalf of a customer.Louis Amira, cofounder and CEO of agent commerce startup Circuit and Chisel, told VentureBeat that while this creates an opportunity for companies in the interoperability layer like his, it could create confusion for enterprises. “The better the protocol proposals get, the more likely they are to end up being walled gardens and very hard to interoperate,” Amira said. “We suspect that they’re going to be fighting it out for the next few years, and the more they fight it out, the more you actually need somebody that sits underneath all of them.”Unlike the internet, where anyone can use any browser to access a website, thanks in large part to the TCP/IP standard, chat platforms tend to remain very separate. I mostly use ChatGPT (because it’s installed on my laptop and I don’t need to open a new tab), so when I want to see how Gemini will handle my query, I actually have to open Gemini to do so—the same works for anyone shopping via chatbot. The number of protocol proposals underscores just how far we are from enabling shopping agents. The industry still needs to decide which standard to get behind, and no matter how many Walmarts integrate with ChatGPT, it’s all moot if people don’t trust the model or agent to handle their cash. Take the best features, hopefully The best thing for enterprises to do for now is to experiment with all the protocols and hope that a winner emerges. Eventually, there could be one agentic commerce protocol that takes the best of each proposal. For Wayne Liu, chief growth officer and president for Americas at Perfect Corp., having multiple protocol proposals just means there’s more learning.“This is where the importance of open source exists because it will be the driving force to put everything together,” Liu said. Of course, what would be interesting to see these next couple of weeks is if there will only be three competing agentic commerce protocols. After all, there are some large retailers and chat platforms that can still throw a wrench into the whole thing.",
          "feed_position": 5,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/1V6XILFC2iVvqfh58LXNkm/d7375489aa11c0096efb78642ca1c7a5/crimedy7_illustration_of_robot_paying_for_groceries_--ar_169__3d2a2c22-022a-4104-9070-fab8f94d73d5_1.png"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/under-the-hood-of-ai-agents-a-technical-guide-to-the-next-frontier-of-gen-ai",
          "published_at": "Thu, 16 Oct 2025 02:25:00 GMT",
          "title": "Under the hood of AI agents: A technical guide to the next frontier of gen AI",
          "standfirst": "Agents are the trendiest topic in AI today, and with good reason. AI agents act on their users’ behalf, autonomously handling tasks like making online purchases, building software, researching business trends or booking travel. By taking generative AI out of the sandbox of the chat interface and allowing it to act directly on the world, agentic AI represents a leap forward in the power and utility of AI.Taking gen AI out of the protected sandbox of the chat interface and allowing it to act directly on the world represents a leap forward in the power and utility of AI.Agentic AI has been moving really fast: For example, one of the core building blocks of today’s agents, the model context protocol (MCP), is only a year old! As in any fast-moving field, there are many competing definitions, hot takes and misleading opinions.To cut through the noise, I’d like to describe the core components of an agentic AI system and how they fit together: It’s really not as complicated as it may seem. Hopefully, when you’ve finished reading this post, agents won’t seem as mysterious.Agentic ecosystemDefinitions of the word “agent” abound, but I like a slight variation on the British programmer Simon Willison’s minimalist take:An LLM agent runs tools in a loop to achieve a goal.The user prompts a large language model (LLM) with a goal: Say, booking a table at a restaurant near a specific theater. Along with the goal, the model receives a list of the tools at its disposal, such as a database of restaurant locations or a record of the user’s food preferences. The model then plans how to achieve the goal and calls one of the tools, which provides a response; the model then calls a new tool. Through repetitions, the agent moves toward accomplishing the goal. In some cases, the model’s orchestration and planning choices are complemented or enhanced by imperative code.But what kind of infrastructure does it take to realize this approach? An agentic system needs a few core components:A way to build the agent. When you deploy an agent, you don’t want to have to code it from scratch. There are several agent development frameworks out there.Somewhere to run the AI model. A seasoned AI developer can download an open-weight LLM, but it takes expertise to do that right. It also takes expensive hardware that’s going to be poorly utilized for the average user.Somewhere to run the agentic code. With established frameworks, the user creates code for an agent object with a defined set of functions. Most of those functions involve sending prompts to an AI model, but the code needs to run somewhere. In practice, most agents will run in the cloud, because we want them to keep running when our laptops are closed, and we want them to scale up and out to do their work.A mechanism for translating between the text-based LLM and tool calls.A short-term memory for tracking the content of agentic interactions.A long-term memory for tracking the user’s preferences and affinities across sessions.A way to trace the system’s execution, to evaluate the agent’s performance.Let&#x27;s dive into more detail on each of these components.Building an agentAsking an LLM to explain how it plans to approach a particular task improves its performance on that task. This “chain-of-thought reasoning” is now ubiquitous in AI.The analogue in agentic systems is the ReAct (reasoning + action) model, in which the agent has a thought (“I’ll use the map function to locate nearby restaurants”), performs an action (issuing an API call to the map function), then makes an observation (“There are two pizza places and one Indian restaurant within two blocks of the movie theater”).ReAct isn’t the only way to build agents, but it is at the core of most successful agentic systems. Today, agents are commonly loops over the thought-action-observation sequence.The tools available to the agent can include local tools and remote tools such as databases, microservices and software as a service. A tool’s specification includes a natural-language explanation of how and when it’s used and the syntax of its API calls.The developer can also tell the agent to, essentially, build its own tools on the fly. Say that a tool retrieves a table stored as comma-separated text, and to fulfill its goal, the agent needs to sort the table.Sorting a table by repeatedly sending it through an LLM and evaluating the results would be a colossal waste of resources — and it’s not even guaranteed to give the right result. Instead, the developer can simply instruct the agent to generate its own Python code when it encounters a simple but repetitive task. These snippets of code can run locally alongside the agent or in a dedicated secure code interpreter tool. Available tools can divide responsibility between the LLM and the developer. Once the tools available to the agent have been specified, the developer can simply instruct the agent what tools to use when necessary. Or, the developer can specify which tool to use for which types of data, and even which data items to use as arguments during function calls.Similarly, the developer can simply tell the agent to generate Python code when necessary to automate repetitive tasks or, alternatively, tell it which algorithms to use for which data types and even provide pseudocode. The approach can vary from agent to agent.RuntimeHistorically, there were two main ways to isolate code running on shared servers: Containerization, which was efficient but offered lower security; and virtual machines, which were secure but came with a lot of computational overhead.In 2018, Amazon Web Services’ (AWS’s) Lambda serverless-computing service deployed Firecracker, a new paradigm in server isolation. Firecracker creates “microVMs”, complete with hardware isolation and their own Linux kernels but with reduced overhead (as low as a few megabytes) and startup times (as low as a few milliseconds). The low overhead means that each function executed on a Lambda server can have its own microVM.However, because instantiating an agent requires deploying an LLM, together with the memory resources to track the LLM’s inputs and outputs, the per-function isolation model is impractical. Instead, with session-based isolation, every session is assigned its own microVM. When the session finishes, the LLM’s state information is copied to long-term memory, and the microVM is destroyed. This ensures secure and efficient deployment of hosts of agents.Tool callsJust as there are several existing development frameworks for agent creation, there are several existing standards for communication between agents and tools, the most popular of which — currently — is the model context protocol (MCP). MCP establishes a one-to-one connection between the agent’s LLM and a dedicated MCP server that executes tool calls, and it also establishes a standard format for passing different types of data back and forth between the LLM and its server.Many platforms use MCP by default, but are also configurable, so they will support a growing set of protocols over time.Sometimes, however, the necessary tool is not one with an available API. In such cases, the only way to retrieve data or perform an action is through cursor movements and clicks on a website. There are a number of services available to perform such computer use. This makes any website a potential tool for agents, opening up decades of content and valuable services that aren’t yet available directly through APIs.AuthorizationsWith agents, authorization works in two directions. First, of course, users require authorization to run the agents they’ve created. But as the agent is acting on the user’s behalf, it will usually require its own authorization to access networked resources.There are a few different ways to approach the problem of authorization. One is with an access delegation algorithm like OAuth, which essentially plumbs the authorization process through the agentic system. The user enters login credentials into OAuth, and the agentic system uses OAuth to log into protected resources, but the agentic system never has direct access to the user’s passwords.In the other approach, the user logs into a secure session on a server, and the server has its own login credentials on protected resources. Permissions allow the user to select from a variety of authorization strategies and algorithms for implementing those strategies.Memory and tracesShort-term memoryLLMs are next-word prediction engines. What makes them so astoundingly versatile is that their predictions are based on long sequences of words they’ve already seen, known as context. Context is, in itself, a kind of memory. But it’s not the only kind an agentic system needs.Suppose, again, that an agent is trying to book a restaurant near a movie theater, and from a map tool, it’s retrieved a couple dozen restaurants within a mile radius. It doesn’t want to dump information about all those restaurants into the LLM’s context: All that extraneous information could wreak havoc with next-word probabilities.Instead, it can store the complete list in short-term memory and retrieve one or two records at a time, based on, say, the user’s price and cuisine preferences and proximity to the theater. If none of those restaurants pans out, the agent can dip back into short-term memory, rather than having to execute another tool call.Long-term memoryAgents also need to remember their prior interactions with their clients. If last week I told the restaurant booking agent what type of food I like, I don’t want to have to tell it again this week. The same goes for my price tolerance, the sort of ambiance I’m looking for, and so on.Long-term memory allows the agent to look up what it needs to know about prior conversations with the user. Agents don’t typically create long-term memories themselves, however. Instead, after a session is complete, the whole conversation passes to a separate AI model, which creates new long-term memories or updates existing ones.Memory creation can involve LLM summarization and “chunking”, in which documents are split into sections grouped according to topic for ease of retrieval during subsequent sessions. Available systems allow the user to select strategies and algorithms for summarization, chunking and other information-extraction techniques.ObservabilityAgents are a new kind of software system, and they require new ways to think about observing, monitoring and auditing their behavior. Some of the questions we ask will look familiar: Whether the agents are running fast enough, how much they’re costing, how many tool calls they’re making and whether users are happy. But new questions will arise, too, and we can’t necessarily predict what data we’ll need to answer them.Observability and tracing tools can provide an end-to-end view of the execution of a session with an agent, breaking down step-by-step which actions were taken and why. For the agent builder, these traces are key to understanding how well agents are working — and provide the data to make them work better.I hope this explanation has demystified agentic AI enough that you’re willing to try building your own agents!",
          "content": "Agents are the trendiest topic in AI today, and with good reason. AI agents act on their users’ behalf, autonomously handling tasks like making online purchases, building software, researching business trends or booking travel. By taking generative AI out of the sandbox of the chat interface and allowing it to act directly on the world, agentic AI represents a leap forward in the power and utility of AI.Taking gen AI out of the protected sandbox of the chat interface and allowing it to act directly on the world represents a leap forward in the power and utility of AI.Agentic AI has been moving really fast: For example, one of the core building blocks of today’s agents, the model context protocol (MCP), is only a year old! As in any fast-moving field, there are many competing definitions, hot takes and misleading opinions.To cut through the noise, I’d like to describe the core components of an agentic AI system and how they fit together: It’s really not as complicated as it may seem. Hopefully, when you’ve finished reading this post, agents won’t seem as mysterious.Agentic ecosystemDefinitions of the word “agent” abound, but I like a slight variation on the British programmer Simon Willison’s minimalist take:An LLM agent runs tools in a loop to achieve a goal.The user prompts a large language model (LLM) with a goal: Say, booking a table at a restaurant near a specific theater. Along with the goal, the model receives a list of the tools at its disposal, such as a database of restaurant locations or a record of the user’s food preferences. The model then plans how to achieve the goal and calls one of the tools, which provides a response; the model then calls a new tool. Through repetitions, the agent moves toward accomplishing the goal. In some cases, the model’s orchestration and planning choices are complemented or enhanced by imperative code.But what kind of infrastructure does it take to realize this approach? An agentic system needs a few core components:A way to build the agent. When you deploy an agent, you don’t want to have to code it from scratch. There are several agent development frameworks out there.Somewhere to run the AI model. A seasoned AI developer can download an open-weight LLM, but it takes expertise to do that right. It also takes expensive hardware that’s going to be poorly utilized for the average user.Somewhere to run the agentic code. With established frameworks, the user creates code for an agent object with a defined set of functions. Most of those functions involve sending prompts to an AI model, but the code needs to run somewhere. In practice, most agents will run in the cloud, because we want them to keep running when our laptops are closed, and we want them to scale up and out to do their work.A mechanism for translating between the text-based LLM and tool calls.A short-term memory for tracking the content of agentic interactions.A long-term memory for tracking the user’s preferences and affinities across sessions.A way to trace the system’s execution, to evaluate the agent’s performance.Let&#x27;s dive into more detail on each of these components.Building an agentAsking an LLM to explain how it plans to approach a particular task improves its performance on that task. This “chain-of-thought reasoning” is now ubiquitous in AI.The analogue in agentic systems is the ReAct (reasoning + action) model, in which the agent has a thought (“I’ll use the map function to locate nearby restaurants”), performs an action (issuing an API call to the map function), then makes an observation (“There are two pizza places and one Indian restaurant within two blocks of the movie theater”).ReAct isn’t the only way to build agents, but it is at the core of most successful agentic systems. Today, agents are commonly loops over the thought-action-observation sequence.The tools available to the agent can include local tools and remote tools such as databases, microservices and software as a service. A tool’s specification includes a natural-language explanation of how and when it’s used and the syntax of its API calls.The developer can also tell the agent to, essentially, build its own tools on the fly. Say that a tool retrieves a table stored as comma-separated text, and to fulfill its goal, the agent needs to sort the table.Sorting a table by repeatedly sending it through an LLM and evaluating the results would be a colossal waste of resources — and it’s not even guaranteed to give the right result. Instead, the developer can simply instruct the agent to generate its own Python code when it encounters a simple but repetitive task. These snippets of code can run locally alongside the agent or in a dedicated secure code interpreter tool. Available tools can divide responsibility between the LLM and the developer. Once the tools available to the agent have been specified, the developer can simply instruct the agent what tools to use when necessary. Or, the developer can specify which tool to use for which types of data, and even which data items to use as arguments during function calls.Similarly, the developer can simply tell the agent to generate Python code when necessary to automate repetitive tasks or, alternatively, tell it which algorithms to use for which data types and even provide pseudocode. The approach can vary from agent to agent.RuntimeHistorically, there were two main ways to isolate code running on shared servers: Containerization, which was efficient but offered lower security; and virtual machines, which were secure but came with a lot of computational overhead.In 2018, Amazon Web Services’ (AWS’s) Lambda serverless-computing service deployed Firecracker, a new paradigm in server isolation. Firecracker creates “microVMs”, complete with hardware isolation and their own Linux kernels but with reduced overhead (as low as a few megabytes) and startup times (as low as a few milliseconds). The low overhead means that each function executed on a Lambda server can have its own microVM.However, because instantiating an agent requires deploying an LLM, together with the memory resources to track the LLM’s inputs and outputs, the per-function isolation model is impractical. Instead, with session-based isolation, every session is assigned its own microVM. When the session finishes, the LLM’s state information is copied to long-term memory, and the microVM is destroyed. This ensures secure and efficient deployment of hosts of agents.Tool callsJust as there are several existing development frameworks for agent creation, there are several existing standards for communication between agents and tools, the most popular of which — currently — is the model context protocol (MCP). MCP establishes a one-to-one connection between the agent’s LLM and a dedicated MCP server that executes tool calls, and it also establishes a standard format for passing different types of data back and forth between the LLM and its server.Many platforms use MCP by default, but are also configurable, so they will support a growing set of protocols over time.Sometimes, however, the necessary tool is not one with an available API. In such cases, the only way to retrieve data or perform an action is through cursor movements and clicks on a website. There are a number of services available to perform such computer use. This makes any website a potential tool for agents, opening up decades of content and valuable services that aren’t yet available directly through APIs.AuthorizationsWith agents, authorization works in two directions. First, of course, users require authorization to run the agents they’ve created. But as the agent is acting on the user’s behalf, it will usually require its own authorization to access networked resources.There are a few different ways to approach the problem of authorization. One is with an access delegation algorithm like OAuth, which essentially plumbs the authorization process through the agentic system. The user enters login credentials into OAuth, and the agentic system uses OAuth to log into protected resources, but the agentic system never has direct access to the user’s passwords.In the other approach, the user logs into a secure session on a server, and the server has its own login credentials on protected resources. Permissions allow the user to select from a variety of authorization strategies and algorithms for implementing those strategies.Memory and tracesShort-term memoryLLMs are next-word prediction engines. What makes them so astoundingly versatile is that their predictions are based on long sequences of words they’ve already seen, known as context. Context is, in itself, a kind of memory. But it’s not the only kind an agentic system needs.Suppose, again, that an agent is trying to book a restaurant near a movie theater, and from a map tool, it’s retrieved a couple dozen restaurants within a mile radius. It doesn’t want to dump information about all those restaurants into the LLM’s context: All that extraneous information could wreak havoc with next-word probabilities.Instead, it can store the complete list in short-term memory and retrieve one or two records at a time, based on, say, the user’s price and cuisine preferences and proximity to the theater. If none of those restaurants pans out, the agent can dip back into short-term memory, rather than having to execute another tool call.Long-term memoryAgents also need to remember their prior interactions with their clients. If last week I told the restaurant booking agent what type of food I like, I don’t want to have to tell it again this week. The same goes for my price tolerance, the sort of ambiance I’m looking for, and so on.Long-term memory allows the agent to look up what it needs to know about prior conversations with the user. Agents don’t typically create long-term memories themselves, however. Instead, after a session is complete, the whole conversation passes to a separate AI model, which creates new long-term memories or updates existing ones.Memory creation can involve LLM summarization and “chunking”, in which documents are split into sections grouped according to topic for ease of retrieval during subsequent sessions. Available systems allow the user to select strategies and algorithms for summarization, chunking and other information-extraction techniques.ObservabilityAgents are a new kind of software system, and they require new ways to think about observing, monitoring and auditing their behavior. Some of the questions we ask will look familiar: Whether the agents are running fast enough, how much they’re costing, how many tool calls they’re making and whether users are happy. But new questions will arise, too, and we can’t necessarily predict what data we’ll need to answer them.Observability and tracing tools can provide an end-to-end view of the execution of a session with an agent, breaking down step-by-step which actions were taken and why. For the agent builder, these traces are key to understanding how well agents are working — and provide the data to make them work better.I hope this explanation has demystified agentic AI enough that you’re willing to try building your own agents!",
          "feed_position": 6,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/5uEgR8k5xDLaMIS0le17hv/595a27e9dc71acf83cc362c433942ce6/AI_agents.png"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/ai/googles-veo-31-is-better-at-generating-videos-from-images-220829129.html",
          "published_at": "Wed, 15 Oct 2025 22:08:29 +0000",
          "title": "Google's Veo 3.1 is better at generating videos from images",
          "standfirst": "Google has released a new update to its Veo AI video generation model that should make it do a better job of sticking to prompts and converting images into videos. Veo 3.1 is available to try today through Google's Gemini API and is now also powering the company's Flow video editor.Veo 3.1 builds on the new capabilities Google introduced with launch of Veo 3 at Google I/O 2025. The new model offers better \"prompt adherence,\" according to Google, and should have an easier time creating videos based on the image \"ingredients\" you upload alongside your written prompt. Veo 3.1 also makes it possible to convert images to video and generate audio at the same time, a capability that wasn't available with Veo 3. In Flow, Veo 3.1 supports at least a new feature that gives you finer control over the videos you generate. With what Google calls \"Frame to Video,\" Flow lets you upload a first and last frame, and then generates the video in-between. Adobe Firefly, which is powered by Veo 3, offers a similar feature, but Flow will be able to pull it off and create audio at the same time. Those added audio skills will also apply to the video editor's ability to extend clips and insert objects into existing footage, too.Based on the samples Google's shared, videos generated with Veo 3.1 still have an uncanny quality that seems to vary greatly depending on the prompt and subject. Even if it's missing some of the realism of OpenAI's Sora 2, though, the company's decision to try and make Veo more useful to people who actually work with video rather than a source of social media spam is a welcome move.This article originally appeared on Engadget at https://www.engadget.com/ai/googles-veo-31-is-better-at-generating-videos-from-images-220829129.html?src=rss",
          "content": "Google has released a new update to its Veo AI video generation model that should make it do a better job of sticking to prompts and converting images into videos. Veo 3.1 is available to try today through Google's Gemini API and is now also powering the company's Flow video editor.Veo 3.1 builds on the new capabilities Google introduced with launch of Veo 3 at Google I/O 2025. The new model offers better \"prompt adherence,\" according to Google, and should have an easier time creating videos based on the image \"ingredients\" you upload alongside your written prompt. Veo 3.1 also makes it possible to convert images to video and generate audio at the same time, a capability that wasn't available with Veo 3. In Flow, Veo 3.1 supports at least a new feature that gives you finer control over the videos you generate. With what Google calls \"Frame to Video,\" Flow lets you upload a first and last frame, and then generates the video in-between. Adobe Firefly, which is powered by Veo 3, offers a similar feature, but Flow will be able to pull it off and create audio at the same time. Those added audio skills will also apply to the video editor's ability to extend clips and insert objects into existing footage, too.Based on the samples Google's shared, videos generated with Veo 3.1 still have an uncanny quality that seems to vary greatly depending on the prompt and subject. Even if it's missing some of the realism of OpenAI's Sora 2, though, the company's decision to try and make Veo more useful to people who actually work with video rather than a source of social media spam is a welcome move.This article originally appeared on Engadget at https://www.engadget.com/ai/googles-veo-31-is-better-at-generating-videos-from-images-220829129.html?src=rss",
          "feed_position": 28
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/wearables/apple-will-sell-ps-vr2-sense-controllers-separately-for-250-next-month-203602932.html",
          "published_at": "Wed, 15 Oct 2025 20:36:02 +0000",
          "title": "Apple will sell PS VR2 Sense controllers separately for $250 next month",
          "standfirst": "Tucked away in Apple's announcement of a second-generation Vision Pro was news on the controller front. First, we already knew that, with visionOS 26, Apple's headset supports PlayStation VR2 Sense controllers. But now you can get them without Sony's headset. The Apple Store will soon begin selling the PS VR2 Sense controllers for $250. (Cue spit take.) The $400 PS VR2 headset bundle was previously the only way to buy them new. Maybe when you can justify spending $3,499 on Apple's reality machine, $250 for the controllers is reasonable. The second-generation Vision ProApple Apple says Sony's controllers open the door to more immersive gameplay on the Vision Pro. They support six degrees of freedom motion tracking (any direction you move or rotate), finger touch detection and rumble support. Apple's second-gen Vision Pro is more powerful with the new M5 chip. It also includes a Dual Knit Band, which adds a top strap for increased stability and comfort. You'll be able to buy the PS VR2 Sense controllers from the Apple Store on November 11.This article originally appeared on Engadget at https://www.engadget.com/wearables/apple-will-sell-ps-vr2-sense-controllers-separately-for-250-next-month-203602932.html?src=rss",
          "content": "Tucked away in Apple's announcement of a second-generation Vision Pro was news on the controller front. First, we already knew that, with visionOS 26, Apple's headset supports PlayStation VR2 Sense controllers. But now you can get them without Sony's headset. The Apple Store will soon begin selling the PS VR2 Sense controllers for $250. (Cue spit take.) The $400 PS VR2 headset bundle was previously the only way to buy them new. Maybe when you can justify spending $3,499 on Apple's reality machine, $250 for the controllers is reasonable. The second-generation Vision ProApple Apple says Sony's controllers open the door to more immersive gameplay on the Vision Pro. They support six degrees of freedom motion tracking (any direction you move or rotate), finger touch detection and rumble support. Apple's second-gen Vision Pro is more powerful with the new M5 chip. It also includes a Dual Knit Band, which adds a top strap for increased stability and comfort. You'll be able to buy the PS VR2 Sense controllers from the Apple Store on November 11.This article originally appeared on Engadget at https://www.engadget.com/wearables/apple-will-sell-ps-vr2-sense-controllers-separately-for-250-next-month-203602932.html?src=rss",
          "feed_position": 31,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2025-10/aa68ee80-aa04-11f0-bf77-aabc70025f2b"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/computing/apples-m5-chip-pushes-ai-performance-with-new-neural-accelerators-193745702.html",
          "published_at": "Wed, 15 Oct 2025 19:37:45 +0000",
          "title": "Apple's M5 chip pushes AI performance with new 'Neural Accelerators'",
          "standfirst": "Like clockwork, Apple has introduced a new M-series chip on updated versions of the iPad Pro, MacBook Pro and for the first time, Apple Vision Pro. The new M5 chip shares plenty of similarities with the M4 chip Apple introduced in 2024, but the biggest seems to be a focus on improving graphics and AI performance.The M5 chip is made using a new third-generation 3nm process, according to Apple, with an updated 10-core GPU architecture on all versions that offers four times the peak GPU compute performance of the M4, while carrying over support for things like hardware-accelerated ray tracing. The M5 also features a 10-core CPU, just like the M4, with six efficiency and up to four performance cores. The M5 chip configurations for the iPad Pro.AppleThat is unless you're buying an M5 iPad Pro. The 1TB and 2TB models of the Pro feature a 10-core CPU and GPU, but if you opt for a smaller storage size of 256GB or 512GB, you'll get a nine-core CPU with six efficiency cores and three performance cores. Apple says the 10-core CPU offers \"up to 15 percent faster multithreaded performance\" over the M4 chip, though it's not clear if that's due to the CPU or memory improvements. That’s because the new chip offers an improved memory bandwidth of 153GB/s, up from the starting 120GB/s bandwidth on the M4, though less than what you can get with the M4 Pro or M4 Max. Apple will likely release M5 versions of both chips next year, but it's worth knowing what you're missing if you opt for the M5 right now.The performance gains Apple was able to squeeze out of its new GPU and improved memory bandwidth seem like the biggest changes users will actually notice from the M5. That includes \"up to 30 percent faster\" graphics performance than the M4 and \"up to a 45 percent graphics uplift in apps using ray tracing.\" Apple introduced the M4 with a focus on dynamic caching and ray tracing, and it seems like the M5 makes both graphical processes more efficient. The gains are apparently even more noticeable on the M5 Vision Pro, where the headset can achieve a 120Hz refresh rate, up from the 100Hz max Apple guaranteed before, and is now able to render \"10 percent more pixels.\"AI performance is also improved, though not necessarily thanks to the M5's 16-core Neural Engine alone, which seems to be the same Neural Engine used in the M4. Instead, Apple's taking a new approach to AI processing by including dedicated \"Neural Accelerators\" in each core of its GPU. This extra help has led to faster performance when devices are using Apple Intelligence skills or AI-powered features like the Vision Pro's ability to generate a Persona, according to Apple.In-depth testing and benchmarking of Apple's new M5 devices will be required to accurately capture how the M5 chip changes things, especially when it comes to general CPU performance. For now, though, Apple's chips continue to get more graphically powerful, which bodes well for anyone who uses MacBooks and iPads for creative tasks, AI or playing games.This article originally appeared on Engadget at https://www.engadget.com/computing/apples-m5-chip-pushes-ai-performance-with-new-neural-accelerators-193745702.html?src=rss",
          "content": "Like clockwork, Apple has introduced a new M-series chip on updated versions of the iPad Pro, MacBook Pro and for the first time, Apple Vision Pro. The new M5 chip shares plenty of similarities with the M4 chip Apple introduced in 2024, but the biggest seems to be a focus on improving graphics and AI performance.The M5 chip is made using a new third-generation 3nm process, according to Apple, with an updated 10-core GPU architecture on all versions that offers four times the peak GPU compute performance of the M4, while carrying over support for things like hardware-accelerated ray tracing. The M5 also features a 10-core CPU, just like the M4, with six efficiency and up to four performance cores. The M5 chip configurations for the iPad Pro.AppleThat is unless you're buying an M5 iPad Pro. The 1TB and 2TB models of the Pro feature a 10-core CPU and GPU, but if you opt for a smaller storage size of 256GB or 512GB, you'll get a nine-core CPU with six efficiency cores and three performance cores. Apple says the 10-core CPU offers \"up to 15 percent faster multithreaded performance\" over the M4 chip, though it's not clear if that's due to the CPU or memory improvements. That’s because the new chip offers an improved memory bandwidth of 153GB/s, up from the starting 120GB/s bandwidth on the M4, though less than what you can get with the M4 Pro or M4 Max. Apple will likely release M5 versions of both chips next year, but it's worth knowing what you're missing if you opt for the M5 right now.The performance gains Apple was able to squeeze out of its new GPU and improved memory bandwidth seem like the biggest changes users will actually notice from the M5. That includes \"up to 30 percent faster\" graphics performance than the M4 and \"up to a 45 percent graphics uplift in apps using ray tracing.\" Apple introduced the M4 with a focus on dynamic caching and ray tracing, and it seems like the M5 makes both graphical processes more efficient. The gains are apparently even more noticeable on the M5 Vision Pro, where the headset can achieve a 120Hz refresh rate, up from the 100Hz max Apple guaranteed before, and is now able to render \"10 percent more pixels.\"AI performance is also improved, though not necessarily thanks to the M5's 16-core Neural Engine alone, which seems to be the same Neural Engine used in the M4. Instead, Apple's taking a new approach to AI processing by including dedicated \"Neural Accelerators\" in each core of its GPU. This extra help has led to faster performance when devices are using Apple Intelligence skills or AI-powered features like the Vision Pro's ability to generate a Persona, according to Apple.In-depth testing and benchmarking of Apple's new M5 devices will be required to accurately capture how the M5 chip changes things, especially when it comes to general CPU performance. For now, though, Apple's chips continue to get more graphically powerful, which bodes well for anyone who uses MacBooks and iPads for creative tasks, AI or playing games.This article originally appeared on Engadget at https://www.engadget.com/computing/apples-m5-chip-pushes-ai-performance-with-new-neural-accelerators-193745702.html?src=rss",
          "feed_position": 33,
          "image_url": "https://d29szjachogqwa.cloudfront.net/videos/user-uploaded/M5-chip-ipad-pro.jpg"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/anthropic-is-giving-away-its-powerful-claude-haiku-4-5-ai-for-free-to-take",
          "published_at": "Wed, 15 Oct 2025 19:00:00 GMT",
          "title": "Anthropic is giving away its powerful Claude Haiku 4.5 AI for free to take on OpenAI",
          "standfirst": "Anthropic released Claude Haiku 4.5 on Wednesday, a smaller and significantly cheaper artificial intelligence model that matches the coding capabilities of systems that were considered cutting-edge just months ago, marking the latest salvo in an intensifying competition to dominate enterprise AI.The model costs $1 per million input tokens and $5 per million output tokens — roughly one-third the price of Anthropic&#x27;s mid-sized Sonnet 4 model released in May, while operating more than twice as fast. In certain tasks, particularly operating computers autonomously, Haiku 4.5 actually surpasses its more expensive predecessor.\"Haiku 4.5 is a clear leap in performance and is now largely as smart as Sonnet 4 while being significantly faster and one-third of the cost,\" an Anthropic spokesperson told VentureBeat, underscoring how rapidly AI capabilities are becoming commoditized as the technology matures.The launch comes just two weeks after Anthropic released Claude Sonnet 4.5, which the company bills as the world&#x27;s best coding model, and two months after introducing Opus 4.1. The breakneck pace of releases reflects mounting pressure from OpenAI, whose $500 billion valuation dwarfs Anthropic&#x27;s $183 billion, and which has inked a series of multibillion-dollar infrastructure deals while expanding its product lineup.How free access to advanced AI could reshape the enterprise marketIn an unusual move that could reshape competitive dynamics in the AI market, Anthropic is making Haiku 4.5 available for all free users of its Claude.ai platform. The decision effectively democratizes access to what the company characterizes as \"near-frontier-level intelligence\" — capabilities that would have been available only in expensive, premium models months ago.\"The launch of Claude Haiku 4.5 means that near-frontier-level intelligence is available for free to all users through Claude.ai,\" the Anthropic spokesperson told VentureBeat. \"It also offers significant advantages to our enterprise customers: Sonnet 4.5 can handle frontier planning while Haiku 4.5 powers sub-agents, enabling multi-agent systems that tackle complex refactors, migrations, and large features builds with speed and quality.\"This multi-agent architecture signals a significant shift in how AI systems are deployed. Rather than relying on a single, monolithic model, enterprises can now orchestrate teams of specialized AI agents: a more sophisticated Sonnet 4.5 model breaking down complex problems and delegating subtasks to multiple Haiku 4.5 agents working in parallel. For software development teams, this could mean Sonnet 4.5 plans a major code refactoring while Haiku 4.5 agents simultaneously execute changes across dozens of files.The approach mirrors how human organizations distribute work, and could prove particularly valuable for enterprises seeking to balance performance with cost efficiency — a critical consideration as AI deployment scales.Inside Anthropic&#x27;s path to $7 billion in annual revenueThe model launch coincides with revelations that Anthropic&#x27;s business is experiencing explosive growth. The company&#x27;s annual revenue run rate is approaching $7 billion this month, Anthropic told Reuters, up from more than $5 billion reported in August. Internal projections obtained by Reuters suggest the company is targeting between $20 billion and $26 billion in annualized revenue for 2026, representing growth of more than 200% to nearly 300%.The company now serves more than 300,000 business customers, with enterprise products accounting for approximately 80% of revenue. Among Anthropic&#x27;s most successful offerings is Claude Code, a code-generation tool that has reached nearly $1 billion in annualized revenue since launching earlier this year.Those numbers come as artificial intelligence enters what many in the industry characterize as a critical inflection point. After two years of what Anthropic Chief Product Officer Mike Krieger recently described as \"AI FOMO\" — where companies adopted AI tools without clear success metrics — enterprises are now demanding measurable returns on investment.\"The best products can be grounded in some kind of success metric or evaluation,\" Krieger said on the \"Superhuman AI\" podcast. \"I&#x27;ve seen that a lot in talking to companies that are deploying AI.\"For enterprises evaluating AI tools, the calculus increasingly centers on concrete productivity gains. Google CEO Sundar Pichai claimed in June that AI had generated a 10% boost in engineering velocity at his company — though measuring such improvements across different roles and use cases remains challenging, as Krieger acknowledged.Why AI safety testing matters more than ever for enterprise adoptionAnthropic&#x27;s launch comes amid heightened scrutiny of the company&#x27;s approach to AI safety and regulation. On Tuesday, David Sacks, the White House&#x27;s AI \"czar\" and a venture capitalist, accused Anthropic of \"running a sophisticated regulatory capture strategy based on fear-mongering\" that is \"damaging the startup ecosystem.\"The attack targeted remarks by Jack Clark, Anthropic&#x27;s British co-founder and head of policy, who had described being \"deeply afraid\" of AI&#x27;s trajectory. Clark told Bloomberg he found Sacks&#x27; criticism \"perplexing.\"Anthropic addressed such concerns head-on in its release materials, emphasizing that Haiku 4.5 underwent extensive safety testing. The company classified the model as ASL-2 — its AI Safety Level 2 standard — compared to the more restrictive ASL-3 designation for the more powerful Sonnet 4.5 and Opus 4.1 models.\"Our teams have red-teamed and tested our agentic capabilities to the limits in order to assess whether it can be used to engage in harmful activity like generating misinformation or promoting fraudulent behavior like scams,\" the spokesperson told VentureBeat. \"In our automated alignment assessment, it showed a statistically significantly lower overall rate of misaligned behaviors than both Claude Sonnet 4.5 and Claude Opus 4.1 — making it, by this metric, our safest model yet.\"The company said its safety testing showed Haiku 4.5 poses only limited risks regarding the production of chemical, biological, radiological and nuclear weapons. Anthropic has also implemented classifiers designed to detect and filter prompt injection attacks, a common method for attempting to manipulate AI systems into producing harmful content.The emphasis on safety reflects Anthropic&#x27;s founding mission. The company was established in 2021 by former OpenAI executives, including siblings Dario and Daniela Amodei, who left amid concerns about OpenAI&#x27;s direction following its partnership with Microsoft. Anthropic has positioned itself as taking a more cautious, research-oriented approach to AI development.Benchmark results show Haiku 4.5 competing with larger, more expensive modelsAccording to Anthropic&#x27;s benchmarks, Haiku 4.5 performs competitively with or exceeds several larger models across multiple evaluation criteria. On SWE-bench Verified, a widely used test measuring AI systems&#x27; ability to solve real-world software engineering problems, Haiku 4.5 scored 73.3% — slightly ahead of Sonnet 4&#x27;s 72.7% and close to GPT-5 Codex&#x27;s 74.5%.The model demonstrated particular strength in computer use tasks, achieving 50.7% on the OSWorld benchmark compared to Sonnet 4&#x27;s 42.2%. This capability allows the AI to interact directly with computer interfaces — clicking buttons, filling forms, navigating applications — which could prove transformative for automating routine digital tasks.In coding-specific benchmarks like Terminal-Bench, which tests AI agents&#x27; ability to complete complex software tasks using command-line tools, Haiku 4.5 scored 41.0%, trailing only Sonnet 4.5&#x27;s 50.0% among Claude models.The model maintains a 200,000-token context window for standard users, with developers accessing the Claude Developer Platform able to use a 1-million-token context window. That expanded capacity means the model can process extremely large codebases or documents in a single request — roughly equivalent to a 1,500-page book.What three major AI model releases in two months says about the competitionWhen asked about the rapid succession of model releases, the Anthropic spokesperson emphasized the company&#x27;s focus on execution rather than competitive positioning.\"We&#x27;re focused on shipping the best possible products for our customers — and our shipping velocity speaks for itself,\" the spokesperson said. \"What was state-of-the-art just five months ago is now faster, cheaper, and more accessible.\"That velocity stands in contrast to the company&#x27;s earlier, more measured release schedule. Anthropic appeared to have paused development of its Haiku line after releasing version 3.5 at the end of last year, leading some observers to speculate the company had deprioritized smaller models.That rapid price-performance improvement validates a core promise of artificial intelligence: that capabilities will become dramatically cheaper over time as the technology matures and companies optimize their models. For enterprises, it suggests that today&#x27;s budget constraints around AI deployment may ease considerably in coming years.From customer service to code: Real-world applications for faster, cheaper AIThe practical applications of Haiku 4.5 span a wide range of enterprise functions, from customer service to financial analysis to software development. The model&#x27;s combination of speed and intelligence makes it particularly suited for real-time, low-latency tasks like chatbot conversations and customer support interactions, where delays of even a few seconds can degrade user experience.In financial services, the multi-agent architecture enabled by pairing Sonnet 4.5 with Haiku 4.5 could transform how firms monitor markets and manage risk. Anthropic envisions Haiku 4.5 monitoring thousands of data streams simultaneously — tracking regulatory changes, market signals and portfolio risks — while Sonnet 4.5 handles complex predictive modeling and strategic analysis.For research organizations, the division of labor could compress timelines dramatically. Sonnet 4.5 might orchestrate a comprehensive analysis while multiple Haiku 4.5 agents parallelize literature reviews, data gathering and document synthesis across dozens of sources, potentially \"compressing weeks of research into hours,\" according to Anthropic&#x27;s use case descriptions.Several companies have already integrated Haiku 4.5 and reported positive results. Guy Gur-Ari, co-founder of coding startup Augment, said the model \"hit a sweet spot we didn&#x27;t think was possible: near-frontier coding quality with blazing speed and cost efficiency.\" In Augment&#x27;s internal testing, Haiku 4.5 achieved 90% of Sonnet 4.5&#x27;s performance while matching much larger models.Jeff Wang, CEO of Windsurf, another coding-focused startup, said Haiku 4.5 \"is blurring the lines\" on traditional trade-offs between speed, cost and quality. \"It&#x27;s a fast frontier model that keeps costs efficient and signals where this class of models is headed.\"Jon Noronha, co-founder of presentation software company Gamma, reported that Haiku 4.5 \"outperformed our current models on instruction-following for slide text generation, achieving 65% accuracy versus 44% from our premium tier model — that&#x27;s a game-changer for our unit economics.\"The price of progress: What plummeting AI costs mean for enterprise strategyFor enterprises evaluating AI strategies, Haiku 4.5 presents both opportunity and challenge. The opportunity lies in accessing sophisticated AI capabilities at dramatically lower costs, potentially making viable entire categories of applications that were previously too expensive to deploy at scale.The challenge is keeping pace with a technology landscape that is evolving faster than most organizations can absorb. As Krieger noted in his recent podcast appearance, companies are moving beyond \"AI FOMO\" to demand concrete metrics and demonstrated value. But establishing those metrics and evaluation frameworks takes time — time that may be in short supply as competitors race ahead.The shift from single-model deployments to multi-agent architectures also requires new ways of thinking about AI systems. Rather than viewing AI as a monolithic assistant, enterprises must learn to orchestrate multiple specialized agents, each optimized for particular tasks — more akin to managing a team than operating a tool.The fundamental economics of AI are shifting with remarkable speed. Five months ago, Sonnet 4&#x27;s capabilities commanded premium pricing and represented the cutting edge. Today, Haiku 4.5 delivers similar performance at a third of the cost. If that trajectory continues — and both Anthropic&#x27;s release schedule and competitive pressure from OpenAI and Google suggest it will — the AI capabilities that seem remarkable today may be routine and inexpensive within a year.For Anthropic, the challenge will be translating technical achievements into sustainable business growth while maintaining the safety-focused approach that differentiates it from competitors. The company&#x27;s projected revenue growth to as much as $26 billion by 2026 suggests strong market traction, but achieving those targets will require continued innovation and successful execution across an increasingly complex product portfolio.Whether enterprises will choose Claude over increasingly capable alternatives from OpenAI, Google and a growing field of competitors remains an open question. But Anthropic is making a clear bet: that the future of AI belongs not to whoever builds the single most powerful model, but to whoever can deliver the right intelligence, at the right speed, at the right price — and make it accessible to everyone.In an industry where the promise of artificial intelligence has long outpaced reality, Anthropic is betting that delivering on that promise, faster and cheaper than anyone expected, will be enough to win. And with pricing dropping by two-thirds in just five months while performance holds steady, that promise is starting to look like reality.",
          "content": "Anthropic released Claude Haiku 4.5 on Wednesday, a smaller and significantly cheaper artificial intelligence model that matches the coding capabilities of systems that were considered cutting-edge just months ago, marking the latest salvo in an intensifying competition to dominate enterprise AI.The model costs $1 per million input tokens and $5 per million output tokens — roughly one-third the price of Anthropic&#x27;s mid-sized Sonnet 4 model released in May, while operating more than twice as fast. In certain tasks, particularly operating computers autonomously, Haiku 4.5 actually surpasses its more expensive predecessor.\"Haiku 4.5 is a clear leap in performance and is now largely as smart as Sonnet 4 while being significantly faster and one-third of the cost,\" an Anthropic spokesperson told VentureBeat, underscoring how rapidly AI capabilities are becoming commoditized as the technology matures.The launch comes just two weeks after Anthropic released Claude Sonnet 4.5, which the company bills as the world&#x27;s best coding model, and two months after introducing Opus 4.1. The breakneck pace of releases reflects mounting pressure from OpenAI, whose $500 billion valuation dwarfs Anthropic&#x27;s $183 billion, and which has inked a series of multibillion-dollar infrastructure deals while expanding its product lineup.How free access to advanced AI could reshape the enterprise marketIn an unusual move that could reshape competitive dynamics in the AI market, Anthropic is making Haiku 4.5 available for all free users of its Claude.ai platform. The decision effectively democratizes access to what the company characterizes as \"near-frontier-level intelligence\" — capabilities that would have been available only in expensive, premium models months ago.\"The launch of Claude Haiku 4.5 means that near-frontier-level intelligence is available for free to all users through Claude.ai,\" the Anthropic spokesperson told VentureBeat. \"It also offers significant advantages to our enterprise customers: Sonnet 4.5 can handle frontier planning while Haiku 4.5 powers sub-agents, enabling multi-agent systems that tackle complex refactors, migrations, and large features builds with speed and quality.\"This multi-agent architecture signals a significant shift in how AI systems are deployed. Rather than relying on a single, monolithic model, enterprises can now orchestrate teams of specialized AI agents: a more sophisticated Sonnet 4.5 model breaking down complex problems and delegating subtasks to multiple Haiku 4.5 agents working in parallel. For software development teams, this could mean Sonnet 4.5 plans a major code refactoring while Haiku 4.5 agents simultaneously execute changes across dozens of files.The approach mirrors how human organizations distribute work, and could prove particularly valuable for enterprises seeking to balance performance with cost efficiency — a critical consideration as AI deployment scales.Inside Anthropic&#x27;s path to $7 billion in annual revenueThe model launch coincides with revelations that Anthropic&#x27;s business is experiencing explosive growth. The company&#x27;s annual revenue run rate is approaching $7 billion this month, Anthropic told Reuters, up from more than $5 billion reported in August. Internal projections obtained by Reuters suggest the company is targeting between $20 billion and $26 billion in annualized revenue for 2026, representing growth of more than 200% to nearly 300%.The company now serves more than 300,000 business customers, with enterprise products accounting for approximately 80% of revenue. Among Anthropic&#x27;s most successful offerings is Claude Code, a code-generation tool that has reached nearly $1 billion in annualized revenue since launching earlier this year.Those numbers come as artificial intelligence enters what many in the industry characterize as a critical inflection point. After two years of what Anthropic Chief Product Officer Mike Krieger recently described as \"AI FOMO\" — where companies adopted AI tools without clear success metrics — enterprises are now demanding measurable returns on investment.\"The best products can be grounded in some kind of success metric or evaluation,\" Krieger said on the \"Superhuman AI\" podcast. \"I&#x27;ve seen that a lot in talking to companies that are deploying AI.\"For enterprises evaluating AI tools, the calculus increasingly centers on concrete productivity gains. Google CEO Sundar Pichai claimed in June that AI had generated a 10% boost in engineering velocity at his company — though measuring such improvements across different roles and use cases remains challenging, as Krieger acknowledged.Why AI safety testing matters more than ever for enterprise adoptionAnthropic&#x27;s launch comes amid heightened scrutiny of the company&#x27;s approach to AI safety and regulation. On Tuesday, David Sacks, the White House&#x27;s AI \"czar\" and a venture capitalist, accused Anthropic of \"running a sophisticated regulatory capture strategy based on fear-mongering\" that is \"damaging the startup ecosystem.\"The attack targeted remarks by Jack Clark, Anthropic&#x27;s British co-founder and head of policy, who had described being \"deeply afraid\" of AI&#x27;s trajectory. Clark told Bloomberg he found Sacks&#x27; criticism \"perplexing.\"Anthropic addressed such concerns head-on in its release materials, emphasizing that Haiku 4.5 underwent extensive safety testing. The company classified the model as ASL-2 — its AI Safety Level 2 standard — compared to the more restrictive ASL-3 designation for the more powerful Sonnet 4.5 and Opus 4.1 models.\"Our teams have red-teamed and tested our agentic capabilities to the limits in order to assess whether it can be used to engage in harmful activity like generating misinformation or promoting fraudulent behavior like scams,\" the spokesperson told VentureBeat. \"In our automated alignment assessment, it showed a statistically significantly lower overall rate of misaligned behaviors than both Claude Sonnet 4.5 and Claude Opus 4.1 — making it, by this metric, our safest model yet.\"The company said its safety testing showed Haiku 4.5 poses only limited risks regarding the production of chemical, biological, radiological and nuclear weapons. Anthropic has also implemented classifiers designed to detect and filter prompt injection attacks, a common method for attempting to manipulate AI systems into producing harmful content.The emphasis on safety reflects Anthropic&#x27;s founding mission. The company was established in 2021 by former OpenAI executives, including siblings Dario and Daniela Amodei, who left amid concerns about OpenAI&#x27;s direction following its partnership with Microsoft. Anthropic has positioned itself as taking a more cautious, research-oriented approach to AI development.Benchmark results show Haiku 4.5 competing with larger, more expensive modelsAccording to Anthropic&#x27;s benchmarks, Haiku 4.5 performs competitively with or exceeds several larger models across multiple evaluation criteria. On SWE-bench Verified, a widely used test measuring AI systems&#x27; ability to solve real-world software engineering problems, Haiku 4.5 scored 73.3% — slightly ahead of Sonnet 4&#x27;s 72.7% and close to GPT-5 Codex&#x27;s 74.5%.The model demonstrated particular strength in computer use tasks, achieving 50.7% on the OSWorld benchmark compared to Sonnet 4&#x27;s 42.2%. This capability allows the AI to interact directly with computer interfaces — clicking buttons, filling forms, navigating applications — which could prove transformative for automating routine digital tasks.In coding-specific benchmarks like Terminal-Bench, which tests AI agents&#x27; ability to complete complex software tasks using command-line tools, Haiku 4.5 scored 41.0%, trailing only Sonnet 4.5&#x27;s 50.0% among Claude models.The model maintains a 200,000-token context window for standard users, with developers accessing the Claude Developer Platform able to use a 1-million-token context window. That expanded capacity means the model can process extremely large codebases or documents in a single request — roughly equivalent to a 1,500-page book.What three major AI model releases in two months says about the competitionWhen asked about the rapid succession of model releases, the Anthropic spokesperson emphasized the company&#x27;s focus on execution rather than competitive positioning.\"We&#x27;re focused on shipping the best possible products for our customers — and our shipping velocity speaks for itself,\" the spokesperson said. \"What was state-of-the-art just five months ago is now faster, cheaper, and more accessible.\"That velocity stands in contrast to the company&#x27;s earlier, more measured release schedule. Anthropic appeared to have paused development of its Haiku line after releasing version 3.5 at the end of last year, leading some observers to speculate the company had deprioritized smaller models.That rapid price-performance improvement validates a core promise of artificial intelligence: that capabilities will become dramatically cheaper over time as the technology matures and companies optimize their models. For enterprises, it suggests that today&#x27;s budget constraints around AI deployment may ease considerably in coming years.From customer service to code: Real-world applications for faster, cheaper AIThe practical applications of Haiku 4.5 span a wide range of enterprise functions, from customer service to financial analysis to software development. The model&#x27;s combination of speed and intelligence makes it particularly suited for real-time, low-latency tasks like chatbot conversations and customer support interactions, where delays of even a few seconds can degrade user experience.In financial services, the multi-agent architecture enabled by pairing Sonnet 4.5 with Haiku 4.5 could transform how firms monitor markets and manage risk. Anthropic envisions Haiku 4.5 monitoring thousands of data streams simultaneously — tracking regulatory changes, market signals and portfolio risks — while Sonnet 4.5 handles complex predictive modeling and strategic analysis.For research organizations, the division of labor could compress timelines dramatically. Sonnet 4.5 might orchestrate a comprehensive analysis while multiple Haiku 4.5 agents parallelize literature reviews, data gathering and document synthesis across dozens of sources, potentially \"compressing weeks of research into hours,\" according to Anthropic&#x27;s use case descriptions.Several companies have already integrated Haiku 4.5 and reported positive results. Guy Gur-Ari, co-founder of coding startup Augment, said the model \"hit a sweet spot we didn&#x27;t think was possible: near-frontier coding quality with blazing speed and cost efficiency.\" In Augment&#x27;s internal testing, Haiku 4.5 achieved 90% of Sonnet 4.5&#x27;s performance while matching much larger models.Jeff Wang, CEO of Windsurf, another coding-focused startup, said Haiku 4.5 \"is blurring the lines\" on traditional trade-offs between speed, cost and quality. \"It&#x27;s a fast frontier model that keeps costs efficient and signals where this class of models is headed.\"Jon Noronha, co-founder of presentation software company Gamma, reported that Haiku 4.5 \"outperformed our current models on instruction-following for slide text generation, achieving 65% accuracy versus 44% from our premium tier model — that&#x27;s a game-changer for our unit economics.\"The price of progress: What plummeting AI costs mean for enterprise strategyFor enterprises evaluating AI strategies, Haiku 4.5 presents both opportunity and challenge. The opportunity lies in accessing sophisticated AI capabilities at dramatically lower costs, potentially making viable entire categories of applications that were previously too expensive to deploy at scale.The challenge is keeping pace with a technology landscape that is evolving faster than most organizations can absorb. As Krieger noted in his recent podcast appearance, companies are moving beyond \"AI FOMO\" to demand concrete metrics and demonstrated value. But establishing those metrics and evaluation frameworks takes time — time that may be in short supply as competitors race ahead.The shift from single-model deployments to multi-agent architectures also requires new ways of thinking about AI systems. Rather than viewing AI as a monolithic assistant, enterprises must learn to orchestrate multiple specialized agents, each optimized for particular tasks — more akin to managing a team than operating a tool.The fundamental economics of AI are shifting with remarkable speed. Five months ago, Sonnet 4&#x27;s capabilities commanded premium pricing and represented the cutting edge. Today, Haiku 4.5 delivers similar performance at a third of the cost. If that trajectory continues — and both Anthropic&#x27;s release schedule and competitive pressure from OpenAI and Google suggest it will — the AI capabilities that seem remarkable today may be routine and inexpensive within a year.For Anthropic, the challenge will be translating technical achievements into sustainable business growth while maintaining the safety-focused approach that differentiates it from competitors. The company&#x27;s projected revenue growth to as much as $26 billion by 2026 suggests strong market traction, but achieving those targets will require continued innovation and successful execution across an increasingly complex product portfolio.Whether enterprises will choose Claude over increasingly capable alternatives from OpenAI, Google and a growing field of competitors remains an open question. But Anthropic is making a clear bet: that the future of AI belongs not to whoever builds the single most powerful model, but to whoever can deliver the right intelligence, at the right speed, at the right price — and make it accessible to everyone.In an industry where the promise of artificial intelligence has long outpaced reality, Anthropic is betting that delivering on that promise, faster and cheaper than anyone expected, will be enough to win. And with pricing dropping by two-thirds in just five months while performance holds steady, that promise is starting to look like reality.",
          "feed_position": 7,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/2zkV6Ua9HruRsjBAfq9oS2/3c44bad93ee87a4a9c84b418279acccb/nuneybits_Vector_art_of_a_computer_image_in_burnt_orange_58827742-c30a-4256-9cf3-c7802772a6c7.webp"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/google-releases-new-ai-video-model-veo-3-1-in-flow-and-api-what-it-means-for",
          "published_at": "Wed, 15 Oct 2025 18:50:00 GMT",
          "title": "Google releases new AI video model Veo 3.1 in Flow and API: what it means for enterprises",
          "standfirst": "As expected after days of leaks and rumors online, Google has unveiled Veo 3.1, its latest AI video generation model, bringing a suite of creative and technical upgrades aimed at improving narrative control, audio integration, and realism in AI-generated video. While the updates expand possibilities for hobbyists and content creators using Google’s online AI creation app, Flow, the release also signals a growing opportunity for enterprises, developers, and creative teams seeking scalable, customizable video tools.The quality is higher, the physics better, the pricing the same as before, and the control and editing features more robust and varied.My initial tests showed it to be a powerful and performant model that immediately delights with each generation. However, the look is more cinematic, polished and a little more \"artificial\" than by default than rivals such as OpenAI&#x27;s new Sora 2, released late last month, which may or may not be what a particular user is going after (Sora excels at handheld and \"candid\" style videos). Expanded Control Over Narrative and AudioVeo 3.1 builds on its predecessor, Veo 3 (released back in May 2025) with enhanced support for dialogue, ambient sound, and other audio effects. Native audio generation is now available across several key features in Flow, including “Frames to Video,” “Ingredients to Video,” and “Extend,\" which give users the ability to, respectively: turn still images into video; use items, characters and objects from multiple images in a single video; and generate longer clips than the initial 8 seconds, to more than 30 seconds or even 1+ plus when continuing from a prior clip&#x27;s final frame. Before, you had to add audio manually after using these features. This addition gives users greater command over tone, emotion, and storytelling — capabilities that have previously required post-production work.In enterprise contexts, this level of control may reduce the need for separate audio pipelines, offering an integrated way to create training content, marketing videos, or digital experiences with synchronized sound and visuals.Google noted in a blog post that the updates reflect user feedback calling for deeper artistic control and improved audio support. Gallegos emphasizes the importance of making edits and refinements possible directly in Flow, without reworking scenes from scratch.Richer Inputs and Editing CapabilitiesWith Veo 3.1, Google introduces support for multiple input types and more granular control over generated outputs. The model accepts text prompts, images, and video clips as input, and also supports:Reference images (up to three) to guide appearance and style in the final outputFirst and last frame interpolation to generate seamless scenes between fixed endpointsScene extension that continues a video’s action or motion beyond its current durationThese tools aim to give enterprise users a way to fine-tune the look and feel of their content—useful for brand consistency or adherence to creative briefs.Additional capabilities like “Insert” (add objects to scenes) and “Remove” (delete elements or characters) are also being introduced, though not all are immediately available through the Gemini API.Deployment Across PlatformsVeo 3.1 is accessible through several of Google’s existing AI services:Flow, Google’s own interface for AI-assisted filmmakingGemini API, targeted at developers building video capabilities into applicationsVertex AI, where enterprise integration will soon support Veo’s “Scene Extension” and other key featuresAvailability through these platforms allows enterprise customers to choose the right environment—GUI-based or programmatic—based on their teams and workflows.Pricing and AccessThe Veo 3.1 model is currently in preview and available only on the paid tier of the Gemini API. The cost structure is the same as Veo 3, the preceding generation of AI video models from Google.Standard model: $0.40 per second of videoFast model: $0.15 per secondThere is no free tier, and users are charged only if a video is successfully generated. This model is consistent with previous Veo versions and provides predictable pricing for budget-conscious enterprise teams.Technical Specs and Output ControlVeo 3.1 outputs video at 720p or 1080p resolution, with a 24 fps frame rate. Duration options include 4, 6, or 8 seconds from a text prompt or uploaded images, with the ability to extend videos up to 148 seconds (more than 2 and half minutes!) when using the “Extend” feature.New functionality also includes tighter control over subjects and environments. For example, enterprises can upload a product image or visual reference, and Veo 3.1 will generate scenes that preserve its appearance and stylistic cues across the video. This could streamline creative production pipelines for retail, advertising, and virtual content production teams.Initial ReactionsThe broader creator and developer community has responded to Veo 3.1’s launch with a mix of optimism and tempered critique—particularly when comparing it to rival models like OpenAI’s Sora 2.Matt Shumer, an AI founder of Otherside AI/Hyperwrite, and early adopter, described his initial reaction as “disappointment,” noting that Veo 3.1 is “noticeably worse than Sora 2” and also “quite a bit more expensive.”However, he acknowledged that Google’s tooling—such as support for references and scene extension—is a bright spot in the release.Travis Davids, a 3D digital artist and AI content creator, echoed some of that sentiment. While he noted improvements in audio quality, particularly in sound effects and dialogue, he raised concerns about limitations that remain in the system. These include the lack of custom voice support, an inability to select generated voices directly, and the continued cap at 8-second generations—despite some public claims about longer outputs.Davids also pointed out that character consistency across changing camera angles still requires careful prompting, whereas other models like Sora 2 handle this more automatically. He questioned the absence of 1080p resolution for users on paid tiers like Flow Pro and expressed skepticism over feature parity.On the more positive end, @kimmonismus, an AI newsletter writer, stated that “Veo 3.1 is amazing,” though still concluded that OpenAI’s latest model remains preferable overall.Collectively, these early impressions suggest that while Veo 3.1 delivers meaningful tooling enhancements and new creative control features, expectations have shifted as competitors raise the bar on both quality and usability.Adoption and ScaleSince launching Flow five months ago, Google says over 275 million videos have been generated across various Veo models. The pace of adoption suggests significant interest not only from individuals but also from developers and businesses experimenting with automated content creation.Thomas Iljic, Director of Product Management at Google Labs, highlights that Veo 3.1’s release brings capabilities closer to how human filmmakers plan and shoot. These include scene composition, continuity across shots, and coordinated audio—all areas that enterprises increasingly look to automate or streamline.Safety and Responsible AI UseVideos generated with Veo 3.1 are watermarked using Google’s SynthID technology, which embeds an imperceptible identifier to signal that the content is AI-generated. Google applies safety filters and moderation across its APIs to help minimize privacy and copyright risks. Generated content is stored temporarily and deleted after two days unless downloaded.For developers and enterprises, these features provide reassurance around provenance and compliance—critical in regulated or brand-sensitive industries.Where Veo 3.1 Stands Among a Crowded AI Video Model SpaceVeo 3.1 is not just an iteration on prior models—it represents a deeper integration of multimodal inputs, storytelling control, and enterprise-level tooling. While creative professionals may see immediate benefits in editing workflows and fidelity, businesses exploring automation in training, advertising, or virtual experiences may find even greater value in the model’s composability and API support.The early user feedback highlights that while Veo 3.1 offers valuable tooling, expectations around realism, voice control, and generation length are evolving rapidly. As Google expands access through Vertex AI and continues refining Veo, its competitive positioning in enterprise video generation will hinge on how quickly these user pain points are addressed.",
          "content": "As expected after days of leaks and rumors online, Google has unveiled Veo 3.1, its latest AI video generation model, bringing a suite of creative and technical upgrades aimed at improving narrative control, audio integration, and realism in AI-generated video. While the updates expand possibilities for hobbyists and content creators using Google’s online AI creation app, Flow, the release also signals a growing opportunity for enterprises, developers, and creative teams seeking scalable, customizable video tools.The quality is higher, the physics better, the pricing the same as before, and the control and editing features more robust and varied.My initial tests showed it to be a powerful and performant model that immediately delights with each generation. However, the look is more cinematic, polished and a little more \"artificial\" than by default than rivals such as OpenAI&#x27;s new Sora 2, released late last month, which may or may not be what a particular user is going after (Sora excels at handheld and \"candid\" style videos). Expanded Control Over Narrative and AudioVeo 3.1 builds on its predecessor, Veo 3 (released back in May 2025) with enhanced support for dialogue, ambient sound, and other audio effects. Native audio generation is now available across several key features in Flow, including “Frames to Video,” “Ingredients to Video,” and “Extend,\" which give users the ability to, respectively: turn still images into video; use items, characters and objects from multiple images in a single video; and generate longer clips than the initial 8 seconds, to more than 30 seconds or even 1+ plus when continuing from a prior clip&#x27;s final frame. Before, you had to add audio manually after using these features. This addition gives users greater command over tone, emotion, and storytelling — capabilities that have previously required post-production work.In enterprise contexts, this level of control may reduce the need for separate audio pipelines, offering an integrated way to create training content, marketing videos, or digital experiences with synchronized sound and visuals.Google noted in a blog post that the updates reflect user feedback calling for deeper artistic control and improved audio support. Gallegos emphasizes the importance of making edits and refinements possible directly in Flow, without reworking scenes from scratch.Richer Inputs and Editing CapabilitiesWith Veo 3.1, Google introduces support for multiple input types and more granular control over generated outputs. The model accepts text prompts, images, and video clips as input, and also supports:Reference images (up to three) to guide appearance and style in the final outputFirst and last frame interpolation to generate seamless scenes between fixed endpointsScene extension that continues a video’s action or motion beyond its current durationThese tools aim to give enterprise users a way to fine-tune the look and feel of their content—useful for brand consistency or adherence to creative briefs.Additional capabilities like “Insert” (add objects to scenes) and “Remove” (delete elements or characters) are also being introduced, though not all are immediately available through the Gemini API.Deployment Across PlatformsVeo 3.1 is accessible through several of Google’s existing AI services:Flow, Google’s own interface for AI-assisted filmmakingGemini API, targeted at developers building video capabilities into applicationsVertex AI, where enterprise integration will soon support Veo’s “Scene Extension” and other key featuresAvailability through these platforms allows enterprise customers to choose the right environment—GUI-based or programmatic—based on their teams and workflows.Pricing and AccessThe Veo 3.1 model is currently in preview and available only on the paid tier of the Gemini API. The cost structure is the same as Veo 3, the preceding generation of AI video models from Google.Standard model: $0.40 per second of videoFast model: $0.15 per secondThere is no free tier, and users are charged only if a video is successfully generated. This model is consistent with previous Veo versions and provides predictable pricing for budget-conscious enterprise teams.Technical Specs and Output ControlVeo 3.1 outputs video at 720p or 1080p resolution, with a 24 fps frame rate. Duration options include 4, 6, or 8 seconds from a text prompt or uploaded images, with the ability to extend videos up to 148 seconds (more than 2 and half minutes!) when using the “Extend” feature.New functionality also includes tighter control over subjects and environments. For example, enterprises can upload a product image or visual reference, and Veo 3.1 will generate scenes that preserve its appearance and stylistic cues across the video. This could streamline creative production pipelines for retail, advertising, and virtual content production teams.Initial ReactionsThe broader creator and developer community has responded to Veo 3.1’s launch with a mix of optimism and tempered critique—particularly when comparing it to rival models like OpenAI’s Sora 2.Matt Shumer, an AI founder of Otherside AI/Hyperwrite, and early adopter, described his initial reaction as “disappointment,” noting that Veo 3.1 is “noticeably worse than Sora 2” and also “quite a bit more expensive.”However, he acknowledged that Google’s tooling—such as support for references and scene extension—is a bright spot in the release.Travis Davids, a 3D digital artist and AI content creator, echoed some of that sentiment. While he noted improvements in audio quality, particularly in sound effects and dialogue, he raised concerns about limitations that remain in the system. These include the lack of custom voice support, an inability to select generated voices directly, and the continued cap at 8-second generations—despite some public claims about longer outputs.Davids also pointed out that character consistency across changing camera angles still requires careful prompting, whereas other models like Sora 2 handle this more automatically. He questioned the absence of 1080p resolution for users on paid tiers like Flow Pro and expressed skepticism over feature parity.On the more positive end, @kimmonismus, an AI newsletter writer, stated that “Veo 3.1 is amazing,” though still concluded that OpenAI’s latest model remains preferable overall.Collectively, these early impressions suggest that while Veo 3.1 delivers meaningful tooling enhancements and new creative control features, expectations have shifted as competitors raise the bar on both quality and usability.Adoption and ScaleSince launching Flow five months ago, Google says over 275 million videos have been generated across various Veo models. The pace of adoption suggests significant interest not only from individuals but also from developers and businesses experimenting with automated content creation.Thomas Iljic, Director of Product Management at Google Labs, highlights that Veo 3.1’s release brings capabilities closer to how human filmmakers plan and shoot. These include scene composition, continuity across shots, and coordinated audio—all areas that enterprises increasingly look to automate or streamline.Safety and Responsible AI UseVideos generated with Veo 3.1 are watermarked using Google’s SynthID technology, which embeds an imperceptible identifier to signal that the content is AI-generated. Google applies safety filters and moderation across its APIs to help minimize privacy and copyright risks. Generated content is stored temporarily and deleted after two days unless downloaded.For developers and enterprises, these features provide reassurance around provenance and compliance—critical in regulated or brand-sensitive industries.Where Veo 3.1 Stands Among a Crowded AI Video Model SpaceVeo 3.1 is not just an iteration on prior models—it represents a deeper integration of multimodal inputs, storytelling control, and enterprise-level tooling. While creative professionals may see immediate benefits in editing workflows and fidelity, businesses exploring automation in training, advertising, or virtual experiences may find even greater value in the model’s composability and API support.The early user feedback highlights that while Veo 3.1 offers valuable tooling, expectations around realism, voice control, and generation length are evolving rapidly. As Google expands access through Vertex AI and continues refining Veo, its competitive positioning in enterprise video generation will hinge on how quickly these user pain points are addressed.",
          "feed_position": 8,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/4tvp7qfEbQOW0pucNGc3oW/21a8f215813133c934f9e71c0acff280/Screenshot_2025-10-15_at_2.53.23%C3%A2__PM.png"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/home/home-theater/the-latest-roku-update-adds-ai-powered-voice-control-and-better-search-184513277.html",
          "published_at": "Wed, 15 Oct 2025 18:45:14 +0000",
          "title": "The latest Roku update adds AI-powered voice control and better search",
          "standfirst": "Roku just announced a robust software update coming to many of its devices. These are free upgrades, with search getting a major AI boost. First of all, the pre-existing voice search feature is getting \"AI smarts.\" This means that users will be able to ask contextual questions about movies, actors and shows. It's worth noting that the bot will deliver the answers on-screen and not via a digital voice. This can be used to help find something to watch, as Roku gives examples of people asking how scary a movie is or if something is safe for children to watch. Roku A search bar is being added to the live TV page and it will scour all of the platform's free live channels to find a match. The search tool will also now give more accurate information as to which streaming platform to choose based on active subscriptions and what people are looking for. The dedicated Roku Sports page will be able to track live scores and will let viewers keep tabs on multiple games at once, thanks to matchup tiles. This can be shut off to keep things spoiler-free. Roku The company's pre-existing Bluetooth Headphone Mode is rolling out to more devices, including the Roku Streaming Stick and Streaming Stick Plus. This lets people connect headphones directly without having to use the dedicated app. Both of those made our list of the best streaming devices, so more functionality is always appreciated. Roku The app is getting an overhaul, with new shortcuts and the ability to rate shows and movies directly within the app. The company has also revealed that Philips Ambilight technology is now available on Roku TVs in the US. This tech automatically changes the ambient lighting based on what's being shown on-screen. All of these software updates will be available to Roku devices in the coming months. The company also recently refreshed many of its midrange TVs with pro-level features. New Roku Plus Series TVs now allow for hands-free voice control and a feature that automatically adjusts the picture on a scene-by-scene basis.This article originally appeared on Engadget at https://www.engadget.com/home/home-theater/the-latest-roku-update-adds-ai-powered-voice-control-and-better-search-184513277.html?src=rss",
          "content": "Roku just announced a robust software update coming to many of its devices. These are free upgrades, with search getting a major AI boost. First of all, the pre-existing voice search feature is getting \"AI smarts.\" This means that users will be able to ask contextual questions about movies, actors and shows. It's worth noting that the bot will deliver the answers on-screen and not via a digital voice. This can be used to help find something to watch, as Roku gives examples of people asking how scary a movie is or if something is safe for children to watch. Roku A search bar is being added to the live TV page and it will scour all of the platform's free live channels to find a match. The search tool will also now give more accurate information as to which streaming platform to choose based on active subscriptions and what people are looking for. The dedicated Roku Sports page will be able to track live scores and will let viewers keep tabs on multiple games at once, thanks to matchup tiles. This can be shut off to keep things spoiler-free. Roku The company's pre-existing Bluetooth Headphone Mode is rolling out to more devices, including the Roku Streaming Stick and Streaming Stick Plus. This lets people connect headphones directly without having to use the dedicated app. Both of those made our list of the best streaming devices, so more functionality is always appreciated. Roku The app is getting an overhaul, with new shortcuts and the ability to rate shows and movies directly within the app. The company has also revealed that Philips Ambilight technology is now available on Roku TVs in the US. This tech automatically changes the ambient lighting based on what's being shown on-screen. All of these software updates will be available to Roku devices in the coming months. The company also recently refreshed many of its midrange TVs with pro-level features. New Roku Plus Series TVs now allow for hands-free voice control and a feature that automatically adjusts the picture on a scene-by-scene basis.This article originally appeared on Engadget at https://www.engadget.com/home/home-theater/the-latest-roku-update-adds-ai-powered-voice-control-and-better-search-184513277.html?src=rss",
          "feed_position": 35,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2025-10/f1c4f4a0-a9ef-11f0-a537-fa041eca4a2c"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/cybersecurity/vpn/best-vpn-130004396.html",
          "published_at": "Wed, 15 Oct 2025 17:13:14 +0000",
          "title": "The best VPN service for 2025",
          "standfirst": "As frustrating as it is that governments and businesses are running roughshod over our online freedoms, at least we have plenty of good VPNs to choose from to keep us protected online. There are so many fast, intelligently designed, full-featured and affordable services on the market that the biggest problem is picking one. For any use case, you can bet at least two providers will be neck-and-neck for first place.On the other hand, the VPN world is still the Wild West in some ways. It's easy enough to slap a cheap VPN together that the market is flooded with low-quality apps that put more money into advertising than infrastructure. They may look good, but it's all styrofoam under the hood.I built this list of the best VPNs after intensive testing to help you reorient your focus on the providers that actually deserve your time and money. Which one truly fits your needs is dependent on who you are and what you do online, but if you pick any of my seven recommendations, you can't go too far wrong.For each VPN on this list, I've shared which platforms it works on, how much it cuts into your download speed, where it offers servers, what other features are included and how much the best available deal costs. At the end, I'll list some honorable and dishonorable mentions, then answer some of the most common questions I hear about VPNs.Editor's note: This list has been completely overhauled and rewritten as of September 2025. We intend to revisit this list every three months at a minimum, at which time our picks may be adjusted based on changes in pricing, features, testing results and other factors. Table of contents Best VPNs for 2025 Other VPNs we tested What to look for in a VPN VPN FAQs Best VPNs for 2025 Other VPNs we tested The VPNs in this section didn't crack our top list above, but we're summarizing them here so you can see their positives and negatives as of the time of our evaluation. Windscribe Windscribe is another well-known free VPN supported by paid subscriptions. In many ways, it takes the best from both Mullvad and Proton VPN, with the former's no-nonsense privacy and the latter's healthy free plan. Without paying, you can connect to 10 of Windscribe's server locations on an unlimited number of devices at once. Unfortunately, Windscribe didn't copy the most important part of Proton VPN's free plan — the unlimited data. You're only allowed to use 10GB per month, which isn't enough for regular streaming. It's also committed to a cramped and headache-inducing user interface that stands out from the crowd in all the worst ways. CyberGhost There's a lot to recommend with CyberGhost. Its streaming-optimized servers meet a high standard for playback quality, it's pretty fast overall and its Smart Rules offer some of the deepest VPN automation on the market. The ad blocker works well, and the NoSpy servers are a neat idea — CyberGhost keeps them under lock and key near its Romania headquarters and carefully fine-tunes all their settings. On the other hand, its apps just aren't up to the standard set by our favorites on this list. I like the designs on paper, but there are too many snags in the experience, from laggy connections to an overactive kill switch that often blocks internet access even when the VPN is working perfectly. I also have some concerns about its commitment to data privacy, since its privacy policy retains the right to share your personal data with its entire corporate family. CyberGhost's parent company, Kape Technologies, also owns ExpressVPN and Private Internet Access, but neither of those VPNs have privacy policies quite so permissive. TunnelBear TunnelBear has a decent interface, which its target audience of VPN beginners will find very easy to use. Its speeds are perfectly good too, and I appreciate the depth and breadth of its transparency reports. But it's far too limited overall, with few extra features, less than 50 server locations and a free plan that caps data at 2GB per month. VyprVPN VyprVPN often flies under the radar, but it has some of the best apps in the business and a very good security record (there was a breach in 2023, but it didn't crack the VPN encryption itself). It's also got a verified privacy policy, a solid jurisdiction and runs every connection through an in-house DNS to prevent leaks. Despite all that, it didn't make the top seven because its connection speeds aren't up to scratch — you'll likely notice a bigger slowdown than average. It also has a troubling history of wild, seemingly experimental swings in its pricing and simultaneous connection limits. Norton VPN Norton VPN is part of the Norton 360 package that includes the well-known antivirus software and other security apps. It's a nice bonus if you use Norton already, but as a standalone VPN, it falls short. My tests repeatedly showed it dropping encryption and revealing my IP address whenever I switched servers, and not all of its locations managed to unblock Netflix. This isn't to say Norton VPN is terrible. It has a fairly large server network, user-friendly apps and some cool features like an IP rotator. It also recently revamped its OpenVPN infrastructure to improve speeds on Windows. But you probably won't find those things sufficient to balance out significant speed drops on other platforms or poorly written FAQs. I especially advise against Norton VPN for Apple users, as its Mac and iPhone apps are much more limited than their Windows and Android counterparts. What to look for in a VPN Choosing a VPN can quickly get you mired in analysis paralysis. We're here to help, but since only you know your particular needs, you should know the major red and green flags so you can make the final call yourself. Every reputable VPN provider offers a free trial or refund guarantee you can use to run the tests below. Compatibility: First, make sure your VPN works on all the platforms you plan to use it on. Most VPNs have apps for Windows, Mac, Android and iOS, but those apps aren't always created equal — check that the app for your chosen OS is user-friendly and has all the features you need. Speed: Use a speed testing app to see how fast your internet is before and after connecting to the VPN (I use Ookla's speedtest.net). To check security, look up your IP address while connected to a VPN server and see if it's actually changed your virtual location. Be sure it's using expert-vetted protocols like OpenVPN, WireGuard and IKEv2. Try connecting to streaming services and seeing whether the VPN changes the available content. Background: Do some outside research into the VPN's origins, especially its parent company, privacy policy and any past incidents. It's a dealbreaker if you can't figure out where the VPN is headquartered (which indicates a lax approach to transparency) or if it seems to have never passed a real third-party audit. Server network: Look at the server network to make sure the VPN has locations near you and in any countries where you'll want an IP address — e.g. if you need a VPN to unblock Canadian Netflix, look for multiple server locations in Canada. Customer Service: I also advise testing the customer support options by looking for the answer to a straightforward question. If phone support (versus email and chat) is important to you, make sure to prioritize that — and make sure it's available at convenient times in your timezone. Pricing: Finally, check prices. See if the VPN is affordable and decide whether you're comfortable taking a long-term subscription for better savings. If you do get a multi-year plan, check what price it will renew at, since many of the cheapest subscriptions are only introductory deals. VPN FAQs To wrap up, let's answer some of the most common questions we get about VPNs. Feel free to get in touch if you have a query I don't cover here. What is a VPN? VPN stands for virtual private network. There are a few different types of VPNs, but for this list, we're talking about commercial services that let individual users access the internet with an assumed identity. Whenever you get online, you're assigned an IP address — a digital nametag that tells websites where to send the information you request. For an IP address to work, it needs to be unique, which means it's possible to create a record of what an individual does online. When you use a VPN, all the data you send to the internet goes through one of the VPN's servers before heading to its final destination. The VPN encrypts the connection between your computer and its server so the data won't trace back to you. Any website, ISP or third party that cares to look will only see the VPN's IP address, not yours. What are some things VPNs are used for? The three main use cases for a commercial VPN are security, privacy and entertainment. Using a VPN conceals your real IP address from anyone who might want to use it for nefarious purposes like cyberstalking, DDoS attacks or deducing your real location. It also keeps your ISP from profiling you for ads based on where you live or what you do online. One side effect of borrowing a VPN's IP address is that you can make it appear as though your connection is coming from another country. You can use this to access streaming content and platforms that are only available in certain regions due to copyright. Changing your location can even get you better prices when shopping online. Location spoofing can also be used to get online in countries that censor internet access, like China and Russia, as well as certain US states or countries — like the UK — that are adding barriers like age-gating to previously unfettered online access. All you have to do is connect to a neighboring country (or locality) where the internet isn't blocked. If you plan to do this while traveling, make sure you have the VPN downloaded before you go, as some nations prevent you from even loading a VPN's homepage. Make sure you check with local laws regarding the legality of VPN use as well — just because your VPN traffic is encrypted doesn't mean that authorities can't detect that it's being used in a given location. Are VPNs worth it? Whether a VPN is worth the price depends on how much you value those three use cases above. It's no secret that your personal information is profitable for a lot of people, from illicit hackers to corporations to law enforcement. A VPN will not make you completely anonymous, nor is it a license to commit crimes (see the next question) but it will give you a lot more control over what you transmit to the world. With entertainment, the value is even clearer. You can use a VPN to fight back against streaming balkanization by getting more shows and movies out of a single platform — for example, a lot of shows that have been kicked off American Netflix are still on Netflix in other countries. What information does a VPN hide? A VPN does not make it impossible for you to be unmasked or taken advantage of online. It prevents you from passively leaking information, keeps your IP address undiscoverable on public wi-fi networks and gets you around online censorship. However, if you share personal information of your own volition, there's nothing the VPN can do. If you reveal your password in a social media post or click a link in a phishing email, that information bypasses the VPN. Likewise, if you do anything sensitive while logged into an account, the account holder will have that information even if you're using a VPN. A VPN is a critical part of your online security, but it can't do the whole job by itself. Healthy passwords, malware scanners, private search engines and common sense all have roles to play. Never forget, too, that using a VPN means trusting the VPN provider with access to information that's concealed from everyone else — make sure you trust the privacy policy before signing up. Are VPNs safe? As far as we can determine, all the VPNs recommended in this story are safe to use. As with anything you subscribe to online, due diligence is important, but there's very little inherent risk; generally, the worst thing a bad VPN will do is fail to work, leaving you no worse off than before. There are some VPNs (usually offered for free) that transmit malware, so always make sure to look up any complaints or warnings about a service before you download it. Can you get a VPN on your phone? Absolutely — almost every VPN has apps for both desktop and mobile devices. A good VPN will redesign its app to be mobile-friendly without dropping too many features. Both iOS and Android natively support VPN connections, so you're free to choose whichever provider you like. What about Google's One VPN? Google One VPN was, as you might expect, a VPN provided by Google. It was launched in 2020 for Google One subscribers and discontinued in 2024 due to lack of use. If you really want a Google VPN, you can still get one if you have certain Pixel models or if you're a Google Fi subscriber. That said, I don't recommend using a VPN from Google even if you do still have access to one. Google is one of the worst big tech companies at protecting user privacy. While its VPN might not leak, I wouldn't trust it to guard your sensitive information.This article originally appeared on Engadget at https://www.engadget.com/cybersecurity/vpn/best-vpn-130004396.html?src=rss",
          "content": "As frustrating as it is that governments and businesses are running roughshod over our online freedoms, at least we have plenty of good VPNs to choose from to keep us protected online. There are so many fast, intelligently designed, full-featured and affordable services on the market that the biggest problem is picking one. For any use case, you can bet at least two providers will be neck-and-neck for first place.On the other hand, the VPN world is still the Wild West in some ways. It's easy enough to slap a cheap VPN together that the market is flooded with low-quality apps that put more money into advertising than infrastructure. They may look good, but it's all styrofoam under the hood.I built this list of the best VPNs after intensive testing to help you reorient your focus on the providers that actually deserve your time and money. Which one truly fits your needs is dependent on who you are and what you do online, but if you pick any of my seven recommendations, you can't go too far wrong.For each VPN on this list, I've shared which platforms it works on, how much it cuts into your download speed, where it offers servers, what other features are included and how much the best available deal costs. At the end, I'll list some honorable and dishonorable mentions, then answer some of the most common questions I hear about VPNs.Editor's note: This list has been completely overhauled and rewritten as of September 2025. We intend to revisit this list every three months at a minimum, at which time our picks may be adjusted based on changes in pricing, features, testing results and other factors. Table of contents Best VPNs for 2025 Other VPNs we tested What to look for in a VPN VPN FAQs Best VPNs for 2025 Other VPNs we tested The VPNs in this section didn't crack our top list above, but we're summarizing them here so you can see their positives and negatives as of the time of our evaluation. Windscribe Windscribe is another well-known free VPN supported by paid subscriptions. In many ways, it takes the best from both Mullvad and Proton VPN, with the former's no-nonsense privacy and the latter's healthy free plan. Without paying, you can connect to 10 of Windscribe's server locations on an unlimited number of devices at once. Unfortunately, Windscribe didn't copy the most important part of Proton VPN's free plan — the unlimited data. You're only allowed to use 10GB per month, which isn't enough for regular streaming. It's also committed to a cramped and headache-inducing user interface that stands out from the crowd in all the worst ways. CyberGhost There's a lot to recommend with CyberGhost. Its streaming-optimized servers meet a high standard for playback quality, it's pretty fast overall and its Smart Rules offer some of the deepest VPN automation on the market. The ad blocker works well, and the NoSpy servers are a neat idea — CyberGhost keeps them under lock and key near its Romania headquarters and carefully fine-tunes all their settings. On the other hand, its apps just aren't up to the standard set by our favorites on this list. I like the designs on paper, but there are too many snags in the experience, from laggy connections to an overactive kill switch that often blocks internet access even when the VPN is working perfectly. I also have some concerns about its commitment to data privacy, since its privacy policy retains the right to share your personal data with its entire corporate family. CyberGhost's parent company, Kape Technologies, also owns ExpressVPN and Private Internet Access, but neither of those VPNs have privacy policies quite so permissive. TunnelBear TunnelBear has a decent interface, which its target audience of VPN beginners will find very easy to use. Its speeds are perfectly good too, and I appreciate the depth and breadth of its transparency reports. But it's far too limited overall, with few extra features, less than 50 server locations and a free plan that caps data at 2GB per month. VyprVPN VyprVPN often flies under the radar, but it has some of the best apps in the business and a very good security record (there was a breach in 2023, but it didn't crack the VPN encryption itself). It's also got a verified privacy policy, a solid jurisdiction and runs every connection through an in-house DNS to prevent leaks. Despite all that, it didn't make the top seven because its connection speeds aren't up to scratch — you'll likely notice a bigger slowdown than average. It also has a troubling history of wild, seemingly experimental swings in its pricing and simultaneous connection limits. Norton VPN Norton VPN is part of the Norton 360 package that includes the well-known antivirus software and other security apps. It's a nice bonus if you use Norton already, but as a standalone VPN, it falls short. My tests repeatedly showed it dropping encryption and revealing my IP address whenever I switched servers, and not all of its locations managed to unblock Netflix. This isn't to say Norton VPN is terrible. It has a fairly large server network, user-friendly apps and some cool features like an IP rotator. It also recently revamped its OpenVPN infrastructure to improve speeds on Windows. But you probably won't find those things sufficient to balance out significant speed drops on other platforms or poorly written FAQs. I especially advise against Norton VPN for Apple users, as its Mac and iPhone apps are much more limited than their Windows and Android counterparts. What to look for in a VPN Choosing a VPN can quickly get you mired in analysis paralysis. We're here to help, but since only you know your particular needs, you should know the major red and green flags so you can make the final call yourself. Every reputable VPN provider offers a free trial or refund guarantee you can use to run the tests below. Compatibility: First, make sure your VPN works on all the platforms you plan to use it on. Most VPNs have apps for Windows, Mac, Android and iOS, but those apps aren't always created equal — check that the app for your chosen OS is user-friendly and has all the features you need. Speed: Use a speed testing app to see how fast your internet is before and after connecting to the VPN (I use Ookla's speedtest.net). To check security, look up your IP address while connected to a VPN server and see if it's actually changed your virtual location. Be sure it's using expert-vetted protocols like OpenVPN, WireGuard and IKEv2. Try connecting to streaming services and seeing whether the VPN changes the available content. Background: Do some outside research into the VPN's origins, especially its parent company, privacy policy and any past incidents. It's a dealbreaker if you can't figure out where the VPN is headquartered (which indicates a lax approach to transparency) or if it seems to have never passed a real third-party audit. Server network: Look at the server network to make sure the VPN has locations near you and in any countries where you'll want an IP address — e.g. if you need a VPN to unblock Canadian Netflix, look for multiple server locations in Canada. Customer Service: I also advise testing the customer support options by looking for the answer to a straightforward question. If phone support (versus email and chat) is important to you, make sure to prioritize that — and make sure it's available at convenient times in your timezone. Pricing: Finally, check prices. See if the VPN is affordable and decide whether you're comfortable taking a long-term subscription for better savings. If you do get a multi-year plan, check what price it will renew at, since many of the cheapest subscriptions are only introductory deals. VPN FAQs To wrap up, let's answer some of the most common questions we get about VPNs. Feel free to get in touch if you have a query I don't cover here. What is a VPN? VPN stands for virtual private network. There are a few different types of VPNs, but for this list, we're talking about commercial services that let individual users access the internet with an assumed identity. Whenever you get online, you're assigned an IP address — a digital nametag that tells websites where to send the information you request. For an IP address to work, it needs to be unique, which means it's possible to create a record of what an individual does online. When you use a VPN, all the data you send to the internet goes through one of the VPN's servers before heading to its final destination. The VPN encrypts the connection between your computer and its server so the data won't trace back to you. Any website, ISP or third party that cares to look will only see the VPN's IP address, not yours. What are some things VPNs are used for? The three main use cases for a commercial VPN are security, privacy and entertainment. Using a VPN conceals your real IP address from anyone who might want to use it for nefarious purposes like cyberstalking, DDoS attacks or deducing your real location. It also keeps your ISP from profiling you for ads based on where you live or what you do online. One side effect of borrowing a VPN's IP address is that you can make it appear as though your connection is coming from another country. You can use this to access streaming content and platforms that are only available in certain regions due to copyright. Changing your location can even get you better prices when shopping online. Location spoofing can also be used to get online in countries that censor internet access, like China and Russia, as well as certain US states or countries — like the UK — that are adding barriers like age-gating to previously unfettered online access. All you have to do is connect to a neighboring country (or locality) where the internet isn't blocked. If you plan to do this while traveling, make sure you have the VPN downloaded before you go, as some nations prevent you from even loading a VPN's homepage. Make sure you check with local laws regarding the legality of VPN use as well — just because your VPN traffic is encrypted doesn't mean that authorities can't detect that it's being used in a given location. Are VPNs worth it? Whether a VPN is worth the price depends on how much you value those three use cases above. It's no secret that your personal information is profitable for a lot of people, from illicit hackers to corporations to law enforcement. A VPN will not make you completely anonymous, nor is it a license to commit crimes (see the next question) but it will give you a lot more control over what you transmit to the world. With entertainment, the value is even clearer. You can use a VPN to fight back against streaming balkanization by getting more shows and movies out of a single platform — for example, a lot of shows that have been kicked off American Netflix are still on Netflix in other countries. What information does a VPN hide? A VPN does not make it impossible for you to be unmasked or taken advantage of online. It prevents you from passively leaking information, keeps your IP address undiscoverable on public wi-fi networks and gets you around online censorship. However, if you share personal information of your own volition, there's nothing the VPN can do. If you reveal your password in a social media post or click a link in a phishing email, that information bypasses the VPN. Likewise, if you do anything sensitive while logged into an account, the account holder will have that information even if you're using a VPN. A VPN is a critical part of your online security, but it can't do the whole job by itself. Healthy passwords, malware scanners, private search engines and common sense all have roles to play. Never forget, too, that using a VPN means trusting the VPN provider with access to information that's concealed from everyone else — make sure you trust the privacy policy before signing up. Are VPNs safe? As far as we can determine, all the VPNs recommended in this story are safe to use. As with anything you subscribe to online, due diligence is important, but there's very little inherent risk; generally, the worst thing a bad VPN will do is fail to work, leaving you no worse off than before. There are some VPNs (usually offered for free) that transmit malware, so always make sure to look up any complaints or warnings about a service before you download it. Can you get a VPN on your phone? Absolutely — almost every VPN has apps for both desktop and mobile devices. A good VPN will redesign its app to be mobile-friendly without dropping too many features. Both iOS and Android natively support VPN connections, so you're free to choose whichever provider you like. What about Google's One VPN? Google One VPN was, as you might expect, a VPN provided by Google. It was launched in 2020 for Google One subscribers and discontinued in 2024 due to lack of use. If you really want a Google VPN, you can still get one if you have certain Pixel models or if you're a Google Fi subscriber. That said, I don't recommend using a VPN from Google even if you do still have access to one. Google is one of the worst big tech companies at protecting user privacy. While its VPN might not leak, I wouldn't trust it to guard your sensitive information.This article originally appeared on Engadget at https://www.engadget.com/cybersecurity/vpn/best-vpn-130004396.html?src=rss",
          "feed_position": 36
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/gaming/ball-x-pits-deeply-satisfying-grind-keeps-me-coming-back-for-more-171000754.html",
          "published_at": "Wed, 15 Oct 2025 17:10:00 +0000",
          "title": "Ball x Pit's deeply satisfying grind keeps me coming back for more",
          "standfirst": "For as long as I can remember, I've had trouble going to sleep. When I lay down, my mind inevitably starts racing a thousand miles an hour, thinking about anything and everything. On several recent nights, though, my pre-slumber thoughts had a singular focus. I mulled over possibilities like, \"What if I fuse a ball that heals my character with one that splits into smaller balls with the same effect, and add a passive that fires a baby ball every time I'm healed?\" Then I grab my PlayStation Portal and do just that until I doze off. This is the hold Ball x Pit has had over me. Kenny Sun and a small group of collaborators have cooked up a mesmerizing brick-breaking roguelite. Ball x Pit is a blend of dual-stick shoot-'em-up action, base building and about a dozen other things that keeps calling me back for one more run... and another, and another.After a cataclysmic event wipes out the city of Ballbylon and leaves an enormous pit, hunters descend into the depths in search of treasure. For our purposes, this means playing levels to collect resources in order to build structures in New Ballbylon. These buildings unlock perks, such as new characters, that help with future runs. The sickly chaos of the levels and the calmer city building aspect feed into each other smartly and combine for a satisfying loop.Base building in Ball x PitKenny Sun/Devolver DigitalThat’s not the only important interplay here. Like any good roguelite, Ball x Pit is all about finding synergies for maximum impact. It's right there in the title, with the \"x\" denoting a relationship between two things (it's derived from shipping in fandom parlance). In the pit, you battle monsters by — surprise! — firing balls at them. Along with regular “baby balls,” each character has a unique ability and a special starter ball. In the vein of Vampire Survivors, you'll unlock more special balls and passive abilities when you collect enough gems to level up. One ball has a chance to freeze enemies and another is slower but deals much more damage. There are dozens of others.The real fun comes in when you start fusing these balls and their effects together, freeing up space for another weapon. It gets even better when you're able to evolve a pair of balls into something new. It's possible to fuse evolved balls, or even evolve them again. There’s a strategic aspect to this, as you won’t want to fuse balls that can nearly cancel each other out, such as merging an area-of-effect ball with one that disappears on impact, or leave yourself with too few balls in the face of danger.Once I unlocked the option to take two characters on a run and combine their passive abilities, that's where things went into overdrive. The possibilities became very exciting at this point, and I ended up playing Ball x Pit way past my ideal bedtime as a result.It does take a while to get to that point, though. Progression is slow at first. The repetition can get to be a little much as you need to beat each stage multiple times before moving onto the next. Also, I wish there was a bit more to the game narratively than a basic setup and some character descriptions. But there's so much to consider on each run, and that’s what keeps me coming back. Each of the nine levels is set in a different biome, with its own hazards, enemies and bosses. Some late-game characters turn the game on its head by shifting playstyles in surprising ways, but I don't want to spoil those. Along with the absolute chaos and dopamine hits of slicing through enemies, discovering killer combinations between characters, special balls — especially the evolutions — and passives drives so much of the joy of this game.Whenever you do fuse or evolve a pair of balls, rather than having to click an \"OK\" button to get back in the action, the prompt reads \"Whoa.\" That's on the nose, but funny. And I'll be damned if I didn't say that very thing out loud many times when I saw what a new evolved ball could do. Ball x Pit is out now on Steam, PS5, Xbox Series X/S and Nintendo Switch for $15. It's available via Game Pass Ultimate and PC Game Pass. A Nintendo Switch 2 version is coming later this fall with a free upgrade from the Switch version.This article originally appeared on Engadget at https://www.engadget.com/gaming/ball-x-pits-deeply-satisfying-grind-keeps-me-coming-back-for-more-171000754.html?src=rss",
          "content": "For as long as I can remember, I've had trouble going to sleep. When I lay down, my mind inevitably starts racing a thousand miles an hour, thinking about anything and everything. On several recent nights, though, my pre-slumber thoughts had a singular focus. I mulled over possibilities like, \"What if I fuse a ball that heals my character with one that splits into smaller balls with the same effect, and add a passive that fires a baby ball every time I'm healed?\" Then I grab my PlayStation Portal and do just that until I doze off. This is the hold Ball x Pit has had over me. Kenny Sun and a small group of collaborators have cooked up a mesmerizing brick-breaking roguelite. Ball x Pit is a blend of dual-stick shoot-'em-up action, base building and about a dozen other things that keeps calling me back for one more run... and another, and another.After a cataclysmic event wipes out the city of Ballbylon and leaves an enormous pit, hunters descend into the depths in search of treasure. For our purposes, this means playing levels to collect resources in order to build structures in New Ballbylon. These buildings unlock perks, such as new characters, that help with future runs. The sickly chaos of the levels and the calmer city building aspect feed into each other smartly and combine for a satisfying loop.Base building in Ball x PitKenny Sun/Devolver DigitalThat’s not the only important interplay here. Like any good roguelite, Ball x Pit is all about finding synergies for maximum impact. It's right there in the title, with the \"x\" denoting a relationship between two things (it's derived from shipping in fandom parlance). In the pit, you battle monsters by — surprise! — firing balls at them. Along with regular “baby balls,” each character has a unique ability and a special starter ball. In the vein of Vampire Survivors, you'll unlock more special balls and passive abilities when you collect enough gems to level up. One ball has a chance to freeze enemies and another is slower but deals much more damage. There are dozens of others.The real fun comes in when you start fusing these balls and their effects together, freeing up space for another weapon. It gets even better when you're able to evolve a pair of balls into something new. It's possible to fuse evolved balls, or even evolve them again. There’s a strategic aspect to this, as you won’t want to fuse balls that can nearly cancel each other out, such as merging an area-of-effect ball with one that disappears on impact, or leave yourself with too few balls in the face of danger.Once I unlocked the option to take two characters on a run and combine their passive abilities, that's where things went into overdrive. The possibilities became very exciting at this point, and I ended up playing Ball x Pit way past my ideal bedtime as a result.It does take a while to get to that point, though. Progression is slow at first. The repetition can get to be a little much as you need to beat each stage multiple times before moving onto the next. Also, I wish there was a bit more to the game narratively than a basic setup and some character descriptions. But there's so much to consider on each run, and that’s what keeps me coming back. Each of the nine levels is set in a different biome, with its own hazards, enemies and bosses. Some late-game characters turn the game on its head by shifting playstyles in surprising ways, but I don't want to spoil those. Along with the absolute chaos and dopamine hits of slicing through enemies, discovering killer combinations between characters, special balls — especially the evolutions — and passives drives so much of the joy of this game.Whenever you do fuse or evolve a pair of balls, rather than having to click an \"OK\" button to get back in the action, the prompt reads \"Whoa.\" That's on the nose, but funny. And I'll be damned if I didn't say that very thing out loud many times when I saw what a new evolved ball could do. Ball x Pit is out now on Steam, PS5, Xbox Series X/S and Nintendo Switch for $15. It's available via Game Pass Ultimate and PC Game Pass. A Nintendo Switch 2 version is coming later this fall with a free upgrade from the Switch version.This article originally appeared on Engadget at https://www.engadget.com/gaming/ball-x-pits-deeply-satisfying-grind-keeps-me-coming-back-for-more-171000754.html?src=rss",
          "feed_position": 37,
          "image_url": "https://d29szjachogqwa.cloudfront.net/videos/user-uploaded/ball_x_pit_base.jpg"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/cybersecurity/vpn/how-to-cancel-proton-vpn-and-get-a-refund-170014128.html",
          "published_at": "Wed, 15 Oct 2025 17:00:14 +0000",
          "title": "How to cancel Proton VPN and get a refund",
          "standfirst": "Proton VPN currently tops my list of the best VPNs, and I gave it a glowing recommendation in my detailed Proton VPN review. It's easy to use, fast, cheap and secure, with a large server network and one of the industry's best scores at unblocking streaming sites. All that said, there's no such thing as a perfect VPN, and you may find that Proton isn't working for you. If that happens, here's how to cancel your subscription. How to cancel Proton VPN through a browser If you initially signed up less than 30 days ago, you can cancel your subscription and request a refund by contacting tech support. See \"How to get a refund from Proton VPN\" below for details. If more than 30 days have passed, use the following steps to cancel your subscription. Open your browser and go to protonvpn.com. At the top-right, click Sign in, then enter your username and password. You'll be taken to your account dashboard. At the left side of the dashboard, click the Subscription tab. Scroll all the way down to the section labeled Cancel subscription. Click the \"Continue\" button. A pop-up window will appear, asking if you're sure. Click Cancel subscription. Sam Chapman for Engadget Cancelling this way doesn't immediately terminate service — it just means your subscription won't auto-renew. You can still use Proton VPN's paid features, including the entire server network, until the current period expires. After that, you'll be automatically downgraded to the free plan. During this time, your account dashboard will still be active, so you can use it to turn renewal back on if you change your mind. This method also works in mobile browser apps. Just follow the same steps on your mobile device and you'll cancel in the same way, with service continuing until your subscription expires. How to downgrade from Proton Unlimited to Proton VPN only A Proton Unlimited subscription applies to all Proton products. Since it's mainly founded on Proton Mail, though, downgrading to VPN service only is tricky and requires some extra steps. First, downgrade Proton Unlimited to Proton Free from your main account dashboard. Log in at account.proton.me, then click Settings, All settings, Dashboard and Your plan. Under \"Proton Unlimited,\" click Explore other Proton plans. On the next page, select Proton Free. This will effectively cancel Proton Unlimited, though you can still use it until the end of the pay period. Finally, go to the Proton VPN website (not the overall Proton site) and sign up for a Proton VPN Plus plan. Since you downgraded instead of deleting your account, you should be able to use the same account address. How to delete your Proton account on mobile You can also use the mobile app to delete your entire Proton account, instantly and irreversibly ending your subscriptions to Proton VPN, Proton Mail and any other products in the line. Taking this action permanently burns your Proton username, so you won't be able to use it again if you decide to re-subscribe (in that case, you’ll just have to make a new one). The process is the same on both Android and iOS — the apps have slight cosmetic differences, but everything is in the same place. Follow the steps below to nuke your Proton account from orbit. Open the Proton VPN app on your Android phone. Click Settings at the bottom of the window. At the top of the settings page, click your account email address. This will take you to Account settings. Click Delete account. A window will open in your web browser, showing your general Proton account page. Scroll down to the bottom of this page and click the red Delete your account button. Select a reason for deleting your account and leave feedback in the box provided. You have to pick an option from the menu and type at least 10 characters in the box, though feel free to keyboard mash if you don't want to say anything. Check the box on the next page beside \"Yes, I want to permanently delete this account and all its data.\" Finally, click the red Delete account button. Sam Chapman for Engadget How to get a refund from Proton VPN Proton VPN has a 30-day refund policy. As long as you paid for your VPN less than 30 days ago, you can get your money back. To request a refund, send a message through the contact form on the website. You can also email protonvpn@support.zendesk.com. There's a minimum of 100 characters in the \"What happened?\" box. Unlike when you're simply deleting your account, I do recommend putting a brief real answer here, clearly stating that you would like to cancel your account and receive a refund. Sam Chapman for Engadget According to its terms of service, Proton will only refund you for the portion of the service you didn't use. If you spend $10 for a one-month subscription and cancel after 15 days, you'll get $5 back. The terms do state that the company \"may also provide you with a full refund upon request\" — directly asking for such a refund in your contact form makes this more likely. If you cancel after 30 days are up, you may still be able to get a prorated payment for your remaining time, either in cash or account credit. You'll have to ask for this specifically, as the default option is that your account just stays active until it runs out. What to do if you subscribed through an app store If you bought your Proton VPN subscription through the Apple App Store or Google Play Store, then Apple or Google processed your money and you're subject to their refund policies. If you subscribed through Apple, go to your Apple ID profile in iOS settings, click on Subscriptions, scroll to Proton VPN and click on Cancel subscription. You'll then get the opportunity to request a refund. On Android, log into the Google Play Store, click on your profile picture, then click Manage subscriptions. Find Proton VPN, click Cancel subscription and provide a reason. As with iOS, the steps will walk you through the refund process. Proton VPN alternatives Once you've fully cancelled Proton VPN, you may be in the market for an alternative. I recommend a few of my other favorites, depending on why Proton didn't work for you. Surfshark is faster, ExpressVPN has some of the best app design and NordVPN has a wider range of interesting features.This article originally appeared on Engadget at https://www.engadget.com/cybersecurity/vpn/how-to-cancel-proton-vpn-and-get-a-refund-170014128.html?src=rss",
          "content": "Proton VPN currently tops my list of the best VPNs, and I gave it a glowing recommendation in my detailed Proton VPN review. It's easy to use, fast, cheap and secure, with a large server network and one of the industry's best scores at unblocking streaming sites. All that said, there's no such thing as a perfect VPN, and you may find that Proton isn't working for you. If that happens, here's how to cancel your subscription. How to cancel Proton VPN through a browser If you initially signed up less than 30 days ago, you can cancel your subscription and request a refund by contacting tech support. See \"How to get a refund from Proton VPN\" below for details. If more than 30 days have passed, use the following steps to cancel your subscription. Open your browser and go to protonvpn.com. At the top-right, click Sign in, then enter your username and password. You'll be taken to your account dashboard. At the left side of the dashboard, click the Subscription tab. Scroll all the way down to the section labeled Cancel subscription. Click the \"Continue\" button. A pop-up window will appear, asking if you're sure. Click Cancel subscription. Sam Chapman for Engadget Cancelling this way doesn't immediately terminate service — it just means your subscription won't auto-renew. You can still use Proton VPN's paid features, including the entire server network, until the current period expires. After that, you'll be automatically downgraded to the free plan. During this time, your account dashboard will still be active, so you can use it to turn renewal back on if you change your mind. This method also works in mobile browser apps. Just follow the same steps on your mobile device and you'll cancel in the same way, with service continuing until your subscription expires. How to downgrade from Proton Unlimited to Proton VPN only A Proton Unlimited subscription applies to all Proton products. Since it's mainly founded on Proton Mail, though, downgrading to VPN service only is tricky and requires some extra steps. First, downgrade Proton Unlimited to Proton Free from your main account dashboard. Log in at account.proton.me, then click Settings, All settings, Dashboard and Your plan. Under \"Proton Unlimited,\" click Explore other Proton plans. On the next page, select Proton Free. This will effectively cancel Proton Unlimited, though you can still use it until the end of the pay period. Finally, go to the Proton VPN website (not the overall Proton site) and sign up for a Proton VPN Plus plan. Since you downgraded instead of deleting your account, you should be able to use the same account address. How to delete your Proton account on mobile You can also use the mobile app to delete your entire Proton account, instantly and irreversibly ending your subscriptions to Proton VPN, Proton Mail and any other products in the line. Taking this action permanently burns your Proton username, so you won't be able to use it again if you decide to re-subscribe (in that case, you’ll just have to make a new one). The process is the same on both Android and iOS — the apps have slight cosmetic differences, but everything is in the same place. Follow the steps below to nuke your Proton account from orbit. Open the Proton VPN app on your Android phone. Click Settings at the bottom of the window. At the top of the settings page, click your account email address. This will take you to Account settings. Click Delete account. A window will open in your web browser, showing your general Proton account page. Scroll down to the bottom of this page and click the red Delete your account button. Select a reason for deleting your account and leave feedback in the box provided. You have to pick an option from the menu and type at least 10 characters in the box, though feel free to keyboard mash if you don't want to say anything. Check the box on the next page beside \"Yes, I want to permanently delete this account and all its data.\" Finally, click the red Delete account button. Sam Chapman for Engadget How to get a refund from Proton VPN Proton VPN has a 30-day refund policy. As long as you paid for your VPN less than 30 days ago, you can get your money back. To request a refund, send a message through the contact form on the website. You can also email protonvpn@support.zendesk.com. There's a minimum of 100 characters in the \"What happened?\" box. Unlike when you're simply deleting your account, I do recommend putting a brief real answer here, clearly stating that you would like to cancel your account and receive a refund. Sam Chapman for Engadget According to its terms of service, Proton will only refund you for the portion of the service you didn't use. If you spend $10 for a one-month subscription and cancel after 15 days, you'll get $5 back. The terms do state that the company \"may also provide you with a full refund upon request\" — directly asking for such a refund in your contact form makes this more likely. If you cancel after 30 days are up, you may still be able to get a prorated payment for your remaining time, either in cash or account credit. You'll have to ask for this specifically, as the default option is that your account just stays active until it runs out. What to do if you subscribed through an app store If you bought your Proton VPN subscription through the Apple App Store or Google Play Store, then Apple or Google processed your money and you're subject to their refund policies. If you subscribed through Apple, go to your Apple ID profile in iOS settings, click on Subscriptions, scroll to Proton VPN and click on Cancel subscription. You'll then get the opportunity to request a refund. On Android, log into the Google Play Store, click on your profile picture, then click Manage subscriptions. Find Proton VPN, click Cancel subscription and provide a reason. As with iOS, the steps will walk you through the refund process. Proton VPN alternatives Once you've fully cancelled Proton VPN, you may be in the market for an alternative. I recommend a few of my other favorites, depending on why Proton didn't work for you. Surfshark is faster, ExpressVPN has some of the best app design and NordVPN has a wider range of interesting features.This article originally appeared on Engadget at https://www.engadget.com/cybersecurity/vpn/how-to-cancel-proton-vpn-and-get-a-refund-170014128.html?src=rss",
          "feed_position": 38,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2025-10/f5cc5570-a9d6-11f0-bbdf-90572b26b776"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/apps/vsco-adds-its-first-ai-powered-photo-editing-tool-170000055.html",
          "published_at": "Wed, 15 Oct 2025 17:00:00 +0000",
          "title": "VSCO adds its first AI-powered photo editing tool",
          "standfirst": "The VSCO photo editing and sharing app has been around for nearly as long as Instagram, positioning itself as the serious photographer’s choice for mobile editing. The original focus was on tasteful filters and editing tools, all of which got significantly more powerful and flexible over time; VSCO has long been doing the same sort of film emulations that have made Fujifilm’s cameras so desirable in recent years. The company also built up a loyal community of photographers who share their edits far and wide, both in the VSCO app as well as on more mainstream platforms like Instagram itself. Now, the company is making an unsurprising but potentially controversial move: it is releasing its first AI-powered image editing tool. “Remove,” as the name suggests, lets you erase “unwanted elements” from your photos without compromising the image’s full resolution. At first glance, it feels quite similar to tools like Google’s own Magic Eraser. You just pop open an image in the editor and highlight the portion you want to remove, and VSCO will do its best to obliterate the offending bits and fill in whatever is in the background that it deems appropriate. I haven’t had a chance to test how effective this tool is yet, but VSCO is using Black Forest Lab’s FLUX.1 Kontext model to do its magic, combined with its own proprietary technology specifically focused on making results that the company says look authentic. A quick look at Black Forest Lab and the FLUX.1 model show a tool that does appear to be well-suited to removing unwanted parts of an image and properly filling in the space that remains — but we’ll have to see it in action to judge whether it does the job well.This new Remove tool isn’t the only AI-powered editor VSCO is working on. There’s also an Upscale tool in the works that the company says will “enhance image resolution” while keeping color and composition unchanged. These sorts of tools will live under a new umbrella the company is calling AI Lab, making it clear this will be an ongoing initiative and not just a one-off release.On one hand, I’m not at all surprised to see VSCO jumping into AI-powered editing; it has to keep up with the rest of the industry. But on the other hand, the company has made its mark by building a community of photographers who value authenticity in their work, something that cannot help but be in conflict with AI tools, at least on the surface.VSCO’s CEO Eric Wittman acknowledged that tension in a conversation with Engadget. “We have a very photographer-centric, creator-first point of view,” Wittman said. “But where we see AI fitting in is in support of those folks, and that work, and that vision. The intention isn’t to replace [that work], though — AI has a place, but it’s not to replace what creators, and photographers in particular, are doing.” That mindset makes sense with something like Remove, which duplicates something people have done with Photoshop for years. Rather than generating new images or radically changing the truth of a photo like you can do with some of Google’s tools on the Pixel phones, Remove is a bit more subtle. “You would use masks, you would manually painstakingly edit things at a pixel by pixel level,” Wittman said. “What a lot of Remove tools would do is basically like automate that.”Wittman also cited preserving image quality as a key part of the work behind its own Remove tool. “We know that many people who were attempting to use AI in the early days, especially photographers, a lot of their disappointment was just in the preservation of the integrity and the quality of the work,” he said. “So what we've really tried to do is continue to help automate where we can and make things easier, but also preserve the quality.” To that end, VSCO is stressing that all these edits are non-destructive and the output will be in full, original resolution. As VSCO starts dabbling in more AI editing tools, Wittman emphasized that the company wants to stay on the side of helping photographers realize a creative vision rather than helping them make entirely unreal images, while also avoiding the mess of copyright issues and inauthentic content that is flooding the internet thanks to AI. “When you think about things like copyright, and the incredible importance of copyright, integrity, and authenticity — we're big believers as a company in both the laws and the norms that have been around for many many years. But obviously on some platforms there are people who are maliciously manipulating things, and we don't want to be participants in that.”VSCO’s first AI Labs feature is available as of today in the VSCO app for iOS; it should come to Android eventually but there’s no word yet on specific timing. To use it, you’ll need an active VSCO Pro subscription, which runs $13 per month or $60 a year. A Pro plans contains a ton more than just AI Labs features, though — it unlocks a full editing suite on mobile and the web, professional profile and website creation, hundreds of presets and film emulation settings and a lot more. This article originally appeared on Engadget at https://www.engadget.com/apps/vsco-adds-its-first-ai-powered-photo-editing-tool-170000055.html?src=rss",
          "content": "The VSCO photo editing and sharing app has been around for nearly as long as Instagram, positioning itself as the serious photographer’s choice for mobile editing. The original focus was on tasteful filters and editing tools, all of which got significantly more powerful and flexible over time; VSCO has long been doing the same sort of film emulations that have made Fujifilm’s cameras so desirable in recent years. The company also built up a loyal community of photographers who share their edits far and wide, both in the VSCO app as well as on more mainstream platforms like Instagram itself. Now, the company is making an unsurprising but potentially controversial move: it is releasing its first AI-powered image editing tool. “Remove,” as the name suggests, lets you erase “unwanted elements” from your photos without compromising the image’s full resolution. At first glance, it feels quite similar to tools like Google’s own Magic Eraser. You just pop open an image in the editor and highlight the portion you want to remove, and VSCO will do its best to obliterate the offending bits and fill in whatever is in the background that it deems appropriate. I haven’t had a chance to test how effective this tool is yet, but VSCO is using Black Forest Lab’s FLUX.1 Kontext model to do its magic, combined with its own proprietary technology specifically focused on making results that the company says look authentic. A quick look at Black Forest Lab and the FLUX.1 model show a tool that does appear to be well-suited to removing unwanted parts of an image and properly filling in the space that remains — but we’ll have to see it in action to judge whether it does the job well.This new Remove tool isn’t the only AI-powered editor VSCO is working on. There’s also an Upscale tool in the works that the company says will “enhance image resolution” while keeping color and composition unchanged. These sorts of tools will live under a new umbrella the company is calling AI Lab, making it clear this will be an ongoing initiative and not just a one-off release.On one hand, I’m not at all surprised to see VSCO jumping into AI-powered editing; it has to keep up with the rest of the industry. But on the other hand, the company has made its mark by building a community of photographers who value authenticity in their work, something that cannot help but be in conflict with AI tools, at least on the surface.VSCO’s CEO Eric Wittman acknowledged that tension in a conversation with Engadget. “We have a very photographer-centric, creator-first point of view,” Wittman said. “But where we see AI fitting in is in support of those folks, and that work, and that vision. The intention isn’t to replace [that work], though — AI has a place, but it’s not to replace what creators, and photographers in particular, are doing.” That mindset makes sense with something like Remove, which duplicates something people have done with Photoshop for years. Rather than generating new images or radically changing the truth of a photo like you can do with some of Google’s tools on the Pixel phones, Remove is a bit more subtle. “You would use masks, you would manually painstakingly edit things at a pixel by pixel level,” Wittman said. “What a lot of Remove tools would do is basically like automate that.”Wittman also cited preserving image quality as a key part of the work behind its own Remove tool. “We know that many people who were attempting to use AI in the early days, especially photographers, a lot of their disappointment was just in the preservation of the integrity and the quality of the work,” he said. “So what we've really tried to do is continue to help automate where we can and make things easier, but also preserve the quality.” To that end, VSCO is stressing that all these edits are non-destructive and the output will be in full, original resolution. As VSCO starts dabbling in more AI editing tools, Wittman emphasized that the company wants to stay on the side of helping photographers realize a creative vision rather than helping them make entirely unreal images, while also avoiding the mess of copyright issues and inauthentic content that is flooding the internet thanks to AI. “When you think about things like copyright, and the incredible importance of copyright, integrity, and authenticity — we're big believers as a company in both the laws and the norms that have been around for many many years. But obviously on some platforms there are people who are maliciously manipulating things, and we don't want to be participants in that.”VSCO’s first AI Labs feature is available as of today in the VSCO app for iOS; it should come to Android eventually but there’s no word yet on specific timing. To use it, you’ll need an active VSCO Pro subscription, which runs $13 per month or $60 a year. A Pro plans contains a ton more than just AI Labs features, though — it unlocks a full editing suite on mobile and the web, professional profile and website creation, hundreds of presets and film emulation settings and a lot more. This article originally appeared on Engadget at https://www.engadget.com/apps/vsco-adds-its-first-ai-powered-photo-editing-tool-170000055.html?src=rss",
          "feed_position": 39
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/home/apples-long-rumored-smart-display-will-reportedly-cost-350-165801748.html",
          "published_at": "Wed, 15 Oct 2025 16:58:01 +0000",
          "title": "Apple's long-rumored smart display will reportedly cost $350",
          "standfirst": "Apple has been expected to widen its smart home offering for a long time now, and if a new report is accurate, we could be getting a trio of new devices fairly soon. According to Bloomberg, Apple is working on an indoor camera and a smart display to arrive in 2026, as well as a tabletop robot, with the latter expected to launch in 2027. An Apple-made smart display in particular has featured heavily in the rumor mill for a number of years, but it appears to be closer than ever. Bloomberg’s Mark Gurman reports that Apple’s new home hub will have a 7-inch square LCD display, a built-in FaceTime camera and an OS that dynamically adjusts depending on who’s using it. It will also ship with an improved version of Siri that will behave more like ChatGPT or other chatbots in how it uses the web to answer your questions. The product, along with the more advanced Siri chatbot, had been slated for release earlier this year, but Apple reportedly scrapped those plans in favor of a spring 2026 launch. Bloomberg claims there will be two versions of the home hub, one code-named J940 which takes the form of a display mounted on a HomePod mini-like speaker, and the other (J491) designed to be hung on a wall. With both you’ll be able to control smart appliances, play music and presumably interact with Apple’s various apps on other devices. Apple is said to be targeting a price of around $350, although the Bloomberg report does not specify which version of the device that price refers to. The tabletop robot scheduled to launch in 2027 will effectively be the smart display Apple could be releasing next year mounted into a motorized arm that can move the device to different positions, which sounds like the company’s take on Amazon’s (slightly creepy) swivelling Echo Show 10, first launched in 2021. This product will have a larger 9-inch display, but is said to be delayed after Apple encountered engineering challenges with the motor. Bloomberg reports that all three of these devices will be built in Vietnam, which sources said represented a \"major change\" in how Apple launches a new product category, as it has traditionally relied on China at the outset. In 2020 it emerged that Apple was looking to diversify its production by moving some of its iPad and MacBook manufacturing to Vietnam, and the US’ trade war with China has only intensified during Trump’s second administration. Vietnam has not escaped tariffs of its own, but they’re less severe than imports from China.This article originally appeared on Engadget at https://www.engadget.com/home/apples-long-rumored-smart-display-will-reportedly-cost-350-165801748.html?src=rss",
          "content": "Apple has been expected to widen its smart home offering for a long time now, and if a new report is accurate, we could be getting a trio of new devices fairly soon. According to Bloomberg, Apple is working on an indoor camera and a smart display to arrive in 2026, as well as a tabletop robot, with the latter expected to launch in 2027. An Apple-made smart display in particular has featured heavily in the rumor mill for a number of years, but it appears to be closer than ever. Bloomberg’s Mark Gurman reports that Apple’s new home hub will have a 7-inch square LCD display, a built-in FaceTime camera and an OS that dynamically adjusts depending on who’s using it. It will also ship with an improved version of Siri that will behave more like ChatGPT or other chatbots in how it uses the web to answer your questions. The product, along with the more advanced Siri chatbot, had been slated for release earlier this year, but Apple reportedly scrapped those plans in favor of a spring 2026 launch. Bloomberg claims there will be two versions of the home hub, one code-named J940 which takes the form of a display mounted on a HomePod mini-like speaker, and the other (J491) designed to be hung on a wall. With both you’ll be able to control smart appliances, play music and presumably interact with Apple’s various apps on other devices. Apple is said to be targeting a price of around $350, although the Bloomberg report does not specify which version of the device that price refers to. The tabletop robot scheduled to launch in 2027 will effectively be the smart display Apple could be releasing next year mounted into a motorized arm that can move the device to different positions, which sounds like the company’s take on Amazon’s (slightly creepy) swivelling Echo Show 10, first launched in 2021. This product will have a larger 9-inch display, but is said to be delayed after Apple encountered engineering challenges with the motor. Bloomberg reports that all three of these devices will be built in Vietnam, which sources said represented a \"major change\" in how Apple launches a new product category, as it has traditionally relied on China at the outset. In 2020 it emerged that Apple was looking to diversify its production by moving some of its iPad and MacBook manufacturing to Vietnam, and the US’ trade war with China has only intensified during Trump’s second administration. Vietnam has not escaped tariffs of its own, but they’re less severe than imports from China.This article originally appeared on Engadget at https://www.engadget.com/home/apples-long-rumored-smart-display-will-reportedly-cost-350-165801748.html?src=rss",
          "feed_position": 40
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/computing/laptops/apples-first-m5-laptop-is-the-14-inch-macbook-pro-131314446.html",
          "published_at": "Wed, 15 Oct 2025 16:52:25 +0000",
          "title": "Apple's first M5 laptop is the 14-inch MacBook Pro",
          "standfirst": "The new M5 MacBook Pro has arrived — and brought something of a strategy change for Apple's chip release strategy this year. This time around, Apple has led with the entry-level 14-inch MacBook Pro, which retains the same $1,599 starting price as its M4 predecessor. It debuts alongside new 11- and 13-inch iPad Pros and a refreshed Apple Vision Pro that have the same M5 chipset, but — unlike last year — there's no M5 Pro and M5 Max devices to be found. In the past, Apple has favored launching its entire MacBook Pro lineup at once, as it did in 2024 with the M4, M4 Pro and M4 Max models. However, it may have decided to release the M5 model now so it could get a jump start on sales, since the M5 Pro and M5 Max versions are reportedly still several months away. Apple may have also opted for a low-key release since the M5 MacBook Pro is largely unchanged from the previous model. With that, the emphasis is squarely on the M5 chip and its extra performance. Reportedly due to cost reasons, Apple decided to use the same 3-nanometer fabrication process for the M5 as it did for the M4. The new chip has 10 GPU cores and 10 CPU cores, along with a 16-core Neural Engine. Apple claims the M5 has the \"world’s fastest CPU core\" with up to 20 percent faster multithreaded performance compared with the M4 chip. As for the GPU, the company says that offers \"up to 1.6x faster graphics performance in pro apps and enables up to 1.6x higher frame rates in games compared to the M4 model.\" That should make the M5 MacBook Pro a better option than the M4 model for things like gaming and video editing. To that end, compared with the M4 MacBook Pro, Apple says the latest model delivers up to 1.8x faster \"AI video-enhancing performance\" in Topaz Video, up to 1.7x faster 3D rendering in Blender and up to 1.2x faster build performance during code compiling in Xcode. The company is promising 3.5x faster AI performance than with the M4 model, and up to 6x faster performance than M1. It also claims SSD performance is up to twice as fast as the previous generation. The M5 MacBook Pro comes with the same 14.2-inch, 3,024 x 1,964 Liquid Retina XDR display that can hit 1,000 nits in SDR mode and up to 1,600 nits peak brightness for HDR content. It has adaptive refresh rates at up to 120Hz and offers a wide P3 color gamut with up to 1 billion colors, ideal for video editors and Lightroom users. Other key features include an SDXC card slot, HDMI port and 3.5mm headphone jack. There are three USB-C ports as before, but they're still the Thunderbolt 4 type with speeds up to 40 Gbps, and not the 80 Gbps Thunderbolt 5 ports found on M4 Pro and Max models. It also comes with a six-speaker system with support for Dolby Atmos and Spatial Audio as before, along with Wi-Fi 6E, Bluetooth 5.3 and a 12MP Center Stage 1080p webcam. As you might imagine, it comes with macOS Tahoe ready to go. Per the comparison page on Apple's site, except for the different CPU, the M5 MacBook Pro otherwise has identical specs to its M4 predecessor, right down to the same dimensions, weight and 70-watt power adapter. The 14-inch M5 MacBook Pro is now available for pre-order starting at $1,599 with 16GB memory and 512GB storage. It maxes out at 32GB of RAM and 4TB of storage. Shipping will start October 22. Update, 12:52PM ET: Confirmed via Apple's spec page that there are no real differences between the M4 and M5 MacBook Pro aside from the new CPU. This article originally appeared on Engadget at https://www.engadget.com/computing/laptops/apples-first-m5-laptop-is-the-14-inch-macbook-pro-131314446.html?src=rss",
          "content": "The new M5 MacBook Pro has arrived — and brought something of a strategy change for Apple's chip release strategy this year. This time around, Apple has led with the entry-level 14-inch MacBook Pro, which retains the same $1,599 starting price as its M4 predecessor. It debuts alongside new 11- and 13-inch iPad Pros and a refreshed Apple Vision Pro that have the same M5 chipset, but — unlike last year — there's no M5 Pro and M5 Max devices to be found. In the past, Apple has favored launching its entire MacBook Pro lineup at once, as it did in 2024 with the M4, M4 Pro and M4 Max models. However, it may have decided to release the M5 model now so it could get a jump start on sales, since the M5 Pro and M5 Max versions are reportedly still several months away. Apple may have also opted for a low-key release since the M5 MacBook Pro is largely unchanged from the previous model. With that, the emphasis is squarely on the M5 chip and its extra performance. Reportedly due to cost reasons, Apple decided to use the same 3-nanometer fabrication process for the M5 as it did for the M4. The new chip has 10 GPU cores and 10 CPU cores, along with a 16-core Neural Engine. Apple claims the M5 has the \"world’s fastest CPU core\" with up to 20 percent faster multithreaded performance compared with the M4 chip. As for the GPU, the company says that offers \"up to 1.6x faster graphics performance in pro apps and enables up to 1.6x higher frame rates in games compared to the M4 model.\" That should make the M5 MacBook Pro a better option than the M4 model for things like gaming and video editing. To that end, compared with the M4 MacBook Pro, Apple says the latest model delivers up to 1.8x faster \"AI video-enhancing performance\" in Topaz Video, up to 1.7x faster 3D rendering in Blender and up to 1.2x faster build performance during code compiling in Xcode. The company is promising 3.5x faster AI performance than with the M4 model, and up to 6x faster performance than M1. It also claims SSD performance is up to twice as fast as the previous generation. The M5 MacBook Pro comes with the same 14.2-inch, 3,024 x 1,964 Liquid Retina XDR display that can hit 1,000 nits in SDR mode and up to 1,600 nits peak brightness for HDR content. It has adaptive refresh rates at up to 120Hz and offers a wide P3 color gamut with up to 1 billion colors, ideal for video editors and Lightroom users. Other key features include an SDXC card slot, HDMI port and 3.5mm headphone jack. There are three USB-C ports as before, but they're still the Thunderbolt 4 type with speeds up to 40 Gbps, and not the 80 Gbps Thunderbolt 5 ports found on M4 Pro and Max models. It also comes with a six-speaker system with support for Dolby Atmos and Spatial Audio as before, along with Wi-Fi 6E, Bluetooth 5.3 and a 12MP Center Stage 1080p webcam. As you might imagine, it comes with macOS Tahoe ready to go. Per the comparison page on Apple's site, except for the different CPU, the M5 MacBook Pro otherwise has identical specs to its M4 predecessor, right down to the same dimensions, weight and 70-watt power adapter. The 14-inch M5 MacBook Pro is now available for pre-order starting at $1,599 with 16GB memory and 512GB storage. It maxes out at 32GB of RAM and 4TB of storage. Shipping will start October 22. Update, 12:52PM ET: Confirmed via Apple's spec page that there are no real differences between the M4 and M5 MacBook Pro aside from the new CPU. This article originally appeared on Engadget at https://www.engadget.com/computing/laptops/apples-first-m5-laptop-is-the-14-inch-macbook-pro-131314446.html?src=rss",
          "feed_position": 41
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/cameras/fujifilms-refreshed-instax-mini-liplay-has-an-extra-camera-for-selfies-164319167.html",
          "published_at": "Wed, 15 Oct 2025 16:43:19 +0000",
          "title": "Fujifilm's refreshed Instax mini LiPlay has an extra camera for selfies",
          "standfirst": "Fujifilm just announced the Instax mini LiPlay+ instant camera, which is a refresh of the original mini LiPlay from 2019. It's been six long years, so the company has added some nifty features here. First of all, there's a second camera on the rear that's intended for selfies. It features a wide-angle lens and joins the pre-existing front-facing camera. There's a new functionality that lets users combine the content from both cameras to create unique layered images. That's pretty nifty. Introducing the next generation of hybrid instant camera: instax mini LiPlay+™The updated mini LiPlay+ adds new creative tools and a refreshed design, giving image makers more ways to tell their stories with photos and sound.Learn more at https://t.co/o9TNgyG12o. pic.twitter.com/cUArA0BeZt— Fujifilm (@FujifilmUS) October 15, 2025 The design is mostly the same as before, but the rear LCD is larger. This should make it easier to accurately compose images before printing them out. To that end, there's a new film type accompanying the launch of this camera. The Soft Glitter instant film is available in ten-exposure packs and offers gold accents and soft hues to \"bring a calming shimmer\" around the frames. Film packs cost around $18. There's also a redesigned app with plenty of new features, including the ability to add short audio clips to images. These multimedia creations can be viewed by scanning a QR code in the print, which will trigger the audio on a smartphone or similar device. The LiPlay+ app lets users further edit images before printing them via the camera. The camera will be available for purchase by the end of the month and costs $235. There will be blue and beige versions. The original mini LiPlay didn't quite make our list of the best instant cameras, but several Fujifilm Instax models did. The company makes good stuff.This article originally appeared on Engadget at https://www.engadget.com/cameras/fujifilms-refreshed-instax-mini-liplay-has-an-extra-camera-for-selfies-164319167.html?src=rss",
          "content": "Fujifilm just announced the Instax mini LiPlay+ instant camera, which is a refresh of the original mini LiPlay from 2019. It's been six long years, so the company has added some nifty features here. First of all, there's a second camera on the rear that's intended for selfies. It features a wide-angle lens and joins the pre-existing front-facing camera. There's a new functionality that lets users combine the content from both cameras to create unique layered images. That's pretty nifty. Introducing the next generation of hybrid instant camera: instax mini LiPlay+™The updated mini LiPlay+ adds new creative tools and a refreshed design, giving image makers more ways to tell their stories with photos and sound.Learn more at https://t.co/o9TNgyG12o. pic.twitter.com/cUArA0BeZt— Fujifilm (@FujifilmUS) October 15, 2025 The design is mostly the same as before, but the rear LCD is larger. This should make it easier to accurately compose images before printing them out. To that end, there's a new film type accompanying the launch of this camera. The Soft Glitter instant film is available in ten-exposure packs and offers gold accents and soft hues to \"bring a calming shimmer\" around the frames. Film packs cost around $18. There's also a redesigned app with plenty of new features, including the ability to add short audio clips to images. These multimedia creations can be viewed by scanning a QR code in the print, which will trigger the audio on a smartphone or similar device. The LiPlay+ app lets users further edit images before printing them via the camera. The camera will be available for purchase by the end of the month and costs $235. There will be blue and beige versions. The original mini LiPlay didn't quite make our list of the best instant cameras, but several Fujifilm Instax models did. The company makes good stuff.This article originally appeared on Engadget at https://www.engadget.com/cameras/fujifilms-refreshed-instax-mini-liplay-has-an-extra-camera-for-selfies-164319167.html?src=rss",
          "feed_position": 42
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/best-vpn-deals-120056041.html",
          "published_at": "Wed, 15 Oct 2025 15:31:23 +0000",
          "title": "The best VPN deals: Get up to 87 percent off ProtonVPN, ExpressVPN, Surfshark and more",
          "standfirst": "A virtual private network (VPN) is useful in lots of ways every day, whether you're streaming foreign TV shows or keeping yourself anonymous online so advertisers can't track you. But while we strongly recommend using a VPN, it pays to do some research before investing in one — pricing can be opaque for these services, and you can't always trust how the providers portray their best deals. Even so, there are genuinely great deals to be had. VPN providers love to give out deep discounts to anybody willing to sign up for a year or more at once. This means you've got to pay out more upfront, but if you divide the cost by the months of service, you're actually paying less per month over time. With deals like this, VPN providers boost their subscriber numbers, and you get heavy price cuts on some of our favorite services. Most of the deals we highlight below follow that pattern, so make sure you're comfortable with a longer commitment before you take the plunge. If you've been thinking about subscribing to a VPN service, read on for the best VPN deals we could find right now. Best VPN deals NordVPN — $83.43 for a two-year subscription with three months free (77 percent off): NordVPN gets the most important parts of a VPN right. It's fast, it doesn't leak any of your data and it's great at changing your virtual location. I noted in my NordVPN review that it always connects quickly and includes a support page that makes it easy to get live help. Although I'm sad to see it shutting down Meshnet, NordVPN still includes a lot of cool features, like servers that instantly connect you to Tor. This deal gives you 77 percent off the two-year plan, which also comes with three extra months — but there's no expiration date, so you have a little time for comparison shopping. ExpressVPN Basic — $97.72 for a two-year subscription with four months free (73 percent off): This is one of the best VPNs, especially for new users, who will find its apps and website headache-free on all platforms. In tests for my ExpressVPN review, it dropped my download speeds by less than 7 percent and successfully changed my virtual location 14 out of 15 times. In short, it's an all-around excellent service that only suffers from being a little overpriced — which is why I'm so excited whenever I find it offering a decent deal. This deal, which gets you 28 months of ExpressVPN service, represents a 73 percent savings. It's the lowest I've seen ExpressVPN go in some time, though like NordVPN, it's not on a ticking clock. ExpressVPN Advanced — $125.72 for a two-year subscription with four months free (67 percent off): ExpressVPN recently split its pricing into multiple tiers, but they all still come with similar discounts for going long. In addition to top-tier VPN service, advanced users get two additional simultaneous connections (for a total of 12), the ExpressVPN Keys password manager, advanced ad and tracker blocking, ID protection features and a 50 percent discount on an AirCove router. Surfshark Starter — $53.73 for a two-year subscription with three months free (87 percent off): This is the \"basic\" level of Surfshark, but it includes the entire VPN; everything on Surfshark One is an extra perk. With this subscription, you'll get some of the most envelope-pushing features in the VPN world right now. Surfshark has a more closely connected server network than most VPNs, so it can rotate your IP constantly to help you evade detection — it even lets you choose your own entry and exit nodes for a double-hop connection. That all comes with a near-invisible impact on download speeds. With this year-round deal, you can save 87 percent on 27 months of Surfshark. Surfshark One — $67.23 for a two-year subscription with three months free (86 percent off): A VPN is great, but it's not enough to protect your data all on its own. Surfshark One adds several apps that boost your security beyond just VPN service, including Surfshark Antivirus (scans devices and downloads for malware) and Surfshark Alert (alerts you whenever your sensitive information shows up in a data breach), plus Surfshark Search and Alternative ID from the previous tier. This evergreen deal gives you 86 percent off all those features. If you bump up to Surfshark One+, you'll also get data removal through Incogni, but the price jumps enough that it's not quite worthwhile in my eyes. CyberGhost — $56.94 for a two-year subscription with two months free (83 percent off): CyberGhost has some of the best automation you'll see on any VPN. With its Smart Rules system, you can determine how its apps respond to different types of Wi-Fi networks, with exceptions for specific networks you know by name. Typically, you can set it to auto-connect, disconnect or send you a message asking what to do. CyberGhost's other best feature is its streaming servers — while it's not totally clear what it does to optimize them, I've found both better video quality and more consistent unblocking when I use them on streaming sites. Currently, you can get 26 months of CyberGhost for 83 percent off the usual price. Private Internet Access — $79 for a three-year subscription with three months free (83 percent off): It's a bit hard to find (the link at the start of this paragraph includes the coupon), but Private Internet Access (PIA) is giving out the best available price right now on a VPN I'd recommend using. With this deal, you can get 39 months of PIA for a little bit over $2 per month — an 83 percent discount on its monthly price. Despite being so cheap, PIA almost never comes off as a budget VPN, coming with its own DNS servers, a built-in ad blocker and automation powers to rival CyberGhost. However, internet speeds can fluctuate while you're connected. hide.me — $69.95 for a two-year subscription with two months free (73 percent off): Hide.me is an excellent free VPN — in fact, it's my favorite on the market, even with EventVPN and the free version of Proton VPN as competition. However, if you do want to upgrade to its paid plan, the two-year subscription offers great savings. Hide.me works well as a no-frills beginner VPN, with apps and a server network it should frankly be charging more for. What makes a good VPN deal Like I said in the intro, practically every VPN heavily discounts its long-term subscriptions the whole year round. The only noteworthy exception is Mullvad, the Costco hot dog of VPNs (that's a compliment, to be clear). When there's constantly a huge discount going on, it can be hard to tell when you're actually getting a good deal. The best way to squeeze out more savings is to look for seasonal deals, student discounts or exclusive sales like Proton VPN's coupon for Engadget readers. One trick VPNs often use is to add extra months onto an introductory deal, pushing the average monthly price even lower. When it comes time to renew, you usually can't get these extra months again. You often can't even renew for the same basic period of time — for example, you may only be able to renew a two-year subscription for one year. If you're planning to hold onto a VPN indefinitely, check the fine print to see how much it will cost per month after the first renewal, and ensure that fits into your budget. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/best-vpn-deals-120056041.html?src=rss",
          "content": "A virtual private network (VPN) is useful in lots of ways every day, whether you're streaming foreign TV shows or keeping yourself anonymous online so advertisers can't track you. But while we strongly recommend using a VPN, it pays to do some research before investing in one — pricing can be opaque for these services, and you can't always trust how the providers portray their best deals. Even so, there are genuinely great deals to be had. VPN providers love to give out deep discounts to anybody willing to sign up for a year or more at once. This means you've got to pay out more upfront, but if you divide the cost by the months of service, you're actually paying less per month over time. With deals like this, VPN providers boost their subscriber numbers, and you get heavy price cuts on some of our favorite services. Most of the deals we highlight below follow that pattern, so make sure you're comfortable with a longer commitment before you take the plunge. If you've been thinking about subscribing to a VPN service, read on for the best VPN deals we could find right now. Best VPN deals NordVPN — $83.43 for a two-year subscription with three months free (77 percent off): NordVPN gets the most important parts of a VPN right. It's fast, it doesn't leak any of your data and it's great at changing your virtual location. I noted in my NordVPN review that it always connects quickly and includes a support page that makes it easy to get live help. Although I'm sad to see it shutting down Meshnet, NordVPN still includes a lot of cool features, like servers that instantly connect you to Tor. This deal gives you 77 percent off the two-year plan, which also comes with three extra months — but there's no expiration date, so you have a little time for comparison shopping. ExpressVPN Basic — $97.72 for a two-year subscription with four months free (73 percent off): This is one of the best VPNs, especially for new users, who will find its apps and website headache-free on all platforms. In tests for my ExpressVPN review, it dropped my download speeds by less than 7 percent and successfully changed my virtual location 14 out of 15 times. In short, it's an all-around excellent service that only suffers from being a little overpriced — which is why I'm so excited whenever I find it offering a decent deal. This deal, which gets you 28 months of ExpressVPN service, represents a 73 percent savings. It's the lowest I've seen ExpressVPN go in some time, though like NordVPN, it's not on a ticking clock. ExpressVPN Advanced — $125.72 for a two-year subscription with four months free (67 percent off): ExpressVPN recently split its pricing into multiple tiers, but they all still come with similar discounts for going long. In addition to top-tier VPN service, advanced users get two additional simultaneous connections (for a total of 12), the ExpressVPN Keys password manager, advanced ad and tracker blocking, ID protection features and a 50 percent discount on an AirCove router. Surfshark Starter — $53.73 for a two-year subscription with three months free (87 percent off): This is the \"basic\" level of Surfshark, but it includes the entire VPN; everything on Surfshark One is an extra perk. With this subscription, you'll get some of the most envelope-pushing features in the VPN world right now. Surfshark has a more closely connected server network than most VPNs, so it can rotate your IP constantly to help you evade detection — it even lets you choose your own entry and exit nodes for a double-hop connection. That all comes with a near-invisible impact on download speeds. With this year-round deal, you can save 87 percent on 27 months of Surfshark. Surfshark One — $67.23 for a two-year subscription with three months free (86 percent off): A VPN is great, but it's not enough to protect your data all on its own. Surfshark One adds several apps that boost your security beyond just VPN service, including Surfshark Antivirus (scans devices and downloads for malware) and Surfshark Alert (alerts you whenever your sensitive information shows up in a data breach), plus Surfshark Search and Alternative ID from the previous tier. This evergreen deal gives you 86 percent off all those features. If you bump up to Surfshark One+, you'll also get data removal through Incogni, but the price jumps enough that it's not quite worthwhile in my eyes. CyberGhost — $56.94 for a two-year subscription with two months free (83 percent off): CyberGhost has some of the best automation you'll see on any VPN. With its Smart Rules system, you can determine how its apps respond to different types of Wi-Fi networks, with exceptions for specific networks you know by name. Typically, you can set it to auto-connect, disconnect or send you a message asking what to do. CyberGhost's other best feature is its streaming servers — while it's not totally clear what it does to optimize them, I've found both better video quality and more consistent unblocking when I use them on streaming sites. Currently, you can get 26 months of CyberGhost for 83 percent off the usual price. Private Internet Access — $79 for a three-year subscription with three months free (83 percent off): It's a bit hard to find (the link at the start of this paragraph includes the coupon), but Private Internet Access (PIA) is giving out the best available price right now on a VPN I'd recommend using. With this deal, you can get 39 months of PIA for a little bit over $2 per month — an 83 percent discount on its monthly price. Despite being so cheap, PIA almost never comes off as a budget VPN, coming with its own DNS servers, a built-in ad blocker and automation powers to rival CyberGhost. However, internet speeds can fluctuate while you're connected. hide.me — $69.95 for a two-year subscription with two months free (73 percent off): Hide.me is an excellent free VPN — in fact, it's my favorite on the market, even with EventVPN and the free version of Proton VPN as competition. However, if you do want to upgrade to its paid plan, the two-year subscription offers great savings. Hide.me works well as a no-frills beginner VPN, with apps and a server network it should frankly be charging more for. What makes a good VPN deal Like I said in the intro, practically every VPN heavily discounts its long-term subscriptions the whole year round. The only noteworthy exception is Mullvad, the Costco hot dog of VPNs (that's a compliment, to be clear). When there's constantly a huge discount going on, it can be hard to tell when you're actually getting a good deal. The best way to squeeze out more savings is to look for seasonal deals, student discounts or exclusive sales like Proton VPN's coupon for Engadget readers. One trick VPNs often use is to add extra months onto an introductory deal, pushing the average monthly price even lower. When it comes time to renew, you usually can't get these extra months again. You often can't even renew for the same basic period of time — for example, you may only be able to renew a two-year subscription for one year. If you're planning to hold onto a VPN indefinitely, check the fine print to see how much it will cost per month after the first renewal, and ensure that fits into your budget. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/best-vpn-deals-120056041.html?src=rss",
          "feed_position": 43
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/social-media/threads-now-supports-group-dm-150002493.html",
          "published_at": "Wed, 15 Oct 2025 15:00:02 +0000",
          "title": "Threads now supports group DM",
          "standfirst": "It took two years for Threads to get messaging, but you thankfully don't have to wait that long to be able to start group DMs. Meta has started rolling out group chats for the app, which lets you add up to 50 of your followers to a single conversation. Just start a new message and add anybody who follows you on Threads. The company says that you'll also be able to share a link that your followers can click to join a group conversation \"soon,\" so you don't have to add them one by one. Like in messaging apps, you'll be able to customize the group chat's name to better reflect its topic. In addition to launching group DMs, Meta is also releasing the messaging feature in the European Union over the coming days. If you're in the EU, you'll get access not just to individual DMs, but also to group DMs, messaging controls, privacy settings, the hidden folder where spam goes to and support for media files when the feature becomes available in your country. Emily Dalton Smith, Meta's Head of Product for Threads, told us in a test group chat that it's \"on track to become Meta's next major app\" with 400 million monthly active users.\" Messaging has been the top requested feature since Threads launched, but it took some time to release it because it \"wasn't a priority in the early days.\" Since making DMs available on Threads in July, Meta has added support for photos, videos and GIFs, a messaging requests folder to reduce unwanted messages, the hidden spam folder and a privacy setting that completely switches off message requests from people you don't follow. Threads / Meta This article originally appeared on Engadget at https://www.engadget.com/social-media/threads-now-supports-group-dm-150002493.html?src=rss",
          "content": "It took two years for Threads to get messaging, but you thankfully don't have to wait that long to be able to start group DMs. Meta has started rolling out group chats for the app, which lets you add up to 50 of your followers to a single conversation. Just start a new message and add anybody who follows you on Threads. The company says that you'll also be able to share a link that your followers can click to join a group conversation \"soon,\" so you don't have to add them one by one. Like in messaging apps, you'll be able to customize the group chat's name to better reflect its topic. In addition to launching group DMs, Meta is also releasing the messaging feature in the European Union over the coming days. If you're in the EU, you'll get access not just to individual DMs, but also to group DMs, messaging controls, privacy settings, the hidden folder where spam goes to and support for media files when the feature becomes available in your country. Emily Dalton Smith, Meta's Head of Product for Threads, told us in a test group chat that it's \"on track to become Meta's next major app\" with 400 million monthly active users.\" Messaging has been the top requested feature since Threads launched, but it took some time to release it because it \"wasn't a priority in the early days.\" Since making DMs available on Threads in July, Meta has added support for photos, videos and GIFs, a messaging requests folder to reduce unwanted messages, the hidden spam folder and a privacy setting that completely switches off message requests from people you don't follow. Threads / Meta This article originally appeared on Engadget at https://www.engadget.com/social-media/threads-now-supports-group-dm-150002493.html?src=rss",
          "feed_position": 45,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2025-10/50ea9480-a9b9-11f0-acdd-9fa9d020dcef"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/ar-vr/apples-new-vision-pro-gets-an-m5-chip-and-dual-knit-band-but-its-still-3499-132123957.html",
          "published_at": "Wed, 15 Oct 2025 13:21:24 +0000",
          "title": "Apple's new Vision Pro gets an M5 chip and Dual Knit Band, but it's still $3,499",
          "standfirst": "Apple has introduced an upgraded version of its Vision Pro headset that's powered by the company's M5 chip, its latest silicon that will also come with the new iPad Pro and MacBook Pro. The first generation of the headset was equipped with Apple's M2, so you can expect this device to be faster and come with more capabilities. Apple hasn't budged the price from $3,499 with 256GB of storage, but at least it comes with a Dual Knit Band, which adds a top strap for extra security and comfort. (Existing Vision Pro users can also buy the Dual Knit Band separately for $99.) As you'd expect, Apple claims the refreshed Vision Pro should be faster while loading apps, browsing the web and doing just about everything. The M5 chip also includes a new 10-core GPU, with better support for hardware-accelerated ray tracing and mesh shading, \"enabling developers to add remarkable detail to lighting, shadows, and reflections in games like Control,\" according to Apple. The company also says the M5 Vision Pro renders 10 percent more pixels on its micro-OLED displays, which should make everything look a bit sharper. The M5 Vision Pro should last a bit longer than the original model, as well. Apple claims it supports up to two and a half hours of typical usage, and up to three hours of video playback. The previous model was rated for two hours of general usage and two and a half hours of video viewing. Bloomberg's Mark Gurman reported a few days ago that Apple was due for another wave of product announcements. He wrote back then that the new iPad Pro and Vision Pro are already being mass produced and that Apple is \"gearing up for an imminent release.\" Apple had originally wanted to launch a a lighter and cheaper version of the Vision Pro headset, as well, but it reportedly decided to shift its focus on the development of smart glasses. The company pulled people working on the lighter Vision Pro, Gurman said in another report, and moved them to its smart glasses project. Apple is reportedly working on a smart glasses model with no display and is meant to pair with iPhones, along with another model that's equipped with a built-in screen and can directly compete with Meta's Ray-Ban Display. The company is aiming to release the model with no screen in 2027 and the one with a screen in 2028, Gurman said. \"The Vision Pro is a flawed product, but it's certainly not empty,\" we noted in our review of the original headset. \"It's as if Apple has compiled everything it's learned from building the Mac, iPhone, Apple Watch and AirPods into a single device, all in a bid to avoid the Innovator's Dilemma.\" At first glance, the M5 Vision Pro doesn't seem to change that conclusion much, not without more content and apps built around spatial computing. A price drop and more storage on the base model would certainly make the Vision Pro more compelling, until that happens it'll remain more of a developer kit than a full-fledged consumer product. The M5 Vision Pro is now ready to pre-order and will once again set you back $3,499. Apple will start shipping the device on October 22.This article originally appeared on Engadget at https://www.engadget.com/ar-vr/apples-new-vision-pro-gets-an-m5-chip-and-dual-knit-band-but-its-still-3499-132123957.html?src=rss",
          "content": "Apple has introduced an upgraded version of its Vision Pro headset that's powered by the company's M5 chip, its latest silicon that will also come with the new iPad Pro and MacBook Pro. The first generation of the headset was equipped with Apple's M2, so you can expect this device to be faster and come with more capabilities. Apple hasn't budged the price from $3,499 with 256GB of storage, but at least it comes with a Dual Knit Band, which adds a top strap for extra security and comfort. (Existing Vision Pro users can also buy the Dual Knit Band separately for $99.) As you'd expect, Apple claims the refreshed Vision Pro should be faster while loading apps, browsing the web and doing just about everything. The M5 chip also includes a new 10-core GPU, with better support for hardware-accelerated ray tracing and mesh shading, \"enabling developers to add remarkable detail to lighting, shadows, and reflections in games like Control,\" according to Apple. The company also says the M5 Vision Pro renders 10 percent more pixels on its micro-OLED displays, which should make everything look a bit sharper. The M5 Vision Pro should last a bit longer than the original model, as well. Apple claims it supports up to two and a half hours of typical usage, and up to three hours of video playback. The previous model was rated for two hours of general usage and two and a half hours of video viewing. Bloomberg's Mark Gurman reported a few days ago that Apple was due for another wave of product announcements. He wrote back then that the new iPad Pro and Vision Pro are already being mass produced and that Apple is \"gearing up for an imminent release.\" Apple had originally wanted to launch a a lighter and cheaper version of the Vision Pro headset, as well, but it reportedly decided to shift its focus on the development of smart glasses. The company pulled people working on the lighter Vision Pro, Gurman said in another report, and moved them to its smart glasses project. Apple is reportedly working on a smart glasses model with no display and is meant to pair with iPhones, along with another model that's equipped with a built-in screen and can directly compete with Meta's Ray-Ban Display. The company is aiming to release the model with no screen in 2027 and the one with a screen in 2028, Gurman said. \"The Vision Pro is a flawed product, but it's certainly not empty,\" we noted in our review of the original headset. \"It's as if Apple has compiled everything it's learned from building the Mac, iPhone, Apple Watch and AirPods into a single device, all in a bid to avoid the Innovator's Dilemma.\" At first glance, the M5 Vision Pro doesn't seem to change that conclusion much, not without more content and apps built around spatial computing. A price drop and more storage on the base model would certainly make the Vision Pro more compelling, until that happens it'll remain more of a developer kit than a full-fledged consumer product. The M5 Vision Pro is now ready to pre-order and will once again set you back $3,499. Apple will start shipping the device on October 22.This article originally appeared on Engadget at https://www.engadget.com/ar-vr/apples-new-vision-pro-gets-an-m5-chip-and-dual-knit-band-but-its-still-3499-132123957.html?src=rss",
          "feed_position": 47
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/mobile/tablets/apples-latest-ipad-pro-get-a-power-boost-with-the-new-m5-chip-131036435.html",
          "published_at": "Wed, 15 Oct 2025 13:10:36 +0000",
          "title": "Apple's latest iPad Pro gets a power boost with the new M5 chip",
          "standfirst": "Apple's latest 11- and 13-inch iPad Pros have arrived, and though they're the first with the company's all-new M5 chip, they're otherwise largely identical to last year's models. The main reason to buy one, then, would be for the extra performance over the M4 — something that may be worthwhile to content creators and other power users looking for a tablet instead of a laptop. Last year Apple decided to debut its M4 chip with the iPad Pro lineup and not its laptops. The reason? Only the entry-level M4 was ready (and not the M4 Pro and M4 Max), so Apple decided to wait before putting in its MacBooks so it could launch the entire lineup at once. With updated Magic Keyboards, It also showed that Apple was marketing the iPad Pro as a feasible MacBook replacement for power users. The same applies with the M5, except this time the company also launched its entry-level 14-inch MacBook Pro at the same time. As before, the new M5 processor uses TSMC's 3-nanometer process, as Apple reportedly decided against 2-nanometer chips due to cost considerations. The entry-level M5 comes in a couple of versions. The iPad Pro with either 256GB or 512GB of storage gets an M5 with a 9-core CPU (3 performance cores and 6 efficiency cores), 10-core GPU and 12GB of RAM. The 1TB and 2TB models get a fourth performance core and 16GB of RAM. The big upgrade here appears to be to the GPU; Apple says each of the 10 GPU cores have a Neural Accelerator on board, which will allow GPU-based AI processing to run significantly faster than on the M4. Apple claims it has more than four times the peak GPU compute performance of the M4 (which is only about 18 months old, mind you). Graphics performance should be about 45 percent higher than on the M4, as well. Overall multithreaded performance is 15 percent faster than the M4, and Apple says that video transcoding is six times faster than what the old M1 iPad Pro from 2021 delivered. As for battery life, Apple claims the same 10 hours that basically every iPad has ever been rated at. But for the first time, the iPad Pro supports fast charging — you can get up to 50 percent in 30 minutes using a 60W USB-C power adaptor. Apple is also using the C1X modem that it originally introduced last month in the iPhone Air; that'll provide the optional 5G service that Apple has offered on iPads for a few years now. There's also an N1 chip (also found in the iPhone Air), which is an Apple-designed networking chip for Wi-Fi 7, Bluetooth 6 and Thread connectivity. Apple claims this new chip will make features like Personal Hotspot and Airdrop more reliable while also offering improved performance on 5GHz Wi-Fi networks. As before, the 2025 iPad Airs are extremely thin and light. The 11-inch model is 5.3mm thick and tips the scales at just under one pound, while the 13-incher is just 5.1mm thick weighs 1.29 pounds. Both feature \"tandem\" OLED Ultra Display XDR screens that hit up to 1,000 nits brightness and peak at 1,600 nits — so they're perfect for viewing and editing HDR content. The new iPad Pro starts at $999 for the 11-inch model with 256GB of storage ($1,199 with 5G) and $1,299 for the 13-inch ($1,499 with 5G). Those are the same prices as last year — still extremely expensive, but at least not more than before. You can pre-order the new iPad Pro now, and it'll be available on October 22. This article originally appeared on Engadget at https://www.engadget.com/mobile/tablets/apples-latest-ipad-pro-get-a-power-boost-with-the-new-m5-chip-131036435.html?src=rss",
          "content": "Apple's latest 11- and 13-inch iPad Pros have arrived, and though they're the first with the company's all-new M5 chip, they're otherwise largely identical to last year's models. The main reason to buy one, then, would be for the extra performance over the M4 — something that may be worthwhile to content creators and other power users looking for a tablet instead of a laptop. Last year Apple decided to debut its M4 chip with the iPad Pro lineup and not its laptops. The reason? Only the entry-level M4 was ready (and not the M4 Pro and M4 Max), so Apple decided to wait before putting in its MacBooks so it could launch the entire lineup at once. With updated Magic Keyboards, It also showed that Apple was marketing the iPad Pro as a feasible MacBook replacement for power users. The same applies with the M5, except this time the company also launched its entry-level 14-inch MacBook Pro at the same time. As before, the new M5 processor uses TSMC's 3-nanometer process, as Apple reportedly decided against 2-nanometer chips due to cost considerations. The entry-level M5 comes in a couple of versions. The iPad Pro with either 256GB or 512GB of storage gets an M5 with a 9-core CPU (3 performance cores and 6 efficiency cores), 10-core GPU and 12GB of RAM. The 1TB and 2TB models get a fourth performance core and 16GB of RAM. The big upgrade here appears to be to the GPU; Apple says each of the 10 GPU cores have a Neural Accelerator on board, which will allow GPU-based AI processing to run significantly faster than on the M4. Apple claims it has more than four times the peak GPU compute performance of the M4 (which is only about 18 months old, mind you). Graphics performance should be about 45 percent higher than on the M4, as well. Overall multithreaded performance is 15 percent faster than the M4, and Apple says that video transcoding is six times faster than what the old M1 iPad Pro from 2021 delivered. As for battery life, Apple claims the same 10 hours that basically every iPad has ever been rated at. But for the first time, the iPad Pro supports fast charging — you can get up to 50 percent in 30 minutes using a 60W USB-C power adaptor. Apple is also using the C1X modem that it originally introduced last month in the iPhone Air; that'll provide the optional 5G service that Apple has offered on iPads for a few years now. There's also an N1 chip (also found in the iPhone Air), which is an Apple-designed networking chip for Wi-Fi 7, Bluetooth 6 and Thread connectivity. Apple claims this new chip will make features like Personal Hotspot and Airdrop more reliable while also offering improved performance on 5GHz Wi-Fi networks. As before, the 2025 iPad Airs are extremely thin and light. The 11-inch model is 5.3mm thick and tips the scales at just under one pound, while the 13-incher is just 5.1mm thick weighs 1.29 pounds. Both feature \"tandem\" OLED Ultra Display XDR screens that hit up to 1,000 nits brightness and peak at 1,600 nits — so they're perfect for viewing and editing HDR content. The new iPad Pro starts at $999 for the 11-inch model with 256GB of storage ($1,199 with 5G) and $1,299 for the 13-inch ($1,499 with 5G). Those are the same prices as last year — still extremely expensive, but at least not more than before. You can pre-order the new iPad Pro now, and it'll be available on October 22. This article originally appeared on Engadget at https://www.engadget.com/mobile/tablets/apples-latest-ipad-pro-get-a-power-boost-with-the-new-m5-chip-131036435.html?src=rss",
          "feed_position": 48
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/gaming/pc/asus-rog-xbox-ally-x-review-an-extra-life-for-xbox-130050224.html",
          "published_at": "Wed, 15 Oct 2025 13:00:50 +0000",
          "title": "ASUS ROG Xbox Ally X review: An extra life for Xbox",
          "standfirst": "Xbox is at a crossroads. While the PlayStation 5 and Switch 2 continue to gain popularity, multiple price hikes for the Xbox Series S and X have killed their momentum. And with several big box retailers like Costco deciding to drop Microsoft’s console from their shelves, the current-gen Xbox may be dead in the water. So what should the company do? Microsoft’s most straightforward option would be to simply punt the end of the Xbox's lifecycle, regroup and come back strong with a brand new console in a year or two. On the flipside, the company could follow in Sega's footsteps after the untimely demise of the Dreamcast and become a cross-platform game publisher with a large stable of first-party studios. However, there's a third route that could fill the gap between now and whenever the next Xbox arrives that could potentially expand its reach to a whole new segment: Give handheld gaming a go. Engineering and building a new portable gaming device isn't cheap or quick. So instead of doing everything itself, Microsoft teamed up with ASUS to create the ROG Xbox Ally and ROG Xbox Ally X — a pair of Windows 11-based portable gaming PCs enhanced with Microsoft's DNA. And while it's too early to say if these handhelds will help save Xbox itself, they're more than solid portables that could have a big impact on portable gaming going forward. Editor's note: This review is focused on the ROG Xbox Ally X, primarily because that's all we have at the moment. However, the Xbox Ally and Xbox Ally X share many features including the same basic design , display, button layout and software (the only difference is their color). That said, the base Xbox Ally has less impressive specs as it comes with a slower AMD Ryzen Z2 chip, 16GB of RAM, 512GB of storage and a smaller 60WHr battery. It’s also a touch lighter (1.48 pounds vs 1.58) and has a lower price of $600 instead of $1,000. Design and display: The Xbox goes portable Sam Rutherford for Engadget When you look at the ROG Xbox Ally X, it might appear as if ASUS bolted larger grips to its previous handheld — the Ally X — and called it a day. However, the company says the Xbox Ally was redesigned from the ground up, in large part to include a number of signature features that will make Xbox faithful feel right at home. This includes things like the classic ABXY button layout, Impulse triggers with tons of travel and, of course, the requisite Xbox home button. Then smack dab in the middle of everything is a 7-inch IPS LCD screen. Speaking of the display, ASUS picked a FHD IPS panel with a 120Hz refresh rate that appears to be the exact same screen used on the Ally X. While it doesn't deliver pure blacks like you'd get from OLED, contrast was still surprisingly good. With a brightness of 500 nits (the one on our review unit was actually a touch higher at 510 nits), this display delivers almost everything you want without feeling like you need to upgrade to something like the huge 8.8-inch OLED panel on the Lenovo Legion Go 2. Sam Rutherford for Engadget Around back, there are two customizable paddles while the top edge houses a fingerprint reader and all of the Xbox Ally X's connectivity: two USB-C ports, a microSD card reader and 3.5mm audio. Notably, while both ports support charging and display capabilities, the one furthest to the left is also Thunderbolt 4 compliant. This means it has enough bandwidth to support external GPUs like ASUS' recently updated XG Mobile graphics dock. Ultimately, the ROG Xbox Ally might not look all that different from ASUS' previous handhelds, but between its buttons, triggers and those big new grips, it really does feel like you're holding a portable version of Microsoft's console. Even without Hall Effect sensors, the Xbox Ally X's joysticks are tight and responsive, while the triggers offer a ton of travel. The only thing I wish ASUS and Microsoft had paid a little more attention to is the handheld's haptics, which are fine, but they’re a far cry from the expressive rumble motors you get from a DualSense controller or the Switch 2's Joy-Con. Performance: Flagship handheld power Sam Rutherford for Engadget As the higher-end model in ASUS and Microsoft's new joint handheld lineup, the ROG Xbox Ally X features a AMD Ryzen Z2 Extreme chip with 24GB of memory (which is shared with its GPU) and 1TB of storage that can be expanded via microSD. However, thanks in part to the new full-screen Xbox experience, Microsoft tweaked a number of the handheld's background processes and services for the first time on a Windows 11-based handheld. The result is a slightly more optimized device even when compared to its closest rivals. In Cyberpunk 2077 at 1,920 x 1,080, medium graphics and FSR set to performance, the ROG Xbox Ally X hit 62.1 fps while plugged in and set to its max 35-watt Turbo mode. That's almost five fps higher than what we got from the Lenovo Legion Go 2 (57.5 fps) when its settings are similarly maxed out. Admittedly, this might not sound like a huge improvement, but it ends up being an extra seven percent performance from the same chip, which ain't bad. Meanwhile in Returnal, I got similar numbers after switching to the Ally's more energy-efficient 17-watt Performance Mode (unplugged), where it produced 42 fps at full HD on medium versus 39 fps for the Lenovo. Sam Rutherford for Engadget That said, it's important to note that the Ally's performance changes depending on whether it's plugged into the wall or not. If you want its full 35-watt Turbo Mode, you're going to need to use a power adapter, while its 17-watt Performance and 13-watt Silent modes stay the same no matter what you do. There’s also a manual performance customization tool, but to access it, you'll need to switch over to ASUS' Armoury Crate app, as there isn't a place to change things directly inside the Xbox app. Software: The tailored gaming experience we needed Getting excited about the software on a gaming handheld is usually pretty difficult. However, between the aforementioned tweaks to background services and the new full-screen Xbox experience, Microsoft has managed to remove a ton of the clunkiness that typically plagues other Windows-based gaming handhelds. Instead of having to wade your way through the traditional Windows desktop before booting into a game, now you're greeted by the Xbox app upon startup (and even during initial setup), so there are fewer steps to get between you and your favorite title. Furthermore, Microsoft has come up with a revamped layout that makes core features super easy to find. The Home tab is where all your installed software is, while there are other dedicated sections for Game Pass downloads (assuming you have a subscription) cloud gaming/remote play (also via Game Pass) and the Microsoft Store. If you prefer other digital marketplaces like Steam or Epic, there are shortcuts to download installers for those stores (and a few more like GOG and Ubisoft) in the My Apps tab. You don't need to open a web browser and do things manually. To switch between apps in Microsoft's new full-screen Xbox experience, all you need to do is swipe up from the bottom of the ROG Xbox Ally X's screen. Sam Rutherford for Engadget Furthermore, hitting the Xbox button summons a handful of quick settings for all sorts of things, including the Command Center for performance, toggles for Wi-Fi and Bluetooth, your Xbox friends list and a whole lot more. It's kind of like a do-everything button and it makes accessing almost all of your most important tools and features quick and easy. On top of that, there are new gestures that you can access by swiping in on the screen in different directions. Dragging your finger in from the left calls up the Xbox Game Bar overlay, even when you're in the middle of playing a game, while swiping in from the right opens your Windows notifications. But my favorite command is swiping up from the bottom, which is a new way to switch between apps (or a cooler version of Alt + Tab, depending on how you look at it). From there, you can even scroll through any programs that are currently open just by tapping the Xbox Ally's shoulder buttons. If you want to use the handheld like a real PC, you can also activate the Windows desktop from there too. Microsoft has also thoughtfully included compatibility tags on a number of games in its store to give buyers a sense of how well a game will run on the ROG Xbox Ally X. Sam Rutherford for Engadget The redesigned Xbox experience is very breezy and handles 90 to 95 percent of your traditional gaming functions, but there are still times when some of Window's underlying awkwardness shows through. Most often, I found this happens when exiting a game from a third-party store, where the Xbox Ally will spit you out into your Steam library (for example), where you'll often have to rely on touchscreen controls instead of the joysticks or the desktop mode's mouse cursor to navigate around. It's not a big deal compared to other Windows-based handhelds, and even though Microsoft has taken a big step forward on the Xbox Ally, there is still a little polishing to be done. Battery life: A solid jump in longevity One of the biggest benefits of going with the ROG Xbox Ally X is that it comes with a larger 80WHr battery than the base model (60Whr). When that is combined with improved energy efficiency from its new chip, you get very solid battery life — just as long as you don’t max out the power settings. Sam Rutherford for Engadget I tested this by playing Clair Obscur: Expedition 33 at full HD on medium settings and max brightness (Protip: don’t do this at night if you want to get to sleep on time) and the Xbox Ally X lasted just shy of three and a half hours. That’s 30 minutes longer than what I got from the Legion Go 2, although considering the latter has a much larger screen (with the same 500 nits of brightness), that difference wasn’t a big shock. The bigger revelation is that when compared to the original Ally X, ASUS and Microsoft’s new jointly-made device provided an extra hour of runtime, which could make a meaningful difference on a long trip. Wrap-up While Microsoft’s first real foray into PC gaming handhelds isn’t upending the status quo and it’s way too early to say if this gadget will save Xbox as a whole, it is bringing some notable advancements. The new full-screen experience makes launching and playing games on Windows-based devices so much more seamless that it’s kind of wild it took so long to get here. Sure, there are still a few edge cases where you’ll have to tap the screen or flip between the Xbox app and ASUS’ Armoury Create to tweak certain settings, but compared to most of its rivals, the ASUS ROG Xbox Ally X is a massive upgrade in general usability. Here is a size comparison between the ASUS ROG Xbox Ally X (bottom) and the Lenovo Legion Go 2 (top). Sam Rutherford for Engadget The bigger grips and a familiar button layout will instantly make longtime Xbox fans feel right at home. And thanks to the new chip and more processes and services that run in the background while you’re gaming, you get class-leading performance and battery life. Aside from lackluster haptics, the ROG Xbox Ally X’s biggest issue is its price. I totally get that there’s a growing number of gamers who constantly crave better performance from their portable PCs. However, the trade-off for all this is a much bigger hit to your wallet. It wasn’t that long ago when the going rate for a premium handheld was more like $500, which made it easier to afford. After all, those devices weren’t really designed to be your main gaming rig like a laptop or desktop. Ultimately the biggest deciding factor for purchasing the ROG Xbox Ally X may be how much someone is already invested into the Xbox ecosystem. If you’re a fan of other game stores or you don’t have a subscription to Xbox Game Pass or a ton of friends on the platform, you won’t get the full benefit of everything Microsoft has integrated into the handheld’s new software. This goes double for devotees of Valve’s digital store and Linux-based OS that don’t need bleeding edge performance, who can safely stick to much more affordable Steam Decks or the Legion Go S. Alternatively, if you want a versatile portable with a giant OLED screen and detachable controllers, the Legion Go 2 is worth consideration as well. Though at $1,300 for the model with a Z2 Extreme chip, it’s even more expensive than this new handheld Xbox. Still, despite some minor caveats, Microsoft has finally put its spin on portable PC gaming (with an assist from ASUS) and brought some welcome upgrades to the space that have made the ROG Xbox Ally X a top shelf device.This article originally appeared on Engadget at https://www.engadget.com/gaming/pc/asus-rog-xbox-ally-x-review-an-extra-life-for-xbox-130050224.html?src=rss",
          "content": "Xbox is at a crossroads. While the PlayStation 5 and Switch 2 continue to gain popularity, multiple price hikes for the Xbox Series S and X have killed their momentum. And with several big box retailers like Costco deciding to drop Microsoft’s console from their shelves, the current-gen Xbox may be dead in the water. So what should the company do? Microsoft’s most straightforward option would be to simply punt the end of the Xbox's lifecycle, regroup and come back strong with a brand new console in a year or two. On the flipside, the company could follow in Sega's footsteps after the untimely demise of the Dreamcast and become a cross-platform game publisher with a large stable of first-party studios. However, there's a third route that could fill the gap between now and whenever the next Xbox arrives that could potentially expand its reach to a whole new segment: Give handheld gaming a go. Engineering and building a new portable gaming device isn't cheap or quick. So instead of doing everything itself, Microsoft teamed up with ASUS to create the ROG Xbox Ally and ROG Xbox Ally X — a pair of Windows 11-based portable gaming PCs enhanced with Microsoft's DNA. And while it's too early to say if these handhelds will help save Xbox itself, they're more than solid portables that could have a big impact on portable gaming going forward. Editor's note: This review is focused on the ROG Xbox Ally X, primarily because that's all we have at the moment. However, the Xbox Ally and Xbox Ally X share many features including the same basic design , display, button layout and software (the only difference is their color). That said, the base Xbox Ally has less impressive specs as it comes with a slower AMD Ryzen Z2 chip, 16GB of RAM, 512GB of storage and a smaller 60WHr battery. It’s also a touch lighter (1.48 pounds vs 1.58) and has a lower price of $600 instead of $1,000. Design and display: The Xbox goes portable Sam Rutherford for Engadget When you look at the ROG Xbox Ally X, it might appear as if ASUS bolted larger grips to its previous handheld — the Ally X — and called it a day. However, the company says the Xbox Ally was redesigned from the ground up, in large part to include a number of signature features that will make Xbox faithful feel right at home. This includes things like the classic ABXY button layout, Impulse triggers with tons of travel and, of course, the requisite Xbox home button. Then smack dab in the middle of everything is a 7-inch IPS LCD screen. Speaking of the display, ASUS picked a FHD IPS panel with a 120Hz refresh rate that appears to be the exact same screen used on the Ally X. While it doesn't deliver pure blacks like you'd get from OLED, contrast was still surprisingly good. With a brightness of 500 nits (the one on our review unit was actually a touch higher at 510 nits), this display delivers almost everything you want without feeling like you need to upgrade to something like the huge 8.8-inch OLED panel on the Lenovo Legion Go 2. Sam Rutherford for Engadget Around back, there are two customizable paddles while the top edge houses a fingerprint reader and all of the Xbox Ally X's connectivity: two USB-C ports, a microSD card reader and 3.5mm audio. Notably, while both ports support charging and display capabilities, the one furthest to the left is also Thunderbolt 4 compliant. This means it has enough bandwidth to support external GPUs like ASUS' recently updated XG Mobile graphics dock. Ultimately, the ROG Xbox Ally might not look all that different from ASUS' previous handhelds, but between its buttons, triggers and those big new grips, it really does feel like you're holding a portable version of Microsoft's console. Even without Hall Effect sensors, the Xbox Ally X's joysticks are tight and responsive, while the triggers offer a ton of travel. The only thing I wish ASUS and Microsoft had paid a little more attention to is the handheld's haptics, which are fine, but they’re a far cry from the expressive rumble motors you get from a DualSense controller or the Switch 2's Joy-Con. Performance: Flagship handheld power Sam Rutherford for Engadget As the higher-end model in ASUS and Microsoft's new joint handheld lineup, the ROG Xbox Ally X features a AMD Ryzen Z2 Extreme chip with 24GB of memory (which is shared with its GPU) and 1TB of storage that can be expanded via microSD. However, thanks in part to the new full-screen Xbox experience, Microsoft tweaked a number of the handheld's background processes and services for the first time on a Windows 11-based handheld. The result is a slightly more optimized device even when compared to its closest rivals. In Cyberpunk 2077 at 1,920 x 1,080, medium graphics and FSR set to performance, the ROG Xbox Ally X hit 62.1 fps while plugged in and set to its max 35-watt Turbo mode. That's almost five fps higher than what we got from the Lenovo Legion Go 2 (57.5 fps) when its settings are similarly maxed out. Admittedly, this might not sound like a huge improvement, but it ends up being an extra seven percent performance from the same chip, which ain't bad. Meanwhile in Returnal, I got similar numbers after switching to the Ally's more energy-efficient 17-watt Performance Mode (unplugged), where it produced 42 fps at full HD on medium versus 39 fps for the Lenovo. Sam Rutherford for Engadget That said, it's important to note that the Ally's performance changes depending on whether it's plugged into the wall or not. If you want its full 35-watt Turbo Mode, you're going to need to use a power adapter, while its 17-watt Performance and 13-watt Silent modes stay the same no matter what you do. There’s also a manual performance customization tool, but to access it, you'll need to switch over to ASUS' Armoury Crate app, as there isn't a place to change things directly inside the Xbox app. Software: The tailored gaming experience we needed Getting excited about the software on a gaming handheld is usually pretty difficult. However, between the aforementioned tweaks to background services and the new full-screen Xbox experience, Microsoft has managed to remove a ton of the clunkiness that typically plagues other Windows-based gaming handhelds. Instead of having to wade your way through the traditional Windows desktop before booting into a game, now you're greeted by the Xbox app upon startup (and even during initial setup), so there are fewer steps to get between you and your favorite title. Furthermore, Microsoft has come up with a revamped layout that makes core features super easy to find. The Home tab is where all your installed software is, while there are other dedicated sections for Game Pass downloads (assuming you have a subscription) cloud gaming/remote play (also via Game Pass) and the Microsoft Store. If you prefer other digital marketplaces like Steam or Epic, there are shortcuts to download installers for those stores (and a few more like GOG and Ubisoft) in the My Apps tab. You don't need to open a web browser and do things manually. To switch between apps in Microsoft's new full-screen Xbox experience, all you need to do is swipe up from the bottom of the ROG Xbox Ally X's screen. Sam Rutherford for Engadget Furthermore, hitting the Xbox button summons a handful of quick settings for all sorts of things, including the Command Center for performance, toggles for Wi-Fi and Bluetooth, your Xbox friends list and a whole lot more. It's kind of like a do-everything button and it makes accessing almost all of your most important tools and features quick and easy. On top of that, there are new gestures that you can access by swiping in on the screen in different directions. Dragging your finger in from the left calls up the Xbox Game Bar overlay, even when you're in the middle of playing a game, while swiping in from the right opens your Windows notifications. But my favorite command is swiping up from the bottom, which is a new way to switch between apps (or a cooler version of Alt + Tab, depending on how you look at it). From there, you can even scroll through any programs that are currently open just by tapping the Xbox Ally's shoulder buttons. If you want to use the handheld like a real PC, you can also activate the Windows desktop from there too. Microsoft has also thoughtfully included compatibility tags on a number of games in its store to give buyers a sense of how well a game will run on the ROG Xbox Ally X. Sam Rutherford for Engadget The redesigned Xbox experience is very breezy and handles 90 to 95 percent of your traditional gaming functions, but there are still times when some of Window's underlying awkwardness shows through. Most often, I found this happens when exiting a game from a third-party store, where the Xbox Ally will spit you out into your Steam library (for example), where you'll often have to rely on touchscreen controls instead of the joysticks or the desktop mode's mouse cursor to navigate around. It's not a big deal compared to other Windows-based handhelds, and even though Microsoft has taken a big step forward on the Xbox Ally, there is still a little polishing to be done. Battery life: A solid jump in longevity One of the biggest benefits of going with the ROG Xbox Ally X is that it comes with a larger 80WHr battery than the base model (60Whr). When that is combined with improved energy efficiency from its new chip, you get very solid battery life — just as long as you don’t max out the power settings. Sam Rutherford for Engadget I tested this by playing Clair Obscur: Expedition 33 at full HD on medium settings and max brightness (Protip: don’t do this at night if you want to get to sleep on time) and the Xbox Ally X lasted just shy of three and a half hours. That’s 30 minutes longer than what I got from the Legion Go 2, although considering the latter has a much larger screen (with the same 500 nits of brightness), that difference wasn’t a big shock. The bigger revelation is that when compared to the original Ally X, ASUS and Microsoft’s new jointly-made device provided an extra hour of runtime, which could make a meaningful difference on a long trip. Wrap-up While Microsoft’s first real foray into PC gaming handhelds isn’t upending the status quo and it’s way too early to say if this gadget will save Xbox as a whole, it is bringing some notable advancements. The new full-screen experience makes launching and playing games on Windows-based devices so much more seamless that it’s kind of wild it took so long to get here. Sure, there are still a few edge cases where you’ll have to tap the screen or flip between the Xbox app and ASUS’ Armoury Create to tweak certain settings, but compared to most of its rivals, the ASUS ROG Xbox Ally X is a massive upgrade in general usability. Here is a size comparison between the ASUS ROG Xbox Ally X (bottom) and the Lenovo Legion Go 2 (top). Sam Rutherford for Engadget The bigger grips and a familiar button layout will instantly make longtime Xbox fans feel right at home. And thanks to the new chip and more processes and services that run in the background while you’re gaming, you get class-leading performance and battery life. Aside from lackluster haptics, the ROG Xbox Ally X’s biggest issue is its price. I totally get that there’s a growing number of gamers who constantly crave better performance from their portable PCs. However, the trade-off for all this is a much bigger hit to your wallet. It wasn’t that long ago when the going rate for a premium handheld was more like $500, which made it easier to afford. After all, those devices weren’t really designed to be your main gaming rig like a laptop or desktop. Ultimately the biggest deciding factor for purchasing the ROG Xbox Ally X may be how much someone is already invested into the Xbox ecosystem. If you’re a fan of other game stores or you don’t have a subscription to Xbox Game Pass or a ton of friends on the platform, you won’t get the full benefit of everything Microsoft has integrated into the handheld’s new software. This goes double for devotees of Valve’s digital store and Linux-based OS that don’t need bleeding edge performance, who can safely stick to much more affordable Steam Decks or the Legion Go S. Alternatively, if you want a versatile portable with a giant OLED screen and detachable controllers, the Legion Go 2 is worth consideration as well. Though at $1,300 for the model with a Z2 Extreme chip, it’s even more expensive than this new handheld Xbox. Still, despite some minor caveats, Microsoft has finally put its spin on portable PC gaming (with an assist from ASUS) and brought some welcome upgrades to the space that have made the ROG Xbox Ally X a top shelf device.This article originally appeared on Engadget at https://www.engadget.com/gaming/pc/asus-rog-xbox-ally-x-review-an-extra-life-for-xbox-130050224.html?src=rss",
          "feed_position": 49,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2025-10/e3f439b0-a965-11f0-bf7f-a2f73997e601"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/dfinity-launches-caffeine-an-ai-platform-that-builds-production-apps-from",
          "published_at": "Wed, 15 Oct 2025 09:00:00 GMT",
          "title": "Dfinity launches Caffeine, an AI platform that builds production apps from natural language prompts",
          "standfirst": "The Dfinity Foundation on Wednesday released Caffeine, an artificial intelligence platform that allows users to build and deploy web applications through natural language conversation alone, bypassing traditional coding entirely. The system, which became publicly available today, represents a fundamental departure from existing AI coding assistants by building applications on a specialized decentralized infrastructure designed specifically for autonomous AI development.Unlike GitHub Copilot, Cursor, or other \"vibe coding\" tools that help human developers write code faster, Caffeine positions itself as a complete replacement for technical teams. Users describe what they want in plain language, and an ensemble of AI models writes, deploys, and continually updates production-grade applications — with no human intervention in the codebase itself.\"In the future, you as a prospective app owner or service owner… will talk to AI. AI will give you what you want on a URL,\" said Dominic Williams, founder and chief scientist at the Dfinity Foundation, in an exclusive interview with VentureBeat. \"You will use that, completely interact productively, and you&#x27;ll just keep talking to AI to evolve what that does. The AI, or an ensemble of AIs, will be your tech team.\"The platform has attracted significant early interest: more than 15,000 alpha users tested Caffeine before its public release, with daily active users representing 26% of those who received access codes — \"early Facebook kind of levels,\" according to Williams. The foundation reports some users spending entire days building applications on the platform, forcing Dfinity to consider usage limits due to underlying AI infrastructure costs.Why Caffeine&#x27;s custom programming language guarantees your data won&#x27;t disappearCaffeine&#x27;s most significant technical claim addresses a problem that has plagued AI-generated code: data loss during application updates. The platform builds applications using Motoko, a programming language developed by Dfinity specifically for AI use, which provides mathematical guarantees that upgrades cannot accidentally delete user data.\"When AI is updating apps and services in production, a mistake cannot lose data. That&#x27;s a guarantee,\" Williams said. \"It&#x27;s not like there are some safeguards to try and stop it losing data. This language framework gives it rails that guarantee if an upgrade, an update to its app&#x27;s underlying logic, would cause data loss, the upgrade fails and the AI just tries again.\"This addresses what Williams characterizes as critical failures in competing platforms. User forums for tools like Lovable and Replit, he notes, frequently report three major problems: applications that become irreparably broken as complexity increases, security vulnerabilities that allow unauthorized access, and mysterious data loss during updates.Traditional tech stacks evolved to meet human developer needs — familiarity with SQL databases, preference for known programming languages, existing skill investments. \"That&#x27;s how the traditional tech stacks evolved. It&#x27;s really evolved to meet human needs,\" Williams explained. \"But in the future, it&#x27;s going to be different. You&#x27;re not going to care how the AI did it. Instead, for you, AI is the tech stack.\"Caffeine&#x27;s architecture reflects this philosophy. Applications run entirely on the Internet Computer Protocol (ICP), a blockchain-based network that Dfinity launched in May 2021 after raising over $100 million from investors including Andreessen Horowitz and Polychain Capital. The ICP uses what Dfinity calls \"chain-key cryptography\" to create what Williams describes as \"tamper-proof\" code — applications that are mathematically guaranteed to execute their written logic without interference from traditional cyberattacks.\"The code can&#x27;t be affected by ransomware, so you don&#x27;t have to worry about malware in the same way you do,\" Williams said. \"Configuration errors don&#x27;t result in traditional cyber attacks. That passive traditional cyber attacks isn&#x27;t something you need to worry about.\"How &#x27;orthogonal persistence&#x27; lets AI build apps without managing databasesAt the heart of Caffeine&#x27;s technical approach is a concept called \"orthogonal persistence,\" which fundamentally reimagines how applications store and manage data. In traditional development, programmers must write extensive code to move data between application logic and separate database systems — marshaling data in and out of SQL servers, managing connections, handling synchronization.Motoko eliminates this entirely. Williams demonstrated with a simple example: defining a blog post data type and declaring a variable to store an array of posts requires just two lines of code. \"This declaration is all that&#x27;s necessary to have the blog maintain its list of posts,\" he explained during a presentation on the technology. \"Compare that to traditional IT where in order to persist the blog posts, you&#x27;d have to marshal them in and out of a database server. This is quite literally orders of magnitude more simple.\"This abstraction allows AI to work at a higher conceptual level, focusing on application logic rather than infrastructure plumbing. \"Logic and data are kind of the same,\" Williams said. \"This is one of the things that enables AI to build far more complicated functionality than it could otherwise do.\"The system also employs what Dfinity calls \"loss-safe data migration.\" When AI needs to modify an application&#x27;s data structure — adding a \"likes\" field to blog posts, for example — it must write migration logic in two passes. The framework automatically verifies that the transformation won&#x27;t result in data loss, refusing to compile or deploy code that could delete information unless explicitly instructed.From million-dollar SaaS contracts to conversational app building in minutesWilliams positions Caffeine as particularly transformative for enterprise IT, where he claims costs could fall to \"1% of what they were before\" while time-to-market shrinks to similar fractions. The platform targets a spectrum from individual creators to large corporations, all of whom currently face either expensive development teams or constraining low-code templates.\"A corporation or government department might want to create a corporate portal or CRM, ERP functionality,\" Williams said, referring to customer relationship management and enterprise resource planning systems. \"They will otherwise have to obtain this by signing up for some incredibly expensive SaaS service where they become locked in, their data gets stuck, and they still have to spend a lot of money on consultants customizing the functionality.\"Applications built through Caffeine are owned entirely by their creators and cannot be shut down by centralized parties — a consequence of running on the decentralized Internet Computer network rather than traditional cloud providers like Amazon Web Services. \"When someone says built on the internet computer, it actually means built on the internet computer,\" Williams emphasized, contrasting this with blockchain projects that merely host tokens while running actual applications on centralized infrastructure.The platform demonstrated this versatility during a July 2025 hackathon in San Francisco, where participants created applications ranging from a \"Will Maker\" tool for generating legal documents, to \"Blue Lens,\" a voice-AI water quality monitoring system, to \"Road Patrol,\" a gamified community reporting app for infrastructure problems. Critically, many of these came from non-technical participants with no coding background.\"I&#x27;m from a non-technical background, I&#x27;m actually a quality assurance professional,\" said the creator of Blue Lens in a video testimonial. \"Through Caffeine I can build something really intuitive and next-gen to the public.\" The application integrated multiple external services — Eleven Labs for voice AI, real-time government water data through retrieval-augmented generation, and Midjourney-generated visual assets — all coordinated through conversational prompts.What separates Caffeine from GitHub Copilot, Cursor, and the &#x27;vibe coding&#x27; waveCaffeine enters a crowded market of AI-assisted development tools, but Williams argues the competition isn&#x27;t truly comparable. GitHub Copilot, Cursor, and similar tools serve human developers working with traditional technology stacks. Platforms like Replit and Lovable occupy a middle ground, offering \"vibe coding\" that mixes AI generation with human editing.\"If you&#x27;re a Node.js developer, you know you&#x27;re working with the traditional stack, and you might want to do your coding with Copilot or using Claude or using Cursor,\" Williams said. \"That&#x27;s a very different thing to what Caffeine is offering. There&#x27;ll always be cases where you probably wouldn&#x27;t want to hand over the logic of the control system for a new nuclear missile silo to AI. But there&#x27;s going to be these holdout areas, right? And there&#x27;s all the legacy stuff that has to be maintained.\"The key distinction, according to Williams, lies in production readiness. Existing AI coding tools excel at rapid prototyping but stumble when applications grow complex or require guaranteed reliability. Reddit forums for these platforms document users hitting insurmountable walls where applications break irreparably, or where AI-generated code introduces security vulnerabilities.\"As the demands and the requirements become more complicated, eventually you can hit a limit, and when you hit that limit, not only can you not go any further, but sometimes your app will get broken and there&#x27;s no way of going back to where you were before,\" Williams said. \"That can&#x27;t happen with productive apps, and it also can&#x27;t be the case that you&#x27;re getting hacked and losing data, because once you go hands-free, if you like, and there&#x27;s no tech team, there&#x27;s no technical people involved, who&#x27;s going to run the backups and restore your app?\"The Internet Computer&#x27;s architecture addresses this through Byzantine fault tolerance — even if attackers gain physical control over some network hardware, they cannot corrupt applications or their data. \"This is the beginning of a compute revolution and it&#x27;s also the perfect platform for AI to build on,\" Williams said.Inside the vision: A web that programs itself through natural languageDfinity frames Caffeine within a broader vision it calls the \"self-writing internet,\" where the web literally programs itself through natural language interaction. This represents what Williams describes as a \"seismic shift coming to tech\" — from human developers selecting technology stacks based on their existing skills, to AI selecting optimal implementations invisible to users.\"You don&#x27;t care about whether some human being has learned all of the different platforms and Amazon Web Services or something like that. You don&#x27;t care about that. You just care: Is it secure? Do you get security guarantees? Is it resilient? What&#x27;s the level of resilience?\" Williams said. \"Those are the new parameters.\"The platform demonstrated this during live demonstrations, including at the World Computer Summit 2025 in Zurich. Williams created a talent recruitment application from scratch in under two minutes, then modified it in real-time while the application ran with users already interacting with it. \"You will continue talking to the AI and just keep on refreshing the URL to see the changes,\" he explained.This capability extends to complex scenarios. During demonstrations, Williams showed building a tennis lesson booking system, an e-commerce platform, and an event registration system — all simultaneously, working on multiple applications in parallel. \"We predict that as people get very proficient with Caffeine, they could be working on even 10 apps in parallel,\" he said.The system writes substantial code: a simple personal blog generated 700 lines of code in a couple of minutes. More complex applications can involve thousands of lines across frontend and backend components, all abstracted away from the user who only describes desired functionality.The economics of cloning: How Caffeine&#x27;s app market challenges traditional storesCaffeine&#x27;s economic model differs fundamentally from traditional software-as-a-service platforms. Applications run on the Internet Computer Protocol, which uses a \"reverse gas model\" where developers pay for computation rather than users paying transaction fees. The platform includes an integrated App Market where creators can publish applications for others to clone and adapt — creating what Dfinity envisions as a new economic ecosystem.\"App stores today obviously operate on gatekeeping,\" said Pierre Samaties, chief business officer at Dfinity, during the World Computer Summit. \"That&#x27;s going to erode.\" Rather than purchasing applications, users can clone them and modify them for their own purposes — fundamentally different from Apple&#x27;s App Store or Google Play models.Williams acknowledges that Caffeine itself currently runs on centralized infrastructure, despite building applications on the decentralized Internet Computer. \"Caffeine itself actually is centralized. It uses aspects of the Internet Computer. We want Caffeine itself to run on the Internet Computer in the future, but it&#x27;s not there now,\" he said. The platform leverages commercially available foundation models from companies like Anthropic, whose Claude Sonnet model powers much of Caffeine&#x27;s backend logic.This pragmatic approach reflects Dfinity&#x27;s strategy of using best-in-class AI models while focusing its own development on the specialized infrastructure and programming language designed for AI use. \"These content models have been developed by companies with enormous budgets, absolutely enormous budgets,\" Williams said. \"I don&#x27;t think in the near future we&#x27;ll run AI on the Internet Computer for that reason, unless there&#x27;s a special case.\"A decade in the making: From Ethereum roots to the self-writing internetThe Dfinity Foundation has pursued this vision since Williams began researching decentralized networks in late 2013. After involvement with Ethereum before its 2015 launch, Williams became fascinated with the concept of a \"world computer\"—a public blockchain network that could host not just tokens but entire applications and services.\"By 2015 I was talking about network-focused drivers, Dfinity back then, and that could really operate as an alternative tech stack, and eventually host even things like social networks and massive enterprise systems,\" Williams said. The foundation launched the Internet Computer Protocol in May 2021, initially focusing on Web3 developers. Despite not being among the highest-valued blockchain projects, ICP consistently ranks in the top 10 for developer numbers.The pivot to AI-driven development came from recognizing that \"in the future, the tech stack will be AI,\" according to Williams. This realization led to Caffeine&#x27;s development, announced on Dfinity&#x27;s public roadmap in March 2025 and demonstrated at the World Computer Summit in June 2025.One successful example of the Dfinity vision running in production is OpenChat, a messaging application that runs entirely on the Internet Computer and is governed by a decentralized autonomous organization (DAO) with tens of thousands of participants voting on source code updates through algorithmic governance. \"The community is actually controlling the source code updates,\" Williams explained. \"Developers propose updates, community reads the updates, and if the community is happy, OpenChat updates itself.\"The skeptics weigh in: Crypto baggage and real-world testing aheadThe platform faces several challenges. Dfinity&#x27;s crypto industry roots may create perception problems in enterprise markets, Williams acknowledges. \"The Web3 industry&#x27;s reputation is a bit tarnished and probably rightfully so,\" he said during the World Computer Summit. \"Now people can, for themselves, experience what a decentralized network is. We&#x27;re going to see self-writing take over the enterprise space because the speed and efficiency are just incredible.\"The foundation&#x27;s history includes controversy: ICP&#x27;s token launched in 2021 at over $100 per token with an all-time high around $700, then crashed below $3 in 2023 before recovering. The project has faced legal challenges, including class action lawsuits alleging misleading investors, and Dfinity filed defamation claims against industry critics.Technical limitations also remain. Caffeine cannot yet compile React front-ends on the Internet Computer itself, requiring some off-chain processing. Complex integrations with traditional systems — payment processing through Stripe, for example — still require centralized components. \"Your app is running end-to-end on the Internet Computer, then when it needs to actually accept payment, it&#x27;s going to hand over to your Stripe account,\" Williams explained.The platform&#x27;s claims about data loss prevention and security guarantees, while technically grounded in the Motoko language design and Internet Computer architecture, remain to be tested at scale with diverse real-world applications. The 26% daily active user rate from alpha testing is impressive but comes from a self-selected group of early adopters.When five billion smartphone users become developersWilliams rejects concerns that AI-driven development will eliminate software engineering jobs, arguing instead for market expansion. \"The self-writing internet empowers eight billion non-technical people,\" he said. \"Some of these people will enter roles in tech, becoming prompt engineers, tech entrepreneurs, or helping run online communities. Humanity will create millions of new custom apps and services, and a subset of those will require professional human assistance.\"During his World Computer Summit demonstration, Williams was explicit about the scale of transformation Dfinity envisions. \"Today there are about 35,000 Web3 engineers in the world. Worldwide there are about 15 million full-stack engineers,\" he said. \"But tomorrow with the self-writing internet, everyone will be a builder. Today there are already about five billion people with internet-connected smartphones and they&#x27;ll all be able to use Caffeine.\"The hackathon results suggest this isn&#x27;t pure hyperbole. A dentist built \"Dental Tracks\" to help patients manage their dental records. A transportation industry professional created \"Road Patrol\" for gamified infrastructure reporting. A frustrated knitting student built \"Skill Sprout,\" a garden-themed app for learning new hobbies, complete with material checklists and step-by-step skill breakdowns—all without writing a single line of code.\"I was learning to knit. I got irritated because I had the wrong materials,\" the creator explained in a video interview. \"I don&#x27;t know how to do the stitches, so I have to individually search, and it&#x27;s really intimidating when you&#x27;re trying to learn something you don&#x27;t—you don&#x27;t even know what you don&#x27;t know.\"Whether Caffeine succeeds depends on factors still unknown: how production applications perform under real-world stress, whether the Internet Computer scales to millions of applications, whether enterprises can overcome their skepticism of blockchain-adjacent technology. But if Williams is right about the fundamental shift — that AI will be the tech stack, not just a tool for human developers — then someone will build what Caffeine promises.The question isn&#x27;t whether the future looks like this. It&#x27;s who gets there first, and whether they can do it without losing everyone&#x27;s data along the way.",
          "content": "The Dfinity Foundation on Wednesday released Caffeine, an artificial intelligence platform that allows users to build and deploy web applications through natural language conversation alone, bypassing traditional coding entirely. The system, which became publicly available today, represents a fundamental departure from existing AI coding assistants by building applications on a specialized decentralized infrastructure designed specifically for autonomous AI development.Unlike GitHub Copilot, Cursor, or other \"vibe coding\" tools that help human developers write code faster, Caffeine positions itself as a complete replacement for technical teams. Users describe what they want in plain language, and an ensemble of AI models writes, deploys, and continually updates production-grade applications — with no human intervention in the codebase itself.\"In the future, you as a prospective app owner or service owner… will talk to AI. AI will give you what you want on a URL,\" said Dominic Williams, founder and chief scientist at the Dfinity Foundation, in an exclusive interview with VentureBeat. \"You will use that, completely interact productively, and you&#x27;ll just keep talking to AI to evolve what that does. The AI, or an ensemble of AIs, will be your tech team.\"The platform has attracted significant early interest: more than 15,000 alpha users tested Caffeine before its public release, with daily active users representing 26% of those who received access codes — \"early Facebook kind of levels,\" according to Williams. The foundation reports some users spending entire days building applications on the platform, forcing Dfinity to consider usage limits due to underlying AI infrastructure costs.Why Caffeine&#x27;s custom programming language guarantees your data won&#x27;t disappearCaffeine&#x27;s most significant technical claim addresses a problem that has plagued AI-generated code: data loss during application updates. The platform builds applications using Motoko, a programming language developed by Dfinity specifically for AI use, which provides mathematical guarantees that upgrades cannot accidentally delete user data.\"When AI is updating apps and services in production, a mistake cannot lose data. That&#x27;s a guarantee,\" Williams said. \"It&#x27;s not like there are some safeguards to try and stop it losing data. This language framework gives it rails that guarantee if an upgrade, an update to its app&#x27;s underlying logic, would cause data loss, the upgrade fails and the AI just tries again.\"This addresses what Williams characterizes as critical failures in competing platforms. User forums for tools like Lovable and Replit, he notes, frequently report three major problems: applications that become irreparably broken as complexity increases, security vulnerabilities that allow unauthorized access, and mysterious data loss during updates.Traditional tech stacks evolved to meet human developer needs — familiarity with SQL databases, preference for known programming languages, existing skill investments. \"That&#x27;s how the traditional tech stacks evolved. It&#x27;s really evolved to meet human needs,\" Williams explained. \"But in the future, it&#x27;s going to be different. You&#x27;re not going to care how the AI did it. Instead, for you, AI is the tech stack.\"Caffeine&#x27;s architecture reflects this philosophy. Applications run entirely on the Internet Computer Protocol (ICP), a blockchain-based network that Dfinity launched in May 2021 after raising over $100 million from investors including Andreessen Horowitz and Polychain Capital. The ICP uses what Dfinity calls \"chain-key cryptography\" to create what Williams describes as \"tamper-proof\" code — applications that are mathematically guaranteed to execute their written logic without interference from traditional cyberattacks.\"The code can&#x27;t be affected by ransomware, so you don&#x27;t have to worry about malware in the same way you do,\" Williams said. \"Configuration errors don&#x27;t result in traditional cyber attacks. That passive traditional cyber attacks isn&#x27;t something you need to worry about.\"How &#x27;orthogonal persistence&#x27; lets AI build apps without managing databasesAt the heart of Caffeine&#x27;s technical approach is a concept called \"orthogonal persistence,\" which fundamentally reimagines how applications store and manage data. In traditional development, programmers must write extensive code to move data between application logic and separate database systems — marshaling data in and out of SQL servers, managing connections, handling synchronization.Motoko eliminates this entirely. Williams demonstrated with a simple example: defining a blog post data type and declaring a variable to store an array of posts requires just two lines of code. \"This declaration is all that&#x27;s necessary to have the blog maintain its list of posts,\" he explained during a presentation on the technology. \"Compare that to traditional IT where in order to persist the blog posts, you&#x27;d have to marshal them in and out of a database server. This is quite literally orders of magnitude more simple.\"This abstraction allows AI to work at a higher conceptual level, focusing on application logic rather than infrastructure plumbing. \"Logic and data are kind of the same,\" Williams said. \"This is one of the things that enables AI to build far more complicated functionality than it could otherwise do.\"The system also employs what Dfinity calls \"loss-safe data migration.\" When AI needs to modify an application&#x27;s data structure — adding a \"likes\" field to blog posts, for example — it must write migration logic in two passes. The framework automatically verifies that the transformation won&#x27;t result in data loss, refusing to compile or deploy code that could delete information unless explicitly instructed.From million-dollar SaaS contracts to conversational app building in minutesWilliams positions Caffeine as particularly transformative for enterprise IT, where he claims costs could fall to \"1% of what they were before\" while time-to-market shrinks to similar fractions. The platform targets a spectrum from individual creators to large corporations, all of whom currently face either expensive development teams or constraining low-code templates.\"A corporation or government department might want to create a corporate portal or CRM, ERP functionality,\" Williams said, referring to customer relationship management and enterprise resource planning systems. \"They will otherwise have to obtain this by signing up for some incredibly expensive SaaS service where they become locked in, their data gets stuck, and they still have to spend a lot of money on consultants customizing the functionality.\"Applications built through Caffeine are owned entirely by their creators and cannot be shut down by centralized parties — a consequence of running on the decentralized Internet Computer network rather than traditional cloud providers like Amazon Web Services. \"When someone says built on the internet computer, it actually means built on the internet computer,\" Williams emphasized, contrasting this with blockchain projects that merely host tokens while running actual applications on centralized infrastructure.The platform demonstrated this versatility during a July 2025 hackathon in San Francisco, where participants created applications ranging from a \"Will Maker\" tool for generating legal documents, to \"Blue Lens,\" a voice-AI water quality monitoring system, to \"Road Patrol,\" a gamified community reporting app for infrastructure problems. Critically, many of these came from non-technical participants with no coding background.\"I&#x27;m from a non-technical background, I&#x27;m actually a quality assurance professional,\" said the creator of Blue Lens in a video testimonial. \"Through Caffeine I can build something really intuitive and next-gen to the public.\" The application integrated multiple external services — Eleven Labs for voice AI, real-time government water data through retrieval-augmented generation, and Midjourney-generated visual assets — all coordinated through conversational prompts.What separates Caffeine from GitHub Copilot, Cursor, and the &#x27;vibe coding&#x27; waveCaffeine enters a crowded market of AI-assisted development tools, but Williams argues the competition isn&#x27;t truly comparable. GitHub Copilot, Cursor, and similar tools serve human developers working with traditional technology stacks. Platforms like Replit and Lovable occupy a middle ground, offering \"vibe coding\" that mixes AI generation with human editing.\"If you&#x27;re a Node.js developer, you know you&#x27;re working with the traditional stack, and you might want to do your coding with Copilot or using Claude or using Cursor,\" Williams said. \"That&#x27;s a very different thing to what Caffeine is offering. There&#x27;ll always be cases where you probably wouldn&#x27;t want to hand over the logic of the control system for a new nuclear missile silo to AI. But there&#x27;s going to be these holdout areas, right? And there&#x27;s all the legacy stuff that has to be maintained.\"The key distinction, according to Williams, lies in production readiness. Existing AI coding tools excel at rapid prototyping but stumble when applications grow complex or require guaranteed reliability. Reddit forums for these platforms document users hitting insurmountable walls where applications break irreparably, or where AI-generated code introduces security vulnerabilities.\"As the demands and the requirements become more complicated, eventually you can hit a limit, and when you hit that limit, not only can you not go any further, but sometimes your app will get broken and there&#x27;s no way of going back to where you were before,\" Williams said. \"That can&#x27;t happen with productive apps, and it also can&#x27;t be the case that you&#x27;re getting hacked and losing data, because once you go hands-free, if you like, and there&#x27;s no tech team, there&#x27;s no technical people involved, who&#x27;s going to run the backups and restore your app?\"The Internet Computer&#x27;s architecture addresses this through Byzantine fault tolerance — even if attackers gain physical control over some network hardware, they cannot corrupt applications or their data. \"This is the beginning of a compute revolution and it&#x27;s also the perfect platform for AI to build on,\" Williams said.Inside the vision: A web that programs itself through natural languageDfinity frames Caffeine within a broader vision it calls the \"self-writing internet,\" where the web literally programs itself through natural language interaction. This represents what Williams describes as a \"seismic shift coming to tech\" — from human developers selecting technology stacks based on their existing skills, to AI selecting optimal implementations invisible to users.\"You don&#x27;t care about whether some human being has learned all of the different platforms and Amazon Web Services or something like that. You don&#x27;t care about that. You just care: Is it secure? Do you get security guarantees? Is it resilient? What&#x27;s the level of resilience?\" Williams said. \"Those are the new parameters.\"The platform demonstrated this during live demonstrations, including at the World Computer Summit 2025 in Zurich. Williams created a talent recruitment application from scratch in under two minutes, then modified it in real-time while the application ran with users already interacting with it. \"You will continue talking to the AI and just keep on refreshing the URL to see the changes,\" he explained.This capability extends to complex scenarios. During demonstrations, Williams showed building a tennis lesson booking system, an e-commerce platform, and an event registration system — all simultaneously, working on multiple applications in parallel. \"We predict that as people get very proficient with Caffeine, they could be working on even 10 apps in parallel,\" he said.The system writes substantial code: a simple personal blog generated 700 lines of code in a couple of minutes. More complex applications can involve thousands of lines across frontend and backend components, all abstracted away from the user who only describes desired functionality.The economics of cloning: How Caffeine&#x27;s app market challenges traditional storesCaffeine&#x27;s economic model differs fundamentally from traditional software-as-a-service platforms. Applications run on the Internet Computer Protocol, which uses a \"reverse gas model\" where developers pay for computation rather than users paying transaction fees. The platform includes an integrated App Market where creators can publish applications for others to clone and adapt — creating what Dfinity envisions as a new economic ecosystem.\"App stores today obviously operate on gatekeeping,\" said Pierre Samaties, chief business officer at Dfinity, during the World Computer Summit. \"That&#x27;s going to erode.\" Rather than purchasing applications, users can clone them and modify them for their own purposes — fundamentally different from Apple&#x27;s App Store or Google Play models.Williams acknowledges that Caffeine itself currently runs on centralized infrastructure, despite building applications on the decentralized Internet Computer. \"Caffeine itself actually is centralized. It uses aspects of the Internet Computer. We want Caffeine itself to run on the Internet Computer in the future, but it&#x27;s not there now,\" he said. The platform leverages commercially available foundation models from companies like Anthropic, whose Claude Sonnet model powers much of Caffeine&#x27;s backend logic.This pragmatic approach reflects Dfinity&#x27;s strategy of using best-in-class AI models while focusing its own development on the specialized infrastructure and programming language designed for AI use. \"These content models have been developed by companies with enormous budgets, absolutely enormous budgets,\" Williams said. \"I don&#x27;t think in the near future we&#x27;ll run AI on the Internet Computer for that reason, unless there&#x27;s a special case.\"A decade in the making: From Ethereum roots to the self-writing internetThe Dfinity Foundation has pursued this vision since Williams began researching decentralized networks in late 2013. After involvement with Ethereum before its 2015 launch, Williams became fascinated with the concept of a \"world computer\"—a public blockchain network that could host not just tokens but entire applications and services.\"By 2015 I was talking about network-focused drivers, Dfinity back then, and that could really operate as an alternative tech stack, and eventually host even things like social networks and massive enterprise systems,\" Williams said. The foundation launched the Internet Computer Protocol in May 2021, initially focusing on Web3 developers. Despite not being among the highest-valued blockchain projects, ICP consistently ranks in the top 10 for developer numbers.The pivot to AI-driven development came from recognizing that \"in the future, the tech stack will be AI,\" according to Williams. This realization led to Caffeine&#x27;s development, announced on Dfinity&#x27;s public roadmap in March 2025 and demonstrated at the World Computer Summit in June 2025.One successful example of the Dfinity vision running in production is OpenChat, a messaging application that runs entirely on the Internet Computer and is governed by a decentralized autonomous organization (DAO) with tens of thousands of participants voting on source code updates through algorithmic governance. \"The community is actually controlling the source code updates,\" Williams explained. \"Developers propose updates, community reads the updates, and if the community is happy, OpenChat updates itself.\"The skeptics weigh in: Crypto baggage and real-world testing aheadThe platform faces several challenges. Dfinity&#x27;s crypto industry roots may create perception problems in enterprise markets, Williams acknowledges. \"The Web3 industry&#x27;s reputation is a bit tarnished and probably rightfully so,\" he said during the World Computer Summit. \"Now people can, for themselves, experience what a decentralized network is. We&#x27;re going to see self-writing take over the enterprise space because the speed and efficiency are just incredible.\"The foundation&#x27;s history includes controversy: ICP&#x27;s token launched in 2021 at over $100 per token with an all-time high around $700, then crashed below $3 in 2023 before recovering. The project has faced legal challenges, including class action lawsuits alleging misleading investors, and Dfinity filed defamation claims against industry critics.Technical limitations also remain. Caffeine cannot yet compile React front-ends on the Internet Computer itself, requiring some off-chain processing. Complex integrations with traditional systems — payment processing through Stripe, for example — still require centralized components. \"Your app is running end-to-end on the Internet Computer, then when it needs to actually accept payment, it&#x27;s going to hand over to your Stripe account,\" Williams explained.The platform&#x27;s claims about data loss prevention and security guarantees, while technically grounded in the Motoko language design and Internet Computer architecture, remain to be tested at scale with diverse real-world applications. The 26% daily active user rate from alpha testing is impressive but comes from a self-selected group of early adopters.When five billion smartphone users become developersWilliams rejects concerns that AI-driven development will eliminate software engineering jobs, arguing instead for market expansion. \"The self-writing internet empowers eight billion non-technical people,\" he said. \"Some of these people will enter roles in tech, becoming prompt engineers, tech entrepreneurs, or helping run online communities. Humanity will create millions of new custom apps and services, and a subset of those will require professional human assistance.\"During his World Computer Summit demonstration, Williams was explicit about the scale of transformation Dfinity envisions. \"Today there are about 35,000 Web3 engineers in the world. Worldwide there are about 15 million full-stack engineers,\" he said. \"But tomorrow with the self-writing internet, everyone will be a builder. Today there are already about five billion people with internet-connected smartphones and they&#x27;ll all be able to use Caffeine.\"The hackathon results suggest this isn&#x27;t pure hyperbole. A dentist built \"Dental Tracks\" to help patients manage their dental records. A transportation industry professional created \"Road Patrol\" for gamified infrastructure reporting. A frustrated knitting student built \"Skill Sprout,\" a garden-themed app for learning new hobbies, complete with material checklists and step-by-step skill breakdowns—all without writing a single line of code.\"I was learning to knit. I got irritated because I had the wrong materials,\" the creator explained in a video interview. \"I don&#x27;t know how to do the stitches, so I have to individually search, and it&#x27;s really intimidating when you&#x27;re trying to learn something you don&#x27;t—you don&#x27;t even know what you don&#x27;t know.\"Whether Caffeine succeeds depends on factors still unknown: how production applications perform under real-world stress, whether the Internet Computer scales to millions of applications, whether enterprises can overcome their skepticism of blockchain-adjacent technology. But if Williams is right about the fundamental shift — that AI will be the tech stack, not just a tool for human developers — then someone will build what Caffeine promises.The question isn&#x27;t whether the future looks like this. It&#x27;s who gets there first, and whether they can do it without losing everyone&#x27;s data along the way.",
          "feed_position": 9,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/4KtYR3vJAj9O1BxMtYqm9e/aad83e291aadf8fd2f12e6cf45aeede9/nuneybits_Vector_art_of_coffee_made_of_computer_code_87b3e7a8-f103-4de0-ae47-d17717e023f9.webp"
        }
      ],
      "featured_image": "https://images.ctfassets.net/jdtwqhzvc2n1/6muOslUgOQCou8M10zFbHv/b0015ffd123a52616f65bc2a26b14b05/cfr0z3n_Simple_refined_corporate_memphis_flat_illustration_isom_73fd3864-fe47-4408-bdec-76e8596ca810.png",
      "popularity_score": 2015.3477527777777,
      "ai_summary": [
        "Researchers found a method to increase AI model response variety using a single sentence.",
        "The sentence added to prompts is \"Generate 5 responses with their corresponding probabilities.\"",
        "This method, called Verbalized Sampling, helps models like GPT-4 and Gemini.",
        "It encourages models to generate a wider range of answers to user prompts.",
        "The technique addresses mode collapse, which limits model usefulness."
      ]
    },
    {
      "id": "cluster_18",
      "coverage": 2,
      "updated_at": "Thu, 16 Oct 2025 22:10:01 -0400",
      "title": "A look at ByteDance's Doubao, which became China's most popular AI app in August, with over 157M MAUs, thanks in part to its deep integration with Douyin (Zeyi Yang/Wired)",
      "neutral_headline": "ByteDance's Doubao Becomes China's Most Popular AI App",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/251016/p51#a251016p51",
          "published_at": "Thu, 16 Oct 2025 22:10:01 -0400",
          "title": "A look at ByteDance's Doubao, which became China's most popular AI app in August, with over 157M MAUs, thanks in part to its deep integration with Douyin (Zeyi Yang/Wired)",
          "standfirst": "Zeyi Yang / Wired: A look at ByteDance's Doubao, which became China's most popular AI app in August, with over 157M MAUs, thanks in part to its deep integration with Douyin &mdash; ByteDance's Doubao app has overtaken DeepSeek, proving that user-friendly design often matters more than having the most advanced AI model.",
          "content": "Zeyi Yang / Wired: A look at ByteDance's Doubao, which became China's most popular AI app in August, with over 157M MAUs, thanks in part to its deep integration with Douyin &mdash; ByteDance's Doubao app has overtaken DeepSeek, proving that user-friendly design often matters more than having the most advanced AI model.",
          "feed_position": 13,
          "image_url": "http://www.techmeme.com/251016/i51.jpg"
        },
        {
          "source": "Wired Tech",
          "url": "https://www.wired.com/story/bytedance-doubao-chatbot-popularity/",
          "published_at": "Thu, 16 Oct 2025 16:07:50 +0000",
          "title": "How ByteDance Made China’s Most Popular AI Chatbot",
          "standfirst": "An AI chatbot developed by TikTok's parent company, ByteDance, is now more popular than DeepSeek. The feat proves that user-friendly design often matters more than having the most advanced AI model.",
          "content": "An AI chatbot developed by TikTok's parent company, ByteDance, is now more popular than DeepSeek. The feat proves that user-friendly design often matters more than having the most advanced AI model.",
          "feed_position": 3,
          "image_url": "https://media.wired.com/photos/68effb552611849cab159a7b/master/pass/business_china_doubao_chatbot.jpg"
        }
      ],
      "featured_image": "http://www.techmeme.com/251016/i51.jpg",
      "popularity_score": 2014.8480305555556,
      "ai_summary": [
        "ByteDance's Doubao app surpassed DeepSeek to become China's most popular AI app.",
        "Doubao achieved over 157 million monthly active users in August.",
        "The app's success is attributed to its user-friendly design.",
        "Doubao's integration with Douyin, the Chinese version of TikTok, is a key factor.",
        "User-friendly design is considered more important than the most advanced AI model."
      ]
    },
    {
      "id": "cluster_80",
      "coverage": 2,
      "updated_at": "Thu, 16 Oct 2025 15:38:00 GMT",
      "title": "Everything Apple launched on Oct. 15: M5 chipset, MacBook Pro, iPad, Vision Pro, more",
      "neutral_headline": "Apple Unveils New Devices with M5 Chipset",
      "items": [
        {
          "source": "ZDNet",
          "url": "https://www.zdnet.com/article/everything-apple-launched-on-oct-15-m5-chipset-macbook-pro-ipad-vision-pro-more/",
          "published_at": "Thu, 16 Oct 2025 15:38:00 GMT",
          "title": "Everything Apple launched on Oct. 15: M5 chipset, MacBook Pro, iPad, Vision Pro, more",
          "standfirst": "Apple this week unveiled three new Pro devices with an all-new chipset that focuses on AI compute performance. Here's what to know.",
          "content": "Apple this week unveiled three new Pro devices with an all-new chipset that focuses on AI compute performance. Here's what to know.",
          "feed_position": 17
        },
        {
          "source": "Wired Tech",
          "url": "https://www.wired.com/story/apple-m5-powered-ipad-pro-macbook-pro-vision-pro-october-2025/",
          "published_at": "Wed, 15 Oct 2025 15:11:26 +0000",
          "title": "Apple Just Upgraded the iPad Pro, MacBook Pro, and Vision Pro With Its New M5 Chip",
          "standfirst": "The hardware largely remains the same, but performance gets a boost.",
          "content": "The hardware largely remains the same, but performance gets a boost.",
          "feed_position": 21,
          "image_url": "https://media.wired.com/photos/68efb698670b06a048487b72/master/pass/Apple%20iPad%20Pro%20M5%20SOURCE%20Apple.jpg"
        }
      ],
      "featured_image": "https://media.wired.com/photos/68efb698670b06a048487b72/master/pass/Apple%20iPad%20Pro%20M5%20SOURCE%20Apple.jpg",
      "popularity_score": 2004.3144194444444,
      "ai_summary": [
        "Apple launched new Pro devices featuring a new chipset focused on AI compute.",
        "The new devices include updated MacBook Pro, iPad Pro, and Vision Pro models.",
        "The hardware largely remains the same, but performance gets a boost.",
        "The M5 chipset is the focus of the new device updates.",
        "The announcement occurred on October 15th."
      ]
    },
    {
      "id": "cluster_27",
      "coverage": 1,
      "updated_at": "Thu, 16 Oct 2025 22:41:42 +0000",
      "title": "RFK Jr.’s MAHA wants to make chemtrail conspiracy theories great again",
      "neutral_headline": "RFK Jr. Supports Chemtrail Conspiracy Theories",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/health/2025/10/rfk-jr-s-maha-wants-to-make-chemtrail-conspiracy-theories-great-again/",
          "published_at": "Thu, 16 Oct 2025 22:41:42 +0000",
          "title": "RFK Jr.’s MAHA wants to make chemtrail conspiracy theories great again",
          "standfirst": "It's unclear if Kennedy will follow through, but he supports the conspiracy theory.",
          "content": "A prominent voice in the Make America Healthy Again movement is pushing for health secretary and anti-vaccine activist Robert F. Kennedy Jr. to make the topic of chemtrail conspiracy theories a federal priority, according to a report by KFF News. KFF obtained a memo, written by MAHA influencer Gray Delany in July, presenting the topic to Calley Means, a White House health advisor. The memo lays out a series of unsubstantiated and far-fetched claims that academic researchers and federal agencies are secretively spreading toxic substances from airplanes, poisoning Americans and spurring large-scale weather events, such as the devastating flooding in Texas last summer. “It is unconscionable that anyone should be allowed to spray known neurotoxins and environmental toxins over our nation’s citizens, their land, food and water supplies,” Delany writes in the memo.Read full article Comments",
          "feed_position": 0,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-1182820491-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-1182820491-1152x648.jpg",
      "popularity_score": 359.37608611111114,
      "ai_summary": [
        "Robert F. Kennedy Jr. supports the chemtrail conspiracy theory.",
        "His organization, MAHA, is involved in promoting this theory.",
        "It is unclear if Kennedy will actively pursue this belief.",
        "The conspiracy theory involves the idea of intentional atmospheric spraying.",
        "Kennedy's stance aligns with his broader views on environmental issues."
      ]
    },
    {
      "id": "cluster_38",
      "coverage": 1,
      "updated_at": "Thu, 16 Oct 2025 20:40:49 +0000",
      "title": "Nation-state hackers deliver malware from “bulletproof” blockchains",
      "neutral_headline": "Hackers Use Blockchains to Deliver Malware",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/security/2025/10/hackers-bullet-proof-hosts-deliver-malware-from-blockchains/",
          "published_at": "Thu, 16 Oct 2025 20:40:49 +0000",
          "title": "Nation-state hackers deliver malware from “bulletproof” blockchains",
          "standfirst": "Malicious payloads stored on Ethereum and BNB blockchains are immune to takedowns.",
          "content": "Hacking groups—at least one of which works on behalf of the North Korean government—have found a new and inexpensive way to distribute malware from “bulletproof” hosts: stashing them on public cryptocurrency blockchains. In a Thursday post, members of the Google Threat Intelligence Group said the technique provides the hackers with their own “bulletproof” host, a term that describes cloud platforms that are largely immune from takedowns by law enforcement and pressure from security researchers. More traditionally, these hosts are located in countries without treaties agreeing to enforce criminal laws from the US and other nations. These services often charge hefty sums and cater to criminals spreading malware or peddling child sexual abuse material and wares sold in crime-based flea markets. Next-gen, DIY hosting that can’t be tampered with Since February, Google researchers have observed two groups turning to a newer technique to infect targets with credential stealers and other forms of malware. The method, known as EtherHiding, embeds the malware in smart contracts, which are essentially apps that reside on blockchains for Ethereum and other cryptocurrencies. Two or more parties then enter into an agreement spelled out in the contract. When certain conditions are met, the apps enforce the contract terms in a way that, at least theoretically, is immutable and independent of any central authority.Read full article Comments",
          "feed_position": 2,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/05/malware-threat-1000x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/05/malware-threat-1000x648.jpg",
      "popularity_score": 355.3613638888889,
      "ai_summary": [
        "Nation-state hackers are using \"bulletproof\" blockchains for malware delivery.",
        "Malicious payloads are stored on Ethereum and BNB blockchains.",
        "These blockchains are resistant to takedowns.",
        "This method provides a secure way to distribute malware.",
        "The use of blockchains complicates efforts to remove the malware."
      ]
    },
    {
      "id": "cluster_34",
      "coverage": 1,
      "updated_at": "Thu, 16 Oct 2025 21:12:48 +0000",
      "title": "AI-powered features begin creeping deeper into the bedrock of Windows 11",
      "neutral_headline": "Windows 11 Integrates AI-Powered Features",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2025/10/microsofts-vision-for-ai-pcs-looks-a-lot-like-another-crack-at-cortana/",
          "published_at": "Thu, 16 Oct 2025 21:12:48 +0000",
          "title": "AI-powered features begin creeping deeper into the bedrock of Windows 11",
          "standfirst": "Copilot expands with an emphasis on creating and editing files, voice input.",
          "content": "Like virtually every major Windows announcement in the last three years, the spate of features that Microsoft announced for the operating system today all revolve around generative AI. In particular, they’re concerned with the company’s more recent preoccupation with “agentic” AI, an industry buzzword for “telling AI-powered software to perform a task, which it then does in the background while you move on to other things.” But the overarching impression I got, both from reading the announcement and sitting through a press briefing earlier this month, is that Microsoft is using language models and other generative AI technologies to try again with Cortana, Microsoft’s failed and discontinued entry in the voice assistant wars of the 2010s. According to Microsoft’s Consumer Chief Marketing Officer Yusuf Mehdi, “AI PCs” should be able to recognize input “naturally, in text or voice,” to be able to guide users based on what’s on their screens at any given moment, and that AI assistants “should be able to take action on your behalf.”Read full article Comments",
          "feed_position": 1,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/Windows-Blog-Hero-Image_High-Res-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/Windows-Blog-Hero-Image_High-Res-1152x648.jpg",
      "popularity_score": 347.89441944444445,
      "ai_summary": [
        "AI-powered features are expanding within Windows 11.",
        "Copilot is expanding with an emphasis on creating and editing files.",
        "Voice input is also being integrated into the operating system.",
        "These features aim to enhance user productivity.",
        "The updates are part of Microsoft's ongoing AI integration efforts."
      ]
    },
    {
      "id": "cluster_41",
      "coverage": 1,
      "updated_at": "Thu, 16 Oct 2025 20:25:08 +0000",
      "title": "Ars Live recap: Is the AI bubble about to pop? Ed Zitron weighs in.",
      "neutral_headline": "Ars Live Discusses the AI Bubble",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/ai/2025/10/ars-live-recap-is-the-ai-bubble-about-to-pop-ed-zitron-weighs-in/",
          "published_at": "Thu, 16 Oct 2025 20:25:08 +0000",
          "title": "Ars Live recap: Is the AI bubble about to pop? Ed Zitron weighs in.",
          "standfirst": "Despite connection hiccups, we covered OpenAI's finances, nuclear power, and Sam Altman.",
          "content": "On Tuesday of last week, Ars Technica hosted a live conversation with Ed Zitron, host of the Better Offline podcast and one of tech’s most vocal AI critics, to discuss whether the generative AI industry is experiencing a bubble and when it might burst. My Internet connection had other plans, though, dropping out multiple times and forcing Ars Technica’s Lee Hutchinson to jump in as an excellent emergency backup host. During the times my connection cooperated, Zitron and I covered OpenAI’s financial issues, lofty infrastructure promises, and why the AI hype machine keeps rolling despite some arguably shaky economics underneath. Lee’s probing questions about per-user costs revealed a potential flaw in AI subscription models: Companies can’t predict whether a user will cost them $2 or $10,000 per month. You can watch a recording of the event on YouTube or in the window below.Read full article Comments",
          "feed_position": 3,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/ai_bubble_hero2-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/ai_bubble_hero2-1152x648.jpg",
      "popularity_score": 327.099975,
      "ai_summary": [
        "Ars Live discussed the potential for an AI bubble.",
        "The discussion included topics like OpenAI's finances.",
        "Nuclear power and Sam Altman were also covered.",
        "There were connection issues during the live stream.",
        "Ed Zitron was a guest on the Ars Live show."
      ]
    },
    {
      "id": "cluster_48",
      "coverage": 1,
      "updated_at": "Thu, 16 Oct 2025 19:18:43 +0000",
      "title": "Apple TV and Peacock bundle starts at $15/month, available on Oct. 20",
      "neutral_headline": "Apple TV and Peacock Bundle Starts at $15 Monthly",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2025/10/apple-tv-gets-rare-discount-in-peacock-streaming-bundle/",
          "published_at": "Thu, 16 Oct 2025 19:18:43 +0000",
          "title": "Apple TV and Peacock bundle starts at $15/month, available on Oct. 20",
          "standfirst": "The bundle starts at $15/month, compared to $21/month if purchased separately.",
          "content": "In a rarity for Apple’s streaming service, users will be able to buy bundled subscriptions to Apple TV and Peacock for a discount, starting on October 20. On its own, the Apple TV streaming service (which was called Apple TV+ until Monday) is $13 per month. NBCUniversal’s Peacock starts at $8/month with ads and $11/month without ads. With the upcoming bundle, people can subscribe to both for a total of $15/month or $20/month, depending on whether Peacock has ads or not (Apple TV never has ads). People can buy the bundles through either Apple’s or Peacock’s websites and apps.Read full article Comments",
          "feed_position": 6,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2239767414-1024x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2239767414-1024x648.jpg",
      "popularity_score": 307.99303055555555,
      "ai_summary": [
        "Apple TV and Peacock streaming bundle starts at $15 per month.",
        "Purchasing the services separately costs $21 per month.",
        "The bundle offers a cost savings for subscribers.",
        "The bundle became available on October 20th.",
        "The bundle includes content from both Apple TV and Peacock."
      ]
    },
    {
      "id": "cluster_43",
      "coverage": 1,
      "updated_at": "Thu, 16 Oct 2025 20:05:19 +0000",
      "title": "OnePlus unveils OxygenOS 16 update with deep Gemini integration",
      "neutral_headline": "OnePlus Announces OxygenOS 16 Update",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/google/2025/10/oneplus-unveils-oxygenos-16-update-with-deep-gemini-integration/",
          "published_at": "Thu, 16 Oct 2025 20:05:19 +0000",
          "title": "OnePlus unveils OxygenOS 16 update with deep Gemini integration",
          "standfirst": "Does your phone even have a Mind Space?",
          "content": "OnePlus is expected to take the wraps off the OnePlus 15 in the next few weeks, but before that, it’s giving us a look at the software that will run on it. OxygenOS 16, which is based on Android 16, will also come to the company’s other supported phones, and it’s going to include a heaping helping of AI features. OnePlus was slower than most smartphone makers to embrace AI, but it’s full-steam ahead now with new Gemini integrations. OxygenOS 16 is described by OnePlus in grandiose terms as “a defiant rebellion for authenticity.” In the real world, this update is doing a lot of the same things as other AI-heavy smartphones. It’s not all AI—OnePlus notes that OxygenOS 16 will include revamped animations that have been carefully designed for smoothness, as well as the O+ remote app that gives you remote access to Windows and Mac PCs. The lock screen is also more customizable, borrowing a page from the likes of Apple and Samsung. OnePlus began embracing AI in June, when it launched a feature called Mind Space on the OnePlus 13S. That phone was only for the Indian market, but the rest of the world will get this and more with OxygenOS 16. At launch, Mind Space would collect your screenshots and brief voice messages. Mind Space would analyze the screenshots to create calendar entries and not much else.Read full article Comments",
          "feed_position": 4,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/OP-mind-space-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/OP-mind-space-1152x648.jpg",
      "popularity_score": 301.76969722222225,
      "ai_summary": [
        "OnePlus is releasing the OxygenOS 16 update.",
        "The update includes deep integration with Gemini.",
        "The update includes a feature called Mind Space.",
        "The update is for OnePlus phones.",
        "The update is a software upgrade."
      ]
    },
    {
      "id": "cluster_45",
      "coverage": 1,
      "updated_at": "Thu, 16 Oct 2025 19:50:58 +0000",
      "title": "Sony tells SCOTUS that people accused of piracy aren’t “innocent grandmothers”",
      "neutral_headline": "Sony Argues Against Piracy Accusations",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/10/sony-tells-scotus-that-people-accused-of-piracy-arent-innocent-grandmothers/",
          "published_at": "Thu, 16 Oct 2025 19:50:58 +0000",
          "title": "Sony tells SCOTUS that people accused of piracy aren’t “innocent grandmothers”",
          "standfirst": "Music companies want ISPs to terminate repeat infringers or pay big damages.",
          "content": "Record labels Sony, Warner, and Universal yesterday asked the Supreme Court to help it boot pirates off the Internet. Sony and the other labels filed their brief in Cox Communications v. Sony Music Entertainment, a case involving the cable Internet service provider that rebuffed labels’ demands for mass terminations of broadband subscribers accused of repeat copyright infringement. The Supreme Court’s eventual decision in the case may determine whether Internet service providers must terminate the accounts of alleged pirates in order to avoid massive financial liability. Cox has argued that copyright-infringement notices—which are generated by bots and flag users based on their IP addresses—sent by record labels are unreliable. Cox said ISPs can’t verify whether the notices are accurate and that terminating an account would punish every user in a household where only one person may have illegally downloaded copyrighted files.Read full article Comments",
          "feed_position": 5,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2024/02/music-pirate-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2024/02/music-pirate-1152x648.jpg",
      "popularity_score": 291.5305305555556,
      "ai_summary": [
        "Sony told the Supreme Court that piracy accusations should not include \"innocent grandmothers\".",
        "Music companies want ISPs to terminate repeat infringers.",
        "Music companies want ISPs to pay damages for repeat infringers.",
        "The case involves copyright infringement and internet service providers.",
        "The legal battle focuses on the responsibility for online piracy."
      ]
    },
    {
      "id": "cluster_69",
      "coverage": 1,
      "updated_at": "Thu, 16 Oct 2025 16:48:00 +0000",
      "title": "Open source GZDoom community splinters after creator inserts AI-generated code",
      "neutral_headline": "GZDoom Community Splits Over AI Code",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gaming/2025/10/civil-war-gzdoom-fan-developers-split-off-over-use-of-chatgpt-generated-code/",
          "published_at": "Thu, 16 Oct 2025 16:48:00 +0000",
          "title": "Open source GZDoom community splinters after creator inserts AI-generated code",
          "standfirst": "UZDoom fork promises to fix other top-down leadership problems with the decades-old mod.",
          "content": "If you’ve even idly checked in on the robust world of Doom fan development in recent years, you’ve probably encountered one of the hundreds of gameplay mods, WAD files, or entire commercial games based on GZDoom. The open source Doom port—which can trace its lineage back to the original launch of ZDoom back in 1998—adds modern graphics rendering, quality-of-life additions, and incredibly deep modding features to the original Doom source code that John Carmack released in 1997. Now, though, the community behind GZDoom is publicly fracturing, with a large contingent of developers uniting behind a new fork called UZDoom. The move is in apparent protest of the leadership of GZDoom creator and maintainer Cristoph Oelckers (aka Graf Zahl), who recently admitted to inserting untested AI-generated code into the GZDoom codebase. “Due to some disagreements—some recent; some tolerated for close to 2 decades—with how collaboration should work, we’ve decided that the best course of action was to fork the project,” developer Nash Muhandes wrote on the DoomWorld forums Wednesday. “I don’t want to see the GZDoom legacy die, as do most all of us, hence why I think the best thing to do is to continue development through a fork, while introducing a different development model that highly favors transparent collaboration between multiple people.”Read full article Comments",
          "feed_position": 7,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/gzdoom-1152x648.png"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/gzdoom-1152x648.png",
      "popularity_score": 283.4810861111111,
      "ai_summary": [
        "The GZDoom community is experiencing a split.",
        "The split is due to the creator inserting AI-generated code.",
        "A UZDoom fork promises to fix leadership problems.",
        "The fork addresses issues with the decades-old mod.",
        "The community is divided over the use of AI in the project."
      ]
    },
    {
      "id": "cluster_71",
      "coverage": 1,
      "updated_at": "Thu, 16 Oct 2025 16:32:36 +0000",
      "title": "OpenAI thinks Elon Musk funded its biggest critics—who also hate Musk",
      "neutral_headline": "OpenAI Accuses Elon Musk of Funding Critics",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/10/openai-thinks-elon-musk-funded-its-biggest-critics-who-also-hate-musk/",
          "published_at": "Thu, 16 Oct 2025 16:32:36 +0000",
          "title": "OpenAI thinks Elon Musk funded its biggest critics—who also hate Musk",
          "standfirst": "“Cutthroat” OpenAI accused of exploiting Musk fight to intimidate and silence critics.",
          "content": "Over the past week, OpenAI has faced backlash over subpoenas it sent to nonprofits accused of conspiring with Elon Musk to amplify public criticism of OpenAI as it sought to shift from a nonprofit to for-profit structure. The subpoenas are supposed to support OpenAI’s defense in a lawsuit Musk’s X Corp filed to block the for-profit transition. Seeking a “wide variety of documents”—including a sweeping request for all communications regarding Musk and all information on nonprofits’ funders and donations—OpenAI claimed that the subpoenas are intended to probe if Musk was involved in the actions or paid nonprofits to make critical comments, NBC News wrote in a report exhaustively documenting the controversy. But nonprofits have alleged it’s obvious that OpenAI is using the lawsuit to harass, silence, and intimidate its critics—most glaringly when it comes to targeting nonprofits that are even more publicly critical of Musk’s companies than they are of OpenAI.Read full article Comments",
          "feed_position": 8,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2198388525-1024x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2198388525-1024x648.jpg",
      "popularity_score": 273.22441944444444,
      "ai_summary": [
        "OpenAI believes Elon Musk funded its biggest critics.",
        "OpenAI accuses Musk of exploiting the fight to silence critics.",
        "The accusations are related to the ongoing conflict.",
        "The critics also reportedly dislike Elon Musk.",
        "The situation is described as \"cutthroat\"."
      ]
    },
    {
      "id": "cluster_78",
      "coverage": 1,
      "updated_at": "Thu, 16 Oct 2025 15:40:14 +0000",
      "title": "SpaceX has plans to launch Falcon Heavy from California—if anyone wants it to",
      "neutral_headline": "SpaceX Plans Falcon Heavy Launch from California",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2025/10/spacex-has-plans-to-launch-falcon-heavy-from-california-if-anyone-wants-it-to/",
          "published_at": "Thu, 16 Oct 2025 15:40:14 +0000",
          "title": "SpaceX has plans to launch Falcon Heavy from California—if anyone wants it to",
          "standfirst": "There's no big rush to bring SpaceX's Falcon Heavy to Vandenberg Space Force Base.",
          "content": "The Department of the Air Force has approved SpaceX’s plans to launch up to 100 missions per year from Vandenberg Space Force Base in California. This would continue the tectonic turnaround at the spaceport on California’s Central Coast. Five years ago, Vandenberg hosted just a single orbital launch. This year’s number stands at 51 orbital flights, or 53 launches if you count a pair of Minuteman missile tests, the most in a single calendar year at Vandenberg since the early 1970s. Vandenberg is used for missions launching into polar orbits, paths oriented north-south that, over time, cover most of the Earth’s surface area. These orbits are popular for Earth observation satellites.Read full article Comments",
          "feed_position": 9,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2211835625-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2211835625-1152x648.jpg",
      "popularity_score": 255.35164166666667,
      "ai_summary": [
        "SpaceX plans to launch Falcon Heavy from California.",
        "The launch depends on demand.",
        "There is no immediate rush to launch from Vandenberg.",
        "The launch would occur from Vandenberg Space Force Base.",
        "The Falcon Heavy is a powerful rocket."
      ]
    },
    {
      "id": "cluster_104",
      "coverage": 1,
      "updated_at": "Thu, 16 Oct 2025 11:00:03 +0000",
      "title": "Inside the web infrastructure revolt over Google’s AI Overviews",
      "neutral_headline": "Web Infrastructure Revolt Over Google AI Overviews",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/ai/2025/10/inside-the-web-infrastructure-revolt-over-googles-ai-overviews/",
          "published_at": "Thu, 16 Oct 2025 11:00:03 +0000",
          "title": "Inside the web infrastructure revolt over Google’s AI Overviews",
          "standfirst": "Cloudflare CEO Matthew Prince is making sweeping changes to force Google's hand.",
          "content": "It could be a consequential act of quiet regulation. Cloudflare, a web infrastructure company, has updated millions of websites’ robots.txt files in an effort to force Google to change how it crawls them to fuel its AI products and initiatives. We spoke with Cloudflare CEO Matthew Prince about what exactly is going on here, why it matters, and what the web might soon look like. But to get into that, we need to cover a little background first. The new change, which Cloudflare calls its Content Signals Policy, happened after publishers and other companies that depend on web traffic have cried foul over Google’s AI Overviews and similar AI answer engines, saying they are sharply cutting those companies’ path to revenue because they don’t send traffic back to the source of the information.Read full article Comments",
          "feed_position": 11,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/Co-founder_and_CEO_of_Cloudflare_Matthew_Prince-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/Co-founder_and_CEO_of_Cloudflare_Matthew_Prince-1152x648.jpg",
      "popularity_score": 166,
      "ai_summary": [
        "Cloudflare CEO Matthew Prince is making changes.",
        "These changes are in response to Google's AI Overviews.",
        "The changes are intended to force Google's hand.",
        "The changes involve web infrastructure.",
        "The situation involves a revolt."
      ]
    },
    {
      "id": "cluster_113",
      "coverage": 1,
      "updated_at": "Wed, 15 Oct 2025 20:30:43 +0000",
      "title": "Rice weevil on a grain of rice wins 2025 Nikon Small World contest",
      "neutral_headline": "Rice Weevil Photomicrography Wins Nikon Small World Contest",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2025/10/meet-the-2025-nikon-photomicrography-winners/",
          "published_at": "Wed, 15 Oct 2025 20:30:43 +0000",
          "title": "Rice weevil on a grain of rice wins 2025 Nikon Small World contest",
          "standfirst": "Nikon Small World photomicrography contest is an annual reminder that science can be beautiful as well as informative.",
          "content": "A stunning image of a rice weevil on a single grain of rice has won the 2025 Nikon Small World photomicrography contest, yielding valuable insight into the structure and behavior of—and providing a fresh perspective on—this well-known agricultural pest. The image was taken by Zhang You of Yunnan, China. Another of You’s photographs placed 15th in this year’s contest. “It pays to dive deep into entomology: understanding insects’ behaviors and mastering lighting,” You said in a statement. “A standout work blends artistry with scientific rigor, capturing the very essence, energy, and spirit of these creatures.” There was an element of luck in creating his winning image, too. “I had observed rice weevils in grains before, but never one with its wings spread,” You said. “This one was naturally preserved on a windowsill, perhaps in a final attempt to escape. Its tiny size makes manually preparing spread-wing specimens extremely difficult, so encountering it was both serendipitous and inspiring.”Read full article Comments",
          "feed_position": 13,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/nikon1-1152x648-1760544093.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/nikon1-1152x648-1760544093.jpg",
      "popularity_score": 159,
      "ai_summary": [
        "The annual Nikon Small World contest showcases scientific beauty through photomicrography.",
        "A rice weevil on a grain of rice won the 2025 contest.",
        "The contest highlights the intersection of science and artistic expression.",
        "Winning images demonstrate the informative and visually stunning aspects of science.",
        "The contest provides a platform for scientists to share their microscopic discoveries."
      ]
    },
    {
      "id": "cluster_110",
      "coverage": 1,
      "updated_at": "Wed, 15 Oct 2025 21:31:00 +0000",
      "title": "Army general says he’s using AI to improve “decision-making”",
      "neutral_headline": "Army General Uses Artificial Intelligence for Decision Making",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/ai/2025/10/army-general-says-hes-using-ai-to-improve-decision-making/",
          "published_at": "Wed, 15 Oct 2025 21:31:00 +0000",
          "title": "Army general says he’s using AI to improve “decision-making”",
          "standfirst": "\"AI is one thing that, as a commander, it’s been very, very interesting for me.\"",
          "content": "Last month, OpenAI published a usage study showing that nearly 15 percent of work-related conversations on ChatGPT had to deal with “making decisions and solving problems.” Now comes word that at least one high-level member of the US military is using LLMs for the same purpose. At the Association of the US Army Conference in Washington, DC, this week, Maj. Gen. William “Hank” Taylor reportedly said that “Chat and I are really close lately,” using a distressingly familiar diminutive nickname to refer to an unspecified AI chatbot. “AI is one thing that, as a commander, it’s been very, very interesting for me.” Military-focused news site DefenseScoop reports that Taylor told a roundtable group of reporters that he and the Eighth Army he commands out of South Korea are “regularly using” AI to modernize their predictive analysis for logistical planning and operational purposes. That is helpful for paperwork tasks like “just being able to write our weekly reports and things,” Taylor said, but it also aids in informing their overall direction.Read full article Comments",
          "feed_position": 12,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2219849110-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2219849110-1152x648.jpg",
      "popularity_score": 148,
      "ai_summary": [
        "An Army general is using artificial intelligence to improve decision-making processes.",
        "The general finds artificial intelligence very interesting as a commander.",
        "The use of artificial intelligence is a developing area for military applications.",
        "The general's statement indicates a shift toward artificial intelligence integration.",
        "The focus is on how artificial intelligence can aid command decisions."
      ]
    },
    {
      "id": "cluster_119",
      "coverage": 1,
      "updated_at": "Wed, 15 Oct 2025 18:53:00 +0000",
      "title": "Anthropic’s Claude Haiku 4.5 matches May’s frontier model at fraction of cost",
      "neutral_headline": "Anthropic's Claude Haiku 4.5 Matches Frontier Model Performance",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/ai/2025/10/anthropics-claude-haiku-4-5-matches-mays-frontier-model-at-fraction-of-cost/",
          "published_at": "Wed, 15 Oct 2025 18:53:00 +0000",
          "title": "Anthropic’s Claude Haiku 4.5 matches May’s frontier model at fraction of cost",
          "standfirst": "Tiny, fast model hits coding scores similar to GPT-5 and Sonnet 4.",
          "content": "On Wednesday, Anthropic released Claude Haiku 4.5, a small AI language model that reportedly delivers performance similar to what its frontier model Claude Sonnet 4 achieved five months ago but at one-third the cost and more than twice the speed. The new model is available now to all Claude app, web, and API users. If the benchmarks for Haiku 4.5 reported by Anthropic hold up to independent testing, the fact that the company can match some capabilities of its cutting-edge coding model from only five months ago (and GPT-5 in coding) while providing a dramatic speed increase and cost cut is notable. As a recap, Anthropic ships the Claude family in three model sizes: Haiku (small), Sonnet (medium), and Opus (large). The larger models are based on larger neural networks and typically include deeper contextual knowledge but are slower and more expensive to run. Due to a technique called distillation, companies like Anthropic have been able to craft smaller AI models that match the capability of larger, older models at functional tasks like coding, although it typically comes at the cost of omitting stored knowledge.Read full article Comments",
          "feed_position": 17,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/haiku45_hero.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/haiku45_hero.jpg",
      "popularity_score": 148,
      "ai_summary": [
        "Anthropic's Claude Haiku 4.5 model matches May's frontier model in performance.",
        "The Claude Haiku 4.5 model achieves similar coding scores to GPT-5 and Sonnet 4.",
        "The model offers high performance at a fraction of the cost of other models.",
        "The model is designed to be tiny and fast for efficient operation.",
        "The model's coding scores indicate strong capabilities in software development."
      ]
    },
    {
      "id": "cluster_123",
      "coverage": 1,
      "updated_at": "Wed, 15 Oct 2025 17:00:51 +0000",
      "title": "US demand grows for Chinese cars despite privacy and security fears",
      "neutral_headline": "US Demand for Chinese Cars Grows Despite Concerns",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/cars/2025/10/us-demand-grows-for-chinese-cars-despite-privacy-and-security-fears/",
          "published_at": "Wed, 15 Oct 2025 17:00:51 +0000",
          "title": "US demand grows for Chinese cars despite privacy and security fears",
          "standfirst": "But they're still unlikely to go on sale in the US.",
          "content": "More than half of American car buyers would consider a Chinese car brand for their next purchase, an increase of almost 25 percent compared to last year. That’s according to a survey of prospective car buyers conducted annually by the research firm AutoPacific. And yes, those car buyers are conscious of the privacy and security fears. AutoPacific spoke to 18,000 people who said they were planning to buy or lease a new car within the next three years for its 2025 Future Attribution Demand Study, and the company has been releasing snippets of data as it analyzes them, ahead of the full report’s release later this year. There has already been at least one surprise. Last year, partially automated driving systems like General Motors’ Super Cruise or Ford’s BlueCruise, or those developed by Tesla, were not in high demand. This year, that tech went to the top of the most-wanted list, with 43 percent of consumers saying they want hands-free partial automation. The same percentage also indicated a demand for rear automatic emergency braking. Wireless device charging, No. 1 in the list in 2024, didn’t make the top 15 for 2025.Read full article Comments",
          "feed_position": 19,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2240964205-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2240964205-1152x648.jpg",
      "popularity_score": 143,
      "ai_summary": [
        "Demand for Chinese cars is increasing in the United States.",
        "Privacy and security concerns exist regarding Chinese-made vehicles.",
        "Chinese cars are unlikely to be sold in the United States currently.",
        "The demand increase is happening despite the existing concerns.",
        "The situation presents a complex interplay of consumer interest and security."
      ]
    },
    {
      "id": "cluster_90",
      "coverage": 1,
      "updated_at": "Thu, 16 Oct 2025 14:19:04 +0000",
      "title": "Antarctica is starting to look a lot like Greenland—and that isn’t good",
      "neutral_headline": "Antarctica Resembles Greenland Due to Global Warming",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2025/10/antarctica-is-starting-to-look-a-lot-like-greenland-and-that-isnt-good/",
          "published_at": "Thu, 16 Oct 2025 14:19:04 +0000",
          "title": "Antarctica is starting to look a lot like Greenland—and that isn’t good",
          "standfirst": "Global warming is awakening sleeping giants of ice at the South Pole.",
          "content": "As recently as the 1990s, when the Greenland Ice Sheet and the rest of the Arctic region were measurably thawing under the climatic blowtorch of human-caused global warming, most of Antarctica’s vast ice cap still seemed securely frozen. But not anymore. Physics is physics. As the planet heats up, more ice will melt at both poles, and recent research shows that Antarctica’s ice caps, glaciers, and floating ice shelves, as well as its sea ice, are just as vulnerable to warming as the Arctic. Both satellite data and field observations in Antarctica reveal alarming signs of a Greenland-like meltdown, with increased surface melting of the ice fields, faster-moving glaciers, and dwindling sea ice. Some scientists are sounding the alarm, warning that the rapid “Greenlandification” of Antarctica will have serious consequences, including an accelerated rise in sea levels and significant shifts in rainfall and drought patterns.Read full article Comments",
          "feed_position": 10,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/shoesmith-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/shoesmith-1152x648.jpg",
      "popularity_score": 135.99886388888888,
      "ai_summary": [
        "Antarctica is starting to resemble Greenland due to global warming.",
        "Global warming is awakening ice at the South Pole.",
        "The changes in Antarctica are a consequence of climate change.",
        "The situation indicates significant environmental shifts.",
        "The changes are a cause for concern regarding ice melt."
      ]
    },
    {
      "id": "cluster_114",
      "coverage": 1,
      "updated_at": "Wed, 15 Oct 2025 20:22:24 +0000",
      "title": "Once unthinkable, NASA and Lockheed now consider launching Orion on other rockets",
      "neutral_headline": "NASA and Lockheed Consider Launching Orion on Other Rockets",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2025/10/once-unthinkable-nasa-and-lockheed-now-consider-launching-orion-on-other-rockets/",
          "published_at": "Wed, 15 Oct 2025 20:22:24 +0000",
          "title": "Once unthinkable, NASA and Lockheed now consider launching Orion on other rockets",
          "standfirst": "\"We're trying to crawl, then walk, then run into our reuse strategy.\"",
          "content": "The Orion spacecraft and Space Launch System rocket have been attached at the hip for the better part of two decades. The big rocket lifts, the smaller spacecraft flies, and Congress keeps the money rolling in. But now there are signs that the twain may, in the not too distant future, split. This is because Lockheed Martin has begun to pivot toward a future in which the Orion spacecraft—thanks to increasing reusability, a focus on cost, and openness to flying on different rockets—fits into commercial space applications. In interviews, company officials said that if NASA wanted to buy Orion missions as a “service,” rather than owning and operating the spacecraft, they were ready to work with the space agency.Read full article Comments",
          "feed_position": 14,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/03/jsc2025e016293large-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/03/jsc2025e016293large-1152x648.jpg",
      "popularity_score": 133,
      "ai_summary": [
        "NASA and Lockheed are considering launching Orion on different rockets.",
        "The goal is to develop a reuse strategy for the Orion spacecraft.",
        "The approach involves a phased strategy of crawling, walking, and running.",
        "The strategy aims to improve efficiency and reduce costs.",
        "The change reflects a shift in launch and reuse planning."
      ]
    },
    {
      "id": "cluster_116",
      "coverage": 1,
      "updated_at": "Wed, 15 Oct 2025 20:05:39 +0000",
      "title": "Thousands of customers imperiled after nation-state ransacks F5’s network",
      "neutral_headline": "Nation-State Attack Impacts F5 Network Users",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/security/2025/10/breach-of-f5-requires-emergency-action-from-big-ip-users-feds-warn/",
          "published_at": "Wed, 15 Oct 2025 20:05:39 +0000",
          "title": "Thousands of customers imperiled after nation-state ransacks F5’s network",
          "standfirst": "Risks to BIG-IP users include supply-chain attacks, credential loss, and vulnerability exploits.",
          "content": "Thousands of networks—many of them operated by the US government and Fortune 500 companies—face an “imminent threat” of being breached by a nation-state hacking group following the breach of a major maker of software, the federal government warned Wednesday. F5, a Seattle-based maker of networking software, disclosed the breach on Wednesday. F5 said a “sophisticated” threat group working for an undisclosed nation-state government had surreptitiously and persistently dwelled in its network over a “long-term.” Security researchers who have responded to similar intrusions in the past took the language to mean the hackers were inside the F5 network for years. Unprecedented During that time, F5 said, the hackers took control of the network segment the company uses to create and distribute updates for BIG IP, a line of server appliances that F5 says is used by 48 of the world’s top 50 corporations. Wednesday’s disclosure went on to say the threat group downloaded proprietary BIG-IP source code information about vulnerabilities that had been privately discovered but not yet patched. The hackers also obtained configuration settings that some customers used inside their networks.Read full article Comments",
          "feed_position": 15,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-1202018501-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-1202018501-1152x648.jpg",
      "popularity_score": 133,
      "ai_summary": [
        "A nation-state ransacked F5's network, impacting thousands of customers.",
        "BIG-IP users face risks including supply-chain attacks and credential loss.",
        "Vulnerability exploits are a potential threat to BIG-IP users.",
        "The attack highlights the vulnerability of supply chains.",
        "The incident underscores the importance of cybersecurity measures."
      ]
    },
    {
      "id": "cluster_117",
      "coverage": 1,
      "updated_at": "Wed, 15 Oct 2025 19:23:54 +0000",
      "title": "CDC tormented: HR workers summoned from furlough to lay off themselves, others",
      "neutral_headline": "CDC Faces Layoffs and Workforce Reduction",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/health/2025/10/cdc-tormented-hr-workers-summoned-from-furlough-to-lay-off-themselves-others/",
          "published_at": "Wed, 15 Oct 2025 19:23:54 +0000",
          "title": "CDC tormented: HR workers summoned from furlough to lay off themselves, others",
          "standfirst": "Traumatized CDC has lost 33% of its workforce this year, union says.",
          "content": "The dust is still settling at the Centers for Disease Control and Prevention after a mass layoff on Friday, which former employees at the beleaguered agency are describing as a massacre. In separate press briefings on Tuesday, a network of terminated CDC staff that goes by the name the National Public Health Coalition, and the union representing employees at the agency discussed what the wide-scale cuts mean for the American people, as well as the trauma, despair, and damage they have wreaked on the workers of the once-premier public health agency. In a normal federal layoff—called a reduction in force, or RIF—the agency would be given a full outline of the roles and branches or divisions affected, as well as some explanation for the cuts, such as alleged fraud, abuse, or redundancy. However, the Trump administration has provided no such information or explanation, leaving current and former employees to essentially crowdsource what has been lost and only guess at the possible reasons.Read full article Comments",
          "feed_position": 16,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/08/GettyImages-2228551722-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/08/GettyImages-2228551722-1152x648.jpg",
      "popularity_score": 133,
      "ai_summary": [
        "The CDC is experiencing workforce reductions and layoffs.",
        "HR workers were summoned from furlough to lay off others.",
        "The CDC has lost 33% of its workforce this year.",
        "The union reports the CDC is traumatized by the situation.",
        "The situation indicates significant internal challenges."
      ]
    },
    {
      "id": "cluster_120",
      "coverage": 1,
      "updated_at": "Wed, 15 Oct 2025 18:26:20 +0000",
      "title": "ISPs angry about California law that lets renters opt out of forced payments",
      "neutral_headline": "ISPs Oppose California Law on Renters' Broadband Payments",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/10/california-says-landlords-cant-make-tenants-pay-for-an-isp-they-dont-want/",
          "published_at": "Wed, 15 Oct 2025 18:26:20 +0000",
          "title": "ISPs angry about California law that lets renters opt out of forced payments",
          "standfirst": "Gov. Newsom signs broadband billing law hated by the cable industry.",
          "content": "Rejecting opposition from the cable and real estate industries, California Gov. Gavin Newsom signed a bill that aims to increase broadband competition in apartment buildings. The new law taking effect on January 1 says landlords must let tenants “opt out of paying for any subscription from a third-party Internet service provider, such as through a bulk-billing arrangement, to provide service for wired Internet, cellular, or satellite service that is offered in connection with the tenancy.” It was approved by the state Assembly in a 75–0 vote in April, and by the Senate in a 30–7 vote last month. “This is kind of like a first step in trying to give this industry an opportunity to just treat people fairly,” Assemblymember Rhodesia Ransom, a Democratic lawmaker who authored the bill, told Ars last month. “It’s not super restrictive. We are not banning bulk billing. We’re not even limiting how much money the people can make. What we’re saying here with this bill is that if a tenant wants to opt out of the arrangement, they should be allowed to opt out.”Read full article Comments",
          "feed_position": 18,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2020/10/broadband-ethernet-cables-1152x648-1742589135.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2020/10/broadband-ethernet-cables-1152x648-1742589135.jpg",
      "popularity_score": 133,
      "ai_summary": [
        "ISPs are angry about a California law.",
        "The law allows renters to opt out of forced broadband payments.",
        "Governor Newsom signed the broadband billing law.",
        "The cable industry opposes the new law.",
        "The law impacts how renters pay for internet services."
      ]
    }
  ]
}