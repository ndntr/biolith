{
  "updated_at": "2025-10-13T11:16:44.315Z",
  "clusters": [
    {
      "id": "cluster_12",
      "coverage": 2,
      "updated_at": "Mon, 13 Oct 2025 09:00:36 +0000",
      "title": "The best smart LED light bulbs for 2025",
      "neutral_headline": "The best smart LED light bulbs for 2025",
      "items": [
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/home/smart-home/best-smart-led-light-bulbs-143022856.html",
          "published_at": "Mon, 13 Oct 2025 09:00:36 +0000",
          "title": "The best smart LED light bulbs for 2025",
          "standfirst": "Smart LED light bulbs are one of the easiest ways to get into the IoT space. These smart lighting solutions let you control your home’s illumination from your phone and other connected devices, and in addition to that practicality, they also inject some fun into your space. Color-changing bulbs have a plethora of RGB options for you to customize the lighting mood for your next movie night, date night or game day, or you can opt for cozy warm white light when you need to unwind at the end of a long day.It goes without saying that many of these smart LED light bulbs work with Amazon’s Alexa and the Google Assistant, so if you already have a smart home setup in the works, you can find one that fits into your chosen ecosystem. And arguably the best thing about these devices is that they can fit into any budget; affordable and advanced options have flooded the space over the past few years. We’ve tested out a bunch of smart lights over the years, and these are our current favorites. Table of contents Best smart lights for 2025 Other smart bulbs we tested What to look for in smart light bulbs Smart light bulb FAQs Best smart lights for 2025 Other smart bulbs we’ve tested Nanoleaf Smarter Kit While we’ve recommended Nanoleaf’s Smarter Kits in guides in the past, they’re a bit more niche than other smart lights on this list. They’re best for adding flare to your living room or game-streaming setup as they come in different shapes like hexagons and triangles and can sync with music. In addition to different colors, light animations and schedules, Nanoleaf’s Smart Kits also support Amazon Alexa and Google Assistant voice commands. What to look for in smart light bulbs Connectivity (To hub or not to hub) One of the biggest appeals of smart lighting solutions is being able to control them from your phone. Most of them are able to do so by connecting to it via Wi-Fi or Bluetooth, or via an external hub, which handles the connection for them. Bluetooth connectivity limits the range in which you’ll be able to control the light, so it’s only best for a limited number of bulbs and ones you don’t expect to control when you’re away. Wi-Fi color-changing bulbs are easy to set up and can be cheaper overall since they don’t require a hub to connect them. However, having something like a central Zigbee hub can make your whole system more reliable since its connection is separate from your home’s network. For that reason, hub-based bulbs tend to be more expandable, so we mainly recommend those if you want to eventually have dozens of smart lights around your home. White or color? Most color-changing bulbs you’ll find today are “white and color” bulbs, meaning they can glow in vibrant RGB color-options like blues, pinks, greens and everything in between, as well as shine with different temperatures of white. But there are some white-only bulbs out there, and they are often a bit more affordable than their color counterparts. While we recommend springing for the white-and-color devices, if you’d prefer white only, make sure you’re getting a bulb that can span the color temperature spectrum (typically from about 2000 to 5000 Kelvin), offering you various levels of cool and warm white light. App features One of the perks of smart lighting solutions is the amount of control you have over them thanks to their various app-control capabilities. Most companion apps let you do things like set lighting schedules and timers, group individual lights into room designations and create your own custom light “scenes” with different RGB options. But we have seen other features that aren’t as ubiquitous like vacation mode for automatically turning lights on and off to enhance your home security, and sync with media, which changes the colors of lights depending on the music you’re listening to or the game you’re currently live-streaming. Smart home compatibility If you use a smart assistant like Amazon’s Alexa or the Google Assistant regularly, make sure the smart lights or smart switches work with your favorite. All of the bulbs we tested supported both Amazon’s and Google’s virtual assistants, allowing you to use voice commands to turn lights on and off, dim them with a virtual dimmer and more. The wildcard here is Siri and Apple’s HomeKit; while numerous smart bulbs have added HomeKit support, not all lights are compatible with Apple’s smart home system. Expandability We alluded to this above, but you’ll want to consider how many smart lights you eventually want in your home. Some brands and lighting systems are easier to expand than others, and we generally recommend going for hub-based bulbs if you plan on putting smart lights in every room in your home. If you’re only looking to deck out your home office or living room with some fancy color-changing bulbs, Wi-Fi options should serve you well. Thankfully, these are some of the most affordable smart home devices you can get, so even if you don’t have a clear answer to this question now, you can reconsider your options down the line if you do decide to outfit your home with multiple smart bulbs. Smart light bulb FAQs What’s the best smart light bulb for Alexa? There is no best smart light bulb for Alexa. Amazon doesn’t make its own smart bulbs (like it does for smart plugs and thermostats), but rather there are dozens of smart lights made by third-parties that work with Alexa — including all of the ones we tested. Before picking the best smart light bulb for you, make sure to check the voice assistants that the contenders support. You’ll find that most smart light bulbs available today work with Amazon’s Alexa and the Google Assistant, and plenty of them also have support for Apple’s Siri and HomeKit. Can you put a smart bulb in any lamp? Smart light bulbs can go into most modern light fixtures — but just like regular bulbs, they need to be the right shape/size for the fixture. A standard A19 smart light bulb should work properly in most table, floor and other lamps. If you have a fixture that takes a specific type of bulb, look for smart bulbs that will fit properly. Do smart light bulbs use electricity when off? Smart light bulbs do use a negligible amount of electricity when their fixtures are turned off. This is due to the fact that the smart bulb needs to stay in constant contact with your home’s internet connection or Bluetooth in order to work properly. However, their energy-saving benefits usually outweigh the small amount of power they consume even while turned off.This article originally appeared on Engadget at https://www.engadget.com/home/smart-home/best-smart-led-light-bulbs-143022856.html?src=rss",
          "content": "Smart LED light bulbs are one of the easiest ways to get into the IoT space. These smart lighting solutions let you control your home’s illumination from your phone and other connected devices, and in addition to that practicality, they also inject some fun into your space. Color-changing bulbs have a plethora of RGB options for you to customize the lighting mood for your next movie night, date night or game day, or you can opt for cozy warm white light when you need to unwind at the end of a long day.It goes without saying that many of these smart LED light bulbs work with Amazon’s Alexa and the Google Assistant, so if you already have a smart home setup in the works, you can find one that fits into your chosen ecosystem. And arguably the best thing about these devices is that they can fit into any budget; affordable and advanced options have flooded the space over the past few years. We’ve tested out a bunch of smart lights over the years, and these are our current favorites. Table of contents Best smart lights for 2025 Other smart bulbs we tested What to look for in smart light bulbs Smart light bulb FAQs Best smart lights for 2025 Other smart bulbs we’ve tested Nanoleaf Smarter Kit While we’ve recommended Nanoleaf’s Smarter Kits in guides in the past, they’re a bit more niche than other smart lights on this list. They’re best for adding flare to your living room or game-streaming setup as they come in different shapes like hexagons and triangles and can sync with music. In addition to different colors, light animations and schedules, Nanoleaf’s Smart Kits also support Amazon Alexa and Google Assistant voice commands. What to look for in smart light bulbs Connectivity (To hub or not to hub) One of the biggest appeals of smart lighting solutions is being able to control them from your phone. Most of them are able to do so by connecting to it via Wi-Fi or Bluetooth, or via an external hub, which handles the connection for them. Bluetooth connectivity limits the range in which you’ll be able to control the light, so it’s only best for a limited number of bulbs and ones you don’t expect to control when you’re away. Wi-Fi color-changing bulbs are easy to set up and can be cheaper overall since they don’t require a hub to connect them. However, having something like a central Zigbee hub can make your whole system more reliable since its connection is separate from your home’s network. For that reason, hub-based bulbs tend to be more expandable, so we mainly recommend those if you want to eventually have dozens of smart lights around your home. White or color? Most color-changing bulbs you’ll find today are “white and color” bulbs, meaning they can glow in vibrant RGB color-options like blues, pinks, greens and everything in between, as well as shine with different temperatures of white. But there are some white-only bulbs out there, and they are often a bit more affordable than their color counterparts. While we recommend springing for the white-and-color devices, if you’d prefer white only, make sure you’re getting a bulb that can span the color temperature spectrum (typically from about 2000 to 5000 Kelvin), offering you various levels of cool and warm white light. App features One of the perks of smart lighting solutions is the amount of control you have over them thanks to their various app-control capabilities. Most companion apps let you do things like set lighting schedules and timers, group individual lights into room designations and create your own custom light “scenes” with different RGB options. But we have seen other features that aren’t as ubiquitous like vacation mode for automatically turning lights on and off to enhance your home security, and sync with media, which changes the colors of lights depending on the music you’re listening to or the game you’re currently live-streaming. Smart home compatibility If you use a smart assistant like Amazon’s Alexa or the Google Assistant regularly, make sure the smart lights or smart switches work with your favorite. All of the bulbs we tested supported both Amazon’s and Google’s virtual assistants, allowing you to use voice commands to turn lights on and off, dim them with a virtual dimmer and more. The wildcard here is Siri and Apple’s HomeKit; while numerous smart bulbs have added HomeKit support, not all lights are compatible with Apple’s smart home system. Expandability We alluded to this above, but you’ll want to consider how many smart lights you eventually want in your home. Some brands and lighting systems are easier to expand than others, and we generally recommend going for hub-based bulbs if you plan on putting smart lights in every room in your home. If you’re only looking to deck out your home office or living room with some fancy color-changing bulbs, Wi-Fi options should serve you well. Thankfully, these are some of the most affordable smart home devices you can get, so even if you don’t have a clear answer to this question now, you can reconsider your options down the line if you do decide to outfit your home with multiple smart bulbs. Smart light bulb FAQs What’s the best smart light bulb for Alexa? There is no best smart light bulb for Alexa. Amazon doesn’t make its own smart bulbs (like it does for smart plugs and thermostats), but rather there are dozens of smart lights made by third-parties that work with Alexa — including all of the ones we tested. Before picking the best smart light bulb for you, make sure to check the voice assistants that the contenders support. You’ll find that most smart light bulbs available today work with Amazon’s Alexa and the Google Assistant, and plenty of them also have support for Apple’s Siri and HomeKit. Can you put a smart bulb in any lamp? Smart light bulbs can go into most modern light fixtures — but just like regular bulbs, they need to be the right shape/size for the fixture. A standard A19 smart light bulb should work properly in most table, floor and other lamps. If you have a fixture that takes a specific type of bulb, look for smart bulbs that will fit properly. Do smart light bulbs use electricity when off? Smart light bulbs do use a negligible amount of electricity when their fixtures are turned off. This is due to the fact that the smart bulb needs to stay in constant contact with your home’s internet connection or Bluetooth in order to work properly. However, their energy-saving benefits usually outweigh the small amount of power they consume even while turned off.This article originally appeared on Engadget at https://www.engadget.com/home/smart-home/best-smart-led-light-bulbs-143022856.html?src=rss",
          "feed_position": 0
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/we-keep-talking-about-ai-agents-but-do-we-ever-know-what-they-are",
          "published_at": "Sun, 12 Oct 2025 19:00:00 GMT",
          "title": "We keep talking about AI agents, but do we ever know what they are?",
          "standfirst": "Imagine you do two things on a Monday morning.First, you ask a chatbot to summarize your new emails. Next, you ask an AI tool to figure out why your top competitor grew so fast last quarter. The AI silently gets to work. It scours financial reports, news articles and social media sentiment. It cross-references that data with your internal sales numbers, drafts a strategy outlining three potential reasons for the competitor&#x27;s success and schedules a 30-minute meeting with your team to present its findings.We&#x27;re calling both of these \"AI agents,\" but they represent worlds of difference in intelligence, capability and the level of trust we place in them. This ambiguity creates a fog that makes it difficult to build, evaluate, and safely govern these powerful new tools. If we can&#x27;t agree on what we&#x27;re building, how can we know when we&#x27;ve succeeded?This post won&#x27;t try to sell you on yet another definitive framework. Instead, think of it as a survey of the current landscape of agent autonomy, a map to help us all navigate the terrain together.What are we even talking about? Defining an \"AI agent\"Before we can measure an agent&#x27;s autonomy, we need to agree on what an \"agent\" actually is. The most widely accepted starting point comes from the foundational textbook on AI, Stuart Russell and Peter Norvig’s “Artificial Intelligence: A Modern Approach.” They define an agent as anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators. A thermostat is a simple agent: Its sensor perceives the room temperature, and its actuator acts by turning the heat on or off.ReAct Model for AI Agents (Credit: Confluent) That classic definition provides a solid mental model. For today&#x27;s technology, we can translate it into four key components that make up a modern AI agent:Perception (the \"senses\"): This is how an agent takes in information about its digital or physical environment. It&#x27;s the input stream that allows the agent to understand the current state of the world relevant to its task.Reasoning engine (the \"brain\"): This is the core logic that processes the perceptions and decides what to do next. For modern agents, this is typically powered by a large language model (LLM). The engine is responsible for planning, breaking down large goals into smaller steps, handling errors and choosing the right tools for the job.Action (the \"hands\"): This is how an agent affects its environment to move closer to its goal. The ability to take action via tools is what gives an agent its power.Goal/objective: This is the overarching task or purpose that guides all of the agent&#x27;s actions. It is the \"why\" that turns a collection of tools into a purposeful system. The goal can be simple (\"Find the best price for this book\") or complex (\"Launch the marketing campaign for our new product\")Putting it all together, a true agent is a full-body system. The reasoning engine is the brain, but it’s useless without the senses (perception) to understand the world and the hands (actions) to change it. This complete system, all guided by a central goal, is what creates genuine agency.With these components in mind, the distinction we made earlier becomes clear. A standard chatbot isn&#x27;t a true agent. It perceives your question and acts by providing an answer, but it lacks an overarching goal and the ability to use external tools to accomplish it.An agent, on the other hand, is software that has agency. It has the capacity to act independently and dynamically toward a goal. And it&#x27;s this capacity that makes a discussion about the levels of autonomy so important.Learning from the past: How we learned to classify autonomyThe dizzying pace of AI can make it feel like we&#x27;re navigating uncharted territory. But when it comes to classifying autonomy, we’re not starting from scratch. Other industries have been working on this problem for decades, and their playbooks offer powerful lessons for the world of AI agents.The core challenge is always the same: How do you create a clear, shared language for the gradual handover of responsibility from a human to a machine?SAE levels of driving automationPerhaps the most successful framework comes from the automotive industry. The SAE J3016 standard defines six levels of driving automation, from Level 0 (fully manual) to Level 5 (fully autonomous).The SAE J3016 Levels of Driving Automation (Credit: SAE International) What makes this model so effective isn&#x27;t its technical detail, but its focus on two simple concepts:Dynamic driving task (DDT): This is everything involved in the real-time act of driving: steering, braking, accelerating and monitoring the road.Operational design domain (ODD): These are the specific conditions under which the system is designed to work. For example, \"only on divided highways\" or \"only in clear weather during the daytime.\"The question for each level is simple: Who is doing the DDT, and what is the ODD? At Level 2, the human must supervise at all times. At Level 3, the car handles the DDT within its ODD, but the human must be ready to take over. At Level 4, the car can handle everything within its ODD, and if it encounters a problem, it can safely pull over on its own.The key insight for AI agents: A robust framework isn&#x27;t about the sophistication of the AI \"brain.\" It&#x27;s about clearly defining the division of responsibility between human and machine under specific, well-defined conditions.Aviation&#x27;s 10 Levels of AutomationWhile the SAE’s six levels are great for broad classification, aviation offers a more granular model for systems designed for close human-machine collaboration. The Parasuraman, Sheridan, and Wickens model proposes a detailed 10-level spectrum of automation.Levels of Automation of Decision and Action Selection for Aviation (Credit: The MITRE Corporation)This framework is less about full autonomy and more about the nuances of interaction. For example:At Level 3, the computer \"narrows the selection down to a few\" for the human to choose from.At Level 6, the computer \"allows the human a restricted time to veto before it executes\" an action.At Level 9, the computer \"informs the human only if it, the computer, decides to.\"The key insight for AI agents: This model is perfect for describing the collaborative \"centaur\" systems we&#x27;re seeing today. Most AI agents won&#x27;t be fully autonomous (Level 10) but will exist somewhere on this spectrum, acting as a co-pilot that suggests, executes with approval or acts with a veto window.Robotics and unmanned systemsFinally, the world of robotics brings in another critical dimension: context. The National Institute of Standards and Technology&#x27;s (NIST) Autonomy Levels for Unmanned Systems (ALFUS) framework was designed for systems like drones and industrial robots.The Three-Axis Model for ALFUS (Credit: NIST) Its main contribution is adding context to the definition of autonomy, assessing it along three axes:Human independence: How much human supervision is required?Mission complexity: How difficult or unstructured is the task?Environmental complexity: How predictable and stable is the environment in which the agent operates?The key insight for AI agents: This framework reminds us that autonomy isn&#x27;t a single number. An agent performing a simple task in a stable, predictable digital environment (like sorting files in a single folder) is fundamentally less autonomous than an agent performing a complex task across the chaotic, unpredictable environment of the open internet, even if the level of human supervision is the same.The emerging frameworks for AI agentsHaving looked at the lessons from automotive, aviation and robotics, we can now examine the emerging frameworks designed for AI agents. While the field is still new and no single standard has won out, most proposals fall into three distinct, but often overlapping, categories based on the primary question they seek to answer.Category 1: The \"What can it do?\" frameworks (capability-focused)These frameworks classify agents based on their underlying technical architecture and what they are capable of achieving. They provide a roadmap for developers, outlining a progression of increasingly sophisticated technical milestones that often correspond directly to code patterns.A prime example of this developer-centric approach comes from Hugging Face. Their framework uses a star rating to show the gradual shift in control from human to AI:Five Levels of AI Agent Autonomy, as proposed by HuggingFace (Credit: Hugging Face) Zero stars (simple processor): The AI has no impact on the program&#x27;s flow. It simply processes information and its output is displayed, like a print statement. The human is in complete control.One star (router): The AI makes a basic decision that directs program flow, like choosing between two predefined paths (if/else). The human still defines how everything is done.Two stars (tool call): The AI chooses which predefined tool to use and what arguments to use with it. The human has defined the available tools, but the AI decides how to execute them.Three stars (multi-step agent): The AI now controls the iteration loop. It decides which tool to use, when to use it and whether to continue working on the task.Four stars (fully autonomous): The AI can generate and execute entirely new code to accomplish a goal, going beyond the predefined tools it was given.Strengths: This model is excellent for engineers. It&#x27;s concrete, maps directly to code and clearly benchmarks the transfer of executive control to the AI. Weaknesses: It is highly technical and less intuitive for non-developers trying to understand an agent&#x27;s real-world impact.Category 2: The \"How do we work together?\" frameworks (interaction-focused)This second category defines autonomy not by the agent’s internal skills, but by the nature of its relationship with the human user. The central question is: Who is in control, and how do we collaborate?This approach often mirrors the nuance we saw in the aviation models. For instance, a framework detailed in the paper Levels of Autonomy for AI Agents defines levels based on the user&#x27;s role:L1 - user as an operator: The human is in direct control (like a person using Photoshop with AI-assist features).L4 - user as an approver: The agent proposes a full plan or action, and the human must give a simple \"yes\" or \"no\" before it proceeds.L5 - user as an observer: The agent has full autonomy to pursue a goal and simply reports its progress and results back to the human.Levels of Autonomy for AI AgentsStrengths: These frameworks are highly intuitive and user-centric. They directly address the critical issues of control, trust, and oversight.Weaknesses: An agent with simple capabilities and one with highly advanced reasoning could both fall into the \"Approver\" level, so this approach can sometimes obscure the underlying technical sophistication.Category 3: The \"Who is responsible?\" frameworks (governance-focused)The final category is less concerned with how an agent works and more with what happens when it fails. These frameworks are designed to help answer crucial questions about law, safety and ethics.Think tanks like Germany&#x27;s Stiftung Neue VTrantwortung have analyzed AI agents through the lens of legal liability. Their work aims to classify agents in a way that helps regulators determine who is responsible for an agent&#x27;s actions: The user who deployed it, the developer who built it or the company that owns the platform it runs on?This perspective is essential for navigating complex regulations like the EU&#x27;s Artificial Intelligence Act, which will treat AI systems differently based on the level of risk they pose.Strengths: This approach is absolutely essential for real-world deployment. It forces the difficult but necessary conversations about accountability that build public trust.Weaknesses: It&#x27;s more of a legal or policy guide than a technical roadmap for developers.A comprehensive understanding requires looking at all three questions at once: An agent&#x27;s capabilities, how we interact with it and who is responsible for the outcome..Identifying the gaps and challengesLooking at the landscape of autonomy frameworks shows us that no single model is sufficient because the true challenges lie in the gaps between them, in areas that are incredibly difficult to define and measure.What is the \"Road\" for a digital agent?The SAE framework for self-driving cars gave us the powerful concept of an ODD, the specific conditions under which a system can operate safely. For a car, that might be \"divided highways, in clear weather, during the day.\" This is a great solution for a physical environment, but what’s the ODD for a digital agent?The \"road\" for an agent is the entire internet. An infinite, chaotic and constantly changing environment. Websites get redesigned overnight, APIs are deprecated and social norms in online communities shift. How do we define a \"safe\" operational boundary for an agent that can browse websites, access databases and interact with third-party services? Answering this is one of the biggest unsolved problems. Without a clear digital ODD, we can&#x27;t make the same safety guarantees that are becoming standard in the automotive world.This is why, for now, the most effective and reliable agents operate within well-defined, closed-world scenarios. As I argued in a recent VentureBeat article, forgetting the open-world fantasies and focusing on \"bounded problems\" is the key to real-world success. This means defining a clear, limited set of tools, data sources and potential actions. Beyond simple tool useToday&#x27;s agents are getting very good at executing straightforward plans. If you tell one to \"find the price of this item using Tool A, then book a meeting with Tool B,\" it can often succeed. But true autonomy requires much more. Many systems today hit a technical wall when faced with tasks that require:Long-term reasoning and planning: Agents struggle to create and adapt complex, multi-step plans in the face of uncertainty. They can follow a recipe, but they can&#x27;t yet invent one from scratch when things go wrong.Robust self-correction: What happens when an API call fails or a website returns an unexpected error? A truly autonomous agent needs the resilience to diagnose the problem, form a new hypothesis and try a different approach, all without a human stepping in.Composability: The future likely involves not one agent, but a team of specialized agents working together. Getting them to collaborate reliably, to pass information back and forth, delegate tasks and resolve conflicts is a monumental software engineering challenge that we are just beginning to tackle.The elephant in the room: Alignment and controlThis is the most critical challenge of all, because it&#x27;s not just technical, it&#x27;s deeply human. Alignment is the problem of ensuring an agent&#x27;s goals and actions are consistent with our intentions and values, even when those values are complex, unstated or nuanced.Imagine you give an agent the seemingly harmless goal of \"maximizing customer engagement for our new product.\" The agent might correctly determine that the most effective strategy is to send a dozen notifications a day to every user. The agent has achieved its literal goal perfectly, but it has violated the unstated, common-sense goal of \"don&#x27;t be incredibly annoying.\"This is a failure of alignment.The core difficulty, which organizations like the AI Alignment Forum are dedicated to studying, is that it is incredibly hard to specify fuzzy, complex human preferences in the precise, literal language of code. As agents become more powerful, ensuring they are not just capable but also safe, predictable and aligned with our true intent becomes the most important challenge we face.The future is agentic (and collaborative)The path forward for AI agents is not a single leap to a god-like super-intelligence, but a more practical and collaborative journey. The immense challenges of open-world reasoning and perfect alignment mean that the future is a team effort.We will see less of the single, all-powerful agent and more of an \"agentic mesh\" — a network of specialized agents, each operating within a bounded domain, working together to tackle complex problems. More importantly, they will work with us. The most valuable and safest applications will keep a human on the loop, casting them as a co-pilot or strategist to augment our intellect with the speed of machine execution. This \"centaur\" model will be the most effective and responsible path forward.The frameworks we&#x27;ve explored aren’t just theoretical. They’re practical tools for building trust, assigning responsibility and setting clear expectations. They help developers define limits and leaders shape vision, laying the groundwork for AI to become a dependable partner in our work and lives.Sean Falconer is Confluent&#x27;s AI entrepreneur in residence.",
          "content": "Imagine you do two things on a Monday morning.First, you ask a chatbot to summarize your new emails. Next, you ask an AI tool to figure out why your top competitor grew so fast last quarter. The AI silently gets to work. It scours financial reports, news articles and social media sentiment. It cross-references that data with your internal sales numbers, drafts a strategy outlining three potential reasons for the competitor&#x27;s success and schedules a 30-minute meeting with your team to present its findings.We&#x27;re calling both of these \"AI agents,\" but they represent worlds of difference in intelligence, capability and the level of trust we place in them. This ambiguity creates a fog that makes it difficult to build, evaluate, and safely govern these powerful new tools. If we can&#x27;t agree on what we&#x27;re building, how can we know when we&#x27;ve succeeded?This post won&#x27;t try to sell you on yet another definitive framework. Instead, think of it as a survey of the current landscape of agent autonomy, a map to help us all navigate the terrain together.What are we even talking about? Defining an \"AI agent\"Before we can measure an agent&#x27;s autonomy, we need to agree on what an \"agent\" actually is. The most widely accepted starting point comes from the foundational textbook on AI, Stuart Russell and Peter Norvig’s “Artificial Intelligence: A Modern Approach.” They define an agent as anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators. A thermostat is a simple agent: Its sensor perceives the room temperature, and its actuator acts by turning the heat on or off.ReAct Model for AI Agents (Credit: Confluent) That classic definition provides a solid mental model. For today&#x27;s technology, we can translate it into four key components that make up a modern AI agent:Perception (the \"senses\"): This is how an agent takes in information about its digital or physical environment. It&#x27;s the input stream that allows the agent to understand the current state of the world relevant to its task.Reasoning engine (the \"brain\"): This is the core logic that processes the perceptions and decides what to do next. For modern agents, this is typically powered by a large language model (LLM). The engine is responsible for planning, breaking down large goals into smaller steps, handling errors and choosing the right tools for the job.Action (the \"hands\"): This is how an agent affects its environment to move closer to its goal. The ability to take action via tools is what gives an agent its power.Goal/objective: This is the overarching task or purpose that guides all of the agent&#x27;s actions. It is the \"why\" that turns a collection of tools into a purposeful system. The goal can be simple (\"Find the best price for this book\") or complex (\"Launch the marketing campaign for our new product\")Putting it all together, a true agent is a full-body system. The reasoning engine is the brain, but it’s useless without the senses (perception) to understand the world and the hands (actions) to change it. This complete system, all guided by a central goal, is what creates genuine agency.With these components in mind, the distinction we made earlier becomes clear. A standard chatbot isn&#x27;t a true agent. It perceives your question and acts by providing an answer, but it lacks an overarching goal and the ability to use external tools to accomplish it.An agent, on the other hand, is software that has agency. It has the capacity to act independently and dynamically toward a goal. And it&#x27;s this capacity that makes a discussion about the levels of autonomy so important.Learning from the past: How we learned to classify autonomyThe dizzying pace of AI can make it feel like we&#x27;re navigating uncharted territory. But when it comes to classifying autonomy, we’re not starting from scratch. Other industries have been working on this problem for decades, and their playbooks offer powerful lessons for the world of AI agents.The core challenge is always the same: How do you create a clear, shared language for the gradual handover of responsibility from a human to a machine?SAE levels of driving automationPerhaps the most successful framework comes from the automotive industry. The SAE J3016 standard defines six levels of driving automation, from Level 0 (fully manual) to Level 5 (fully autonomous).The SAE J3016 Levels of Driving Automation (Credit: SAE International) What makes this model so effective isn&#x27;t its technical detail, but its focus on two simple concepts:Dynamic driving task (DDT): This is everything involved in the real-time act of driving: steering, braking, accelerating and monitoring the road.Operational design domain (ODD): These are the specific conditions under which the system is designed to work. For example, \"only on divided highways\" or \"only in clear weather during the daytime.\"The question for each level is simple: Who is doing the DDT, and what is the ODD? At Level 2, the human must supervise at all times. At Level 3, the car handles the DDT within its ODD, but the human must be ready to take over. At Level 4, the car can handle everything within its ODD, and if it encounters a problem, it can safely pull over on its own.The key insight for AI agents: A robust framework isn&#x27;t about the sophistication of the AI \"brain.\" It&#x27;s about clearly defining the division of responsibility between human and machine under specific, well-defined conditions.Aviation&#x27;s 10 Levels of AutomationWhile the SAE’s six levels are great for broad classification, aviation offers a more granular model for systems designed for close human-machine collaboration. The Parasuraman, Sheridan, and Wickens model proposes a detailed 10-level spectrum of automation.Levels of Automation of Decision and Action Selection for Aviation (Credit: The MITRE Corporation)This framework is less about full autonomy and more about the nuances of interaction. For example:At Level 3, the computer \"narrows the selection down to a few\" for the human to choose from.At Level 6, the computer \"allows the human a restricted time to veto before it executes\" an action.At Level 9, the computer \"informs the human only if it, the computer, decides to.\"The key insight for AI agents: This model is perfect for describing the collaborative \"centaur\" systems we&#x27;re seeing today. Most AI agents won&#x27;t be fully autonomous (Level 10) but will exist somewhere on this spectrum, acting as a co-pilot that suggests, executes with approval or acts with a veto window.Robotics and unmanned systemsFinally, the world of robotics brings in another critical dimension: context. The National Institute of Standards and Technology&#x27;s (NIST) Autonomy Levels for Unmanned Systems (ALFUS) framework was designed for systems like drones and industrial robots.The Three-Axis Model for ALFUS (Credit: NIST) Its main contribution is adding context to the definition of autonomy, assessing it along three axes:Human independence: How much human supervision is required?Mission complexity: How difficult or unstructured is the task?Environmental complexity: How predictable and stable is the environment in which the agent operates?The key insight for AI agents: This framework reminds us that autonomy isn&#x27;t a single number. An agent performing a simple task in a stable, predictable digital environment (like sorting files in a single folder) is fundamentally less autonomous than an agent performing a complex task across the chaotic, unpredictable environment of the open internet, even if the level of human supervision is the same.The emerging frameworks for AI agentsHaving looked at the lessons from automotive, aviation and robotics, we can now examine the emerging frameworks designed for AI agents. While the field is still new and no single standard has won out, most proposals fall into three distinct, but often overlapping, categories based on the primary question they seek to answer.Category 1: The \"What can it do?\" frameworks (capability-focused)These frameworks classify agents based on their underlying technical architecture and what they are capable of achieving. They provide a roadmap for developers, outlining a progression of increasingly sophisticated technical milestones that often correspond directly to code patterns.A prime example of this developer-centric approach comes from Hugging Face. Their framework uses a star rating to show the gradual shift in control from human to AI:Five Levels of AI Agent Autonomy, as proposed by HuggingFace (Credit: Hugging Face) Zero stars (simple processor): The AI has no impact on the program&#x27;s flow. It simply processes information and its output is displayed, like a print statement. The human is in complete control.One star (router): The AI makes a basic decision that directs program flow, like choosing between two predefined paths (if/else). The human still defines how everything is done.Two stars (tool call): The AI chooses which predefined tool to use and what arguments to use with it. The human has defined the available tools, but the AI decides how to execute them.Three stars (multi-step agent): The AI now controls the iteration loop. It decides which tool to use, when to use it and whether to continue working on the task.Four stars (fully autonomous): The AI can generate and execute entirely new code to accomplish a goal, going beyond the predefined tools it was given.Strengths: This model is excellent for engineers. It&#x27;s concrete, maps directly to code and clearly benchmarks the transfer of executive control to the AI. Weaknesses: It is highly technical and less intuitive for non-developers trying to understand an agent&#x27;s real-world impact.Category 2: The \"How do we work together?\" frameworks (interaction-focused)This second category defines autonomy not by the agent’s internal skills, but by the nature of its relationship with the human user. The central question is: Who is in control, and how do we collaborate?This approach often mirrors the nuance we saw in the aviation models. For instance, a framework detailed in the paper Levels of Autonomy for AI Agents defines levels based on the user&#x27;s role:L1 - user as an operator: The human is in direct control (like a person using Photoshop with AI-assist features).L4 - user as an approver: The agent proposes a full plan or action, and the human must give a simple \"yes\" or \"no\" before it proceeds.L5 - user as an observer: The agent has full autonomy to pursue a goal and simply reports its progress and results back to the human.Levels of Autonomy for AI AgentsStrengths: These frameworks are highly intuitive and user-centric. They directly address the critical issues of control, trust, and oversight.Weaknesses: An agent with simple capabilities and one with highly advanced reasoning could both fall into the \"Approver\" level, so this approach can sometimes obscure the underlying technical sophistication.Category 3: The \"Who is responsible?\" frameworks (governance-focused)The final category is less concerned with how an agent works and more with what happens when it fails. These frameworks are designed to help answer crucial questions about law, safety and ethics.Think tanks like Germany&#x27;s Stiftung Neue VTrantwortung have analyzed AI agents through the lens of legal liability. Their work aims to classify agents in a way that helps regulators determine who is responsible for an agent&#x27;s actions: The user who deployed it, the developer who built it or the company that owns the platform it runs on?This perspective is essential for navigating complex regulations like the EU&#x27;s Artificial Intelligence Act, which will treat AI systems differently based on the level of risk they pose.Strengths: This approach is absolutely essential for real-world deployment. It forces the difficult but necessary conversations about accountability that build public trust.Weaknesses: It&#x27;s more of a legal or policy guide than a technical roadmap for developers.A comprehensive understanding requires looking at all three questions at once: An agent&#x27;s capabilities, how we interact with it and who is responsible for the outcome..Identifying the gaps and challengesLooking at the landscape of autonomy frameworks shows us that no single model is sufficient because the true challenges lie in the gaps between them, in areas that are incredibly difficult to define and measure.What is the \"Road\" for a digital agent?The SAE framework for self-driving cars gave us the powerful concept of an ODD, the specific conditions under which a system can operate safely. For a car, that might be \"divided highways, in clear weather, during the day.\" This is a great solution for a physical environment, but what’s the ODD for a digital agent?The \"road\" for an agent is the entire internet. An infinite, chaotic and constantly changing environment. Websites get redesigned overnight, APIs are deprecated and social norms in online communities shift. How do we define a \"safe\" operational boundary for an agent that can browse websites, access databases and interact with third-party services? Answering this is one of the biggest unsolved problems. Without a clear digital ODD, we can&#x27;t make the same safety guarantees that are becoming standard in the automotive world.This is why, for now, the most effective and reliable agents operate within well-defined, closed-world scenarios. As I argued in a recent VentureBeat article, forgetting the open-world fantasies and focusing on \"bounded problems\" is the key to real-world success. This means defining a clear, limited set of tools, data sources and potential actions. Beyond simple tool useToday&#x27;s agents are getting very good at executing straightforward plans. If you tell one to \"find the price of this item using Tool A, then book a meeting with Tool B,\" it can often succeed. But true autonomy requires much more. Many systems today hit a technical wall when faced with tasks that require:Long-term reasoning and planning: Agents struggle to create and adapt complex, multi-step plans in the face of uncertainty. They can follow a recipe, but they can&#x27;t yet invent one from scratch when things go wrong.Robust self-correction: What happens when an API call fails or a website returns an unexpected error? A truly autonomous agent needs the resilience to diagnose the problem, form a new hypothesis and try a different approach, all without a human stepping in.Composability: The future likely involves not one agent, but a team of specialized agents working together. Getting them to collaborate reliably, to pass information back and forth, delegate tasks and resolve conflicts is a monumental software engineering challenge that we are just beginning to tackle.The elephant in the room: Alignment and controlThis is the most critical challenge of all, because it&#x27;s not just technical, it&#x27;s deeply human. Alignment is the problem of ensuring an agent&#x27;s goals and actions are consistent with our intentions and values, even when those values are complex, unstated or nuanced.Imagine you give an agent the seemingly harmless goal of \"maximizing customer engagement for our new product.\" The agent might correctly determine that the most effective strategy is to send a dozen notifications a day to every user. The agent has achieved its literal goal perfectly, but it has violated the unstated, common-sense goal of \"don&#x27;t be incredibly annoying.\"This is a failure of alignment.The core difficulty, which organizations like the AI Alignment Forum are dedicated to studying, is that it is incredibly hard to specify fuzzy, complex human preferences in the precise, literal language of code. As agents become more powerful, ensuring they are not just capable but also safe, predictable and aligned with our true intent becomes the most important challenge we face.The future is agentic (and collaborative)The path forward for AI agents is not a single leap to a god-like super-intelligence, but a more practical and collaborative journey. The immense challenges of open-world reasoning and perfect alignment mean that the future is a team effort.We will see less of the single, all-powerful agent and more of an \"agentic mesh\" — a network of specialized agents, each operating within a bounded domain, working together to tackle complex problems. More importantly, they will work with us. The most valuable and safest applications will keep a human on the loop, casting them as a co-pilot or strategist to augment our intellect with the speed of machine execution. This \"centaur\" model will be the most effective and responsible path forward.The frameworks we&#x27;ve explored aren’t just theoretical. They’re practical tools for building trust, assigning responsibility and setting clear expectations. They help developers define limits and leaders shape vision, laying the groundwork for AI to become a dependable partner in our work and lives.Sean Falconer is Confluent&#x27;s AI entrepreneur in residence.",
          "feed_position": 0,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/5b0hTPUKh7PB9D7CfWwO22/3b31fdc3f95c670ce5096758d8760ccd/Agent_autonomy.png"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/heres-whats-slowing-down-your-ai-strategy-and-how-to-fix-it",
          "published_at": "Sun, 12 Oct 2025 07:00:00 GMT",
          "title": "Here's what's slowing down your AI strategy — and how to fix it",
          "standfirst": "Your best data science team just spent six months building a model that predicts customer churn with 90% accuracy. It’s sitting on a server, unused. Why? Because it’s been stuck in a risk review queue for a very long period of time, waiting for a committee that doesn’t understand stochastic models to sign off. This isn’t a hypothetical — it’s the daily reality in most large companies. In AI, the models move at internet speed. Enterprises don’t. Every few weeks, a new model family drops, open-source toolchains mutate and entire MLOps practices get rewritten. But in most companies, anything touching production AI has to pass through risk reviews, audit trails, change-management boards and model-risk sign-off. The result is a widening velocity gap: The research community accelerates; the enterprise stalls. This gap isn’t a headline problem like “AI will take your job.” It’s quieter and more expensive: missed productivity, shadow AI sprawl, duplicated spend and compliance drag that turns promising pilots into perpetual proofs-of-concept.The numbers say the quiet part out loudTwo trends collide. First, the pace of innovation: Industry is now the dominant force, producing the vast majority of notable AI models, according to Stanford&#x27;s 2024 AI Index Report. The core inputs for this innovation are compounding at a historic rate, with training compute needs doubling rapidly every few years. That pace all but guarantees rapid model churn and tool fragmentation. Second, enterprise adoption is accelerating. According to IBM&#x27;s, 42% of enterprise-scale companies have actively deployed AI, with many more actively exploring it. Yet the same surveys show governance roles are only now being formalized, leaving many companies to retrofit control after deployment. Layer on new regulation. The EU AI Act’s staged obligations are locked in — unacceptable-risk bans are already active and General Purpose AI (GPAI) transparency duties hit in mid-2025, with high-risk rules following. Brussels has made clear there’s no pause coming. If your governance isn’t ready, your roadmap will be.The real blocker isn&#x27;t modeling, it&#x27;s auditIn most enterprises, the slowest step isn’t fine-tuning a model; it’s proving your model follows certain guidelines. Three frictions dominate:Audit debt: Policies were written for static software, not stochastic models. You can ship a microservice with unit tests; you can’t “unit test” fairness drift without data access, lineage and ongoing monitoring. When controls don’t map, reviews balloon.. MRM overload: Model risk management (MRM), a discipline perfected in banking, is spreading beyond finance — often translated literally, not functionally. Explainability and data-governance checks make sense; forcing every retrieval-augmented chatbot through credit-risk style documentation does not.Shadow AI sprawl: Teams adopt vertical AI inside SaaS tools without central oversight. It feels fast — until the third audit asks who owns the prompts, where embeddings live and how to revoke data. Sprawl is speed’s illusion; integration and governance are the long-term velocity.Frameworks exist, but they&#x27;re not operational by defaultThe NIST AI Risk Management Framework is a solid north star: govern, map, measure, manage. It’s voluntary, adaptable and aligned with international standards. But it’s a blueprint, not a building. Companies still need concrete control catalogs, evidence templates and tooling that turn principles into repeatable reviews. Similarly, the EU AI Act sets deadlines and duties. It doesn’t install your model registry, wire your dataset lineage or resolve the age-old question of who signs off when accuracy and bias trade off. That’s on you soon.What winning enterprises are doing differentlyThe leaders I see closing the velocity gap aren’t chasing every model; they’re making the path to production routine. Five moves show up again and again:Ship a control plane, not a memo: Codify governance as code. Create a small library or service that enforces non-negotiables: Dataset lineage required, evaluation suite attached, risk tier chosen, PII scan passed, human-in-the-loop defined (if required). If a project can’t satisfy the checks, it can’t deploy.Pre-approve patterns: Approve reference architectures — “GPAI with retrieval augmented generation (RAG) on approved vector store,” “high-risk tabular model with feature store X and bias audit Y,” “vendor LLM via API with no data retention.” Pre-approval shifts review from bespoke debates to pattern conformance. (Your auditors will thank you.)Stage your governance by risk, not by team: Tie review depth to use-case criticality (safety, finance, regulated outcomes). A marketing copy assistant shouldn’t endure the same gauntlet as a loan adjudicator. Risk-proportionate review is both defensible and fast.Create an “evidence once, reuse everywhere” backbone: Centralize model cards, eval results, data sheets, prompt templates and vendor attestations. Every subsequent audit should start at 60% done because you’ve already proven the common pieces.Make audit a product: Give legal, risk and compliance a real roadmap. Instrument dashboards that show: Models in production by risk tier, upcoming re-evals, incidents and data-retention attestations. If audit can self-serve, engineering can ship.A pragmatic cadence for the next 12 monthsIf you’re serious about catching up, pick a 12-month governance sprint:Quarter 1: Stand up a minimal AI registry (models, datasets, prompts, evaluations). Draft risk-tiering and control mapping aligned to NIST AI RMF functions; publish two pre-approved patterns.Quarter 2: Turn controls into pipelines (CI checks for evals, data scans, model cards). Convert two fast-moving teams from shadow AI to platform AI by making the paved road easier than the side road.Quarter 3: Pilot a GxP-style review (a rigorous documentation standard from life sciences) for one high-risk use case; automate evidence capture. Start your EU AI Act gap analysis if you touch Europe; assign owners and deadlines.Quarter 4: Expand your pattern catalog (RAG, batch inference, streaming prediction). Roll out dashboards for risk/compliance. Bake governance SLAs into your OKRs. By this point, you haven’t slowed down innovation — you’ve standardized it. The research community can keep moving at light speed; you can keep shipping at enterprise speed — without the audit queue becoming your critical path.The competitive edge isn&#x27;t the next model — it&#x27;s the next mileIt’s tempting to chase each week’s leaderboard. But the durable advantage is the mile between a paper and production: The platform, the patterns, the proofs. That’s what your competitors can’t copy from GitHub, and it’s the only way to keep velocity without trading compliance for chaos. In other words: Make governance the grease, not the grit. Jayachander Reddy Kandakatla is senior machine learning operations (MLOps) engineer at Ford Motor Credit Company.",
          "content": "Your best data science team just spent six months building a model that predicts customer churn with 90% accuracy. It’s sitting on a server, unused. Why? Because it’s been stuck in a risk review queue for a very long period of time, waiting for a committee that doesn’t understand stochastic models to sign off. This isn’t a hypothetical — it’s the daily reality in most large companies. In AI, the models move at internet speed. Enterprises don’t. Every few weeks, a new model family drops, open-source toolchains mutate and entire MLOps practices get rewritten. But in most companies, anything touching production AI has to pass through risk reviews, audit trails, change-management boards and model-risk sign-off. The result is a widening velocity gap: The research community accelerates; the enterprise stalls. This gap isn’t a headline problem like “AI will take your job.” It’s quieter and more expensive: missed productivity, shadow AI sprawl, duplicated spend and compliance drag that turns promising pilots into perpetual proofs-of-concept.The numbers say the quiet part out loudTwo trends collide. First, the pace of innovation: Industry is now the dominant force, producing the vast majority of notable AI models, according to Stanford&#x27;s 2024 AI Index Report. The core inputs for this innovation are compounding at a historic rate, with training compute needs doubling rapidly every few years. That pace all but guarantees rapid model churn and tool fragmentation. Second, enterprise adoption is accelerating. According to IBM&#x27;s, 42% of enterprise-scale companies have actively deployed AI, with many more actively exploring it. Yet the same surveys show governance roles are only now being formalized, leaving many companies to retrofit control after deployment. Layer on new regulation. The EU AI Act’s staged obligations are locked in — unacceptable-risk bans are already active and General Purpose AI (GPAI) transparency duties hit in mid-2025, with high-risk rules following. Brussels has made clear there’s no pause coming. If your governance isn’t ready, your roadmap will be.The real blocker isn&#x27;t modeling, it&#x27;s auditIn most enterprises, the slowest step isn’t fine-tuning a model; it’s proving your model follows certain guidelines. Three frictions dominate:Audit debt: Policies were written for static software, not stochastic models. You can ship a microservice with unit tests; you can’t “unit test” fairness drift without data access, lineage and ongoing monitoring. When controls don’t map, reviews balloon.. MRM overload: Model risk management (MRM), a discipline perfected in banking, is spreading beyond finance — often translated literally, not functionally. Explainability and data-governance checks make sense; forcing every retrieval-augmented chatbot through credit-risk style documentation does not.Shadow AI sprawl: Teams adopt vertical AI inside SaaS tools without central oversight. It feels fast — until the third audit asks who owns the prompts, where embeddings live and how to revoke data. Sprawl is speed’s illusion; integration and governance are the long-term velocity.Frameworks exist, but they&#x27;re not operational by defaultThe NIST AI Risk Management Framework is a solid north star: govern, map, measure, manage. It’s voluntary, adaptable and aligned with international standards. But it’s a blueprint, not a building. Companies still need concrete control catalogs, evidence templates and tooling that turn principles into repeatable reviews. Similarly, the EU AI Act sets deadlines and duties. It doesn’t install your model registry, wire your dataset lineage or resolve the age-old question of who signs off when accuracy and bias trade off. That’s on you soon.What winning enterprises are doing differentlyThe leaders I see closing the velocity gap aren’t chasing every model; they’re making the path to production routine. Five moves show up again and again:Ship a control plane, not a memo: Codify governance as code. Create a small library or service that enforces non-negotiables: Dataset lineage required, evaluation suite attached, risk tier chosen, PII scan passed, human-in-the-loop defined (if required). If a project can’t satisfy the checks, it can’t deploy.Pre-approve patterns: Approve reference architectures — “GPAI with retrieval augmented generation (RAG) on approved vector store,” “high-risk tabular model with feature store X and bias audit Y,” “vendor LLM via API with no data retention.” Pre-approval shifts review from bespoke debates to pattern conformance. (Your auditors will thank you.)Stage your governance by risk, not by team: Tie review depth to use-case criticality (safety, finance, regulated outcomes). A marketing copy assistant shouldn’t endure the same gauntlet as a loan adjudicator. Risk-proportionate review is both defensible and fast.Create an “evidence once, reuse everywhere” backbone: Centralize model cards, eval results, data sheets, prompt templates and vendor attestations. Every subsequent audit should start at 60% done because you’ve already proven the common pieces.Make audit a product: Give legal, risk and compliance a real roadmap. Instrument dashboards that show: Models in production by risk tier, upcoming re-evals, incidents and data-retention attestations. If audit can self-serve, engineering can ship.A pragmatic cadence for the next 12 monthsIf you’re serious about catching up, pick a 12-month governance sprint:Quarter 1: Stand up a minimal AI registry (models, datasets, prompts, evaluations). Draft risk-tiering and control mapping aligned to NIST AI RMF functions; publish two pre-approved patterns.Quarter 2: Turn controls into pipelines (CI checks for evals, data scans, model cards). Convert two fast-moving teams from shadow AI to platform AI by making the paved road easier than the side road.Quarter 3: Pilot a GxP-style review (a rigorous documentation standard from life sciences) for one high-risk use case; automate evidence capture. Start your EU AI Act gap analysis if you touch Europe; assign owners and deadlines.Quarter 4: Expand your pattern catalog (RAG, batch inference, streaming prediction). Roll out dashboards for risk/compliance. Bake governance SLAs into your OKRs. By this point, you haven’t slowed down innovation — you’ve standardized it. The research community can keep moving at light speed; you can keep shipping at enterprise speed — without the audit queue becoming your critical path.The competitive edge isn&#x27;t the next model — it&#x27;s the next mileIt’s tempting to chase each week’s leaderboard. But the durable advantage is the mile between a paper and production: The platform, the patterns, the proofs. That’s what your competitors can’t copy from GitHub, and it’s the only way to keep velocity without trading compliance for chaos. In other words: Make governance the grease, not the grit. Jayachander Reddy Kandakatla is senior machine learning operations (MLOps) engineer at Ford Motor Credit Company.",
          "feed_position": 1,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/77WjI7ALpxOD2JSAUGMDRD/e938cb7b3827956c6043438c7af3f0d6/Velocity_gap.png"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/is-vibe-coding-ruining-a-generation-of-engineers",
          "published_at": "Sat, 11 Oct 2025 19:00:00 GMT",
          "title": "Is vibe coding ruining a generation of engineers?",
          "standfirst": "AI tools are revolutionizing software development by automating repetitive tasks, refactoring bloated code, and identifying bugs in real-time. Developers can now generate well-structured code from plain language prompts, saving hours of manual effort. These tools learn from vast codebases, offering context-aware recommendations that enhance productivity and reduce errors. Rather than starting from scratch, engineers can prototype quickly, iterate faster and focus on solving increasingly complex problems.As code generation tools grow in popularity, they raise questions about the future size and structure of engineering teams. Earlier this year, Garry Tan, CEO of startup accelerator Y Combinator, noted that about one-quarter of its current clients use AI to write 95% or more of their software. In an interview with CNBC, Tan said: “What that means for founders is that you don’t need a team of 50 or 100 engineers, you don’t have to raise as much. The capital goes much longer.”AI-powered coding may offer a fast solution for businesses under budget pressure — but its long-term effects on the field and labor pool cannot be ignored.As AI-powered coding rises, human expertise may diminish In the era of AI, the traditional journey to coding expertise that has long supported senior developers may be at risk. Easy access to large language models (LLMs) enables junior coders to quickly identify issues in code. While this speeds up software development, it can distance developers from their own work, delaying the growth of core problem-solving skills. As a result, they may avoid the focused, sometimes uncomfortable hours required to build expertise and progress on the path to becoming successful senior developers.Consider Anthropic’s Claude Code, a terminal-based assistant built on the Claude 3.7 Sonnet model, which automates bug detection and resolution, test creation and code refactoring. Using natural language commands, it reduces repetitive manual work and boosts productivity.Microsoft has also released two open-source frameworks — AutoGen and Semantic Kernel — to support the development of agentic AI systems. AutoGen enables asynchronous messaging, modular components, and distributed agent collaboration to build complex workflows with minimal human input. Semantic Kernel is an SDK that integrates LLMs with languages like C#, Python and Java, letting developers build AI agents to automate tasks and manage enterprise applications.The increasing availability of these tools from Anthropic, Microsoft and others may reduce opportunities for coders to refine and deepen their skills. Rather than “banging their heads against the wall” to debug a few lines or select a library to unlock new features, junior developers may simply turn to AI for an assist. This means senior coders with problem-solving skills honed over decades may become an endangered species.Overreliance on AI for writing code risks weakening developers’ hands-on experience and understanding of key programming concepts. Without regular practice, they may struggle to independently debug, optimize or design systems. Ultimately, this erosion of skill can undermine critical thinking, creativity and adaptability — qualities that are essential not just for coding, but for assessing the quality and logic of AI-generated solutions.AI as mentor: Turning code automation into hands-on learningWhile concerns about AI diminishing human developer skills are valid, businesses shouldn’t dismiss AI-supported coding. They just need to think carefully about when and how to deploy AI tools in development. These tools can be more than productivity boosters; they can act as interactive mentors, guiding coders in real time with explanations, alternatives and best practices.When used as a training tool, AI can reinforce learning by showing coders why code is broken and how to fix it—rather than simply applying a solution. For example, a junior developer using Claude Code might receive immediate feedback on inefficient syntax or logic errors, along with suggestions linked to detailed explanations. This enables active learning, not passive correction. It’s a win-win: Accelerating project timelines without doing all the work for junior coders.Additionally, coding frameworks can support experimentation by letting developers prototype agent workflows or integrate LLMs without needing expert-level knowledge upfront. By observing how AI builds and refines code, junior developers who actively engage with these tools can internalize patterns, architectural decisions and debugging strategies — mirroring the traditional learning process of trial and error, code reviews and mentorship.However, AI coding assistants shouldn’t replace real mentorship or pair programming. Pull requests and formal code reviews remain essential for guiding newer, less experienced team members. We are nowhere near the point at which AI can single-handedly upskill a junior developer.Companies and educators can build structured development programs around these tools that emphasize code comprehension to ensure AI is used as a training partner rather than a crutch. This encourages coders to question AI outputs and requires manual refactoring exercises. In this way, AI becomes less of a replacement for human ingenuity and more of a catalyst for accelerated, experiential learning.Bridging the gap between automation and educationWhen utilized with intention, AI doesn’t just write code; it teaches coding, blending automation with education to prepare developers for a future where deep understanding and adaptability remain indispensable.By embracing AI as a mentor, as a programming partner and as a team of developers we can direct to the problem at hand, we can bridge the gap between effective automation and education. We can empower developers to grow alongside the tools they use. We can ensure that, as AI evolves, so too does the human skill set, fostering a generation of coders who are both efficient and deeply knowledgeable.Richard Sonnenblick is chief data scientist at Planview.",
          "content": "AI tools are revolutionizing software development by automating repetitive tasks, refactoring bloated code, and identifying bugs in real-time. Developers can now generate well-structured code from plain language prompts, saving hours of manual effort. These tools learn from vast codebases, offering context-aware recommendations that enhance productivity and reduce errors. Rather than starting from scratch, engineers can prototype quickly, iterate faster and focus on solving increasingly complex problems.As code generation tools grow in popularity, they raise questions about the future size and structure of engineering teams. Earlier this year, Garry Tan, CEO of startup accelerator Y Combinator, noted that about one-quarter of its current clients use AI to write 95% or more of their software. In an interview with CNBC, Tan said: “What that means for founders is that you don’t need a team of 50 or 100 engineers, you don’t have to raise as much. The capital goes much longer.”AI-powered coding may offer a fast solution for businesses under budget pressure — but its long-term effects on the field and labor pool cannot be ignored.As AI-powered coding rises, human expertise may diminish In the era of AI, the traditional journey to coding expertise that has long supported senior developers may be at risk. Easy access to large language models (LLMs) enables junior coders to quickly identify issues in code. While this speeds up software development, it can distance developers from their own work, delaying the growth of core problem-solving skills. As a result, they may avoid the focused, sometimes uncomfortable hours required to build expertise and progress on the path to becoming successful senior developers.Consider Anthropic’s Claude Code, a terminal-based assistant built on the Claude 3.7 Sonnet model, which automates bug detection and resolution, test creation and code refactoring. Using natural language commands, it reduces repetitive manual work and boosts productivity.Microsoft has also released two open-source frameworks — AutoGen and Semantic Kernel — to support the development of agentic AI systems. AutoGen enables asynchronous messaging, modular components, and distributed agent collaboration to build complex workflows with minimal human input. Semantic Kernel is an SDK that integrates LLMs with languages like C#, Python and Java, letting developers build AI agents to automate tasks and manage enterprise applications.The increasing availability of these tools from Anthropic, Microsoft and others may reduce opportunities for coders to refine and deepen their skills. Rather than “banging their heads against the wall” to debug a few lines or select a library to unlock new features, junior developers may simply turn to AI for an assist. This means senior coders with problem-solving skills honed over decades may become an endangered species.Overreliance on AI for writing code risks weakening developers’ hands-on experience and understanding of key programming concepts. Without regular practice, they may struggle to independently debug, optimize or design systems. Ultimately, this erosion of skill can undermine critical thinking, creativity and adaptability — qualities that are essential not just for coding, but for assessing the quality and logic of AI-generated solutions.AI as mentor: Turning code automation into hands-on learningWhile concerns about AI diminishing human developer skills are valid, businesses shouldn’t dismiss AI-supported coding. They just need to think carefully about when and how to deploy AI tools in development. These tools can be more than productivity boosters; they can act as interactive mentors, guiding coders in real time with explanations, alternatives and best practices.When used as a training tool, AI can reinforce learning by showing coders why code is broken and how to fix it—rather than simply applying a solution. For example, a junior developer using Claude Code might receive immediate feedback on inefficient syntax or logic errors, along with suggestions linked to detailed explanations. This enables active learning, not passive correction. It’s a win-win: Accelerating project timelines without doing all the work for junior coders.Additionally, coding frameworks can support experimentation by letting developers prototype agent workflows or integrate LLMs without needing expert-level knowledge upfront. By observing how AI builds and refines code, junior developers who actively engage with these tools can internalize patterns, architectural decisions and debugging strategies — mirroring the traditional learning process of trial and error, code reviews and mentorship.However, AI coding assistants shouldn’t replace real mentorship or pair programming. Pull requests and formal code reviews remain essential for guiding newer, less experienced team members. We are nowhere near the point at which AI can single-handedly upskill a junior developer.Companies and educators can build structured development programs around these tools that emphasize code comprehension to ensure AI is used as a training partner rather than a crutch. This encourages coders to question AI outputs and requires manual refactoring exercises. In this way, AI becomes less of a replacement for human ingenuity and more of a catalyst for accelerated, experiential learning.Bridging the gap between automation and educationWhen utilized with intention, AI doesn’t just write code; it teaches coding, blending automation with education to prepare developers for a future where deep understanding and adaptability remain indispensable.By embracing AI as a mentor, as a programming partner and as a team of developers we can direct to the problem at hand, we can bridge the gap between effective automation and education. We can empower developers to grow alongside the tools they use. We can ensure that, as AI evolves, so too does the human skill set, fostering a generation of coders who are both efficient and deeply knowledgeable.Richard Sonnenblick is chief data scientist at Planview.",
          "feed_position": 2,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/6O2FgBkLOdYG0bQrbbceee/70c01c9a6f6498e1888781c6ec759dd8/Junior_coders.png"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/when-dirt-meets-data-scottsmiracle-gro",
          "published_at": "Sat, 11 Oct 2025 13:00:00 GMT",
          "title": "When dirt meets data: ScottsMiracle-Gro saved $150M using AI",
          "standfirst": "How a semiconductor veteran turned over a century of horticultural wisdom into AI-led competitive advantage For decades, a ritual played out across ScottsMiracle-Gro’s media facilities. Every few weeks, workers walked acres of towering compost and wood chip piles with nothing more than measuring sticks. They wrapped rulers around each mound, estimated height, and did what company President Nate Baxter now describes as “sixth-grade geometry to figure out volume.”Today, drones glide over those same plants with mechanical precision. Vision systems calculate volumes in real time. The move from measuring sticks to artificial intelligence signals more than efficiency. It is the visible proof of one of corporate America’s most unlikely technology stories.The AI revolution finds an unexpected leaderEnterprise AI has been led by predictable players. Software companies with cloud-native architectures. Financial services firms with vast data lakes. Retailers with rich digital touchpoints. Consumer packaged goods companies that handle physical products like fertilizer and soil were not expected to lead.Yet ScottsMiracle-Gro has realized more than half of a targeted $150 million in supply chain savings. It reports a 90 percent improvement in customer service response times. Its predictive models enable weekly reallocation of marketing resources across regional markets.A Silicon Valley veteran bets on soil scienceBaxter’s path to ScottsMiracle-Gro (SMG) reads like a calculated pivot, not a corporate rescue. After two decades in semiconductor manufacturing at Intel and Tokyo Electron, he knew how to apply advanced technology to complex operations.“I sort of initially said, ‘Why would I do this? I’m running a tech company. It’s an industry I’ve been in for 25 years,’” Baxter recalls of his reaction when ScottsMiracle-Gro CEO Jim Hagedorn approached him in 2023. The company was reeling from a collapsed $1.2 billion hydroponics investment and facing what he describes as “pressure from a leverage standpoint.”His wife challenged him with a direct prompt. If you are not learning or putting yourself in uncomfortable situations, you should change that.Baxter saw clear parallels between semiconductor manufacturing and SMG’s operations. Both require precision, quality control, and the optimization of complex systems. He also saw untapped potential in SMG’s domain knowledge. One hundred fifty years of horticultural expertise, regulatory know-how, and customer insight had never been fully digitized.“It became apparent to me whether it was on the backend with data analytics, business process transformation, and obviously now with AI being front and center of the consumer experience, a lot of opportunities are there,” he explains.The declaration that changed everythingThe pivot began at an all-hands meeting. “I just said, you know, guys, we’re a tech company. You just don’t know it yet,” Baxter recalls. “There’s so much opportunity here to drive this company to where it needs to go.”The first challenge was organizational. SMG had evolved into functional silos. IT, supply chain, and brand teams ran independent systems with little coordination. Drawing on his experience with complex technology organizations, Baxter restructured the consumer business into three business units. General managers became accountable not just for financial results but also for technology implementation within their domains.“I came in and said, we’re going to create new business units,” he explains. “The buck stops with you and I’m holding you accountable not only for the business results, for the quality of the creative and marketing, but for the implementation of technology.”To support the new structure, SMG set up centers of excellence for digital capabilities, insights and analytics, and creative functions. The hybrid design placed centralized expertise behind distributed accountability.Mining corporate memory for AI goldTurning legacy knowledge into machine-ready intelligence required what Fausto Fleites, VP of Data Intelligence, calls “archaeological work.” The team excavated decades of business logic embedded in legacy SAP systems and converted filing cabinets of research into AI-ready datasets. Fleites, a Cuban immigrant with a doctorate from FIU who led Florida’s public hurricane loss model before roles at Sears and Cemex, understood the stakes.“The costly part of the migration was the business reporting layer we have in SAP Business Warehouse,” Fleites explains. “You need to uncover business logic created in many cases over decades.”SMG chose Databricks as its unified data platform. The team had Apache Spark expertise. Databricks offered strong SAP integration and aligned with a preference for open-source technologies that minimize vendor lock-in.The breakthrough came through systematic knowledge management. SMG built an AI bot using Google’s Gemini large language model to catalog and clean internal repositories. The system identified duplicates, grouped content by topic, and restructured information for AI consumption. The effort reduced knowledge articles by 30 percent while increasing their utility.“We used Gemini LLMs to actually categorize them into topics, find similar documents,” Fleites explains. A hybrid approach that combined modern AI with techniques like cosine similarity became the foundation for later applications.Building AI systems that actually understand fertilizerEarly trials with off-the-shelf AI exposed a real risk. General-purpose models confused products designed for killing weeds with those for preventing them. That mistake can ruin a lawn.“Different products, if you use one in the wrong place, would actually have a very negative outcome,” Fleites notes. “But those are kind of synonyms in certain contexts to the LLM. So they were recommending the wrong products.”The solution was a new architecture. SMG created what Fleites calls a “hierarchy of agents.” A supervisor agent routes queries to specialized worker agents organized by brand. Each agent draws on deep product knowledge encoded from a 400-page internal training manual.The system also changes the conversation. When users ask for recommendations, the agents start with questions about location, goals, and lawn conditions. They narrow possibilities step by step before offering suggestions. The stack integrates with APIs for product availability and state-specific regulatory compliance.From drones to demand forecasting across the enterpriseThe transformation runs across the company. Drones measure inventory piles. Demand forecasting models analyze more than 60 factors, including weather patterns, consumer sentiment, and macroeconomic indicators.These predictions enable faster moves. When drought struck Texas, the models supported a shift in promotional spending to regions with favorable weather. The reallocation helped drive positive quarterly results.“We not only have the ability to move marketing and promotion dollars around, but we’ve even gotten to the point where if it’s going to be a big weekend in the Northeast, we’ll shift our field sales resources from other regions up there,” Baxter explains.Consumer Services changed as well. AI agents now process incoming emails through Salesforce, draft responses based on the knowledge base, and flag them for brief human review. Draft times dropped from ten minutes to seconds and response quality improved.The company emphasizes explainable AI. Using SHAP, SMG built dashboards that decompose each forecast and show how weather, promotions, or media spending contribute to predictions.“Typically, if you open a prediction to a business person and you don’t say why, they’ll say, ‘I don’t believe you,’” Fleites explains. Transparency made it possible to move resource allocation from quarterly to weekly cycles.Competing like a startupSMG’s results challenge assumptions about AI readiness in traditional industries. The advantage does not come from owning the most sophisticated models. It comes from combining general-purpose AI with unique, structured domain knowledge.“LLMs are going to be a commodity,” Fleites observes. “The strategic differentiator is what is the additional level of [internal] knowledge we can fit to them.”Partnerships are central. SMG works with Google Vertex AI for foundational models, Sierra.ai for production-ready conversational agents, and Kindwise for computer vision. The ecosystem approach lets a small internal team recruited from Meta, Google, and AI startups deliver outsized impact without building everything from scratch.Talent follows impact. Conventional wisdom says traditional companies cannot compete with Meta salaries or Google stock. SMG offered something different. It offered the chance to build transformative AI applications with immediate business impact.“When we have these interviews, what we propose to them is basically the ability to have real value with the latest knowledge in these spaces,” Fleites explains. “A lot of people feel motivated to come to us” because much of big tech AI work, despite the hype, “doesn’t really have an impact.”Team design mirrors that philosophy. “My direct reports are leaders and not only manage people, but are technically savvy,” Fleites notes. “We always are constantly switching hands between developing or maintaining a solution versus strategy versus managing people.” He still writes code weekly. The small team of 15 to 20 AI and engineering professionals stays lean by contracting out implementation while keeping “the know-how and the direction and the architecture” in-house.When innovation meets immovable objectsNot every pilot succeeded. SMG tested semi-autonomous forklifts in a 1.3 million square foot distribution facility. Remote drivers in the Philippines controlled up to five vehicles at once with strong safety records.“The technology was actually really great,” Baxter acknowledges. The vehicles could not lift enough weight for SMG’s heavy products. The company paused implementation.“Not everything we’ve tried has gone smoothly,” Baxter admits. “But I think another important point is you have to focus on a few critical ones and you have to know when something isn’t going to work and readjust.”The lesson tracks with semiconductor discipline. Investments must show measurable returns within set timeframes. Regulatory complexity adds difficulty. Products must comply with EPA rules and a patchwork of state restrictions, which AI systems must navigate correctly.The gardening sommelier and agent-to-agent futuresThe roadmap reflects a long-term view. SMG plans a “gardening sommelier” mobile app in 2026 that identifies plants, weeds, and lawn problems from photos and provides instant guidance. A beta already helps field sales teams answer complex product questions by querying the 400-page knowledge base.The company is exploring agent-to-agent communication so its specialized AI can interface with retail partners’ systems. A customer who asks a Walmart chatbot for lawn advice could trigger an SMG query that returns accurate, regulation-compliant recommendations.SMG has launched AI-powered search on its website, replacing keyword systems with conversational engines based on the internal stack. The future vision pairs predictive models with conversational agents so the system can reach out when conditions suggest a customer may need help.What traditional industries can learnScottsMiracle-Gro&#x27;s transformation offers a clear playbook for enterprises. The advantage doesn&#x27;t come from deploying the most sophisticated models. Instead, it comes from combining AI with proprietary domain knowledge that competitors can&#x27;t easily replicate.By making general managers responsible for both business results and technology implementation, SMG ensured AI wasn&#x27;t just an IT initiative but a business imperative. The 150 years of horticultural expertise only became valuable when it was digitized, structured, and made accessible to AI systems.Legacy companies competing for AI engineers can&#x27;t match Silicon Valley compensation packages. But they can offer something tech giants often can&#x27;t: immediate, measurable impact. When engineers see their weather forecasting models directly influence quarterly results or their agent architecture prevent customers from ruining their lawns, the work carries weight that another incremental improvement to an ad algorithm never will.“We have a right to win,” Baxter says. “We have 150 years of this experience.” That experience is now data, and data is the company’s competitive edge. ScottsMiracle-Gro didn’t outspend its rivals or chase the newest AI model. It turned knowledge into an operating system for growth. For a company built on soil, its biggest breakthrough might be cultivating data.",
          "content": "How a semiconductor veteran turned over a century of horticultural wisdom into AI-led competitive advantage For decades, a ritual played out across ScottsMiracle-Gro’s media facilities. Every few weeks, workers walked acres of towering compost and wood chip piles with nothing more than measuring sticks. They wrapped rulers around each mound, estimated height, and did what company President Nate Baxter now describes as “sixth-grade geometry to figure out volume.”Today, drones glide over those same plants with mechanical precision. Vision systems calculate volumes in real time. The move from measuring sticks to artificial intelligence signals more than efficiency. It is the visible proof of one of corporate America’s most unlikely technology stories.The AI revolution finds an unexpected leaderEnterprise AI has been led by predictable players. Software companies with cloud-native architectures. Financial services firms with vast data lakes. Retailers with rich digital touchpoints. Consumer packaged goods companies that handle physical products like fertilizer and soil were not expected to lead.Yet ScottsMiracle-Gro has realized more than half of a targeted $150 million in supply chain savings. It reports a 90 percent improvement in customer service response times. Its predictive models enable weekly reallocation of marketing resources across regional markets.A Silicon Valley veteran bets on soil scienceBaxter’s path to ScottsMiracle-Gro (SMG) reads like a calculated pivot, not a corporate rescue. After two decades in semiconductor manufacturing at Intel and Tokyo Electron, he knew how to apply advanced technology to complex operations.“I sort of initially said, ‘Why would I do this? I’m running a tech company. It’s an industry I’ve been in for 25 years,’” Baxter recalls of his reaction when ScottsMiracle-Gro CEO Jim Hagedorn approached him in 2023. The company was reeling from a collapsed $1.2 billion hydroponics investment and facing what he describes as “pressure from a leverage standpoint.”His wife challenged him with a direct prompt. If you are not learning or putting yourself in uncomfortable situations, you should change that.Baxter saw clear parallels between semiconductor manufacturing and SMG’s operations. Both require precision, quality control, and the optimization of complex systems. He also saw untapped potential in SMG’s domain knowledge. One hundred fifty years of horticultural expertise, regulatory know-how, and customer insight had never been fully digitized.“It became apparent to me whether it was on the backend with data analytics, business process transformation, and obviously now with AI being front and center of the consumer experience, a lot of opportunities are there,” he explains.The declaration that changed everythingThe pivot began at an all-hands meeting. “I just said, you know, guys, we’re a tech company. You just don’t know it yet,” Baxter recalls. “There’s so much opportunity here to drive this company to where it needs to go.”The first challenge was organizational. SMG had evolved into functional silos. IT, supply chain, and brand teams ran independent systems with little coordination. Drawing on his experience with complex technology organizations, Baxter restructured the consumer business into three business units. General managers became accountable not just for financial results but also for technology implementation within their domains.“I came in and said, we’re going to create new business units,” he explains. “The buck stops with you and I’m holding you accountable not only for the business results, for the quality of the creative and marketing, but for the implementation of technology.”To support the new structure, SMG set up centers of excellence for digital capabilities, insights and analytics, and creative functions. The hybrid design placed centralized expertise behind distributed accountability.Mining corporate memory for AI goldTurning legacy knowledge into machine-ready intelligence required what Fausto Fleites, VP of Data Intelligence, calls “archaeological work.” The team excavated decades of business logic embedded in legacy SAP systems and converted filing cabinets of research into AI-ready datasets. Fleites, a Cuban immigrant with a doctorate from FIU who led Florida’s public hurricane loss model before roles at Sears and Cemex, understood the stakes.“The costly part of the migration was the business reporting layer we have in SAP Business Warehouse,” Fleites explains. “You need to uncover business logic created in many cases over decades.”SMG chose Databricks as its unified data platform. The team had Apache Spark expertise. Databricks offered strong SAP integration and aligned with a preference for open-source technologies that minimize vendor lock-in.The breakthrough came through systematic knowledge management. SMG built an AI bot using Google’s Gemini large language model to catalog and clean internal repositories. The system identified duplicates, grouped content by topic, and restructured information for AI consumption. The effort reduced knowledge articles by 30 percent while increasing their utility.“We used Gemini LLMs to actually categorize them into topics, find similar documents,” Fleites explains. A hybrid approach that combined modern AI with techniques like cosine similarity became the foundation for later applications.Building AI systems that actually understand fertilizerEarly trials with off-the-shelf AI exposed a real risk. General-purpose models confused products designed for killing weeds with those for preventing them. That mistake can ruin a lawn.“Different products, if you use one in the wrong place, would actually have a very negative outcome,” Fleites notes. “But those are kind of synonyms in certain contexts to the LLM. So they were recommending the wrong products.”The solution was a new architecture. SMG created what Fleites calls a “hierarchy of agents.” A supervisor agent routes queries to specialized worker agents organized by brand. Each agent draws on deep product knowledge encoded from a 400-page internal training manual.The system also changes the conversation. When users ask for recommendations, the agents start with questions about location, goals, and lawn conditions. They narrow possibilities step by step before offering suggestions. The stack integrates with APIs for product availability and state-specific regulatory compliance.From drones to demand forecasting across the enterpriseThe transformation runs across the company. Drones measure inventory piles. Demand forecasting models analyze more than 60 factors, including weather patterns, consumer sentiment, and macroeconomic indicators.These predictions enable faster moves. When drought struck Texas, the models supported a shift in promotional spending to regions with favorable weather. The reallocation helped drive positive quarterly results.“We not only have the ability to move marketing and promotion dollars around, but we’ve even gotten to the point where if it’s going to be a big weekend in the Northeast, we’ll shift our field sales resources from other regions up there,” Baxter explains.Consumer Services changed as well. AI agents now process incoming emails through Salesforce, draft responses based on the knowledge base, and flag them for brief human review. Draft times dropped from ten minutes to seconds and response quality improved.The company emphasizes explainable AI. Using SHAP, SMG built dashboards that decompose each forecast and show how weather, promotions, or media spending contribute to predictions.“Typically, if you open a prediction to a business person and you don’t say why, they’ll say, ‘I don’t believe you,’” Fleites explains. Transparency made it possible to move resource allocation from quarterly to weekly cycles.Competing like a startupSMG’s results challenge assumptions about AI readiness in traditional industries. The advantage does not come from owning the most sophisticated models. It comes from combining general-purpose AI with unique, structured domain knowledge.“LLMs are going to be a commodity,” Fleites observes. “The strategic differentiator is what is the additional level of [internal] knowledge we can fit to them.”Partnerships are central. SMG works with Google Vertex AI for foundational models, Sierra.ai for production-ready conversational agents, and Kindwise for computer vision. The ecosystem approach lets a small internal team recruited from Meta, Google, and AI startups deliver outsized impact without building everything from scratch.Talent follows impact. Conventional wisdom says traditional companies cannot compete with Meta salaries or Google stock. SMG offered something different. It offered the chance to build transformative AI applications with immediate business impact.“When we have these interviews, what we propose to them is basically the ability to have real value with the latest knowledge in these spaces,” Fleites explains. “A lot of people feel motivated to come to us” because much of big tech AI work, despite the hype, “doesn’t really have an impact.”Team design mirrors that philosophy. “My direct reports are leaders and not only manage people, but are technically savvy,” Fleites notes. “We always are constantly switching hands between developing or maintaining a solution versus strategy versus managing people.” He still writes code weekly. The small team of 15 to 20 AI and engineering professionals stays lean by contracting out implementation while keeping “the know-how and the direction and the architecture” in-house.When innovation meets immovable objectsNot every pilot succeeded. SMG tested semi-autonomous forklifts in a 1.3 million square foot distribution facility. Remote drivers in the Philippines controlled up to five vehicles at once with strong safety records.“The technology was actually really great,” Baxter acknowledges. The vehicles could not lift enough weight for SMG’s heavy products. The company paused implementation.“Not everything we’ve tried has gone smoothly,” Baxter admits. “But I think another important point is you have to focus on a few critical ones and you have to know when something isn’t going to work and readjust.”The lesson tracks with semiconductor discipline. Investments must show measurable returns within set timeframes. Regulatory complexity adds difficulty. Products must comply with EPA rules and a patchwork of state restrictions, which AI systems must navigate correctly.The gardening sommelier and agent-to-agent futuresThe roadmap reflects a long-term view. SMG plans a “gardening sommelier” mobile app in 2026 that identifies plants, weeds, and lawn problems from photos and provides instant guidance. A beta already helps field sales teams answer complex product questions by querying the 400-page knowledge base.The company is exploring agent-to-agent communication so its specialized AI can interface with retail partners’ systems. A customer who asks a Walmart chatbot for lawn advice could trigger an SMG query that returns accurate, regulation-compliant recommendations.SMG has launched AI-powered search on its website, replacing keyword systems with conversational engines based on the internal stack. The future vision pairs predictive models with conversational agents so the system can reach out when conditions suggest a customer may need help.What traditional industries can learnScottsMiracle-Gro&#x27;s transformation offers a clear playbook for enterprises. The advantage doesn&#x27;t come from deploying the most sophisticated models. Instead, it comes from combining AI with proprietary domain knowledge that competitors can&#x27;t easily replicate.By making general managers responsible for both business results and technology implementation, SMG ensured AI wasn&#x27;t just an IT initiative but a business imperative. The 150 years of horticultural expertise only became valuable when it was digitized, structured, and made accessible to AI systems.Legacy companies competing for AI engineers can&#x27;t match Silicon Valley compensation packages. But they can offer something tech giants often can&#x27;t: immediate, measurable impact. When engineers see their weather forecasting models directly influence quarterly results or their agent architecture prevent customers from ruining their lawns, the work carries weight that another incremental improvement to an ad algorithm never will.“We have a right to win,” Baxter says. “We have 150 years of this experience.” That experience is now data, and data is the company’s competitive edge. ScottsMiracle-Gro didn’t outspend its rivals or chase the newest AI model. It turned knowledge into an operating system for growth. For a company built on soil, its biggest breakthrough might be cultivating data.",
          "feed_position": 3,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/7d0hJ4qtWTk5GaypvPfHJY/53aa8c26a39b5f10e16b3073dd73a388/videoframe_878.png"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/computing/windows-10-support-ends-october-14-but-heres-how-to-get-an-extra-year-for-free-125118616.html",
          "published_at": "Sat, 11 Oct 2025 12:51:18 +0000",
          "title": "Windows 10 support ends October 14, but here's how to get an extra year for free",
          "standfirst": "You'll get access to Windows 10 a little longer by doing this. (Getty Images) You've known it for a while: Microsoft has determined that it's moving Windows 10 to \"end of life\" status starting on October 14. That means while Windows 10 PCs will continue to work after that date, they'll stop getting important security updates by default. That leaves you with three choices if you want to make sure your computer remains secure: You can choose to upgrade to Windows 11 for free if your computer is compatible. You can buy a new PC that already has Windows 11 pre-installed (or opt for an alternative, like a Mac or a Chromebook). Forget about Windows 11 right now and sign up for the Extended Security Updates (ESU), which lets you kick the can down the road for a year. Since the last one is easy — and can now be done for free in many cases — that's what we're focusing on here. We'll walk you through the steps of keeping Windows 10 on your PC… for now, at least. How to sign up for Windows 10 Extended Security Updates on your computer We can question Microsoft's motives for killing off Windows 10, even though it works perfectly well on most older PCs. But without those periodic security updates, your PC will become increasingly susceptible to malware with each passing week. To that end, enrolling in Extended Security Updates (ESU) will give you another year of using Windows 10 securely. At one point, Microsoft suggested the 12-month extension would require a $30 fee. While that's still an option, there's now a free path for Windows 10 users in the US. Here's how to make it happen. Step 1: Make sure your PC is up to date You can find out if your computer is up-to-date by going into your Settings > System > About, then scroll down to see what version you're running. If not, you'll want to make sure you also install all the Windows 10 updates available. Step 2: Make sure you're using an administrator account If you share a computer with multiple people in your household, make sure you're signed in to the administrator account. Typically, it's the first account created on the computer. You'll know it's the right one when you see \"Administrator\" under the name. (You can double-check under Settings > Your Info.) Step 3: Verify if your PC is eligible to upgrade to Windows 11 (or not) If you see an option to upgrade to Windows 11, just do that. It's free and it keeps you in the Windows loop. Otherwise, continue following the steps below so you can keep your computer safe with security updates. Step 4: Enroll in Extended Security Updates Sign up for ESU by selecting Update & Security from the Settings menu. Click the \"Enroll Now\" sign-up link, as pictured below. Again, you may see an option to download Windows 11 if your computer meets the requirements (again, definitely do that if you see it). Find out if you need to update your computer. (Screenshot/Engadget) If you're not seeing the \"Enroll now\" link, you probably need to update and install the latest Windows 10 updates (as noted above). By enrolling in Extended Security Updates, you'll have another year before you need to upgrade to Windows 11. (Screenshots/Engadget) Step 5: Choose your upgrade method Next up is choosing how you want to enroll, and you have a few options. The easiest way is to back up your PC settings. It's free, but it takes a little bit of time since you'll need to back up your data. Again, you'll need to be using your administrator account to get started. Back up your PC before you enroll in ESU. (ExplainingComputers via YouTube) That said, the free option here comes with two catches, at least for users in the US. (European users will get the free option with no strings attached.) The first is that you'll be linking your Windows login to Microsoft's cloud-based online service. Most users have likely already done this (if they're using CoPilot, Office 365, GamePass, OneDrive or one of Microsoft's other various online services). But if you've specifically opted for a local login to Windows, the price you're paying for this \"free\" extension is joining the cloud-connected Microsoft universe. The other potential issue is that the free backup only applies to the first 5 GB of storage. Anything more, and you’ll need to pay up for Microsoft's OneDrive services. But thankfully, you can turn off anything you don't want to back up by going to Settings > OneDrive and toggling off options like Documents, Pictures and Videos to get in under the free threshold to start. Once you're signed in, a window will pop up that says \"Add this device to receive Extended Security Updates.\" Click Add Device to enroll it. Click Done. A note: Thanks to YouTube's Explaining Computers channel, where we grabbed the screenshot above (since our test PC was already signed up for cloud backups, and didn't provide the splash screen to choose options). You can watch their full video if you'd like a deeper dive into the process. That's it, you're done! (Until next year) You've got 12 more months to figure out an alternative upgrade path to Windows 11. If anything changes next year, we'll update this story with what your next steps are. You did it right if you see this window. (Screenshot/Engadget) This article originally appeared on Engadget at https://www.engadget.com/computing/windows-10-support-ends-october-14-but-heres-how-to-get-an-extra-year-for-free-125118616.html?src=rss",
          "content": "You'll get access to Windows 10 a little longer by doing this. (Getty Images) You've known it for a while: Microsoft has determined that it's moving Windows 10 to \"end of life\" status starting on October 14. That means while Windows 10 PCs will continue to work after that date, they'll stop getting important security updates by default. That leaves you with three choices if you want to make sure your computer remains secure: You can choose to upgrade to Windows 11 for free if your computer is compatible. You can buy a new PC that already has Windows 11 pre-installed (or opt for an alternative, like a Mac or a Chromebook). Forget about Windows 11 right now and sign up for the Extended Security Updates (ESU), which lets you kick the can down the road for a year. Since the last one is easy — and can now be done for free in many cases — that's what we're focusing on here. We'll walk you through the steps of keeping Windows 10 on your PC… for now, at least. How to sign up for Windows 10 Extended Security Updates on your computer We can question Microsoft's motives for killing off Windows 10, even though it works perfectly well on most older PCs. But without those periodic security updates, your PC will become increasingly susceptible to malware with each passing week. To that end, enrolling in Extended Security Updates (ESU) will give you another year of using Windows 10 securely. At one point, Microsoft suggested the 12-month extension would require a $30 fee. While that's still an option, there's now a free path for Windows 10 users in the US. Here's how to make it happen. Step 1: Make sure your PC is up to date You can find out if your computer is up-to-date by going into your Settings > System > About, then scroll down to see what version you're running. If not, you'll want to make sure you also install all the Windows 10 updates available. Step 2: Make sure you're using an administrator account If you share a computer with multiple people in your household, make sure you're signed in to the administrator account. Typically, it's the first account created on the computer. You'll know it's the right one when you see \"Administrator\" under the name. (You can double-check under Settings > Your Info.) Step 3: Verify if your PC is eligible to upgrade to Windows 11 (or not) If you see an option to upgrade to Windows 11, just do that. It's free and it keeps you in the Windows loop. Otherwise, continue following the steps below so you can keep your computer safe with security updates. Step 4: Enroll in Extended Security Updates Sign up for ESU by selecting Update & Security from the Settings menu. Click the \"Enroll Now\" sign-up link, as pictured below. Again, you may see an option to download Windows 11 if your computer meets the requirements (again, definitely do that if you see it). Find out if you need to update your computer. (Screenshot/Engadget) If you're not seeing the \"Enroll now\" link, you probably need to update and install the latest Windows 10 updates (as noted above). By enrolling in Extended Security Updates, you'll have another year before you need to upgrade to Windows 11. (Screenshots/Engadget) Step 5: Choose your upgrade method Next up is choosing how you want to enroll, and you have a few options. The easiest way is to back up your PC settings. It's free, but it takes a little bit of time since you'll need to back up your data. Again, you'll need to be using your administrator account to get started. Back up your PC before you enroll in ESU. (ExplainingComputers via YouTube) That said, the free option here comes with two catches, at least for users in the US. (European users will get the free option with no strings attached.) The first is that you'll be linking your Windows login to Microsoft's cloud-based online service. Most users have likely already done this (if they're using CoPilot, Office 365, GamePass, OneDrive or one of Microsoft's other various online services). But if you've specifically opted for a local login to Windows, the price you're paying for this \"free\" extension is joining the cloud-connected Microsoft universe. The other potential issue is that the free backup only applies to the first 5 GB of storage. Anything more, and you’ll need to pay up for Microsoft's OneDrive services. But thankfully, you can turn off anything you don't want to back up by going to Settings > OneDrive and toggling off options like Documents, Pictures and Videos to get in under the free threshold to start. Once you're signed in, a window will pop up that says \"Add this device to receive Extended Security Updates.\" Click Add Device to enroll it. Click Done. A note: Thanks to YouTube's Explaining Computers channel, where we grabbed the screenshot above (since our test PC was already signed up for cloud backups, and didn't provide the splash screen to choose options). You can watch their full video if you'd like a deeper dive into the process. That's it, you're done! (Until next year) You've got 12 more months to figure out an alternative upgrade path to Windows 11. If anything changes next year, we'll update this story with what your next steps are. You did it right if you see this window. (Screenshot/Engadget) This article originally appeared on Engadget at https://www.engadget.com/computing/windows-10-support-ends-october-14-but-heres-how-to-get-an-extra-year-for-free-125118616.html?src=rss",
          "feed_position": 10,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2025-10/c836b6e0-a60d-11f0-aff0-71a091f199fd"
        }
      ],
      "featured_image": "https://images.ctfassets.net/jdtwqhzvc2n1/5b0hTPUKh7PB9D7CfWwO22/3b31fdc3f95c670ce5096758d8760ccd/Agent_autonomy.png",
      "popularity_score": 2017.7310236111111,
      "ai_summary": [
        "Smart LED light bulbs offer easy integration into the IoT space.",
        "They allow control of home lighting via phone and connected devices.",
        "Color-changing bulbs provide various RGB options for customization.",
        "Many bulbs work with Amazon Alexa and Google Assistant.",
        "Affordable and advanced options are available for different budgets."
      ]
    },
    {
      "id": "cluster_0",
      "coverage": 1,
      "updated_at": "2025-10-13T11:16:42.916Z",
      "title": "“Like putting on glasses for the first time”—how AI improves earthquake detection",
      "neutral_headline": "“Like putting on glasses for the first time”—how AI improves earthquake detection",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2025/10/like-putting-on-glasses-for-the-first-time-how-ai-improves-earthquake-detection/",
          "published_at": "2025-10-13T11:16:42.916Z",
          "title": "“Like putting on glasses for the first time”—how AI improves earthquake detection",
          "standfirst": "",
          "content": "",
          "feed_position": 0
        }
      ],
      "popularity_score": 367.99961138888887,
      "ai_summary": [
        "AI significantly enhances earthquake detection capabilities.",
        "The technology provides improved clarity in seismic data analysis.",
        "AI helps to identify and analyze earthquake patterns more effectively.",
        "The advancements are comparable to improved vision.",
        "The technology is revolutionizing earthquake monitoring."
      ]
    },
    {
      "id": "cluster_1",
      "coverage": 1,
      "updated_at": "2025-10-13T11:16:42.916Z",
      "title": "Apple ups the reward for finding major exploits to $2 million",
      "neutral_headline": "Apple ups the reward for finding major exploits to $2 million",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/security/2025/10/apple-ups-the-reward-for-finding-major-exploits-to-2-million/",
          "published_at": "2025-10-13T11:16:42.916Z",
          "title": "Apple ups the reward for finding major exploits to $2 million",
          "standfirst": "",
          "content": "",
          "feed_position": 1
        }
      ],
      "popularity_score": 354.99961138888887,
      "ai_summary": [
        "Apple has increased its bug bounty program rewards.",
        "The maximum reward for finding exploits is now $2 million.",
        "The program incentivizes security researchers.",
        "The increase aims to improve Apple product security.",
        "The reward applies to critical vulnerabilities."
      ]
    },
    {
      "id": "cluster_32",
      "coverage": 1,
      "updated_at": "Sun, 12 Oct 2025 20:50:35 +0000",
      "title": "New Starfleet Academy trailer debuts at NYCC",
      "neutral_headline": "New Starfleet Academy trailer debuts at NYCC: Also: our first look at S4 of Star Trek: Strange New Worlds",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/culture/2025/10/new-starfleet-academy-trailer-debuts-at-nycc/",
          "published_at": "Sun, 12 Oct 2025 20:50:35 +0000",
          "title": "New Starfleet Academy trailer debuts at NYCC",
          "standfirst": "Also: our first look at S4 of Star Trek: Strange New Worlds",
          "content": "The Star Trek universe panel at New York Comic Con (NYCC) this weekend concluded with a brand new trailer for Star Trek: Starfleet Academy. The ten-episode series will follow the exploits of the first new crop of cadets in a century. Per the official premise: Star Trek: Starfleet Academy introduces viewers to a young group of cadets who come together to pursue a common dream of hope and optimism. Under the watchful and demanding eyes of their instructors, they discover what it takes to become Starfleet officers as they navigate blossoming friendships, explosive rivalries, first loves and a new enemy that threatens both the Academy and the Federation itself. The new cadets include Sandro Rosta as human orphan Caleb Mir; Karim Diané as a Klingon cadet, Jay-Den Kraag; Kerrice Brooks as Sam (Series Acclimation Mil), the first Kasquain to attend the Academy; George Hawkins as Darem Reymi, a Khionian cadet who wants to be a captain; Bella Shepard as Genesis Lythe, a Dar-Sha cadet; and Zoë Steiner as Tarima Sadal, daughter of the president of Betazed. In addition, Holly Hunter plays Academy chancellor Captain Nahla Ake; Gina Yashere plays Cadet Master Lura Thok; Becky Lynch plays a member of Starfleet bridge crew; and Paul Giamatti plays a half-Klingon, half-Tellarite character named Nus Braka, the series' chief villain.Read full article Comments",
          "feed_position": 0,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/academy1-1152x648-1760302457.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/academy1-1152x648-1760302457.jpg",
      "popularity_score": 350.56407916666666,
      "ai_summary": [
        "A trailer for Starfleet Academy was unveiled at NYCC.",
        "The event also featured a first look at Star Trek: Strange New Worlds Season 4.",
        "The trailer provided a preview of the upcoming Starfleet Academy series.",
        "The new season of Strange New Worlds was also previewed.",
        "The announcements excited Star Trek fans."
      ]
    },
    {
      "id": "cluster_2",
      "coverage": 1,
      "updated_at": "2025-10-13T11:16:42.916Z",
      "title": "How close are we to solid state batteries for electric vehicles?",
      "neutral_headline": "How close are we to solid state batteries for electric vehicles?",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/cars/2025/10/how-close-are-we-to-solid-state-batteries-for-electric-vehicles/",
          "published_at": "2025-10-13T11:16:42.916Z",
          "title": "How close are we to solid state batteries for electric vehicles?",
          "standfirst": "",
          "content": "",
          "feed_position": 2
        }
      ],
      "popularity_score": 338.99961138888887,
      "ai_summary": [
        "Solid-state batteries are a key technology for electric vehicles.",
        "The technology promises improved energy density and safety.",
        "Research and development are ongoing to bring them to market.",
        "The timeline for widespread adoption remains uncertain.",
        "Solid-state batteries could revolutionize EV performance."
      ]
    },
    {
      "id": "cluster_74",
      "coverage": 1,
      "updated_at": "Sat, 11 Oct 2025 12:00:05 +0000",
      "title": "Why doesn’t Cards Against Humanity print its game in the US? It’s complicated.",
      "neutral_headline": "Why doesn’t Cards Against Humanity print its game in the US? It’s complicated",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/culture/2025/10/why-doesnt-cards-against-humanity-print-its-game-in-the-us-its-complicated/",
          "published_at": "Sat, 11 Oct 2025 12:00:05 +0000",
          "title": "Why doesn’t Cards Against Humanity print its game in the US? It’s complicated.",
          "standfirst": "Price, quality, speed, and relationships all matter.",
          "content": "Cards Against Humanity (CAH) this week announced its newest stunt: a \"Cards Against Humanity Explains the Joke\" edition that ditches the game's rules and adds explanatory notes to each card in the box. This makes the project \"informational material\" rather than a \"game,\" and therefore CAH can avoid import tariffs. All profits from the one-off project will be donated to the American Library Association to fight censorship. While a clever way to stick it to Trump, this week's news did raise a question I've heard from several readers: If CAH is this upset about the whiplash-inducing tariff rates, which are added and then removed with almost no warning, why doesn't it print the game in the US? I mean, it's just a box of cards! How hard can it be? In the board game space, designers have wrestled with this question for years. While many US-based designers would like to work with local manufacturers, in reality, it's often not possible. Complex board games today may feature cardboard creations like constructible dice towers, custom-shaped and painted wooden markers, multicolored jewel pieces, plastic bits of nearly every possible variety, custom-printed component bags, molded miniatures, cards in multiple sizes, metallic coins, dry-erase boards, fancy box inserts, massive dual-sided playing boards, and long manuals. The only manufacturers capable of doing all this work are generally in China or central Europe (Germany still has good manufacturing, and there are also sites in Poland and the Czech Republic).Read full article Comments",
          "feed_position": 1,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/2f017ee4c36a46e81f80d56890b7b64fc257272d-1400x1260-1-1152x648-1760129115.png"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/2f017ee4c36a46e81f80d56890b7b64fc257272d-1400x1260-1-1152x648-1760129115.png",
      "popularity_score": 338,
      "ai_summary": [
        "Cards Against Humanity does not print its game in the United States.",
        "Price, quality, speed, and relationships influence the decision.",
        "The decision involves complex considerations.",
        "The company prioritizes various factors in its printing process.",
        "The printing location impacts the game's production."
      ]
    },
    {
      "id": "cluster_3",
      "coverage": 1,
      "updated_at": "2025-10-13T11:16:42.916Z",
      "title": "Trump admin fires more health employees amid government shutdown",
      "neutral_headline": "Trump admin fires more health employees amid government shutdown",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/health/2025/10/more-federal-health-employees-axed-amid-shutdown-linked-terminations/",
          "published_at": "2025-10-13T11:16:42.916Z",
          "title": "Trump admin fires more health employees amid government shutdown",
          "standfirst": "",
          "content": "",
          "feed_position": 3
        }
      ],
      "popularity_score": 322.99961138888887,
      "ai_summary": [
        "The Trump administration dismissed more health employees.",
        "The firings occurred during a government shutdown.",
        "The actions impacted the health sector workforce.",
        "The government shutdown was a contributing factor.",
        "The firings had implications for healthcare operations."
      ]
    },
    {
      "id": "cluster_4",
      "coverage": 1,
      "updated_at": "2025-10-13T11:16:42.916Z",
      "title": "Putin OKs plan to turn Russian spacecraft into flying billboards",
      "neutral_headline": "Putin OKs plan to turn Russian spacecraft into flying billboards",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2025/10/putin-oks-plan-to-turn-russian-spacecraft-into-flying-billboards/",
          "published_at": "2025-10-13T11:16:42.916Z",
          "title": "Putin OKs plan to turn Russian spacecraft into flying billboards",
          "standfirst": "",
          "content": "",
          "feed_position": 4
        }
      ],
      "popularity_score": 320.99961138888887,
      "ai_summary": [
        "Putin approved a plan to use Russian spacecraft for advertising.",
        "The spacecraft will be utilized as flying billboards.",
        "The initiative aims to generate revenue.",
        "The plan involves commercializing space assets.",
        "The project will change the appearance of spacecraft."
      ]
    },
    {
      "id": "cluster_5",
      "coverage": 1,
      "updated_at": "2025-10-13T11:16:42.916Z",
      "title": "People regret buying Amazon smart displays after being bombarded with ads",
      "neutral_headline": "Amazon Smart Displays Draw Regret Due to Advertising",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2025/10/people-regret-buying-amazon-smart-displays-after-being-bombarded-with-ads/",
          "published_at": "2025-10-13T11:16:42.916Z",
          "title": "People regret buying Amazon smart displays after being bombarded with ads",
          "standfirst": "",
          "content": "",
          "feed_position": 5
        }
      ],
      "popularity_score": 302.99961138888887,
      "ai_summary": [
        "Many Amazon smart display users express regret after being bombarded with advertisements.",
        "Users report feeling overwhelmed by the frequency and intrusiveness of the ads.",
        "The advertising experience has negatively impacted user satisfaction with the devices.",
        "Some users are considering switching to alternative smart display options.",
        "The issue highlights concerns about the balance between advertising and user experience."
      ]
    },
    {
      "id": "cluster_6",
      "coverage": 1,
      "updated_at": "2025-10-13T11:16:42.916Z",
      "title": "“Extremely angry” Trump threatens “massive” tariff on all Chinese exports",
      "neutral_headline": "Trump Threatens Tariffs on Chinese Exports",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/10/extremely-angry-trump-threatens-massive-tariff-on-all-chinese-exports/",
          "published_at": "2025-10-13T11:16:42.916Z",
          "title": "“Extremely angry” Trump threatens “massive” tariff on all Chinese exports",
          "standfirst": "",
          "content": "",
          "feed_position": 6
        }
      ],
      "popularity_score": 292.99961138888887,
      "ai_summary": [
        "Former President Trump threatened \"massive\" tariffs on all Chinese exports.",
        "The threat was made in response to unspecified actions by China.",
        "The potential tariffs could significantly impact international trade.",
        "The announcement has raised concerns about a trade war.",
        "The situation is developing and could lead to further economic consequences."
      ]
    },
    {
      "id": "cluster_7",
      "coverage": 1,
      "updated_at": "2025-10-13T11:16:42.916Z",
      "title": "Bose SoundTouch home theater systems regress into dumb speakers Feb. 18",
      "neutral_headline": "Bose SoundTouch Systems Lose Features on February 18",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2025/10/bose-soundtouch-home-theater-systems-regress-into-dumb-speakers-feb-18/",
          "published_at": "2025-10-13T11:16:42.916Z",
          "title": "Bose SoundTouch home theater systems regress into dumb speakers Feb. 18",
          "standfirst": "",
          "content": "",
          "feed_position": 7
        }
      ],
      "popularity_score": 282.99961138888887,
      "ai_summary": [
        "Bose SoundTouch home theater systems will lose functionality on February 18.",
        "The systems will regress into basic speaker functionality.",
        "Users will no longer have access to certain streaming services.",
        "The change is due to the end of support for specific features.",
        "Bose has not announced any plans to restore the lost features."
      ]
    },
    {
      "id": "cluster_9",
      "coverage": 1,
      "updated_at": "2025-10-13T11:16:42.916Z",
      "title": "Microsoft warns of new “Payroll Pirate” scam stealing employees’ direct deposits",
      "neutral_headline": "Microsoft Warns of Payroll Pirate Scam",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/security/2025/10/payroll-pirate-phishing-scam-that-takes-over-workday-accounts-steals-paychecks/",
          "published_at": "2025-10-13T11:16:42.916Z",
          "title": "Microsoft warns of new “Payroll Pirate” scam stealing employees’ direct deposits",
          "standfirst": "",
          "content": "",
          "feed_position": 9
        }
      ],
      "popularity_score": 274.99961138888887,
      "ai_summary": [
        "Microsoft warns of a new \"Payroll Pirate\" scam targeting employees.",
        "The scam involves stealing employees' direct deposit information.",
        "Attackers are using phishing and social engineering tactics.",
        "The goal is to redirect payroll funds to fraudulent accounts.",
        "Microsoft advises users to be vigilant and verify all requests."
      ]
    },
    {
      "id": "cluster_8",
      "coverage": 1,
      "updated_at": "2025-10-13T11:16:42.916Z",
      "title": "We’re about to find many more interstellar interlopers—here’s how to visit one",
      "neutral_headline": "Interstellar Interlopers May Be Visited Soon",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/features/2025/10/were-about-to-find-many-more-interstellar-interlopers-heres-how-to-visit-one/",
          "published_at": "2025-10-13T11:16:42.916Z",
          "title": "We’re about to find many more interstellar interlopers—here’s how to visit one",
          "standfirst": "",
          "content": "",
          "feed_position": 8
        }
      ],
      "popularity_score": 272.99961138888887,
      "ai_summary": [
        "Scientists anticipate finding more interstellar objects in the future.",
        "The article discusses the possibility of visiting these objects.",
        "It explores the methods and challenges of interstellar travel.",
        "The focus is on the potential for scientific exploration.",
        "The article highlights the advancements in space technology."
      ]
    }
  ]
}