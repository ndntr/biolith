{
  "updated_at": "2025-10-17T23:16:39.725Z",
  "clusters": [
    {
      "id": "cluster_2",
      "coverage": 2,
      "updated_at": "Fri, 17 Oct 2025 22:31:00 GMT",
      "title": "Developers can now add live Google Maps data to Gemini-powered AI app outputs",
      "neutral_headline": "Developers can now add live Google Maps data to Gemini-powered AI app outputs",
      "items": [
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/developers-can-now-add-live-google-maps-data-to-gemini-powered-ai-app",
          "published_at": "Fri, 17 Oct 2025 22:31:00 GMT",
          "title": "Developers can now add live Google Maps data to Gemini-powered AI app outputs",
          "standfirst": "Google is adding a new feature for third-party developers building atop its Gemini AI models that rivals like OpenAI&#x27;s ChatGPT, Anthropic&#x27;s Claude, and the growing array of Chinese open source options are unlikely to get anytime soon: grounding with Google Maps.This addition allows developers to connect Google&#x27;s Gemini AI models&#x27; reasoning capabilities with live geospatial data from Google Maps, enabling applications to deliver detailed, location-relevant responses to user queries—such as business hours, reviews, or the atmosphere of a specific venue. By tapping into data from over 250 million places, developers can now build more intelligent and responsive location-aware experiences.This is particularly useful for applications where proximity, real-time availability, or location-specific personalization matter—such as local search, delivery services, real estate, and travel planning. When the user’s location is known, developers can pass latitude and longitude into the request to enhance the response quality.By tightly integrating real-time and historical Maps data into the Gemini API, Google enables applications to generate grounded, location-specific responses with factual accuracy and contextual depth that are uniquely possible through its mapping infrastructure.Merging AI and Geospatial IntelligenceThe new feature is accessible in Google AI Studio, where developers can try a live demo powered by the Gemini Live API. Models that support the grounding with Google Maps include:Gemini 2.5 ProGemini 2.5 FlashGemini 2.5 Flash-LiteGemini 2.0 FlashIn one demonstration, a user asked for Italian restaurant recommendations in Chicago. The assistant, leveraging Maps data, retrieved top-rated options and clarified a misspelled restaurant name before locating the correct venue with accurate business details.Developers can also retrieve a context token to embed a Google Maps widget in their app’s user interface. This interactive component displays photos, reviews, and other familiar content typically found in Google Maps.Integration is handled via the generateContent method in the Gemini API, where developers include googleMaps as a tool. They can also enable a Maps widget by setting a parameter in the request. The widget, rendered using a returned context token, can provide a visual layer alongside the AI-generated text.Use Cases Across IndustriesThe Maps grounding tool is designed to support a wide range of practical use cases:Itinerary generation: Travel apps can create detailed daily plans with routing, timing, and venue information.Personalized local recommendations: Real estate platforms can highlight listings near kid-friendly amenities like schools and parks.Detailed location queries: Applications can provide specific information, such as whether a cafe offers outdoor seating, using community reviews and Maps metadata.Developers are encouraged to only enable the tool when geographic context is relevant, to optimize both performance and cost. According to the developer documentation, pricing starts at $25 per 1,000 grounded prompts — a steep sum for those trafficking in numerous queries.Combining Search and Maps for Enhanced ContextDevelopers can use Grounding with Google Maps alongside Grounding with Google Search in the same request.While the Maps tool contributes factual data—like addresses, hours, and ratings—the Search tool adds broader context from web content, such as news or event listings.For example, when asked about live music on Beale Street, the combined tools provide venue details from Maps and event times from Search. According to Google, internal testing shows that using both tools together leads to significantly improved response quality.Customization and Developer FlexibilityThe experience is built for customization. Developers can tweak system prompts, choose from different Gemini models, and configure voice settings to tailor interactions. The demo app in Google AI Studio is also remixable, enabling developers to test ideas, add features, and iterate on designs within a flexible development environment.The API returns structured metadata—including source links, place IDs, and citation spans—that developers can use to build inline citations or verify the AI-generated outputs. This supports transparency and enhances trust in user-facing applications. Google also requires that Maps-based sources be attributed clearly and linked back to the source using their URI.Implementation Considerations for AI BuildersFor technical teams integrating this capability, Google recommends:Passing user location context when known, for better results.Displaying Google Maps source links directly beneath the relevant content.Only enabling the tool when the query clearly involves geographic context.Monitoring latency and disabling grounding when performance is critical.Grounding with Google Maps is currently available globally, though prohibited in several territories (including China, Iran, North Korea, and Cuba), and not permitted for emergency response use cases.Availability and AccessGrounding with Google Maps is now generally available through the Gemini API. With this release, Google continues to expand the capabilities of the Gemini API, empowering developers to build AI-driven applications that understand and respond to the world around them.",
          "content": "Google is adding a new feature for third-party developers building atop its Gemini AI models that rivals like OpenAI&#x27;s ChatGPT, Anthropic&#x27;s Claude, and the growing array of Chinese open source options are unlikely to get anytime soon: grounding with Google Maps.This addition allows developers to connect Google&#x27;s Gemini AI models&#x27; reasoning capabilities with live geospatial data from Google Maps, enabling applications to deliver detailed, location-relevant responses to user queries—such as business hours, reviews, or the atmosphere of a specific venue. By tapping into data from over 250 million places, developers can now build more intelligent and responsive location-aware experiences.This is particularly useful for applications where proximity, real-time availability, or location-specific personalization matter—such as local search, delivery services, real estate, and travel planning. When the user’s location is known, developers can pass latitude and longitude into the request to enhance the response quality.By tightly integrating real-time and historical Maps data into the Gemini API, Google enables applications to generate grounded, location-specific responses with factual accuracy and contextual depth that are uniquely possible through its mapping infrastructure.Merging AI and Geospatial IntelligenceThe new feature is accessible in Google AI Studio, where developers can try a live demo powered by the Gemini Live API. Models that support the grounding with Google Maps include:Gemini 2.5 ProGemini 2.5 FlashGemini 2.5 Flash-LiteGemini 2.0 FlashIn one demonstration, a user asked for Italian restaurant recommendations in Chicago. The assistant, leveraging Maps data, retrieved top-rated options and clarified a misspelled restaurant name before locating the correct venue with accurate business details.Developers can also retrieve a context token to embed a Google Maps widget in their app’s user interface. This interactive component displays photos, reviews, and other familiar content typically found in Google Maps.Integration is handled via the generateContent method in the Gemini API, where developers include googleMaps as a tool. They can also enable a Maps widget by setting a parameter in the request. The widget, rendered using a returned context token, can provide a visual layer alongside the AI-generated text.Use Cases Across IndustriesThe Maps grounding tool is designed to support a wide range of practical use cases:Itinerary generation: Travel apps can create detailed daily plans with routing, timing, and venue information.Personalized local recommendations: Real estate platforms can highlight listings near kid-friendly amenities like schools and parks.Detailed location queries: Applications can provide specific information, such as whether a cafe offers outdoor seating, using community reviews and Maps metadata.Developers are encouraged to only enable the tool when geographic context is relevant, to optimize both performance and cost. According to the developer documentation, pricing starts at $25 per 1,000 grounded prompts — a steep sum for those trafficking in numerous queries.Combining Search and Maps for Enhanced ContextDevelopers can use Grounding with Google Maps alongside Grounding with Google Search in the same request.While the Maps tool contributes factual data—like addresses, hours, and ratings—the Search tool adds broader context from web content, such as news or event listings.For example, when asked about live music on Beale Street, the combined tools provide venue details from Maps and event times from Search. According to Google, internal testing shows that using both tools together leads to significantly improved response quality.Customization and Developer FlexibilityThe experience is built for customization. Developers can tweak system prompts, choose from different Gemini models, and configure voice settings to tailor interactions. The demo app in Google AI Studio is also remixable, enabling developers to test ideas, add features, and iterate on designs within a flexible development environment.The API returns structured metadata—including source links, place IDs, and citation spans—that developers can use to build inline citations or verify the AI-generated outputs. This supports transparency and enhances trust in user-facing applications. Google also requires that Maps-based sources be attributed clearly and linked back to the source using their URI.Implementation Considerations for AI BuildersFor technical teams integrating this capability, Google recommends:Passing user location context when known, for better results.Displaying Google Maps source links directly beneath the relevant content.Only enabling the tool when the query clearly involves geographic context.Monitoring latency and disabling grounding when performance is critical.Grounding with Google Maps is currently available globally, though prohibited in several territories (including China, Iran, North Korea, and Cuba), and not permitted for emergency response use cases.Availability and AccessGrounding with Google Maps is now generally available through the Gemini API. With this release, Google continues to expand the capabilities of the Gemini API, empowering developers to build AI-driven applications that understand and respond to the world around them.",
          "feed_position": 0,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/2ehvkDsQ7NC7wQuECOcDGS/cb89da9ccaf13f2888f005ac93cecffa/cfr0z3n_realistic_graphic_novel_hyper_detailed_flat_illustratio_8aae43d5-1e50-4ebd-8760-ea28dbc0f328.png"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/wearables/meta-ray-ban-display-review-chunky-frames-with-impressive-abilities-193127070.html",
          "published_at": "Fri, 17 Oct 2025 19:31:27 +0000",
          "title": "Meta Ray-Ban Display review: Chunky frames with impressive abilities",
          "standfirst": "I've been wearing the $800 Meta Ray-Ban Display glasses daily for ten days and I'm still a bit conflicted. On one hand, I'm still not entirely comfortable with how they look. I've worn them on the bus, at the office, on walks around my neighborhood and during hangouts with friends. Each time, I'm very aware that I probably look a bit strange. On the other hand, there's a lot I really like about using these glasses. The built-in display has helped me look at my phone less throughout the day. The neural band feels more innovative than any wrist-based device I've tried. Together, it feels like a significant milestone for smart glasses overall. But it's also very much a first-generation device with some issues that still need to be worked out. Chunky statement glasses or hideously nerdy? To once again state the obvious: The frames are extremely chunky and too wide for my face. The dark black frames I tried for this review unfortunately accentuate the extra thickness. I won't pretend it's my best look and I did feel a bit self-conscious at times wearing these in public. Meta also makes a light brown \"sand\" color that I tried at the Connect event, and I think that color is a bit more flattering, even if the frames are just as oversized. (Sidenote: Smart glasses companies, please, please make your frames available in something other than black!) But, everyone has a different face shape, skin tone and general ability to \"pull off\" what one of my friends charitably described as \"chunky statement glasses.\" What looks not-great on my face, may look good on someone else. I really wish Meta could have squeezed this tech into slightly smaller frames, but I did get more used to the look the more I wore them. Overall, I do think the size is a reasonable tradeoff for a first-generation product that's pretty clearly aimed at early adopters. Here's how they look in the lighter \"sand\" color. Karissa Bell for Engadget The reason the glasses are so thick compared with Meta's other frames is because there are a lot of extra components to power the display, including a mini projector and waveguide. And, at 69 grams, the display glasses are noticeably heavier. I didn't find it particularly uncomfortable at first, but there is a noticeable pressure after six or seven hours of wear. Plus, the extra weight and width also made them consistently slide down my nose. I'm not sure I'd feel comfortable wearing these on a bike ride or a jog as I'd worry about them falling off. While I tested these, I was very interested to get reactions from friends and family. I didn't get many positive comments about how they looked on my face, though a few particularly generous colleagues assured me I was \"pulling them off.\" But seeing people's reactions as soon as the display activated was another matter. Almost everyone has had the same initial reaction: \"whoa.\" Quality display with some limitations As I discussed in my initial impressions, these glasses have a monocular display on the right side, so it doesn't offer the kind of immersive AR I experienced with the Orion prototype last year. You have to look slightly up and to the right to focus on the full-color display. It's impressively bright and clear, but doesn't overtake your vision. At 20 degrees, the field of view is small, but it never felt like a limitation. Because the content you see isn't meant to be immersive, it never feels like what's on the display is being cut off or like you have to adjust where you're looking to properly see it. The display itself has three main menus: an app launcher, a kind of home screen where you can access Meta AI and view notifications and a settings page for adjusting brightness, volume and other preferences. The display is in the right lens. Karissa Bell for Engadget For now, there are only a handful of Meta-created \"apps\" available. You can check your Instagram, WhatsApp and Messenger inboxes and chat with Meta AI. There's also a simple maps app for walking navigation, a music/audio player, camera and live translation and captioning features. There's also a mini puzzle game called \"Hypertrail.\" One of my favorite integrations was the ability to check Instagram DMs. Not only can you quickly read and respond to messages, you can watch Reels sent by your friends. While the video quality isn't as high as what you'd see on your phone, there's something very cool about quickly watching a clip without having to pull out your phone. Meta is also working on a standalone Reels experience that I'm very much looking forward to. I also enjoyed being able to view media sent in my family group chats on WhatsApp. I often would end up revisiting the photos on videos once I pulled out my phone, but being able to instantly see these messages as they came in tickled whatever part of my brain responds to instant gratification. There's some impressive tech inside those thick frames. Karissa Bell for Engadget The display also solves one of my biggest complaints with Meta's other smart glasses: that it's really difficult to frame photos. When you open the camera app on the display model, you can see a preview of the photo and even use a gesture to zoom in to properly frame your shot. Similarly, if you're on a WhatsApp video call you can see both the other person's video as well as a small preview of your own like you would on your phone's screen. It's a cool trick but the small display felt too cramped for a proper video call. People I used this with also told me that my video feed had some quality issues despite being on Wi-Fi. The glasses' live captioning and translation features are probably the best examples of Meta bringing its existing AI features into the display. I've written before about how Meta AI's translation abilities are one of my favorite features of the Ray-Ban smart glasses. Live translation on the display is even better, because it delivers a real-time text feed of what the person in front of you is saying. I tried it out with my husband, a native Spanish speaker, and it was even more natural than the non-display glasses because I didn't have to pause and wait for the audio to relay what he was saying. It still wasn't an exactly perfect translation, and there were still a few occasions when it didn't catch everything he said, but it made the process so much simpler overall. Likewise, live captions transcribes conversations in real-time into a similar text feed. I've found that it's a cool way to demo these glasses' capabilities, but I haven't yet found an occasion to use this in anything other than a demo. However, I still think it could be useful as an accessibility aid for anyone who has trouble hearing or processing audio. Another feature that's useful for travel is walking navigation. Dictate an address or location (you can say something like \"take me to the closest Starbucks\") and the glasses' display will guide you on your route. The first time I tried this was the roughly 10-minute walk from my bus stop to Yahoo's San Francisco office. The route only required two turns, but it didn't quite work. My glasses confidently navigated me to an alleyway behind the office building rather than the entrance. These kinds of mishaps happen with lots of mapping tools — Meta's maps rely on data from OpenStreetMap and Overture — but it was a good reminder that it's still early days for this product. I don't use Meta AI a ton on any of my smart glasses, but having a bit of visual feedback for these interactions was a nice change. I retain information much better from reading than listening, so seeing text-based output to my queries felt a lot more helpful. It's also nice that for longer responses from the assistant, you can stop the audio playback and swipe through informational cards instead. Meta AI on the glasses' display delivers information in a card-like interface. Meta While cooking dinner one night, I asked for a quick recipe for teriyaki salmon and Meta AI supplied what seemed like a passable recipe onto the display. The only drawback was the display goes to sleep pretty quickly unless you continue to interact with the content you're seeing, so the recipe I liked disappeared before I could actually attempt it. (You can view your Meta AI history in the Meta AI app if you really want to revisit something.) My main complaint is that I want to be able to do much more with the display. Messaging app integrations are nice, but I wish the display worked with more of the apps on my phone. When it worked best, I was happy to be able to view and dismiss messaging notifications without having to touch my phone; I just wish it worked with all my phone's notifications. There are also some frustrating limitations on sending and receiving texts. For example, there's no simple way to take a photo on your glasses and text it to a friend with the glasses. You have to wait for the glasses to send a \"preview\" of your message to your phone and then manually send the text. Or, you can opt in to Meta's cloud services and send the photo immediately as a link, but I'm not sure many of my friends would readily open a \"media.meta.com\" URL. The glasses also don't really support non-WhatsApp group chats, at least on iOS. You can receive messages sent in group chats, but there's no indication the message originated in a group thread. And, it's impossible to reply in the same thread; instead, replies are sent directly to the person who texted, which can get confusing if you're not checking your phone. It was also a little annoying that reading and even replying to texts from my glasses wouldn't mark the text as read in my phone's inbox. Meta blames all this on Apple's iOS restrictions, and says it's hoping to work with the company to improve the experience. The company tells me that group messaging should work normally for people with Android devices and that there is also a dedicated inbox for checking texts on the glasses. I haven’t tested this out yet. The band + battery life The glasses are controlled using Meta's Neural Band, which can translate subtle gestures like finger taps into actions on the display. Because the band relies on electromyography (EMG), you do need a fairly snug fit for it to work properly. I didn't find it uncomfortable, but, like the glasses, I don't love how it looks as a daily accessory. It also requires daily charging if you wear the glasses all day. But the band does work surprisingly well. In more than a week, it almost never missed a gesture, and it never falsely registered a gesture, despite my efforts to confuse it by fidgeting or rubbing my fingers together. The gestures themselves are also pretty intuitive and don't take long to get used to: double tapping your thumb and middle fingers wakes up or puts the display to sleep, single taps of your index and middle fingers allow you to select an item or go back, and swiping your thumb along the side of your index finger lets you navigate around the display. There are a few others, but those are the ones I used most often. The Meta Neural Band requires a snug fit to work properly. Karissa Bell for Engadget Each time you make a gesture, the band emits a small vibration so you get a bit of haptic feedback letting you know it registered. I've used hand tracking-based navigation in various VR, AR and mixed reality devices and I've always felt a bit goofy waving my hands around. But the neural band gestures work when your hand is by your side or in your pocket. The other major drawback of these glasses is that heavy use of the display drains the battery pretty quickly. Meta says the Ray-Ban Display’s battery can go about six hours on a single charge, but it really depends on how much you're using the display. With very limited use, l was able to stretch the battery to about seven hours, but if you're doing display-intensive tasks like video calling or live translation, it will die much, much more quickly. The Meta Ray-Ban Display glasses, charging case and neural band. Karissa Bell for Engadget The glasses do come with a charging case that can deliver a few extra charges on-the-go, but I was a bit surprised at how often I had to recharge the case. With my normal Ray-Ban Meta glasses I can go several days without topping up the charging case, but with the Meta Ray-Ban Display case, I'm charging it at least every other day. Privacy and safety Whenever I write or post on social media about a pair of Meta-branded glasses, I inevitably hear from people concerned about the privacy implications of these devices. As I wrote in my recent review of Meta's second-gen Ray-Ban glasses, I share a lot of these concerns. Meta has made subtle but meaningful changes to its glasses' privacy policy over the last year, and its track record suggests these devices will inevitably scoop up more of our data over time. In terms of privacy implications of the display-enabled glasses, there isn't a meaningful difference compared to their counterparts. Meta's policies are the same for all its wearables. I suppose you could use live translation to surreptitiously eavesdrop on a conversation you wouldn't typically understand, though that's technically possible with Meta's other glasses too. And the addition of a wrist-based controller means taking photos is a bit less obvious, but there's still an LED indicator that lights up when the camera is on. The neural band allows you to snap photos without touching the capture button or using a voice command. Karissa Bell for Engadget I have been surprised at how many people have asked me if these glasses have some kind of facial recognition abilities. I'm not sure if that's a sign of people's general distrust of Meta, or an assumption based on seeing similar glasses in sci-fi flicks, but I do think it's telling. (They don't, to be clear. Meta currently only uses facial recognition for two safety-related features on Facebook and Instagram.) Meta hasn't done much to earn people's trust when it comes to privacy, and I wish the company would use its growing wearables business to try to prove otherwise. On a more practical level, I have some safety concerns. The display didn't hinder my situational awareness while walking, but I could see how it might for others. And I'm definitely not comfortable using the display while driving. Meta does have an audio-only \"driving detection\" setting that can automatically kick in when you're traveling in a car, but the feature is optional, which seems potentially problematic. Should you buy these? In short: probably not. As much as I've been genuinely impressed with Meta's display tech, I don't think these glasses make sense for most people right now. And, at $800, the Meta Ray-Ban Display glasses are more than twice as much as the company's very good second-generation Ray-Ban glasses, which come in a wide range of much more normal-looking frame styles and colors. The Meta Ray-Ban Display glasses, on the other hand, still look very much like a first-gen product. There are some really compelling use cases for the display, but its functionality is limited. The glasses are also too thick and bulky for what's meant to be an everyday accessory. At the end of the day, most people want glasses that make them look good. There’s also the fact that right now, these glasses are somewhat difficult to actually buy. They are only available at a handful of physical retailers, which currently have a very limited supply, Meta is also requiring would-be buyers to schedule demo appointments in order to buy, though some stores — like the LensCrafters where I bought my pair — aren’t enforcing this. Still, there's a lot to be excited about. Watching people's reactions to trying these has been almost as much fun as using them myself. Meta also has a solid lineup of new features already in the works, including a standalone Reels app, a teleprompter and gesture-based handwriting for message replies. If you're already all-in on smart glasses or, like me, you've been patiently waiting for glasses with a high quality, usable display, then the Meta Ray-Ban Display glasses are worth the investment now — as long as you can accept the thick frames. Update, October 17, 2025, 3:42PM PT: Added more information about group text functionality on Android. This article originally appeared on Engadget at https://www.engadget.com/wearables/meta-ray-ban-display-review-chunky-frames-with-impressive-abilities-193127070.html?src=rss",
          "content": "I've been wearing the $800 Meta Ray-Ban Display glasses daily for ten days and I'm still a bit conflicted. On one hand, I'm still not entirely comfortable with how they look. I've worn them on the bus, at the office, on walks around my neighborhood and during hangouts with friends. Each time, I'm very aware that I probably look a bit strange. On the other hand, there's a lot I really like about using these glasses. The built-in display has helped me look at my phone less throughout the day. The neural band feels more innovative than any wrist-based device I've tried. Together, it feels like a significant milestone for smart glasses overall. But it's also very much a first-generation device with some issues that still need to be worked out. Chunky statement glasses or hideously nerdy? To once again state the obvious: The frames are extremely chunky and too wide for my face. The dark black frames I tried for this review unfortunately accentuate the extra thickness. I won't pretend it's my best look and I did feel a bit self-conscious at times wearing these in public. Meta also makes a light brown \"sand\" color that I tried at the Connect event, and I think that color is a bit more flattering, even if the frames are just as oversized. (Sidenote: Smart glasses companies, please, please make your frames available in something other than black!) But, everyone has a different face shape, skin tone and general ability to \"pull off\" what one of my friends charitably described as \"chunky statement glasses.\" What looks not-great on my face, may look good on someone else. I really wish Meta could have squeezed this tech into slightly smaller frames, but I did get more used to the look the more I wore them. Overall, I do think the size is a reasonable tradeoff for a first-generation product that's pretty clearly aimed at early adopters. Here's how they look in the lighter \"sand\" color. Karissa Bell for Engadget The reason the glasses are so thick compared with Meta's other frames is because there are a lot of extra components to power the display, including a mini projector and waveguide. And, at 69 grams, the display glasses are noticeably heavier. I didn't find it particularly uncomfortable at first, but there is a noticeable pressure after six or seven hours of wear. Plus, the extra weight and width also made them consistently slide down my nose. I'm not sure I'd feel comfortable wearing these on a bike ride or a jog as I'd worry about them falling off. While I tested these, I was very interested to get reactions from friends and family. I didn't get many positive comments about how they looked on my face, though a few particularly generous colleagues assured me I was \"pulling them off.\" But seeing people's reactions as soon as the display activated was another matter. Almost everyone has had the same initial reaction: \"whoa.\" Quality display with some limitations As I discussed in my initial impressions, these glasses have a monocular display on the right side, so it doesn't offer the kind of immersive AR I experienced with the Orion prototype last year. You have to look slightly up and to the right to focus on the full-color display. It's impressively bright and clear, but doesn't overtake your vision. At 20 degrees, the field of view is small, but it never felt like a limitation. Because the content you see isn't meant to be immersive, it never feels like what's on the display is being cut off or like you have to adjust where you're looking to properly see it. The display itself has three main menus: an app launcher, a kind of home screen where you can access Meta AI and view notifications and a settings page for adjusting brightness, volume and other preferences. The display is in the right lens. Karissa Bell for Engadget For now, there are only a handful of Meta-created \"apps\" available. You can check your Instagram, WhatsApp and Messenger inboxes and chat with Meta AI. There's also a simple maps app for walking navigation, a music/audio player, camera and live translation and captioning features. There's also a mini puzzle game called \"Hypertrail.\" One of my favorite integrations was the ability to check Instagram DMs. Not only can you quickly read and respond to messages, you can watch Reels sent by your friends. While the video quality isn't as high as what you'd see on your phone, there's something very cool about quickly watching a clip without having to pull out your phone. Meta is also working on a standalone Reels experience that I'm very much looking forward to. I also enjoyed being able to view media sent in my family group chats on WhatsApp. I often would end up revisiting the photos on videos once I pulled out my phone, but being able to instantly see these messages as they came in tickled whatever part of my brain responds to instant gratification. There's some impressive tech inside those thick frames. Karissa Bell for Engadget The display also solves one of my biggest complaints with Meta's other smart glasses: that it's really difficult to frame photos. When you open the camera app on the display model, you can see a preview of the photo and even use a gesture to zoom in to properly frame your shot. Similarly, if you're on a WhatsApp video call you can see both the other person's video as well as a small preview of your own like you would on your phone's screen. It's a cool trick but the small display felt too cramped for a proper video call. People I used this with also told me that my video feed had some quality issues despite being on Wi-Fi. The glasses' live captioning and translation features are probably the best examples of Meta bringing its existing AI features into the display. I've written before about how Meta AI's translation abilities are one of my favorite features of the Ray-Ban smart glasses. Live translation on the display is even better, because it delivers a real-time text feed of what the person in front of you is saying. I tried it out with my husband, a native Spanish speaker, and it was even more natural than the non-display glasses because I didn't have to pause and wait for the audio to relay what he was saying. It still wasn't an exactly perfect translation, and there were still a few occasions when it didn't catch everything he said, but it made the process so much simpler overall. Likewise, live captions transcribes conversations in real-time into a similar text feed. I've found that it's a cool way to demo these glasses' capabilities, but I haven't yet found an occasion to use this in anything other than a demo. However, I still think it could be useful as an accessibility aid for anyone who has trouble hearing or processing audio. Another feature that's useful for travel is walking navigation. Dictate an address or location (you can say something like \"take me to the closest Starbucks\") and the glasses' display will guide you on your route. The first time I tried this was the roughly 10-minute walk from my bus stop to Yahoo's San Francisco office. The route only required two turns, but it didn't quite work. My glasses confidently navigated me to an alleyway behind the office building rather than the entrance. These kinds of mishaps happen with lots of mapping tools — Meta's maps rely on data from OpenStreetMap and Overture — but it was a good reminder that it's still early days for this product. I don't use Meta AI a ton on any of my smart glasses, but having a bit of visual feedback for these interactions was a nice change. I retain information much better from reading than listening, so seeing text-based output to my queries felt a lot more helpful. It's also nice that for longer responses from the assistant, you can stop the audio playback and swipe through informational cards instead. Meta AI on the glasses' display delivers information in a card-like interface. Meta While cooking dinner one night, I asked for a quick recipe for teriyaki salmon and Meta AI supplied what seemed like a passable recipe onto the display. The only drawback was the display goes to sleep pretty quickly unless you continue to interact with the content you're seeing, so the recipe I liked disappeared before I could actually attempt it. (You can view your Meta AI history in the Meta AI app if you really want to revisit something.) My main complaint is that I want to be able to do much more with the display. Messaging app integrations are nice, but I wish the display worked with more of the apps on my phone. When it worked best, I was happy to be able to view and dismiss messaging notifications without having to touch my phone; I just wish it worked with all my phone's notifications. There are also some frustrating limitations on sending and receiving texts. For example, there's no simple way to take a photo on your glasses and text it to a friend with the glasses. You have to wait for the glasses to send a \"preview\" of your message to your phone and then manually send the text. Or, you can opt in to Meta's cloud services and send the photo immediately as a link, but I'm not sure many of my friends would readily open a \"media.meta.com\" URL. The glasses also don't really support non-WhatsApp group chats, at least on iOS. You can receive messages sent in group chats, but there's no indication the message originated in a group thread. And, it's impossible to reply in the same thread; instead, replies are sent directly to the person who texted, which can get confusing if you're not checking your phone. It was also a little annoying that reading and even replying to texts from my glasses wouldn't mark the text as read in my phone's inbox. Meta blames all this on Apple's iOS restrictions, and says it's hoping to work with the company to improve the experience. The company tells me that group messaging should work normally for people with Android devices and that there is also a dedicated inbox for checking texts on the glasses. I haven’t tested this out yet. The band + battery life The glasses are controlled using Meta's Neural Band, which can translate subtle gestures like finger taps into actions on the display. Because the band relies on electromyography (EMG), you do need a fairly snug fit for it to work properly. I didn't find it uncomfortable, but, like the glasses, I don't love how it looks as a daily accessory. It also requires daily charging if you wear the glasses all day. But the band does work surprisingly well. In more than a week, it almost never missed a gesture, and it never falsely registered a gesture, despite my efforts to confuse it by fidgeting or rubbing my fingers together. The gestures themselves are also pretty intuitive and don't take long to get used to: double tapping your thumb and middle fingers wakes up or puts the display to sleep, single taps of your index and middle fingers allow you to select an item or go back, and swiping your thumb along the side of your index finger lets you navigate around the display. There are a few others, but those are the ones I used most often. The Meta Neural Band requires a snug fit to work properly. Karissa Bell for Engadget Each time you make a gesture, the band emits a small vibration so you get a bit of haptic feedback letting you know it registered. I've used hand tracking-based navigation in various VR, AR and mixed reality devices and I've always felt a bit goofy waving my hands around. But the neural band gestures work when your hand is by your side or in your pocket. The other major drawback of these glasses is that heavy use of the display drains the battery pretty quickly. Meta says the Ray-Ban Display’s battery can go about six hours on a single charge, but it really depends on how much you're using the display. With very limited use, l was able to stretch the battery to about seven hours, but if you're doing display-intensive tasks like video calling or live translation, it will die much, much more quickly. The Meta Ray-Ban Display glasses, charging case and neural band. Karissa Bell for Engadget The glasses do come with a charging case that can deliver a few extra charges on-the-go, but I was a bit surprised at how often I had to recharge the case. With my normal Ray-Ban Meta glasses I can go several days without topping up the charging case, but with the Meta Ray-Ban Display case, I'm charging it at least every other day. Privacy and safety Whenever I write or post on social media about a pair of Meta-branded glasses, I inevitably hear from people concerned about the privacy implications of these devices. As I wrote in my recent review of Meta's second-gen Ray-Ban glasses, I share a lot of these concerns. Meta has made subtle but meaningful changes to its glasses' privacy policy over the last year, and its track record suggests these devices will inevitably scoop up more of our data over time. In terms of privacy implications of the display-enabled glasses, there isn't a meaningful difference compared to their counterparts. Meta's policies are the same for all its wearables. I suppose you could use live translation to surreptitiously eavesdrop on a conversation you wouldn't typically understand, though that's technically possible with Meta's other glasses too. And the addition of a wrist-based controller means taking photos is a bit less obvious, but there's still an LED indicator that lights up when the camera is on. The neural band allows you to snap photos without touching the capture button or using a voice command. Karissa Bell for Engadget I have been surprised at how many people have asked me if these glasses have some kind of facial recognition abilities. I'm not sure if that's a sign of people's general distrust of Meta, or an assumption based on seeing similar glasses in sci-fi flicks, but I do think it's telling. (They don't, to be clear. Meta currently only uses facial recognition for two safety-related features on Facebook and Instagram.) Meta hasn't done much to earn people's trust when it comes to privacy, and I wish the company would use its growing wearables business to try to prove otherwise. On a more practical level, I have some safety concerns. The display didn't hinder my situational awareness while walking, but I could see how it might for others. And I'm definitely not comfortable using the display while driving. Meta does have an audio-only \"driving detection\" setting that can automatically kick in when you're traveling in a car, but the feature is optional, which seems potentially problematic. Should you buy these? In short: probably not. As much as I've been genuinely impressed with Meta's display tech, I don't think these glasses make sense for most people right now. And, at $800, the Meta Ray-Ban Display glasses are more than twice as much as the company's very good second-generation Ray-Ban glasses, which come in a wide range of much more normal-looking frame styles and colors. The Meta Ray-Ban Display glasses, on the other hand, still look very much like a first-gen product. There are some really compelling use cases for the display, but its functionality is limited. The glasses are also too thick and bulky for what's meant to be an everyday accessory. At the end of the day, most people want glasses that make them look good. There’s also the fact that right now, these glasses are somewhat difficult to actually buy. They are only available at a handful of physical retailers, which currently have a very limited supply, Meta is also requiring would-be buyers to schedule demo appointments in order to buy, though some stores — like the LensCrafters where I bought my pair — aren’t enforcing this. Still, there's a lot to be excited about. Watching people's reactions to trying these has been almost as much fun as using them myself. Meta also has a solid lineup of new features already in the works, including a standalone Reels app, a teleprompter and gesture-based handwriting for message replies. If you're already all-in on smart glasses or, like me, you've been patiently waiting for glasses with a high quality, usable display, then the Meta Ray-Ban Display glasses are worth the investment now — as long as you can accept the thick frames. Update, October 17, 2025, 3:42PM PT: Added more information about group text functionality on Android. This article originally appeared on Engadget at https://www.engadget.com/wearables/meta-ray-ban-display-review-chunky-frames-with-impressive-abilities-193127070.html?src=rss",
          "feed_position": 3,
          "image_url": "https://d29szjachogqwa.cloudfront.net/videos/user-uploaded/meta_display_sand_on_kb.jpg"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/security/cisco-warns-enterprises-without-tapping-machine-data-your-ai-strategy-is",
          "published_at": "Fri, 17 Oct 2025 18:10:00 GMT",
          "title": "Cisco warns enterprises: Without tapping machine data, your AI strategy is incomplete",
          "standfirst": "Cisco executives make the case that the distinction between product and model companies is disappearing, and that accessing the 55% of enterprise data growth that current AI ignores will separate winners from losers.VentureBeat recently caught up with Jeetu Patel, Cisco&#x27;s President and Chief Product Officer and DJ Sampath, Senior Vice President of AI Software and Platform, to gain new insights into a compelling thesis both leaders share. They and their teams contend that every successful product company must become an AI model company to survive the next decade.When one considers how compressed product lifecycles are becoming, combined with the many advantages of digital twin technology to accelerate time-to-market of next-gen products, the thesis makes sense.The conversation revealed why this transformation is inevitable, backed by solid data points. The team contends that 55% of all data growth is machine data that current AI models don&#x27;t touch. OpenAI&#x27;s Greg Brockman estimates we need 10 billion GPUs to give every human the AI agents they&#x27;ll need, and Cisco&#x27;s open source security model, Foundation-Sec-8B, has already seen 200,000 downloads on Hugging Face.Why the model is becoming the product VentureBeat: You&#x27;ve stated that in the future, every product company will become a model company. Why is this inevitable rather than just one possible path?Jeetu Patel: In the future, there&#x27;s no distinction between model companies and product companies. Great product companies will be model companies. The close tie-in between model and product is a closed loop. To enhance the product, you enhance the model, not just a UI shim.These companies being formed right now that are a thin shim on top of a model; their days are numbered. The true moat is the model you build that drives product behavior. This requires being simultaneously good at two things: building great models in domains where you have great data, and building great product experiences powered by those models in an iterative loop where the models adapt and evolve when you have product enhancement requests.DJ Sampath: This becomes even more critical when you think about things moving to agents. Agents are going to be governed by these models. Your moat is really going to be how well your model reacts to the changes it needs to.Harnessing machine data&#x27;s growth is key VentureBeat: You mentioned that 55% of data growth is machine data, yet current models aren&#x27;t trained on it. Why does this represent such a massive opportunity?Patel: So far, models have been very good at being trained on publicly available, human-generated data freely available on the internet. But we&#x27;re done with the amount of public data you could crawl. Where else do you go next? It&#x27;s all locked up inside enterprises.55% of data growth is machine data, but models are not trained on machine data. Every company says &#x27;my data is my moat,&#x27; but most don&#x27;t have an effective way to condition that data into an organized pipeline so they can train AI with it and harness its full potential.Imagine how much log data will be generated when agents work 24/7 and every human has 100 agents. Greg Brockman from OpenAI said if you assume every human has a GPU, you&#x27;re three orders of magnitude away from where you need to be; you need 10 billion GPUs. When you think that way, if you don&#x27;t train your models with machine data effectively, you&#x27;re incomplete in your ability to harness the full potential of AI.Sampath: Most of the models are being trained on public data. The data that&#x27;s inside enterprises is mostly machine data. We&#x27;re unlocking that machine data. We give each enterprise a starting model. Think of it as a starter kit. They&#x27;ll take that model and build applications and agents fine-tuned on their proprietary data inside their enterprises. We&#x27;re going to be a model company, but we&#x27;re also going to make it incredibly easy for every single enterprise to build their own models using the infrastructure we provide.Why hardware companies have an advantageVentureBeat: Many see hardware as a liability in the software and AI era. You argue the opposite. Why?Patel: A lot of people look down on hardware. I actually think hardware is a great asset to have, because if you know how to build great hardware and great software and great AI models and tie them all together, that&#x27;s when magic starts to happen.Think about what we can do by correlating machine data from logs with our time series model. If there&#x27;s a one-degree change in your switch or router, you might predict system failure in three days, something you couldn&#x27;t correlate before. You identify the change, reroute traffic to prevent problems, and solve the issue. Get much more predictive in outages and infrastructure stability.Cisco is the critical infrastructure company for AI. This completely changes the level of stability we can generate for our infrastructure. Manufacturing is one of the top industries for the data volume generated daily. Combined with agentic AI and accumulated metadata, it completely changes the competitive nature of manufacturing or asset-intensive industries. With enough data, they can transcend disruptions around tariffs or supply chain variations, getting them out of price and availability commoditization.Cisco&#x27;s deep commitment to Open SourceVentureBeat: Why make your security models open source when that seems to give away competitive advantage?Sampath: The cat is out of the bag; attackers also have access to open source models. The next step is equipping as many defenders as possible with models that make defense stronger. That&#x27;s really what we did at RSAC 2025 when we launched our open source model, Foundation-Sec-8B.Funding for open source initiatives has stalled. There&#x27;s an increased drain in the open source community, needing sustainable, collaborative funding sources. It&#x27;s a corporate responsibility to make these models available, plus it provides access to communities to start working with AI from a defense perspective.We&#x27;ve integrated ClamAV, a widely used open source antivirus tool, with Hugging Face, which hosts over 2 million models. Every single model gets scanned for malware. You have to ensure the AI supply chain is appropriately protected, and we&#x27;re at the forefront of doing that.Patel: We launched not just the security model that&#x27;s open source, but also one on Splunk for time series data. These correlate data; time series and security incident data, to be able to find very interesting outcomes. With 200,000 downloads on Hugging Face, we&#x27;re seeing resellers starting to build applications with it.Taking the customers&#x27; pulse after Cisco LiveVentureBeat: Following Cisco Live&#x27;s product launches, how are customers responding?Patel: There are three categories. First, completely ecstatic customers: &#x27;We&#x27;ve been asking for this for a while. Hallelujah.&#x27;Second, those saying &#x27;I&#x27;m going to try this out.&#x27; DJ shows them a demo with white glove treatment, they do a POC, and they&#x27;re dumbfounded that it&#x27;s even better than what we said in three minutes on stage.Third are skeptics who verify that every announcement comes out on the exact days. That group used to be much bigger three years ago. As it&#x27;s shrunk, we&#x27;ve seen meaningful improvements in our financial results and how the market sees us.We don&#x27;t talk about things three years out, only within a six-month window. The payload is so large that we have enough to discuss for six months. Our biggest challenge, frankly, is keeping our customers up to date with the velocity of innovation we have.Obsessing over customers, not hardwareVentureBeat: How are you migrating your hardware-centric installed base without creating too much disruption?Patel: Rather than fixating on &#x27;hardware versus software,&#x27; you start from where the customer is. Your strategy can no longer be a perimeter-based firewall for network security because the market has moved. It&#x27;s hyper-distributed. But you currently have firewalls that need efficient management.We&#x27;re giving you a fully refreshed firewall lineup. If you want to look at what we&#x27;ve done with public cloud, managing egress traffic with Multicloud Defense with zero trust, not just user-to-application, but application-to-application. We&#x27;ve built Hypershield technology. We&#x27;ve built a revolutionary Smart Switch. All managed by the same Security Cloud Control with AI Canvas on top.We tell our customers they can go at their own pace. Start with firewalls, move to Multicloud Defense, add Hypershield enforcement points with Cilium for observability, and add Smart Switches. You don&#x27;t have to add more complexity because we have a true platform advantage with Security Cloud Control. Rather than saying &#x27;forget everything and move to the new thing&#x27;, creating too much cognitive load, we start where the customer is and take them through the journey.What&#x27;s next: energizing global partners to turn AI into a revenue opportunityThe interview concluded with discussions of November&#x27;s Partner Summit in San Diego, where Cisco plans significant partner activation announcements. As Patel noted, \"Sustained, consistent emphasis is needed to get the entire reseller engine moving.\" VentureBeat is convinced that a globally strong partner organization is indispensable for any cybersecurity company to attain its long-term AI vision.",
          "content": "Cisco executives make the case that the distinction between product and model companies is disappearing, and that accessing the 55% of enterprise data growth that current AI ignores will separate winners from losers.VentureBeat recently caught up with Jeetu Patel, Cisco&#x27;s President and Chief Product Officer and DJ Sampath, Senior Vice President of AI Software and Platform, to gain new insights into a compelling thesis both leaders share. They and their teams contend that every successful product company must become an AI model company to survive the next decade.When one considers how compressed product lifecycles are becoming, combined with the many advantages of digital twin technology to accelerate time-to-market of next-gen products, the thesis makes sense.The conversation revealed why this transformation is inevitable, backed by solid data points. The team contends that 55% of all data growth is machine data that current AI models don&#x27;t touch. OpenAI&#x27;s Greg Brockman estimates we need 10 billion GPUs to give every human the AI agents they&#x27;ll need, and Cisco&#x27;s open source security model, Foundation-Sec-8B, has already seen 200,000 downloads on Hugging Face.Why the model is becoming the product VentureBeat: You&#x27;ve stated that in the future, every product company will become a model company. Why is this inevitable rather than just one possible path?Jeetu Patel: In the future, there&#x27;s no distinction between model companies and product companies. Great product companies will be model companies. The close tie-in between model and product is a closed loop. To enhance the product, you enhance the model, not just a UI shim.These companies being formed right now that are a thin shim on top of a model; their days are numbered. The true moat is the model you build that drives product behavior. This requires being simultaneously good at two things: building great models in domains where you have great data, and building great product experiences powered by those models in an iterative loop where the models adapt and evolve when you have product enhancement requests.DJ Sampath: This becomes even more critical when you think about things moving to agents. Agents are going to be governed by these models. Your moat is really going to be how well your model reacts to the changes it needs to.Harnessing machine data&#x27;s growth is key VentureBeat: You mentioned that 55% of data growth is machine data, yet current models aren&#x27;t trained on it. Why does this represent such a massive opportunity?Patel: So far, models have been very good at being trained on publicly available, human-generated data freely available on the internet. But we&#x27;re done with the amount of public data you could crawl. Where else do you go next? It&#x27;s all locked up inside enterprises.55% of data growth is machine data, but models are not trained on machine data. Every company says &#x27;my data is my moat,&#x27; but most don&#x27;t have an effective way to condition that data into an organized pipeline so they can train AI with it and harness its full potential.Imagine how much log data will be generated when agents work 24/7 and every human has 100 agents. Greg Brockman from OpenAI said if you assume every human has a GPU, you&#x27;re three orders of magnitude away from where you need to be; you need 10 billion GPUs. When you think that way, if you don&#x27;t train your models with machine data effectively, you&#x27;re incomplete in your ability to harness the full potential of AI.Sampath: Most of the models are being trained on public data. The data that&#x27;s inside enterprises is mostly machine data. We&#x27;re unlocking that machine data. We give each enterprise a starting model. Think of it as a starter kit. They&#x27;ll take that model and build applications and agents fine-tuned on their proprietary data inside their enterprises. We&#x27;re going to be a model company, but we&#x27;re also going to make it incredibly easy for every single enterprise to build their own models using the infrastructure we provide.Why hardware companies have an advantageVentureBeat: Many see hardware as a liability in the software and AI era. You argue the opposite. Why?Patel: A lot of people look down on hardware. I actually think hardware is a great asset to have, because if you know how to build great hardware and great software and great AI models and tie them all together, that&#x27;s when magic starts to happen.Think about what we can do by correlating machine data from logs with our time series model. If there&#x27;s a one-degree change in your switch or router, you might predict system failure in three days, something you couldn&#x27;t correlate before. You identify the change, reroute traffic to prevent problems, and solve the issue. Get much more predictive in outages and infrastructure stability.Cisco is the critical infrastructure company for AI. This completely changes the level of stability we can generate for our infrastructure. Manufacturing is one of the top industries for the data volume generated daily. Combined with agentic AI and accumulated metadata, it completely changes the competitive nature of manufacturing or asset-intensive industries. With enough data, they can transcend disruptions around tariffs or supply chain variations, getting them out of price and availability commoditization.Cisco&#x27;s deep commitment to Open SourceVentureBeat: Why make your security models open source when that seems to give away competitive advantage?Sampath: The cat is out of the bag; attackers also have access to open source models. The next step is equipping as many defenders as possible with models that make defense stronger. That&#x27;s really what we did at RSAC 2025 when we launched our open source model, Foundation-Sec-8B.Funding for open source initiatives has stalled. There&#x27;s an increased drain in the open source community, needing sustainable, collaborative funding sources. It&#x27;s a corporate responsibility to make these models available, plus it provides access to communities to start working with AI from a defense perspective.We&#x27;ve integrated ClamAV, a widely used open source antivirus tool, with Hugging Face, which hosts over 2 million models. Every single model gets scanned for malware. You have to ensure the AI supply chain is appropriately protected, and we&#x27;re at the forefront of doing that.Patel: We launched not just the security model that&#x27;s open source, but also one on Splunk for time series data. These correlate data; time series and security incident data, to be able to find very interesting outcomes. With 200,000 downloads on Hugging Face, we&#x27;re seeing resellers starting to build applications with it.Taking the customers&#x27; pulse after Cisco LiveVentureBeat: Following Cisco Live&#x27;s product launches, how are customers responding?Patel: There are three categories. First, completely ecstatic customers: &#x27;We&#x27;ve been asking for this for a while. Hallelujah.&#x27;Second, those saying &#x27;I&#x27;m going to try this out.&#x27; DJ shows them a demo with white glove treatment, they do a POC, and they&#x27;re dumbfounded that it&#x27;s even better than what we said in three minutes on stage.Third are skeptics who verify that every announcement comes out on the exact days. That group used to be much bigger three years ago. As it&#x27;s shrunk, we&#x27;ve seen meaningful improvements in our financial results and how the market sees us.We don&#x27;t talk about things three years out, only within a six-month window. The payload is so large that we have enough to discuss for six months. Our biggest challenge, frankly, is keeping our customers up to date with the velocity of innovation we have.Obsessing over customers, not hardwareVentureBeat: How are you migrating your hardware-centric installed base without creating too much disruption?Patel: Rather than fixating on &#x27;hardware versus software,&#x27; you start from where the customer is. Your strategy can no longer be a perimeter-based firewall for network security because the market has moved. It&#x27;s hyper-distributed. But you currently have firewalls that need efficient management.We&#x27;re giving you a fully refreshed firewall lineup. If you want to look at what we&#x27;ve done with public cloud, managing egress traffic with Multicloud Defense with zero trust, not just user-to-application, but application-to-application. We&#x27;ve built Hypershield technology. We&#x27;ve built a revolutionary Smart Switch. All managed by the same Security Cloud Control with AI Canvas on top.We tell our customers they can go at their own pace. Start with firewalls, move to Multicloud Defense, add Hypershield enforcement points with Cilium for observability, and add Smart Switches. You don&#x27;t have to add more complexity because we have a true platform advantage with Security Cloud Control. Rather than saying &#x27;forget everything and move to the new thing&#x27;, creating too much cognitive load, we start where the customer is and take them through the journey.What&#x27;s next: energizing global partners to turn AI into a revenue opportunityThe interview concluded with discussions of November&#x27;s Partner Summit in San Diego, where Cisco plans significant partner activation announcements. As Patel noted, \"Sustained, consistent emphasis is needed to get the entire reseller engine moving.\" VentureBeat is convinced that a globally strong partner organization is indispensable for any cybersecurity company to attain its long-term AI vision.",
          "feed_position": 1,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/1ooehT0PzSVnkDAGGhBfos/f7f832028725f8f1ca1af654c398ae7a/hero_2.jpg"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/codev-lets-enterprises-avoid-vibe-coding-hangovers-with-a-team-of-agents",
          "published_at": "Fri, 17 Oct 2025 17:45:00 GMT",
          "title": "Codev lets enterprises avoid vibe coding hangovers with a team of agents that generate and document code",
          "standfirst": "For many software developers using generative AI, vibe coding is a double-edged sword. The process delivers rapid prototypes but often leaves a trail of brittle, undocumented code that creates significant technical debt. A new open-source platform, Codev, addresses this by proposing a fundamental shift: treating the natural language conversation with an AI as part of the actual source code. Codev is based on SP(IDE)R, a framework designed to turn vibe-coding conversations into structured, versioned, and auditable assets that become part of the code repository.What is Codev?At its core, Codev is a methodology that treats natural language context as an integral part of the development lifecycle as opposed to a disposable artifact as is the case with vanilla vibe coding. According to co-founder Waleed Kadous, the goal is to invert the typical engineering workflow. \"A key principle of Codev is that documents like the specification are the actual code of the system,\" he told VentureBeat. \"It&#x27;s almost like natural language is compiled down into Typescript by our agents.\"This approach avoids the common pitfall where documentation is created after the fact, if at all.Its flagship protocol, SP(IDE)R, provides a lightweight but formal structure for building software. The process begins with Specify, where a human and multiple AI agents collaborate to turn a high-level request into concrete acceptance criteria. Next, in the Plan stage, an AI proposes a phased implementation, which is again reviewed. For each phase, the AI enters an IDE loop: it Implements the code, Defends it against bugs and regression with comprehensive tests, and Evaluates the result against the specification. The final step is Review, where the team documents lessons learned to update and improve the SP(IDE)R protocol itself for future projects.The framework’s key differentiator is its use of multiple agents and explicit human review at different stages. Kadous notes that each agent brings unique strengths to the review process. \"Gemini is extremely good at catching security issues,\" he said, citing a critical cross-site scripting (XSS) flaw and another bug that \"would have shared an OpenAI API key with the client, which could cost thousands of dollars.\" Meanwhile, \"GPT-5 is very good at understanding how to simplify a design.\" This structured review, with a human providing final approval at each stage, prevents the kind of runaway automation that leads to flawed code.The platform’s AI-native philosophy extends to its installation. There is no complex installer; instead, a user instructs their AI agent to apply the Codev GitHub repository to set up the project. The developers \"dogfooded\" their framework, using Codev to build Codev.“The key point here is that natural language is executable now, with the agent being the interpreter,” Kadous said. “This is great because it means it&#x27;s not a ‘blind’ integration of Codev, the agent gets to choose the best way to integrate it and can intelligently make decisions.”Codev case studyTo test the framework&#x27;s effectiveness, its creators ran a direct comparison between vanilla vibe-coding and Codev. They gave Claude Opus 4.1 a request to build a modern web-based todo manager. The first attempt used a conversational, vibe-coding approach. The result was a plausible-looking demo. However, an automated analysis conducted by three independent AI agents found that it had implemented 0% of the required functionality, contained no tests, and lacked a database or API.The second attempt used the same AI model and prompt but applied the SP(IDE)R protocol. This time, the AI produced a production-ready application with 32 source files, 100% of the specified functionality, five test suites, a SQLite database, and a complete RESTful API. Throughout this process, the human developers reported they never directly edited a single line of source code. While this was a single experiment, Kadous estimates the impact is substantial. \"Subjectively, it feels like I&#x27;m about three times as productive with Codev as without,\" he says. The quality also speaks for itself. \"I used LLMs as a judge, and one of them described the output like what a well-oiled engineering team would produce. That was exactly what I was aiming for.\"While the process is powerful, it redefines the developer&#x27;s role from a hands-on coder to a system architect and reviewer. According to Kadous, the initial spec and plan stages can each take between 45 minutes to two hours of focused collaboration. This is in contrast to the impression given by many vibe-coding platforms, where a single prompt and a few minutes of processing gives you a fully functional and scalable application.\"All of the value I add is in the background knowledge I apply to the specs and plans,\" he explains. He emphasizes that the framework is designed to augment, not replace, experienced talent. \"The people who will do the best... are senior engineers and above because they know the pitfalls... It just takes the senior engineer you already have and makes them much more productive.\"A future of human and AI collaborationFrameworks like Codev signal a shift where the primary creative act of software development moves from writing code to crafting precise, machine-readable specifications and plans. For enterprise teams, this means AI-generated code can become auditable, maintainable, and reliable. By capturing the entire development conversation in version control and enforcing it with CI, the process turns ephemeral chats into durable engineering assets.Codev proposes a future where the AI acts not as a chaotic assistant, but as a disciplined collaborator in a structured, human-led workflow. However, Kadous acknowledges this shift creates new challenges for the workforce. \"Senior engineers that reject AI outright will be outpaced by senior engineers who embrace it,\" he predicts. He also expresses concern for junior developers who may not get the chance \"to build their architectural chops,\" a skill that becomes even more critical when guiding AI. This highlights a central challenge for the industry: ensuring that as AI elevates top performers, it also creates pathways to develop the next generation of talent.",
          "content": "For many software developers using generative AI, vibe coding is a double-edged sword. The process delivers rapid prototypes but often leaves a trail of brittle, undocumented code that creates significant technical debt. A new open-source platform, Codev, addresses this by proposing a fundamental shift: treating the natural language conversation with an AI as part of the actual source code. Codev is based on SP(IDE)R, a framework designed to turn vibe-coding conversations into structured, versioned, and auditable assets that become part of the code repository.What is Codev?At its core, Codev is a methodology that treats natural language context as an integral part of the development lifecycle as opposed to a disposable artifact as is the case with vanilla vibe coding. According to co-founder Waleed Kadous, the goal is to invert the typical engineering workflow. \"A key principle of Codev is that documents like the specification are the actual code of the system,\" he told VentureBeat. \"It&#x27;s almost like natural language is compiled down into Typescript by our agents.\"This approach avoids the common pitfall where documentation is created after the fact, if at all.Its flagship protocol, SP(IDE)R, provides a lightweight but formal structure for building software. The process begins with Specify, where a human and multiple AI agents collaborate to turn a high-level request into concrete acceptance criteria. Next, in the Plan stage, an AI proposes a phased implementation, which is again reviewed. For each phase, the AI enters an IDE loop: it Implements the code, Defends it against bugs and regression with comprehensive tests, and Evaluates the result against the specification. The final step is Review, where the team documents lessons learned to update and improve the SP(IDE)R protocol itself for future projects.The framework’s key differentiator is its use of multiple agents and explicit human review at different stages. Kadous notes that each agent brings unique strengths to the review process. \"Gemini is extremely good at catching security issues,\" he said, citing a critical cross-site scripting (XSS) flaw and another bug that \"would have shared an OpenAI API key with the client, which could cost thousands of dollars.\" Meanwhile, \"GPT-5 is very good at understanding how to simplify a design.\" This structured review, with a human providing final approval at each stage, prevents the kind of runaway automation that leads to flawed code.The platform’s AI-native philosophy extends to its installation. There is no complex installer; instead, a user instructs their AI agent to apply the Codev GitHub repository to set up the project. The developers \"dogfooded\" their framework, using Codev to build Codev.“The key point here is that natural language is executable now, with the agent being the interpreter,” Kadous said. “This is great because it means it&#x27;s not a ‘blind’ integration of Codev, the agent gets to choose the best way to integrate it and can intelligently make decisions.”Codev case studyTo test the framework&#x27;s effectiveness, its creators ran a direct comparison between vanilla vibe-coding and Codev. They gave Claude Opus 4.1 a request to build a modern web-based todo manager. The first attempt used a conversational, vibe-coding approach. The result was a plausible-looking demo. However, an automated analysis conducted by three independent AI agents found that it had implemented 0% of the required functionality, contained no tests, and lacked a database or API.The second attempt used the same AI model and prompt but applied the SP(IDE)R protocol. This time, the AI produced a production-ready application with 32 source files, 100% of the specified functionality, five test suites, a SQLite database, and a complete RESTful API. Throughout this process, the human developers reported they never directly edited a single line of source code. While this was a single experiment, Kadous estimates the impact is substantial. \"Subjectively, it feels like I&#x27;m about three times as productive with Codev as without,\" he says. The quality also speaks for itself. \"I used LLMs as a judge, and one of them described the output like what a well-oiled engineering team would produce. That was exactly what I was aiming for.\"While the process is powerful, it redefines the developer&#x27;s role from a hands-on coder to a system architect and reviewer. According to Kadous, the initial spec and plan stages can each take between 45 minutes to two hours of focused collaboration. This is in contrast to the impression given by many vibe-coding platforms, where a single prompt and a few minutes of processing gives you a fully functional and scalable application.\"All of the value I add is in the background knowledge I apply to the specs and plans,\" he explains. He emphasizes that the framework is designed to augment, not replace, experienced talent. \"The people who will do the best... are senior engineers and above because they know the pitfalls... It just takes the senior engineer you already have and makes them much more productive.\"A future of human and AI collaborationFrameworks like Codev signal a shift where the primary creative act of software development moves from writing code to crafting precise, machine-readable specifications and plans. For enterprise teams, this means AI-generated code can become auditable, maintainable, and reliable. By capturing the entire development conversation in version control and enforcing it with CI, the process turns ephemeral chats into durable engineering assets.Codev proposes a future where the AI acts not as a chaotic assistant, but as a disciplined collaborator in a structured, human-led workflow. However, Kadous acknowledges this shift creates new challenges for the workforce. \"Senior engineers that reject AI outright will be outpaced by senior engineers who embrace it,\" he predicts. He also expresses concern for junior developers who may not get the chance \"to build their architectural chops,\" a skill that becomes even more critical when guiding AI. This highlights a central challenge for the industry: ensuring that as AI elevates top performers, it also creates pathways to develop the next generation of talent.",
          "feed_position": 2,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/74nSfOtaxxWUY7F5d25vJp/2f602a8058b366f3bdd73cd30d00cf58/AI-human_coding.jpg"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/audio/headphones/bose-quietcomfort-ultra-headphones-2nd-gen-review-impactful-upgrades-to-a-familiar-formula-150000709.html",
          "published_at": "Fri, 17 Oct 2025 15:00:00 +0000",
          "title": "Bose QuietComfort Ultra Headphones (2nd gen) review: Impactful upgrades to a familiar formula",
          "standfirst": "Bose took a different approach with its new products in 2025. Instead of entirely redesigning its QuietComfort Ultra lineup, the company unveiled upgraded second-generation models of the flagship-level earbuds and headphones. Like the QuietComfort Ultra Earbuds that debuted earlier this year, the new Quiet Comfort Ultra headphones ($449) don’t offer a comprehensive overhaul. However, the changes provide enough performance improvements to further cement these as the best noise-canceling headphones that you can buy right now. Trust me, we’re much better off with this revamped version than we would be with a year (or longer) wait for something brand new with the 2023 model. What’s new on the QuietComfort Ultra Headphones? Bose debuted a few new features on the second-gen QuietComfort Ultra Earbuds in August that it carried over to these new headphones. First, the company improved its already stellar active noise cancelation (ANC) with tweaks to its ActiveSense technology. Specifically, the system can respond to sudden spikes in environmental noise by adapting more precisely. It’s not something you’ll notice all the time, but when you need it, you’ll be glad it's there. Otherwise, the excellent ANC performance here is just as effective as it was on the previous model. More on that in a bit. The immersive Cinema Mode that Bose added to the QC Ultra Earbuds is also available on these headphones. It’s a sound profile that enhances dialogue clarity while keeping the rest of the soundstage as wide and enveloping as possible. I like it best for movies and TV, as the name suggests, but per Bose’s suggestion I also tried it with podcasts and audiobooks. Cinema Mode is probably overkill for those types of content, unless you’re listening to shows or titles with lots of background effects. One of the biggest changes on the second-gen QC Ultra Headphones is how Bose decided to handle power management. Most importantly, the company extended battery life in all use cases. With ANC on (and Immersive Audio off), you’ll get up to 30 hours of listening time. Turn off ANC and that jumps to 45 hours. When you decide to enable both ANC and Bose’s spatial Immersive Audio, you can expect up to 23 hours on a charge. Compared to those on the first-generation model, all of these numbers are up by at least five hours, which is a significant boost. These headphones rotate flat and fold in for compact transport. Billy Steele for Engadget Like the previous QC Ultra Headphones, this model has an automatic disconnection feature after 10 minutes of standby . But the company went a step further on this version by adding a low-power mode that the headphones enter after 30 minutes of idle time. And if you want to disconnect them quickly, you can rotate the earcups and lay them on a flat surface. That’ll make them go into a deeper standby mode that Bose says can run “for months.” All of this means you can effectively turn the new QuietComfort Ultra Headphones on and off by putting them on and taking them off. If you’re using them regularly, you’ll never have to press the power button. Sound-wise, the big upgrade on these headphones is the addition of lossless audio over USB-C. Like the AirPods Max, this model can be connected with a cable to your phone, tablet, laptop or desktop to stream or play higher-quality tunes from compatible services or your library. Bose says you can expect 16-bit 44.1kHz or 48kHz audio depending on your source. It’s yet another nice-to-have feature that’s becoming standard fare on premium wireless headphones. What else is good about the QuietComfort Ultra Headphones? Like most Bose over-ear headphones, the second-gen QuietComfort Ultra Headphones are supremely comfortable. Even for long periods of time, they never become a burden, and that’s thanks in large part to the soft, pillowy ear pads. I could easily wear these for an entire trans-Atlantic flight with minimal discomfort and I’ve been wearing them for entire workdays at home. As I already mentioned, the ANC performance here is still top-tier. In fact, these QC Ultra Headphones will soon replace the first-gen model on our best noise-canceling headphones list. Both the Immersion (ANC + spatial audio) and Quiet (just ANC) modes provide robust noise blocking that surpasses those by Sony, Sennheiser and others. If you’re making your buying decision based solely on ANC performance, this is the best option. You’ll enjoy relief from constant ambient noise sources like fans and sound machines, plus the QC Ultra Headphones do a respectable job with human voices. Heck, I couldn’t even hear my dog barking at the imminent threat from falling leaves outside. Lastly, Bose’s take on spatial audio is still quite good. The company calls it Immersive Audio and the feature doesn’t rely on specialized content like other headphones. Music sounds obviously fuller and slightly louder when the sound profile is active thanks to Bose’s method for upscaling stereo content. There’s also enhanced vocal clarity and elements like percussion and synths are less compressed than usual. The headphones lend a particularly airy feel to the tracks of Ruston Kelly’s Pale, Through the Window, an acoustic-driven collection of soulful, country-tinged tunes. His vocals float atop enveloping acoustic guitars and tight, punchy drums. What’s not so good about the QC Ultra Headphones? The Bose app gives you access to controls and customization. Billy Steele for Engadget The biggest issue with the second-gen QuietComfort Ultra Headphones is the price. To be clear, the likes of Sony, Apple and others charge around the same amount for their top-of-the-line models, but $449 is still a significant investment. If that’s too steep for you, Bose has the highly capable QuietComfort Headphones in its arsenal for a slightly more palatable $359. My other gripe is that the only real design change Bose made for the updated QC Ultra Headphones is that the metal headband yokes now have a gloss finish. Depending on your personal preference, this might be a dealbreaker for you. It’s least noticeable on the black and violet colorways, since these have a tone-on-tone look. After a few weeks with the bronze and tan Driftwood Sand hue, I’m not a fan of the more stylized aesthetic. It’s flashy, for sure, but it’s a tweak I could’ve done without. Wrap-up Similar to the second-gen QC Ultra Earbuds over the summer, Bose didn’t make huge upgrades for the updated version of the QC Ultra Headphones. But what you do get here is a decent improvement over its predecessor. The company devised an intuitive setup for power management and even addressed one of my main gripes with the original by adding support for lossless audio over USB-C. Plus, the extended battery life is significant in all sound modes, and not just by an hour or two here or there. To top it all off, the QuietComfort Ultra Headphones remain the best option for pure noise-blocking ability, and that’s not likely to change any time soon. This article originally appeared on Engadget at https://www.engadget.com/audio/headphones/bose-quietcomfort-ultra-headphones-2nd-gen-review-impactful-upgrades-to-a-familiar-formula-150000709.html?src=rss",
          "content": "Bose took a different approach with its new products in 2025. Instead of entirely redesigning its QuietComfort Ultra lineup, the company unveiled upgraded second-generation models of the flagship-level earbuds and headphones. Like the QuietComfort Ultra Earbuds that debuted earlier this year, the new Quiet Comfort Ultra headphones ($449) don’t offer a comprehensive overhaul. However, the changes provide enough performance improvements to further cement these as the best noise-canceling headphones that you can buy right now. Trust me, we’re much better off with this revamped version than we would be with a year (or longer) wait for something brand new with the 2023 model. What’s new on the QuietComfort Ultra Headphones? Bose debuted a few new features on the second-gen QuietComfort Ultra Earbuds in August that it carried over to these new headphones. First, the company improved its already stellar active noise cancelation (ANC) with tweaks to its ActiveSense technology. Specifically, the system can respond to sudden spikes in environmental noise by adapting more precisely. It’s not something you’ll notice all the time, but when you need it, you’ll be glad it's there. Otherwise, the excellent ANC performance here is just as effective as it was on the previous model. More on that in a bit. The immersive Cinema Mode that Bose added to the QC Ultra Earbuds is also available on these headphones. It’s a sound profile that enhances dialogue clarity while keeping the rest of the soundstage as wide and enveloping as possible. I like it best for movies and TV, as the name suggests, but per Bose’s suggestion I also tried it with podcasts and audiobooks. Cinema Mode is probably overkill for those types of content, unless you’re listening to shows or titles with lots of background effects. One of the biggest changes on the second-gen QC Ultra Headphones is how Bose decided to handle power management. Most importantly, the company extended battery life in all use cases. With ANC on (and Immersive Audio off), you’ll get up to 30 hours of listening time. Turn off ANC and that jumps to 45 hours. When you decide to enable both ANC and Bose’s spatial Immersive Audio, you can expect up to 23 hours on a charge. Compared to those on the first-generation model, all of these numbers are up by at least five hours, which is a significant boost. These headphones rotate flat and fold in for compact transport. Billy Steele for Engadget Like the previous QC Ultra Headphones, this model has an automatic disconnection feature after 10 minutes of standby . But the company went a step further on this version by adding a low-power mode that the headphones enter after 30 minutes of idle time. And if you want to disconnect them quickly, you can rotate the earcups and lay them on a flat surface. That’ll make them go into a deeper standby mode that Bose says can run “for months.” All of this means you can effectively turn the new QuietComfort Ultra Headphones on and off by putting them on and taking them off. If you’re using them regularly, you’ll never have to press the power button. Sound-wise, the big upgrade on these headphones is the addition of lossless audio over USB-C. Like the AirPods Max, this model can be connected with a cable to your phone, tablet, laptop or desktop to stream or play higher-quality tunes from compatible services or your library. Bose says you can expect 16-bit 44.1kHz or 48kHz audio depending on your source. It’s yet another nice-to-have feature that’s becoming standard fare on premium wireless headphones. What else is good about the QuietComfort Ultra Headphones? Like most Bose over-ear headphones, the second-gen QuietComfort Ultra Headphones are supremely comfortable. Even for long periods of time, they never become a burden, and that’s thanks in large part to the soft, pillowy ear pads. I could easily wear these for an entire trans-Atlantic flight with minimal discomfort and I’ve been wearing them for entire workdays at home. As I already mentioned, the ANC performance here is still top-tier. In fact, these QC Ultra Headphones will soon replace the first-gen model on our best noise-canceling headphones list. Both the Immersion (ANC + spatial audio) and Quiet (just ANC) modes provide robust noise blocking that surpasses those by Sony, Sennheiser and others. If you’re making your buying decision based solely on ANC performance, this is the best option. You’ll enjoy relief from constant ambient noise sources like fans and sound machines, plus the QC Ultra Headphones do a respectable job with human voices. Heck, I couldn’t even hear my dog barking at the imminent threat from falling leaves outside. Lastly, Bose’s take on spatial audio is still quite good. The company calls it Immersive Audio and the feature doesn’t rely on specialized content like other headphones. Music sounds obviously fuller and slightly louder when the sound profile is active thanks to Bose’s method for upscaling stereo content. There’s also enhanced vocal clarity and elements like percussion and synths are less compressed than usual. The headphones lend a particularly airy feel to the tracks of Ruston Kelly’s Pale, Through the Window, an acoustic-driven collection of soulful, country-tinged tunes. His vocals float atop enveloping acoustic guitars and tight, punchy drums. What’s not so good about the QC Ultra Headphones? The Bose app gives you access to controls and customization. Billy Steele for Engadget The biggest issue with the second-gen QuietComfort Ultra Headphones is the price. To be clear, the likes of Sony, Apple and others charge around the same amount for their top-of-the-line models, but $449 is still a significant investment. If that’s too steep for you, Bose has the highly capable QuietComfort Headphones in its arsenal for a slightly more palatable $359. My other gripe is that the only real design change Bose made for the updated QC Ultra Headphones is that the metal headband yokes now have a gloss finish. Depending on your personal preference, this might be a dealbreaker for you. It’s least noticeable on the black and violet colorways, since these have a tone-on-tone look. After a few weeks with the bronze and tan Driftwood Sand hue, I’m not a fan of the more stylized aesthetic. It’s flashy, for sure, but it’s a tweak I could’ve done without. Wrap-up Similar to the second-gen QC Ultra Earbuds over the summer, Bose didn’t make huge upgrades for the updated version of the QC Ultra Headphones. But what you do get here is a decent improvement over its predecessor. The company devised an intuitive setup for power management and even addressed one of my main gripes with the original by adding support for lossless audio over USB-C. Plus, the extended battery life is significant in all sound modes, and not just by an hour or two here or there. To top it all off, the QuietComfort Ultra Headphones remain the best option for pure noise-blocking ability, and that’s not likely to change any time soon. This article originally appeared on Engadget at https://www.engadget.com/audio/headphones/bose-quietcomfort-ultra-headphones-2nd-gen-review-impactful-upgrades-to-a-familiar-formula-150000709.html?src=rss",
          "feed_position": 11,
          "image_url": "https://d29szjachogqwa.cloudfront.net/videos/user-uploaded/DSC_5360.jpg"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/data-infrastructure/worlds-largest-open-source-multimodal-dataset-delivers-17x-training",
          "published_at": "Fri, 17 Oct 2025 13:00:00 GMT",
          "title": "World's largest open-source multimodal dataset delivers 17x training efficiency, unlocking enterprise AI that connects documents, audio and video",
          "standfirst": "AI models are only as good as the data they&#x27;re trained on. That data generally needs to be labeled, curated and organized before models can learn from it in an effective way.One of the big missing links in the AI ecosystem has been the availability of a large high-quality open-source multimodal dataset. That changes today with the debut of the EMM-1 dataset which is comprised of 1 billion data pairs and 100M data groups across 5 modalities: text, image, video, audio and 3d point clouds. Multimodal datasets combine different types of data that AI systems can process together. This mirrors how humans perceive the world using multiple senses simultaneously. These datasets enable AI systems to make richer inferences by understanding relationships across data types, rather than processing each modality in isolation.EMM-1 is developed by data labeling platform vendor Encord. The company&#x27;s platform enables teams to curate, label and manage training data at scale using both automated and human-in-the-loop workflows. Alongside the new model, Encord developed the EBind training methodology that prioritizes data quality over raw computational scale. The approach enabled a compact 1.8 billion parameter model to match the performance of models up to 17 times larger while slashing training time from days to hours on a single GPU rather than GPU clusters.\"The big trick for us was to really focus on the data and to make the data very, very high quality,\" Encord Co-Founder and CEO Eric Landau told VentureBeat in an exclusive interview. \"We were able to get to the same level of performance as models 20 times larger, not because we were super clever on the architecture, but because we trained it with really good data overall.\"The data quality advantageEncord&#x27;s dataset is 100 times larger than the next comparable multimodal dataset, according to Landau. It operates at petabyte scale with terabytes of raw data and over 1 million human annotations.But scale alone doesn&#x27;t explain the performance gains. The technical innovation centers on addressing what Landau calls an \"under-appreciated\" problem in AI training: data leakage between training and evaluation sets.\"The leakage problem was one which we spent a lot of time on,\" Landau explained. \"In a lot of data sets, there is a kind of leakage between different subsets of the data. Leakage actually boosts your results. It makes your evaluations look better. But it&#x27;s one thing that we were quite diligent about.\"Data leakage occurs when information from test data inadvertently appears in training data, artificially inflating model performance metrics. Many benchmark datasets suffer from this contamination. Encord deployed hierarchical clustering techniques to ensure clean separation while maintaining representative distribution across data types. The company also used clustering to address bias and ensure diverse representation. How EBind boosts efficiencyThe data quality improvements work in tandem with an architectural approach designed for efficiencyEncord&#x27;s EBind extends the CLIP (Contrastive Language-Image Pre-training) approach (originally developed by OpenAI) from two modalities to five. CLIP learns to associate images and text in a shared representation space, enabling tasks like searching for images using text descriptions.Where CLIP learns to associate images and text in a shared latent space, EBind does the same across images, text, audio, 3D point clouds and video.The architectural choice prioritizes parameter efficiency. Rather than deploying separate specialized models for each modality pair, EBind uses a single base model with one encoder per modality.\"Other methodologies, what they do is they use a bunch of different models, and they route to the best model for embedding these pairs, so they tend to explode in the number of parameters,\" Landau said. \"We found we could use a single base model and just train one encoder per modality, so keeping it very simple and very parameter efficient, if we fed that overall architecture really, really good data.\"The resulting model rivals OmniBind, a much larger competitor in the multimodal space, but requires dramatically fewer computational resources for both training and inference. This makes EBind deployable in resource-constrained environments including edge devices for robotics and autonomous systems.The enterprise value of a multi-modal datasetMultimodal models enable enterprise use cases that span different data types.Most organizations store different data types in separate systems: documents in content management platforms, audio recordings in communication tools, training videos in learning management systems and structured data in databases. Multimodal models can search and retrieve across all of these simultaneously.\"Enterprises have all different types of data. They don&#x27;t just have documents. They have audio recordings, and they have training videos, and they have CSV files,\" Landau said. \"Let&#x27;s say you&#x27;re a lawyer and you have a case file that has video evidence and also documents and recordings, and it&#x27;s all scattered across a lot of silos of data. You can use EBind to pick all of the relevant data and bundle together to search and surface the right data much quicker than you would have before.\"The same principle applies across verticals. Healthcare providers can link patient imaging data to clinical notes and diagnostic audio. Financial services firms can connect transaction records to compliance call recordings and customer communications. Manufacturing operations can tie equipment sensor data to maintenance video logs and inspection reports.Beyond office environments, physical AI represents another frontier. Landau highlighted autonomous vehicles that benefit from both visual perception and audio cues like emergency sirens. In manufacturing and warehousing, robots that combine visual recognition with audio feedback and spatial awareness can operate more safely and effectively than vision-only systems.Enterprise use case: Extending computer vision with multimodal contextCaptur AI, an Encord customer, illustrates how companies are planning to use the dataset for specific business applications. The startup provides on-device image verification for mobile apps, validating photos in real-time for authenticity, compliance and quality before upload. The company works with shared mobility providers like Lime and delivery companies capturing billions of package photos.Captur AI processes over 100 million images on-device and specializes in distilling models to 6-10 megabytes so they can run on smartphones without cloud connectivity. But CEO Charlotte Bax sees multimodal capabilities as critical for expanding into higher-value use cases.\"The market for us is massive. You submit photos for returns and retails. You submit photos to insurance companies for claims. You submit photos when you&#x27;re listing something on eBay,\" Bax told VentureBeat in an exclusive interview. \"Some of those use cases are very high risk or high value if something goes wrong, like insurance, the image only captures part of the context and audio can be an important signal.\"Bax cited digital vehicle inspections as a prime example. When customers photograph vehicle damage for insurance claims, they often describe what happened verbally while capturing images. Audio context can significantly improve claim accuracy and reduce fraud.\"As you&#x27;re doing that, oftentimes the customer is actually describing what&#x27;s happened,\" Bax said. \"A few of our potential prospects in InsurTech have asked us if we can actually do audio as well, because then that adds this additional bit of context for the user who&#x27;s submitting the claim.\"The challenge lies in maintaining Captur AI&#x27;s core advantage: running models efficiently on-device rather than requiring cloud processing. The company plans to use Encord&#x27;s dataset to train compact multimodal models that preserve real-time, offline capabilities while adding audio and sequential image context.\"The most important thing you can do is try and get as much context as possible,\" Bax said. \"Can you get LLMs to be small enough to run on a device within the next three years, or can you run multimodal models on the device? Solving data quality before image upload is the interesting frontier.\"What this means for enterprisesEncord&#x27;s results challenge fundamental assumptions about AI development and suggest that the next competitive battleground may be data operations rather than infrastructure scale.Multimodal datasets unlock new capabilities. The ability to train models that understand relationships across data types opens use cases that single-modality systems cannot address.Data operations deserve equal investment with compute infrastructure. The 17x parameter efficiency gain from better data curation represents orders of magnitude in cost savings. Organizations pouring resources into GPU clusters while treating data quality as an afterthought may be optimizing the wrong variable.For enterprises building multimodal AI systems, Landau&#x27;s assessment captures the strategic shift. \"We were able to get to the same level of performance as models much larger, not because we were super clever on the architecture, but because we trained it with really good data overall,\" he said.",
          "content": "AI models are only as good as the data they&#x27;re trained on. That data generally needs to be labeled, curated and organized before models can learn from it in an effective way.One of the big missing links in the AI ecosystem has been the availability of a large high-quality open-source multimodal dataset. That changes today with the debut of the EMM-1 dataset which is comprised of 1 billion data pairs and 100M data groups across 5 modalities: text, image, video, audio and 3d point clouds. Multimodal datasets combine different types of data that AI systems can process together. This mirrors how humans perceive the world using multiple senses simultaneously. These datasets enable AI systems to make richer inferences by understanding relationships across data types, rather than processing each modality in isolation.EMM-1 is developed by data labeling platform vendor Encord. The company&#x27;s platform enables teams to curate, label and manage training data at scale using both automated and human-in-the-loop workflows. Alongside the new model, Encord developed the EBind training methodology that prioritizes data quality over raw computational scale. The approach enabled a compact 1.8 billion parameter model to match the performance of models up to 17 times larger while slashing training time from days to hours on a single GPU rather than GPU clusters.\"The big trick for us was to really focus on the data and to make the data very, very high quality,\" Encord Co-Founder and CEO Eric Landau told VentureBeat in an exclusive interview. \"We were able to get to the same level of performance as models 20 times larger, not because we were super clever on the architecture, but because we trained it with really good data overall.\"The data quality advantageEncord&#x27;s dataset is 100 times larger than the next comparable multimodal dataset, according to Landau. It operates at petabyte scale with terabytes of raw data and over 1 million human annotations.But scale alone doesn&#x27;t explain the performance gains. The technical innovation centers on addressing what Landau calls an \"under-appreciated\" problem in AI training: data leakage between training and evaluation sets.\"The leakage problem was one which we spent a lot of time on,\" Landau explained. \"In a lot of data sets, there is a kind of leakage between different subsets of the data. Leakage actually boosts your results. It makes your evaluations look better. But it&#x27;s one thing that we were quite diligent about.\"Data leakage occurs when information from test data inadvertently appears in training data, artificially inflating model performance metrics. Many benchmark datasets suffer from this contamination. Encord deployed hierarchical clustering techniques to ensure clean separation while maintaining representative distribution across data types. The company also used clustering to address bias and ensure diverse representation. How EBind boosts efficiencyThe data quality improvements work in tandem with an architectural approach designed for efficiencyEncord&#x27;s EBind extends the CLIP (Contrastive Language-Image Pre-training) approach (originally developed by OpenAI) from two modalities to five. CLIP learns to associate images and text in a shared representation space, enabling tasks like searching for images using text descriptions.Where CLIP learns to associate images and text in a shared latent space, EBind does the same across images, text, audio, 3D point clouds and video.The architectural choice prioritizes parameter efficiency. Rather than deploying separate specialized models for each modality pair, EBind uses a single base model with one encoder per modality.\"Other methodologies, what they do is they use a bunch of different models, and they route to the best model for embedding these pairs, so they tend to explode in the number of parameters,\" Landau said. \"We found we could use a single base model and just train one encoder per modality, so keeping it very simple and very parameter efficient, if we fed that overall architecture really, really good data.\"The resulting model rivals OmniBind, a much larger competitor in the multimodal space, but requires dramatically fewer computational resources for both training and inference. This makes EBind deployable in resource-constrained environments including edge devices for robotics and autonomous systems.The enterprise value of a multi-modal datasetMultimodal models enable enterprise use cases that span different data types.Most organizations store different data types in separate systems: documents in content management platforms, audio recordings in communication tools, training videos in learning management systems and structured data in databases. Multimodal models can search and retrieve across all of these simultaneously.\"Enterprises have all different types of data. They don&#x27;t just have documents. They have audio recordings, and they have training videos, and they have CSV files,\" Landau said. \"Let&#x27;s say you&#x27;re a lawyer and you have a case file that has video evidence and also documents and recordings, and it&#x27;s all scattered across a lot of silos of data. You can use EBind to pick all of the relevant data and bundle together to search and surface the right data much quicker than you would have before.\"The same principle applies across verticals. Healthcare providers can link patient imaging data to clinical notes and diagnostic audio. Financial services firms can connect transaction records to compliance call recordings and customer communications. Manufacturing operations can tie equipment sensor data to maintenance video logs and inspection reports.Beyond office environments, physical AI represents another frontier. Landau highlighted autonomous vehicles that benefit from both visual perception and audio cues like emergency sirens. In manufacturing and warehousing, robots that combine visual recognition with audio feedback and spatial awareness can operate more safely and effectively than vision-only systems.Enterprise use case: Extending computer vision with multimodal contextCaptur AI, an Encord customer, illustrates how companies are planning to use the dataset for specific business applications. The startup provides on-device image verification for mobile apps, validating photos in real-time for authenticity, compliance and quality before upload. The company works with shared mobility providers like Lime and delivery companies capturing billions of package photos.Captur AI processes over 100 million images on-device and specializes in distilling models to 6-10 megabytes so they can run on smartphones without cloud connectivity. But CEO Charlotte Bax sees multimodal capabilities as critical for expanding into higher-value use cases.\"The market for us is massive. You submit photos for returns and retails. You submit photos to insurance companies for claims. You submit photos when you&#x27;re listing something on eBay,\" Bax told VentureBeat in an exclusive interview. \"Some of those use cases are very high risk or high value if something goes wrong, like insurance, the image only captures part of the context and audio can be an important signal.\"Bax cited digital vehicle inspections as a prime example. When customers photograph vehicle damage for insurance claims, they often describe what happened verbally while capturing images. Audio context can significantly improve claim accuracy and reduce fraud.\"As you&#x27;re doing that, oftentimes the customer is actually describing what&#x27;s happened,\" Bax said. \"A few of our potential prospects in InsurTech have asked us if we can actually do audio as well, because then that adds this additional bit of context for the user who&#x27;s submitting the claim.\"The challenge lies in maintaining Captur AI&#x27;s core advantage: running models efficiently on-device rather than requiring cloud processing. The company plans to use Encord&#x27;s dataset to train compact multimodal models that preserve real-time, offline capabilities while adding audio and sequential image context.\"The most important thing you can do is try and get as much context as possible,\" Bax said. \"Can you get LLMs to be small enough to run on a device within the next three years, or can you run multimodal models on the device? Solving data quality before image upload is the interesting frontier.\"What this means for enterprisesEncord&#x27;s results challenge fundamental assumptions about AI development and suggest that the next competitive battleground may be data operations rather than infrastructure scale.Multimodal datasets unlock new capabilities. The ability to train models that understand relationships across data types opens use cases that single-modality systems cannot address.Data operations deserve equal investment with compute infrastructure. The 17x parameter efficiency gain from better data curation represents orders of magnitude in cost savings. Organizations pouring resources into GPU clusters while treating data quality as an afterthought may be optimizing the wrong variable.For enterprises building multimodal AI systems, Landau&#x27;s assessment captures the strategic shift. \"We were able to get to the same level of performance as models much larger, not because we were super clever on the architecture, but because we trained it with really good data overall,\" he said.",
          "feed_position": 3,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/79Sf3Ti0NPEcvRc7c5dqV9/695d4798edd273cafd650056133fdfe7/multimodal-dataset-smk.png"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/todays-best-ipad-deals-include-50-off-the-256gb-ipad-a16-150020379.html",
          "published_at": "Fri, 17 Oct 2025 12:33:09 +0000",
          "title": "Today's best iPad deals include $50 off the 256GB iPad A16",
          "standfirst": "We generally think Apple’s iPads are the best tablets for most people, but they usually don’t come cheap. To help those looking to grab one today get the most value possible, we’re keeping an eye on sale prices and rounding up the best iPad deals we can find each week.Unfortunately, after the barrage of discounts we saw during Prime Day and other retailer sales last week, the selection available now is back to being pretty light. Most models in the iPad, iPad Air and iPad mini families aren’t significantly discounted, and the new iPad Pros were only just announced on Wednesday, so there aren’t any deals of note for those just yet. Call it a pre-Black Friday lull. That said, a couple higher-capacity configurations of the Air and base iPad are still on sale, including the 256GB version of the iPad (A16) for $50 off. And beyond tablets, we’re also seeing healthy price drops for devices like the AirPods 4, AirTags and Mac mini. Here are all the top deals on Apple gear we could find this week. Best iPad deals Apple iPad Air (11-inch, M3, 1TB) for $949 ($150 off MSRP): The most recent iPad Air is a relatively minor update, as the only major addition is a more powerful M3 chip. However, we still recommend the Air over the base model in our iPad buying guide: Its display is laminated, more color-rich and better at fending off glare (though it's still 60Hz); its speakers are more robust; it works with Apple’s best accessories and its performance should hold up better in the years ahead. This deal is only for the maxed-out model with 1TB of storage, but it ties the lowest price we've seen all the same. Apple iPad Air (13-inch, M3, 512GB, Cellular) for $1,000 ($250 off): Engadget's Nate Ingraham gave the 13-inch iPad Air a score of 89 when it was released in March. It has a bigger and slightly brighter display than its 11-inch counterpart but is otherwise the same. If you plan to keep your iPad hooked up to a keyboard, the extra screen space is lovely for multitasking or just taking in movies. This discount is an all-time low, but it only applies to the 512GB model with built-in cellular support, so it's certainly not for everyone. Best Apple deals Apple AirTags (4-pack) for $65 ($34 off): We may see an updated model by the end of the year, but the current AirTags are the best Bluetooth trackers for iPhone owners right now thanks to their vast finding network and accurate ultra-wideband tech that makes it easy to locate nearby items. Just note that you'll need a separate AirTag holder to attach them to your keys, wallet or bag. This deal comes within a dollar of the lowest price we've seen for a four-pack. Also at Walmart. Apple Pencil Pro for $99 ($30 off): The top-end option in Apple’s confusing stylus lineup, the Pencil Pro supports pressure sensitivity, wireless charging, tilt detection, haptic feedback and Apple’s double tap and squeeze gestures, among other perks. It’s a lovely tool for more intricate sketching and note-taking, but the catch is that it’s only compatible with the M4 iPad Pro, M2 and M3 iPad Air and most recent iPad mini. We've seen this deal fairly often over the course of the year, but it's a fine discount compared to buying from Apple directly. Also at Walmart. Apple MacBook Air (13-inch, M4, 512GB) for $999 ($200 off): Apple's latest MacBook Air is the top pick in our guide to the best laptops, and it earned a score of 92 in our review. It's not a major overhaul, but the design is still exceptionally thin, light and well-built, with long battery life and a top-notch keyboard and trackpad. Now it's a bit faster. (Though we'd still love more ports and a refresh rate higher than 60Hz.) This discount ties the all-time low for the model with 16GB of RAM and a 512GB SSD. Apple Mac mini (M4) for $499 ($100 off): The newest version of Apple’s tiny desktop PC has a smaller overall footprint, a faster M4 chip, 16GB of RAM as standard (finally), two front-facing USB-C ports, an extra Thunderbolt 4 port and the ability to drive three external displays. It doesn't have any USB-A ports, however. We gave the M4 Pro model a review score of 90. This deal is for the entry-level version with a base M4 chip, 16GB of RAM and a 256GB SSD — we’ve seen it fall as low as $469 in the past, but this is still a decent savings. Also at Best Buy, Walmart and B&H. Apple iMac (M4) for $1,149 ($150 off): We like the M4 iMac as an all-in-one computer thanks to its powerful performance, standard 16GB of RAM and improved webcam. Just note that it only comes in a 24-inch screen size option. This deal on the base model isn't quite an all-time low, but it's roughly $40 lower than the desktop's usual street price and a decent savings compared to buying directly from Apple. Apple Watch Series 11 (GPS, 42mm) for $389 ($10 off): The latest flagship Apple Watch only hit store shelves last month, but Amazon is already selling it for $10 off. It doesn't show up as a percentage off, but you'll see some models listed at $389 instead of Apple's $399 MSRP. If you're new to Apple's wearables or are ready to upgrade from a Series 9 or older, this is a good model to grab. If you're coming from a Series 10, however, there's not much need to upgrade as the only major change from last year's model is a slightly larger battery and a tougher screen. Apple Watch SE 3 (GPS, 40mm) for $240 ($9 off): There's a similar stealth discount for the newest budget model, the Apple Watch SE 3, at Amazon. It normally goes for $249 — again, not a big discount, but better than nothing if you're looking to get onboard early. Apple gave this model some badly needed updates compared to its predecessor, including an always-on display, faster charging, better sensors and the same processor that you'll find in the new Apple Watch Series 11. Read more Apple coverage: The best AirPods The best Apple Watches The best MacBooks The best iPhones The best iPads Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/todays-best-ipad-deals-include-50-off-the-256gb-ipad-a16-150020379.html?src=rss",
          "content": "We generally think Apple’s iPads are the best tablets for most people, but they usually don’t come cheap. To help those looking to grab one today get the most value possible, we’re keeping an eye on sale prices and rounding up the best iPad deals we can find each week.Unfortunately, after the barrage of discounts we saw during Prime Day and other retailer sales last week, the selection available now is back to being pretty light. Most models in the iPad, iPad Air and iPad mini families aren’t significantly discounted, and the new iPad Pros were only just announced on Wednesday, so there aren’t any deals of note for those just yet. Call it a pre-Black Friday lull. That said, a couple higher-capacity configurations of the Air and base iPad are still on sale, including the 256GB version of the iPad (A16) for $50 off. And beyond tablets, we’re also seeing healthy price drops for devices like the AirPods 4, AirTags and Mac mini. Here are all the top deals on Apple gear we could find this week. Best iPad deals Apple iPad Air (11-inch, M3, 1TB) for $949 ($150 off MSRP): The most recent iPad Air is a relatively minor update, as the only major addition is a more powerful M3 chip. However, we still recommend the Air over the base model in our iPad buying guide: Its display is laminated, more color-rich and better at fending off glare (though it's still 60Hz); its speakers are more robust; it works with Apple’s best accessories and its performance should hold up better in the years ahead. This deal is only for the maxed-out model with 1TB of storage, but it ties the lowest price we've seen all the same. Apple iPad Air (13-inch, M3, 512GB, Cellular) for $1,000 ($250 off): Engadget's Nate Ingraham gave the 13-inch iPad Air a score of 89 when it was released in March. It has a bigger and slightly brighter display than its 11-inch counterpart but is otherwise the same. If you plan to keep your iPad hooked up to a keyboard, the extra screen space is lovely for multitasking or just taking in movies. This discount is an all-time low, but it only applies to the 512GB model with built-in cellular support, so it's certainly not for everyone. Best Apple deals Apple AirTags (4-pack) for $65 ($34 off): We may see an updated model by the end of the year, but the current AirTags are the best Bluetooth trackers for iPhone owners right now thanks to their vast finding network and accurate ultra-wideband tech that makes it easy to locate nearby items. Just note that you'll need a separate AirTag holder to attach them to your keys, wallet or bag. This deal comes within a dollar of the lowest price we've seen for a four-pack. Also at Walmart. Apple Pencil Pro for $99 ($30 off): The top-end option in Apple’s confusing stylus lineup, the Pencil Pro supports pressure sensitivity, wireless charging, tilt detection, haptic feedback and Apple’s double tap and squeeze gestures, among other perks. It’s a lovely tool for more intricate sketching and note-taking, but the catch is that it’s only compatible with the M4 iPad Pro, M2 and M3 iPad Air and most recent iPad mini. We've seen this deal fairly often over the course of the year, but it's a fine discount compared to buying from Apple directly. Also at Walmart. Apple MacBook Air (13-inch, M4, 512GB) for $999 ($200 off): Apple's latest MacBook Air is the top pick in our guide to the best laptops, and it earned a score of 92 in our review. It's not a major overhaul, but the design is still exceptionally thin, light and well-built, with long battery life and a top-notch keyboard and trackpad. Now it's a bit faster. (Though we'd still love more ports and a refresh rate higher than 60Hz.) This discount ties the all-time low for the model with 16GB of RAM and a 512GB SSD. Apple Mac mini (M4) for $499 ($100 off): The newest version of Apple’s tiny desktop PC has a smaller overall footprint, a faster M4 chip, 16GB of RAM as standard (finally), two front-facing USB-C ports, an extra Thunderbolt 4 port and the ability to drive three external displays. It doesn't have any USB-A ports, however. We gave the M4 Pro model a review score of 90. This deal is for the entry-level version with a base M4 chip, 16GB of RAM and a 256GB SSD — we’ve seen it fall as low as $469 in the past, but this is still a decent savings. Also at Best Buy, Walmart and B&H. Apple iMac (M4) for $1,149 ($150 off): We like the M4 iMac as an all-in-one computer thanks to its powerful performance, standard 16GB of RAM and improved webcam. Just note that it only comes in a 24-inch screen size option. This deal on the base model isn't quite an all-time low, but it's roughly $40 lower than the desktop's usual street price and a decent savings compared to buying directly from Apple. Apple Watch Series 11 (GPS, 42mm) for $389 ($10 off): The latest flagship Apple Watch only hit store shelves last month, but Amazon is already selling it for $10 off. It doesn't show up as a percentage off, but you'll see some models listed at $389 instead of Apple's $399 MSRP. If you're new to Apple's wearables or are ready to upgrade from a Series 9 or older, this is a good model to grab. If you're coming from a Series 10, however, there's not much need to upgrade as the only major change from last year's model is a slightly larger battery and a tougher screen. Apple Watch SE 3 (GPS, 40mm) for $240 ($9 off): There's a similar stealth discount for the newest budget model, the Apple Watch SE 3, at Amazon. It normally goes for $249 — again, not a big discount, but better than nothing if you're looking to get onboard early. Apple gave this model some badly needed updates compared to its predecessor, including an always-on display, faster charging, better sensors and the same processor that you'll find in the new Apple Watch Series 11. Read more Apple coverage: The best AirPods The best Apple Watches The best MacBooks The best iPhones The best iPads Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/todays-best-ipad-deals-include-50-off-the-256gb-ipad-a16-150020379.html?src=rss",
          "feed_position": 15
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/general/the-morning-after-engadget-newsletter-111523653.html",
          "published_at": "Fri, 17 Oct 2025 11:15:23 +0000",
          "title": "The Morning After: Apple adds its new M5 chip to iPads, MacBooks and even the Vision Pro",
          "standfirst": "This week, Apple announced fall hardware updates across multiple devices — pretty much every major category, besides iPhones and AirPods. Don’t get too excited: It’s not a redesign reveal, but we’re expecting a tangible performance jump for both the iPad Pro and MacBook Pro. With the new M5 chip (no Pro or Max versions so far), Apple used the same 3-nanometer fabrication process for the M5 as it did for the M4. The new chip has 10 GPU cores and 10 CPU cores, along with a 16-core Neural Engine. Apple claims the M5 has the “world’s fastest CPU core” with up to 20 percent faster multithreaded performance compared to the M4 chip of the previous MacBook Pro. Graphics performance also gets a significant boost too. The M5 MacBook Pro ($1,599), otherwise, has identical specs to its M4 predecessor, right down to the same dimensions, weight and 70-watt power adapter. Meanwhile, inside the iPad Pro, Apple claims it has more than four times the peak GPU compute performance of the M4. If you’re looking to use the new iPad Pro for video tasks, Apple says that video transcoding is six times faster than the old M1 iPad Pro from 2021. The 11-inch iPad Pro starts at $999 for the 11-inch model and $1,299 for the 13-inch model. And it seems to have a lot of M5 chips to use: The Vision Pro gets one and a seemingly more comfortable Dual Knit Band. The M5 Vision Pro should last half an hour longer than the original model, as well, according to Apple. — Mat Smith Get Engadget's newsletter delivered direct to your inbox. Subscribe right here! The news you might have missed The US Mint is putting Steve Jobs on a $1 coin Ball x Pit’s deeply satisfying grind keeps me coming back for more Apple’s M6 MacBook Pro generation will reportedly offer touchscreens The Honor ‘robot’ phone Less robot, more arm. Honor Chinese phone maker Honor says its next phone will feature a camera on a pop-out mechanical arm. Talking to CNBC, Honor said it will be a robot phone, framing it around AI innovation — something the company is throwing millions of dollars at. I enjoy that its camera arm reminds me of the ubiquitous DJI Osmo Pocket 3, beloved by bloggers, creators and tourists that get in my way. If its foldout camera can track, stabilize video footage and focus on its own, it could be a cool feature. The camera seems to fold away inside the back of the future device, but can it be used while tucked away there? We don’t know. What are these amazing future AI experiences? No idea. Questions, questions, questions. Honor said it plans to share more details at Mobile World Congress in Barcelona early next year. Continue reading. ROG Xbox Ally X handheld gaming PC review Not sure if this is an Xbox. Engadget The co-creation handheld from ASUS ROG and Xbox is here. The Ally X is arguably the best handheld console for Xbox games yet. It’s not just bigger grips and familiar button layouts, but they do help. No, the bigger evolution is how Microsoft has finessed the UI and software, making it more console-like and less like you need a mouse to navigate everything. Perhaps most importantly, when the ROG Xbox Ally X costs $1,000, the AMD Ryzen Z2 Extreme chip offers a lot of power, enough to handle rich flagship games, with some setting tinkering. If you want the best performance from the Ally X, you’ll need to plug it into the wall. Check out our detailed review.This article originally appeared on Engadget at https://www.engadget.com/general/the-morning-after-engadget-newsletter-111523653.html?src=rss",
          "content": "This week, Apple announced fall hardware updates across multiple devices — pretty much every major category, besides iPhones and AirPods. Don’t get too excited: It’s not a redesign reveal, but we’re expecting a tangible performance jump for both the iPad Pro and MacBook Pro. With the new M5 chip (no Pro or Max versions so far), Apple used the same 3-nanometer fabrication process for the M5 as it did for the M4. The new chip has 10 GPU cores and 10 CPU cores, along with a 16-core Neural Engine. Apple claims the M5 has the “world’s fastest CPU core” with up to 20 percent faster multithreaded performance compared to the M4 chip of the previous MacBook Pro. Graphics performance also gets a significant boost too. The M5 MacBook Pro ($1,599), otherwise, has identical specs to its M4 predecessor, right down to the same dimensions, weight and 70-watt power adapter. Meanwhile, inside the iPad Pro, Apple claims it has more than four times the peak GPU compute performance of the M4. If you’re looking to use the new iPad Pro for video tasks, Apple says that video transcoding is six times faster than the old M1 iPad Pro from 2021. The 11-inch iPad Pro starts at $999 for the 11-inch model and $1,299 for the 13-inch model. And it seems to have a lot of M5 chips to use: The Vision Pro gets one and a seemingly more comfortable Dual Knit Band. The M5 Vision Pro should last half an hour longer than the original model, as well, according to Apple. — Mat Smith Get Engadget's newsletter delivered direct to your inbox. Subscribe right here! The news you might have missed The US Mint is putting Steve Jobs on a $1 coin Ball x Pit’s deeply satisfying grind keeps me coming back for more Apple’s M6 MacBook Pro generation will reportedly offer touchscreens The Honor ‘robot’ phone Less robot, more arm. Honor Chinese phone maker Honor says its next phone will feature a camera on a pop-out mechanical arm. Talking to CNBC, Honor said it will be a robot phone, framing it around AI innovation — something the company is throwing millions of dollars at. I enjoy that its camera arm reminds me of the ubiquitous DJI Osmo Pocket 3, beloved by bloggers, creators and tourists that get in my way. If its foldout camera can track, stabilize video footage and focus on its own, it could be a cool feature. The camera seems to fold away inside the back of the future device, but can it be used while tucked away there? We don’t know. What are these amazing future AI experiences? No idea. Questions, questions, questions. Honor said it plans to share more details at Mobile World Congress in Barcelona early next year. Continue reading. ROG Xbox Ally X handheld gaming PC review Not sure if this is an Xbox. Engadget The co-creation handheld from ASUS ROG and Xbox is here. The Ally X is arguably the best handheld console for Xbox games yet. It’s not just bigger grips and familiar button layouts, but they do help. No, the bigger evolution is how Microsoft has finessed the UI and software, making it more console-like and less like you need a mouse to navigate everything. Perhaps most importantly, when the ROG Xbox Ally X costs $1,000, the AMD Ryzen Z2 Extreme chip offers a lot of power, enough to handle rich flagship games, with some setting tinkering. If you want the best performance from the Ally X, you’ll need to plug it into the wall. Check out our detailed review.This article originally appeared on Engadget at https://www.engadget.com/general/the-morning-after-engadget-newsletter-111523653.html?src=rss",
          "feed_position": 19,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2025-10/3f83ffb0-ab3f-11f0-bfff-d212cd1e7880"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/mobile/smartphones/best-android-phone-130030805.html",
          "published_at": "Fri, 17 Oct 2025 09:00:36 +0000",
          "title": "The best Android phones for 2025",
          "standfirst": "Choosing the best Android phone can feel overwhelming as there are so many options from so many brands, it’s hard to know where to start. Unlike Apple, which sticks to its sleek lineup of iPhones, Android offers a world of variety. Whether you're eyeing the latest flagship from Samsung, a budget-friendly smartphone from Motorola or something unique with a foldable design, there’s an Android device out there to suit your needs.The beauty of Android is its flexibility. You’ll find phones with different screen sizes, camera setups, battery life and even quirky extras like stylus support or rugged builds. Plus, Android lets you customize your device to your heart's content – something Apple fans might envy. We’ve tested and researched the top Android phones to help you find the right one for your budget, lifestyle, and tech preferences. Best Android phones for 2025 Other Android phones we tested Google Pixel 10 Pro Fold While the design and performance of the Galaxy Z Fold 7 is so good that we had to pick it as our favorite foldable of this generation, the Pixel 10 Pro Fold isn’t that far behind. Sure, it’s bigger and bulkier, but it still has the best cameras on any foldable phone along with better software and a larger battery. But perhaps most importantly, it now has a proper IP68 rating for dust and water resistance — something you won’t find on any of its rivals. This could save the phone from an early demise and prevent a lot of headaches if you frequent the beach or pretty much anywhere with little particles that could threaten the insides of your device. What to look for in a new Android phone Performance When it comes to picking our favorite Android phones, the main things we look for are pretty straightforward: good performance (both compute and AI), a nice display, solid design, sharp cameras, long battery life and a significant commitment to ongoing software support. For performance, not only do we look at benchmarks and other metrics, but we also evaluate phones based on responsiveness. Regardless of whether you’re reading, text messaging, scrolling through social media or playing a game, no one wants a gadget that feels sluggish. Display When it comes to displays, we generally prefer OLED panels that can produce rich, saturated colors with at least 600 nits of brightness, though many of our top mid-range and high-end phones can hit 1,000 nits or more. And more recently, most of our favorite devices also support screens with fast refresh rates of 90Hz or 120Hz, which adds an extra level of smoothness and fluidity. Design Now we will admit there is a bit of subjectivity when deciding which phones look the best, but there are other design aspects like dust and water resistance or screen durability that can make a big difference to long-term survival. It’s also important to consider things like support for wireless charging, power sharing (aka reverse wireless charging) and UWB connectivity, which can have an impact on how your phone interacts with your other devices. Cameras Obviously, for photos we’re looking for sharp, colorful shots in both bright and low-light conditions. And we want video clips with high dynamic range, rich audio and smooth image stabilization. Extra cameras for ultra-wide and telephoto lenses are a plus. The best cameras should also include features like dedicated night modes, support for various video recording resolutions, and additional photo modes like timelapse, slow motion and more. Battery and software Finally, in terms of longevity, we’re looking for all-day battery life on devices that also delivered great results on our local video rundown test (at least 16 hours on a charge, but more is obviously better). Wireless charging capabilities have become almost ubiquitous over the past few years, and most of our top picks have this extra perk. Fast-charging is available on some Android phones, too. Finally, with people holding onto their phones longer than ever, we like to see companies commit to at least three years of software support, upgrades and regular security updates.This article originally appeared on Engadget at https://www.engadget.com/mobile/smartphones/best-android-phone-130030805.html?src=rss",
          "content": "Choosing the best Android phone can feel overwhelming as there are so many options from so many brands, it’s hard to know where to start. Unlike Apple, which sticks to its sleek lineup of iPhones, Android offers a world of variety. Whether you're eyeing the latest flagship from Samsung, a budget-friendly smartphone from Motorola or something unique with a foldable design, there’s an Android device out there to suit your needs.The beauty of Android is its flexibility. You’ll find phones with different screen sizes, camera setups, battery life and even quirky extras like stylus support or rugged builds. Plus, Android lets you customize your device to your heart's content – something Apple fans might envy. We’ve tested and researched the top Android phones to help you find the right one for your budget, lifestyle, and tech preferences. Best Android phones for 2025 Other Android phones we tested Google Pixel 10 Pro Fold While the design and performance of the Galaxy Z Fold 7 is so good that we had to pick it as our favorite foldable of this generation, the Pixel 10 Pro Fold isn’t that far behind. Sure, it’s bigger and bulkier, but it still has the best cameras on any foldable phone along with better software and a larger battery. But perhaps most importantly, it now has a proper IP68 rating for dust and water resistance — something you won’t find on any of its rivals. This could save the phone from an early demise and prevent a lot of headaches if you frequent the beach or pretty much anywhere with little particles that could threaten the insides of your device. What to look for in a new Android phone Performance When it comes to picking our favorite Android phones, the main things we look for are pretty straightforward: good performance (both compute and AI), a nice display, solid design, sharp cameras, long battery life and a significant commitment to ongoing software support. For performance, not only do we look at benchmarks and other metrics, but we also evaluate phones based on responsiveness. Regardless of whether you’re reading, text messaging, scrolling through social media or playing a game, no one wants a gadget that feels sluggish. Display When it comes to displays, we generally prefer OLED panels that can produce rich, saturated colors with at least 600 nits of brightness, though many of our top mid-range and high-end phones can hit 1,000 nits or more. And more recently, most of our favorite devices also support screens with fast refresh rates of 90Hz or 120Hz, which adds an extra level of smoothness and fluidity. Design Now we will admit there is a bit of subjectivity when deciding which phones look the best, but there are other design aspects like dust and water resistance or screen durability that can make a big difference to long-term survival. It’s also important to consider things like support for wireless charging, power sharing (aka reverse wireless charging) and UWB connectivity, which can have an impact on how your phone interacts with your other devices. Cameras Obviously, for photos we’re looking for sharp, colorful shots in both bright and low-light conditions. And we want video clips with high dynamic range, rich audio and smooth image stabilization. Extra cameras for ultra-wide and telephoto lenses are a plus. The best cameras should also include features like dedicated night modes, support for various video recording resolutions, and additional photo modes like timelapse, slow motion and more. Battery and software Finally, in terms of longevity, we’re looking for all-day battery life on devices that also delivered great results on our local video rundown test (at least 16 hours on a charge, but more is obviously better). Wireless charging capabilities have become almost ubiquitous over the past few years, and most of our top picks have this extra perk. Fast-charging is available on some Android phones, too. Finally, with people holding onto their phones longer than ever, we like to see companies commit to at least three years of software support, upgrades and regular security updates.This article originally appeared on Engadget at https://www.engadget.com/mobile/smartphones/best-android-phone-130030805.html?src=rss",
          "feed_position": 22,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2023-02/54f98de0-a87e-11ed-afd1-54c1e8d571fc"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/researchers-find-adding-this-one-simple-sentence-to-prompts-makes-ai-models",
          "published_at": "Fri, 17 Oct 2025 02:40:00 GMT",
          "title": "Researchers find adding this one simple sentence to prompts makes AI models way more creative",
          "standfirst": "One of the coolest things about generative AI models — both large language models (LLMs) and diffusion-based image generators — is that they are \"non-deterministic.\" That is, despite their reputation among some critics as being \"fancy autocorrect,\" generative AI models actually generate their outputs by choosing from a distribution of the most probable next tokens (units of information) to fill out their response.Asking an LLM: \"What is the capital of France?\" will have it sample its probability distribution for France, capitals, cities, etc. to arrive at the answer \"Paris.\" But that answer could come in the format of \"The capital of France is Paris,\" or simply \"Paris\" or \"Paris, though it was Versailles at one point.\" Still, those of us that use these models frequently day-to-day will note that sometimes, their answers can feel annoyingly repetitive or similar. A common joke about coffee is recycled across generations of queries. Story prompts generate similar arcs. Even tasks that should yield many plausible answers—like naming U.S. states—tend to collapse into only a few. This phenomenon, known as mode collapse, arises during post-training alignment and limits the usefulness of otherwise powerful models.Especially when using LLMs to generate new creative works in writing, communications, strategy, or illustrations, we actually want their outputs to be even more varied than they already are. Now a team of researchers at Northeastern University, Stanford University and West Virginia University have come up with an ingenuously simple method to get language and image models to generate a wider variety of responses to nearly any user prompt by adding a single, simple sentence: \"Generate 5 responses with their corresponding probabilities, sampled from the full distribution.\"The method, called Verbalized Sampling (VS), helps models like GPT-4, Claude, and Gemini produce more diverse and human-like outputs—without retraining or access to internal parameters. It is described in a paper published on the open access journal arxiv.org online in early October 2025.When prompted in this way, the model no longer defaults to its safest, most typical output. Instead, it verbalizes its internal distribution over potential completions and samples across a wider spectrum of possibilities. This one-line change leads to substantial gains in output diversity across multiple domains.As Weiyan Shi, an assistant professor at Northeastern University and co-author of the paper, wrote on X: \"LLMs&#x27; potentials are not fully unlocked yet! As shown in our paper, prompt optimization can be guided by thinking about how LLMs are trained and aligned, and can be proved theoretically.\"Why Models Collapse—and How VS Reverses ItAccording to the research team, the root cause of mode collapse lies not just in algorithms like reinforcement learning from human feedback (RLHF), but in the structure of human preferences. People tend to rate more familiar or typical answers as better, which nudges LLMs toward “safe” choices over diverse ones during fine-tuning.However, this bias doesn’t erase the model’s underlying knowledge—it just suppresses it. VS works by bypassing this suppression. Instead of asking for the single most likely output, it invites the model to reveal a set of plausible responses and their relative probabilities. This distribution-level prompting restores access to the richer diversity present in the base pretraining model.Real-World Performance Across TasksThe research team tested Verbalized Sampling across several common use cases:Creative Writing: In story generation, VS increased diversity scores by up to 2.1× compared to standard prompting, while maintaining quality. One story prompt—“Without a goodbye”—produced formulaic breakup scenes under direct prompting, but yielded narratives involving cosmic events, silent emails, and music stopping mid-dance when prompted via VS.Dialogue Simulation: In persuasive dialogue tasks, VS enabled models to simulate human-like patterns, such as hesitation, resistance, and changes of mind. Donation behavior distributions under VS better aligned with real human data compared to baseline methods.Open-ended QA: When asked to enumerate valid answers (e.g., naming U.S. states), models using VS generated responses that more closely matched the diversity of real-world data. They covered a broader set of answers without sacrificing factual accuracy.Synthetic Data Generation: When used to generate math problems for model training, VS created more varied datasets. These, in turn, improved downstream performance in competitive math benchmarks, outperforming synthetic data generated via direct prompting.Tunable Diversity and Better Use of Larger ModelsA notable advantage of VS is its tunability. Users can set a probability threshold in the prompt to sample from lower-probability “tails” of the model’s distribution. Lower thresholds correspond to higher diversity. This tuning can be done via prompt text alone, without changing any decoding settings like temperature or top-p.In one test using the Gemini-2.5-Flash model, diversity in story writing increased steadily as the probability threshold dropped from 1 to 0.001. The chart accompanying the study showed VS outperforming both direct and sequence-based prompting across all thresholds.Interestingly, the method scales well with model size. Larger models like GPT-4.1 and Claude-4 showed even greater gains from VS compared to smaller ones. While smaller models benefitted, the improvement in diversity was roughly 1.5–2× stronger in larger counterparts—suggesting VS helps unlock more of the latent capabilities in advanced models.Deployment and AvailabilityThe Verbalized Sampling method is available now as a Python package:pip install verbalized-samplingThe package includes integration with LangChain and supports a simple interface for sampling from the verbalized distribution. Users can also adjust parameters like k (number of responses), thresholds, and temperature to suit their applications. A live Colab notebook and documentation are available under an enterprise friendly Apache 2.0 license on GitHub at: https://github.com/CHATS-lab/verbalized-samplingPractical Tips and Common IssuesWhile the method works across all major LLMs, some users may initially encounter refusals or errors. In these cases, the authors suggest using the system prompt version of the template or referring to alternative formats listed on the GitHub page. Some models interpret complex instructions as jailbreak attempts and refuse to comply unless the structure is clearer.For example, prompting via a system-level instruction like this improves reliability:You are a helpful assistant. For each query, generate five responses within separate tags, each with a probability below 0.10.This small change typically resolves any issues.A Lightweight Fix for a Big ProblemVerbalized Sampling represents a practical, inference-time fix to a deep limitation in how modern language models behave. It doesn’t require model retraining or internal access. It is not dependent on any one model family. And it improves not only the diversity of outputs, but their quality—as judged by both human evaluation and benchmark scores.With growing interest in tools that enhance model creativity, VS is likely to see rapid adoption in domains like writing, design, simulation, education, and synthetic data generation.For users and developers frustrated by the sameness of LLM responses, the fix may be as simple as changing the question.",
          "content": "One of the coolest things about generative AI models — both large language models (LLMs) and diffusion-based image generators — is that they are \"non-deterministic.\" That is, despite their reputation among some critics as being \"fancy autocorrect,\" generative AI models actually generate their outputs by choosing from a distribution of the most probable next tokens (units of information) to fill out their response.Asking an LLM: \"What is the capital of France?\" will have it sample its probability distribution for France, capitals, cities, etc. to arrive at the answer \"Paris.\" But that answer could come in the format of \"The capital of France is Paris,\" or simply \"Paris\" or \"Paris, though it was Versailles at one point.\" Still, those of us that use these models frequently day-to-day will note that sometimes, their answers can feel annoyingly repetitive or similar. A common joke about coffee is recycled across generations of queries. Story prompts generate similar arcs. Even tasks that should yield many plausible answers—like naming U.S. states—tend to collapse into only a few. This phenomenon, known as mode collapse, arises during post-training alignment and limits the usefulness of otherwise powerful models.Especially when using LLMs to generate new creative works in writing, communications, strategy, or illustrations, we actually want their outputs to be even more varied than they already are. Now a team of researchers at Northeastern University, Stanford University and West Virginia University have come up with an ingenuously simple method to get language and image models to generate a wider variety of responses to nearly any user prompt by adding a single, simple sentence: \"Generate 5 responses with their corresponding probabilities, sampled from the full distribution.\"The method, called Verbalized Sampling (VS), helps models like GPT-4, Claude, and Gemini produce more diverse and human-like outputs—without retraining or access to internal parameters. It is described in a paper published on the open access journal arxiv.org online in early October 2025.When prompted in this way, the model no longer defaults to its safest, most typical output. Instead, it verbalizes its internal distribution over potential completions and samples across a wider spectrum of possibilities. This one-line change leads to substantial gains in output diversity across multiple domains.As Weiyan Shi, an assistant professor at Northeastern University and co-author of the paper, wrote on X: \"LLMs&#x27; potentials are not fully unlocked yet! As shown in our paper, prompt optimization can be guided by thinking about how LLMs are trained and aligned, and can be proved theoretically.\"Why Models Collapse—and How VS Reverses ItAccording to the research team, the root cause of mode collapse lies not just in algorithms like reinforcement learning from human feedback (RLHF), but in the structure of human preferences. People tend to rate more familiar or typical answers as better, which nudges LLMs toward “safe” choices over diverse ones during fine-tuning.However, this bias doesn’t erase the model’s underlying knowledge—it just suppresses it. VS works by bypassing this suppression. Instead of asking for the single most likely output, it invites the model to reveal a set of plausible responses and their relative probabilities. This distribution-level prompting restores access to the richer diversity present in the base pretraining model.Real-World Performance Across TasksThe research team tested Verbalized Sampling across several common use cases:Creative Writing: In story generation, VS increased diversity scores by up to 2.1× compared to standard prompting, while maintaining quality. One story prompt—“Without a goodbye”—produced formulaic breakup scenes under direct prompting, but yielded narratives involving cosmic events, silent emails, and music stopping mid-dance when prompted via VS.Dialogue Simulation: In persuasive dialogue tasks, VS enabled models to simulate human-like patterns, such as hesitation, resistance, and changes of mind. Donation behavior distributions under VS better aligned with real human data compared to baseline methods.Open-ended QA: When asked to enumerate valid answers (e.g., naming U.S. states), models using VS generated responses that more closely matched the diversity of real-world data. They covered a broader set of answers without sacrificing factual accuracy.Synthetic Data Generation: When used to generate math problems for model training, VS created more varied datasets. These, in turn, improved downstream performance in competitive math benchmarks, outperforming synthetic data generated via direct prompting.Tunable Diversity and Better Use of Larger ModelsA notable advantage of VS is its tunability. Users can set a probability threshold in the prompt to sample from lower-probability “tails” of the model’s distribution. Lower thresholds correspond to higher diversity. This tuning can be done via prompt text alone, without changing any decoding settings like temperature or top-p.In one test using the Gemini-2.5-Flash model, diversity in story writing increased steadily as the probability threshold dropped from 1 to 0.001. The chart accompanying the study showed VS outperforming both direct and sequence-based prompting across all thresholds.Interestingly, the method scales well with model size. Larger models like GPT-4.1 and Claude-4 showed even greater gains from VS compared to smaller ones. While smaller models benefitted, the improvement in diversity was roughly 1.5–2× stronger in larger counterparts—suggesting VS helps unlock more of the latent capabilities in advanced models.Deployment and AvailabilityThe Verbalized Sampling method is available now as a Python package:pip install verbalized-samplingThe package includes integration with LangChain and supports a simple interface for sampling from the verbalized distribution. Users can also adjust parameters like k (number of responses), thresholds, and temperature to suit their applications. A live Colab notebook and documentation are available under an enterprise friendly Apache 2.0 license on GitHub at: https://github.com/CHATS-lab/verbalized-samplingPractical Tips and Common IssuesWhile the method works across all major LLMs, some users may initially encounter refusals or errors. In these cases, the authors suggest using the system prompt version of the template or referring to alternative formats listed on the GitHub page. Some models interpret complex instructions as jailbreak attempts and refuse to comply unless the structure is clearer.For example, prompting via a system-level instruction like this improves reliability:You are a helpful assistant. For each query, generate five responses within separate tags, each with a probability below 0.10.This small change typically resolves any issues.A Lightweight Fix for a Big ProblemVerbalized Sampling represents a practical, inference-time fix to a deep limitation in how modern language models behave. It doesn’t require model retraining or internal access. It is not dependent on any one model family. And it improves not only the diversity of outputs, but their quality—as judged by both human evaluation and benchmark scores.With growing interest in tools that enhance model creativity, VS is likely to see rapid adoption in domains like writing, design, simulation, education, and synthetic data generation.For users and developers frustrated by the sameness of LLM responses, the fix may be as simple as changing the question.",
          "feed_position": 4,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/6muOslUgOQCou8M10zFbHv/b0015ffd123a52616f65bc2a26b14b05/cfr0z3n_Simple_refined_corporate_memphis_flat_illustration_isom_73fd3864-fe47-4408-bdec-76e8596ca810.png"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/the-crew-2-is-now-playable-offline-211629508.html",
          "published_at": "Thu, 16 Oct 2025 21:16:29 +0000",
          "title": "The Crew 2 is now playable offline",
          "standfirst": "The Crew 2 was updated today to include Hybrid Mode, adding an offline mode to the driving game. Online and offline modes are separate saves, so if you snag a sweet ride while playing offline, it won't be available the next time you join an online session. Players will have the option to re-export their online save to the offline one, but it will overwrite and erase all offline-only progress. Multiplayer content, user-generated content, LIVE Summits and Crew Credits purchases will not be available in the offline mode. It's a bare-bones option, but the addition of offline mode is a welcome move from Ubisoft. The company's decision to delete The Crew from players' libraries after servers for the online game shut down sparked some big debates about ownership and preservation. One of the notable voices emerging from that conversation was the Stop Killing Games movement, which is pushing for EU legislation to ensure access to games even after their devs stop supporting a project.Having a way to continue accessing The Crew 2 even if (or more likely when) the game loses online support was something the developer had promised, so it's nice to have Ubisoft follow through. \"Whether you're looking to preserve your progression for the future or simply enjoy the freedom of playing without a connection, Hybrid Mode ensures The Crew 2 remains accessible for years to come,\" the company said in the blog post.This article originally appeared on Engadget at https://www.engadget.com/the-crew-2-is-now-playable-offline-211629508.html?src=rss",
          "content": "The Crew 2 was updated today to include Hybrid Mode, adding an offline mode to the driving game. Online and offline modes are separate saves, so if you snag a sweet ride while playing offline, it won't be available the next time you join an online session. Players will have the option to re-export their online save to the offline one, but it will overwrite and erase all offline-only progress. Multiplayer content, user-generated content, LIVE Summits and Crew Credits purchases will not be available in the offline mode. It's a bare-bones option, but the addition of offline mode is a welcome move from Ubisoft. The company's decision to delete The Crew from players' libraries after servers for the online game shut down sparked some big debates about ownership and preservation. One of the notable voices emerging from that conversation was the Stop Killing Games movement, which is pushing for EU legislation to ensure access to games even after their devs stop supporting a project.Having a way to continue accessing The Crew 2 even if (or more likely when) the game loses online support was something the developer had promised, so it's nice to have Ubisoft follow through. \"Whether you're looking to preserve your progression for the future or simply enjoy the freedom of playing without a connection, Hybrid Mode ensures The Crew 2 remains accessible for years to come,\" the company said in the blog post.This article originally appeared on Engadget at https://www.engadget.com/the-crew-2-is-now-playable-offline-211629508.html?src=rss",
          "feed_position": 25
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/oxygenos-16-has-new-lock-screen-customization-options-and-a-novel-gemini-integration-184958404.html",
          "published_at": "Thu, 16 Oct 2025 18:49:58 +0000",
          "title": "OxygenOS 16 has new lock screen customization options and a novel Gemini integration",
          "standfirst": "OnePlus has finally shown off its take on Android 16. OxygenOS 16, first shipping on the upcoming OnePlus 15, combines the new customization options of Android, with smoother animations and a take on AI that seems directly lifted from Nothing OS.The centerpiece of OxygenOS 16 is a deeper integration between OnePlus' \"Mind Space\" app and Google Gemini. Mind Space debuted alongside the Plus Key — the replacement for OnePlus' classic Alert Slider — on some OnePlus 13 phones earlier this year. Like Nothing's Essential Space, it captures screenshots and voice memos and automatically sorts them into folders you can refer to later. The big innovation of OxygenOS 16 is that you can now ask Gemini to refer to content in Mind Space to personalize responses. The idea being that the added context will make the AI assistant's responses more helpful.New features being introduced with OxygenOS 16.OnePlusOnePlus is also hopping on the AI writing and photo editing bandwagon. AI Writer in OxygenOS 16 can convert text into mind maps and tables with a few taps, and also generate social media captions based on a photo. The usual options for proofreading and summarizing text are also built-in. For photos, OnePlus is adding what it calls AI Portrait Glow to make faces visible even in poor lighting conditions and AI Perfect Shot, which appears to combine multiple photos to generate a single image where everyone's eyes are open, like Google's Best Take feature.Beyond those AI-enabled features, OnePlus says it's also improving customization options and animations across OxygenOS. With OxygenOS 16 you'll be able to customize your lock screen with a variety different fonts and layouts, including the option to convert a still image into an animated GIF or use a video lock screen. On your home screen, OxygenOS 16 is also getting a collection of new widgets and the ability to scale app icons — another idea present in Nothing OS. Opening and moving between apps should also feel smoother thanks to new, speedier animations and an update to how the OS loads content. Essentially, with Parallel Processing 2.0, OxygenOS 16 \"allows new animations to begin before previous actions complete,\" which is supposed to make everything feel more fluid.Those are just the highlights of OxygenOS 16, which also includes an expansion of the tablet multitasking system OnePlus uses on its OnePlus Pad tablets, and new connectivity options that let you mirror your phone screen to macOS or Windows computers.OxygenOS 16 will be released alongside the OnePlus 15, which doesn't have a release date, but is expected to launch this fall. The new OS update will also be available on recent OnePlus devices starting in November 2025, like the OnePlus 13, OnePlus Pad 3 and OnePlus Open. A full list of compatible devices is available on OnePlus' website.This article originally appeared on Engadget at https://www.engadget.com/oxygenos-16-has-new-lock-screen-customization-options-and-a-novel-gemini-integration-184958404.html?src=rss",
          "content": "OnePlus has finally shown off its take on Android 16. OxygenOS 16, first shipping on the upcoming OnePlus 15, combines the new customization options of Android, with smoother animations and a take on AI that seems directly lifted from Nothing OS.The centerpiece of OxygenOS 16 is a deeper integration between OnePlus' \"Mind Space\" app and Google Gemini. Mind Space debuted alongside the Plus Key — the replacement for OnePlus' classic Alert Slider — on some OnePlus 13 phones earlier this year. Like Nothing's Essential Space, it captures screenshots and voice memos and automatically sorts them into folders you can refer to later. The big innovation of OxygenOS 16 is that you can now ask Gemini to refer to content in Mind Space to personalize responses. The idea being that the added context will make the AI assistant's responses more helpful.New features being introduced with OxygenOS 16.OnePlusOnePlus is also hopping on the AI writing and photo editing bandwagon. AI Writer in OxygenOS 16 can convert text into mind maps and tables with a few taps, and also generate social media captions based on a photo. The usual options for proofreading and summarizing text are also built-in. For photos, OnePlus is adding what it calls AI Portrait Glow to make faces visible even in poor lighting conditions and AI Perfect Shot, which appears to combine multiple photos to generate a single image where everyone's eyes are open, like Google's Best Take feature.Beyond those AI-enabled features, OnePlus says it's also improving customization options and animations across OxygenOS. With OxygenOS 16 you'll be able to customize your lock screen with a variety different fonts and layouts, including the option to convert a still image into an animated GIF or use a video lock screen. On your home screen, OxygenOS 16 is also getting a collection of new widgets and the ability to scale app icons — another idea present in Nothing OS. Opening and moving between apps should also feel smoother thanks to new, speedier animations and an update to how the OS loads content. Essentially, with Parallel Processing 2.0, OxygenOS 16 \"allows new animations to begin before previous actions complete,\" which is supposed to make everything feel more fluid.Those are just the highlights of OxygenOS 16, which also includes an expansion of the tablet multitasking system OnePlus uses on its OnePlus Pad tablets, and new connectivity options that let you mirror your phone screen to macOS or Windows computers.OxygenOS 16 will be released alongside the OnePlus 15, which doesn't have a release date, but is expected to launch this fall. The new OS update will also be available on recent OnePlus devices starting in November 2025, like the OnePlus 13, OnePlus Pad 3 and OnePlus Open. A full list of compatible devices is available on OnePlus' website.This article originally appeared on Engadget at https://www.engadget.com/oxygenos-16-has-new-lock-screen-customization-options-and-a-novel-gemini-integration-184958404.html?src=rss",
          "feed_position": 30,
          "image_url": "https://d29szjachogqwa.cloudfront.net/videos/user-uploaded/OxygenOS_16_Image.jpg"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/entertainment/streaming/why-does-amazon-need-five-tv-streamers-163014576.html",
          "published_at": "Thu, 16 Oct 2025 16:30:14 +0000",
          "title": "Why does Amazon need five TV streamers?",
          "standfirst": "What’s in a name? Apparently quite a bit, according to Amazon. The company recently announced yet another change to its Fire TV devices lineup, which just means they renamed a few things yet again. The family now includes the $35 Fire TV Stick HD, the $40 Fire TV Stick 4K Select, the $50 Fire TV Stick 4K Plus, the $60 Fire TV Stick 4K Max and the $140 Fire TV Cube. That was a pain to type out and probably a pain to read (my apologies). Two of those devices were “rebranded” previously within the past year, so if you’re confused, you’re likely not alone. What’s a humble shopper to do when you’re trying to decide which is the best (and budget-friendly) option to upgrade an old TV so you can binge-watch Hunting Wives and ask Alexa about tomorrow’s weather forecast? I’ll make your decision quite easy: just get the Fire TV Stick 4K Max. Really, if you’re looking for the best streaming device, period, we recommend turning to Google for that. But if you’ve decided Amazon’s Fire TV lineup is where you want to spend your money, the 4K Max is the best option of the bunch. Not only has it stuck around without being subject to a “rebrand” for quite some time, but it also has arguably the best balance of features and price of any Fire TV streaming device. The Fire TV Stick 4K Max gives you 4K streaming capabilities with Dolby Vision and all the HDRs that matter, Dolby Atmos audio and support for Amazon Luna and Xbox Game Pass. (It has some decent retro gaming chops, too, as our Jeff Dunn has previously explained.) Aside from the lack of an onboard Ethernet port present on the Fire TV Cube, the 4K Max has the same Wi-Fi 6E support as the more expensive Cube, plus the same 16GB of storage and 2GB of memory. When compared to the other dongles in the Fire TV lineup, things get even more perplexing. The $60 4K Max and the $50 4K Plus are essentially the same stick, but the latter has less storage, only Wi-Fi 6 capabilities (not 6E), a standard Alexa Voice remote and no support for the Fire TV ambient experience, which turns your TV into an Alexa smart display when you’re not actively watching anything. Step down further once more to the $40 4K Select and you miss out on Dolby Vision and extra memory, and you’ll have to settle for Wi-Fi 5. If you’re going to make all those compromises to save a few dollars, then you should just get the entry-level $35 Fire TV Stick HD. The biggest thing here is that it only supports 1080p streaming, but that will be ok for some people. We consider it to be the best budget streaming device on the market right now, and for folks just looking to make a cheap, basic upgrade to an aging set — go off and know your $35 was well spent (or, pro tip: wait for a sale and pick one up for less than $20). The case for the $140 Fire TV Cube isn’t a strong one, but it’s one that I’ll admit might be attractive to some users. It adds into the mix an Ethernet port, hands-free Alexa controls (meaning you don’t have to press a button on its remote to activate the virtual assistant, you can just talk to it) and it can control your other entertainment devices like a cable box and game console. It ultimately gives you more control over both the other things in your entertainment ecosystem and Alexa all in one device. That means Amazon has three strong streaming devices with very clear value propositions: the $35 Stick HD, the $60 4K Max and the $140 TV Cube. Affordable, mid-tier and high-end categories are covered and most people will find something that fits in their budget and their needs with these three. The two Sticks sandwiched in the middle do nothing but confuse consumers. Looking at a comparison chart of all the Fire TV streaming devices, you might start to ask yourself, do I really need Wi-Fi 6E over Wi-Fi 6? Will one extra gigabyte of memory make a difference? Can I live without the Alexa Voice Remote Enhanced? You shouldn’t be asking yourself these questions; you have better things to do. There are only three Fire TV streaming devices worth considering, and I’d take it one step further and say most people should just get the Fire TV Stick 4K Max when it inevitably goes on sale for Black Friday for around $35. You’ll spend less and get a better product.This article originally appeared on Engadget at https://www.engadget.com/entertainment/streaming/why-does-amazon-need-five-tv-streamers-163014576.html?src=rss",
          "content": "What’s in a name? Apparently quite a bit, according to Amazon. The company recently announced yet another change to its Fire TV devices lineup, which just means they renamed a few things yet again. The family now includes the $35 Fire TV Stick HD, the $40 Fire TV Stick 4K Select, the $50 Fire TV Stick 4K Plus, the $60 Fire TV Stick 4K Max and the $140 Fire TV Cube. That was a pain to type out and probably a pain to read (my apologies). Two of those devices were “rebranded” previously within the past year, so if you’re confused, you’re likely not alone. What’s a humble shopper to do when you’re trying to decide which is the best (and budget-friendly) option to upgrade an old TV so you can binge-watch Hunting Wives and ask Alexa about tomorrow’s weather forecast? I’ll make your decision quite easy: just get the Fire TV Stick 4K Max. Really, if you’re looking for the best streaming device, period, we recommend turning to Google for that. But if you’ve decided Amazon’s Fire TV lineup is where you want to spend your money, the 4K Max is the best option of the bunch. Not only has it stuck around without being subject to a “rebrand” for quite some time, but it also has arguably the best balance of features and price of any Fire TV streaming device. The Fire TV Stick 4K Max gives you 4K streaming capabilities with Dolby Vision and all the HDRs that matter, Dolby Atmos audio and support for Amazon Luna and Xbox Game Pass. (It has some decent retro gaming chops, too, as our Jeff Dunn has previously explained.) Aside from the lack of an onboard Ethernet port present on the Fire TV Cube, the 4K Max has the same Wi-Fi 6E support as the more expensive Cube, plus the same 16GB of storage and 2GB of memory. When compared to the other dongles in the Fire TV lineup, things get even more perplexing. The $60 4K Max and the $50 4K Plus are essentially the same stick, but the latter has less storage, only Wi-Fi 6 capabilities (not 6E), a standard Alexa Voice remote and no support for the Fire TV ambient experience, which turns your TV into an Alexa smart display when you’re not actively watching anything. Step down further once more to the $40 4K Select and you miss out on Dolby Vision and extra memory, and you’ll have to settle for Wi-Fi 5. If you’re going to make all those compromises to save a few dollars, then you should just get the entry-level $35 Fire TV Stick HD. The biggest thing here is that it only supports 1080p streaming, but that will be ok for some people. We consider it to be the best budget streaming device on the market right now, and for folks just looking to make a cheap, basic upgrade to an aging set — go off and know your $35 was well spent (or, pro tip: wait for a sale and pick one up for less than $20). The case for the $140 Fire TV Cube isn’t a strong one, but it’s one that I’ll admit might be attractive to some users. It adds into the mix an Ethernet port, hands-free Alexa controls (meaning you don’t have to press a button on its remote to activate the virtual assistant, you can just talk to it) and it can control your other entertainment devices like a cable box and game console. It ultimately gives you more control over both the other things in your entertainment ecosystem and Alexa all in one device. That means Amazon has three strong streaming devices with very clear value propositions: the $35 Stick HD, the $60 4K Max and the $140 TV Cube. Affordable, mid-tier and high-end categories are covered and most people will find something that fits in their budget and their needs with these three. The two Sticks sandwiched in the middle do nothing but confuse consumers. Looking at a comparison chart of all the Fire TV streaming devices, you might start to ask yourself, do I really need Wi-Fi 6E over Wi-Fi 6? Will one extra gigabyte of memory make a difference? Can I live without the Alexa Voice Remote Enhanced? You shouldn’t be asking yourself these questions; you have better things to do. There are only three Fire TV streaming devices worth considering, and I’d take it one step further and say most people should just get the Fire TV Stick 4K Max when it inevitably goes on sale for Black Friday for around $35. You’ll spend less and get a better product.This article originally appeared on Engadget at https://www.engadget.com/entertainment/streaming/why-does-amazon-need-five-tv-streamers-163014576.html?src=rss",
          "feed_position": 34
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/how-anthropics-skills-make-claude-faster-cheaper-and-more-consistent-for",
          "published_at": "Thu, 16 Oct 2025 16:00:00 GMT",
          "title": "How Anthropic’s ‘Skills’ make Claude faster, cheaper, and more consistent for business workflows",
          "standfirst": "Anthropic launched a new capability on Thursday that allows its Claude AI assistant to tap into specialized expertise on demand, marking the company&#x27;s latest effort to make artificial intelligence more practical for enterprise workflows as it chases rival OpenAI in the intensifying competition over AI-powered software development.The feature, called Skills, enables users to create folders containing instructions, code scripts, and reference materials that Claude can automatically load when relevant to a task. The system marks a fundamental shift in how organizations can customize AI assistants, moving beyond one-off prompts to reusable packages of domain expertise that work consistently across an entire company.\"Skills are based on our belief and vision that as model intelligence continues to improve, we&#x27;ll continue moving towards general-purpose agents that often have access to their own filesystem and computing environment,\" said Mahesh Murag, a member of Anthropic&#x27;s technical staff, in an exclusive interview with VentureBeat. \"The agent is initially made aware only of the names and descriptions of each available skill and can choose to load more information about a particular skill when relevant to the task at hand.\"The launch comes as Anthropic, valued at $183 billion after a recent $13 billion funding round, projects its annual revenue could nearly triple to as much as $26 billion in 2026, according to a recent Reuters report. The company is currently approaching a $7 billion annual revenue run rate, up from $5 billion in August, fueled largely by enterprise adoption of its AI coding tools — a market where it faces fierce competition from OpenAI&#x27;s recently upgraded Codex platform.How &#x27;progressive disclosure&#x27; solves the context window problemSkills differ fundamentally from existing approaches to customizing AI assistants, such as prompt engineering or retrieval-augmented generation (RAG), Murag explained. The architecture relies on what Anthropic calls \"progressive disclosure\" — Claude initially sees only skill names and brief descriptions, then autonomously decides which skills to load based on the task at hand, accessing only the specific files and information needed at that moment.\"Unlike RAG, this relies on simple tools that let Claude manage and read files from a filesystem,\" Murag told VentureBeat. \"Skills can contain an unbounded amount of context to teach Claude how to complete a task or series of tasks. This is because Skills are based on the premise of an agent being able to autonomously and intelligently navigate a filesystem and execute code.\"This approach allows organizations to bundle far more information than traditional context windows permit, while maintaining the speed and efficiency that enterprise users demand. A single skill can include step-by-step procedures, code templates, reference documents, brand guidelines, compliance checklists, and executable scripts — all organized in a folder structure that Claude navigates intelligently.The system&#x27;s composability provides another technical advantage. Multiple skills automatically stack together when needed for complex workflows. For instance, Claude might simultaneously invoke a company&#x27;s brand guidelines skill, a financial reporting skill, and a presentation formatting skill to generate a quarterly investor deck — coordinating between all three without manual intervention.What makes Skills different from OpenAI&#x27;s Custom GPTs and Microsoft&#x27;s CopilotAnthropic is positioning Skills as distinct from competing offerings like OpenAI&#x27;s Custom GPTs and Microsoft&#x27;s Copilot Studio, though the features address similar enterprise needs around AI customization and consistency.\"Skills&#x27; combination of progressive disclosure, composability, and executable code bundling is unique in the market,\" Murag said. \"While other platforms require developers to build custom scaffolding, Skills let anyone — technical or not — create specialized agents by organizing procedural knowledge into files.\"The cross-platform portability also sets Skills apart. The same skill works identically across Claude.ai, Claude Code (Anthropic&#x27;s AI coding environment), the company&#x27;s API, and the Claude Agent SDK for building custom AI agents. Organizations can develop a skill once and deploy it everywhere their teams use Claude, a significant advantage for enterprises seeking consistency.The feature supports any programming language compatible with the underlying container environment, and Anthropic provides sandboxing for security — though the company acknowledges that allowing AI to execute code requires users to carefully vet which skills they trust.Early customers report 8x productivity gains on finance workflowsEarly customer implementations reveal how organizations are applying Skills to automate complex knowledge work. At Japanese e-commerce giant Rakuten, the AI team is using Skills to transform finance operations that previously required manual coordination across multiple departments.\"Skills streamline our management accounting and finance workflows,\" said Yusuke Kaji, general manager of AI at Rakuten in a statement. \"Claude processes multiple spreadsheets, catches critical anomalies, and generates reports using our procedures. What once took a day, we can now accomplish in an hour.\"That&#x27;s an 8x improvement in productivity for specific workflows — the kind of measurable return on investment that enterprises increasingly demand from AI implementations. Mike Krieger, Anthropic&#x27;s chief product officer and Instagram co-founder, recently noted that companies have moved past \"AI FOMO\" to requiring concrete success metrics.Design platform Canva plans to integrate Skills into its own AI agent workflows. \"Canva plans to leverage Skills to customize agents and expand what they can do,\" said Anwar Haneef, general manager and head of ecosystem at Canva in a statement. \"This unlocks new ways to bring Canva deeper into agentic workflows—helping teams capture their unique context and create stunning, high-quality designs effortlessly.\"Cloud storage provider Box sees Skills as a way to make corporate content repositories more actionable. \"Skills teaches Claude how to work with Box content,\" said Yashodha Bhavnani, head of AI at Box. \"Users can transform stored files into PowerPoint presentations, Excel spreadsheets, and Word documents that follow their organization&#x27;s standards—saving hours of effort.\"The enterprise security question: Who controls which AI skills employees can use?For enterprise IT departments, Skills raise important questions about governance and control—particularly since the feature allows AI to execute arbitrary code in sandboxed environments. Anthropic has built administrative controls that allow enterprise customers to manage access at the organizational level.\"Enterprise admins control access to the Skills capability via admin settings, where they can enable or disable access and monitor usage patterns,\" Murag said. \"Once enabled at the organizational level, individual users still need to opt in.\"That two-layer consent model — organizational enablement plus individual opt-in — reflects lessons learned from previous enterprise AI deployments where blanket rollouts created compliance concerns. However, Anthropic&#x27;s governance tools appear more limited than some enterprise customers might expect. The company doesn&#x27;t currently offer granular controls over which specific skills employees can use, or detailed audit trails of custom skill content.Organizations concerned about data security should note that Skills require Claude&#x27;s code execution environment, which runs in isolated containers. Anthropic advises users to \"stick to trusted sources\" when installing skills and provides security documentation, but the company acknowledges this is an inherently higher-risk capability than traditional AI interactions.From API to no-code: How Anthropic is making Skills accessible to everyoneAnthropic is taking several approaches to make Skills accessible to users with varying technical sophistication. For non-technical users on Claude.ai, the company provides a \"skill-creator\" skill that interactively guides users through building new skills by asking questions about their workflow, then automatically generating the folder structure and documentation.Developers working with Anthropic&#x27;s API get programmatic control through a new /skills endpoint and can manage skill versions through the Claude Console web interface. The feature requires enabling the Code Execution Tool beta in API requests. For Claude Code users, skills can be installed via plugins from the anthropics/skills GitHub marketplace, and teams can share skills through version control systems.\"Skills are included in Max, Pro, Teams, and Enterprise plans at no additional cost,\" Murag confirmed. \"API usage follows standard API pricing,\" meaning organizations pay only for the tokens consumed during skill execution, not for the skills themselves.Anthropic provides several pre-built skills for common business tasks, including professional generation of Excel spreadsheets with formulas, PowerPoint presentations, Word documents, and fillable PDFs. These Anthropic-created skills will remain free.Why the Skills launch matters in the AI coding wars with OpenAIThe Skills announcement arrives during a pivotal moment in Anthropic&#x27;s competition with OpenAI, particularly around AI-assisted software development. Just one day before releasing Skills, Anthropic launched Claude Haiku 4.5, a smaller and cheaper model that nonetheless matches the coding performance of Claude Sonnet 4 — which was state-of-the-art when released just five months ago.That rapid improvement curve reflects the breakneck pace of AI development, where today&#x27;s frontier capabilities become tomorrow&#x27;s commodity offerings. OpenAI has been pushing hard on coding tools as well, recently upgrading its Codex platform with GPT-5 and expanding GitHub Copilot&#x27;s capabilities.Anthropic&#x27;s revenue trajectory — potentially reaching $26 billion in 2026 from an estimated $9 billion by year-end 2025 — suggests the company is successfully converting enterprise interest into paying customers. The timing also follows Salesforce&#x27;s announcement this week that it&#x27;s deepening AI partnerships with both OpenAI and Anthropic to power its Agentforce platform, signaling that enterprises are adopting a multi-vendor approach rather than standardizing on a single provider.Skills addresses a real pain point: the \"prompt engineering\" problem where effective AI usage depends on individual employees crafting elaborate instructions for routine tasks, with no way to share that expertise across teams. Skills transforms implicit knowledge into explicit, shareable assets. For startups and developers, the feature could accelerate product development significantly — adding sophisticated document generation capabilities that previously required dedicated engineering teams and weeks of development.The composability aspect hints at a future where organizations build libraries of specialized skills that can be mixed and matched for increasingly complex workflows. A pharmaceutical company might develop skills for regulatory compliance, clinical trial analysis, molecular modeling, and patient data privacy that work together seamlessly — creating a customized AI assistant with deep domain expertise across multiple specialties.Anthropic indicates it&#x27;s working on simplified skill creation workflows and enterprise-wide deployment capabilities to make it easier for organizations to distribute skills across large teams. As the feature rolls out to Anthropic&#x27;s more than 300,000 business customers, the true test will be whether organizations find Skills substantively more useful than existing customization approaches.For now, Skills offers Anthropic&#x27;s clearest articulation yet of its vision for AI agents: not generalists that try to do everything reasonably well, but intelligent systems that know when to access specialized expertise and can coordinate multiple domains of knowledge to accomplish complex tasks. If that vision catches on, the question won&#x27;t be whether your company uses AI — it will be whether your AI knows how your company actually works.",
          "content": "Anthropic launched a new capability on Thursday that allows its Claude AI assistant to tap into specialized expertise on demand, marking the company&#x27;s latest effort to make artificial intelligence more practical for enterprise workflows as it chases rival OpenAI in the intensifying competition over AI-powered software development.The feature, called Skills, enables users to create folders containing instructions, code scripts, and reference materials that Claude can automatically load when relevant to a task. The system marks a fundamental shift in how organizations can customize AI assistants, moving beyond one-off prompts to reusable packages of domain expertise that work consistently across an entire company.\"Skills are based on our belief and vision that as model intelligence continues to improve, we&#x27;ll continue moving towards general-purpose agents that often have access to their own filesystem and computing environment,\" said Mahesh Murag, a member of Anthropic&#x27;s technical staff, in an exclusive interview with VentureBeat. \"The agent is initially made aware only of the names and descriptions of each available skill and can choose to load more information about a particular skill when relevant to the task at hand.\"The launch comes as Anthropic, valued at $183 billion after a recent $13 billion funding round, projects its annual revenue could nearly triple to as much as $26 billion in 2026, according to a recent Reuters report. The company is currently approaching a $7 billion annual revenue run rate, up from $5 billion in August, fueled largely by enterprise adoption of its AI coding tools — a market where it faces fierce competition from OpenAI&#x27;s recently upgraded Codex platform.How &#x27;progressive disclosure&#x27; solves the context window problemSkills differ fundamentally from existing approaches to customizing AI assistants, such as prompt engineering or retrieval-augmented generation (RAG), Murag explained. The architecture relies on what Anthropic calls \"progressive disclosure\" — Claude initially sees only skill names and brief descriptions, then autonomously decides which skills to load based on the task at hand, accessing only the specific files and information needed at that moment.\"Unlike RAG, this relies on simple tools that let Claude manage and read files from a filesystem,\" Murag told VentureBeat. \"Skills can contain an unbounded amount of context to teach Claude how to complete a task or series of tasks. This is because Skills are based on the premise of an agent being able to autonomously and intelligently navigate a filesystem and execute code.\"This approach allows organizations to bundle far more information than traditional context windows permit, while maintaining the speed and efficiency that enterprise users demand. A single skill can include step-by-step procedures, code templates, reference documents, brand guidelines, compliance checklists, and executable scripts — all organized in a folder structure that Claude navigates intelligently.The system&#x27;s composability provides another technical advantage. Multiple skills automatically stack together when needed for complex workflows. For instance, Claude might simultaneously invoke a company&#x27;s brand guidelines skill, a financial reporting skill, and a presentation formatting skill to generate a quarterly investor deck — coordinating between all three without manual intervention.What makes Skills different from OpenAI&#x27;s Custom GPTs and Microsoft&#x27;s CopilotAnthropic is positioning Skills as distinct from competing offerings like OpenAI&#x27;s Custom GPTs and Microsoft&#x27;s Copilot Studio, though the features address similar enterprise needs around AI customization and consistency.\"Skills&#x27; combination of progressive disclosure, composability, and executable code bundling is unique in the market,\" Murag said. \"While other platforms require developers to build custom scaffolding, Skills let anyone — technical or not — create specialized agents by organizing procedural knowledge into files.\"The cross-platform portability also sets Skills apart. The same skill works identically across Claude.ai, Claude Code (Anthropic&#x27;s AI coding environment), the company&#x27;s API, and the Claude Agent SDK for building custom AI agents. Organizations can develop a skill once and deploy it everywhere their teams use Claude, a significant advantage for enterprises seeking consistency.The feature supports any programming language compatible with the underlying container environment, and Anthropic provides sandboxing for security — though the company acknowledges that allowing AI to execute code requires users to carefully vet which skills they trust.Early customers report 8x productivity gains on finance workflowsEarly customer implementations reveal how organizations are applying Skills to automate complex knowledge work. At Japanese e-commerce giant Rakuten, the AI team is using Skills to transform finance operations that previously required manual coordination across multiple departments.\"Skills streamline our management accounting and finance workflows,\" said Yusuke Kaji, general manager of AI at Rakuten in a statement. \"Claude processes multiple spreadsheets, catches critical anomalies, and generates reports using our procedures. What once took a day, we can now accomplish in an hour.\"That&#x27;s an 8x improvement in productivity for specific workflows — the kind of measurable return on investment that enterprises increasingly demand from AI implementations. Mike Krieger, Anthropic&#x27;s chief product officer and Instagram co-founder, recently noted that companies have moved past \"AI FOMO\" to requiring concrete success metrics.Design platform Canva plans to integrate Skills into its own AI agent workflows. \"Canva plans to leverage Skills to customize agents and expand what they can do,\" said Anwar Haneef, general manager and head of ecosystem at Canva in a statement. \"This unlocks new ways to bring Canva deeper into agentic workflows—helping teams capture their unique context and create stunning, high-quality designs effortlessly.\"Cloud storage provider Box sees Skills as a way to make corporate content repositories more actionable. \"Skills teaches Claude how to work with Box content,\" said Yashodha Bhavnani, head of AI at Box. \"Users can transform stored files into PowerPoint presentations, Excel spreadsheets, and Word documents that follow their organization&#x27;s standards—saving hours of effort.\"The enterprise security question: Who controls which AI skills employees can use?For enterprise IT departments, Skills raise important questions about governance and control—particularly since the feature allows AI to execute arbitrary code in sandboxed environments. Anthropic has built administrative controls that allow enterprise customers to manage access at the organizational level.\"Enterprise admins control access to the Skills capability via admin settings, where they can enable or disable access and monitor usage patterns,\" Murag said. \"Once enabled at the organizational level, individual users still need to opt in.\"That two-layer consent model — organizational enablement plus individual opt-in — reflects lessons learned from previous enterprise AI deployments where blanket rollouts created compliance concerns. However, Anthropic&#x27;s governance tools appear more limited than some enterprise customers might expect. The company doesn&#x27;t currently offer granular controls over which specific skills employees can use, or detailed audit trails of custom skill content.Organizations concerned about data security should note that Skills require Claude&#x27;s code execution environment, which runs in isolated containers. Anthropic advises users to \"stick to trusted sources\" when installing skills and provides security documentation, but the company acknowledges this is an inherently higher-risk capability than traditional AI interactions.From API to no-code: How Anthropic is making Skills accessible to everyoneAnthropic is taking several approaches to make Skills accessible to users with varying technical sophistication. For non-technical users on Claude.ai, the company provides a \"skill-creator\" skill that interactively guides users through building new skills by asking questions about their workflow, then automatically generating the folder structure and documentation.Developers working with Anthropic&#x27;s API get programmatic control through a new /skills endpoint and can manage skill versions through the Claude Console web interface. The feature requires enabling the Code Execution Tool beta in API requests. For Claude Code users, skills can be installed via plugins from the anthropics/skills GitHub marketplace, and teams can share skills through version control systems.\"Skills are included in Max, Pro, Teams, and Enterprise plans at no additional cost,\" Murag confirmed. \"API usage follows standard API pricing,\" meaning organizations pay only for the tokens consumed during skill execution, not for the skills themselves.Anthropic provides several pre-built skills for common business tasks, including professional generation of Excel spreadsheets with formulas, PowerPoint presentations, Word documents, and fillable PDFs. These Anthropic-created skills will remain free.Why the Skills launch matters in the AI coding wars with OpenAIThe Skills announcement arrives during a pivotal moment in Anthropic&#x27;s competition with OpenAI, particularly around AI-assisted software development. Just one day before releasing Skills, Anthropic launched Claude Haiku 4.5, a smaller and cheaper model that nonetheless matches the coding performance of Claude Sonnet 4 — which was state-of-the-art when released just five months ago.That rapid improvement curve reflects the breakneck pace of AI development, where today&#x27;s frontier capabilities become tomorrow&#x27;s commodity offerings. OpenAI has been pushing hard on coding tools as well, recently upgrading its Codex platform with GPT-5 and expanding GitHub Copilot&#x27;s capabilities.Anthropic&#x27;s revenue trajectory — potentially reaching $26 billion in 2026 from an estimated $9 billion by year-end 2025 — suggests the company is successfully converting enterprise interest into paying customers. The timing also follows Salesforce&#x27;s announcement this week that it&#x27;s deepening AI partnerships with both OpenAI and Anthropic to power its Agentforce platform, signaling that enterprises are adopting a multi-vendor approach rather than standardizing on a single provider.Skills addresses a real pain point: the \"prompt engineering\" problem where effective AI usage depends on individual employees crafting elaborate instructions for routine tasks, with no way to share that expertise across teams. Skills transforms implicit knowledge into explicit, shareable assets. For startups and developers, the feature could accelerate product development significantly — adding sophisticated document generation capabilities that previously required dedicated engineering teams and weeks of development.The composability aspect hints at a future where organizations build libraries of specialized skills that can be mixed and matched for increasingly complex workflows. A pharmaceutical company might develop skills for regulatory compliance, clinical trial analysis, molecular modeling, and patient data privacy that work together seamlessly — creating a customized AI assistant with deep domain expertise across multiple specialties.Anthropic indicates it&#x27;s working on simplified skill creation workflows and enterprise-wide deployment capabilities to make it easier for organizations to distribute skills across large teams. As the feature rolls out to Anthropic&#x27;s more than 300,000 business customers, the true test will be whether organizations find Skills substantively more useful than existing customization approaches.For now, Skills offers Anthropic&#x27;s clearest articulation yet of its vision for AI agents: not generalists that try to do everything reasonably well, but intelligent systems that know when to access specialized expertise and can coordinate multiple domains of knowledge to accomplish complex tasks. If that vision catches on, the question won&#x27;t be whether your company uses AI — it will be whether your AI knows how your company actually works.",
          "feed_position": 5,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/5KwMYU2hucZ0L584A3lOVV/e7cd9bb4ab6fa04669f700eaa8ee6c74/nuneybits_Vector_art_of_a_retro_personal_computer_image_in_burn_55247853-b50a-4330-b1f1-3185f079bb17.webp"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/gaming/for-its-next-trick-quantic-dream-is-trying-to-compete-with-league-of-legends-and-dota-150000283.html",
          "published_at": "Thu, 16 Oct 2025 15:00:00 +0000",
          "title": "For its next trick, Quantic Dream is trying to compete with League of Legends and Dota",
          "standfirst": "It's been quite a while since we've heard much about Quantic Dream's Star Wars: Eclipse. The studio revealed that project at The Game Awards back in 2021 and details have been scarce since then. As it turns out, the developer of Heavy Rain and Detroit: Become Human had been working on a second game this whole time. It's one that sees Quantic Dream venturing into entirely new territory, because the studio is making its first multiplayer game.Spellcasters Chronicles is a 3 vs. 3 MOBA with a third-person perspective that's akin to Marvel Rivals. Each round lasts 25 minutes, with teams summoning minions, battling to conquer territory and earning victory by destroying their opponents' lifestones. So far, so typical MOBA. But Quantic Dream has a few tricks up its sleeve that it hopes will help make Spellcasters Chronicles stand out in a highly competitive live-service market.It's a magic-based MOBA with characters that have unique abilities, personalities and backstories. Every one of these mages has the ability to fly at any time and for as long as they want. So you can freely take to the skies to get a bird's eye view of the battlefield and help you make decisions about what to do next. You can duke it out with enemies in the air too.Along with attacks, support spells and summoning armies with hundreds of creatures, players can use their magic to plunk down buildings and shore up their defenses while altering the environment. There's interplay between characters too, as you can infuse allies (including summoned creatures) with spells. One mage, for instance, might add fire to a tankier teammate's hammer, so there are synergies to discover. \"Something we wanted to push is the sense of creativity,\" game director Greg Diaconu told reporters ahead of the reveal.Spellcasters ChroniclesQuantic DreamEventually, you'll be able to bring giant, game-changing titans into battles. Each player can summon one. Whenever a titan appears, it's an all-hands-on-deck situation for the opposing team, since these are powerful creatures that can completely change the course of a round.\"It was important for us to create a sense of spectacle,\" Diaconu said. \"Something that's as fun to watch as it is to play.\"It all seems quite action-packed, but there's a heavy strategic element to Spellcasters Chronicles as well. Before you go into a battle, you'll select your spells and summons, including your titan — so this is a deckbuilder game too. In the thick of the action, your team will need to decide when to pressure the map and try to expand your territory while capturing altars of power, totems that will grant you resources. Speaking of which, each spell has a limited number of uses, so resource management is a factor too.Spellcasters Chronicles is free-to-play, but there are no pay-to-win concerns here. In-game purchases will be purely cosmetic. Expect battle passes full of new looks for the characters. Lots of updates are in the pipeline too, including new mages, spells and creatures.Seven years in the ovenQuantic Dream started making Spellcasters Chronicles seven years ago (so before Netease bought a stake of the company and eventually the whole shebang). Although the studio decided to keep making narrative-driven single-player games after Detroit: Become Human, it wanted to try something new as well. The idea was to take the team's experience of working on interactive storytelling to a different genre by creating a multiplayer game with a stylized look. \"Multiple teams are fully dedicated to crafting the next generation of great games, including something very different, a competitive multiplayer experience, born from the same spirit of curiosity and creativity that has always defined us,\" Quantic Dream founder David Cage wrote in a blog post on Thursday. \"This new title may surprise our fans as it is very different from what we have done so far. But taking risks, challenging ourselves, exploring new ways of playing and telling stories, and attempting what seems impossible, has always been part of our DNA.\"In the world of Spellcasters Chronicles, gods are no more and mages who are able to harness an energy called the Source will shape the future. Quantic Dream hasn't shared too many details about the plot and characters of Spellcasters Chronicles just yet — the reveal focused on gameplay. That's perhaps in part because the studio is leaning into a community-driven narrative approach. Victories and defeats will contribute \"to the evolving Tapestry of Fate, where seasonal decisions will change gameplay, lore and map meta.\"We shouldn't have to wait too long to see how all of that works in practice. Quantic Dream will run a closed beta for Spellcasters Chronicles on Steam later this year, and the game is set to hit consoles with cross-play support in 2026. Those who are attending TwitchCon San Diego this week can try out the MOBA there.This article originally appeared on Engadget at https://www.engadget.com/gaming/for-its-next-trick-quantic-dream-is-trying-to-compete-with-league-of-legends-and-dota-150000283.html?src=rss",
          "content": "It's been quite a while since we've heard much about Quantic Dream's Star Wars: Eclipse. The studio revealed that project at The Game Awards back in 2021 and details have been scarce since then. As it turns out, the developer of Heavy Rain and Detroit: Become Human had been working on a second game this whole time. It's one that sees Quantic Dream venturing into entirely new territory, because the studio is making its first multiplayer game.Spellcasters Chronicles is a 3 vs. 3 MOBA with a third-person perspective that's akin to Marvel Rivals. Each round lasts 25 minutes, with teams summoning minions, battling to conquer territory and earning victory by destroying their opponents' lifestones. So far, so typical MOBA. But Quantic Dream has a few tricks up its sleeve that it hopes will help make Spellcasters Chronicles stand out in a highly competitive live-service market.It's a magic-based MOBA with characters that have unique abilities, personalities and backstories. Every one of these mages has the ability to fly at any time and for as long as they want. So you can freely take to the skies to get a bird's eye view of the battlefield and help you make decisions about what to do next. You can duke it out with enemies in the air too.Along with attacks, support spells and summoning armies with hundreds of creatures, players can use their magic to plunk down buildings and shore up their defenses while altering the environment. There's interplay between characters too, as you can infuse allies (including summoned creatures) with spells. One mage, for instance, might add fire to a tankier teammate's hammer, so there are synergies to discover. \"Something we wanted to push is the sense of creativity,\" game director Greg Diaconu told reporters ahead of the reveal.Spellcasters ChroniclesQuantic DreamEventually, you'll be able to bring giant, game-changing titans into battles. Each player can summon one. Whenever a titan appears, it's an all-hands-on-deck situation for the opposing team, since these are powerful creatures that can completely change the course of a round.\"It was important for us to create a sense of spectacle,\" Diaconu said. \"Something that's as fun to watch as it is to play.\"It all seems quite action-packed, but there's a heavy strategic element to Spellcasters Chronicles as well. Before you go into a battle, you'll select your spells and summons, including your titan — so this is a deckbuilder game too. In the thick of the action, your team will need to decide when to pressure the map and try to expand your territory while capturing altars of power, totems that will grant you resources. Speaking of which, each spell has a limited number of uses, so resource management is a factor too.Spellcasters Chronicles is free-to-play, but there are no pay-to-win concerns here. In-game purchases will be purely cosmetic. Expect battle passes full of new looks for the characters. Lots of updates are in the pipeline too, including new mages, spells and creatures.Seven years in the ovenQuantic Dream started making Spellcasters Chronicles seven years ago (so before Netease bought a stake of the company and eventually the whole shebang). Although the studio decided to keep making narrative-driven single-player games after Detroit: Become Human, it wanted to try something new as well. The idea was to take the team's experience of working on interactive storytelling to a different genre by creating a multiplayer game with a stylized look. \"Multiple teams are fully dedicated to crafting the next generation of great games, including something very different, a competitive multiplayer experience, born from the same spirit of curiosity and creativity that has always defined us,\" Quantic Dream founder David Cage wrote in a blog post on Thursday. \"This new title may surprise our fans as it is very different from what we have done so far. But taking risks, challenging ourselves, exploring new ways of playing and telling stories, and attempting what seems impossible, has always been part of our DNA.\"In the world of Spellcasters Chronicles, gods are no more and mages who are able to harness an energy called the Source will shape the future. Quantic Dream hasn't shared too many details about the plot and characters of Spellcasters Chronicles just yet — the reveal focused on gameplay. That's perhaps in part because the studio is leaning into a community-driven narrative approach. Victories and defeats will contribute \"to the evolving Tapestry of Fate, where seasonal decisions will change gameplay, lore and map meta.\"We shouldn't have to wait too long to see how all of that works in practice. Quantic Dream will run a closed beta for Spellcasters Chronicles on Steam later this year, and the game is set to hit consoles with cross-play support in 2026. Those who are attending TwitchCon San Diego this week can try out the MOBA there.This article originally appeared on Engadget at https://www.engadget.com/gaming/for-its-next-trick-quantic-dream-is-trying-to-compete-with-league-of-legends-and-dota-150000283.html?src=rss",
          "feed_position": 40,
          "image_url": "https://d29szjachogqwa.cloudfront.net/videos/user-uploaded/qd_mages.jpeg"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/apps/the-vision-pro-will-get-an-ipad-app-in-upcoming-ipados-update-142904090.html",
          "published_at": "Thu, 16 Oct 2025 14:29:05 +0000",
          "title": "The Vision Pro will get an iPad app in upcoming iPadOS update",
          "standfirst": "Buried in the press release for the upgraded Vision Pro headset that Apple announced yesterday was the news that the dedicated Apple Vision Pro app is making its way to iPad via iPadOS 26.1 later this fall. This means iPad users can browse Vision Pro content like apps and games from their tablet and queue downloads for the headset without needing to put it on each time. The Vision Pro app has been available on iPhone since the arrival of iOS 18.4 in April, and features a regularly updated selection of curated spatial content that Apple thinks Vision Pro users might want to try. Open it up today, for example, and it’ll point you towards the Lungy app’s audiovisual meditations, the travel show Elevated on (the recently and very confusingly rebranded) Apple TV, some recommended games and a list of education-focused apps available on Vision Pro. The Vision Pro app also includes news and tips for using the headset, as well product information and account settings. All of the same features will be offered in the iPad version of the app. In case you missed yesterday’s announcement, Apple has introduced an improved Vision Pro headset powered by the same M5 chip housed in its new iPad Pro and MacBook Pro. This should represent a fairly significant jump up from the first-gen product’s M2 chip. Web-browsing will be faster, as will loading apps and just generally navigating menus on your headset. The M5 chip also features a new 10-core GPU, which should significantly boost gaming performance, and without handing us any battery specs (which the company never likes to do), Apple says the upgraded Vision Pro should last a bit longer too. The M5 Vision Pro is now ready to pre-order now and still costs $3,499. Apple will start shipping the device on October 22.This article originally appeared on Engadget at https://www.engadget.com/apps/the-vision-pro-will-get-an-ipad-app-in-upcoming-ipados-update-142904090.html?src=rss",
          "content": "Buried in the press release for the upgraded Vision Pro headset that Apple announced yesterday was the news that the dedicated Apple Vision Pro app is making its way to iPad via iPadOS 26.1 later this fall. This means iPad users can browse Vision Pro content like apps and games from their tablet and queue downloads for the headset without needing to put it on each time. The Vision Pro app has been available on iPhone since the arrival of iOS 18.4 in April, and features a regularly updated selection of curated spatial content that Apple thinks Vision Pro users might want to try. Open it up today, for example, and it’ll point you towards the Lungy app’s audiovisual meditations, the travel show Elevated on (the recently and very confusingly rebranded) Apple TV, some recommended games and a list of education-focused apps available on Vision Pro. The Vision Pro app also includes news and tips for using the headset, as well product information and account settings. All of the same features will be offered in the iPad version of the app. In case you missed yesterday’s announcement, Apple has introduced an improved Vision Pro headset powered by the same M5 chip housed in its new iPad Pro and MacBook Pro. This should represent a fairly significant jump up from the first-gen product’s M2 chip. Web-browsing will be faster, as will loading apps and just generally navigating menus on your headset. The M5 chip also features a new 10-core GPU, which should significantly boost gaming performance, and without handing us any battery specs (which the company never likes to do), Apple says the upgraded Vision Pro should last a bit longer too. The M5 Vision Pro is now ready to pre-order now and still costs $3,499. Apple will start shipping the device on October 22.This article originally appeared on Engadget at https://www.engadget.com/apps/the-vision-pro-will-get-an-ipad-app-in-upcoming-ipados-update-142904090.html?src=rss",
          "feed_position": 41
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/amazon-and-chobani-adopt-strellas-ai-interviews-for-customer-research-as",
          "published_at": "Thu, 16 Oct 2025 14:00:00 GMT",
          "title": "Amazon and Chobani adopt Strella's AI interviews for customer research as fast-growing startup raises $14M",
          "standfirst": "One year after emerging from stealth, Strella has raised $14 million in Series A funding to expand its AI-powered customer research platform, the company announced Thursday. The round, led by Bessemer Venture Partners with participation from Decibel Partners, Bain Future Back Ventures, MVP Ventures and 645 Ventures, comes as enterprises increasingly turn to artificial intelligence to understand customers faster and more deeply than traditional methods allow.The investment marks a sharp acceleration for the startup founded by Lydia Hylton and Priya Krishnan, two former consultants and product managers who watched companies struggle with a customer research process that could take eight weeks from start to finish. Since October, Strella has grown revenue tenfold, quadrupled its customer base to more than 40 paying enterprises, and tripled its average contract values by moving upmarket to serve Fortune 500 companies.\"Research tends to be bookended by two very strategic steps: first, we have a problem—what research should we do? And second, we&#x27;ve done the research—now what are we going to do with it?\" said Hylton, Strella&#x27;s CEO, in an exclusive interview with VentureBeat. \"All the stuff in the middle tends to be execution and lower-skill work. We view Strella as doing that middle 90% of the work.\"The platform now serves Amazon, Duolingo, Apollo GraphQL, and Chobani, collectively conducting thousands of AI-moderated interviews that deliver what the company claims is a 90% average time savings on manual research work. The company is approaching $1 million in revenue after beginning monetization only in January, with month-over-month growth of 50% and zero customer churn to date.How AI-powered interviews compress eight-week research projects into daysStrella&#x27;s technology addresses a workflow that has frustrated product teams, marketers, and designers for decades. Traditional customer research requires writing interview guides, recruiting participants, scheduling calls, conducting interviews, taking notes, synthesizing findings, and creating presentations — a process that consumes weeks of highly-skilled labor and often delays critical product decisions.The platform compresses that timeline to days by using AI to moderate voice-based interviews that run like Zoom calls, but with an artificial intelligence agent asking questions, following up on interesting responses, and detecting when participants are being evasive or fraudulent. The system then synthesizes findings automatically, creating highlight reels and charts from unstructured qualitative data.\"It used to take eight weeks. Now you can do it in the span of a couple days,\" Hylton told VentureBeat. \"The primary technology is through an AI-moderated interview. It&#x27;s like being in a Zoom call with an AI instead of a human — it&#x27;s completely free form and voice based.\"Critically, the platform also supports human moderators joining the same calls, reflecting the founders&#x27; belief that humans won&#x27;t disappear from the research process. \"Human moderation won&#x27;t go away, which is why we&#x27;ve supported human moderation from our Genesis,\" Hylton said. \"Research tends to be bookended by two very strategic steps: we have a problem, what&#x27;s the research that we should do? And we&#x27;ve done the research, now what are we going to do with it? All the stuff in the middle tends to be execution and lower skill work. We view Strella as doing that middle 90% of the work.\"Why customers tell AI moderators the truth they won&#x27;t share with humansOne of Strella&#x27;s most surprising findings challenges assumptions about AI in qualitative research: participants appear more honest with AI moderators than with humans. The founders discovered this pattern repeatedly as customers ran head-to-head comparisons between traditional human-moderated studies and Strella&#x27;s AI approach.\"If you&#x27;re a designer and you get on a Zoom call with a customer and you say, &#x27;Do you like my design?&#x27; they&#x27;re always gonna say yes. They don&#x27;t want to hurt your feelings,\" Hylton explained. \"But it&#x27;s not a problem at all for Strella. They would tell you exactly what they think about it, which is really valuable. It&#x27;s very hard to get honest feedback.\"Krishnan, Strella&#x27;s COO, said companies initially worried about using AI and \"eroding quality,\" but the platform has \"actually found the opposite to be true. People are much more open and honest with an AI moderator, and so the level of insight that you get is much richer because people are giving their unfiltered feedback.\"This dynamic has practical business implications. Brian Santiago, Senior Product Design Manager at Apollo GraphQL, said in a statement: \"Before Strella, studies took weeks. Now we get insights in a day — sometimes in just a few hours. And because participants open up more with the AI moderator, the feedback is deeper and more honest.\"The platform also addresses endemic fraud in online surveys, particularly when participants are compensated. Because Strella interviews happen on camera in real time, the AI moderator can detect when someone pauses suspiciously long — perhaps to consult ChatGPT — and flags them as potentially fraudulent. \"We are fraud resistant,\" Hylton said, contrasting this with traditional surveys where fraud rates can be substantial.Solving mobile app research with persistent screen sharing technologyA major focus of the Series A funding will be expanding Strella&#x27;s recently-launched mobile application, which Krishnan identified as critical competitive differentiation. The mobile app enables persistent screen sharing during interviews — allowing researchers to watch users navigate mobile applications in real time while the AI moderator asks about their experience.\"We are the only player in the market that supports screen sharing on mobile,\" Hylton said. \"You know, I want to understand what are the pain points with my app? Why do people not seem to be able to find the checkout flow? Well, in order to do that effectively, you&#x27;d like to see the user screen while they&#x27;re doing an interview.\"For consumer-facing companies where mobile represents the primary customer interface, this capability opens entirely new use cases. The founders noted that \"several of our customers didn&#x27;t do research before\" but have now built research practices around Strella because the platform finally made mobile research accessible at scale.The platform also supports embedding traditional survey question types directly into the conversational interview, approaching what Hylton called \"feature parity with a survey\" while maintaining the engagement advantages of a natural conversation. Strella interviews regularly run 60 to 90 minutes with nearly 100% completion rates—a duration that would see 60-70% drop-off in a traditional survey format.How Strella differentiated in a market crowded with AI research startupsStrella enters a market that appears crowded at first glance, with established players like Qualtrics and a wave of AI-powered startups promising to transform customer research. The founders themselves initially pursued a different approach — synthetic respondents, or \"digital twins\" that simulate customer perspectives using large language models.\"We actually pivoted from that. That was our initial idea,\" Hylton revealed, referring to synthetic respondents. \"People are very intrigued by that concept, but found in practice, no willingness to pay right now.\"Recent research suggesting companies could use language models as digital twins for customer feedback has reignited interest in that approach. But Hylton remains skeptical: \"The capabilities of the LLMs as they are today are not good enough, in my opinion, to justify a standalone company. Right now you could just ask ChatGPT, &#x27;What would new users of Duolingo think about this ad copy?&#x27; You can do that. Adding the standalone idea of a synthetic panel is sort of just putting a wrapper on that.\"Instead, Strella&#x27;s bet is that the real value lies in collecting proprietary qualitative data at scale — building what could become \"the system of truth for all qualitative insights\" within enterprises, as Lindsey Li, Vice President at Bessemer Venture Partners, described it.Li, who led the investment just one year after Strella emerged from stealth, said the firm was convinced by both the technology and the team. \"Strella has built highly differentiated technology that enables a continuous interview rather than a survey,\" Li said. \"We heard time and time again that customers loved this product experience relative to other offerings.\"On the defensibility question that concerns many AI investors, Li emphasized product execution over patents: \"We think the long game here will be won with a million small product decisions, all of which must be driven by deep empathy for customer pain and an understanding of how best to address their needs. Lydia and Priya exhibit that in spades.\"The founders point to technical depth that&#x27;s difficult to replicate. Most competitors started with adaptive surveys — text-based interfaces where users type responses and wait for the next question. Some have added voice, but typically as uploaded audio clips rather than free-flowing conversation.\"Our approach is fundamentally better, which is the fact that it is a free form conversation,\" Hylton said. \"You never have to control anything. You&#x27;re never typing, there&#x27;s no buttons, there&#x27;s no upload and wait for the next question. It&#x27;s completely free form, and that has been an extraordinarily hard product to build. There&#x27;s a tremendous amount of IP in the way that we prompt our moderator, the way that we run analysis.\"The platform also improves with use, learning from each customer&#x27;s research patterns to fine-tune future interview guides and questions. \"Our product gets better for our customers as they continue to use us,\" Hylton said. All research accumulates in a central repository where teams can generate new insights by chatting with the data or creating visualizations from previously unstructured qualitative feedback.Creating new research budgets instead of just automating existing onesPerhaps more important than displacing existing research is expanding the total market. Krishnan said growth has been \"fundamentally related to our product\" creating new research that wouldn&#x27;t have happened otherwise.\"We have expanded the use cases in which people would conduct research,\" Krishnan explained. \"Several of our customers didn&#x27;t do research before, have always wanted to do research, but didn&#x27;t have a dedicated researcher or team at their company that was devoted to it, and have purchased Strella to kick off and enable their research practice. That&#x27;s been really cool where we&#x27;ve seen this market just opening up.\"This expansion comes as enterprises face mounting pressure to improve customer experience amid declining satisfaction scores. According to Forrester Research&#x27;s 2024 Customer Experience Index, customer experience quality has declined for three consecutive years — an unprecedented trend. The report found that 39% of brands saw CX quality deteriorate, with declines across effectiveness, ease, and emotional connection.Meanwhile, Deloitte&#x27;s 2025 Technology, Media & Telecommunications Predictions report forecasts that 25% of enterprises using generative AI will deploy AI agents by 2025, growing to 50% by 2027. The report specifically highlighted AI&#x27;s potential to enhance customer satisfaction by 15-20% while reducing cost to serve by 20-30% when properly implemented.Gartner identified conversational user interfaces — the category Strella inhabits — as one of three technologies poised to transform customer service by 2028, noting that \"customers increasingly expect to be able to interact with the applications they use in a natural way.\"Against this backdrop, Li sees substantial room for growth. \"UX Research is a sub-sector of the $140B+ global market-research industry,\" Li said. \"This includes both the software layer historically (~$430M) and professional services spend on UX research, design, product strategy, etc. which is conservatively estimated to be ~$6.4B+ annually. As software in this vertical, led by Strella, becomes more powerful, we believe the TAM will continue to expand meaningfully.\"Making customer feedback accessible across the enterprise, not just research teamsThe founders describe their mission as \"democratizing access to the customer\" — making it possible for anyone in an organization to understand customer perspectives without waiting for dedicated research teams to complete months-long studies.\"Many, many, many positions in the organization would like to get customer feedback, but it&#x27;s so hard right now,\" Hylton said. With Strella, she explained, someone can \"log into Strella and through a chat, create any highlight reel that you want and actually see customers in their own words answering the question that you have based on the research that&#x27;s already been done.\"This video-first approach to research repositories changes organizational dynamics around customer feedback. \"Then you can say, &#x27;Okay, engineering team, we need to build this feature. And here&#x27;s the customer actually saying it,&#x27;\" Hylton continued. \"&#x27;This is not me. This isn&#x27;t politics. Here are seven customers saying they can&#x27;t find the Checkout button.&#x27; The fact that we are a very video-based platform really allows us to do that quickly and painlessly.\"The company has moved decisively upmarket, with contract values now typically in the five-figure range and \"several six figure contracts\" signed, according to Krishnan. The pricing strategy reflects a premium positioning: \"Our product is very good, it&#x27;s very premium. We&#x27;re charging based on the value it provides to customers,\" Krishnan said, rather than competing on cost alone.This approach appears to be working. The company reports 100% conversion from pilot programs to paid contracts and zero churn among its 40-45 customers, with month-over-month revenue growth of 50%.The roadmap: Computer vision, agentic AI, and human-machine collaborationThe Series A funding will primarily support scaling product and go-to-market teams. \"We&#x27;re really confident that we have product-market fit,\" Hylton said. \"And now the question is execution, and we want to hire a lot of really talented people to help us execute.\"On the product roadmap, Hylton emphasized continued focus on the participant experience as the key to winning the market. \"Everything else is downstream of a joyful participant experience,\" she said, including \"the quality of insights, the amount you have to pay people to do the interviews, and the way that your customers feel about a company.\"Near-term priorities include adding visual capabilities so the AI moderator can respond to facial expressions and other nonverbal cues, and building more sophisticated collaboration features between human researchers and AI moderators. \"Maybe you want to listen while an AI moderator is running a call and you might want to be able to jump in with specific questions,\" Hylton said. \"Or you want to run an interview yourself, but you want the moderator to be there as backup or to help you.\"These features move toward what the industry calls \"agentic AI\" — systems that can act more autonomously while still collaborating with humans. The founders see this human-AI collaboration, rather than full automation, as the sustainable path forward.\"We believe that a lot of the really strategic work that companies do will continue to be human moderated,\" Hylton said. \"And you can still do that through Strella and just use us for synthesis in those cases.\"For Li and Bessemer, the bet is on founders who understand this nuance. \"Lydia and Priya exhibit the exact archetype of founders we are excited to partner with for the long term — customer-obsessed, transparent, thoughtful, and singularly driven towards the home-run scenario,\" she said.The company declined to disclose specific revenue figures or valuation. With the new funding, Strella has now raised $18 million total, including a $4 million seed round led by Decibel Partners announced in October.As Strella scales, the founders remain focused on a vision where technology enhances rather than eliminates human judgment—where an engineering team doesn&#x27;t just read a research report, but watches seven customers struggle to find the same button. Where a product manager can query months of accumulated interviews in seconds. Where companies don&#x27;t choose between speed and depth, but get both.\"The interesting part of the business is actually collecting that proprietary dataset, collecting qualitative research at scale,\" Hylton said, describing what she sees as Strella&#x27;s long-term moat. Not replacing the researcher, but making everyone in the company one.",
          "content": "One year after emerging from stealth, Strella has raised $14 million in Series A funding to expand its AI-powered customer research platform, the company announced Thursday. The round, led by Bessemer Venture Partners with participation from Decibel Partners, Bain Future Back Ventures, MVP Ventures and 645 Ventures, comes as enterprises increasingly turn to artificial intelligence to understand customers faster and more deeply than traditional methods allow.The investment marks a sharp acceleration for the startup founded by Lydia Hylton and Priya Krishnan, two former consultants and product managers who watched companies struggle with a customer research process that could take eight weeks from start to finish. Since October, Strella has grown revenue tenfold, quadrupled its customer base to more than 40 paying enterprises, and tripled its average contract values by moving upmarket to serve Fortune 500 companies.\"Research tends to be bookended by two very strategic steps: first, we have a problem—what research should we do? And second, we&#x27;ve done the research—now what are we going to do with it?\" said Hylton, Strella&#x27;s CEO, in an exclusive interview with VentureBeat. \"All the stuff in the middle tends to be execution and lower-skill work. We view Strella as doing that middle 90% of the work.\"The platform now serves Amazon, Duolingo, Apollo GraphQL, and Chobani, collectively conducting thousands of AI-moderated interviews that deliver what the company claims is a 90% average time savings on manual research work. The company is approaching $1 million in revenue after beginning monetization only in January, with month-over-month growth of 50% and zero customer churn to date.How AI-powered interviews compress eight-week research projects into daysStrella&#x27;s technology addresses a workflow that has frustrated product teams, marketers, and designers for decades. Traditional customer research requires writing interview guides, recruiting participants, scheduling calls, conducting interviews, taking notes, synthesizing findings, and creating presentations — a process that consumes weeks of highly-skilled labor and often delays critical product decisions.The platform compresses that timeline to days by using AI to moderate voice-based interviews that run like Zoom calls, but with an artificial intelligence agent asking questions, following up on interesting responses, and detecting when participants are being evasive or fraudulent. The system then synthesizes findings automatically, creating highlight reels and charts from unstructured qualitative data.\"It used to take eight weeks. Now you can do it in the span of a couple days,\" Hylton told VentureBeat. \"The primary technology is through an AI-moderated interview. It&#x27;s like being in a Zoom call with an AI instead of a human — it&#x27;s completely free form and voice based.\"Critically, the platform also supports human moderators joining the same calls, reflecting the founders&#x27; belief that humans won&#x27;t disappear from the research process. \"Human moderation won&#x27;t go away, which is why we&#x27;ve supported human moderation from our Genesis,\" Hylton said. \"Research tends to be bookended by two very strategic steps: we have a problem, what&#x27;s the research that we should do? And we&#x27;ve done the research, now what are we going to do with it? All the stuff in the middle tends to be execution and lower skill work. We view Strella as doing that middle 90% of the work.\"Why customers tell AI moderators the truth they won&#x27;t share with humansOne of Strella&#x27;s most surprising findings challenges assumptions about AI in qualitative research: participants appear more honest with AI moderators than with humans. The founders discovered this pattern repeatedly as customers ran head-to-head comparisons between traditional human-moderated studies and Strella&#x27;s AI approach.\"If you&#x27;re a designer and you get on a Zoom call with a customer and you say, &#x27;Do you like my design?&#x27; they&#x27;re always gonna say yes. They don&#x27;t want to hurt your feelings,\" Hylton explained. \"But it&#x27;s not a problem at all for Strella. They would tell you exactly what they think about it, which is really valuable. It&#x27;s very hard to get honest feedback.\"Krishnan, Strella&#x27;s COO, said companies initially worried about using AI and \"eroding quality,\" but the platform has \"actually found the opposite to be true. People are much more open and honest with an AI moderator, and so the level of insight that you get is much richer because people are giving their unfiltered feedback.\"This dynamic has practical business implications. Brian Santiago, Senior Product Design Manager at Apollo GraphQL, said in a statement: \"Before Strella, studies took weeks. Now we get insights in a day — sometimes in just a few hours. And because participants open up more with the AI moderator, the feedback is deeper and more honest.\"The platform also addresses endemic fraud in online surveys, particularly when participants are compensated. Because Strella interviews happen on camera in real time, the AI moderator can detect when someone pauses suspiciously long — perhaps to consult ChatGPT — and flags them as potentially fraudulent. \"We are fraud resistant,\" Hylton said, contrasting this with traditional surveys where fraud rates can be substantial.Solving mobile app research with persistent screen sharing technologyA major focus of the Series A funding will be expanding Strella&#x27;s recently-launched mobile application, which Krishnan identified as critical competitive differentiation. The mobile app enables persistent screen sharing during interviews — allowing researchers to watch users navigate mobile applications in real time while the AI moderator asks about their experience.\"We are the only player in the market that supports screen sharing on mobile,\" Hylton said. \"You know, I want to understand what are the pain points with my app? Why do people not seem to be able to find the checkout flow? Well, in order to do that effectively, you&#x27;d like to see the user screen while they&#x27;re doing an interview.\"For consumer-facing companies where mobile represents the primary customer interface, this capability opens entirely new use cases. The founders noted that \"several of our customers didn&#x27;t do research before\" but have now built research practices around Strella because the platform finally made mobile research accessible at scale.The platform also supports embedding traditional survey question types directly into the conversational interview, approaching what Hylton called \"feature parity with a survey\" while maintaining the engagement advantages of a natural conversation. Strella interviews regularly run 60 to 90 minutes with nearly 100% completion rates—a duration that would see 60-70% drop-off in a traditional survey format.How Strella differentiated in a market crowded with AI research startupsStrella enters a market that appears crowded at first glance, with established players like Qualtrics and a wave of AI-powered startups promising to transform customer research. The founders themselves initially pursued a different approach — synthetic respondents, or \"digital twins\" that simulate customer perspectives using large language models.\"We actually pivoted from that. That was our initial idea,\" Hylton revealed, referring to synthetic respondents. \"People are very intrigued by that concept, but found in practice, no willingness to pay right now.\"Recent research suggesting companies could use language models as digital twins for customer feedback has reignited interest in that approach. But Hylton remains skeptical: \"The capabilities of the LLMs as they are today are not good enough, in my opinion, to justify a standalone company. Right now you could just ask ChatGPT, &#x27;What would new users of Duolingo think about this ad copy?&#x27; You can do that. Adding the standalone idea of a synthetic panel is sort of just putting a wrapper on that.\"Instead, Strella&#x27;s bet is that the real value lies in collecting proprietary qualitative data at scale — building what could become \"the system of truth for all qualitative insights\" within enterprises, as Lindsey Li, Vice President at Bessemer Venture Partners, described it.Li, who led the investment just one year after Strella emerged from stealth, said the firm was convinced by both the technology and the team. \"Strella has built highly differentiated technology that enables a continuous interview rather than a survey,\" Li said. \"We heard time and time again that customers loved this product experience relative to other offerings.\"On the defensibility question that concerns many AI investors, Li emphasized product execution over patents: \"We think the long game here will be won with a million small product decisions, all of which must be driven by deep empathy for customer pain and an understanding of how best to address their needs. Lydia and Priya exhibit that in spades.\"The founders point to technical depth that&#x27;s difficult to replicate. Most competitors started with adaptive surveys — text-based interfaces where users type responses and wait for the next question. Some have added voice, but typically as uploaded audio clips rather than free-flowing conversation.\"Our approach is fundamentally better, which is the fact that it is a free form conversation,\" Hylton said. \"You never have to control anything. You&#x27;re never typing, there&#x27;s no buttons, there&#x27;s no upload and wait for the next question. It&#x27;s completely free form, and that has been an extraordinarily hard product to build. There&#x27;s a tremendous amount of IP in the way that we prompt our moderator, the way that we run analysis.\"The platform also improves with use, learning from each customer&#x27;s research patterns to fine-tune future interview guides and questions. \"Our product gets better for our customers as they continue to use us,\" Hylton said. All research accumulates in a central repository where teams can generate new insights by chatting with the data or creating visualizations from previously unstructured qualitative feedback.Creating new research budgets instead of just automating existing onesPerhaps more important than displacing existing research is expanding the total market. Krishnan said growth has been \"fundamentally related to our product\" creating new research that wouldn&#x27;t have happened otherwise.\"We have expanded the use cases in which people would conduct research,\" Krishnan explained. \"Several of our customers didn&#x27;t do research before, have always wanted to do research, but didn&#x27;t have a dedicated researcher or team at their company that was devoted to it, and have purchased Strella to kick off and enable their research practice. That&#x27;s been really cool where we&#x27;ve seen this market just opening up.\"This expansion comes as enterprises face mounting pressure to improve customer experience amid declining satisfaction scores. According to Forrester Research&#x27;s 2024 Customer Experience Index, customer experience quality has declined for three consecutive years — an unprecedented trend. The report found that 39% of brands saw CX quality deteriorate, with declines across effectiveness, ease, and emotional connection.Meanwhile, Deloitte&#x27;s 2025 Technology, Media & Telecommunications Predictions report forecasts that 25% of enterprises using generative AI will deploy AI agents by 2025, growing to 50% by 2027. The report specifically highlighted AI&#x27;s potential to enhance customer satisfaction by 15-20% while reducing cost to serve by 20-30% when properly implemented.Gartner identified conversational user interfaces — the category Strella inhabits — as one of three technologies poised to transform customer service by 2028, noting that \"customers increasingly expect to be able to interact with the applications they use in a natural way.\"Against this backdrop, Li sees substantial room for growth. \"UX Research is a sub-sector of the $140B+ global market-research industry,\" Li said. \"This includes both the software layer historically (~$430M) and professional services spend on UX research, design, product strategy, etc. which is conservatively estimated to be ~$6.4B+ annually. As software in this vertical, led by Strella, becomes more powerful, we believe the TAM will continue to expand meaningfully.\"Making customer feedback accessible across the enterprise, not just research teamsThe founders describe their mission as \"democratizing access to the customer\" — making it possible for anyone in an organization to understand customer perspectives without waiting for dedicated research teams to complete months-long studies.\"Many, many, many positions in the organization would like to get customer feedback, but it&#x27;s so hard right now,\" Hylton said. With Strella, she explained, someone can \"log into Strella and through a chat, create any highlight reel that you want and actually see customers in their own words answering the question that you have based on the research that&#x27;s already been done.\"This video-first approach to research repositories changes organizational dynamics around customer feedback. \"Then you can say, &#x27;Okay, engineering team, we need to build this feature. And here&#x27;s the customer actually saying it,&#x27;\" Hylton continued. \"&#x27;This is not me. This isn&#x27;t politics. Here are seven customers saying they can&#x27;t find the Checkout button.&#x27; The fact that we are a very video-based platform really allows us to do that quickly and painlessly.\"The company has moved decisively upmarket, with contract values now typically in the five-figure range and \"several six figure contracts\" signed, according to Krishnan. The pricing strategy reflects a premium positioning: \"Our product is very good, it&#x27;s very premium. We&#x27;re charging based on the value it provides to customers,\" Krishnan said, rather than competing on cost alone.This approach appears to be working. The company reports 100% conversion from pilot programs to paid contracts and zero churn among its 40-45 customers, with month-over-month revenue growth of 50%.The roadmap: Computer vision, agentic AI, and human-machine collaborationThe Series A funding will primarily support scaling product and go-to-market teams. \"We&#x27;re really confident that we have product-market fit,\" Hylton said. \"And now the question is execution, and we want to hire a lot of really talented people to help us execute.\"On the product roadmap, Hylton emphasized continued focus on the participant experience as the key to winning the market. \"Everything else is downstream of a joyful participant experience,\" she said, including \"the quality of insights, the amount you have to pay people to do the interviews, and the way that your customers feel about a company.\"Near-term priorities include adding visual capabilities so the AI moderator can respond to facial expressions and other nonverbal cues, and building more sophisticated collaboration features between human researchers and AI moderators. \"Maybe you want to listen while an AI moderator is running a call and you might want to be able to jump in with specific questions,\" Hylton said. \"Or you want to run an interview yourself, but you want the moderator to be there as backup or to help you.\"These features move toward what the industry calls \"agentic AI\" — systems that can act more autonomously while still collaborating with humans. The founders see this human-AI collaboration, rather than full automation, as the sustainable path forward.\"We believe that a lot of the really strategic work that companies do will continue to be human moderated,\" Hylton said. \"And you can still do that through Strella and just use us for synthesis in those cases.\"For Li and Bessemer, the bet is on founders who understand this nuance. \"Lydia and Priya exhibit the exact archetype of founders we are excited to partner with for the long term — customer-obsessed, transparent, thoughtful, and singularly driven towards the home-run scenario,\" she said.The company declined to disclose specific revenue figures or valuation. With the new funding, Strella has now raised $18 million total, including a $4 million seed round led by Decibel Partners announced in October.As Strella scales, the founders remain focused on a vision where technology enhances rather than eliminates human judgment—where an engineering team doesn&#x27;t just read a research report, but watches seven customers struggle to find the same button. Where a product manager can query months of accumulated interviews in seconds. Where companies don&#x27;t choose between speed and depth, but get both.\"The interesting part of the business is actually collecting that proprietary dataset, collecting qualitative research at scale,\" Hylton said, describing what she sees as Strella&#x27;s long-term moat. Not replacing the researcher, but making everyone in the company one.",
          "feed_position": 6,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/2MHgJSYW7aGoHqAVeFBa49/cc077429552cf090646e0177202a2e31/strella_lydia_priya_photo2.jpg"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/microsoft-launches-hey-copilot-voice-assistant-and-autonomous-agents-for-all",
          "published_at": "Thu, 16 Oct 2025 13:00:00 GMT",
          "title": "Microsoft launches 'Hey Copilot' voice assistant and autonomous agents for all Windows 11 PCs",
          "standfirst": "Microsoft is fundamentally reimagining how people interact with their computers, announcing Thursday a sweeping transformation of Windows 11 that brings voice-activated AI assistants, autonomous software agents, and contextual intelligence to every PC running the operating system — not just premium devices with specialized chips.The announcement represents Microsoft&#x27;s most aggressive push yet to integrate generative artificial intelligence into the desktop computing experience, moving beyond the chatbot interfaces that have defined the first wave of consumer AI products toward a more ambient, conversational model where users can simply talk to their computers and have AI agents complete complex tasks on their behalf.\"When we think about what the promise of an AI PC is, it should be capable of three things,\" Yusuf Mehdi, Microsoft&#x27;s Executive Vice President and Consumer Chief Marketing Officer, told reporters at a press conference last week. \"First, you should be able to interact with it naturally, in text or voice, and have it understand you. Second, it should be able to see what you see and be able to offer guided support. And third, it should be able to take action on your behalf.\"The shift could prove consequential for an industry searching for the \"killer app\" for generative AI. While hundreds of millions of people have experimented with ChatGPT and similar chatbots, integrating AI directly into the operating system that powers the vast majority of workplace computers could dramatically accelerate mainstream adoption — or create new security and privacy headaches for organizations already struggling to govern employee use of AI tools.How &#x27;Hey Copilot&#x27; aims to replace typing with talking on Windows PCsAt the heart of Microsoft&#x27;s vision is voice interaction, which the company is positioning as the third fundamental input method for PCs after the mouse and keyboard — a comparison that underscores Microsoft&#x27;s ambitions for reshaping human-computer interaction nearly four decades after the graphical user interface became standard.Starting this week, any Windows 11 user can enable the \"Hey Copilot\" wake word with a single click, allowing them to summon Microsoft&#x27;s AI assistant by voice from anywhere in the operating system. The feature, which had been in limited testing, is now being rolled out to hundreds of millions of devices globally.\"It&#x27;s been almost four decades since the PC has changed the way you interact with it, which is primarily mouse and keyboard,\" Mehdi said. \"When you think about it, we find that people type on a given day up to 14,000 words on their keyboard, which is really kind of mind-boggling. But what if now you can go beyond that and talk to it?\"The emphasis on voice reflects internal Microsoft data showing that users engage with Copilot twice as much when using voice compared to text input — a finding the company attributes to the lower cognitive barrier of speaking versus crafting precise written prompts.\"The magic unlock with Copilot Voice and Copilot Vision is the ease of interaction,\" according to the company&#x27;s announcement. \"Using the new wake word, &#x27;Hey Copilot,&#x27; getting something done is as easy as just asking for it.\"But Microsoft&#x27;s bet on voice computing faces real-world constraints that Mehdi acknowledged during the briefing. When asked whether workers in shared office environments would use voice features, potentially compromising privacy, Mehdi noted that millions already conduct voice calls through their PCs with headphones, and predicted users would adapt: \"Just like when the mouse came out, people have to figure out when to use it, what&#x27;s the right way, how to make it happen.\"Crucially, Microsoft is hedging its voice-first strategy by making all features accessible through traditional text input as well, recognizing that voice isn&#x27;t always appropriate or accessible.AI that sees your screen: Copilot Vision expands worldwide with new capabilitiesPerhaps more transformative than voice control is the expansion of Copilot Vision, a feature Microsoft introduced earlier this year that allows the AI to analyze what&#x27;s displayed on a user&#x27;s screen and provide contextual assistance.Previously limited to voice interaction, Copilot Vision is now rolling out worldwide with a new text-based interface, allowing users to type questions about what they&#x27;re viewing rather than speaking them aloud. The feature can now access full document context in Microsoft Office applications — meaning it can analyze an entire PowerPoint presentation or Excel spreadsheet without the user needing to scroll through every page.\"With 68 percent of consumers reporting using AI to support their decision making, voice is making this easier,\" Microsoft explained in its announcement. \"The magic unlock with Copilot Voice and Copilot Vision is the ease of interaction.\"During the press briefing, Microsoft demonstrated Copilot Vision helping users navigate Spotify&#x27;s settings to enable lossless audio streaming, coaching an artist through writing a professional bio based on their visual portfolio, and providing shopping recommendations based on products visible in YouTube videos.\"What brings AI to life is when you can give it rich context, when you can type great prompts,\" Mehdi explained. \"The big challenge for the majority of people is we&#x27;ve been trained with search to do the opposite. We&#x27;ve been trained to essentially type in fewer keywords, because it turns out the less keywords you type on search, the better your answers are.\"He noted that average search queries remain just 2.3 keywords, while AI systems perform better with detailed prompts — creating a disconnect between user habits and AI capabilities. Copilot Vision aims to bridge that gap by automatically gathering visual context.\"With Copilot Vision, you can simply share your screen and Copilot in literally milliseconds can understand everything on the screen and then provide intelligence,\" Mehdi said.The vision capabilities work with any application without requiring developers to build specific integrations, using computer vision to interpret on-screen content — a powerful capability that also raises questions about what the AI can access and when.Software robots take control: Inside Copilot Actions&#x27; controversial autonomyThe most ambitious—and potentially controversial—new capability is Copilot Actions, an experimental feature that allows AI to take control of a user&#x27;s computer to complete tasks autonomously.Coming first to Windows Insiders enrolled in Copilot Labs, the feature builds on Microsoft&#x27;s May announcement of Copilot Actions on the web, extending the capability to manipulate local files and applications on Windows PCs.During demonstrations, Microsoft showed the AI agent organizing photo libraries, extracting data from documents, and working through multi-step tasks while users attended to other work. The agent operates in a separate, sandboxed environment and provides running commentary on its actions, with users able to take control at any time.\"As a general-purpose agent — simply describe the task you want to complete in your own words, and the agent will attempt to complete it by interacting with desktop and web applications,\" according to the announcement. \"While this is happening, you can choose to focus on other tasks. At any time, you can take over the task or check in on the progress of the action, including reviewing what actions have been taken.\"Navjot Virk, Microsoft&#x27;s Windows Experience Leader, acknowledged the technology&#x27;s current limitations during the briefing. \"We&#x27;ll be starting with a narrow set of use cases while we optimize model performance and learn,\" Virk said. \"You may see the agent make mistakes or encounter challenges with complex interfaces, which is why real-world testing of this experience is so critical.\"The experimental nature of Copilot Actions reflects broader industry challenges with agentic AI — systems that can take actions rather than simply providing information. While the potential productivity gains are substantial, AI systems still occasionally \"hallucinate\" incorrect information and can be vulnerable to novel attacks.Can AI agents be trusted? Microsoft&#x27;s new security framework explainedRecognizing the security implications of giving AI control over users&#x27; computers and files, Microsoft introduced a new security framework built on four core principles: user control, operational transparency, limited privileges, and privacy-preserving design.Central to this approach is the concept of \"agent accounts\" — separate Windows user accounts under which AI agents operate, distinct from the human user&#x27;s account. Combined with a new \"agent workspace\" that provides a sandboxed desktop environment, the architecture aims to create clear boundaries around what agents can access and modify.Peter Waxman, Microsoft&#x27;s Windows Security Engineering Leader, emphasized that Copilot Actions is disabled by default and requires explicit user opt-in. \"You&#x27;re always in control of what Copilot Actions can do,\" Waxman said. \"Copilot Actions is turned off by default and you&#x27;re able to pause, take control, or disable it at any time.\"During operation, users can monitor the agent&#x27;s progress in real-time, and the system requests additional approval before taking \"sensitive or important\" actions. All agent activity occurs under the dedicated agent account, creating an audit trail that distinguishes AI actions from human ones.However, the agent will have default access to users&#x27; Documents, Downloads, Desktop, and Pictures folders—a broad permission grant that could concern enterprise IT administrators.Dana Huang, Corporate Vice President for Windows Security, acknowledged in a blog post that \"agentic AI applications introduce novel security risks, such as cross-prompt injection (XPIA), where malicious content embedded in UI elements or documents can override agent instructions, leading to unintended actions like data exfiltration or malware installation.\"Microsoft promises more details about enterprise controls at its Ignite conference in November.Gaming, taskbar redesign, and deeper Office integration round out updatesBeyond voice and autonomous agents, Microsoft introduced changes across Windows 11&#x27;s core interfaces and extended AI to new domains.A new \"Ask Copilot\" feature integrates AI directly into the Windows taskbar, providing one-click access to start conversations, activate vision capabilities, or search for files and settings with \"lightning-fast\" results. The opt-in feature doesn&#x27;t replace traditional Windows search.File Explorer gains AI capabilities through integration with third-party services. A partnership with Manus AI allows users to right-click on local image files and generate complete websites without manual uploading or coding. Integration with Filmora enables quick jumps into video editing workflows.Microsoft also introduced Copilot Connectors, allowing users to link cloud services like OneDrive, Outlook, Google Drive, Gmail, and Google Calendar directly to Copilot on Windows. Once connected, users can query personal content across platforms using natural language.In a notable expansion beyond productivity, Microsoft and Xbox introduced Gaming Copilot for the ROG Xbox Ally handheld gaming devices developed with ASUS. The feature, accessible via a dedicated hardware button, provides an AI assistant that can answer gameplay questions, offer strategic advice, and help navigate game interfaces through natural voice conversation.Why Microsoft is racing to embed AI everywhere before Apple and GoogleMicrosoft&#x27;s announcement comes as technology giants race to embed generative AI into their core products following the November 2022 launch of ChatGPT. While Microsoft moved quickly to integrate OpenAI&#x27;s technology into Bing search and introduce Copilot across its product line, the company has faced questions about whether AI features are driving meaningful engagement. Recent data shows Bing&#x27;s search market share remaining largely flat despite AI integration.The Windows integration represents a different approach: rather than charging separately for AI features, Microsoft is building them into the operating system itself, betting that embedded AI will drive Windows 11 adoption and competitive differentiation against Apple and Google.Apple has taken a more cautious approach with Apple Intelligence, introducing AI features gradually and emphasizing privacy through on-device processing. Google has integrated AI across its services but has faced challenges with accuracy and reliability.Crucially, while Microsoft highlighted new Copilot+ PC models from partners with prices ranging from $649.99 to $1,499.99, the core AI features announced today work on any Windows 11 PC — a significant departure from earlier positioning that suggested AI capabilities required new hardware with specialized neural processing units.\"Everything we showed you here is for all Windows 11 PCs. You don&#x27;t need to run it on a copilot plus PC. It works on any Windows 11 PC,\" Mehdi clarified.This democratization of AI features across the Windows 11 installed base potentially accelerates adoption but also complicates Microsoft&#x27;s hardware sales pitch for premium devices.What Microsoft&#x27;s AI bet means for the future of computingMehdi framed the announcement in sweeping terms, describing Microsoft&#x27;s goal as fundamentally reimagining the operating system for the AI era.\"We&#x27;re taking kind of a bold view of it. We really feel that the vision that we have is, let&#x27;s rewrite the entire operating system around AI and build essentially what becomes truly the AI PC,\" he said.For Microsoft, the success of AI-powered Windows 11 could help drive the company&#x27;s next phase of growth as PC sales have matured and cloud growth faces increased competition.For users and organizations, the announcement represents a potential inflection point in how humans interact with computers — one that could significantly boost productivity if executed well, or create new security headaches if the AI proves unreliable or difficult to control.The technology industry will be watching closely to see whether Microsoft&#x27;s bet on conversational computing and agentic AI marks the beginning of a genuine paradigm shift, or proves to be another ambitious interface reimagining that fails to gain mainstream traction.What&#x27;s clear is that Microsoft is moving aggressively to stake its claim as the leader in AI-powered personal computing, leveraging its dominant position in desktop operating systems to bring generative AI directly into the daily workflows of potentially a billion users.Copilot Voice and Vision are available today to Windows 11 users worldwide, with experimental capabilities coming to Windows Insiders in the coming weeks.",
          "content": "Microsoft is fundamentally reimagining how people interact with their computers, announcing Thursday a sweeping transformation of Windows 11 that brings voice-activated AI assistants, autonomous software agents, and contextual intelligence to every PC running the operating system — not just premium devices with specialized chips.The announcement represents Microsoft&#x27;s most aggressive push yet to integrate generative artificial intelligence into the desktop computing experience, moving beyond the chatbot interfaces that have defined the first wave of consumer AI products toward a more ambient, conversational model where users can simply talk to their computers and have AI agents complete complex tasks on their behalf.\"When we think about what the promise of an AI PC is, it should be capable of three things,\" Yusuf Mehdi, Microsoft&#x27;s Executive Vice President and Consumer Chief Marketing Officer, told reporters at a press conference last week. \"First, you should be able to interact with it naturally, in text or voice, and have it understand you. Second, it should be able to see what you see and be able to offer guided support. And third, it should be able to take action on your behalf.\"The shift could prove consequential for an industry searching for the \"killer app\" for generative AI. While hundreds of millions of people have experimented with ChatGPT and similar chatbots, integrating AI directly into the operating system that powers the vast majority of workplace computers could dramatically accelerate mainstream adoption — or create new security and privacy headaches for organizations already struggling to govern employee use of AI tools.How &#x27;Hey Copilot&#x27; aims to replace typing with talking on Windows PCsAt the heart of Microsoft&#x27;s vision is voice interaction, which the company is positioning as the third fundamental input method for PCs after the mouse and keyboard — a comparison that underscores Microsoft&#x27;s ambitions for reshaping human-computer interaction nearly four decades after the graphical user interface became standard.Starting this week, any Windows 11 user can enable the \"Hey Copilot\" wake word with a single click, allowing them to summon Microsoft&#x27;s AI assistant by voice from anywhere in the operating system. The feature, which had been in limited testing, is now being rolled out to hundreds of millions of devices globally.\"It&#x27;s been almost four decades since the PC has changed the way you interact with it, which is primarily mouse and keyboard,\" Mehdi said. \"When you think about it, we find that people type on a given day up to 14,000 words on their keyboard, which is really kind of mind-boggling. But what if now you can go beyond that and talk to it?\"The emphasis on voice reflects internal Microsoft data showing that users engage with Copilot twice as much when using voice compared to text input — a finding the company attributes to the lower cognitive barrier of speaking versus crafting precise written prompts.\"The magic unlock with Copilot Voice and Copilot Vision is the ease of interaction,\" according to the company&#x27;s announcement. \"Using the new wake word, &#x27;Hey Copilot,&#x27; getting something done is as easy as just asking for it.\"But Microsoft&#x27;s bet on voice computing faces real-world constraints that Mehdi acknowledged during the briefing. When asked whether workers in shared office environments would use voice features, potentially compromising privacy, Mehdi noted that millions already conduct voice calls through their PCs with headphones, and predicted users would adapt: \"Just like when the mouse came out, people have to figure out when to use it, what&#x27;s the right way, how to make it happen.\"Crucially, Microsoft is hedging its voice-first strategy by making all features accessible through traditional text input as well, recognizing that voice isn&#x27;t always appropriate or accessible.AI that sees your screen: Copilot Vision expands worldwide with new capabilitiesPerhaps more transformative than voice control is the expansion of Copilot Vision, a feature Microsoft introduced earlier this year that allows the AI to analyze what&#x27;s displayed on a user&#x27;s screen and provide contextual assistance.Previously limited to voice interaction, Copilot Vision is now rolling out worldwide with a new text-based interface, allowing users to type questions about what they&#x27;re viewing rather than speaking them aloud. The feature can now access full document context in Microsoft Office applications — meaning it can analyze an entire PowerPoint presentation or Excel spreadsheet without the user needing to scroll through every page.\"With 68 percent of consumers reporting using AI to support their decision making, voice is making this easier,\" Microsoft explained in its announcement. \"The magic unlock with Copilot Voice and Copilot Vision is the ease of interaction.\"During the press briefing, Microsoft demonstrated Copilot Vision helping users navigate Spotify&#x27;s settings to enable lossless audio streaming, coaching an artist through writing a professional bio based on their visual portfolio, and providing shopping recommendations based on products visible in YouTube videos.\"What brings AI to life is when you can give it rich context, when you can type great prompts,\" Mehdi explained. \"The big challenge for the majority of people is we&#x27;ve been trained with search to do the opposite. We&#x27;ve been trained to essentially type in fewer keywords, because it turns out the less keywords you type on search, the better your answers are.\"He noted that average search queries remain just 2.3 keywords, while AI systems perform better with detailed prompts — creating a disconnect between user habits and AI capabilities. Copilot Vision aims to bridge that gap by automatically gathering visual context.\"With Copilot Vision, you can simply share your screen and Copilot in literally milliseconds can understand everything on the screen and then provide intelligence,\" Mehdi said.The vision capabilities work with any application without requiring developers to build specific integrations, using computer vision to interpret on-screen content — a powerful capability that also raises questions about what the AI can access and when.Software robots take control: Inside Copilot Actions&#x27; controversial autonomyThe most ambitious—and potentially controversial—new capability is Copilot Actions, an experimental feature that allows AI to take control of a user&#x27;s computer to complete tasks autonomously.Coming first to Windows Insiders enrolled in Copilot Labs, the feature builds on Microsoft&#x27;s May announcement of Copilot Actions on the web, extending the capability to manipulate local files and applications on Windows PCs.During demonstrations, Microsoft showed the AI agent organizing photo libraries, extracting data from documents, and working through multi-step tasks while users attended to other work. The agent operates in a separate, sandboxed environment and provides running commentary on its actions, with users able to take control at any time.\"As a general-purpose agent — simply describe the task you want to complete in your own words, and the agent will attempt to complete it by interacting with desktop and web applications,\" according to the announcement. \"While this is happening, you can choose to focus on other tasks. At any time, you can take over the task or check in on the progress of the action, including reviewing what actions have been taken.\"Navjot Virk, Microsoft&#x27;s Windows Experience Leader, acknowledged the technology&#x27;s current limitations during the briefing. \"We&#x27;ll be starting with a narrow set of use cases while we optimize model performance and learn,\" Virk said. \"You may see the agent make mistakes or encounter challenges with complex interfaces, which is why real-world testing of this experience is so critical.\"The experimental nature of Copilot Actions reflects broader industry challenges with agentic AI — systems that can take actions rather than simply providing information. While the potential productivity gains are substantial, AI systems still occasionally \"hallucinate\" incorrect information and can be vulnerable to novel attacks.Can AI agents be trusted? Microsoft&#x27;s new security framework explainedRecognizing the security implications of giving AI control over users&#x27; computers and files, Microsoft introduced a new security framework built on four core principles: user control, operational transparency, limited privileges, and privacy-preserving design.Central to this approach is the concept of \"agent accounts\" — separate Windows user accounts under which AI agents operate, distinct from the human user&#x27;s account. Combined with a new \"agent workspace\" that provides a sandboxed desktop environment, the architecture aims to create clear boundaries around what agents can access and modify.Peter Waxman, Microsoft&#x27;s Windows Security Engineering Leader, emphasized that Copilot Actions is disabled by default and requires explicit user opt-in. \"You&#x27;re always in control of what Copilot Actions can do,\" Waxman said. \"Copilot Actions is turned off by default and you&#x27;re able to pause, take control, or disable it at any time.\"During operation, users can monitor the agent&#x27;s progress in real-time, and the system requests additional approval before taking \"sensitive or important\" actions. All agent activity occurs under the dedicated agent account, creating an audit trail that distinguishes AI actions from human ones.However, the agent will have default access to users&#x27; Documents, Downloads, Desktop, and Pictures folders—a broad permission grant that could concern enterprise IT administrators.Dana Huang, Corporate Vice President for Windows Security, acknowledged in a blog post that \"agentic AI applications introduce novel security risks, such as cross-prompt injection (XPIA), where malicious content embedded in UI elements or documents can override agent instructions, leading to unintended actions like data exfiltration or malware installation.\"Microsoft promises more details about enterprise controls at its Ignite conference in November.Gaming, taskbar redesign, and deeper Office integration round out updatesBeyond voice and autonomous agents, Microsoft introduced changes across Windows 11&#x27;s core interfaces and extended AI to new domains.A new \"Ask Copilot\" feature integrates AI directly into the Windows taskbar, providing one-click access to start conversations, activate vision capabilities, or search for files and settings with \"lightning-fast\" results. The opt-in feature doesn&#x27;t replace traditional Windows search.File Explorer gains AI capabilities through integration with third-party services. A partnership with Manus AI allows users to right-click on local image files and generate complete websites without manual uploading or coding. Integration with Filmora enables quick jumps into video editing workflows.Microsoft also introduced Copilot Connectors, allowing users to link cloud services like OneDrive, Outlook, Google Drive, Gmail, and Google Calendar directly to Copilot on Windows. Once connected, users can query personal content across platforms using natural language.In a notable expansion beyond productivity, Microsoft and Xbox introduced Gaming Copilot for the ROG Xbox Ally handheld gaming devices developed with ASUS. The feature, accessible via a dedicated hardware button, provides an AI assistant that can answer gameplay questions, offer strategic advice, and help navigate game interfaces through natural voice conversation.Why Microsoft is racing to embed AI everywhere before Apple and GoogleMicrosoft&#x27;s announcement comes as technology giants race to embed generative AI into their core products following the November 2022 launch of ChatGPT. While Microsoft moved quickly to integrate OpenAI&#x27;s technology into Bing search and introduce Copilot across its product line, the company has faced questions about whether AI features are driving meaningful engagement. Recent data shows Bing&#x27;s search market share remaining largely flat despite AI integration.The Windows integration represents a different approach: rather than charging separately for AI features, Microsoft is building them into the operating system itself, betting that embedded AI will drive Windows 11 adoption and competitive differentiation against Apple and Google.Apple has taken a more cautious approach with Apple Intelligence, introducing AI features gradually and emphasizing privacy through on-device processing. Google has integrated AI across its services but has faced challenges with accuracy and reliability.Crucially, while Microsoft highlighted new Copilot+ PC models from partners with prices ranging from $649.99 to $1,499.99, the core AI features announced today work on any Windows 11 PC — a significant departure from earlier positioning that suggested AI capabilities required new hardware with specialized neural processing units.\"Everything we showed you here is for all Windows 11 PCs. You don&#x27;t need to run it on a copilot plus PC. It works on any Windows 11 PC,\" Mehdi clarified.This democratization of AI features across the Windows 11 installed base potentially accelerates adoption but also complicates Microsoft&#x27;s hardware sales pitch for premium devices.What Microsoft&#x27;s AI bet means for the future of computingMehdi framed the announcement in sweeping terms, describing Microsoft&#x27;s goal as fundamentally reimagining the operating system for the AI era.\"We&#x27;re taking kind of a bold view of it. We really feel that the vision that we have is, let&#x27;s rewrite the entire operating system around AI and build essentially what becomes truly the AI PC,\" he said.For Microsoft, the success of AI-powered Windows 11 could help drive the company&#x27;s next phase of growth as PC sales have matured and cloud growth faces increased competition.For users and organizations, the announcement represents a potential inflection point in how humans interact with computers — one that could significantly boost productivity if executed well, or create new security headaches if the AI proves unreliable or difficult to control.The technology industry will be watching closely to see whether Microsoft&#x27;s bet on conversational computing and agentic AI marks the beginning of a genuine paradigm shift, or proves to be another ambitious interface reimagining that fails to gain mainstream traction.What&#x27;s clear is that Microsoft is moving aggressively to stake its claim as the leader in AI-powered personal computing, leveraging its dominant position in desktop operating systems to bring generative AI directly into the daily workflows of potentially a billion users.Copilot Voice and Vision are available today to Windows 11 users worldwide, with experimental capabilities coming to Windows Insiders in the coming weeks.",
          "feed_position": 7,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/1LtYH6GIABRoimOmrndJLF/cc718a2cc21b1fef4c64bebfe4a3a965/nuneybits_Vector_art_of_Microsoft_Windows_desktop_computer_mode_02e6a80a-72d4-467e-94ba-e6dfdd7d49c5.webp"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/social-media/pinterest-will-let-you-dial-down-ai-slop-in-your-feeds-130000337.html",
          "published_at": "Thu, 16 Oct 2025 13:00:00 +0000",
          "title": "Pinterest will let you 'dial down' AI slop in your feeds",
          "standfirst": "Pinterest is taking new steps to reduce the amount of AI slop its users are seeing. The service is adding settings that allow people to \"dial down\" AI-generated content in a number of categories that are \"highly-prone\" to such imagery, the company said in an update.While most social platforms have grappled with how to deal with the rise of AI-created content, Pinterest has been particularly inundated. Its image-board UI has proven particularly susceptible to AI slop, and users have complained about the difficulty of finding content created by humans. Now, Pinterest is offering users more control over how much AI content appears in their recommendations. The service is adding a \"refine your recommendations\" setting that allows you to toggle generative AI content from specific categories, including art, architecture, beauty, fashion, entertainment, health, home decor and sport. According to the company, these topics have seen an influx of AI-generated content, but users should \"expect even more additions in the future.\"Notably, Pinterest isn't promising to root out generative AI content entirely. Rather, it says the new settings should \"dial down\" the amount of AI-based content they're seeing in a particular category. A spokesperson for the company says this is because not all AI-generated content on the platform is low quality and some users are in fact open to seeing AI-generated material.The setting also applies only to image pins, not video, so it likely won't do much to prevent Sora or other AI-created video clips from appearing in your feeds. For AI-created or AI-edited content that does continue to surface, Pinterest says it will label these posts more prominently. The company started experimenting with labels back in May, but has now \"ramped up\" its tools for identifying such content. Pinterest's new settings are available now on desktop and Android and will be available on iOS in the next few weeks.This article originally appeared on Engadget at https://www.engadget.com/social-media/pinterest-will-let-you-dial-down-ai-slop-in-your-feeds-130000337.html?src=rss",
          "content": "Pinterest is taking new steps to reduce the amount of AI slop its users are seeing. The service is adding settings that allow people to \"dial down\" AI-generated content in a number of categories that are \"highly-prone\" to such imagery, the company said in an update.While most social platforms have grappled with how to deal with the rise of AI-created content, Pinterest has been particularly inundated. Its image-board UI has proven particularly susceptible to AI slop, and users have complained about the difficulty of finding content created by humans. Now, Pinterest is offering users more control over how much AI content appears in their recommendations. The service is adding a \"refine your recommendations\" setting that allows you to toggle generative AI content from specific categories, including art, architecture, beauty, fashion, entertainment, health, home decor and sport. According to the company, these topics have seen an influx of AI-generated content, but users should \"expect even more additions in the future.\"Notably, Pinterest isn't promising to root out generative AI content entirely. Rather, it says the new settings should \"dial down\" the amount of AI-based content they're seeing in a particular category. A spokesperson for the company says this is because not all AI-generated content on the platform is low quality and some users are in fact open to seeing AI-generated material.The setting also applies only to image pins, not video, so it likely won't do much to prevent Sora or other AI-created video clips from appearing in your feeds. For AI-created or AI-edited content that does continue to surface, Pinterest says it will label these posts more prominently. The company started experimenting with labels back in May, but has now \"ramped up\" its tools for identifying such content. Pinterest's new settings are available now on desktop and Android and will be available on iOS in the next few weeks.This article originally appeared on Engadget at https://www.engadget.com/social-media/pinterest-will-let-you-dial-down-ai-slop-in-your-feeds-130000337.html?src=rss",
          "feed_position": 43
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/computing/microsofts-next-windows-11-ai-gamble-just-say-hey-copilot-130000875.html",
          "published_at": "Thu, 16 Oct 2025 13:00:00 +0000",
          "title": "Microsoft's next Windows 11 AI gamble: Just say 'Hey Copilot'",
          "standfirst": "Over a decade since Microsoft tried to make talking to Cortana on PCs a thing -- and spectacularly failed in the process -- the Windows giant is taking another swing at voice commands with its Copilot AI assistant in Windows 11. Starting today, the company is rolling out an upgrade to its existing Copilot Voice and Vision features which will let you say \"Hey, Copilot\" and then ask your PC a question based on what's on the screen. If you're looking at pictures of Hawaii, for example, you could ask your Windows 11 PC where exactly they were taken, have it plot you a flight plan and potentially even give you some budgeting tips to afford that island vacation.Microsoft's jaunty promotional videos for the Copilot features, set to Vampire Weekend's almost two-decade-old \"A Punk,\" make the process look practically seamless. One user asks Copilot to show them how to stream their music in the \"best possible quality,\" and the AI proceeds to highlight the exact location of the streaming settings in Spotify, while suggesting they choose the lossless option. Another person asks Copilot to write up a short biography based on their photo portfolio. Now Copilot isn't just about searching the web or generating novelty AI art, it's making it easy for users to perform practical tasks without much effort.Microsoft is clearly striving for the convenience of the Star Trek ship computer, a dream that also pushed Amazon to invest billions in its Echo devices and Alexa. The difference with Copilot is that you're not just talking to a faceless speaker -- Microsoft is also trying to make Windows 11 aware of what you're doing on your screen. The \"Hey Copilot\" feature and all of the Copilot Vision are cloud-based, so you'll have to live with image data of your desktop making its way to Microsoft's servers. That involves a level of trust the company has lost with many users, especially after the messy debut of Recall, its first flagship AI-powered feature.It doesn't help that many people are still peeved about the death of Windows 10 support this week. Unsurprisingly, the company stresses that \"Hey, Copilot\" is a purely opt-in feature that's buried in the Copilot app settings. (Of course, that can always change, especially if the company wants to juice AI engagement stats in a few years.)Copilot ActionsMicrosoftI suspect it'll be even harder for users to swallow where Microsoft wants to take Copilot: Giving it the ability to perform Windows tasks on its own. That's the goal of the experimental Copilot Actions feature, which initially debuted as a tool that could perform tasks on websites. Once enabled, Copilot Actions can be prompted to handle manual tasks, like resizing and straightening an a folder of photos. If any questions pop up, it can prompt you to answer them within the Copilot app. And as Copilot Actions is handling its job in the background, you're free to do anything else you'd like on your computer.Conceptually, Copilot Actions sounds similar to handing off a task to a real life assistant -- but just like a human assistant, there's always a chance something could go wrong along the way. It's also not hard to imagine the feature being coopted by nefarious malware down the line, since it's basically a Windows script in a better interface. Microsoft says it's tested Copilot Actions \"extensively\" internally, and it's rolling out the feature slowly to gather feedback.Just like \"Hey, Copilot,\" it's entirely opt-in, and you can see everything Copilot Actions is doing step-by-step in the Copilot app. Microsoft says you'l be able to jump in and take control of a Copilot Action job at any point, as well as control the permissions of AI agents in Windows 11’s user settings. Copilot tasks are also performed in a contained environment, according to Microsoft, which allows for even more specific permissions controls as well as runtime isolation (so Copilot can’t affect the rest of your system beyond its specific task).And as if we're not already inundated with Copilot all over Windows 11 already, Microsoft also plans to add an \"Ask Copilot\" search function right on Windows 11's taskbar. The company claims it's part of a mission to make the taskbar \"a dynamic hub\" for accomplishing tasks, but personally I like to keep my taskbar clear so I can cram in more app windows. Like everything Microsoft is announcing today, the Ask Copilot bar will also be entirely opt-in.As someone who’s been skeptical of Microsoft’s Copilot initiatives so far, I could actually see myself using “Hey Copilot” if it works as advertised. It sounds far more practical than the old Siri voice commands, which were limited by simplistic language models from a decade ago. Microsoft is also expanding AI actions built into Windows 11, including a new integration with Manus, an AI agent that can do things like turn several documents into a website, as well as Filmora, which lets you create AI videos right from the File Explorer.The new \"Hey Copilot\" and Copilot Vision features are available today on all Windows 11 PCs that have access to Copilot. Microsoft is also making Copilot Vision broadly available around the world today where Copilot is available. Copilot Actions and the Ask Copilot taskbar feature will \"gradually\" become available to Windows 11 Insiders, according to Microsoft.This article originally appeared on Engadget at https://www.engadget.com/computing/microsofts-next-windows-11-ai-gamble-just-say-hey-copilot-130000875.html?src=rss",
          "content": "Over a decade since Microsoft tried to make talking to Cortana on PCs a thing -- and spectacularly failed in the process -- the Windows giant is taking another swing at voice commands with its Copilot AI assistant in Windows 11. Starting today, the company is rolling out an upgrade to its existing Copilot Voice and Vision features which will let you say \"Hey, Copilot\" and then ask your PC a question based on what's on the screen. If you're looking at pictures of Hawaii, for example, you could ask your Windows 11 PC where exactly they were taken, have it plot you a flight plan and potentially even give you some budgeting tips to afford that island vacation.Microsoft's jaunty promotional videos for the Copilot features, set to Vampire Weekend's almost two-decade-old \"A Punk,\" make the process look practically seamless. One user asks Copilot to show them how to stream their music in the \"best possible quality,\" and the AI proceeds to highlight the exact location of the streaming settings in Spotify, while suggesting they choose the lossless option. Another person asks Copilot to write up a short biography based on their photo portfolio. Now Copilot isn't just about searching the web or generating novelty AI art, it's making it easy for users to perform practical tasks without much effort.Microsoft is clearly striving for the convenience of the Star Trek ship computer, a dream that also pushed Amazon to invest billions in its Echo devices and Alexa. The difference with Copilot is that you're not just talking to a faceless speaker -- Microsoft is also trying to make Windows 11 aware of what you're doing on your screen. The \"Hey Copilot\" feature and all of the Copilot Vision are cloud-based, so you'll have to live with image data of your desktop making its way to Microsoft's servers. That involves a level of trust the company has lost with many users, especially after the messy debut of Recall, its first flagship AI-powered feature.It doesn't help that many people are still peeved about the death of Windows 10 support this week. Unsurprisingly, the company stresses that \"Hey, Copilot\" is a purely opt-in feature that's buried in the Copilot app settings. (Of course, that can always change, especially if the company wants to juice AI engagement stats in a few years.)Copilot ActionsMicrosoftI suspect it'll be even harder for users to swallow where Microsoft wants to take Copilot: Giving it the ability to perform Windows tasks on its own. That's the goal of the experimental Copilot Actions feature, which initially debuted as a tool that could perform tasks on websites. Once enabled, Copilot Actions can be prompted to handle manual tasks, like resizing and straightening an a folder of photos. If any questions pop up, it can prompt you to answer them within the Copilot app. And as Copilot Actions is handling its job in the background, you're free to do anything else you'd like on your computer.Conceptually, Copilot Actions sounds similar to handing off a task to a real life assistant -- but just like a human assistant, there's always a chance something could go wrong along the way. It's also not hard to imagine the feature being coopted by nefarious malware down the line, since it's basically a Windows script in a better interface. Microsoft says it's tested Copilot Actions \"extensively\" internally, and it's rolling out the feature slowly to gather feedback.Just like \"Hey, Copilot,\" it's entirely opt-in, and you can see everything Copilot Actions is doing step-by-step in the Copilot app. Microsoft says you'l be able to jump in and take control of a Copilot Action job at any point, as well as control the permissions of AI agents in Windows 11’s user settings. Copilot tasks are also performed in a contained environment, according to Microsoft, which allows for even more specific permissions controls as well as runtime isolation (so Copilot can’t affect the rest of your system beyond its specific task).And as if we're not already inundated with Copilot all over Windows 11 already, Microsoft also plans to add an \"Ask Copilot\" search function right on Windows 11's taskbar. The company claims it's part of a mission to make the taskbar \"a dynamic hub\" for accomplishing tasks, but personally I like to keep my taskbar clear so I can cram in more app windows. Like everything Microsoft is announcing today, the Ask Copilot bar will also be entirely opt-in.As someone who’s been skeptical of Microsoft’s Copilot initiatives so far, I could actually see myself using “Hey Copilot” if it works as advertised. It sounds far more practical than the old Siri voice commands, which were limited by simplistic language models from a decade ago. Microsoft is also expanding AI actions built into Windows 11, including a new integration with Manus, an AI agent that can do things like turn several documents into a website, as well as Filmora, which lets you create AI videos right from the File Explorer.The new \"Hey Copilot\" and Copilot Vision features are available today on all Windows 11 PCs that have access to Copilot. Microsoft is also making Copilot Vision broadly available around the world today where Copilot is available. Copilot Actions and the Ask Copilot taskbar feature will \"gradually\" become available to Windows 11 Insiders, according to Microsoft.This article originally appeared on Engadget at https://www.engadget.com/computing/microsofts-next-windows-11-ai-gamble-just-say-hey-copilot-130000875.html?src=rss",
          "feed_position": 44,
          "image_url": "https://d29szjachogqwa.cloudfront.net/videos/user-uploaded/Copilot_Actions_Image.jpg"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/cybersecurity/vpn/how-to-cancel-expressvpn-and-get-a-full-refund-123020142.html",
          "published_at": "Thu, 16 Oct 2025 12:30:20 +0000",
          "title": "How to cancel ExpressVPN and get a full refund",
          "standfirst": "ExpressVPN is one of the best VPNs on the market, with user-friendly apps, excellent speed test scores and a strong security record. In my ExpressVPN review, I found it to live up to its positive word of mouth, especially when unblocking foreign streaming sites. But no service is perfect, and my cup of VPN tea is not everybody's. If you're looking to switch away, follow this guide to cancel ExpressVPN. How to cancel ExpressVPN on desktop No matter where you originally signed up for ExpressVPN, you can cancel through your browser on a desktop platform. With any browser (i.e. Chrome, Safari, etc), the steps are as follows. Note that doing this will instantly cancel your ExpressVPN subscription and revoke your access to the service. Go to expressvpn.com. In the top bar, click on My Account. Enter your username and password to sign in. You'll be taken to your account dashboard. In the menu on the left-hand side of the screen, click Subscription. Scroll down until you find the words \"Subscription details.\" You should see a box containing the monthly cost of your subscription. At the right of that box, click Cancel Subscription. From here, follow the on-screen prompts to complete cancellation. Sam Chapman for Engadget If you're not ready to lose ExpressVPN service just yet, you can end auto-renewal instead of cancelling altogether. You'll still be able to use ExpressVPN until your subscription runs out. To cancel auto-renew, go to the subscription tab of your account dashboard and find the box with your subscription ID (it should be right at the top). In that box, click the link that says Edit subscription settings, then scroll down and click Turn Off Automatic Renewal. If you change your mind before your subscription lapses, you'll be able to turn it back on later. How to cancel ExpressVPN on mobile You can also cancel ExpressVPN on your phone or tablet, but the process is largely the same — some buttons are just in different places. As above, this instantly ends your ExpressVPN subscription. Here's how to do it. Go to expressvpn.com in your mobile browser. Tap the three horizontal lines at the top-right, then in the menu that appears, tap My Account. In the account dashboard, tap the three horizontal lines at the top-right once again. This time, scroll down to the expanded \"My Account\" menu and tap Subscription. Scroll down to the \"Subscription details\" box and tap Cancel Subscription. Follow the prompts on the screen to complete cancellation. Like on desktop, you can also turn off auto-renewal so your subscription ends when your current pay period expires. Go to the subscription tab of your account dashboard as described in the steps, find the box with your subscription ID, then follow the steps from the last paragraph of the previous section. Cancelling ExpressVPN through an app store The website is almost always the right way to cancel ExpressVPN, but there is one exception: if you originally signed up through the Google Play Store or Apple App Store. This includes both paid subscriptions and the 7-day free trials ExpressVPN offers to app store users. In this case, you'll need to cancel through the app store where you originally made the purchase. On Apple, open the app store, then tap Subscriptions and scroll down to find ExpressVPN. On Android, open Google Play and tap Payment & subscriptions, then Subscriptions. In both cases, once you've found ExpressVPN, tap it and scroll down to find the button for cancellation. How to get a refund from ExpressVPN ExpressVPN offers a full refund to anyone who cancels a subscription within 30 days of purchase. You can also get a refund if your subscription renews without your consent — if that happens, you have 14 days to request your money back. Sam Chapman for Engadget There's no dedicated button for asking for a refund. Instead, you'll need to contact ExpressVPN. To do that, go to expressvpn.com/support, then click on the button in the bottom-right corner that says Need help? Chat with us! This will open a chat with a bot which you can use to ask for your money back. Unfortunately, there's no public link to email the support team, so live chat is the only option here. ExpressVPN alternatives Depending on what made you want to cancel ExpressVPN, there are a few other VPNs you might like better. Proton VPN is my favorite provider at the moment, combining great apps with a unique focus on privacy rights (it's also cheaper). NordVPN has a lot of useful features ExpressVPN leaves off its no-frills clients. And if you're all about speed, Surfshark is the current fastest VPN in my tests.This article originally appeared on Engadget at https://www.engadget.com/cybersecurity/vpn/how-to-cancel-expressvpn-and-get-a-full-refund-123020142.html?src=rss",
          "content": "ExpressVPN is one of the best VPNs on the market, with user-friendly apps, excellent speed test scores and a strong security record. In my ExpressVPN review, I found it to live up to its positive word of mouth, especially when unblocking foreign streaming sites. But no service is perfect, and my cup of VPN tea is not everybody's. If you're looking to switch away, follow this guide to cancel ExpressVPN. How to cancel ExpressVPN on desktop No matter where you originally signed up for ExpressVPN, you can cancel through your browser on a desktop platform. With any browser (i.e. Chrome, Safari, etc), the steps are as follows. Note that doing this will instantly cancel your ExpressVPN subscription and revoke your access to the service. Go to expressvpn.com. In the top bar, click on My Account. Enter your username and password to sign in. You'll be taken to your account dashboard. In the menu on the left-hand side of the screen, click Subscription. Scroll down until you find the words \"Subscription details.\" You should see a box containing the monthly cost of your subscription. At the right of that box, click Cancel Subscription. From here, follow the on-screen prompts to complete cancellation. Sam Chapman for Engadget If you're not ready to lose ExpressVPN service just yet, you can end auto-renewal instead of cancelling altogether. You'll still be able to use ExpressVPN until your subscription runs out. To cancel auto-renew, go to the subscription tab of your account dashboard and find the box with your subscription ID (it should be right at the top). In that box, click the link that says Edit subscription settings, then scroll down and click Turn Off Automatic Renewal. If you change your mind before your subscription lapses, you'll be able to turn it back on later. How to cancel ExpressVPN on mobile You can also cancel ExpressVPN on your phone or tablet, but the process is largely the same — some buttons are just in different places. As above, this instantly ends your ExpressVPN subscription. Here's how to do it. Go to expressvpn.com in your mobile browser. Tap the three horizontal lines at the top-right, then in the menu that appears, tap My Account. In the account dashboard, tap the three horizontal lines at the top-right once again. This time, scroll down to the expanded \"My Account\" menu and tap Subscription. Scroll down to the \"Subscription details\" box and tap Cancel Subscription. Follow the prompts on the screen to complete cancellation. Like on desktop, you can also turn off auto-renewal so your subscription ends when your current pay period expires. Go to the subscription tab of your account dashboard as described in the steps, find the box with your subscription ID, then follow the steps from the last paragraph of the previous section. Cancelling ExpressVPN through an app store The website is almost always the right way to cancel ExpressVPN, but there is one exception: if you originally signed up through the Google Play Store or Apple App Store. This includes both paid subscriptions and the 7-day free trials ExpressVPN offers to app store users. In this case, you'll need to cancel through the app store where you originally made the purchase. On Apple, open the app store, then tap Subscriptions and scroll down to find ExpressVPN. On Android, open Google Play and tap Payment & subscriptions, then Subscriptions. In both cases, once you've found ExpressVPN, tap it and scroll down to find the button for cancellation. How to get a refund from ExpressVPN ExpressVPN offers a full refund to anyone who cancels a subscription within 30 days of purchase. You can also get a refund if your subscription renews without your consent — if that happens, you have 14 days to request your money back. Sam Chapman for Engadget There's no dedicated button for asking for a refund. Instead, you'll need to contact ExpressVPN. To do that, go to expressvpn.com/support, then click on the button in the bottom-right corner that says Need help? Chat with us! This will open a chat with a bot which you can use to ask for your money back. Unfortunately, there's no public link to email the support team, so live chat is the only option here. ExpressVPN alternatives Depending on what made you want to cancel ExpressVPN, there are a few other VPNs you might like better. Proton VPN is my favorite provider at the moment, combining great apps with a unique focus on privacy rights (it's also cheaper). NordVPN has a lot of useful features ExpressVPN leaves off its no-frills clients. And if you're all about speed, Surfshark is the current fastest VPN in my tests.This article originally appeared on Engadget at https://www.engadget.com/cybersecurity/vpn/how-to-cancel-expressvpn-and-get-a-full-refund-123020142.html?src=rss",
          "feed_position": 45,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2025-10/f6e12b90-a9fc-11f0-b39d-36379e1ff141"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/ace-prevents-context-collapse-with-evolving-playbooks-for-self-improving-ai",
          "published_at": "Thu, 16 Oct 2025 12:00:00 GMT",
          "title": "ACE prevents context collapse with ‘evolving playbooks’ for self-improving AI agents",
          "standfirst": "A new framework from Stanford University and SambaNova addresses a critical challenge in building robust AI agents: context engineering. Called Agentic Context Engineering (ACE), the framework automatically populates and modifies the context window of large language model (LLM) applications by treating it as an “evolving playbook” that creates and refines strategies as the agent gains experience in its environment.ACE is designed to overcome key limitations of other context-engineering frameworks, preventing the model’s context from degrading as it accumulates more information. Experiments show that ACE works for both optimizing system prompts and managing an agent&#x27;s memory, outperforming other methods while also being significantly more efficient.The challenge of context engineeringAdvanced AI applications that use LLMs largely rely on \"context adaptation,\" or context engineering, to guide their behavior. Instead of the costly process of retraining or fine-tuning the model, developers use the LLM’s in-context learning abilities to guide its behavior by modifying the input prompts with specific instructions, reasoning steps, or domain-specific knowledge. This additional information is usually obtained as the agent interacts with its environment and gathers new data and experience. The key goal of context engineering is to organize this new information in a way that improves the model’s performance and avoids confusing it. This approach is becoming a central paradigm for building capable, scalable, and self-improving AI systems.Context engineering has several advantages for enterprise applications. Contexts are interpretable for both users and developers, can be updated with new knowledge at runtime, and can be shared across different models. Context engineering also benefits from ongoing hardware and software advances, such as the growing context windows of LLMs and efficient inference techniques like prompt and context caching.There are various automated context-engineering techniques, but most of them face two key limitations. The first is a “brevity bias,” where prompt optimization methods tend to favor concise, generic instructions over comprehensive, detailed ones. This can undermine performance in complex domains. The second, more severe issue is \"context collapse.\" When an LLM is tasked with repeatedly rewriting its entire accumulated context, it can suffer from a kind of digital amnesia.“What we call ‘context collapse’ happens when an AI tries to rewrite or compress everything it has learned into a single new version of its prompt or memory,” the researchers said in written comments to VentureBeat. “Over time, that rewriting process erases important details—like overwriting a document so many times that key notes disappear. In customer-facing systems, this could mean a support agent suddenly losing awareness of past interactions... causing erratic or inconsistent behavior.”The researchers argue that “contexts should function not as concise summaries, but as comprehensive, evolving playbooks—detailed, inclusive, and rich with domain insights.” This approach leans into the strength of modern LLMs, which can effectively distill relevance from long and detailed contexts.How Agentic Context Engineering (ACE) worksACE is a framework for comprehensive context adaptation designed for both offline tasks, like system prompt optimization, and online scenarios, such as real-time memory updates for agents. Rather than compressing information, ACE treats the context like a dynamic playbook that gathers and organizes strategies over time.The framework divides the labor across three specialized roles: a Generator, a Reflector, and a Curator. This modular design is inspired by “how humans learn—experimenting, reflecting, and consolidating—while avoiding the bottleneck of overloading a single model with all responsibilities,” according to the paper.The workflow starts with the Generator, which produces reasoning paths for input prompts, highlighting both effective strategies and common mistakes. The Reflector then analyzes these paths to extract key lessons. Finally, the Curator synthesizes these lessons into compact updates and merges them into the existing playbook.To prevent context collapse and brevity bias, ACE incorporates two key design principles. First, it uses incremental updates. The context is represented as a collection of structured, itemized bullets instead of a single block of text. This allows ACE to make granular changes and retrieve the most relevant information without rewriting the entire context.Second, ACE uses a “grow-and-refine” mechanism. As new experiences are gathered, new bullets are appended to the playbook and existing ones are updated. A de-duplication step regularly removes redundant entries, ensuring the context remains comprehensive yet relevant and compact over time.ACE in actionThe researchers evaluated ACE on two types of tasks that benefit from evolving context: agent benchmarks requiring multi-turn reasoning and tool use, and domain-specific financial analysis benchmarks demanding specialized knowledge. For high-stakes industries like finance, the benefits extend beyond pure performance. As the researchers said, the framework is “far more transparent: a compliance officer can literally read what the AI learned, since it’s stored in human-readable text rather than hidden in billions of parameters.”The results showed that ACE consistently outperformed strong baselines such as GEPA and classic in-context learning, achieving average performance gains of 10.6% on agent tasks and 8.6% on domain-specific benchmarks in both offline and online settings.Critically, ACE can build effective contexts by analyzing the feedback from its actions and environment instead of requiring manually labeled data. The researchers note that this ability is a \"key ingredient for self-improving LLMs and agents.\" On the public AppWorld benchmark, designed to evaluate agentic systems, an agent using ACE with a smaller open-source model (DeepSeek-V3.1) matched the performance of the top-ranked, GPT-4.1-powered agent on average and surpassed it on the more difficult test set.The takeaway for businesses is significant. “This means companies don’t have to depend on massive proprietary models to stay competitive,” the research team said. “They can deploy local models, protect sensitive data, and still get top-tier results by continuously refining context instead of retraining weights.”Beyond accuracy, ACE proved to be highly efficient. It adapts to new tasks with an average 86.9% lower latency than existing methods and requires fewer steps and tokens. The researchers point out that this efficiency demonstrates that “scalable self-improvement can be achieved with both higher accuracy and lower overhead.”For enterprises concerned about inference costs, the researchers point out that the longer contexts produced by ACE do not translate to proportionally higher costs. Modern serving infrastructures are increasingly optimized for long-context workloads with techniques like KV cache reuse, compression, and offloading, which amortize the cost of handling extensive context.Ultimately, ACE points toward a future where AI systems are dynamic and continuously improving. \"Today, only AI engineers can update models, but context engineering opens the door for domain experts—lawyers, analysts, doctors—to directly shape what the AI knows by editing its contextual playbook,\" the researchers said. This also makes governance more practical. \"Selective unlearning becomes much more tractable: if a piece of information is outdated or legally sensitive, it can simply be removed or replaced in the context, without retraining the model.”",
          "content": "A new framework from Stanford University and SambaNova addresses a critical challenge in building robust AI agents: context engineering. Called Agentic Context Engineering (ACE), the framework automatically populates and modifies the context window of large language model (LLM) applications by treating it as an “evolving playbook” that creates and refines strategies as the agent gains experience in its environment.ACE is designed to overcome key limitations of other context-engineering frameworks, preventing the model’s context from degrading as it accumulates more information. Experiments show that ACE works for both optimizing system prompts and managing an agent&#x27;s memory, outperforming other methods while also being significantly more efficient.The challenge of context engineeringAdvanced AI applications that use LLMs largely rely on \"context adaptation,\" or context engineering, to guide their behavior. Instead of the costly process of retraining or fine-tuning the model, developers use the LLM’s in-context learning abilities to guide its behavior by modifying the input prompts with specific instructions, reasoning steps, or domain-specific knowledge. This additional information is usually obtained as the agent interacts with its environment and gathers new data and experience. The key goal of context engineering is to organize this new information in a way that improves the model’s performance and avoids confusing it. This approach is becoming a central paradigm for building capable, scalable, and self-improving AI systems.Context engineering has several advantages for enterprise applications. Contexts are interpretable for both users and developers, can be updated with new knowledge at runtime, and can be shared across different models. Context engineering also benefits from ongoing hardware and software advances, such as the growing context windows of LLMs and efficient inference techniques like prompt and context caching.There are various automated context-engineering techniques, but most of them face two key limitations. The first is a “brevity bias,” where prompt optimization methods tend to favor concise, generic instructions over comprehensive, detailed ones. This can undermine performance in complex domains. The second, more severe issue is \"context collapse.\" When an LLM is tasked with repeatedly rewriting its entire accumulated context, it can suffer from a kind of digital amnesia.“What we call ‘context collapse’ happens when an AI tries to rewrite or compress everything it has learned into a single new version of its prompt or memory,” the researchers said in written comments to VentureBeat. “Over time, that rewriting process erases important details—like overwriting a document so many times that key notes disappear. In customer-facing systems, this could mean a support agent suddenly losing awareness of past interactions... causing erratic or inconsistent behavior.”The researchers argue that “contexts should function not as concise summaries, but as comprehensive, evolving playbooks—detailed, inclusive, and rich with domain insights.” This approach leans into the strength of modern LLMs, which can effectively distill relevance from long and detailed contexts.How Agentic Context Engineering (ACE) worksACE is a framework for comprehensive context adaptation designed for both offline tasks, like system prompt optimization, and online scenarios, such as real-time memory updates for agents. Rather than compressing information, ACE treats the context like a dynamic playbook that gathers and organizes strategies over time.The framework divides the labor across three specialized roles: a Generator, a Reflector, and a Curator. This modular design is inspired by “how humans learn—experimenting, reflecting, and consolidating—while avoiding the bottleneck of overloading a single model with all responsibilities,” according to the paper.The workflow starts with the Generator, which produces reasoning paths for input prompts, highlighting both effective strategies and common mistakes. The Reflector then analyzes these paths to extract key lessons. Finally, the Curator synthesizes these lessons into compact updates and merges them into the existing playbook.To prevent context collapse and brevity bias, ACE incorporates two key design principles. First, it uses incremental updates. The context is represented as a collection of structured, itemized bullets instead of a single block of text. This allows ACE to make granular changes and retrieve the most relevant information without rewriting the entire context.Second, ACE uses a “grow-and-refine” mechanism. As new experiences are gathered, new bullets are appended to the playbook and existing ones are updated. A de-duplication step regularly removes redundant entries, ensuring the context remains comprehensive yet relevant and compact over time.ACE in actionThe researchers evaluated ACE on two types of tasks that benefit from evolving context: agent benchmarks requiring multi-turn reasoning and tool use, and domain-specific financial analysis benchmarks demanding specialized knowledge. For high-stakes industries like finance, the benefits extend beyond pure performance. As the researchers said, the framework is “far more transparent: a compliance officer can literally read what the AI learned, since it’s stored in human-readable text rather than hidden in billions of parameters.”The results showed that ACE consistently outperformed strong baselines such as GEPA and classic in-context learning, achieving average performance gains of 10.6% on agent tasks and 8.6% on domain-specific benchmarks in both offline and online settings.Critically, ACE can build effective contexts by analyzing the feedback from its actions and environment instead of requiring manually labeled data. The researchers note that this ability is a \"key ingredient for self-improving LLMs and agents.\" On the public AppWorld benchmark, designed to evaluate agentic systems, an agent using ACE with a smaller open-source model (DeepSeek-V3.1) matched the performance of the top-ranked, GPT-4.1-powered agent on average and surpassed it on the more difficult test set.The takeaway for businesses is significant. “This means companies don’t have to depend on massive proprietary models to stay competitive,” the research team said. “They can deploy local models, protect sensitive data, and still get top-tier results by continuously refining context instead of retraining weights.”Beyond accuracy, ACE proved to be highly efficient. It adapts to new tasks with an average 86.9% lower latency than existing methods and requires fewer steps and tokens. The researchers point out that this efficiency demonstrates that “scalable self-improvement can be achieved with both higher accuracy and lower overhead.”For enterprises concerned about inference costs, the researchers point out that the longer contexts produced by ACE do not translate to proportionally higher costs. Modern serving infrastructures are increasingly optimized for long-context workloads with techniques like KV cache reuse, compression, and offloading, which amortize the cost of handling extensive context.Ultimately, ACE points toward a future where AI systems are dynamic and continuously improving. \"Today, only AI engineers can update models, but context engineering opens the door for domain experts—lawyers, analysts, doctors—to directly shape what the AI knows by editing its contextual playbook,\" the researchers said. This also makes governance more practical. \"Selective unlearning becomes much more tractable: if a piece of information is outdated or legally sensitive, it can simply be removed or replaced in the context, without retraining the model.”",
          "feed_position": 8,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/4EJUoAdNGN4myXdb69GnMD/f8ab6aaf2305a72ad84dcc5e4afb1beb/agentic_context_engineering.jpg"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/get-50-percent-off-your-first-year-subscription-to-one-of-our-favorite-budgeting-apps-174011646.html",
          "published_at": "Thu, 16 Oct 2025 09:01:26 +0000",
          "title": "Get 50 percent off your first year subscription to one of our favorite budgeting apps",
          "standfirst": "Monarch Money is one of our favorite budgeting apps and, fittingly enough, there's a way for newcomers to save money on a subscription right now. If you use the code MONARCHVIP at checkout, you can get an annual plan for 50 percent off. It typically costs $100, but you can get 12 months of access for $50 with this code. There are some key caveats here. The discount is only for new users, and it can't be combined with other offers. The code only works when you sign up through the web. You can't redeem it through the Monarch mobile app. We feel that Monarch has a steeper learning curve than some other budget trackers and that certain aspects of the app are slightly more complex than they probably need to be. But it offers a great deal of customization and granularity, which outweighs our misgivings. On the main dashboard, you'll see your net worth along with your latest transactions, spending versus the previous month, your income so far for the month and details about upcoming bills, your investments and goals you've set. There's also a link to a month-in-review page, which offers an in-depth overview of what's been happening with your money that month. You'll also be able to take a peek at how your net worth has changed over time. Monarch can connect to your bank and track Apple Card, Apple Cash and Savings accounts. It can pull in your transactions and balance history automatically and detect your recurring expenses and income. The app can even keep your car valuation up to date. While it might take a little work to set up Monarch (and you might have to tweak things here and there), it's a detailed budgeting app that can help you keep better track of your income, expenditure and net worth.This article originally appeared on Engadget at https://www.engadget.com/deals/get-50-percent-off-your-first-year-subscription-to-one-of-our-favorite-budgeting-apps-174011646.html?src=rss",
          "content": "Monarch Money is one of our favorite budgeting apps and, fittingly enough, there's a way for newcomers to save money on a subscription right now. If you use the code MONARCHVIP at checkout, you can get an annual plan for 50 percent off. It typically costs $100, but you can get 12 months of access for $50 with this code. There are some key caveats here. The discount is only for new users, and it can't be combined with other offers. The code only works when you sign up through the web. You can't redeem it through the Monarch mobile app. We feel that Monarch has a steeper learning curve than some other budget trackers and that certain aspects of the app are slightly more complex than they probably need to be. But it offers a great deal of customization and granularity, which outweighs our misgivings. On the main dashboard, you'll see your net worth along with your latest transactions, spending versus the previous month, your income so far for the month and details about upcoming bills, your investments and goals you've set. There's also a link to a month-in-review page, which offers an in-depth overview of what's been happening with your money that month. You'll also be able to take a peek at how your net worth has changed over time. Monarch can connect to your bank and track Apple Card, Apple Cash and Savings accounts. It can pull in your transactions and balance history automatically and detect your recurring expenses and income. The app can even keep your car valuation up to date. While it might take a little work to set up Monarch (and you might have to tweak things here and there), it's a detailed budgeting app that can help you keep better track of your income, expenditure and net worth.This article originally appeared on Engadget at https://www.engadget.com/deals/get-50-percent-off-your-first-year-subscription-to-one-of-our-favorite-budgeting-apps-174011646.html?src=rss",
          "feed_position": 47
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/computing/accessories/best-bluetooth-tracker-140028377.html",
          "published_at": "Thu, 16 Oct 2025 09:00:36 +0000",
          "title": "The best Bluetooth trackers for 2025",
          "standfirst": "Most people think of AirTags when they picture a Bluetooth tracker. And indeed, Apple’s little white discs used to be the most capable option, relying on a vast finding network of nearby iPhones to pinpoint lost tags. But now, both Google and Samsung have implemented finding networks of their own. And other Bluetooth tracker companies, like Chipolo and Pebblebee, now have trackers that pair with either Google or Apple’s network too. In short, you’ve got a lot of options for tagging and tracking your keys, backpacks, luggage and more. So we tested all the major brands out there to see how they work and put together a guide to help you get the most out of your chosen tracker. Here are the best Bluetooth trackers you can buy. Table of contents The best Bluetooth trackers for 2025 What to look for in a Bluetooth tracking device How we tested Bluetooth trackers Other Bluetooth trackers we tested Bluetooth tracker FAQs Best Bluetooth trackers for 2025 What to look for in a Bluetooth tracking device Bluetooth trackers are small discs or cards that rely on short-range, low-energy wireless signals to communicate with your smartphone. Attach one of these gadgets your stuff and, if it’s in range, your phone can “ring” the chip so you can find it. These tracking devices offer other features like separation alerts to tell you when you’ve left a tagged item behind, or where a lost item was last detected. Some can even tap into a larger network of smartphones to track down your device when you’re out of range. Depending on what you want the tracker to do, there are a few specs to look for when deciding which to get. Device compatibility Like most things from the folks in Cupertino, AirTags only work with products in the Apple ecosystem. Both Apple and Google have opened up access to the Find My and Find Hub networks to third-party manufacturers, including Chipolo and Pebblebee. Those two companies make device-agnostic models that will work with the larger tracking network from either brand, so iPhone and Android users can buy the same tag. Tile trackers work with either Android or Apple devices, but use Tile’s own Life 360 finding network. Samsung’s latest fob, the Galaxy SmartTag2, only works with Samsung phones and taps into a finding system that relies on other Samsung devices to locate lost tags. Finding network Crowd-sourced finding capabilities are what make headlines, with stories about recovering stolen equipment or tracking lost luggage across the globe. Using anonymous signals that ping other people’s devices, these Bluetooth tracking devices can potentially tell you where a tagged item is, even if your smartphone is out of Bluetooth range. Apple’s Find My network is the largest, with over a billion iPhones and iPads in service all running Apple’s Find My app by default. So unless an iPhone user opts out, their phone silently acts as a location detector for any nearby AirTags. Apple recently increased the AirTag’s finding power by enabling you to share the location of a lost tag with a third party, party, like an airline. Chipolo fobs that work on Apple’s network have the same ability. Google launched its Find My Device network in 2024 and has since renamed it Find Hub, which, like Apple's fining app, combines devices and people finding in one place. That network is now a close second for the largest in the US Now that Google’s Find Hub network is up and running, it’s a close second for the largest in the US. Like Apple, Android users are automatically part of the network, but can opt-out by selecting the Google services option in their phone’s Settings app and toggling the option in the Find Hub menu. Samsung’s SmartTag 2 and related network also defaults to an opt-in status for finding tags and other devices. Tile offers a large finding grid that includes Tile users, Amazon Sidewalk customers and people running the Life360 network. Life360 acquired Tile in 2021, and, according to the company, the Life360 network has more than 70 million monthly active users. In our tests, AirTags and third-party tags using its network, like the Chipolo Loop and Pop and the Pebblebee Clip Universal, were the fastest to track down lost items. They offered nearly real-time location data in moderately to heavily trafficked spots around Albuquerque, including a bar, bookstore and coffee shops in Nob Hill, along with various outdoor hangouts on UNM’s campus. Samsung's SmartTags were able to locate our lost items most of the time, though not with the same precision finding accuracy as AirTags. When we tested Google’s Find Hub (then called Find My Device) network right after launch, it was noticeably slower than Apple’s network when using the community finding feature. Testing it again in 2025, the time it took to locate a lost item was considerably improved, taking less than 20 minutes on average for the community to track a fob. In our tests, Tile’s finding network wasn’t able to consistently locate its lost fobs. Amy Skorheim / Engadget Separation alerts A tracker’s day-to-day utility becomes really apparent when it prevents you from losing something in the first place. Separation alerts tell you when you’ve traveled too far from your tagged items. Useful if you want to make sure your laptop bag, jacket or umbrella always comes with you when you leave the house. Apple’s Find My app delivers these notifications, but Google’s Find Hub does not. However, if you have a Chipolo device and allow its companion app to run in the background on your Android phone, left-behind alerts are enabled. Tile trackers require a yearly subscription to enable the alerts (currently $7 to $25 monthly). Both AirTags and Tiles allow you to turn off separation alerts at certain locations, meaning you can set your home as a “safe” place where items can be left behind, but alerts will still trigger elsewhere. In our tests, AirTags and others using the Find My network alerted us between the 600- and 1,400-foot mark. Tiles sent a notification after about an average of 1,500 feet and were more consistent when using an Android phone than an iPhone. Chipolo Pop tags paired with an Android phone and using its own app sent an alert when we got around 450 feet away from our tagged item. Connectivity and volume The feature you may use most often is the key finder function, which makes the tracker ring when you hit a button in the app. With Apple's AirTags, you can say \"Hey Siri, where are my keys?\" and the assistant will ring the tag (assuming it doesn't mistakenly think you're asking for directions to the Floridian archipelago). You can also use the Find Item app in your Apple Watch to ring your fob. Asking smart home/personal assistants like Alexa or the Google Assistant to find your keys will work with Chipolo, Tile and Pebblebee trackers linked to your Android device. If you have your tag but can’t find your phone, some trackers will let you ring them to find your handset. SmartTag2 fobs reliably rang our Galaxy phone when we double-pressed it. Tile trackers have the same feature. Chipolo Pop and Loop trackers can ring your phone, but uses the Chipolo app to do so, which can run concurrently with the Find My or Find Hub connection. AirTags and third-party tags using Google’s network don’t offer this feature. The volume of the Bluetooth tracking device may determine whether you can find an item buried in your couch cushions or in a noisy room. AirTags have a reputation for being on the quiet side, and that aligned with what we saw (measuring roughly 65 decibels). Chipolo’s Pop tags and Tile’s Pro model measure between 83 and 86 decibels on average. Pebblebee’s new Clip Universal was the loudest of any tag we’ve tested, clocking in at 91 ear-splitting decibels. Design and alternative formats Design will determine what you can attach the tracker to. AirTags are small, smooth discs that can’t be secured to anything without accessories, which are numerous, but that is an additional cost to consider. Chipolo, Pebblebee and Tile offer trackers with holes that easily attach to your key ring, and all three companies also offer card-shaped versions designed to fit in your wallet. Pebblebee Clip Universal tags come with a handy carabiner-style key ring. You can even get trackers embedded into useful items like luggage locks. The SmartLock from KeySmart is a TSA-approved luggage lock, but in addition to the three digit code, it’s also a Bluetooth tracker that’s compatible with Apple Find My. It wasn’t quite as loud as other trackers in my tests, and the range wasn’t as long, but it paired easily and worked with Apple’s finding network just like an AirTag. Battery life AirTag, Tile Pro, SmartTag2 and Chipolo Pop fobs use replaceable batteries and each should go for at least a year before needing to be swapped. Tile Mate and card-shaped trackers don’t have replaceable batteries, which means you’ll have to replace the entire unit whenever it dies. Pebblebee Clip Universal Clip Universal and Chipolo Loop trackers are rechargeable via a standard USB-C port. They’re also equipped with onboard LEDs (though the light on the Loop is barely noticeable). Stalking, theft and data privacy AirTags have gotten a lot of attention and even prompted some lawsuits for Apple due to bad actors planting them on people in order to stalk them. While this fact may not influence your buying decision, any discussion of Bluetooth trackers should note what steps Apple, Google and Tile have taken to address the issue. Last year, all the major players in the Bluetooth tracker business teamed up to combat misuse and standardize how unauthorized tracking detection and alerts work for iOS and Android. Last year, Tile launched a feature called Anti-Theft Mode, which enables you to render one of its trackers undetectable by others. That means if someone steals your tagged item, they won’t be able to use the anti-stalking features to find and disable the tracker. That sort of negates one of the major ways potential stalking victims can stay safe, so Tile hopes ID verification and a $1 million penalty will deter misuse. As a theft deterrent, a Bluetooth tracker may or may not be the best option. Anecdotal stories abound in which people have recovered stolen goods using a tracker — but other tales are more cautionary. Neither Apple nor Google promotes its trackers or finding networks as a way to deal with theft. GPS trackers, on the other hand, are typically marketed for just that purpose. How we tested Bluetooth trackers Before deciding on which trackers to test, we researched the field, looking at user reviews on Amazon, Best Buy and other retailers, along with discussions on sites like Reddit. We also checked out what other publications had to say on the matter before narrowing down our options. Here’s the full list of every tracker we tested: Apple AirTag Chipolo Card Spot Chipolo One Spot Chipolo One Chipolo Card Chipolo Loop Chipolo Pop KeySmart SmartLock Motorola Moto Tag Pebblebe Clip Universal Pebblebee Clip Samsung SmartTag 2 Chipolo One Point Pebblebee Clip for Android Tile Pro (2024) Tile Mate (2024) Tile Mate (2022) Tile Pro (2022) Tile Slim (2022) After acquiring the trackers, I tested each one over the course of a few weeks using both an iPhone 11 followed by an iPhone 16 and a Samsung Galaxy S22 then an S23 Ultra. I recreated likely user experiences, such as losing and leaving items behind at home and out in the city. I planted trackers at different spots near downtown Albuquerque, mostly concentrated in and around the University of New Mexico and the surrounding neighborhood of Nob Hill. Later, I conducted tests in the Queen Anne neighborhood of Seattle. Each test was performed multiple times, both while walking and driving and I used the measure distance feature on Google Maps to track footage for alerts. I paid attention to how easy the app was to use, how reliable the phone-to-tracker connection was and any other perks and drawbacks that came up during regular use. As new trackers come to market, or as we learn of worthy models to try, I'll test them and add the results to this guide. Other Bluetooth trackers we tested Motorola Moto Tag The Moto Tag haunts me. At this very moment, my Galaxy phone says the fob is “Near you right now.” But I don’t know where. I tap to play a sound and the Find Hub tries, but ultimately says it can’t. I tap the Find Nearby function that’s supposed to visually guide you to the tag. I parade my phone around the house like a divining rod, take it down into the basement, walk it all over the garage. Nothing. But the Hub app unendingly says the Moto Tag is “Near you right now” and I get flashes of every old-school horror movie where the telephone operator tells the soon-to-be victim that the call is coming from inside the house. It’s partly my fault. I tend to keep good tabs on the gadgets I test for work. But during my most recent move, the tiny green disc didn’t make it into the safety of my review unit cabinet after relocation. Perhaps in retribution for my neglect, the Moto Tag keeps itself just out of reach. Taunting me. I’ll let you know if I ever find it, but in the meantime, it’s clear this finding device doesn’t want to be found. The recommended tags in this guide will serve you better. Tile Pro and Tile Mate (2024) Tile recently came out with a new suite of trackers, replacing the Tile Mate, Tile Pro, Tile Sticker and Tile Slim with updated models. In addition to fun new colors for the Mate and Slim, Tile added an SOS feature that can send a notification to your Life360 Circle when you triple press the button on the tracker. It’s a clever addition that turns your keys into a panic button, something offered by personal safety companies as standalone devices. There are a few caveats: You and the people you want to notify in an emergency will need the Life360 app installed on your phones. If you want your Tile to also trigger a call to emergency services, you’ll need a $15-per-month Life360 subscription (that’s in addition to a Tile membership, which starts at $3/month or $30 annually). And enabling the SOS triple-press disables the ability to ring your phone with the fob. I tested the SOS feature and it did indeed send a text message to my Circle, with the message that I had triggered an SOS and a link to a website that showed my current location. I thought it odd that the link didn’t open the Life360 app (which shows the location of users' phones), but I wasn’t as much concerned with Tile’s personal safety features as I was with the tracking capabilities, which turned out to be less than ideal. For my tests, I planted Tile trackers in a densely populated area of Seattle (about 15,000 people per square mile). After setting the trackers to “lost” in the Tile app, I waited. After four hours, one of the trackers was not discovered by the finding community, so I went and retrieved it. Another fob I planted alerted me that the tracker had been found by the Tile community after three hours — but the location it gave me was off by a third of a mile. I then decided to plant a tracker in the busiest place I could think of — the dried fruit and nuts aisle of a Trader Joes on a Friday evening before a major holiday. It still took over a half an hour before another Tile user anonymously pinged my lost tracker. In my tests with Samsung’s trackers and the fobs on Google’s Find Hub network, it took around ten minutes for them to be discovered. AirTags took half that time and all were tested in a far less populated city. Four hours with no ping and over a half hour before getting a hit in a crowded TJs were pretty long stretches. Tile devices work with both mobile operating systems and its latest models are indeed louder than they were before. But they aren’t as quick to connect and you need to pay for a membership to activate left-behind alerts. And when you do, those notifications don’t kick in as quickly as they do with competing trackers. Bluetooth tracker FAQs Which Bluetooth tracker has the longest range? Both the Tile Pro and the Samsung Galaxy SmartTag2 claim a maximum range of around 400 feet, which is longer than the 300-foot claim for Chipolo’s Pop tags. The Pebblebee Clip Universal claims a 500-foot range, though other trackers with a shorter claimed range performed better in our tests. Apple doesn’t make range claims for AirTags. Any Bluetooth signal, of course, is dependent on a few factors. Obstacles like walls and people can block the signal, so a clear line of sight is the only way to achieve the maximum range. Other signals, like Wi-Fi, can also interfere with Bluetooth connections. Even high humidity can have an effect and lessen the distance at which your phone will connect to your tracker. Remember, when considering the range of Bluetooth trackers, the size of the “finding network” also comes into play. This is the number of nearby phones that can be used to anonymously ping your tracker when your own phone is out of Bluetooth range. As of now, Apple AirTags have the largest network, followed by Google’s Find Hub, Samsung’s finding community and Tile’s Life360 members. What is the best Bluetooth tracker for a car? Bluetooth trackers are designed to track small, personal items like keys, jackets, backpacks and the like. All trackers have safeguards to prohibit the tag from being used to stalk people, so most will alert someone if a tracker that does not belong to them is detected following them. That means a car thief may get tipped off that there’s a tracker in the car they’re trying to steal. That said, you’ll see plenty of stories about people finding their car thanks to a Bluetooth tracker. Some police departments have even handed out trackers to combat high rates of carjacking. In most instances, the tracker of choice has been AirTags thanks to their wide finding network. If you’re looking for a tracker for your car, you may want to look into GPS trackers, some of which are designed for just that purpose. How accurate are Bluetooth trackers? Accuracy for Bluetooth trackers can be looked at in two ways: Finding items nearby and finding items misplaced outside your home. For nearby items, you’ll most often use the ring function on the device to hunt it down. Apple’s AirTags also use ultra-wideband technology, which creates directional navigation on your phone to get you within a foot of the tracker. Accurately finding lost items outside your home depends on the size of the finding network. Since this relies on the serendipity of a random phone passing within Bluetooth range of your tracker, the more phones on a given network, the better. And since Bluetooth ranges and distance estimates are only precise within about a meter or so, getting pings from more than one phone will help locating items. Here again, it’s worth noting that Apple’s Find My network is the largest, followed by Google, Samsung and Tile (both Chipolo and Pebblebee have fobs that work with the Apple and Google networks). Recent Updates October 2025: Added Chipolo Loop as a new pick for best rechargeable Bluetooth tracker. Detailed our experience with the Moto Tag and KeySmart SmartLock. Updated details about separation alerts and Ultra Wideband tech. August 2025: Updated the name of Google's finding network to Find Hub, instead of Find My Device. Added details about Pebblebee's new Alert feature. Added a table of contents. This article originally appeared on Engadget at https://www.engadget.com/computing/accessories/best-bluetooth-tracker-140028377.html?src=rss",
          "content": "Most people think of AirTags when they picture a Bluetooth tracker. And indeed, Apple’s little white discs used to be the most capable option, relying on a vast finding network of nearby iPhones to pinpoint lost tags. But now, both Google and Samsung have implemented finding networks of their own. And other Bluetooth tracker companies, like Chipolo and Pebblebee, now have trackers that pair with either Google or Apple’s network too. In short, you’ve got a lot of options for tagging and tracking your keys, backpacks, luggage and more. So we tested all the major brands out there to see how they work and put together a guide to help you get the most out of your chosen tracker. Here are the best Bluetooth trackers you can buy. Table of contents The best Bluetooth trackers for 2025 What to look for in a Bluetooth tracking device How we tested Bluetooth trackers Other Bluetooth trackers we tested Bluetooth tracker FAQs Best Bluetooth trackers for 2025 What to look for in a Bluetooth tracking device Bluetooth trackers are small discs or cards that rely on short-range, low-energy wireless signals to communicate with your smartphone. Attach one of these gadgets your stuff and, if it’s in range, your phone can “ring” the chip so you can find it. These tracking devices offer other features like separation alerts to tell you when you’ve left a tagged item behind, or where a lost item was last detected. Some can even tap into a larger network of smartphones to track down your device when you’re out of range. Depending on what you want the tracker to do, there are a few specs to look for when deciding which to get. Device compatibility Like most things from the folks in Cupertino, AirTags only work with products in the Apple ecosystem. Both Apple and Google have opened up access to the Find My and Find Hub networks to third-party manufacturers, including Chipolo and Pebblebee. Those two companies make device-agnostic models that will work with the larger tracking network from either brand, so iPhone and Android users can buy the same tag. Tile trackers work with either Android or Apple devices, but use Tile’s own Life 360 finding network. Samsung’s latest fob, the Galaxy SmartTag2, only works with Samsung phones and taps into a finding system that relies on other Samsung devices to locate lost tags. Finding network Crowd-sourced finding capabilities are what make headlines, with stories about recovering stolen equipment or tracking lost luggage across the globe. Using anonymous signals that ping other people’s devices, these Bluetooth tracking devices can potentially tell you where a tagged item is, even if your smartphone is out of Bluetooth range. Apple’s Find My network is the largest, with over a billion iPhones and iPads in service all running Apple’s Find My app by default. So unless an iPhone user opts out, their phone silently acts as a location detector for any nearby AirTags. Apple recently increased the AirTag’s finding power by enabling you to share the location of a lost tag with a third party, party, like an airline. Chipolo fobs that work on Apple’s network have the same ability. Google launched its Find My Device network in 2024 and has since renamed it Find Hub, which, like Apple's fining app, combines devices and people finding in one place. That network is now a close second for the largest in the US Now that Google’s Find Hub network is up and running, it’s a close second for the largest in the US. Like Apple, Android users are automatically part of the network, but can opt-out by selecting the Google services option in their phone’s Settings app and toggling the option in the Find Hub menu. Samsung’s SmartTag 2 and related network also defaults to an opt-in status for finding tags and other devices. Tile offers a large finding grid that includes Tile users, Amazon Sidewalk customers and people running the Life360 network. Life360 acquired Tile in 2021, and, according to the company, the Life360 network has more than 70 million monthly active users. In our tests, AirTags and third-party tags using its network, like the Chipolo Loop and Pop and the Pebblebee Clip Universal, were the fastest to track down lost items. They offered nearly real-time location data in moderately to heavily trafficked spots around Albuquerque, including a bar, bookstore and coffee shops in Nob Hill, along with various outdoor hangouts on UNM’s campus. Samsung's SmartTags were able to locate our lost items most of the time, though not with the same precision finding accuracy as AirTags. When we tested Google’s Find Hub (then called Find My Device) network right after launch, it was noticeably slower than Apple’s network when using the community finding feature. Testing it again in 2025, the time it took to locate a lost item was considerably improved, taking less than 20 minutes on average for the community to track a fob. In our tests, Tile’s finding network wasn’t able to consistently locate its lost fobs. Amy Skorheim / Engadget Separation alerts A tracker’s day-to-day utility becomes really apparent when it prevents you from losing something in the first place. Separation alerts tell you when you’ve traveled too far from your tagged items. Useful if you want to make sure your laptop bag, jacket or umbrella always comes with you when you leave the house. Apple’s Find My app delivers these notifications, but Google’s Find Hub does not. However, if you have a Chipolo device and allow its companion app to run in the background on your Android phone, left-behind alerts are enabled. Tile trackers require a yearly subscription to enable the alerts (currently $7 to $25 monthly). Both AirTags and Tiles allow you to turn off separation alerts at certain locations, meaning you can set your home as a “safe” place where items can be left behind, but alerts will still trigger elsewhere. In our tests, AirTags and others using the Find My network alerted us between the 600- and 1,400-foot mark. Tiles sent a notification after about an average of 1,500 feet and were more consistent when using an Android phone than an iPhone. Chipolo Pop tags paired with an Android phone and using its own app sent an alert when we got around 450 feet away from our tagged item. Connectivity and volume The feature you may use most often is the key finder function, which makes the tracker ring when you hit a button in the app. With Apple's AirTags, you can say \"Hey Siri, where are my keys?\" and the assistant will ring the tag (assuming it doesn't mistakenly think you're asking for directions to the Floridian archipelago). You can also use the Find Item app in your Apple Watch to ring your fob. Asking smart home/personal assistants like Alexa or the Google Assistant to find your keys will work with Chipolo, Tile and Pebblebee trackers linked to your Android device. If you have your tag but can’t find your phone, some trackers will let you ring them to find your handset. SmartTag2 fobs reliably rang our Galaxy phone when we double-pressed it. Tile trackers have the same feature. Chipolo Pop and Loop trackers can ring your phone, but uses the Chipolo app to do so, which can run concurrently with the Find My or Find Hub connection. AirTags and third-party tags using Google’s network don’t offer this feature. The volume of the Bluetooth tracking device may determine whether you can find an item buried in your couch cushions or in a noisy room. AirTags have a reputation for being on the quiet side, and that aligned with what we saw (measuring roughly 65 decibels). Chipolo’s Pop tags and Tile’s Pro model measure between 83 and 86 decibels on average. Pebblebee’s new Clip Universal was the loudest of any tag we’ve tested, clocking in at 91 ear-splitting decibels. Design and alternative formats Design will determine what you can attach the tracker to. AirTags are small, smooth discs that can’t be secured to anything without accessories, which are numerous, but that is an additional cost to consider. Chipolo, Pebblebee and Tile offer trackers with holes that easily attach to your key ring, and all three companies also offer card-shaped versions designed to fit in your wallet. Pebblebee Clip Universal tags come with a handy carabiner-style key ring. You can even get trackers embedded into useful items like luggage locks. The SmartLock from KeySmart is a TSA-approved luggage lock, but in addition to the three digit code, it’s also a Bluetooth tracker that’s compatible with Apple Find My. It wasn’t quite as loud as other trackers in my tests, and the range wasn’t as long, but it paired easily and worked with Apple’s finding network just like an AirTag. Battery life AirTag, Tile Pro, SmartTag2 and Chipolo Pop fobs use replaceable batteries and each should go for at least a year before needing to be swapped. Tile Mate and card-shaped trackers don’t have replaceable batteries, which means you’ll have to replace the entire unit whenever it dies. Pebblebee Clip Universal Clip Universal and Chipolo Loop trackers are rechargeable via a standard USB-C port. They’re also equipped with onboard LEDs (though the light on the Loop is barely noticeable). Stalking, theft and data privacy AirTags have gotten a lot of attention and even prompted some lawsuits for Apple due to bad actors planting them on people in order to stalk them. While this fact may not influence your buying decision, any discussion of Bluetooth trackers should note what steps Apple, Google and Tile have taken to address the issue. Last year, all the major players in the Bluetooth tracker business teamed up to combat misuse and standardize how unauthorized tracking detection and alerts work for iOS and Android. Last year, Tile launched a feature called Anti-Theft Mode, which enables you to render one of its trackers undetectable by others. That means if someone steals your tagged item, they won’t be able to use the anti-stalking features to find and disable the tracker. That sort of negates one of the major ways potential stalking victims can stay safe, so Tile hopes ID verification and a $1 million penalty will deter misuse. As a theft deterrent, a Bluetooth tracker may or may not be the best option. Anecdotal stories abound in which people have recovered stolen goods using a tracker — but other tales are more cautionary. Neither Apple nor Google promotes its trackers or finding networks as a way to deal with theft. GPS trackers, on the other hand, are typically marketed for just that purpose. How we tested Bluetooth trackers Before deciding on which trackers to test, we researched the field, looking at user reviews on Amazon, Best Buy and other retailers, along with discussions on sites like Reddit. We also checked out what other publications had to say on the matter before narrowing down our options. Here’s the full list of every tracker we tested: Apple AirTag Chipolo Card Spot Chipolo One Spot Chipolo One Chipolo Card Chipolo Loop Chipolo Pop KeySmart SmartLock Motorola Moto Tag Pebblebe Clip Universal Pebblebee Clip Samsung SmartTag 2 Chipolo One Point Pebblebee Clip for Android Tile Pro (2024) Tile Mate (2024) Tile Mate (2022) Tile Pro (2022) Tile Slim (2022) After acquiring the trackers, I tested each one over the course of a few weeks using both an iPhone 11 followed by an iPhone 16 and a Samsung Galaxy S22 then an S23 Ultra. I recreated likely user experiences, such as losing and leaving items behind at home and out in the city. I planted trackers at different spots near downtown Albuquerque, mostly concentrated in and around the University of New Mexico and the surrounding neighborhood of Nob Hill. Later, I conducted tests in the Queen Anne neighborhood of Seattle. Each test was performed multiple times, both while walking and driving and I used the measure distance feature on Google Maps to track footage for alerts. I paid attention to how easy the app was to use, how reliable the phone-to-tracker connection was and any other perks and drawbacks that came up during regular use. As new trackers come to market, or as we learn of worthy models to try, I'll test them and add the results to this guide. Other Bluetooth trackers we tested Motorola Moto Tag The Moto Tag haunts me. At this very moment, my Galaxy phone says the fob is “Near you right now.” But I don’t know where. I tap to play a sound and the Find Hub tries, but ultimately says it can’t. I tap the Find Nearby function that’s supposed to visually guide you to the tag. I parade my phone around the house like a divining rod, take it down into the basement, walk it all over the garage. Nothing. But the Hub app unendingly says the Moto Tag is “Near you right now” and I get flashes of every old-school horror movie where the telephone operator tells the soon-to-be victim that the call is coming from inside the house. It’s partly my fault. I tend to keep good tabs on the gadgets I test for work. But during my most recent move, the tiny green disc didn’t make it into the safety of my review unit cabinet after relocation. Perhaps in retribution for my neglect, the Moto Tag keeps itself just out of reach. Taunting me. I’ll let you know if I ever find it, but in the meantime, it’s clear this finding device doesn’t want to be found. The recommended tags in this guide will serve you better. Tile Pro and Tile Mate (2024) Tile recently came out with a new suite of trackers, replacing the Tile Mate, Tile Pro, Tile Sticker and Tile Slim with updated models. In addition to fun new colors for the Mate and Slim, Tile added an SOS feature that can send a notification to your Life360 Circle when you triple press the button on the tracker. It’s a clever addition that turns your keys into a panic button, something offered by personal safety companies as standalone devices. There are a few caveats: You and the people you want to notify in an emergency will need the Life360 app installed on your phones. If you want your Tile to also trigger a call to emergency services, you’ll need a $15-per-month Life360 subscription (that’s in addition to a Tile membership, which starts at $3/month or $30 annually). And enabling the SOS triple-press disables the ability to ring your phone with the fob. I tested the SOS feature and it did indeed send a text message to my Circle, with the message that I had triggered an SOS and a link to a website that showed my current location. I thought it odd that the link didn’t open the Life360 app (which shows the location of users' phones), but I wasn’t as much concerned with Tile’s personal safety features as I was with the tracking capabilities, which turned out to be less than ideal. For my tests, I planted Tile trackers in a densely populated area of Seattle (about 15,000 people per square mile). After setting the trackers to “lost” in the Tile app, I waited. After four hours, one of the trackers was not discovered by the finding community, so I went and retrieved it. Another fob I planted alerted me that the tracker had been found by the Tile community after three hours — but the location it gave me was off by a third of a mile. I then decided to plant a tracker in the busiest place I could think of — the dried fruit and nuts aisle of a Trader Joes on a Friday evening before a major holiday. It still took over a half an hour before another Tile user anonymously pinged my lost tracker. In my tests with Samsung’s trackers and the fobs on Google’s Find Hub network, it took around ten minutes for them to be discovered. AirTags took half that time and all were tested in a far less populated city. Four hours with no ping and over a half hour before getting a hit in a crowded TJs were pretty long stretches. Tile devices work with both mobile operating systems and its latest models are indeed louder than they were before. But they aren’t as quick to connect and you need to pay for a membership to activate left-behind alerts. And when you do, those notifications don’t kick in as quickly as they do with competing trackers. Bluetooth tracker FAQs Which Bluetooth tracker has the longest range? Both the Tile Pro and the Samsung Galaxy SmartTag2 claim a maximum range of around 400 feet, which is longer than the 300-foot claim for Chipolo’s Pop tags. The Pebblebee Clip Universal claims a 500-foot range, though other trackers with a shorter claimed range performed better in our tests. Apple doesn’t make range claims for AirTags. Any Bluetooth signal, of course, is dependent on a few factors. Obstacles like walls and people can block the signal, so a clear line of sight is the only way to achieve the maximum range. Other signals, like Wi-Fi, can also interfere with Bluetooth connections. Even high humidity can have an effect and lessen the distance at which your phone will connect to your tracker. Remember, when considering the range of Bluetooth trackers, the size of the “finding network” also comes into play. This is the number of nearby phones that can be used to anonymously ping your tracker when your own phone is out of Bluetooth range. As of now, Apple AirTags have the largest network, followed by Google’s Find Hub, Samsung’s finding community and Tile’s Life360 members. What is the best Bluetooth tracker for a car? Bluetooth trackers are designed to track small, personal items like keys, jackets, backpacks and the like. All trackers have safeguards to prohibit the tag from being used to stalk people, so most will alert someone if a tracker that does not belong to them is detected following them. That means a car thief may get tipped off that there’s a tracker in the car they’re trying to steal. That said, you’ll see plenty of stories about people finding their car thanks to a Bluetooth tracker. Some police departments have even handed out trackers to combat high rates of carjacking. In most instances, the tracker of choice has been AirTags thanks to their wide finding network. If you’re looking for a tracker for your car, you may want to look into GPS trackers, some of which are designed for just that purpose. How accurate are Bluetooth trackers? Accuracy for Bluetooth trackers can be looked at in two ways: Finding items nearby and finding items misplaced outside your home. For nearby items, you’ll most often use the ring function on the device to hunt it down. Apple’s AirTags also use ultra-wideband technology, which creates directional navigation on your phone to get you within a foot of the tracker. Accurately finding lost items outside your home depends on the size of the finding network. Since this relies on the serendipity of a random phone passing within Bluetooth range of your tracker, the more phones on a given network, the better. And since Bluetooth ranges and distance estimates are only precise within about a meter or so, getting pings from more than one phone will help locating items. Here again, it’s worth noting that Apple’s Find My network is the largest, followed by Google, Samsung and Tile (both Chipolo and Pebblebee have fobs that work with the Apple and Google networks). Recent Updates October 2025: Added Chipolo Loop as a new pick for best rechargeable Bluetooth tracker. Detailed our experience with the Moto Tag and KeySmart SmartLock. Updated details about separation alerts and Ultra Wideband tech. August 2025: Updated the name of Google's finding network to Find Hub, instead of Find My Device. Added details about Pebblebee's new Alert feature. Added a table of contents. This article originally appeared on Engadget at https://www.engadget.com/computing/accessories/best-bluetooth-tracker-140028377.html?src=rss",
          "feed_position": 48,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2023-02/94489620-abc7-11ed-b375-842957054bf1"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/entertainment/streaming/best-live-tv-streaming-service-133000410.html",
          "published_at": "Thu, 16 Oct 2025 07:00:36 +0000",
          "title": "The best live TV streaming services to cut cable in 2025",
          "standfirst": "We still think getting a live TV streaming service is a better deal than paying for cable — but the gulf between the two options is narrowing. Now that many of the major providers go for more than $80 per month, it’s not the amazing deal it once was. Still, live TV streaming plans have no contract, offer relatively simpler pricing and need no special equipment beyond a smart TV and an internet connection.There are a handful of major players and, after testing them all, we found YouTube TV to be the most well-rounded. But there are a lot of changes happening in the world of live TV streaming, with new services from ESPN and Fox, as well as the upcoming consolidation of Hulu into Disney. We’re testing out the new services and we’ll let you know how the new options affect your options. For now, these are the best live TV streaming services we tested, along with our breakdowns of how to stream and what you get for your money.Editor's note: There’s been a lot of hubbub in the streaming world lately. YouTube TV subscribers nearly missed out on access to NBC Universal channels — but the companies ultimately came to a multi-year agreement. Disney (and by extension Hulu+ Live TV) lost a whole bunch of subscribers over free speech dissent — and then announced a price hike for Hulu+ Live TV, bringing it to $90 monthly. Philo implemented a $5 monthly price increase, but users will now get to watch HBO Max and Discovery+, so it’s not all bad. Speaking of HBO Max, customers will no longer get CNN’s livestream as of November 17 because the latter entity is going to try yet again to offer its own service. Table of contents Best live TV streaming services for 2025 How to stream live NFL games Best free live TV streaming services for 2025 What to look for in a live TV streaming service How we tested Live TV Streaming FAQs Recent updates Best live TV streaming services for 2025 Back to top How to stream live NFL games The rights to air regular-season NFL games belong to a number of networks. Around 200 games are scheduled to appear Sundays on CBS/Paramount+ and Fox/Fox One. NBC/Peacock will host one Sunday night competition each week while Prime Video will air Thursday night contests (except for Thanksgiving week) and ABC/ESPN will show Monday night matchups. A few games will be exclusive to the NFL Network and Christmas-day games will air live on Netflix. YouTube aired a single week-one game. You can see the complete 2025 NFL schedule here (the airing network appears just below the game time on the list). On many Sundays, multiple games are scheduled to air at the same time by the same broadcaster. That means Fox and CBS will broadcast regional games through the associated local affiliate station. Select national games will air through Fox One and Paramount+. To see all Sunday (daytime) matchups, you’ll need the NFL Sunday Ticket that’s now exclusive to YouTube TV and costs between $35 and $115 per month depending on the type of subscription you choose (YouTube recently announced monthly options for the Sunday Ticket). Note that the subscription doesn’t include Sunday night games — for that, you’ll need Peacock and/or local NBC station access through YouTube TV or elsewhere. Most of the paid live TV streaming services we recommend here include the stations you’ll need to see most of the games. YouTube TV, Fubo TV (including the new, cheaper Fubo Sports package), Hulu + Live TV and DirecTV (Signature packages and MySports Genre packs) offer local Fox, CBS, ABC and NBC stations in most (but not all areas). They also carry sports-focused channels from those networks, like Fox Sports, CBS Sports and ESPN. Sling’s Orange plan includes access to a few local channels (varying by area), and also carries ESPN, but you’ll need the combined Orange and Blue plan to also get the Fox Sports channel — but neither plan carries CBS Sports. How can I stream NFL games for free? If you have a digital antenna hooked up to your TV, you can grab games that are broadcast over the airways for your region by tuning into your local CBS, Fox, NBC and ABC stations. You can buy a digital antenna for between $20 and $60. Of course, that won’t get you the games that are exclusive to the NFL Network, Prime Video or Netflix, and you won’t be able to watch games broadcast outside your area. Nearly all paid live TV streaming services are currently offering free trials ranging from a few days to a week. You could hop from service to service, catching a few games before cancelling and not pay anything, but with 18 weeks in the regular season, you’ll obviously not be able to watch all games for free. Alternatively, you can check out your local sports bar and watch a game for the price of a soda and maybe some nachos. As it turns out, bars and restaurants that provide those games to customers have to pay a ton of cash to do so, so you may as well take advantage of the opportunity. Does Paramount Plus stream live NFL games? Yes. Paramount owns CBS, which has historically held the rights to air many NFL games each season. This year, NFL on CBS includes more than 100 regular-season games, most of them Sunday matchups. You can see which NFL games will air on CBS/Paramount + here. Note that to watch your local CBS station you need Paramount+ Premium (formerly Paramount+ with Showtime) for $13 per month. Can you stream live football on YouTube? September 5, 2025 marked the first time YouTube was an official live NFL broadcaster when it aired a Friday night, week-one game of the 2025 NFL season from São Paulo, Brazil. It pit the Los Angeles Chargers against the Kansas City Chiefs (LA won 21-27) and aired worldwide on YouTube for free as well as for subscribers to YouTube TV. There are no other plans for YouTube to air live NFL games for the 2025 season for free, but paid YouTube TV customers will be able to watch many live matchups on their local CBS, Fox, NBC and ABC stations as part of their subscription. Both YouTube TV subscribers and anyone with the YouTube app can subscribe to the NFL Sunday Ticket add-on for $35 to $60 monthly, depending on promotions. Through the YouTube app, you can also purchase access to other Primetime Channels including Paramount+, but it costs the same as paying for those accounts directly. Best free live TV streaming services for 2025 There are loads of ways to get free TV these days. To start, many standard streaming apps have added live components to their lineups — even Netflix. Peacock Premium Plus subscriptions include regional NBC stations. Paramount+ Premium subscribers can watch on-air CBS programming. The new Fox One service includes multiple live Fox stations. True, if you’re already paying for a service it’s not technically “free” but at least the live content isn’t extra. The smart TV operating system (OS) you use likely provides free live content too: Amazon’s Fire TV, Google/Android TV, Roku’s built-in Roku Channel and Samsung’s TV Plus all have hundreds of live channels and original programming. Some of the paid services we recommend above have a free version — namely Sling Freestream, Fubo Free (available after you cancel) and DirecTV’s MyFree. But if you’re looking for more, here are the best free ad-supported TV (FAST) apps with live TV that we tried: Back to top What to look for in a live TV streaming service How to stream live TV Streaming live TV is a lot like using Netflix. You get access through apps on your phone, tablet, smart TV or streaming device and the signal arrives over the internet. A faster and more stable connection tends to give you a better experience. Most live TV apps require you to sign up and pay via a web browser. After that, you can activate the app on all of your devices. Monthly Price When I started testing these cord-cutting alternatives, I was struck by the price difference between live TV and a standard video streaming app. Where the latter cost between $5 and $20 per month, most live TV services hit the $80 mark and can go higher than $200 with additional perks, channel packages and premium extras. The higher starting price is mostly due to the cost of providing multiple networks — particularly sports and local stations. And, in the past year or so, every service has raised base plan prices. Local channels Only two of the services I tried don’t include full local channel coverage for subscribers and one of those makes no effort to carry sports at all. That would be Philo and, as you might guess, it’s the cheapest. The next most affordable option, Sling, only carries three local stations — and only in larger markets — but it still manages to include some of the top sports channels. When you sign up with any provider that handles local TV, you’ll enter your zip code, ensuring you get your area’s broadcast affiliates for ABC, CBS, FOX and NBC. Of course, you can also get those stations for free. Nearly all modern television sets support a radio frequency (RF) connection, also known as the coaxial port, which means if you buy an HD antenna, you’ll receive locally broadcast stations like ABC, CBS, PBS, FOX and NBC. And since the signal is digital, reception is much improved over the staticky rabbit-ears era. But local channel access is another area where traditional streaming services, like Netflix, are bleeding into broadcast territory. For example, you can watch your local NBC station with a Peacock subscription and you can tune into your area’s CBS station through your Paramount+ subscription. Netflix is even getting into the mix with a recently announced deal with one of France’s broadcast companies, TF1. The streaming service will now air TF1's live TV channels and on-demand content inside the Netflix app. No word if the concept will expand to other regions, but it’s an interesting move to anyone interested in the future of streaming. Live sports coverage One reality that spun my head was the sheer number and iterations of sports networks in existence. Trying to figure out which network will carry the match-up you want to see can be tricky. I found that Google makes it a little easier for sports fans by listing out upcoming games (just swap in NBA, NFL, MLB, NHL and so on in the search bar). When you click an event, the “TV & streaming” button will tell you which network is covering it. That just leaves figuring out if your chosen service carries the RSNs (regional sports networks) you want. Unfortunately, even with add-ons and extra packages, some providers simply don’t have certain channels in their lineups. It would take a lawyer to understand the ins and outs of streaming rights negotiations, and networks leave and return to live TV carriers all the time. That said, most major sporting events in the US are covered by ESPN, Fox Sports, TNT, USA and local affiliates. I should also point out that traditional streaming services have started adding live sports to their lineups. Peacock carries live Premier League matches, Sunday Night Football games and aired the 2024 Olympic Games from Paris. Thursday Night Football as well as NBA and WNBA games are on Amazon Prime and Christmas Day Football airs on Netflix. HBO Max (formerly, er, HBO Max) now airs select, regular season games from the NHL, MLB, NCAA and NBA with a $10-per-month add-on. You can watch MLS games with an add-on through the Apple TV app, and Apple TV+ (now just called Apple TV) includes some MLB games. Roku users can watch the just-added free sports channel and those who subscribe to Paramount Plus can see many of the matches aired on CBS Sports, including live NFL games. In 2025, January's Super Bowl was live-streamed for free on Tubi. While all of these alternatives may not cover as much ground as live TV streamers, they could end up being cheaper avenues to the sports you want. And if sports is all you’re after, there are sports-only plans that are a touch cheaper, too. The promised sports streaming service from ESPN, Fox and Warner Bros. called Venu was cancelled early this year. But on August 21, ESPN launched its own streaming service that includes all ESPN channels and costs $30 per month. Fubo Sports is $56 monthly and includes local broadcast stations from ABC, CBS and FOX plus a slew of sports networks (CBS Sport and FS1 among them) as well as all networks included with ESPN Unlimited. Fox launched its own standalone service in August as well and it includes Fox Sports and all other Fox properties (News, Business, Weather) for $20 monthly. DirecTV also has a $70-per-month, sports-only streaming package called MySports and Comcast has a sports and news bundle for that same price (as long as you're an Xfinity customer with auto-pay, otherwise it's more expensive). Traditional cable networks Dozens of linear programming networks were once only available with cable TV, like Bravo, BET, Food Network, HGTV, CNN, Lifetime, SYFY and MTV. If you only subscribe to, say, Netflix or Apple TV+, you won’t have access to those. But as with sports, standard streamers are starting to incorporate this content into their offerings. After the Warner Bros. merger, Max incorporated some content from HGTV, Discovery and TLC. Peacock has Bravo and Hallmark shows, and Paramount+ has material from Nickelodeon, MTV and Comedy Central. Other entertainment channels like AMC+ have stand-alone apps. The Discovery+ app gives you 15 channels ad-free for $10 per month (or with ads for $6 monthly). And a service called Frndly TV starts at a mere $7 per month and streams A&E, Lifetime, Game Show Network, Outdoor Channel and about 35 others. Of course, most live TV streaming options will deliver more sizable lists of cable networks, but just note that you may already be paying for some of them — and if all you need is a certain channel, you could get it cheaper by subscribing directly. On-demand streaming Most live TV subscriptions include access to a selection of video-on-demand (VOD) content, like you would get with a traditional streaming service. Much of this content is made up of the movies and TV series that have recently aired on your subscribed networks. This typically doesn’t cover live events and news programming, but I was able to watch specific episodes of ongoing shows like Top Chef or BET’s Diarra from Detroit. Just search the on-demand library for the program, pick an episode and hit play. Partnerships, like Hulu’s relationship with Disney, and add-ons, such as bundling Max with your YouTube TV subscription or Starz with your Sling plan, will let you watch even larger libraries of on-demand content. But again, if VOD is all you’re after, paying for those networks directly instead of through a live TV plan will be far cheaper. Digital video recordings (DVR) limits Every option I tried offers some cloud DVR storage without needing a separate physical device. You’ll either get unlimited storage for recordings that expires after nine months or a year, or you’ll get a set number of hours (between 50 and 1,000) that you can keep indefinitely. Typically, all you need to do is designate what ongoing TV series you want to record and the DVR component will do all the hard work of saving subsequent episodes for you to watch later. You can do the same thing with sports events. Aside from being able to watch whenever it’s most convenient, you can also fast-forward through commercials in recorded content. In contrast, you can’t skip them on live TV or VOD. Simultaneous streams and profiles per account Each plan gives you a certain number of simultaneous streams, aka how many screens can play content at the same time. And while most providers will let you travel with your subscription, there are usually location restrictions that require you to sign in from your home IP address periodically. Stream allowances range from one at a time to unlimited screens (or as many as your ISP’s bandwidth can handle). Some plans require add-ons to get more screens. Most services also let you set up a few profiles so I was able to give different people in my family the ability to build their own watch histories and libraries, set their favorite channels and get individual recommendations. Picture-in-picture mode and multiview Picture-in-picture (PiP) usually refers to shrinking a video window on a mobile device or computer browser so you can watch it while using other apps. Sling, YouTube TV, FuboTV, Philo, DirecTV Stream and Hulu + Live TV all have PiP modes on computers and mobile devices. Another feature, multiview, lets you view multiple (usually four) sports matches or other live content at once on your TV screen. YouTube TV, FuboTV and now DirecTV all let you do this. With YouTube TV, you can select up to four views from a few preset selection of streams. FuboTV offers the same feature, but only if you're using an Apple TV or Roku streaming device. DirecTV lets you do so through “mixes” which include sports, news, business and kids variants with a set four channels in each mix. 4K live streams Right now, just FuboTV, YouTube TV and DirecTV Stream offer 4K live streams — but with caveats. YouTube TV requires a $20-per-month add-on, after which you’ll only be able to watch certain live content in 4K. DirecTV Stream has three channels that show live 4K content — one with shows and original series, and two with occasional sporting events. You don’t have to pay extra for these but you do need to have either DirecTV’s Gemini receiver, or a device from Fire TV, Apple TV or Roku. You’ll need those same streaming devices to watch the select 4K programming on Sling as well. FuboTV shows certain live events in 4K but access is limited to the Elite and Premier packages, not the base-level Pro plan. Of course, watching any 4K content also requires equipment that can handle it: a 4K smart TV or 4K streaming device paired with a cord and screen that can handle 4K resolution. Tiers, packages and add-ons Comparing price-to-offering ratios is a task for a spreadsheet. I… made three. The base plans range from $28 to $85 per month. From there, you can add packages, which are usually groups of live TV channels bundled by themes like news, sports, entertainment or international content. Premium VOD extras like Max, AMC+ and Starz are also available. Add-ons cost an extra $5 to $20 each per month and simply show up in the guide where you find the rest of your live TV. This is where streaming can quickly get expensive, pushing an $80 subscription to $200 monthly, depending on what you choose. How to stream live TV for free I also downloaded and tried out a few apps that offer free ad-supported TV (FAST) including Freevee, Tubi, PlutoTV and Sling Freestream. These let you drop in and watch a more limited selection of live networks at zero cost. Most don’t even require an email address, let alone a credit card. And if you have a Roku device, an Amazon Fire TV or Stick, a Samsung TV, a Chromecast device or a Google TV, you already have access to hundreds of live channels via the Roku Channel, the live tab in Fire TV, through the Samsung TV Plus app or through Google TV. Back to top How we tested live TV streaming services When I begin testing for a guide, I research the most popular and well-reviewed players in the category and narrow down which are worth trying. For the paid plans, just six services dominate so I tried them all. There are considerably more free live TV contenders so I tested the four most popular. After getting accounts set up using my laptop, I downloaded the apps on a Samsung smart TV running the latest version of Tizen OS. I counted the local stations and regional sports coverage, and noted how many of the top cable networks were available. I then weighed the prices, base packages and available add-ons. I then looked at how the programming was organized in each app’s UI and judged how easy everything was to navigate, from the top navigation to the settings. To test the search function, I searched for the same few TV shows on BET, Food Network, HGTV and Comedy Central, since all six providers carry those channels. I noted how helpful the searches were and how quickly they got me to season 6, episode 13 of Home Town. I used DVR to record entire series and single movies and watched VOD shows, making sure to test the pause and scan functions. On each service with sports, I searched for the same four upcoming NHL, NBA, MLS and NCAA basketball matches and used the record option to save the games and play them back a day or two later. Finally, I noted any extra perks or irritating quirks. All live TV streaming services we’ve tested: Philo Sling YouTube TV Hulu + Live TV DirecTV Stream FuboTV Freevee Tubi PlutoTV Sling Freestream Plex Back to top Live TV Streaming FAQs What is live streaming? Streaming simply refers to video content that is delivered to your screen over the internet. Live streaming can be split into two categories: linear programming and simultaneous transmission. That first one is similar to what you get with cable or broadcast TV, with channels that play a constant flow of movies and shows (sort of what TV looked like before Netflix). Simultaneous streaming lets you watch live events (like a basketball game) or a program (like the evening news) as they happen. What is the difference between streaming and live streaming? Standard streaming, the most popular example being Netflix, lets you pick what you want to watch from a menu of choices. It’s also referred to as “video on demand.” Live streaming refers to sports and news events that you can stream as they happen in real time. It also refers to channels that show a continuous, linear flow of programming. What streaming service is best for live TV? FuboTV does the best job of letting you organize live channels to help you find just what you want to watch. The interface is uncluttered and when you search for something, the UI clearly tells you whether something is live now or on-demand. YouTube TV also does a good job making that info clear. Both have just over 100 live channels on offer. What is the most cost effective TV streaming service? Free TV streaming services like PlutoTV, Plex, Tubi and FreeVee show plenty of ad-supported TV shows and movies without charging you anything. Of course, they won’t have the same channels or content that more premium subscriptions have. Ultimately it depends on what you want to watch and finding the service that can supply that to you in the most streamlined form so you’re not paying for stuff you don’t need. Is it cheaper to have cable or streaming? A basic cable package used to be more expensive than the base-level live TV streaming service. But now that nearly all major providers have raised their prices to over $75 per month, that’s no longer the case. And with add-ons and other premiums, you can easily pay over $200 a month for either cable or a live TV streaming service. But those who want to cut the cord will appreciate that streaming services don't have contracts. What streaming service has all the TV channels? No service that we tested had every available channel. Hulu + Live TV and DirecTV Stream carry the the highest number of the top rated channels, according to Neilsen. Hulu’s service also gets you Disney+ fare, which you can’t get elsewhere. FuboTV has the most sports channels and YouTube TV gives you the widest selection of add-ons. What is the most popular live TV streaming platform? YouTube TV has the most paying customers. According to 2024's letter from the CEO, the service has over eight million subscribers. Disney’s 2024 third quarter earnings put the Hulu + Live TV viewer count at 4.6 million. Sling’s customer count dipped from two million to about 1.9 million in 2024 and FuboTV grew its subscriber list to 1.6 million. How safe are free streaming services and websites? You may have heard certain sites that provide free content can be dangerous, leading to stolen info and/or exposing you to malware. That’s likely in reference to certain peer-to-peer (P2P) networks and file-sharing sites that let people download free movies and series — which can come bundled with malicious code. But if you’re talking about the free ad-supported streaming television (FAST) services listed here, from providers like PlutoTV, Tubi and Plex, they are just as safe as any other streaming service. Since you sometimes don’t even have to provide your email address or credit card info, they can even be more anonymous for cord cutters than apps that require login credentials. Back to top Recent updates October 2025: Added information about the upcoming price increases for Hulu+ Live TV and Philo. Back to topThis article originally appeared on Engadget at https://www.engadget.com/entertainment/streaming/best-live-tv-streaming-service-133000410.html?src=rss",
          "content": "We still think getting a live TV streaming service is a better deal than paying for cable — but the gulf between the two options is narrowing. Now that many of the major providers go for more than $80 per month, it’s not the amazing deal it once was. Still, live TV streaming plans have no contract, offer relatively simpler pricing and need no special equipment beyond a smart TV and an internet connection.There are a handful of major players and, after testing them all, we found YouTube TV to be the most well-rounded. But there are a lot of changes happening in the world of live TV streaming, with new services from ESPN and Fox, as well as the upcoming consolidation of Hulu into Disney. We’re testing out the new services and we’ll let you know how the new options affect your options. For now, these are the best live TV streaming services we tested, along with our breakdowns of how to stream and what you get for your money.Editor's note: There’s been a lot of hubbub in the streaming world lately. YouTube TV subscribers nearly missed out on access to NBC Universal channels — but the companies ultimately came to a multi-year agreement. Disney (and by extension Hulu+ Live TV) lost a whole bunch of subscribers over free speech dissent — and then announced a price hike for Hulu+ Live TV, bringing it to $90 monthly. Philo implemented a $5 monthly price increase, but users will now get to watch HBO Max and Discovery+, so it’s not all bad. Speaking of HBO Max, customers will no longer get CNN’s livestream as of November 17 because the latter entity is going to try yet again to offer its own service. Table of contents Best live TV streaming services for 2025 How to stream live NFL games Best free live TV streaming services for 2025 What to look for in a live TV streaming service How we tested Live TV Streaming FAQs Recent updates Best live TV streaming services for 2025 Back to top How to stream live NFL games The rights to air regular-season NFL games belong to a number of networks. Around 200 games are scheduled to appear Sundays on CBS/Paramount+ and Fox/Fox One. NBC/Peacock will host one Sunday night competition each week while Prime Video will air Thursday night contests (except for Thanksgiving week) and ABC/ESPN will show Monday night matchups. A few games will be exclusive to the NFL Network and Christmas-day games will air live on Netflix. YouTube aired a single week-one game. You can see the complete 2025 NFL schedule here (the airing network appears just below the game time on the list). On many Sundays, multiple games are scheduled to air at the same time by the same broadcaster. That means Fox and CBS will broadcast regional games through the associated local affiliate station. Select national games will air through Fox One and Paramount+. To see all Sunday (daytime) matchups, you’ll need the NFL Sunday Ticket that’s now exclusive to YouTube TV and costs between $35 and $115 per month depending on the type of subscription you choose (YouTube recently announced monthly options for the Sunday Ticket). Note that the subscription doesn’t include Sunday night games — for that, you’ll need Peacock and/or local NBC station access through YouTube TV or elsewhere. Most of the paid live TV streaming services we recommend here include the stations you’ll need to see most of the games. YouTube TV, Fubo TV (including the new, cheaper Fubo Sports package), Hulu + Live TV and DirecTV (Signature packages and MySports Genre packs) offer local Fox, CBS, ABC and NBC stations in most (but not all areas). They also carry sports-focused channels from those networks, like Fox Sports, CBS Sports and ESPN. Sling’s Orange plan includes access to a few local channels (varying by area), and also carries ESPN, but you’ll need the combined Orange and Blue plan to also get the Fox Sports channel — but neither plan carries CBS Sports. How can I stream NFL games for free? If you have a digital antenna hooked up to your TV, you can grab games that are broadcast over the airways for your region by tuning into your local CBS, Fox, NBC and ABC stations. You can buy a digital antenna for between $20 and $60. Of course, that won’t get you the games that are exclusive to the NFL Network, Prime Video or Netflix, and you won’t be able to watch games broadcast outside your area. Nearly all paid live TV streaming services are currently offering free trials ranging from a few days to a week. You could hop from service to service, catching a few games before cancelling and not pay anything, but with 18 weeks in the regular season, you’ll obviously not be able to watch all games for free. Alternatively, you can check out your local sports bar and watch a game for the price of a soda and maybe some nachos. As it turns out, bars and restaurants that provide those games to customers have to pay a ton of cash to do so, so you may as well take advantage of the opportunity. Does Paramount Plus stream live NFL games? Yes. Paramount owns CBS, which has historically held the rights to air many NFL games each season. This year, NFL on CBS includes more than 100 regular-season games, most of them Sunday matchups. You can see which NFL games will air on CBS/Paramount + here. Note that to watch your local CBS station you need Paramount+ Premium (formerly Paramount+ with Showtime) for $13 per month. Can you stream live football on YouTube? September 5, 2025 marked the first time YouTube was an official live NFL broadcaster when it aired a Friday night, week-one game of the 2025 NFL season from São Paulo, Brazil. It pit the Los Angeles Chargers against the Kansas City Chiefs (LA won 21-27) and aired worldwide on YouTube for free as well as for subscribers to YouTube TV. There are no other plans for YouTube to air live NFL games for the 2025 season for free, but paid YouTube TV customers will be able to watch many live matchups on their local CBS, Fox, NBC and ABC stations as part of their subscription. Both YouTube TV subscribers and anyone with the YouTube app can subscribe to the NFL Sunday Ticket add-on for $35 to $60 monthly, depending on promotions. Through the YouTube app, you can also purchase access to other Primetime Channels including Paramount+, but it costs the same as paying for those accounts directly. Best free live TV streaming services for 2025 There are loads of ways to get free TV these days. To start, many standard streaming apps have added live components to their lineups — even Netflix. Peacock Premium Plus subscriptions include regional NBC stations. Paramount+ Premium subscribers can watch on-air CBS programming. The new Fox One service includes multiple live Fox stations. True, if you’re already paying for a service it’s not technically “free” but at least the live content isn’t extra. The smart TV operating system (OS) you use likely provides free live content too: Amazon’s Fire TV, Google/Android TV, Roku’s built-in Roku Channel and Samsung’s TV Plus all have hundreds of live channels and original programming. Some of the paid services we recommend above have a free version — namely Sling Freestream, Fubo Free (available after you cancel) and DirecTV’s MyFree. But if you’re looking for more, here are the best free ad-supported TV (FAST) apps with live TV that we tried: Back to top What to look for in a live TV streaming service How to stream live TV Streaming live TV is a lot like using Netflix. You get access through apps on your phone, tablet, smart TV or streaming device and the signal arrives over the internet. A faster and more stable connection tends to give you a better experience. Most live TV apps require you to sign up and pay via a web browser. After that, you can activate the app on all of your devices. Monthly Price When I started testing these cord-cutting alternatives, I was struck by the price difference between live TV and a standard video streaming app. Where the latter cost between $5 and $20 per month, most live TV services hit the $80 mark and can go higher than $200 with additional perks, channel packages and premium extras. The higher starting price is mostly due to the cost of providing multiple networks — particularly sports and local stations. And, in the past year or so, every service has raised base plan prices. Local channels Only two of the services I tried don’t include full local channel coverage for subscribers and one of those makes no effort to carry sports at all. That would be Philo and, as you might guess, it’s the cheapest. The next most affordable option, Sling, only carries three local stations — and only in larger markets — but it still manages to include some of the top sports channels. When you sign up with any provider that handles local TV, you’ll enter your zip code, ensuring you get your area’s broadcast affiliates for ABC, CBS, FOX and NBC. Of course, you can also get those stations for free. Nearly all modern television sets support a radio frequency (RF) connection, also known as the coaxial port, which means if you buy an HD antenna, you’ll receive locally broadcast stations like ABC, CBS, PBS, FOX and NBC. And since the signal is digital, reception is much improved over the staticky rabbit-ears era. But local channel access is another area where traditional streaming services, like Netflix, are bleeding into broadcast territory. For example, you can watch your local NBC station with a Peacock subscription and you can tune into your area’s CBS station through your Paramount+ subscription. Netflix is even getting into the mix with a recently announced deal with one of France’s broadcast companies, TF1. The streaming service will now air TF1's live TV channels and on-demand content inside the Netflix app. No word if the concept will expand to other regions, but it’s an interesting move to anyone interested in the future of streaming. Live sports coverage One reality that spun my head was the sheer number and iterations of sports networks in existence. Trying to figure out which network will carry the match-up you want to see can be tricky. I found that Google makes it a little easier for sports fans by listing out upcoming games (just swap in NBA, NFL, MLB, NHL and so on in the search bar). When you click an event, the “TV & streaming” button will tell you which network is covering it. That just leaves figuring out if your chosen service carries the RSNs (regional sports networks) you want. Unfortunately, even with add-ons and extra packages, some providers simply don’t have certain channels in their lineups. It would take a lawyer to understand the ins and outs of streaming rights negotiations, and networks leave and return to live TV carriers all the time. That said, most major sporting events in the US are covered by ESPN, Fox Sports, TNT, USA and local affiliates. I should also point out that traditional streaming services have started adding live sports to their lineups. Peacock carries live Premier League matches, Sunday Night Football games and aired the 2024 Olympic Games from Paris. Thursday Night Football as well as NBA and WNBA games are on Amazon Prime and Christmas Day Football airs on Netflix. HBO Max (formerly, er, HBO Max) now airs select, regular season games from the NHL, MLB, NCAA and NBA with a $10-per-month add-on. You can watch MLS games with an add-on through the Apple TV app, and Apple TV+ (now just called Apple TV) includes some MLB games. Roku users can watch the just-added free sports channel and those who subscribe to Paramount Plus can see many of the matches aired on CBS Sports, including live NFL games. In 2025, January's Super Bowl was live-streamed for free on Tubi. While all of these alternatives may not cover as much ground as live TV streamers, they could end up being cheaper avenues to the sports you want. And if sports is all you’re after, there are sports-only plans that are a touch cheaper, too. The promised sports streaming service from ESPN, Fox and Warner Bros. called Venu was cancelled early this year. But on August 21, ESPN launched its own streaming service that includes all ESPN channels and costs $30 per month. Fubo Sports is $56 monthly and includes local broadcast stations from ABC, CBS and FOX plus a slew of sports networks (CBS Sport and FS1 among them) as well as all networks included with ESPN Unlimited. Fox launched its own standalone service in August as well and it includes Fox Sports and all other Fox properties (News, Business, Weather) for $20 monthly. DirecTV also has a $70-per-month, sports-only streaming package called MySports and Comcast has a sports and news bundle for that same price (as long as you're an Xfinity customer with auto-pay, otherwise it's more expensive). Traditional cable networks Dozens of linear programming networks were once only available with cable TV, like Bravo, BET, Food Network, HGTV, CNN, Lifetime, SYFY and MTV. If you only subscribe to, say, Netflix or Apple TV+, you won’t have access to those. But as with sports, standard streamers are starting to incorporate this content into their offerings. After the Warner Bros. merger, Max incorporated some content from HGTV, Discovery and TLC. Peacock has Bravo and Hallmark shows, and Paramount+ has material from Nickelodeon, MTV and Comedy Central. Other entertainment channels like AMC+ have stand-alone apps. The Discovery+ app gives you 15 channels ad-free for $10 per month (or with ads for $6 monthly). And a service called Frndly TV starts at a mere $7 per month and streams A&E, Lifetime, Game Show Network, Outdoor Channel and about 35 others. Of course, most live TV streaming options will deliver more sizable lists of cable networks, but just note that you may already be paying for some of them — and if all you need is a certain channel, you could get it cheaper by subscribing directly. On-demand streaming Most live TV subscriptions include access to a selection of video-on-demand (VOD) content, like you would get with a traditional streaming service. Much of this content is made up of the movies and TV series that have recently aired on your subscribed networks. This typically doesn’t cover live events and news programming, but I was able to watch specific episodes of ongoing shows like Top Chef or BET’s Diarra from Detroit. Just search the on-demand library for the program, pick an episode and hit play. Partnerships, like Hulu’s relationship with Disney, and add-ons, such as bundling Max with your YouTube TV subscription or Starz with your Sling plan, will let you watch even larger libraries of on-demand content. But again, if VOD is all you’re after, paying for those networks directly instead of through a live TV plan will be far cheaper. Digital video recordings (DVR) limits Every option I tried offers some cloud DVR storage without needing a separate physical device. You’ll either get unlimited storage for recordings that expires after nine months or a year, or you’ll get a set number of hours (between 50 and 1,000) that you can keep indefinitely. Typically, all you need to do is designate what ongoing TV series you want to record and the DVR component will do all the hard work of saving subsequent episodes for you to watch later. You can do the same thing with sports events. Aside from being able to watch whenever it’s most convenient, you can also fast-forward through commercials in recorded content. In contrast, you can’t skip them on live TV or VOD. Simultaneous streams and profiles per account Each plan gives you a certain number of simultaneous streams, aka how many screens can play content at the same time. And while most providers will let you travel with your subscription, there are usually location restrictions that require you to sign in from your home IP address periodically. Stream allowances range from one at a time to unlimited screens (or as many as your ISP’s bandwidth can handle). Some plans require add-ons to get more screens. Most services also let you set up a few profiles so I was able to give different people in my family the ability to build their own watch histories and libraries, set their favorite channels and get individual recommendations. Picture-in-picture mode and multiview Picture-in-picture (PiP) usually refers to shrinking a video window on a mobile device or computer browser so you can watch it while using other apps. Sling, YouTube TV, FuboTV, Philo, DirecTV Stream and Hulu + Live TV all have PiP modes on computers and mobile devices. Another feature, multiview, lets you view multiple (usually four) sports matches or other live content at once on your TV screen. YouTube TV, FuboTV and now DirecTV all let you do this. With YouTube TV, you can select up to four views from a few preset selection of streams. FuboTV offers the same feature, but only if you're using an Apple TV or Roku streaming device. DirecTV lets you do so through “mixes” which include sports, news, business and kids variants with a set four channels in each mix. 4K live streams Right now, just FuboTV, YouTube TV and DirecTV Stream offer 4K live streams — but with caveats. YouTube TV requires a $20-per-month add-on, after which you’ll only be able to watch certain live content in 4K. DirecTV Stream has three channels that show live 4K content — one with shows and original series, and two with occasional sporting events. You don’t have to pay extra for these but you do need to have either DirecTV’s Gemini receiver, or a device from Fire TV, Apple TV or Roku. You’ll need those same streaming devices to watch the select 4K programming on Sling as well. FuboTV shows certain live events in 4K but access is limited to the Elite and Premier packages, not the base-level Pro plan. Of course, watching any 4K content also requires equipment that can handle it: a 4K smart TV or 4K streaming device paired with a cord and screen that can handle 4K resolution. Tiers, packages and add-ons Comparing price-to-offering ratios is a task for a spreadsheet. I… made three. The base plans range from $28 to $85 per month. From there, you can add packages, which are usually groups of live TV channels bundled by themes like news, sports, entertainment or international content. Premium VOD extras like Max, AMC+ and Starz are also available. Add-ons cost an extra $5 to $20 each per month and simply show up in the guide where you find the rest of your live TV. This is where streaming can quickly get expensive, pushing an $80 subscription to $200 monthly, depending on what you choose. How to stream live TV for free I also downloaded and tried out a few apps that offer free ad-supported TV (FAST) including Freevee, Tubi, PlutoTV and Sling Freestream. These let you drop in and watch a more limited selection of live networks at zero cost. Most don’t even require an email address, let alone a credit card. And if you have a Roku device, an Amazon Fire TV or Stick, a Samsung TV, a Chromecast device or a Google TV, you already have access to hundreds of live channels via the Roku Channel, the live tab in Fire TV, through the Samsung TV Plus app or through Google TV. Back to top How we tested live TV streaming services When I begin testing for a guide, I research the most popular and well-reviewed players in the category and narrow down which are worth trying. For the paid plans, just six services dominate so I tried them all. There are considerably more free live TV contenders so I tested the four most popular. After getting accounts set up using my laptop, I downloaded the apps on a Samsung smart TV running the latest version of Tizen OS. I counted the local stations and regional sports coverage, and noted how many of the top cable networks were available. I then weighed the prices, base packages and available add-ons. I then looked at how the programming was organized in each app’s UI and judged how easy everything was to navigate, from the top navigation to the settings. To test the search function, I searched for the same few TV shows on BET, Food Network, HGTV and Comedy Central, since all six providers carry those channels. I noted how helpful the searches were and how quickly they got me to season 6, episode 13 of Home Town. I used DVR to record entire series and single movies and watched VOD shows, making sure to test the pause and scan functions. On each service with sports, I searched for the same four upcoming NHL, NBA, MLS and NCAA basketball matches and used the record option to save the games and play them back a day or two later. Finally, I noted any extra perks or irritating quirks. All live TV streaming services we’ve tested: Philo Sling YouTube TV Hulu + Live TV DirecTV Stream FuboTV Freevee Tubi PlutoTV Sling Freestream Plex Back to top Live TV Streaming FAQs What is live streaming? Streaming simply refers to video content that is delivered to your screen over the internet. Live streaming can be split into two categories: linear programming and simultaneous transmission. That first one is similar to what you get with cable or broadcast TV, with channels that play a constant flow of movies and shows (sort of what TV looked like before Netflix). Simultaneous streaming lets you watch live events (like a basketball game) or a program (like the evening news) as they happen. What is the difference between streaming and live streaming? Standard streaming, the most popular example being Netflix, lets you pick what you want to watch from a menu of choices. It’s also referred to as “video on demand.” Live streaming refers to sports and news events that you can stream as they happen in real time. It also refers to channels that show a continuous, linear flow of programming. What streaming service is best for live TV? FuboTV does the best job of letting you organize live channels to help you find just what you want to watch. The interface is uncluttered and when you search for something, the UI clearly tells you whether something is live now or on-demand. YouTube TV also does a good job making that info clear. Both have just over 100 live channels on offer. What is the most cost effective TV streaming service? Free TV streaming services like PlutoTV, Plex, Tubi and FreeVee show plenty of ad-supported TV shows and movies without charging you anything. Of course, they won’t have the same channels or content that more premium subscriptions have. Ultimately it depends on what you want to watch and finding the service that can supply that to you in the most streamlined form so you’re not paying for stuff you don’t need. Is it cheaper to have cable or streaming? A basic cable package used to be more expensive than the base-level live TV streaming service. But now that nearly all major providers have raised their prices to over $75 per month, that’s no longer the case. And with add-ons and other premiums, you can easily pay over $200 a month for either cable or a live TV streaming service. But those who want to cut the cord will appreciate that streaming services don't have contracts. What streaming service has all the TV channels? No service that we tested had every available channel. Hulu + Live TV and DirecTV Stream carry the the highest number of the top rated channels, according to Neilsen. Hulu’s service also gets you Disney+ fare, which you can’t get elsewhere. FuboTV has the most sports channels and YouTube TV gives you the widest selection of add-ons. What is the most popular live TV streaming platform? YouTube TV has the most paying customers. According to 2024's letter from the CEO, the service has over eight million subscribers. Disney’s 2024 third quarter earnings put the Hulu + Live TV viewer count at 4.6 million. Sling’s customer count dipped from two million to about 1.9 million in 2024 and FuboTV grew its subscriber list to 1.6 million. How safe are free streaming services and websites? You may have heard certain sites that provide free content can be dangerous, leading to stolen info and/or exposing you to malware. That’s likely in reference to certain peer-to-peer (P2P) networks and file-sharing sites that let people download free movies and series — which can come bundled with malicious code. But if you’re talking about the free ad-supported streaming television (FAST) services listed here, from providers like PlutoTV, Tubi and Plex, they are just as safe as any other streaming service. Since you sometimes don’t even have to provide your email address or credit card info, they can even be more anonymous for cord cutters than apps that require login credentials. Back to top Recent updates October 2025: Added information about the upcoming price increases for Hulu+ Live TV and Philo. Back to topThis article originally appeared on Engadget at https://www.engadget.com/entertainment/streaming/best-live-tv-streaming-service-133000410.html?src=rss",
          "feed_position": 49
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/google-vs-openai-vs-visa-competing-agent-protocols-threaten-the-future-of-ai",
          "published_at": "Thu, 16 Oct 2025 04:00:00 GMT",
          "title": "Google vs. OpenAI vs. Visa: competing agent protocols threaten the future of AI commerce",
          "standfirst": "When Walmart and OpenAI announced that the retailer would integrate with ChatGPT, the question became how quickly OpenAI could deliver on the promise of agents buying things for people. In the battle of AI-enabled commerce, getting agents to securely complete transactions is one of the biggest hurdles. More and more, chat platforms like ChatGPT are replacing browsers and getting very good at surfacing information people search for. Users will ask ChatGPT for the best humidifiers on the market, and when the model returns results, people have no choice but to click the item link and complete the purchase online. AI agents, as of now, don’t have the ability or the trust infrastructure to make people and banking institutions feel safe enough to let it loose on someone’s cash. Enterprises and other industry players understand that, to allow agents to pay for purchases, there must be a common language shared among the model and agent providers, the bank, the merchant, and, to a lesser extent, the buyer. And so, over the past few weeks, three competing agentic commerce standards have emerged: Google announced the Agent Pay Protocol (AP2) with partners including PayPal, American Express, Mastercard, Salesforce and ServiceNow. Soon after, OpenAI and Stripe debuted the Agentic Commerce Protocol (ACP), and just this week, Visa launched the Trusted Agent Protocol (TAP).All these protocols aim to give agents the trust layer they need to convince banks and their customers that they’re money is safe in the hands of an AI agent. But these may also create walled gardens, showing just how immature agentic commerce really is. This is a problem that could cause enterprises to bet on one chat platform and the agentic pay protocol it runs on, instead of interoperability. How are they differentIt’s not new for players to propose several standards. It usually takes years for the industry to coalesce around a single standard, or even to use different protocols and figure out a way to harmonize them. However, the pace of innovation in enterprise moved the needle on that. Fairly quickly, MCP became the de facto channel for tool-use identification, and most companies began setting up MCP servers or connecting to one. (To be clear, it is not a standard yet) But having three different potential standards might slow that process down a bit, because it’s harder to coalesce on a single standard when there are so many to choose from. These protocols all aim to prove authorization. Both AP2 and TAP rely on cryptographic proofs to show an agent is acting on an individual&#x27;s behalf. For TAP, agents are added to an approved list and get a digital key identifying them. AP2 uses a digital contract that serves as a proxy for human approval for the agent. OpenAI’s ACP doesn’t require too much of an infrastructure change, where ACP essentially acts as a courier to the merchant because the agent relays information to the merchant. Walled gardensThese three protocols ideally work across different chat platforms, but that is never guaranteed, especially when your biggest chat platform competitor has its own protocol. A danger with competing protocols is that they can create wall gardens, where they only work on specific platforms. Enterprises face the problem of getting stuck in a platform and an agentic payment standard that will not interoperate with another. Organizations receive not only the product recommended by the agent, but are also most often the merchants of record and need to trust that the agent contacting them is acting on behalf of a customer.Louis Amira, cofounder and CEO of agent commerce startup Circuit and Chisel, told VentureBeat that while this creates an opportunity for companies in the interoperability layer like his, it could create confusion for enterprises. “The better the protocol proposals get, the more likely they are to end up being walled gardens and very hard to interoperate,” Amira said. “We suspect that they’re going to be fighting it out for the next few years, and the more they fight it out, the more you actually need somebody that sits underneath all of them.”Unlike the internet, where anyone can use any browser to access a website, thanks in large part to the TCP/IP standard, chat platforms tend to remain very separate. I mostly use ChatGPT (because it’s installed on my laptop and I don’t need to open a new tab), so when I want to see how Gemini will handle my query, I actually have to open Gemini to do so—the same works for anyone shopping via chatbot. The number of protocol proposals underscores just how far we are from enabling shopping agents. The industry still needs to decide which standard to get behind, and no matter how many Walmarts integrate with ChatGPT, it’s all moot if people don’t trust the model or agent to handle their cash. Take the best features, hopefully The best thing for enterprises to do for now is to experiment with all the protocols and hope that a winner emerges. Eventually, there could be one agentic commerce protocol that takes the best of each proposal. For Wayne Liu, chief growth officer and president for Americas at Perfect Corp., having multiple protocol proposals just means there’s more learning.“This is where the importance of open source exists because it will be the driving force to put everything together,” Liu said. Of course, what would be interesting to see these next couple of weeks is if there will only be three competing agentic commerce protocols. After all, there are some large retailers and chat platforms that can still throw a wrench into the whole thing.",
          "content": "When Walmart and OpenAI announced that the retailer would integrate with ChatGPT, the question became how quickly OpenAI could deliver on the promise of agents buying things for people. In the battle of AI-enabled commerce, getting agents to securely complete transactions is one of the biggest hurdles. More and more, chat platforms like ChatGPT are replacing browsers and getting very good at surfacing information people search for. Users will ask ChatGPT for the best humidifiers on the market, and when the model returns results, people have no choice but to click the item link and complete the purchase online. AI agents, as of now, don’t have the ability or the trust infrastructure to make people and banking institutions feel safe enough to let it loose on someone’s cash. Enterprises and other industry players understand that, to allow agents to pay for purchases, there must be a common language shared among the model and agent providers, the bank, the merchant, and, to a lesser extent, the buyer. And so, over the past few weeks, three competing agentic commerce standards have emerged: Google announced the Agent Pay Protocol (AP2) with partners including PayPal, American Express, Mastercard, Salesforce and ServiceNow. Soon after, OpenAI and Stripe debuted the Agentic Commerce Protocol (ACP), and just this week, Visa launched the Trusted Agent Protocol (TAP).All these protocols aim to give agents the trust layer they need to convince banks and their customers that they’re money is safe in the hands of an AI agent. But these may also create walled gardens, showing just how immature agentic commerce really is. This is a problem that could cause enterprises to bet on one chat platform and the agentic pay protocol it runs on, instead of interoperability. How are they differentIt’s not new for players to propose several standards. It usually takes years for the industry to coalesce around a single standard, or even to use different protocols and figure out a way to harmonize them. However, the pace of innovation in enterprise moved the needle on that. Fairly quickly, MCP became the de facto channel for tool-use identification, and most companies began setting up MCP servers or connecting to one. (To be clear, it is not a standard yet) But having three different potential standards might slow that process down a bit, because it’s harder to coalesce on a single standard when there are so many to choose from. These protocols all aim to prove authorization. Both AP2 and TAP rely on cryptographic proofs to show an agent is acting on an individual&#x27;s behalf. For TAP, agents are added to an approved list and get a digital key identifying them. AP2 uses a digital contract that serves as a proxy for human approval for the agent. OpenAI’s ACP doesn’t require too much of an infrastructure change, where ACP essentially acts as a courier to the merchant because the agent relays information to the merchant. Walled gardensThese three protocols ideally work across different chat platforms, but that is never guaranteed, especially when your biggest chat platform competitor has its own protocol. A danger with competing protocols is that they can create wall gardens, where they only work on specific platforms. Enterprises face the problem of getting stuck in a platform and an agentic payment standard that will not interoperate with another. Organizations receive not only the product recommended by the agent, but are also most often the merchants of record and need to trust that the agent contacting them is acting on behalf of a customer.Louis Amira, cofounder and CEO of agent commerce startup Circuit and Chisel, told VentureBeat that while this creates an opportunity for companies in the interoperability layer like his, it could create confusion for enterprises. “The better the protocol proposals get, the more likely they are to end up being walled gardens and very hard to interoperate,” Amira said. “We suspect that they’re going to be fighting it out for the next few years, and the more they fight it out, the more you actually need somebody that sits underneath all of them.”Unlike the internet, where anyone can use any browser to access a website, thanks in large part to the TCP/IP standard, chat platforms tend to remain very separate. I mostly use ChatGPT (because it’s installed on my laptop and I don’t need to open a new tab), so when I want to see how Gemini will handle my query, I actually have to open Gemini to do so—the same works for anyone shopping via chatbot. The number of protocol proposals underscores just how far we are from enabling shopping agents. The industry still needs to decide which standard to get behind, and no matter how many Walmarts integrate with ChatGPT, it’s all moot if people don’t trust the model or agent to handle their cash. Take the best features, hopefully The best thing for enterprises to do for now is to experiment with all the protocols and hope that a winner emerges. Eventually, there could be one agentic commerce protocol that takes the best of each proposal. For Wayne Liu, chief growth officer and president for Americas at Perfect Corp., having multiple protocol proposals just means there’s more learning.“This is where the importance of open source exists because it will be the driving force to put everything together,” Liu said. Of course, what would be interesting to see these next couple of weeks is if there will only be three competing agentic commerce protocols. After all, there are some large retailers and chat platforms that can still throw a wrench into the whole thing.",
          "feed_position": 9,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/1V6XILFC2iVvqfh58LXNkm/d7375489aa11c0096efb78642ca1c7a5/crimedy7_illustration_of_robot_paying_for_groceries_--ar_169__3d2a2c22-022a-4104-9070-fab8f94d73d5_1.png"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/under-the-hood-of-ai-agents-a-technical-guide-to-the-next-frontier-of-gen-ai",
          "published_at": "Thu, 16 Oct 2025 02:25:00 GMT",
          "title": "Under the hood of AI agents: A technical guide to the next frontier of gen AI",
          "standfirst": "Agents are the trendiest topic in AI today, and with good reason. AI agents act on their users’ behalf, autonomously handling tasks like making online purchases, building software, researching business trends or booking travel. By taking generative AI out of the sandbox of the chat interface and allowing it to act directly on the world, agentic AI represents a leap forward in the power and utility of AI.Taking gen AI out of the protected sandbox of the chat interface and allowing it to act directly on the world represents a leap forward in the power and utility of AI.Agentic AI has been moving really fast: For example, one of the core building blocks of today’s agents, the model context protocol (MCP), is only a year old! As in any fast-moving field, there are many competing definitions, hot takes and misleading opinions.To cut through the noise, I’d like to describe the core components of an agentic AI system and how they fit together: It’s really not as complicated as it may seem. Hopefully, when you’ve finished reading this post, agents won’t seem as mysterious.Agentic ecosystemDefinitions of the word “agent” abound, but I like a slight variation on the British programmer Simon Willison’s minimalist take:An LLM agent runs tools in a loop to achieve a goal.The user prompts a large language model (LLM) with a goal: Say, booking a table at a restaurant near a specific theater. Along with the goal, the model receives a list of the tools at its disposal, such as a database of restaurant locations or a record of the user’s food preferences. The model then plans how to achieve the goal and calls one of the tools, which provides a response; the model then calls a new tool. Through repetitions, the agent moves toward accomplishing the goal. In some cases, the model’s orchestration and planning choices are complemented or enhanced by imperative code.But what kind of infrastructure does it take to realize this approach? An agentic system needs a few core components:A way to build the agent. When you deploy an agent, you don’t want to have to code it from scratch. There are several agent development frameworks out there.Somewhere to run the AI model. A seasoned AI developer can download an open-weight LLM, but it takes expertise to do that right. It also takes expensive hardware that’s going to be poorly utilized for the average user.Somewhere to run the agentic code. With established frameworks, the user creates code for an agent object with a defined set of functions. Most of those functions involve sending prompts to an AI model, but the code needs to run somewhere. In practice, most agents will run in the cloud, because we want them to keep running when our laptops are closed, and we want them to scale up and out to do their work.A mechanism for translating between the text-based LLM and tool calls.A short-term memory for tracking the content of agentic interactions.A long-term memory for tracking the user’s preferences and affinities across sessions.A way to trace the system’s execution, to evaluate the agent’s performance.Let&#x27;s dive into more detail on each of these components.Building an agentAsking an LLM to explain how it plans to approach a particular task improves its performance on that task. This “chain-of-thought reasoning” is now ubiquitous in AI.The analogue in agentic systems is the ReAct (reasoning + action) model, in which the agent has a thought (“I’ll use the map function to locate nearby restaurants”), performs an action (issuing an API call to the map function), then makes an observation (“There are two pizza places and one Indian restaurant within two blocks of the movie theater”).ReAct isn’t the only way to build agents, but it is at the core of most successful agentic systems. Today, agents are commonly loops over the thought-action-observation sequence.The tools available to the agent can include local tools and remote tools such as databases, microservices and software as a service. A tool’s specification includes a natural-language explanation of how and when it’s used and the syntax of its API calls.The developer can also tell the agent to, essentially, build its own tools on the fly. Say that a tool retrieves a table stored as comma-separated text, and to fulfill its goal, the agent needs to sort the table.Sorting a table by repeatedly sending it through an LLM and evaluating the results would be a colossal waste of resources — and it’s not even guaranteed to give the right result. Instead, the developer can simply instruct the agent to generate its own Python code when it encounters a simple but repetitive task. These snippets of code can run locally alongside the agent or in a dedicated secure code interpreter tool. Available tools can divide responsibility between the LLM and the developer. Once the tools available to the agent have been specified, the developer can simply instruct the agent what tools to use when necessary. Or, the developer can specify which tool to use for which types of data, and even which data items to use as arguments during function calls.Similarly, the developer can simply tell the agent to generate Python code when necessary to automate repetitive tasks or, alternatively, tell it which algorithms to use for which data types and even provide pseudocode. The approach can vary from agent to agent.RuntimeHistorically, there were two main ways to isolate code running on shared servers: Containerization, which was efficient but offered lower security; and virtual machines, which were secure but came with a lot of computational overhead.In 2018, Amazon Web Services’ (AWS’s) Lambda serverless-computing service deployed Firecracker, a new paradigm in server isolation. Firecracker creates “microVMs”, complete with hardware isolation and their own Linux kernels but with reduced overhead (as low as a few megabytes) and startup times (as low as a few milliseconds). The low overhead means that each function executed on a Lambda server can have its own microVM.However, because instantiating an agent requires deploying an LLM, together with the memory resources to track the LLM’s inputs and outputs, the per-function isolation model is impractical. Instead, with session-based isolation, every session is assigned its own microVM. When the session finishes, the LLM’s state information is copied to long-term memory, and the microVM is destroyed. This ensures secure and efficient deployment of hosts of agents.Tool callsJust as there are several existing development frameworks for agent creation, there are several existing standards for communication between agents and tools, the most popular of which — currently — is the model context protocol (MCP). MCP establishes a one-to-one connection between the agent’s LLM and a dedicated MCP server that executes tool calls, and it also establishes a standard format for passing different types of data back and forth between the LLM and its server.Many platforms use MCP by default, but are also configurable, so they will support a growing set of protocols over time.Sometimes, however, the necessary tool is not one with an available API. In such cases, the only way to retrieve data or perform an action is through cursor movements and clicks on a website. There are a number of services available to perform such computer use. This makes any website a potential tool for agents, opening up decades of content and valuable services that aren’t yet available directly through APIs.AuthorizationsWith agents, authorization works in two directions. First, of course, users require authorization to run the agents they’ve created. But as the agent is acting on the user’s behalf, it will usually require its own authorization to access networked resources.There are a few different ways to approach the problem of authorization. One is with an access delegation algorithm like OAuth, which essentially plumbs the authorization process through the agentic system. The user enters login credentials into OAuth, and the agentic system uses OAuth to log into protected resources, but the agentic system never has direct access to the user’s passwords.In the other approach, the user logs into a secure session on a server, and the server has its own login credentials on protected resources. Permissions allow the user to select from a variety of authorization strategies and algorithms for implementing those strategies.Memory and tracesShort-term memoryLLMs are next-word prediction engines. What makes them so astoundingly versatile is that their predictions are based on long sequences of words they’ve already seen, known as context. Context is, in itself, a kind of memory. But it’s not the only kind an agentic system needs.Suppose, again, that an agent is trying to book a restaurant near a movie theater, and from a map tool, it’s retrieved a couple dozen restaurants within a mile radius. It doesn’t want to dump information about all those restaurants into the LLM’s context: All that extraneous information could wreak havoc with next-word probabilities.Instead, it can store the complete list in short-term memory and retrieve one or two records at a time, based on, say, the user’s price and cuisine preferences and proximity to the theater. If none of those restaurants pans out, the agent can dip back into short-term memory, rather than having to execute another tool call.Long-term memoryAgents also need to remember their prior interactions with their clients. If last week I told the restaurant booking agent what type of food I like, I don’t want to have to tell it again this week. The same goes for my price tolerance, the sort of ambiance I’m looking for, and so on.Long-term memory allows the agent to look up what it needs to know about prior conversations with the user. Agents don’t typically create long-term memories themselves, however. Instead, after a session is complete, the whole conversation passes to a separate AI model, which creates new long-term memories or updates existing ones.Memory creation can involve LLM summarization and “chunking”, in which documents are split into sections grouped according to topic for ease of retrieval during subsequent sessions. Available systems allow the user to select strategies and algorithms for summarization, chunking and other information-extraction techniques.ObservabilityAgents are a new kind of software system, and they require new ways to think about observing, monitoring and auditing their behavior. Some of the questions we ask will look familiar: Whether the agents are running fast enough, how much they’re costing, how many tool calls they’re making and whether users are happy. But new questions will arise, too, and we can’t necessarily predict what data we’ll need to answer them.Observability and tracing tools can provide an end-to-end view of the execution of a session with an agent, breaking down step-by-step which actions were taken and why. For the agent builder, these traces are key to understanding how well agents are working — and provide the data to make them work better.I hope this explanation has demystified agentic AI enough that you’re willing to try building your own agents!",
          "content": "Agents are the trendiest topic in AI today, and with good reason. AI agents act on their users’ behalf, autonomously handling tasks like making online purchases, building software, researching business trends or booking travel. By taking generative AI out of the sandbox of the chat interface and allowing it to act directly on the world, agentic AI represents a leap forward in the power and utility of AI.Taking gen AI out of the protected sandbox of the chat interface and allowing it to act directly on the world represents a leap forward in the power and utility of AI.Agentic AI has been moving really fast: For example, one of the core building blocks of today’s agents, the model context protocol (MCP), is only a year old! As in any fast-moving field, there are many competing definitions, hot takes and misleading opinions.To cut through the noise, I’d like to describe the core components of an agentic AI system and how they fit together: It’s really not as complicated as it may seem. Hopefully, when you’ve finished reading this post, agents won’t seem as mysterious.Agentic ecosystemDefinitions of the word “agent” abound, but I like a slight variation on the British programmer Simon Willison’s minimalist take:An LLM agent runs tools in a loop to achieve a goal.The user prompts a large language model (LLM) with a goal: Say, booking a table at a restaurant near a specific theater. Along with the goal, the model receives a list of the tools at its disposal, such as a database of restaurant locations or a record of the user’s food preferences. The model then plans how to achieve the goal and calls one of the tools, which provides a response; the model then calls a new tool. Through repetitions, the agent moves toward accomplishing the goal. In some cases, the model’s orchestration and planning choices are complemented or enhanced by imperative code.But what kind of infrastructure does it take to realize this approach? An agentic system needs a few core components:A way to build the agent. When you deploy an agent, you don’t want to have to code it from scratch. There are several agent development frameworks out there.Somewhere to run the AI model. A seasoned AI developer can download an open-weight LLM, but it takes expertise to do that right. It also takes expensive hardware that’s going to be poorly utilized for the average user.Somewhere to run the agentic code. With established frameworks, the user creates code for an agent object with a defined set of functions. Most of those functions involve sending prompts to an AI model, but the code needs to run somewhere. In practice, most agents will run in the cloud, because we want them to keep running when our laptops are closed, and we want them to scale up and out to do their work.A mechanism for translating between the text-based LLM and tool calls.A short-term memory for tracking the content of agentic interactions.A long-term memory for tracking the user’s preferences and affinities across sessions.A way to trace the system’s execution, to evaluate the agent’s performance.Let&#x27;s dive into more detail on each of these components.Building an agentAsking an LLM to explain how it plans to approach a particular task improves its performance on that task. This “chain-of-thought reasoning” is now ubiquitous in AI.The analogue in agentic systems is the ReAct (reasoning + action) model, in which the agent has a thought (“I’ll use the map function to locate nearby restaurants”), performs an action (issuing an API call to the map function), then makes an observation (“There are two pizza places and one Indian restaurant within two blocks of the movie theater”).ReAct isn’t the only way to build agents, but it is at the core of most successful agentic systems. Today, agents are commonly loops over the thought-action-observation sequence.The tools available to the agent can include local tools and remote tools such as databases, microservices and software as a service. A tool’s specification includes a natural-language explanation of how and when it’s used and the syntax of its API calls.The developer can also tell the agent to, essentially, build its own tools on the fly. Say that a tool retrieves a table stored as comma-separated text, and to fulfill its goal, the agent needs to sort the table.Sorting a table by repeatedly sending it through an LLM and evaluating the results would be a colossal waste of resources — and it’s not even guaranteed to give the right result. Instead, the developer can simply instruct the agent to generate its own Python code when it encounters a simple but repetitive task. These snippets of code can run locally alongside the agent or in a dedicated secure code interpreter tool. Available tools can divide responsibility between the LLM and the developer. Once the tools available to the agent have been specified, the developer can simply instruct the agent what tools to use when necessary. Or, the developer can specify which tool to use for which types of data, and even which data items to use as arguments during function calls.Similarly, the developer can simply tell the agent to generate Python code when necessary to automate repetitive tasks or, alternatively, tell it which algorithms to use for which data types and even provide pseudocode. The approach can vary from agent to agent.RuntimeHistorically, there were two main ways to isolate code running on shared servers: Containerization, which was efficient but offered lower security; and virtual machines, which were secure but came with a lot of computational overhead.In 2018, Amazon Web Services’ (AWS’s) Lambda serverless-computing service deployed Firecracker, a new paradigm in server isolation. Firecracker creates “microVMs”, complete with hardware isolation and their own Linux kernels but with reduced overhead (as low as a few megabytes) and startup times (as low as a few milliseconds). The low overhead means that each function executed on a Lambda server can have its own microVM.However, because instantiating an agent requires deploying an LLM, together with the memory resources to track the LLM’s inputs and outputs, the per-function isolation model is impractical. Instead, with session-based isolation, every session is assigned its own microVM. When the session finishes, the LLM’s state information is copied to long-term memory, and the microVM is destroyed. This ensures secure and efficient deployment of hosts of agents.Tool callsJust as there are several existing development frameworks for agent creation, there are several existing standards for communication between agents and tools, the most popular of which — currently — is the model context protocol (MCP). MCP establishes a one-to-one connection between the agent’s LLM and a dedicated MCP server that executes tool calls, and it also establishes a standard format for passing different types of data back and forth between the LLM and its server.Many platforms use MCP by default, but are also configurable, so they will support a growing set of protocols over time.Sometimes, however, the necessary tool is not one with an available API. In such cases, the only way to retrieve data or perform an action is through cursor movements and clicks on a website. There are a number of services available to perform such computer use. This makes any website a potential tool for agents, opening up decades of content and valuable services that aren’t yet available directly through APIs.AuthorizationsWith agents, authorization works in two directions. First, of course, users require authorization to run the agents they’ve created. But as the agent is acting on the user’s behalf, it will usually require its own authorization to access networked resources.There are a few different ways to approach the problem of authorization. One is with an access delegation algorithm like OAuth, which essentially plumbs the authorization process through the agentic system. The user enters login credentials into OAuth, and the agentic system uses OAuth to log into protected resources, but the agentic system never has direct access to the user’s passwords.In the other approach, the user logs into a secure session on a server, and the server has its own login credentials on protected resources. Permissions allow the user to select from a variety of authorization strategies and algorithms for implementing those strategies.Memory and tracesShort-term memoryLLMs are next-word prediction engines. What makes them so astoundingly versatile is that their predictions are based on long sequences of words they’ve already seen, known as context. Context is, in itself, a kind of memory. But it’s not the only kind an agentic system needs.Suppose, again, that an agent is trying to book a restaurant near a movie theater, and from a map tool, it’s retrieved a couple dozen restaurants within a mile radius. It doesn’t want to dump information about all those restaurants into the LLM’s context: All that extraneous information could wreak havoc with next-word probabilities.Instead, it can store the complete list in short-term memory and retrieve one or two records at a time, based on, say, the user’s price and cuisine preferences and proximity to the theater. If none of those restaurants pans out, the agent can dip back into short-term memory, rather than having to execute another tool call.Long-term memoryAgents also need to remember their prior interactions with their clients. If last week I told the restaurant booking agent what type of food I like, I don’t want to have to tell it again this week. The same goes for my price tolerance, the sort of ambiance I’m looking for, and so on.Long-term memory allows the agent to look up what it needs to know about prior conversations with the user. Agents don’t typically create long-term memories themselves, however. Instead, after a session is complete, the whole conversation passes to a separate AI model, which creates new long-term memories or updates existing ones.Memory creation can involve LLM summarization and “chunking”, in which documents are split into sections grouped according to topic for ease of retrieval during subsequent sessions. Available systems allow the user to select strategies and algorithms for summarization, chunking and other information-extraction techniques.ObservabilityAgents are a new kind of software system, and they require new ways to think about observing, monitoring and auditing their behavior. Some of the questions we ask will look familiar: Whether the agents are running fast enough, how much they’re costing, how many tool calls they’re making and whether users are happy. But new questions will arise, too, and we can’t necessarily predict what data we’ll need to answer them.Observability and tracing tools can provide an end-to-end view of the execution of a session with an agent, breaking down step-by-step which actions were taken and why. For the agent builder, these traces are key to understanding how well agents are working — and provide the data to make them work better.I hope this explanation has demystified agentic AI enough that you’re willing to try building your own agents!",
          "feed_position": 10,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/5uEgR8k5xDLaMIS0le17hv/595a27e9dc71acf83cc362c433942ce6/AI_agents.png"
        }
      ],
      "featured_image": "https://images.ctfassets.net/jdtwqhzvc2n1/2ehvkDsQ7NC7wQuECOcDGS/cb89da9ccaf13f2888f005ac93cecffa/cfr0z3n_realistic_graphic_novel_hyper_detailed_flat_illustratio_8aae43d5-1e50-4ebd-8760-ea28dbc0f328.png",
      "popularity_score": 2019.2389652777779,
      "ai_summary": [
        "Google is adding live Google Maps data to Gemini AI models.",
        "This allows developers to connect Gemini with geospatial data.",
        "Applications can deliver location-relevant responses to queries.",
        "Developers can build intelligent, location-aware experiences.",
        "The feature is useful for local search, delivery, and travel."
      ]
    },
    {
      "id": "cluster_30",
      "coverage": 2,
      "updated_at": "Fri, 17 Oct 2025 14:45:01 -0400",
      "title": "Automaker Stellantis and Pony.ai sign a non-binding agreement to develop robotaxis for deployment in Europe, with plans to start testing in the coming months (Rebecca Bellan/TechCrunch)",
      "neutral_headline": "Automaker Stellantis and Pony.ai sign a non-binding agreement to develop robotaxis for deployment in Europe, with plans to start testing in the coming months (Rebecca Bellan/TechCrunch)",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/251017/p27#a251017p27",
          "published_at": "Fri, 17 Oct 2025 14:45:01 -0400",
          "title": "Automaker Stellantis and Pony.ai sign a non-binding agreement to develop robotaxis for deployment in Europe, with plans to start testing in the coming months (Rebecca Bellan/TechCrunch)",
          "standfirst": "Rebecca Bellan / TechCrunch: Automaker Stellantis and Pony.ai sign a non-binding agreement to develop robotaxis for deployment in Europe, with plans to start testing in the coming months &mdash; Automaker Stellantis and Chinese autonomous vehicle company Pony.ai have signed a nonbinding agreement to build robotaxis for deployment in Europe.",
          "content": "Rebecca Bellan / TechCrunch: Automaker Stellantis and Pony.ai sign a non-binding agreement to develop robotaxis for deployment in Europe, with plans to start testing in the coming months &mdash; Automaker Stellantis and Chinese autonomous vehicle company Pony.ai have signed a nonbinding agreement to build robotaxis for deployment in Europe.",
          "feed_position": 9,
          "image_url": "http://www.techmeme.com/251017/i27.jpg"
        },
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2025/10/17/stellantis-teams-up-with-pony-ai-to-develop-robotaxis-in-europe/",
          "published_at": "Fri, 17 Oct 2025 15:12:55 +0000",
          "title": "Stellantis teams up with Pony AI to develop robotaxis in Europe",
          "standfirst": "The collaboration will integrate Pony’s self-driving software into Stellantis’ electric medium-size van platform, which is built with advanced sensors to support autonomous vehicles.",
          "content": "The collaboration will integrate Pony’s self-driving software into Stellantis’ electric medium-size van platform, which is built with advanced sensors to support autonomous vehicles.",
          "feed_position": 9
        }
      ],
      "featured_image": "http://www.techmeme.com/251017/i27.jpg",
      "popularity_score": 2015.472576388889,
      "ai_summary": [
        "Stellantis and Pony.ai signed a non-binding agreement.",
        "They will develop robotaxis for deployment in Europe.",
        "The companies plan to start testing in the coming months.",
        "Pony.ai is a Chinese autonomous vehicle company.",
        "The collaboration will integrate Pony's self-driving software."
      ]
    },
    {
      "id": "cluster_53",
      "coverage": 2,
      "updated_at": "Fri, 17 Oct 2025 12:05:01 -0400",
      "title": "WhatsApp is testing limits on how many messages a user or business can send monthly to unknown people without a response; nearing the limit displays a warning (Ivan Mehta/TechCrunch)",
      "neutral_headline": "WhatsApp Tests Limits on Messages to Curb Spam",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/251017/p23#a251017p23",
          "published_at": "Fri, 17 Oct 2025 12:05:01 -0400",
          "title": "WhatsApp is testing limits on how many messages a user or business can send monthly to unknown people without a response; nearing the limit displays a warning (Ivan Mehta/TechCrunch)",
          "standfirst": "Ivan Mehta / TechCrunch: WhatsApp is testing limits on how many messages a user or business can send monthly to unknown people without a response; nearing the limit displays a warning &mdash; WhatsApp is attempting to solve its spam problem by putting a curb on how many messages individual users and businesses can send to unknown people without getting a response.",
          "content": "Ivan Mehta / TechCrunch: WhatsApp is testing limits on how many messages a user or business can send monthly to unknown people without a response; nearing the limit displays a warning &mdash; WhatsApp is attempting to solve its spam problem by putting a curb on how many messages individual users and businesses can send to unknown people without getting a response.",
          "feed_position": 13,
          "image_url": "http://www.techmeme.com/251017/i23.jpg"
        },
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2025/10/17/whatsapp-will-curb-the-number-of-messages-people-and-businesses-can-send-without-a-response/",
          "published_at": "Fri, 17 Oct 2025 14:11:33 +0000",
          "title": "WhatsApp will curb the number of messages people and businesses can send without a response",
          "standfirst": "The company said the controls are designed to be effective against people and businesses that blast messages and spam people.",
          "content": "The company said the controls are designed to be effective against people and businesses that blast messages and spam people.",
          "feed_position": 12
        }
      ],
      "featured_image": "http://www.techmeme.com/251017/i23.jpg",
      "popularity_score": 2012.8059097222222,
      "ai_summary": [
        "WhatsApp is testing limits on monthly messages sent to unknown people without a response.",
        "Users nearing the limit will see a warning message displayed within the application.",
        "The company aims to reduce spam by controlling messages from individuals and businesses.",
        "These controls are designed to be effective against mass messaging and spamming practices.",
        "The changes are intended to improve the user experience by reducing unwanted communications."
      ]
    },
    {
      "id": "cluster_5",
      "coverage": 1,
      "updated_at": "Fri, 17 Oct 2025 22:14:59 +0000",
      "title": "With deadline looming, 4 of 9 universities reject Trump’s “compact” to remake higher ed",
      "neutral_headline": "Four Universities Reject Trump's Higher Education Compact",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/culture/2025/10/with-deadline-looming-4-of-9-universities-reject-trumps-compact-to-remake-higher-ed/",
          "published_at": "Fri, 17 Oct 2025 22:14:59 +0000",
          "title": "With deadline looming, 4 of 9 universities reject Trump’s “compact” to remake higher ed",
          "standfirst": "But Trump is pressuring the other five.",
          "content": "Earlier this month, the Trump administration made nine elite universities an offer they couldn’t refuse: bring in more conservatives while shutting down “institutional units that purposefully punish, belittle, and even spark violence against conservative ideas,” give up control of admissions and hiring decisions, agree to “biological” definitions of sex and gender, don’t raise tuition for five years, clamp down on student protests, and stay institutionally “neutral” on current events. Do this and you won’t be cut off from “federal benefits,” which could include research funding, student loans, federal contracts, and even student and faculty immigration visas. Instead, you may gain “substantial and meaningful federal grants.” But the universities are refusing. With the initial deadline of October 20 approaching, four of the nine universities—the University of Pennsylvania, Brown, University of Southern California, and MIT—that received the federal “compact” have announced that they will not sign it. In addition, the American Council on Education, which represents more than 1,600 colleges and universities, today issued a statement calling for the compact to be completely withdrawn.Read full article Comments",
          "feed_position": 0,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/05/GettyImages-588709290-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/05/GettyImages-588709290-1152x648.jpg",
      "popularity_score": 351.9720208333333,
      "ai_summary": [
        "Four of nine universities have rejected Trump's proposed compact for higher education.",
        "The deadline for accepting the compact is approaching for the remaining universities.",
        "Trump is currently pressuring the other five universities to accept the compact.",
        "The compact aims to reshape the landscape of higher education institutions.",
        "The specific details of the compact remain subject to ongoing negotiations."
      ]
    },
    {
      "id": "cluster_6",
      "coverage": 1,
      "updated_at": "Fri, 17 Oct 2025 22:02:54 +0000",
      "title": "Vaginal condition treatment update: Men should get treated, too",
      "neutral_headline": "Treating Vaginal Conditions Requires Partner Involvement",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/health/2025/10/vaginal-condition-treatment-update-men-should-get-treated-too/",
          "published_at": "Fri, 17 Oct 2025 22:02:54 +0000",
          "title": "Vaginal condition treatment update: Men should get treated, too",
          "standfirst": "For bacterial vaginosis, partners are part of the problem—and the solution.",
          "content": "For some cases of bacterial vaginosis, treatment should include a package deal, doctors now say. The American College of Obstetricians & Gynecologists (ACOG) updated its clinical guidance Friday to fit with recent data indicating that treatment for recurring bacterial vaginosis (BV) in women is significantly more effective if their male partners are also treated at the same time—with both an oral antibiotic and an antibiotic cream directly onto the potentially offending member. “Partner therapy offers us another avenue for hopefully preventing recurrence and helping people feel better faster,” Christopher Zahn, chief of clinical practice and health equity and quality at ACOG, said in a statement.Read full article Comments",
          "feed_position": 1,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2185753669-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2185753669-1152x648.jpg",
      "popularity_score": 341.77063194444446,
      "ai_summary": [
        "Bacterial vaginosis treatment should include treatment for male partners.",
        "Partners are part of the problem and also part of the solution for this condition.",
        "This approach aims to prevent reinfection and improve treatment outcomes.",
        "The recommendation emphasizes a comprehensive approach to healthcare.",
        "The focus is on addressing the root causes of the condition."
      ]
    },
    {
      "id": "cluster_7",
      "coverage": 1,
      "updated_at": "Fri, 17 Oct 2025 21:50:10 +0000",
      "title": "Ring cameras are about to get increasingly chummy with law enforcement",
      "neutral_headline": "Ring Cameras Partner with Law Enforcement Technology Company",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2025/10/ring-cameras-are-about-to-get-increasingly-chummy-with-law-enforcement/",
          "published_at": "Fri, 17 Oct 2025 21:50:10 +0000",
          "title": "Ring cameras are about to get increasingly chummy with law enforcement",
          "standfirst": "Amazon's Ring partners with company whose tech has reportedly been used by ICE.",
          "content": "Law enforcement agencies will soon have easier access to footage captured by Amazon’s Ring smart cameras. In a partnership announced this week, Amazon will allow approximately 5,000 local law enforcement agencies to request access to Ring camera footage via surveillance platforms from Flock Safety. Ring cooperating with law enforcement and the reported use of Flock technologies by federal agencies, including US Immigration and Customs Enforcement (ICE), has resurfaced privacy concerns that have followed the devices for years. According to Flock’s announcement, its Ring partnership allows local law enforcement members to use Flock software “to send a direct post in the Ring Neighbors app with details about the investigation and request voluntary assistance.” Requests must include “specific location and timeframe of the incident, a unique investigation code, and details about what is being investigated,” and users can look at the requests anonymously, Flock said. “Any footage a Ring customer chooses to submit will be securely packaged by Flock and shared directly with the requesting local public safety agency through the FlockOS or Flock Nova platform,” the announcement reads.Read full article Comments",
          "feed_position": 2,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/20250611-lifestyle-outdoorcampro-plugin-home-exterior-wall-wht-rgb-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/20250611-lifestyle-outdoorcampro-plugin-home-exterior-wall-wht-rgb-1152x648.jpg",
      "popularity_score": 331.5584097222222,
      "ai_summary": [
        "Amazon's Ring is partnering with a company whose tech has been used by ICE.",
        "The partnership could increase the integration of Ring with law enforcement agencies.",
        "This collaboration raises concerns about data privacy and surveillance practices.",
        "The technology company's involvement with ICE has drawn criticism.",
        "The partnership may expand the use of Ring camera footage by authorities."
      ]
    },
    {
      "id": "cluster_32",
      "coverage": 1,
      "updated_at": "Fri, 17 Oct 2025 18:36:18 +0000",
      "title": "Dead Ends is a fun, macabre medical history for kids",
      "neutral_headline": "Dead Ends is a Macabre Medical History Book for Children",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2025/10/dead-ends-is-a-fun-macabre-medical-history-for-kids/",
          "published_at": "Fri, 17 Oct 2025 18:36:18 +0000",
          "title": "Dead Ends is a fun, macabre medical history for kids",
          "standfirst": "Ars chats with co-authors Lindsey Fitzharris and Adrian Teal about their delightful new children's book.",
          "content": "In 1890, a German scientist named Robert Koch thought he’d invented a cure for tuberculosis, a substance derived from the infecting bacterium itself that he dubbed Tuberculin. His substance didn’t actually cure anyone, but it was eventually widely used as a diagnostic skin test. Koch’s successful failure is just one of the many colorful cases featured in Dead Ends! Flukes, Flops, and Failures that Sparked Medical Marvels, a new nonfiction illustrated children’s book by science historian Lindsey Fitzharris and her husband, cartoonist Adrian Teal. A noted science communicator with a fondness for the medically macabre, Fitzharris published a biography of surgical pioneer Joseph Lister, The Butchering Art, in 2017—a great, if occasionally grisly, read. She followed up with 2022’s The Facemaker: A Visionary Surgeon’s Battle to Mend the Disfigured Soldiers of World War I, about a WWI surgeon named Harold Gillies who rebuilt the faces of injured soldiers. And in 2020, she hosted a documentary for the Smithsonian Channel, The Curious Life and Death Of…, exploring famous deaths, ranging from drug lord Pablo Escobar to magician Harry Houdini. Fitzharris performed virtual autopsies, experimented with blood samples, interviewed witnesses, and conducted real-time demonstrations in hopes of gleaning fresh insights. For his part, Teal is a well-known caricaturist and illustrator, best known for his work on the British TV series Spitting Image. His work has also appeared in The Guardian and the Sunday Telegraph, among other outlets.Read full article Comments",
          "feed_position": 3,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/09/deadends4-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/09/deadends4-1152x648.jpg",
      "popularity_score": 318.32729861111113,
      "ai_summary": [
        "\"Dead Ends\" is a new children's book exploring medical history.",
        "The book is co-authored by Lindsey Fitzharris and Adrian Teal.",
        "The book aims to make medical history accessible and engaging for children.",
        "The authors discuss the book's content and creative process.",
        "The book's tone is described as fun and macabre."
      ]
    },
    {
      "id": "cluster_38",
      "coverage": 1,
      "updated_at": "Fri, 17 Oct 2025 17:55:19 +0000",
      "title": "Big Tech sues Texas, says age-verification law is “broad censorship regime”",
      "neutral_headline": "Big Tech Sues Texas Over Age-Verification Law",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/10/big-tech-sues-texas-says-age-verification-law-is-broad-censorship-regime/",
          "published_at": "Fri, 17 Oct 2025 17:55:19 +0000",
          "title": "Big Tech sues Texas, says age-verification law is “broad censorship regime”",
          "standfirst": "Texas app law compared to checking IDs at bookstores and shopping malls.",
          "content": "Texas is being sued by a Big Tech lobby group over the state’s new law that will require app stores to verify users’ ages and impose restrictions on users under 18. “The Texas App Store Accountability Act imposes a broad censorship regime on the entire universe of mobile apps,” the Computer & Communications Industry Association (CCIA) said yesterday in a lawsuit. “In a misguided attempt to protect minors, Texas has decided to require proof of age before anyone with a smartphone or tablet can download an app. Anyone under 18 must obtain parental consent for every app and in-app purchase they try to download—from ebooks to email to entertainment.” The CCIA said in a press release that the law violates the First Amendment by imposing “a sweeping age-verification, parental consent, and compelled speech regime on both app stores and app developers.” When app stores determine that a user is under 18, “the law prohibits them from downloading virtually all apps and software programs and from making any in-app purchases unless their parent consents and is given control over the minor’s account,” the CCIA said. “Minors who are unable to link their accounts with a parent’s or guardian’s, or who do not receive permission, would be prohibited from accessing app store content.”Read full article Comments",
          "feed_position": 4,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/boy-with-phone-1152x648-1760722441.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/boy-with-phone-1152x648-1760722441.jpg",
      "popularity_score": 307.64424305555553,
      "ai_summary": [
        "Big Tech companies are suing Texas over its age-verification law.",
        "The companies claim the law constitutes a \"broad censorship regime.\"",
        "The law is compared to requiring ID checks at bookstores and malls.",
        "The lawsuit challenges the constitutionality of the Texas law.",
        "The legal battle could have significant implications for online content regulation."
      ]
    },
    {
      "id": "cluster_42",
      "coverage": 1,
      "updated_at": "Fri, 17 Oct 2025 17:14:03 +0000",
      "title": "Teen sues to destroy the nudify app that left her in constant fear",
      "neutral_headline": "Teen Sues to Destroy Nudify App After Constant Fear",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/10/teen-haunted-by-fake-nudes-sues-to-kill-nudify-app-block-telegram-bots/",
          "published_at": "Fri, 17 Oct 2025 17:14:03 +0000",
          "title": "Teen sues to destroy the nudify app that left her in constant fear",
          "standfirst": "Lawsuit accuses nudify apps of training on teen victims' images.",
          "content": "One of the earliest teen victims bullied by fake nudes has sued to destroy the app she said left her living in “constant fear.” In her complaint, the teen—who was granted anonymity as a 17-year-old minor—accused ClothOff of intentionally making it easy to generate and distribute child sexual abuse materials (CSAM), as well as nonconsensual intimate images (NCII) of adults. She also alleged that the social media network Telegram helps promote ClothOff through automated bots that have attracted hundreds of thousands of subscribers. ClothOff’s operation, the teen alleged, goes beyond promoting a single app, which can be used for free to turn an ordinary Instagram photo into CSAM or NCII in “three clicks.”Read full article Comments",
          "feed_position": 5,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-1204474749-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-1204474749-1152x648.jpg",
      "popularity_score": 296.9564652777778,
      "ai_summary": [
        "A teenager is suing to destroy the \"nudify\" app.",
        "The lawsuit alleges the app caused the teen constant fear.",
        "The lawsuit accuses the app of training on teen victims' images.",
        "The case highlights the dangers of AI-powered image manipulation.",
        "The legal action seeks to hold the app developers accountable."
      ]
    },
    {
      "id": "cluster_43",
      "coverage": 1,
      "updated_at": "Fri, 17 Oct 2025 16:54:32 +0000",
      "title": "NASA’s next Moonship reaches last stop before launch pad",
      "neutral_headline": "NASA's Moonship Reaches Final Stop Before Launch",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2025/10/nasas-next-moonship-reaches-last-stop-before-launch-pad/",
          "published_at": "Fri, 17 Oct 2025 16:54:32 +0000",
          "title": "NASA’s next Moonship reaches last stop before launch pad",
          "standfirst": "Preparations for the Artemis II mission continue despite the federal government shutdown.",
          "content": "The Orion spacecraft, which will fly four people around the Moon, arrived inside the cavernous Vehicle Assembly Building at NASA’s Kennedy Space Center in Florida late Thursday night, ready to be stacked on top of its rocket for launch early next year. The late-night transfer covered about 6 miles (10 kilometers) from one facility to another at the Florida spaceport. NASA and its contractors are continuing preparations for the Artemis II mission after the White House approved the program as an exception to work through the ongoing government shutdown, which began on October 1. The sustained work could set up Artemis II for a launch opportunity as soon as February 5 of next year. Astronauts Reid Wiseman, Victor Glover, Christina Koch, and Jeremy Hansen will be the first humans to fly on the Orion spacecraft, a vehicle that has been in development for nearly two decades. The Artemis II crew will make history on their 10-day flight by becoming the first people to travel to the vicinity of the Moon since 1972.Read full article Comments",
          "feed_position": 6,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/orion_transfer_artii-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/orion_transfer_artii-1152x648.jpg",
      "popularity_score": 286.6311875,
      "ai_summary": [
        "NASA's next Moonship has reached its final stop before the launch pad.",
        "Preparations for the Artemis II mission are ongoing.",
        "The federal government shutdown has not halted mission preparations.",
        "The Artemis II mission aims to send astronauts around the Moon.",
        "The mission is a crucial step in NASA's lunar exploration program."
      ]
    },
    {
      "id": "cluster_49",
      "coverage": 1,
      "updated_at": "Fri, 17 Oct 2025 16:27:02 +0000",
      "title": "12 years of HDD analysis brings insight to the bathtub curve’s reliability",
      "neutral_headline": "HDD Analysis Reveals Insights into Reliability",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2025/10/backblaze-owner-of-317230-hdds-says-hdds-are-lasting-longer/",
          "published_at": "Fri, 17 Oct 2025 16:27:02 +0000",
          "title": "12 years of HDD analysis brings insight to the bathtub curve’s reliability",
          "standfirst": "Backup firm brings a unique, informed perspective to HDD failure rates.",
          "content": "Backblaze is a backup and cloud storage company that has been tracking the annualized failure rates (AFRs) of the hard drives in its datacenter since 2013. As you can imagine, that’s netted the firm a lot of data. And that data has led the company to conclude that HDDs “are lasting longer” and showing fewer errors. That conclusion came from a blog post this week by Stephanie Doyle, Backblaze’s writer and blog operations specialist, and Pat Patterson, Backblaze’s chief technical evangelist. The authors compared the AFRs for the approximately 317,230 drives in Backblaze’s datacenter to the AFRs the company recorded when examining the 21,195 drives it had in 2013 and 206,928 drives in 2021. Doyle and Patterson said they identified “a pretty solid deviation in both age of drive failure and the high point of AFR from the last two times we’ve run the analyses.” Credit: Backblaze Read full article Comments",
          "feed_position": 7,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2020/08/GettyImages-462699436-1024x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2020/08/GettyImages-462699436-1024x648.jpg",
      "popularity_score": 276.17285416666664,
      "ai_summary": [
        "A backup firm has conducted 12 years of HDD analysis.",
        "The analysis provides insight into the bathtub curve's reliability.",
        "The firm offers a unique perspective on HDD failure rates.",
        "The data helps understand the lifespan and failure patterns of HDDs.",
        "The findings are valuable for data storage and management."
      ]
    },
    {
      "id": "cluster_58",
      "coverage": 1,
      "updated_at": "Fri, 17 Oct 2025 15:40:15 +0000",
      "title": "Lead poisoning has been a feature of our evolution",
      "neutral_headline": "Lead Poisoning Found in Ancient Hominin Fossils",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2025/10/hominins-suffered-lead-poisoning-starting-at-least-2-million-years-ago/",
          "published_at": "Fri, 17 Oct 2025 15:40:15 +0000",
          "title": "Lead poisoning has been a feature of our evolution",
          "standfirst": "A recent study found lead in teeth from 2 million-year-old hominin fossils.",
          "content": "Our hominid ancestors faced a Pleistocene world full of dangers—and apparently one of those dangers was lead poisoning. Lead exposure sounds like a modern problem, at least if you define “modern” the way a paleoanthropologist might: a time that started a few thousand years ago with ancient Roman silver smelting and lead pipes. According to a recent study, however, lead is a much more ancient nemesis, one that predates not just the Romans but the existence of our genus Homo. Paleoanthropologist Renaud Joannes-Boyau of Australia’s Southern Cross University and his colleagues found evidence of exposure to dangerous amounts of lead in the teeth of fossil apes and hominins dating back almost 2 million years. And somewhat controversially, they suggest that the toxic element’s pervasiveness may have helped shape our evolutionary history. The skull of an early hominid. Credit: Einsamer Schütze / Wikimedia The Romans didn’t invent lead poisoning Joannes-Boyau and his colleagues took tiny samples of preserved enamel and dentin from the teeth of 51 fossils. In most of those teeth, the paleoanthropologists found evidence that these apes and hominins had been exposed to lead—sometimes in dangerous quantities—fairly often during their early years.Read full article Comments",
          "feed_position": 8,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-1285261327-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-1285261327-1152x648.jpg",
      "popularity_score": 268.39313194444446,
      "ai_summary": [
        "A recent study found lead in teeth from 2-million-year-old hominin fossils.",
        "Lead poisoning has been a feature of human evolution.",
        "The study provides evidence of lead exposure in ancient hominins.",
        "The findings shed light on the history of environmental toxins.",
        "The research contributes to understanding human health over time."
      ]
    },
    {
      "id": "cluster_66",
      "coverage": 1,
      "updated_at": "Fri, 17 Oct 2025 15:07:01 +0000",
      "title": "Apple pays $750 million for US Formula 1 streaming coverage",
      "neutral_headline": "Apple Pays $750 Million for Formula 1 Streaming Coverage",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/cars/2025/10/apple-pays-750-million-for-us-formula-1-streaming-coverage/",
          "published_at": "Fri, 17 Oct 2025 15:07:01 +0000",
          "title": "Apple pays $750 million for US Formula 1 streaming coverage",
          "standfirst": "Some races will be free as F1 TV moves from standalone streaming to Apple TV.",
          "content": "The United States Grand Prix takes place this weekend at the Circuit of the Americas in Texas, and this morning, Formula 1 used the occasion to announce a new broadcast deal for the sport in the US. Starting next year, F1 will no longer be broadcast on ESPN—it’s moving to Apple TV in a five-year, $750 million deal. Apple boss Tim Cook has been seen at F1 races in the past, and earlier this year, Apple released F1: The Movie, starring Brad Pitt as a 50-something racing driver who improbably gets a second bite at the cherry 30 years after a brutal crash seemingly ended his F1 career. But securing the rights to the sport itself means Apple has snagged a very fast-growing series, with races almost every other week—currently, the sport has expanded to 24 races a year.Read full article Comments",
          "feed_position": 9,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/Apple-exclusive-F1-partner-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/Apple-exclusive-F1-partner-1152x648.jpg",
      "popularity_score": 266.8392430555556,
      "ai_summary": [
        "Apple is paying $750 million for US Formula 1 streaming coverage.",
        "Some races will be available for free on Apple TV.",
        "F1 TV is transitioning from a standalone streaming service.",
        "The move integrates F1 content into Apple's streaming platform.",
        "The deal expands Apple's sports content offerings."
      ]
    },
    {
      "id": "cluster_119",
      "coverage": 1,
      "updated_at": "Thu, 16 Oct 2025 20:40:49 +0000",
      "title": "Nation-state hackers deliver malware from “bulletproof” blockchains",
      "neutral_headline": "Nation-State Hackers Deliver Malware from Blockchains",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/security/2025/10/hackers-bullet-proof-hosts-deliver-malware-from-blockchains/",
          "published_at": "Thu, 16 Oct 2025 20:40:49 +0000",
          "title": "Nation-state hackers deliver malware from “bulletproof” blockchains",
          "standfirst": "Malicious payloads stored on Ethereum and BNB blockchains are immune to takedowns.",
          "content": "Hacking groups—at least one of which works on behalf of the North Korean government—have found a new and inexpensive way to distribute malware from “bulletproof” hosts: stashing them on public cryptocurrency blockchains. In a Thursday post, members of the Google Threat Intelligence Group said the technique provides the hackers with their own “bulletproof” host, a term that describes cloud platforms that are largely immune from takedowns by law enforcement and pressure from security researchers. More traditionally, these hosts are located in countries without treaties agreeing to enforce criminal laws from the US and other nations. These services often charge hefty sums and cater to criminals spreading malware or peddling child sexual abuse material and wares sold in crime-based flea markets. Next-gen, DIY hosting that can’t be tampered with Since February, Google researchers have observed two groups turning to a newer technique to infect targets with credential stealers and other forms of malware. The method, known as EtherHiding, embeds the malware in smart contracts, which are essentially apps that reside on blockchains for Ethereum and other cryptocurrencies. Two or more parties then enter into an agreement spelled out in the contract. When certain conditions are met, the apps enforce the contract terms in a way that, at least theoretically, is immutable and independent of any central authority.Read full article Comments",
          "feed_position": 16,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/05/malware-threat-1000x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/05/malware-threat-1000x648.jpg",
      "popularity_score": 166,
      "ai_summary": [
        "Nation-state hackers are using \"bulletproof\" blockchains for malware.",
        "Malicious payloads are stored on Ethereum and BNB blockchains.",
        "These blockchains are immune to takedowns.",
        "The technique allows hackers to maintain access to their malware.",
        "The method poses a significant challenge to cybersecurity efforts."
      ]
    },
    {
      "id": "cluster_82",
      "coverage": 1,
      "updated_at": "Fri, 17 Oct 2025 14:16:58 +0000",
      "title": "Teachers get an F on AI-generated lesson plans",
      "neutral_headline": "AI-Generated Lesson Plans Fail to Inspire Students",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/ai/2025/10/teachers-get-an-f-on-ai-generated-lesson-plans/",
          "published_at": "Fri, 17 Oct 2025 14:16:58 +0000",
          "title": "Teachers get an F on AI-generated lesson plans",
          "standfirst": "AI-generated lesson plans fall short on inspiring students and promoting critical thinking.",
          "content": "When teachers rely on commonly used artificial intelligence chatbots to devise lesson plans, it does not result in more engaging, immersive, or effective learning experiences compared with existing techniques, we found in our recent study. The AI-generated civics lesson plans we analyzed also left out opportunities for students to explore the stories and experiences of traditionally marginalized people. The allure of generative AI as a teaching aid has caught the attention of educators. A Gallup survey from September 2025 found that 60 percent of K-12 teachers are already using AI in their work, with the most common reported use being teaching preparation and lesson planning. Without the assistance of AI, teachers might spend hours every week crafting lessons for their students. With AI, time-stretched teachers can generate detailed lesson plans featuring learning objectives, materials, activities, assessments, extension activities, and homework tasks in a matter of seconds.Read full article Comments",
          "feed_position": 11,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/09/GettyImages-1287663546-1152x648-1758570568.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/09/GettyImages-1287663546-1152x648-1758570568.jpg",
      "popularity_score": 156.00507638888888,
      "ai_summary": [
        "AI-generated lesson plans are falling short in education.",
        "The plans struggle to inspire students and promote critical thinking.",
        "The quality of AI-generated educational content is a concern.",
        "The findings highlight limitations of current AI in education.",
        "The study suggests a need for human oversight in lesson planning."
      ]
    },
    {
      "id": "cluster_115",
      "coverage": 1,
      "updated_at": "Thu, 16 Oct 2025 22:41:42 +0000",
      "title": "RFK Jr.’s MAHA wants to make chemtrail conspiracy theories great again",
      "neutral_headline": "RFK Jr.'s MAHA Supports Chemtrail Conspiracy Theories",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/health/2025/10/rfk-jr-s-maha-wants-to-make-chemtrail-conspiracy-theories-great-again/",
          "published_at": "Thu, 16 Oct 2025 22:41:42 +0000",
          "title": "RFK Jr.’s MAHA wants to make chemtrail conspiracy theories great again",
          "standfirst": "It's unclear if Kennedy will follow through, but he supports the conspiracy theory.",
          "content": "A prominent voice in the Make America Healthy Again movement is pushing for health secretary and anti-vaccine activist Robert F. Kennedy Jr. to make the topic of chemtrail conspiracy theories a federal priority, according to a report by KFF News. KFF obtained a memo, written by MAHA influencer Gray Delany in July, presenting the topic to Calley Means, a White House health advisor. The memo lays out a series of unsubstantiated and far-fetched claims that academic researchers and federal agencies are secretively spreading toxic substances from airplanes, poisoning Americans, and spurring large-scale weather events, such as the devastating flooding in Texas last summer. “It is unconscionable that anyone should be allowed to spray known neurotoxins and environmental toxins over our nation’s citizens, their land, food and water supplies,” Delany writes in the memo.Read full article Comments",
          "feed_position": 14,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-1182820491-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-1182820491-1152x648.jpg",
      "popularity_score": 148,
      "ai_summary": [
        "RFK Jr.'s MAHA wants to promote chemtrail conspiracy theories.",
        "It is unclear if Kennedy will follow through with this support.",
        "Kennedy supports the conspiracy theory.",
        "The conspiracy theory claims that aircraft release harmful chemicals.",
        "The theory lacks scientific evidence."
      ]
    },
    {
      "id": "cluster_117",
      "coverage": 1,
      "updated_at": "Thu, 16 Oct 2025 21:12:48 +0000",
      "title": "AI-powered features begin creeping deeper into the bedrock of Windows 11",
      "neutral_headline": "AI-Powered Features Creep into Windows 11",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2025/10/microsofts-vision-for-ai-pcs-looks-a-lot-like-another-crack-at-cortana/",
          "published_at": "Thu, 16 Oct 2025 21:12:48 +0000",
          "title": "AI-powered features begin creeping deeper into the bedrock of Windows 11",
          "standfirst": "Copilot expands with an emphasis on creating and editing files, voice input.",
          "content": "Like virtually every major Windows announcement in the last three years, the spate of features that Microsoft announced for the operating system today all revolve around generative AI. In particular, they’re concerned with the company’s more recent preoccupation with “agentic” AI, an industry buzzword for “telling AI-powered software to perform a task, which it then does in the background while you move on to other things.” But the overarching impression I got, both from reading the announcement and sitting through a press briefing earlier this month, is that Microsoft is using language models and other generative AI technologies to try again with Cortana, Microsoft’s failed and discontinued entry in the voice assistant wars of the 2010s. According to Microsoft’s Consumer Chief Marketing Officer Yusuf Mehdi, “AI PCs” should be able to recognize input “naturally, in text or voice,” to be able to guide users based on what’s on their screens at any given moment, and that AI assistants “should be able to take action on your behalf.”Read full article Comments",
          "feed_position": 15,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/Windows-Blog-Hero-Image_High-Res-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/Windows-Blog-Hero-Image_High-Res-1152x648.jpg",
      "popularity_score": 148,
      "ai_summary": [
        "AI-powered features are expanding within Windows 11.",
        "Copilot is emphasizing file creation, editing, and voice input.",
        "The updates aim to enhance user productivity and experience.",
        "The integration of AI is becoming more deeply embedded.",
        "The changes reflect a broader trend in software development."
      ]
    },
    {
      "id": "cluster_122",
      "coverage": 1,
      "updated_at": "Thu, 16 Oct 2025 20:25:08 +0000",
      "title": "Ars Live recap: Is the AI bubble about to pop? Ed Zitron weighs in.",
      "neutral_headline": "Ars Live Discusses AI Bubble, Finances, and Nuclear Power",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/ai/2025/10/ars-live-recap-is-the-ai-bubble-about-to-pop-ed-zitron-weighs-in/",
          "published_at": "Thu, 16 Oct 2025 20:25:08 +0000",
          "title": "Ars Live recap: Is the AI bubble about to pop? Ed Zitron weighs in.",
          "standfirst": "Despite connection hiccups, we covered OpenAI's finances, nuclear power, and Sam Altman.",
          "content": "On Tuesday of last week, Ars Technica hosted a live conversation with Ed Zitron, host of the Better Offline podcast and one of tech’s most vocal AI critics, to discuss whether the generative AI industry is experiencing a bubble and when it might burst. My Internet connection had other plans, though, dropping out multiple times and forcing Ars Technica’s Lee Hutchinson to jump in as an excellent emergency backup host. During the times my connection cooperated, Zitron and I covered OpenAI’s financial issues, lofty infrastructure promises, and why the AI hype machine keeps rolling despite some arguably shaky economics underneath. Lee’s probing questions about per-user costs revealed a potential flaw in AI subscription models: Companies can’t predict whether a user will cost them $2 or $10,000 per month. You can watch a recording of the event on YouTube or in the window below.Read full article Comments",
          "feed_position": 17,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/ai_bubble_hero2-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/ai_bubble_hero2-1152x648.jpg",
      "popularity_score": 148,
      "ai_summary": [
        "Ars Technica discussed OpenAI's finances during the live recap.",
        "The live stream also covered topics related to nuclear power.",
        "Technical difficulties occurred during the live stream.",
        "Ed Zitron provided commentary on the current state of AI.",
        "Sam Altman was also a topic of discussion during the live event."
      ]
    },
    {
      "id": "cluster_105",
      "coverage": 1,
      "updated_at": "Fri, 17 Oct 2025 10:30:59 +0000",
      "title": "Yes, everything online sucks now—but it doesn’t have to",
      "neutral_headline": "Ars Technica Discusses Online Issues with Cory Doctorow",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2025/10/yes-everything-online-sucks-now-but-it-doesnt-have-to/",
          "published_at": "Fri, 17 Oct 2025 10:30:59 +0000",
          "title": "Yes, everything online sucks now—but it doesn’t have to",
          "standfirst": "Ars chats with Cory Doctorow about his new book Enshittification.",
          "content": "We all feel it: Our once-happy digital spaces have become increasingly less user-friendly and more toxic, cluttered with extras nobody asked for and hardly anybody wants. There’s even a word for it: “enshittification,” named 2023 Word of the Year by the American Dialect Society. The term was coined by tech journalist/science fiction author Cory Doctorow, a longtime advocate of digital rights. Doctorow has spun his analysis of what’s been ailing the tech industry into an eminently readable new book, Enshittification: Why Everything Suddenly Got Worse and What To Do About It. As Doctorow tells it, he was on vacation in Puerto Rico, staying in a remote cabin nestled in a cloud forest with microwave Internet service—i.e., very bad Internet service, since microwave signals struggle to penetrate through clouds. It was a 90-minute drive to town, but when they tried to consult TripAdvisor for good local places to have dinner one night, they couldn’t get the site to load. “All you would get is the little TripAdvisor logo as an SVG filling your whole tab and nothing else,” Doctorow told Ars. “So I tweeted, ‘Has anyone at TripAdvisor ever been on a trip? This is the most enshittified website I’ve ever used.'” Initially, he just got a few “haha, that’s a funny word” responses. “It was when I married that to this technical critique, at a moment when things were quite visibly bad to a much larger group of people, that made it take off,” Doctorow said. “I didn’t deliberately set out to do it. I bought a million lottery tickets and one of them won the lottery. It only took two decades.”Read full article Comments",
          "feed_position": 13,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/cory1-1152x648-1759765718.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/cory1-1152x648-1759765718.jpg",
      "popularity_score": 146.2386875,
      "ai_summary": [
        "Ars Technica interviewed Cory Doctorow regarding online problems.",
        "The interview focused on Doctorow's new book, \"Enshittification\".",
        "The discussion centered on the current state of the internet.",
        "Doctorow offered insights into the issues of online platforms.",
        "The interview explored potential solutions to these problems."
      ]
    },
    {
      "id": "cluster_72",
      "coverage": 1,
      "updated_at": "Fri, 17 Oct 2025 14:57:07 +0000",
      "title": "3 years, 4 championships, but 0 Le Mans wins: Assessing the Porsche 963",
      "neutral_headline": "Porsche 963: IMSA Success, WEC Withdrawal Analyzed",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/cars/2025/10/3-years-4-championships-but-0-le-mans-wins-assessing-the-porsche-963/",
          "published_at": "Fri, 17 Oct 2025 14:57:07 +0000",
          "title": "3 years, 4 championships, but 0 Le Mans wins: Assessing the Porsche 963",
          "standfirst": "Riding high in IMSA but pulling out of WEC paints a complicated picture for the factory team.",
          "content": "Porsche provided flights from Washington to Atlanta and accommodation so Ars could attend Petit Le Mans. Ars does not accept paid editorial content. The car world has long had a thing about numbers. Engine outputs. Top speeds. Zero-to-60 times. Displacement. But the numbers go beyond bench racing specs. Some cars have numbers for names, and few more memorably than Porsche. Its most famous model shares its appellation with the emergency services here in North America; although the car should accurately be “nine-11,” you call it “nine-one-one.” Some numbers are less well-known, but perhaps more special to Porsche’s fans, especially those who like racing. 908. 917. 956. 962. 919. But how about 963? That’s Porsche’s current sports prototype, a 670-hp (500 kW) hybrid that for the last three years has battled against rivals in what is starting to look like, if not a golden era for endurance racing, then at least a very purple patch. And the 963 has done well, racing here in IMSA’s WeatherTech Sportscar Championship and around the globe in the FIA World Endurance Championship.Read full article Comments",
          "feed_position": 10,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/IMSA2025PetitLeMans_JT19117-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/IMSA2025PetitLeMans_JT19117-1152x648.jpg",
      "popularity_score": 144.67424305555556,
      "ai_summary": [
        "Porsche's 963 has achieved four championships in three years.",
        "The team is successful in the IMSA racing series.",
        "Porsche is withdrawing from the World Endurance Championship (WEC).",
        "This decision creates a complex situation for the factory team.",
        "The article assesses the implications of these contrasting situations."
      ]
    },
    {
      "id": "cluster_103",
      "coverage": 1,
      "updated_at": "Fri, 17 Oct 2025 11:00:49 +0000",
      "title": "Rocket Report: China launches with no advance warning; Europe’s drone ship",
      "neutral_headline": "Rocket Report: China Launches, US Mega-Constellations Grow",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2025/10/rocket-report-a-nearly-perfect-flight-for-starship-chinas-surprise-launch/",
          "published_at": "Fri, 17 Oct 2025 11:00:49 +0000",
          "title": "Rocket Report: China launches with no advance warning; Europe’s drone ship",
          "standfirst": "Starlink, Kuiper, and the US military all saw additions to their mega-constellations this week.",
          "content": "Welcome to Edition 8.15 of the Rocket Report! This year has been, at best, one of mixed results for SpaceX’s Starship program. There have been important steps forward, including the successful reuse of the rocket’s massive Super Heavy booster. Clearly, SpaceX is getting really good at launching and recovering the 33-engine booster stage. But Starship itself, part spacecraft and part upper stage, hasn’t fared as well—at least it hadn’t until the last couple of months. After four Starships were destroyed in flight and on the ground in the first half of 2025, the last two missions ended with pinpoint splashdowns in the Indian Ocean. The most recent mission this week was arguably the most successful yet for Starship, which returned to Earth with little damage, suggesting SpaceX’s improvements to the heat shield are working. As always, we welcome reader submissions. If you don’t want to miss an issue, please subscribe using the box below (the form will not appear on AMP-enabled versions of the site). Each report will include information on small-, medium-, and heavy-lift rockets, as well as a quick look ahead at the next three launches on the calendar. SpaceX vet will fly with Blue Origin. Hans Koenigsmann is one of SpaceX’s earliest, longest-tenured, and most-revered employees. He worked at Elon Musk’s space company for nearly two decades, rising to the role of vice president for mission assurance and safety before leaving SpaceX in 2021. He led the investigations into every Falcon rocket failure, mentored young engineers, and became a public face for SpaceX through numerous presentations and press conferences. And now he has announced he is going to space on a future suborbital flight on Blue Origin’s New Shepard vehicle, Ars reports.Read full article Comments",
          "feed_position": 12,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/starshipflight11-presplash-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/starshipflight11-presplash-1152x648.jpg",
      "popularity_score": 140.73590972222223,
      "ai_summary": [
        "China launched a rocket without prior warning.",
        "Starlink and Kuiper added satellites to their constellations.",
        "The US military also expanded its satellite presence.",
        "Europe's drone ship was also a topic of discussion.",
        "The report covers various space-related developments."
      ]
    },
    {
      "id": "cluster_123",
      "coverage": 1,
      "updated_at": "Thu, 16 Oct 2025 20:05:19 +0000",
      "title": "OnePlus unveils OxygenOS 16 update with deep Gemini integration",
      "neutral_headline": "OnePlus Announces OxygenOS 16 Update with Gemini Integration",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/google/2025/10/oneplus-unveils-oxygenos-16-update-with-deep-gemini-integration/",
          "published_at": "Thu, 16 Oct 2025 20:05:19 +0000",
          "title": "OnePlus unveils OxygenOS 16 update with deep Gemini integration",
          "standfirst": "Does your phone even have a Mind Space?",
          "content": "OnePlus is expected to take the wraps off the OnePlus 15 in the next few weeks, but before that, it’s giving us a look at the software that will run on it. OxygenOS 16, which is based on Android 16, will also come to the company’s other supported phones, and it’s going to include a heaping helping of AI features. OnePlus was slower than most smartphone makers to embrace AI, but it’s full-steam ahead now with new Gemini integrations. OxygenOS 16 is described by OnePlus in grandiose terms as “a defiant rebellion for authenticity.” In the real world, this update is doing a lot of the same things as other AI-heavy smartphones. It’s not all AI—OnePlus notes that OxygenOS 16 will include revamped animations that have been carefully designed for smoothness, as well as the O+ remote app that gives you remote access to Windows and Mac PCs. The lock screen is also more customizable, borrowing a page from the likes of Apple and Samsung. OnePlus began embracing AI in June, when it launched a feature called Mind Space on the OnePlus 13S. That phone was only for the Indian market, but the rest of the world will get this and more with OxygenOS 16. At launch, Mind Space would collect your screenshots and brief voice messages. Mind Space would analyze the screenshots to create calendar entries and not much else.Read full article Comments",
          "feed_position": 18,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/OP-mind-space-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/OP-mind-space-1152x648.jpg",
      "popularity_score": 133,
      "ai_summary": [
        "OnePlus is releasing the OxygenOS 16 update.",
        "The update includes deep integration with Gemini.",
        "The article questions if users have a \"Mind Space\".",
        "The update will affect the user experience.",
        "The new features are designed to enhance the phone."
      ]
    },
    {
      "id": "cluster_124",
      "coverage": 1,
      "updated_at": "Thu, 16 Oct 2025 19:50:58 +0000",
      "title": "Sony tells SCOTUS that people accused of piracy aren’t “innocent grandmothers”",
      "neutral_headline": "Sony Argues Against \"Innocent Grandmother\" Piracy Defense",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/10/sony-tells-scotus-that-people-accused-of-piracy-arent-innocent-grandmothers/",
          "published_at": "Thu, 16 Oct 2025 19:50:58 +0000",
          "title": "Sony tells SCOTUS that people accused of piracy aren’t “innocent grandmothers”",
          "standfirst": "Music companies want ISPs to terminate repeat infringers or pay big damages.",
          "content": "Record labels Sony, Warner, and Universal yesterday asked the Supreme Court to help it boot pirates off the Internet. Sony and the other labels filed their brief in Cox Communications v. Sony Music Entertainment, a case involving the cable Internet service provider that rebuffed labels’ demands for mass terminations of broadband subscribers accused of repeat copyright infringement. The Supreme Court’s eventual decision in the case may determine whether Internet service providers must terminate the accounts of alleged pirates in order to avoid massive financial liability. Cox has argued that copyright-infringement notices—which are generated by bots and flag users based on their IP addresses—sent by record labels are unreliable. Cox said ISPs can’t verify whether the notices are accurate and that terminating an account would punish every user in a household where only one person may have illegally downloaded copyrighted files.Read full article Comments",
          "feed_position": 19,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2024/02/music-pirate-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2024/02/music-pirate-1152x648.jpg",
      "popularity_score": 133,
      "ai_summary": [
        "Sony presented arguments to the Supreme Court.",
        "The company addressed the issue of music piracy.",
        "Sony opposes the \"innocent grandmother\" defense.",
        "Music companies seek ISP action against repeat infringers.",
        "They want ISPs to pay damages for copyright violations."
      ]
    }
  ]
}