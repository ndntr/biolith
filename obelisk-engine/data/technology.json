{
  "updated_at": "2025-12-15T19:21:34.421Z",
  "clusters": [
    {
      "id": "cluster_0",
      "coverage": 3,
      "updated_at": "Mon, 15 Dec 2025 19:19:18 +0000",
      "title": "Google’s ‘dark web report’ feature will no longer be available starting in February",
      "neutral_headline": "Google’s ‘dark web report’ feature will no longer be available starting in February",
      "items": [
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2025/12/15/googles-dark-web-report-feature-will-no-longer-be-available-starting-in-february/",
          "published_at": "Mon, 15 Dec 2025 19:19:18 +0000",
          "title": "Google’s ‘dark web report’ feature will no longer be available starting in February",
          "standfirst": "Google has revealed that its “dark web report” feature will be discontinued starting February 16, 2026. Launched initially about a year and a half ago, this tool aimed to help users monitor their personal information on the dark web For some users, the dark web report seemed like a valuable tool, scanning various data breach [&#8230;]",
          "content": "Google has revealed that its “dark web report” feature will be discontinued starting February 16, 2026. Launched initially about a year and a half ago, this tool aimed to help users monitor their personal information on the dark web For some users, the dark web report seemed like a valuable tool, scanning various data breach [&#8230;]",
          "feed_position": 0
        },
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/251215/p27#a251215p27",
          "published_at": "Mon, 15 Dec 2025 12:45:00 -0500",
          "title": "Google is shutting down its dark web monitoring tool, launched in 2024 to alert users when their personal info is detected on the dark web, on February 16, 2026 (Ben Schoon/9to5Google)",
          "standfirst": "Ben Schoon / 9to5Google: Google is shutting down its dark web monitoring tool, launched in 2024 to alert users when their personal info is detected on the dark web, on February 16, 2026 &mdash; One of the lesser-known features that Google offers is a &ldquo;Dark Web Report&rdquo; monitoring tool, but it's about to shut down in early 2026.",
          "content": "Ben Schoon / 9to5Google: Google is shutting down its dark web monitoring tool, launched in 2024 to alert users when their personal info is detected on the dark web, on February 16, 2026 &mdash; One of the lesser-known features that Google offers is a &ldquo;Dark Web Report&rdquo; monitoring tool, but it's about to shut down in early 2026.",
          "feed_position": 4,
          "image_url": "http://www.techmeme.com/251215/i27.jpg"
        },
        {
          "source": "The Verge",
          "url": "https://www.theverge.com/news/844700/google-dark-web-reports-shut-down",
          "published_at": "2025-12-15T12:30:23-05:00",
          "title": "Google&#8217;s turning off its dark web monitoring service that scoured data breaches for your info",
          "standfirst": "Google notified users in an email today that, beginning next month, it will stop sending its dark web reports, an opt-in feature that alerted users when Google detected their personal information on the dark web, as reported by 9to5Google. On January 15th, Google will stop scanning for that data, like contact info and home addresses, [&#8230;]",
          "content": "Google notified users in an email today that, beginning next month, it will stop sending its dark web reports, an opt-in feature that alerted users when Google detected their personal information on the dark web, as reported by 9to5Google. On January 15th, Google will stop scanning for that data, like contact info and home addresses, and any data it collected so far will no longer be available to users starting on February 16th. The email sent to users who were signed up for the feature explains, \"While the report offered general information, feedback showed that it did not provide helpful next steps. We're making this change to instead fo … Read the full story at The Verge.",
          "feed_position": 5
        }
      ],
      "featured_image": "http://www.techmeme.com/251215/i27.jpg",
      "popularity_score": 3019.962105277778
    },
    {
      "id": "cluster_6",
      "coverage": 2,
      "updated_at": "Mon, 15 Dec 2025 13:45:01 -0500",
      "title": "GM is adding native Apple Music to select vehicles, and rolls out support for digital keys, after it started phasing out CarPlay in 2023 (Andrew J. Hawkins/The Verge)",
      "neutral_headline": "Cadillac and Chevy are getting native Apple Music",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/251215/p31#a251215p31",
          "published_at": "Mon, 15 Dec 2025 13:45:01 -0500",
          "title": "GM is adding native Apple Music to select vehicles, and rolls out support for digital keys, after it started phasing out CarPlay in 2023 (Andrew J. Hawkins/The Verge)",
          "standfirst": "Andrew J. Hawkins / The Verge: GM is adding native Apple Music to select vehicles, and rolls out support for digital keys, after it started phasing out CarPlay in 2023 &mdash; General Motors is racing to add more native apps to make up for its decision to block Apple CarPlay and Android Auto.",
          "content": "Andrew J. Hawkins / The Verge: GM is adding native Apple Music to select vehicles, and rolls out support for digital keys, after it started phasing out CarPlay in 2023 &mdash; General Motors is racing to add more native apps to make up for its decision to block Apple CarPlay and Android Auto.",
          "feed_position": 0,
          "image_url": "http://www.techmeme.com/251215/i31.jpg"
        },
        {
          "source": "The Verge",
          "url": "https://www.theverge.com/transportation/844697/gm-cadillac-chevy-apple-music-native",
          "published_at": "2025-12-15T13:00:00-05:00",
          "title": "Cadillac and Chevy are getting native Apple Music",
          "standfirst": "General Motors is adding native Apple Music to the infotainment systems of select 2025 model year Cadillac and Chevrolet vehicles, the company announced today. The news comes as the automaker is racing to add more native app experiences to some of its vehicles to make up for the absence of popular phone mirroring services like [&#8230;]",
          "content": "General Motors is adding native Apple Music to the infotainment systems of select 2025 model year Cadillac and Chevrolet vehicles, the company announced today. The news comes as the automaker is racing to add more native app experiences to some of its vehicles to make up for the absence of popular phone mirroring services like Apple CarPlay and Android Auto. Apple Music will arrive in 2025 or newer Chevy and Cadillac models through an over-the-air software update. The update is being made available through GM's OnStar Basics package, which comes standard on all 2025 and newer vehicles at no additional cost for eight years. Eligible models i … Read the full story at The Verge.",
          "feed_position": 4
        }
      ],
      "featured_image": "http://www.techmeme.com/251215/i31.jpg",
      "popularity_score": 2019.390716388889
    },
    {
      "id": "cluster_33",
      "coverage": 2,
      "updated_at": "Mon, 15 Dec 2025 16:30:00 +0000",
      "title": "In 2025, tech giants decided smart glasses are the next big thing",
      "neutral_headline": "In 2025, tech giants decided smart glasses are the next big thing",
      "items": [
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/wearables/in-2025-tech-giants-decided-smart-glasses-are-the-next-big-thing-163000812.html",
          "published_at": "Mon, 15 Dec 2025 16:30:00 +0000",
          "title": "In 2025, tech giants decided smart glasses are the next big thing",
          "standfirst": "There's a growing sentiment that gadgets have gotten boring. And while I don't fully agree, I understand why people might feel that way. Just think about some of the novel device types that companies have tried to push since the original iPhone came out. 3D TVs were a massive flop and tablets still feel like extra-large smartphones despite Apple's efforts to prop them up as laptop replacements. Meanwhile, even with huge technological advancements over the last decade, VR headsets remain relatively niche due to factors like high prices and a lack of compelling content. And although big names like Google, Microsoft, Meta and others continue to dump billions into AI development, the first wave of dedicated AI devices was an abject failure. When you think about it, the only new(ish) class of gadget that has made major inroads to the mainstream market is smartwatches. That said, because they've evolved into wearable health and fitness sensors instead of the wrist-based computers that many once thought they would be, they haven't really disrupted our lives like the personal computer and smartphone did. But that seems poised to change because the tech giants have decided that smart glasses are going to be the next big thing. Headsets versus smart glasses, what’s the difference?Google is planning to support both smart glasses and headsets with Android XR, though the increased size and weight of devices like the Galaxy XR means it's not a great choice for all-day functionality. Sam Rutherford for EngadgetAt this point, you might be saying, \"Wait, hold on. Aren't VR headsets and smart glasses kind of the same thing?\" Well, yes and no. Both types of gadgets require similar software and hardware, but they utilize them in very different ways. Not only are VR goggles typically much bigger and heavier, they also provide a more isolated experience that can make it feel like you've been transported to another world. Sure, most modern headsets have exterior cameras that support some level of mixed reality (blending virtual graphics with physical objects) or let you peek quickly into meatspace (passthrough view) for when you need to get a drink or acknowledge other humans in the room. But in many respects, that closed-off feeling is the goal because it creates the ideal environment for playing games, taking virtual meetings or modeling 3D objects without real-world distractions. Furthermore, while many headsets like the Vision Pro and the Meta Quest 3 can function as standalone systems and support accessories like controllers or other motion trackers, they can also be tethered to a nearby PC for enhanced functionality. On the other hand, the default use case for smart glasses is a mixed reality environment where the spectacles can overlay helpful info or messages while you stay active and aware of your surroundings. Notably, while smart glasses might come with lenses or clip-on attachments that allow them to get darker or serve as sunglasses when you're outside, there typically isn't a way to completely block out the world like you can with a headset, mostly because that's simply not the point. And even though most smart spectacles can be paired with a phone to get access to mobile data or notifications, they're generally not meant to be tethered to a PC full-time (though there are some exceptions). The goal for smart glasses is more to provide a mobile-first heads-up display that augments what you see with your eyes instead of replacing things entirely with a digital environment. OK, but what makes you so sure that smart glasses are \"it?\"Now that we've discussed what separates smart glasses from headsets, what makes it so obvious that they are going to be the next big thing? This one is a bit easier to answer because we can simply look at the sheer number of companies that have released smart glasses or are planning to do so in the future. If we skip past the Google Glass from 2013 as forward-thinking specs that were ahead of their time, the most well-known example of modern smart glasses is the Meta Ray-Ban (or the even earlier Ray-Ban Stories from back when Facebook was still Facebook). While they are a bit chunky, the Meta Ray-Ban Display are some of the most sophisticated smart glasses on the market right now due in large part to their single full-color screen. Karissa Bell for EngadgetEven though they don't have built-in displays, the ability to capture photos and videos and play audio via built-in speakers brought the idea of smart glasses into the mainstream without making the concept look or feel completely ridiculous. Those earlier models then paved the way for even more sophisticated iterations like the Meta Ray-Ban Display from earlier this fall, which features a stunning RGB HUD (though only in the right lens) that has gotten us tantalizingly close to a true wearable display that doesn't make you look like a cyborg. Of course, Meta isn't the only game in town: there's a rapidly growing number of competitors from companies like Even Realities, Rokkid, TCL, Xreal, Viture and more. But for an even clearer sign of where the tech giants are heading, we can just look at Meta's two biggest competitors: Apple and Google. While Apple hasn't publicly announced plans to make its own smart glasses, Bloomberg's Mark Gurman — who is one of the company's most reliable analysts — provided inside info earlier this fall that Tim Cook and Co. are planning to pivot away from a proper follow-up to the Vision Pro in favor of more lightweight spectacles with greater mass appeal. This shouldn't really come as a major surprise, as sales of Apple's $3,500 headset have been lackluster. But more importantly, for a company that's extremely cautious about entering new product categories (foldable iPhone anyone?), it feels very telling to hear that Apple is shifting to smart glasses instead of abandoning the idea of wearable displays entirely. This is a company that doesn't swing and miss very often, so the idea of two flops in a row seems preposterous. If this pivot is real, there must be some Apple execs who are big believers that glasses and not goggles are the right choice for future development. Here are two of Google's reference design smart glasses. The one in the front features dual RGB waveguide displays while the one in the back relies on a single monocular screen.Sam Rutherford for EngadgetMeanwhile, Google is taking a two-pronged approach. In addition to releasing a new mixed reality OS — Android XR — on Samsung's Galaxy XR headset in October, the company has also teased upcoming smart glasses along with a handful of partners including glasses makers Gentle Monster and Warby Parker. Just this week, the company also added a number of new features to Android XR designed to support a wide range of upcoming devices while simultaneously making it easier for developers to port existing apps over to smart glasses and headsets. And if you still need additional evidence regarding Google's desire to get into smart glasses, consider that even with its ongoing collaboration, the company also spent $100 million to acquire a 4 percent stake in Gentle Monster. Regardless of who is making them though, the big draw for these companies is the idea that smart glasses will become a new piece of core personal computing, similar to how people rely on smartphones and laptops today (or to a lesser extent wireless headphones and smartwatches). If true, that could become a trillion-dollar market in the next 10 to 15 years (or sooner, who knows), which not only makes it a natural avenue for expansion but possibly a future existential crisis for certain companies. After all, none of these organizations want to be the next Microsoft after it failed to develop a successful smartphone or mobile OS.Fine, the smart glasses trend is real, but why would we even want them?At this point, I hope it's clear that the push for smart glasses is very real and very serious. But so far, we've only addressed why companies are betting big on them. So what's in it for us, the people who might actually buy and use them? Well, to answer that, we need to separate the current models into three main categories.A great use case for smart glasses would be to provide heads-up mapping without the need to constantly look down at your phone as seen in this demo clip of Android XR. GoogleFirst, there are the most basic smart glasses that don't come with built-in displays and typically rely on cameras and built-in speakers for enhanced functionality. The best example of this class of devices is the Meta Ray-Ban smart glasses (or the original Ray-Ban Stories) along with rivals like the Bose Sound Frames, which, believe it or not, have been on the market since 2019. However, before anyone gets attached to these early models, the simplest smart glasses already kind of feel like dinosaurs and will probably, in the not-too-distant future, go extinct. They were an interesting attempt to add things like music playback or photo and video capture to regular-looking sunglasses, but their limited feature set puts a clear ceiling on what they can do. Plus, if this is what people really wanted, they would have taken off already. Waveguides like the ones built into the Even Realities G2 project images directly onto their lenses allowing for super sleek glasses featuring a heads-up display. Sam Rutherford for EngadgetThis brings us to more recent offerings like the Meta Ray-Ban Display, Even Realities G2, the Halliday glasses and others which add some type of built-in display to the mix. Most often, these models rely on waveguide displays as they enable thinner and lighter designs while propagating images onto the glasses' lenses. Currently, most of these smart glasses feature single-color optics (usually green) to reduce complexity and power draw, but there are others like the Meta Ray-Ban Display and both the TCL RayNeo X2 and X3 that support full color.In this day and age when everyone is surrounded by screens, the idea of yet another display mounted inches away from your eyeballs might sound like the last thing you want. However, because modern smart glasses are much more discreet and less awkward-looking, I find that they can actually help cut down on distractions. That's because instead of having to peek down at your phone or smartwatch to check notifications, reply to messages or look up directions, you can do many or all of these things using smart glasses — all in the middle of a conversation without anyone noticing. Not only does this keep your focus where it should be — on people instead of gadgets — the glasses are also just as easy to wear as a smartwatch and far more comfortable than bulky VR headsets. Then, when you consider some other features of modern smart glasses like on-the-fly translation, the ability to function as a teleprompter hidden in plain sight or additional support from AI, suddenly you have a wearable that allows you to keep all of your other devices neatly stashed away. In many respects, smart glasses could be the portable displays that people might not even know they want.Compared to rivals with waveguides, glasses featuring \"birdbath\" optics are often significantly thicker and bulkier. Sam Rutherford for EngadgetSpeaking of portable displays: If you recall, I mentioned above how most smart glasses generally don't need to be tethered to other devices. The exception to that comes from a subclass of specs that are primarily designed to function as wearable monitors capable of supporting one or more virtual screens that can be in excess of 100 inches in size, relatively speaking. The most well-known smart glasses in this category come from Xreal and Viture, with both companies offering a range of models with varying levels of performance. One interesting thing to note is instead of waveguides, some of these smart glasses rely on birdbath optics. This means instead of projecting an image into the lens itself, they use a beamsplitter and mirror to reflect images into your eye. The benefit of this is that you get good image quality from components that cost less than an equivalent waveguide setup, with the downside being increased light loss, potentially lower brightness and a much thicker design. This results in chunky frames that often look like they are sitting too far away from your face, which might not be immediately apparent if you see someone using them from afar. But up close, they don't look quite right. Or at least they don’t look like a pair of \"normal\" glasses.Another issue is that due to more light loss, birdbath smart glasses require darker lenses (similar to sunglasses), which means they aren't great for wearing all day in a variety of environments. And because we still don't really have a great protocol for wireless displays (though it looks like Valve may be cooking up something with the Steam Frames), most of these need to be connected by wire to a nearby PC. So you plug them in, put them on, get your work done and then you take them off. Project Aura is Xreal's next-gen smart glasses and they feature a large 70-degree field of view and fancy electrochromic lenses. Sam Rutherford for EngadgetThat said, for those who need a ton of screen real estate, this type of smart glasses can be a very attractive alternative to traditional portable monitors. On top of being smaller and more portable, they provide additional privacy when working in public spaces like a cafe or plane, which is what prompted a doctor friend of mine to get a pair instead of going with a portable display. And for the gamers out there, because they can be connected to a phone or even a portable PC or Switch 2 (with the proper dock, of course), they're great for people who might not have room for or access to a big screen TV.So where do we go from here?Ultimately, I think all three types of smart glasses will merge into one as engineers perfect the tech and steal ideas from one another, though there will surely be plenty of room for more niche designs. But more importantly, if we consider the types of gadgets most people carry around today, it boils down to just a handful of devices: a smartphone, some type of wireless audio (either earbuds or headphones) and maybe a health and fitness tracker of some kind (typically a smartwatch or smart ring). Even tough they didn't have a built-in display, the Meta Ray-Ban smart glasses from 2023 raised a ton of awareness for the category.Sam Rutherford for EngadgetSmart glasses have the potential to really round out that kit by allowing us to keep most of those devices in our pocket while the wearables serve up helpful info when we need it, but without being overly intrusive or distracting. In the short term, you'll still need a laptop for work, but smart glasses may have a role to play there too, as they can provide way more screen space than a traditional physical display (even the new-fangled flexible ones). It might never happen, but I wouldn’t rule out a future scenario where your next employer gives you a company-issued phone and a pair of smart glasses and that's it. Before that happens though, there are still a bunch of other things that need to be figured out. Without help from a mouse or keyboard, navigating a virtual display is a bit of a challenge. AI combined with hand and eye tracking can help, but no one has really nailed that combo yet. Not even Apple could do so on the much bulkier Vision Pro. To address this, Meta created a bracelet (they call it a neural band) that pairs with the Ray-Ban Display that can detect subtle movements so you can type or navigate menus practically anywhere. Even Realities opted for a ring accessory that does some basic health monitoring and comes with a tiny touchpad. In the more distant future, this hurdle may be solved by BCIs (brain-computer interfaces), but even the most optimistic view suggests that those aren't going to be mainstream for a long time.Even though we're still a long ways away, one day everyone might be able to have something like Tony Stark's E.D.I.T.H. smart glasses from the Marvel Universe. MarvelThe issue for Meta is that it's pretty obvious that its wristband really ought to be incorporated into a smartwatch. The idea of a single-purpose bracelet that doesn't track your health or do anything else sort of feels like a step backwards. And there's the problem of Meta's glasses being largely tied down to its own platforms (i.e. Instagram, Whatsapp and Facebook), which may end up being a major hindrance after rivals like Google and Apple catch up.And then there's the cost. Right now, a pair of Meta Ray-Ban Displays (which thankfully come with the wristband) costs $800. That's a lot for what is basically a publicly available beta test. But when you consider that an Even Realties G2 and an R2 ring costs even more at $850, it's clear that wearing smart glasses is going to be a very expensive hobby for at least the next few years. And while more single-purpose smart glasses from Xreal and Viture are a bit more affordable, with models ranging from $400 to $550 or $600, they still aren't cheap. On top of that, getting prescription lenses for smart glasses can often be a major pain in the ass and may not even be an option for people with more limited eyesight. But those are problems for another day. And just because tech giants are pouring billions into the development of smart glasses doesn't mean they will be a guaranteed hit. If you care about tech, alongside AI and possibly EVTOL aircraft (aka flying taxis), pay attention to the advancements in smart glasses. Otherwise, you could miss out on what might be the next major wave in sci-fi gadgetry made real.This article originally appeared on Engadget at https://www.engadget.com/wearables/in-2025-tech-giants-decided-smart-glasses-are-the-next-big-thing-163000812.html?src=rss",
          "content": "There's a growing sentiment that gadgets have gotten boring. And while I don't fully agree, I understand why people might feel that way. Just think about some of the novel device types that companies have tried to push since the original iPhone came out. 3D TVs were a massive flop and tablets still feel like extra-large smartphones despite Apple's efforts to prop them up as laptop replacements. Meanwhile, even with huge technological advancements over the last decade, VR headsets remain relatively niche due to factors like high prices and a lack of compelling content. And although big names like Google, Microsoft, Meta and others continue to dump billions into AI development, the first wave of dedicated AI devices was an abject failure. When you think about it, the only new(ish) class of gadget that has made major inroads to the mainstream market is smartwatches. That said, because they've evolved into wearable health and fitness sensors instead of the wrist-based computers that many once thought they would be, they haven't really disrupted our lives like the personal computer and smartphone did. But that seems poised to change because the tech giants have decided that smart glasses are going to be the next big thing. Headsets versus smart glasses, what’s the difference?Google is planning to support both smart glasses and headsets with Android XR, though the increased size and weight of devices like the Galaxy XR means it's not a great choice for all-day functionality. Sam Rutherford for EngadgetAt this point, you might be saying, \"Wait, hold on. Aren't VR headsets and smart glasses kind of the same thing?\" Well, yes and no. Both types of gadgets require similar software and hardware, but they utilize them in very different ways. Not only are VR goggles typically much bigger and heavier, they also provide a more isolated experience that can make it feel like you've been transported to another world. Sure, most modern headsets have exterior cameras that support some level of mixed reality (blending virtual graphics with physical objects) or let you peek quickly into meatspace (passthrough view) for when you need to get a drink or acknowledge other humans in the room. But in many respects, that closed-off feeling is the goal because it creates the ideal environment for playing games, taking virtual meetings or modeling 3D objects without real-world distractions. Furthermore, while many headsets like the Vision Pro and the Meta Quest 3 can function as standalone systems and support accessories like controllers or other motion trackers, they can also be tethered to a nearby PC for enhanced functionality. On the other hand, the default use case for smart glasses is a mixed reality environment where the spectacles can overlay helpful info or messages while you stay active and aware of your surroundings. Notably, while smart glasses might come with lenses or clip-on attachments that allow them to get darker or serve as sunglasses when you're outside, there typically isn't a way to completely block out the world like you can with a headset, mostly because that's simply not the point. And even though most smart spectacles can be paired with a phone to get access to mobile data or notifications, they're generally not meant to be tethered to a PC full-time (though there are some exceptions). The goal for smart glasses is more to provide a mobile-first heads-up display that augments what you see with your eyes instead of replacing things entirely with a digital environment. OK, but what makes you so sure that smart glasses are \"it?\"Now that we've discussed what separates smart glasses from headsets, what makes it so obvious that they are going to be the next big thing? This one is a bit easier to answer because we can simply look at the sheer number of companies that have released smart glasses or are planning to do so in the future. If we skip past the Google Glass from 2013 as forward-thinking specs that were ahead of their time, the most well-known example of modern smart glasses is the Meta Ray-Ban (or the even earlier Ray-Ban Stories from back when Facebook was still Facebook). While they are a bit chunky, the Meta Ray-Ban Display are some of the most sophisticated smart glasses on the market right now due in large part to their single full-color screen. Karissa Bell for EngadgetEven though they don't have built-in displays, the ability to capture photos and videos and play audio via built-in speakers brought the idea of smart glasses into the mainstream without making the concept look or feel completely ridiculous. Those earlier models then paved the way for even more sophisticated iterations like the Meta Ray-Ban Display from earlier this fall, which features a stunning RGB HUD (though only in the right lens) that has gotten us tantalizingly close to a true wearable display that doesn't make you look like a cyborg. Of course, Meta isn't the only game in town: there's a rapidly growing number of competitors from companies like Even Realities, Rokkid, TCL, Xreal, Viture and more. But for an even clearer sign of where the tech giants are heading, we can just look at Meta's two biggest competitors: Apple and Google. While Apple hasn't publicly announced plans to make its own smart glasses, Bloomberg's Mark Gurman — who is one of the company's most reliable analysts — provided inside info earlier this fall that Tim Cook and Co. are planning to pivot away from a proper follow-up to the Vision Pro in favor of more lightweight spectacles with greater mass appeal. This shouldn't really come as a major surprise, as sales of Apple's $3,500 headset have been lackluster. But more importantly, for a company that's extremely cautious about entering new product categories (foldable iPhone anyone?), it feels very telling to hear that Apple is shifting to smart glasses instead of abandoning the idea of wearable displays entirely. This is a company that doesn't swing and miss very often, so the idea of two flops in a row seems preposterous. If this pivot is real, there must be some Apple execs who are big believers that glasses and not goggles are the right choice for future development. Here are two of Google's reference design smart glasses. The one in the front features dual RGB waveguide displays while the one in the back relies on a single monocular screen.Sam Rutherford for EngadgetMeanwhile, Google is taking a two-pronged approach. In addition to releasing a new mixed reality OS — Android XR — on Samsung's Galaxy XR headset in October, the company has also teased upcoming smart glasses along with a handful of partners including glasses makers Gentle Monster and Warby Parker. Just this week, the company also added a number of new features to Android XR designed to support a wide range of upcoming devices while simultaneously making it easier for developers to port existing apps over to smart glasses and headsets. And if you still need additional evidence regarding Google's desire to get into smart glasses, consider that even with its ongoing collaboration, the company also spent $100 million to acquire a 4 percent stake in Gentle Monster. Regardless of who is making them though, the big draw for these companies is the idea that smart glasses will become a new piece of core personal computing, similar to how people rely on smartphones and laptops today (or to a lesser extent wireless headphones and smartwatches). If true, that could become a trillion-dollar market in the next 10 to 15 years (or sooner, who knows), which not only makes it a natural avenue for expansion but possibly a future existential crisis for certain companies. After all, none of these organizations want to be the next Microsoft after it failed to develop a successful smartphone or mobile OS.Fine, the smart glasses trend is real, but why would we even want them?At this point, I hope it's clear that the push for smart glasses is very real and very serious. But so far, we've only addressed why companies are betting big on them. So what's in it for us, the people who might actually buy and use them? Well, to answer that, we need to separate the current models into three main categories.A great use case for smart glasses would be to provide heads-up mapping without the need to constantly look down at your phone as seen in this demo clip of Android XR. GoogleFirst, there are the most basic smart glasses that don't come with built-in displays and typically rely on cameras and built-in speakers for enhanced functionality. The best example of this class of devices is the Meta Ray-Ban smart glasses (or the original Ray-Ban Stories) along with rivals like the Bose Sound Frames, which, believe it or not, have been on the market since 2019. However, before anyone gets attached to these early models, the simplest smart glasses already kind of feel like dinosaurs and will probably, in the not-too-distant future, go extinct. They were an interesting attempt to add things like music playback or photo and video capture to regular-looking sunglasses, but their limited feature set puts a clear ceiling on what they can do. Plus, if this is what people really wanted, they would have taken off already. Waveguides like the ones built into the Even Realities G2 project images directly onto their lenses allowing for super sleek glasses featuring a heads-up display. Sam Rutherford for EngadgetThis brings us to more recent offerings like the Meta Ray-Ban Display, Even Realities G2, the Halliday glasses and others which add some type of built-in display to the mix. Most often, these models rely on waveguide displays as they enable thinner and lighter designs while propagating images onto the glasses' lenses. Currently, most of these smart glasses feature single-color optics (usually green) to reduce complexity and power draw, but there are others like the Meta Ray-Ban Display and both the TCL RayNeo X2 and X3 that support full color.In this day and age when everyone is surrounded by screens, the idea of yet another display mounted inches away from your eyeballs might sound like the last thing you want. However, because modern smart glasses are much more discreet and less awkward-looking, I find that they can actually help cut down on distractions. That's because instead of having to peek down at your phone or smartwatch to check notifications, reply to messages or look up directions, you can do many or all of these things using smart glasses — all in the middle of a conversation without anyone noticing. Not only does this keep your focus where it should be — on people instead of gadgets — the glasses are also just as easy to wear as a smartwatch and far more comfortable than bulky VR headsets. Then, when you consider some other features of modern smart glasses like on-the-fly translation, the ability to function as a teleprompter hidden in plain sight or additional support from AI, suddenly you have a wearable that allows you to keep all of your other devices neatly stashed away. In many respects, smart glasses could be the portable displays that people might not even know they want.Compared to rivals with waveguides, glasses featuring \"birdbath\" optics are often significantly thicker and bulkier. Sam Rutherford for EngadgetSpeaking of portable displays: If you recall, I mentioned above how most smart glasses generally don't need to be tethered to other devices. The exception to that comes from a subclass of specs that are primarily designed to function as wearable monitors capable of supporting one or more virtual screens that can be in excess of 100 inches in size, relatively speaking. The most well-known smart glasses in this category come from Xreal and Viture, with both companies offering a range of models with varying levels of performance. One interesting thing to note is instead of waveguides, some of these smart glasses rely on birdbath optics. This means instead of projecting an image into the lens itself, they use a beamsplitter and mirror to reflect images into your eye. The benefit of this is that you get good image quality from components that cost less than an equivalent waveguide setup, with the downside being increased light loss, potentially lower brightness and a much thicker design. This results in chunky frames that often look like they are sitting too far away from your face, which might not be immediately apparent if you see someone using them from afar. But up close, they don't look quite right. Or at least they don’t look like a pair of \"normal\" glasses.Another issue is that due to more light loss, birdbath smart glasses require darker lenses (similar to sunglasses), which means they aren't great for wearing all day in a variety of environments. And because we still don't really have a great protocol for wireless displays (though it looks like Valve may be cooking up something with the Steam Frames), most of these need to be connected by wire to a nearby PC. So you plug them in, put them on, get your work done and then you take them off. Project Aura is Xreal's next-gen smart glasses and they feature a large 70-degree field of view and fancy electrochromic lenses. Sam Rutherford for EngadgetThat said, for those who need a ton of screen real estate, this type of smart glasses can be a very attractive alternative to traditional portable monitors. On top of being smaller and more portable, they provide additional privacy when working in public spaces like a cafe or plane, which is what prompted a doctor friend of mine to get a pair instead of going with a portable display. And for the gamers out there, because they can be connected to a phone or even a portable PC or Switch 2 (with the proper dock, of course), they're great for people who might not have room for or access to a big screen TV.So where do we go from here?Ultimately, I think all three types of smart glasses will merge into one as engineers perfect the tech and steal ideas from one another, though there will surely be plenty of room for more niche designs. But more importantly, if we consider the types of gadgets most people carry around today, it boils down to just a handful of devices: a smartphone, some type of wireless audio (either earbuds or headphones) and maybe a health and fitness tracker of some kind (typically a smartwatch or smart ring). Even tough they didn't have a built-in display, the Meta Ray-Ban smart glasses from 2023 raised a ton of awareness for the category.Sam Rutherford for EngadgetSmart glasses have the potential to really round out that kit by allowing us to keep most of those devices in our pocket while the wearables serve up helpful info when we need it, but without being overly intrusive or distracting. In the short term, you'll still need a laptop for work, but smart glasses may have a role to play there too, as they can provide way more screen space than a traditional physical display (even the new-fangled flexible ones). It might never happen, but I wouldn’t rule out a future scenario where your next employer gives you a company-issued phone and a pair of smart glasses and that's it. Before that happens though, there are still a bunch of other things that need to be figured out. Without help from a mouse or keyboard, navigating a virtual display is a bit of a challenge. AI combined with hand and eye tracking can help, but no one has really nailed that combo yet. Not even Apple could do so on the much bulkier Vision Pro. To address this, Meta created a bracelet (they call it a neural band) that pairs with the Ray-Ban Display that can detect subtle movements so you can type or navigate menus practically anywhere. Even Realities opted for a ring accessory that does some basic health monitoring and comes with a tiny touchpad. In the more distant future, this hurdle may be solved by BCIs (brain-computer interfaces), but even the most optimistic view suggests that those aren't going to be mainstream for a long time.Even though we're still a long ways away, one day everyone might be able to have something like Tony Stark's E.D.I.T.H. smart glasses from the Marvel Universe. MarvelThe issue for Meta is that it's pretty obvious that its wristband really ought to be incorporated into a smartwatch. The idea of a single-purpose bracelet that doesn't track your health or do anything else sort of feels like a step backwards. And there's the problem of Meta's glasses being largely tied down to its own platforms (i.e. Instagram, Whatsapp and Facebook), which may end up being a major hindrance after rivals like Google and Apple catch up.And then there's the cost. Right now, a pair of Meta Ray-Ban Displays (which thankfully come with the wristband) costs $800. That's a lot for what is basically a publicly available beta test. But when you consider that an Even Realties G2 and an R2 ring costs even more at $850, it's clear that wearing smart glasses is going to be a very expensive hobby for at least the next few years. And while more single-purpose smart glasses from Xreal and Viture are a bit more affordable, with models ranging from $400 to $550 or $600, they still aren't cheap. On top of that, getting prescription lenses for smart glasses can often be a major pain in the ass and may not even be an option for people with more limited eyesight. But those are problems for another day. And just because tech giants are pouring billions into the development of smart glasses doesn't mean they will be a guaranteed hit. If you care about tech, alongside AI and possibly EVTOL aircraft (aka flying taxis), pay attention to the advancements in smart glasses. Otherwise, you could miss out on what might be the next major wave in sci-fi gadgetry made real.This article originally appeared on Engadget at https://www.engadget.com/wearables/in-2025-tech-giants-decided-smart-glasses-are-the-next-big-thing-163000812.html?src=rss",
          "feed_position": 3,
          "image_url": "https://d29szjachogqwa.cloudfront.net/images/user-uploaded/Galaxy-XR-lead.jpg"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/gaming/swallowing-the-moon-and-other-new-indie-games-worth-checking-out-154937071.html",
          "published_at": "Mon, 15 Dec 2025 15:49:37 +0000",
          "title": "Swallowing the Moon and other new indie games worth checking out",
          "standfirst": "Welcome to our latest roundup of what's going on in the indie game space. Between The Game Awards and showcases like Day of the Devs, Wholesome Snack, Latin American Games and Women-Led Games, there’s been a ton of video game news over the last week (I need Control Resonant ASAP, please and thank you, Remedy). And hey, guess what? I've got even more for you to dig into, including info on some new releases that you can play right now.One of those is a game I’d been looking forward to since about 2022, and it’s always nice when something you’ve remained patient for turns out to be worth the wait. In Sam Eng's Skate Story, you play as a demon who accepts a deal offered by the Devil. If the demon can ride a skateboard to the Moon and swallow it after being transformed into a creature made of “glass and pain,” the Devil will grant them their freedom. It’s just as strange as that setup sounds. While you’ll need to execute combos to defeat bosses, this is a skateboarding game that leans more heavily on story than pure gameplay. It’s visually and sonically arresting too, with Blood Cultures and John Fio crafting a killer soundtrack I know I’ll be listening to for a long time to come. Skate Story is out now on Nintendo Switch 2, Steam and PS5 for $20. PlayStation Plus Extra and Premium members can play it at no extra cost. New releasesUnbeatable is another game I’ve had on my radar for some time, though I haven’t had a chance to jump in yet. This is another stylish game in which you play as Beat, who sings in a band. However, music is outlawed in this world (oh no!). Through rhythm-based minigames and battles with cops, Beat tries to bring back the music. There's a separate arcade mode with a dedicated progression system too.I'm a sucker for stories about rebellious underdogs, and this rhythm adventure could well hook me in. Unbeatable — from D-Cell Games and publisher Playstack — is available on Steam and PS5 for $28 (there’s a 10 percent discount on Steam until December 23). It's set to hit Xbox Series X/S very soon too.Speaking of games I've been keeping an eye on, Adrift (from solo developer S.K.9.8 and co-publisher Secret Sauce) was one of the first games I covered when I started doing this weekly roundup earlier this year. It's a driving game in which your aim is to deliver a volatile energy core. Since you're traversing a hot desert, you'll need to be careful to prevent the core from overheating and blowing up. Thankfully, there are safe spots and cooling stations where you can bring down the temperature.The vaporwave aesthetic of Adrift caught my eye and although I dig the presentation, the game isn't quite clicking for me in the early going. It didn't take long before my vehicle got stuck and I had to reset, and I'm finding the top-down world a little confusing to navigate. I'll stick with it for at least a little longer, though. Adrift is out now on Steam. It usually costs $13 though there's a 25 percent discount until December 23. I've very happy that a game like Drywall Eating Simulator can exist. Peripheral Playbox's satirical walking sim sees your character trying to deal with the maddening realities of daily life and the frustration that one may find in dealing with other people. Get mad enough and you'll be able to punch through a wall (something you'll have to do to move through the levels anyway). Then, you can munch on some drywall to calm yourself down.I had a good time with it and there’s some pointedly funny writing here. “I thought AI sucks but it told me that was wrong and I believed it,” says one person. That's all well and good, but I mainly just want the NPCs to leave me alone so I can eat drywall in peace. Drywall Eating Simulator is out now on Steam. It'll usually run you $10, but there's a 10 percent discount until December 17.Planet of Lana was one of my favorite games of 2023 and now it's available on iOS and Android for $9. It sees teenage Lana and her cute companion Mui making their way through a world that's been taken over by alien robots as they try to rescue Lana's sister.This is a puzzle platformer in the vein of Inside and Limbo, and despite the pretty and often bright presentation, it's just as dystopian as those games. It sounds gorgeous too, thanks in large part to a beautiful score from The Last Guardian composer Takeshi Furukawa. I'm very much looking forward to the sequel from Wishfully and publisher Thunderful. That's set to arrive next year.A Game About Digging A Hole is one of this year's real indie success stories. It’s a game that a developer started making in their spare time that has sold more than 1.2 million copies since February. After landing on PC and mobile, the $5 game from Doublebee and publisher Rokaplay is now on Nintendo Switch, Xbox Series X/S and PS5. It's on Game Pass Ultimate, Game Pass Premium and PC Game Pass.It's a straightforward loop. Start digging a hole in your backyard, sell the stuff you find, upgrade your equipment and keep going. Just, uh, be careful down there. You never quite know what you'll run into.Upcoming Vampire Therapist developer Little Bat Games has revealed its latest project, Better Than Us, which is coming to Steam in 2026. It's a narrative-driven sci-fi narrative game in which you'll infiltrate swanky parties thrown by wealth hoarders in the future to steal spoils back from them. Violence isn't the solution here, as you'll need to charm the ultra-rich, who buy elections and have \"monopolized AI development to ensure machines serve their interests\" (I dunno, this all seems extremely far-fetched). You can spin up a web of lies about things like how your husband died and how much Worldcoin you have. To maintain your ruse, you'll need to keep your story straight by remembering what you said and to which characters. Okomotive (Herdling, Far: Changing Tides) just revealed its next game. PinKeep is a roguelike deckbuilder in which you'll place structures on a playing field to fend off enemies. To collect resources, you'll need to play some pinball. By using the flippers (and flicking the ball for more precise movement), you can pick up what you need to fight back against your opponents. You can damage bad guys directly with the balls too. As a Ball x Pit enthusiast with a tepid but growing interest in deckbuilders, this speaks to me. A PinKeep demo is coming to Steam in January, with the full game set to arrive late in 2026.AudioMech is a neat-looking game that popped up for the first time during the pre-show of The Game Awards. This is a rhythm-based action title from Dylan Fitterer, the creator of Audiosurf. It taps into whatever music you have playing on your computer (even something that you're streaming or playing through a microphone) to customize both your weapons and opponents.A track that's heavy on bass might give you a longer sword, while vocals and lead instruments can power a cannon. There are several ways to play, including a mode in which you don't take damage and a boss rush option. AudioMech is coming to Steam and there's a demo available now. Let's wrap things up with something a little more relaxing. Lost and Found Co. is a hidden object game from Bit Egg Inc. and co-publisher Gamirror Games. During the latest Wholesome Snack showcase, it was revealed that the game is coming to Steam on February 11.It's little wonder that more than 170,000 Steam users have wishlisted this game. It looks absolutely lovely. The developers sought to recapture the \"magic\" of childhood puzzle books in their hand-drawn world. Here, you'll help Ducky, a duck-turned-human intern at a startup that hunts for items that townspeople have lost. There's a demo available that features the option to decorate a part of the world using items you find.This article originally appeared on Engadget at https://www.engadget.com/gaming/swallowing-the-moon-and-other-new-indie-games-worth-checking-out-154937071.html?src=rss",
          "content": "Welcome to our latest roundup of what's going on in the indie game space. Between The Game Awards and showcases like Day of the Devs, Wholesome Snack, Latin American Games and Women-Led Games, there’s been a ton of video game news over the last week (I need Control Resonant ASAP, please and thank you, Remedy). And hey, guess what? I've got even more for you to dig into, including info on some new releases that you can play right now.One of those is a game I’d been looking forward to since about 2022, and it’s always nice when something you’ve remained patient for turns out to be worth the wait. In Sam Eng's Skate Story, you play as a demon who accepts a deal offered by the Devil. If the demon can ride a skateboard to the Moon and swallow it after being transformed into a creature made of “glass and pain,” the Devil will grant them their freedom. It’s just as strange as that setup sounds. While you’ll need to execute combos to defeat bosses, this is a skateboarding game that leans more heavily on story than pure gameplay. It’s visually and sonically arresting too, with Blood Cultures and John Fio crafting a killer soundtrack I know I’ll be listening to for a long time to come. Skate Story is out now on Nintendo Switch 2, Steam and PS5 for $20. PlayStation Plus Extra and Premium members can play it at no extra cost. New releasesUnbeatable is another game I’ve had on my radar for some time, though I haven’t had a chance to jump in yet. This is another stylish game in which you play as Beat, who sings in a band. However, music is outlawed in this world (oh no!). Through rhythm-based minigames and battles with cops, Beat tries to bring back the music. There's a separate arcade mode with a dedicated progression system too.I'm a sucker for stories about rebellious underdogs, and this rhythm adventure could well hook me in. Unbeatable — from D-Cell Games and publisher Playstack — is available on Steam and PS5 for $28 (there’s a 10 percent discount on Steam until December 23). It's set to hit Xbox Series X/S very soon too.Speaking of games I've been keeping an eye on, Adrift (from solo developer S.K.9.8 and co-publisher Secret Sauce) was one of the first games I covered when I started doing this weekly roundup earlier this year. It's a driving game in which your aim is to deliver a volatile energy core. Since you're traversing a hot desert, you'll need to be careful to prevent the core from overheating and blowing up. Thankfully, there are safe spots and cooling stations where you can bring down the temperature.The vaporwave aesthetic of Adrift caught my eye and although I dig the presentation, the game isn't quite clicking for me in the early going. It didn't take long before my vehicle got stuck and I had to reset, and I'm finding the top-down world a little confusing to navigate. I'll stick with it for at least a little longer, though. Adrift is out now on Steam. It usually costs $13 though there's a 25 percent discount until December 23. I've very happy that a game like Drywall Eating Simulator can exist. Peripheral Playbox's satirical walking sim sees your character trying to deal with the maddening realities of daily life and the frustration that one may find in dealing with other people. Get mad enough and you'll be able to punch through a wall (something you'll have to do to move through the levels anyway). Then, you can munch on some drywall to calm yourself down.I had a good time with it and there’s some pointedly funny writing here. “I thought AI sucks but it told me that was wrong and I believed it,” says one person. That's all well and good, but I mainly just want the NPCs to leave me alone so I can eat drywall in peace. Drywall Eating Simulator is out now on Steam. It'll usually run you $10, but there's a 10 percent discount until December 17.Planet of Lana was one of my favorite games of 2023 and now it's available on iOS and Android for $9. It sees teenage Lana and her cute companion Mui making their way through a world that's been taken over by alien robots as they try to rescue Lana's sister.This is a puzzle platformer in the vein of Inside and Limbo, and despite the pretty and often bright presentation, it's just as dystopian as those games. It sounds gorgeous too, thanks in large part to a beautiful score from The Last Guardian composer Takeshi Furukawa. I'm very much looking forward to the sequel from Wishfully and publisher Thunderful. That's set to arrive next year.A Game About Digging A Hole is one of this year's real indie success stories. It’s a game that a developer started making in their spare time that has sold more than 1.2 million copies since February. After landing on PC and mobile, the $5 game from Doublebee and publisher Rokaplay is now on Nintendo Switch, Xbox Series X/S and PS5. It's on Game Pass Ultimate, Game Pass Premium and PC Game Pass.It's a straightforward loop. Start digging a hole in your backyard, sell the stuff you find, upgrade your equipment and keep going. Just, uh, be careful down there. You never quite know what you'll run into.Upcoming Vampire Therapist developer Little Bat Games has revealed its latest project, Better Than Us, which is coming to Steam in 2026. It's a narrative-driven sci-fi narrative game in which you'll infiltrate swanky parties thrown by wealth hoarders in the future to steal spoils back from them. Violence isn't the solution here, as you'll need to charm the ultra-rich, who buy elections and have \"monopolized AI development to ensure machines serve their interests\" (I dunno, this all seems extremely far-fetched). You can spin up a web of lies about things like how your husband died and how much Worldcoin you have. To maintain your ruse, you'll need to keep your story straight by remembering what you said and to which characters. Okomotive (Herdling, Far: Changing Tides) just revealed its next game. PinKeep is a roguelike deckbuilder in which you'll place structures on a playing field to fend off enemies. To collect resources, you'll need to play some pinball. By using the flippers (and flicking the ball for more precise movement), you can pick up what you need to fight back against your opponents. You can damage bad guys directly with the balls too. As a Ball x Pit enthusiast with a tepid but growing interest in deckbuilders, this speaks to me. A PinKeep demo is coming to Steam in January, with the full game set to arrive late in 2026.AudioMech is a neat-looking game that popped up for the first time during the pre-show of The Game Awards. This is a rhythm-based action title from Dylan Fitterer, the creator of Audiosurf. It taps into whatever music you have playing on your computer (even something that you're streaming or playing through a microphone) to customize both your weapons and opponents.A track that's heavy on bass might give you a longer sword, while vocals and lead instruments can power a cannon. There are several ways to play, including a mode in which you don't take damage and a boss rush option. AudioMech is coming to Steam and there's a demo available now. Let's wrap things up with something a little more relaxing. Lost and Found Co. is a hidden object game from Bit Egg Inc. and co-publisher Gamirror Games. During the latest Wholesome Snack showcase, it was revealed that the game is coming to Steam on February 11.It's little wonder that more than 170,000 Steam users have wishlisted this game. It looks absolutely lovely. The developers sought to recapture the \"magic\" of childhood puzzle books in their hand-drawn world. Here, you'll help Ducky, a duck-turned-human intern at a startup that hunts for items that townspeople have lost. There's a demo available that features the option to decorate a part of the world using items you find.This article originally appeared on Engadget at https://www.engadget.com/gaming/swallowing-the-moon-and-other-new-indie-games-worth-checking-out-154937071.html?src=rss",
          "feed_position": 5
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/tokenization-takes-the-lead-in-the-fight-for-data-security",
          "published_at": "Mon, 15 Dec 2025 15:00:00 GMT",
          "title": "Tokenization takes the lead in the fight for data security",
          "standfirst": "Presented by Capital One SoftwareTokenization is emerging as a cornerstone of modern data security, helping businesses separate the value of their data from its risk. During this VB in Conversation, Ravi Raghu, president, Capital One Software, talks about the ways tokenization can help reduce the value of breached data and preserve underlying data format and usability, including Capital One’s own experience leveraging tokenization at scale. Tokenization, Raghu asserts, is a far superior technology. It converts sensitive data into a nonsensitive digital replacement, called a token, that maps back to the original, which is secured in a digital vault. The token placeholder preserves both the format and the utility of the sensitive data, and can be used across applications — including AI models. Because tokenization removes the need to manage encryption keys or dedicate compute to constant encrypting and decrypting, it offers one of the most scalable ways for companies to protect their most sensitive data, he added.\"The killer part, from a security standpoint, when you think about it relative to other methods, if a bad actor gets hold of the data, they get hold of tokens,\" he explained. \"The actual data is not sitting with the token, unlike other methods like encryption, where the actual data sits there, just waiting for someone to get hold of a key or use brute force to get to the real data. From every angle this is the ideal way one ought to go about protecting sensitive data.\"The tokenization differentiator Most organizations are just scratching the surface of data security, adding security at the very end, when data is read, to prevent an end user from accessing it. At minimum, organizations should focus on securing data on write, as it’s being stored. But best-in-class organizations go even further, protecting data at birth, the moment it’s created.At one end of the safety spectrum is a simple lock-and-key approach that restricts access but leaves the underlying data intact. More advanced methods, like masking or modifying data, permanently alter its meaning — which can compromise its usefulness. File-level encryption provides broader protection for large volumes of stored data, but when you get down to field-level encryption (for example, a Social Security number), it becomes a bigger challenge. It takes a great deal of compute to encrypt a single field, and then to decrypt it at the point of usage. And still it has a fatal flaw: the original data is still right there, only needing the key to get access. Tokenization avoids these pitfalls by replacing the original data with a surrogate that has no intrinsic value. If the token is intercepted — whether by the wrong person or the wrong machine — the data itself remains secure.The business value of tokenization\"Fundamentally you’re protecting data, and that’s priceless,\" Raghu said. \"Another thing that’s priceless – can you use that for modeling purposes subsequently? On the one hand, it’s a protection thing, and on the other hand it’s a business enabling thing.\" Because tokenization preserves the structure and ordinality of the original data, it can still be used for modeling and analytics, turning protection into a business enabler. Take private health data governed by HIPAA for example: tokenization means that data canbeused to build pricing models or for gene therapy research, while remaining compliant. \"If your data is already protected, you can then proliferate the usage of data across the entire enterprise and have everybody creating more and more value out of the data,\" Raghu said. \"Conversely, if you don’t have that, there’s a lot of reticence for enterprises today to have more people access it, or have more and more AI agents access their data. Ironically, they’re limiting the blast radius of innovation. The tokenization impact is massive, and there are many metrics you could use to measure that – operational impact, revenue impact, and obviously the peace of mind from a security standpoint.\"Breaking down adoption barriersUntil now, the fundamental challenge with traditional tokenization has been performance. AI requires a scale and speed that is unprecedented. That&#x27;s one of the major challenges Capital One addresses with Databolt, its vaultless tokenization solution, which can produce up to 4 million tokens per second.\"Capital One has gone through tokenization for more than a decade. We started doing it because we’re serving our 100 million banking customers. We want to protect that sensitive data,\" Raghu said. \"We’ve eaten our own dog food with our internal tokenization capability, over 100 billion times a month. We’ve taken that know-how and that capability, scale, and speed, and innovated so that the world can leverage it, so that it’s a commercial offering.\"Vaultless tokenization is an advanced form of tokenization that does not require a central database (vault) to store token mappings. Instead, it uses mathematical algorithms, cryptographic techniques, and deterministic mapping to generate tokens dynamically.This approach is faster, more scalable, and eliminates the security risk associated with managing a vault.\"We realized that for the scale and speed demands that we had, we needed to build out that capability ourselves,\" Raghu said. \"We’ve been iterating continuously on making sure that it can scale up to hundreds of billions of operations a month. All of our innovation has been around building IP and capability to do that thing at a battle-tested scale within our enterprise, for the purpose of serving our customers.\"While conventional tokenization methods can involve some complexity and slow down operations, Databolt seamlessly integrates with encrypted data warehouses, allowing businesses to maintain robust security without slowing performance or operations. Tokenization occurs in the customer’s environment, removing the need to communicate with an external network to perform tokenization operations, which can also slow performance.\"We believe that fundamentally, tokenization should be easy to adopt,\" Raghu said. \"You should be able to secure your data very quickly and operate at the speed and scale and cost needs that organizations have. I think that’s been a critical barrier so far for the mass scale adoption of tokenization. In an AI world, that’s going to become a huge enabler.\"Don&#x27;t miss the whole conversation with Ravi Raghu, president, Capital One Software, here.Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.",
          "content": "Presented by Capital One SoftwareTokenization is emerging as a cornerstone of modern data security, helping businesses separate the value of their data from its risk. During this VB in Conversation, Ravi Raghu, president, Capital One Software, talks about the ways tokenization can help reduce the value of breached data and preserve underlying data format and usability, including Capital One’s own experience leveraging tokenization at scale. Tokenization, Raghu asserts, is a far superior technology. It converts sensitive data into a nonsensitive digital replacement, called a token, that maps back to the original, which is secured in a digital vault. The token placeholder preserves both the format and the utility of the sensitive data, and can be used across applications — including AI models. Because tokenization removes the need to manage encryption keys or dedicate compute to constant encrypting and decrypting, it offers one of the most scalable ways for companies to protect their most sensitive data, he added.\"The killer part, from a security standpoint, when you think about it relative to other methods, if a bad actor gets hold of the data, they get hold of tokens,\" he explained. \"The actual data is not sitting with the token, unlike other methods like encryption, where the actual data sits there, just waiting for someone to get hold of a key or use brute force to get to the real data. From every angle this is the ideal way one ought to go about protecting sensitive data.\"The tokenization differentiator Most organizations are just scratching the surface of data security, adding security at the very end, when data is read, to prevent an end user from accessing it. At minimum, organizations should focus on securing data on write, as it’s being stored. But best-in-class organizations go even further, protecting data at birth, the moment it’s created.At one end of the safety spectrum is a simple lock-and-key approach that restricts access but leaves the underlying data intact. More advanced methods, like masking or modifying data, permanently alter its meaning — which can compromise its usefulness. File-level encryption provides broader protection for large volumes of stored data, but when you get down to field-level encryption (for example, a Social Security number), it becomes a bigger challenge. It takes a great deal of compute to encrypt a single field, and then to decrypt it at the point of usage. And still it has a fatal flaw: the original data is still right there, only needing the key to get access. Tokenization avoids these pitfalls by replacing the original data with a surrogate that has no intrinsic value. If the token is intercepted — whether by the wrong person or the wrong machine — the data itself remains secure.The business value of tokenization\"Fundamentally you’re protecting data, and that’s priceless,\" Raghu said. \"Another thing that’s priceless – can you use that for modeling purposes subsequently? On the one hand, it’s a protection thing, and on the other hand it’s a business enabling thing.\" Because tokenization preserves the structure and ordinality of the original data, it can still be used for modeling and analytics, turning protection into a business enabler. Take private health data governed by HIPAA for example: tokenization means that data canbeused to build pricing models or for gene therapy research, while remaining compliant. \"If your data is already protected, you can then proliferate the usage of data across the entire enterprise and have everybody creating more and more value out of the data,\" Raghu said. \"Conversely, if you don’t have that, there’s a lot of reticence for enterprises today to have more people access it, or have more and more AI agents access their data. Ironically, they’re limiting the blast radius of innovation. The tokenization impact is massive, and there are many metrics you could use to measure that – operational impact, revenue impact, and obviously the peace of mind from a security standpoint.\"Breaking down adoption barriersUntil now, the fundamental challenge with traditional tokenization has been performance. AI requires a scale and speed that is unprecedented. That&#x27;s one of the major challenges Capital One addresses with Databolt, its vaultless tokenization solution, which can produce up to 4 million tokens per second.\"Capital One has gone through tokenization for more than a decade. We started doing it because we’re serving our 100 million banking customers. We want to protect that sensitive data,\" Raghu said. \"We’ve eaten our own dog food with our internal tokenization capability, over 100 billion times a month. We’ve taken that know-how and that capability, scale, and speed, and innovated so that the world can leverage it, so that it’s a commercial offering.\"Vaultless tokenization is an advanced form of tokenization that does not require a central database (vault) to store token mappings. Instead, it uses mathematical algorithms, cryptographic techniques, and deterministic mapping to generate tokens dynamically.This approach is faster, more scalable, and eliminates the security risk associated with managing a vault.\"We realized that for the scale and speed demands that we had, we needed to build out that capability ourselves,\" Raghu said. \"We’ve been iterating continuously on making sure that it can scale up to hundreds of billions of operations a month. All of our innovation has been around building IP and capability to do that thing at a battle-tested scale within our enterprise, for the purpose of serving our customers.\"While conventional tokenization methods can involve some complexity and slow down operations, Databolt seamlessly integrates with encrypted data warehouses, allowing businesses to maintain robust security without slowing performance or operations. Tokenization occurs in the customer’s environment, removing the need to communicate with an external network to perform tokenization operations, which can also slow performance.\"We believe that fundamentally, tokenization should be easy to adopt,\" Raghu said. \"You should be able to secure your data very quickly and operate at the speed and scale and cost needs that organizations have. I think that’s been a critical barrier so far for the mass scale adoption of tokenization. In an AI world, that’s going to become a huge enabler.\"Don&#x27;t miss the whole conversation with Ravi Raghu, president, Capital One Software, here.Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.",
          "feed_position": 0,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/3TXe0hgWfdDkdCl4d3mMcL/848b1f96ed7aa639b6f5dc38e157f29e/Capital_One_VB_Convo_hero.jpg?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/technology/nvidia-debuts-nemotron-3-with-hybrid-moe-and-mamba-transformer-to-drive",
          "published_at": "Mon, 15 Dec 2025 05:00:00 GMT",
          "title": "Nvidia debuts Nemotron 3 with hybrid MoE and Mamba-Transformer to drive efficient agentic AI",
          "standfirst": "Nvidia launched the new version of its frontier models, Nemotron 3, by leaning in on a model architecture that the world’s most valuable company said offers more accuracy and reliability for agents. Nemotron 3 will be available in three sizes: Nemotron 3 Nano with 30B parameters, mainly for targeted, highly efficient tasks; Nemotron 3 Super, which is a 100B parameter model for multi-agent applications and with high-accuracy reasoning and Nemotron 3 Ultra, with its large reasoning engine and around 500B parameters for more complex applications. To build the Nemotron 3 models, Nvidia said it leaned into a hybrid mixture-of-experts (MoE) architecture to improve scalability and efficiency. By using this architecture, Nvidia said in a press release that its new models also offer enterprises more openness and performance when building multi-agent autonomous systems. Kari Briski, Nvidia vice president for generative AI software, told reporters in a briefing that the company wanted to demonstrate its commitment to learn and improving from previous iterations of its models. “We believe that we are uniquely positioned to serve a wide range of developers who want full flexibility to customize models for building specialized AI by combining that new hybrid mixture of our mixture of experts architecture with a 1 million token context length,” Briski said. Nvidia said early adopters of the Nemotron 3 models include Accenture, CrowdStrike, Cursor, Deloitte, EY, Oracle Cloud Infrastructure, Palantir, Perplexity, ServiceNow, Siemens and Zoom.Breakthrough architectures Nvidia has been using the hybrid Mamba-Transformer mixture-of-experts architecture for many of its models, including Nemotron-Nano-9B-v2.The architecture is based on research from Carnegie Mellon University and Princeton, which weaves in selective state-space models to handle long pieces of information while maintaining states. It can reduce compute costs even through long contexts. Nvidia noted its design “achieves up to 4x higher token throughput” compared to Nemotron 2 Nano and can significantly lower inference costs by reducing reasoning token generation by up 60%.“We really need to be able to bring that efficiency up and the cost per token down. And you can do it through a number of ways, but we&#x27;re really doing it through the innovations of that model architecture,” Briski said. “The hybrid Mamba transformer architecture runs several times faster with less memory, because it avoids these huge attention maps and key value caches for every single token.”Nvidia also introduced an additional innovation for the Nemotron 3 Super and Ultra models. For these, Briski said Nvidia deployed “a breakthrough called latent MoE.”“That’s all these experts that are in your model share a common core and keep only a small part private. It’s kind of like chefs sharing one big kitchen, but they need to get their own spice rack,” Briski added. Nvidia is not the only company that employs this kind of architecture to build models. AI21 Labs uses it for its Jamba models, most recently in its Jamba Reasoning 3B model.The Nemotron 3 models benefited from extended reinforcement learning. The larger models, Super and Ultra, used the company’s 4-bit NVFP4 training format, which allows them to train on existing infrastructure without compromising accuracy.Benchmark testing from Artificial Analysis placed the Nemotron models highly among models of similar size. New environments for models to ‘work out’As part of the Nemotron 3 launch, Nvidia will also give users access to its research by releasing its papers and sample prompts, offering open datasets where people can use and look at pre-training tokens and post-training samples, and most importantly, a new NeMo Gym where customers can let their models and agents “workout.” The NeMo Gym is a reinforcement learning lab where users can let their models run in simulated environments to test their post-training performance. AWS announced a similar tool through its Nova Forge platform, targeted for enterprises that want to test out their newly created distilled or smaller models. Briski said the samples of post-training data Nvidia plans to release “are orders of magnitude larger than any available post-training data set and are also very permissive and open.”Nvidia pointed to developers seeking highly intelligent and performant open models, so they can better understand how to guide them if needed, as the basis for releasing more information about how it trains its models. “Model developers today hit this tough trifecta. They need to find models that are ultra open, that are extremely intelligent and are highly efficient,” she said. “Most open models force developers into painful trade-offs between efficiencies like token costs, latency, and throughput.”She said developers want to know how a model was trained, where the training data came from and how they can evaluate it.",
          "content": "Nvidia launched the new version of its frontier models, Nemotron 3, by leaning in on a model architecture that the world’s most valuable company said offers more accuracy and reliability for agents. Nemotron 3 will be available in three sizes: Nemotron 3 Nano with 30B parameters, mainly for targeted, highly efficient tasks; Nemotron 3 Super, which is a 100B parameter model for multi-agent applications and with high-accuracy reasoning and Nemotron 3 Ultra, with its large reasoning engine and around 500B parameters for more complex applications. To build the Nemotron 3 models, Nvidia said it leaned into a hybrid mixture-of-experts (MoE) architecture to improve scalability and efficiency. By using this architecture, Nvidia said in a press release that its new models also offer enterprises more openness and performance when building multi-agent autonomous systems. Kari Briski, Nvidia vice president for generative AI software, told reporters in a briefing that the company wanted to demonstrate its commitment to learn and improving from previous iterations of its models. “We believe that we are uniquely positioned to serve a wide range of developers who want full flexibility to customize models for building specialized AI by combining that new hybrid mixture of our mixture of experts architecture with a 1 million token context length,” Briski said. Nvidia said early adopters of the Nemotron 3 models include Accenture, CrowdStrike, Cursor, Deloitte, EY, Oracle Cloud Infrastructure, Palantir, Perplexity, ServiceNow, Siemens and Zoom.Breakthrough architectures Nvidia has been using the hybrid Mamba-Transformer mixture-of-experts architecture for many of its models, including Nemotron-Nano-9B-v2.The architecture is based on research from Carnegie Mellon University and Princeton, which weaves in selective state-space models to handle long pieces of information while maintaining states. It can reduce compute costs even through long contexts. Nvidia noted its design “achieves up to 4x higher token throughput” compared to Nemotron 2 Nano and can significantly lower inference costs by reducing reasoning token generation by up 60%.“We really need to be able to bring that efficiency up and the cost per token down. And you can do it through a number of ways, but we&#x27;re really doing it through the innovations of that model architecture,” Briski said. “The hybrid Mamba transformer architecture runs several times faster with less memory, because it avoids these huge attention maps and key value caches for every single token.”Nvidia also introduced an additional innovation for the Nemotron 3 Super and Ultra models. For these, Briski said Nvidia deployed “a breakthrough called latent MoE.”“That’s all these experts that are in your model share a common core and keep only a small part private. It’s kind of like chefs sharing one big kitchen, but they need to get their own spice rack,” Briski added. Nvidia is not the only company that employs this kind of architecture to build models. AI21 Labs uses it for its Jamba models, most recently in its Jamba Reasoning 3B model.The Nemotron 3 models benefited from extended reinforcement learning. The larger models, Super and Ultra, used the company’s 4-bit NVFP4 training format, which allows them to train on existing infrastructure without compromising accuracy.Benchmark testing from Artificial Analysis placed the Nemotron models highly among models of similar size. New environments for models to ‘work out’As part of the Nemotron 3 launch, Nvidia will also give users access to its research by releasing its papers and sample prompts, offering open datasets where people can use and look at pre-training tokens and post-training samples, and most importantly, a new NeMo Gym where customers can let their models and agents “workout.” The NeMo Gym is a reinforcement learning lab where users can let their models run in simulated environments to test their post-training performance. AWS announced a similar tool through its Nova Forge platform, targeted for enterprises that want to test out their newly created distilled or smaller models. Briski said the samples of post-training data Nvidia plans to release “are orders of magnitude larger than any available post-training data set and are also very permissive and open.”Nvidia pointed to developers seeking highly intelligent and performant open models, so they can better understand how to guide them if needed, as the basis for releasing more information about how it trains its models. “Model developers today hit this tough trifecta. They need to find models that are ultra open, that are extremely intelligent and are highly efficient,” she said. “Most open models force developers into painful trade-offs between efficiencies like token costs, latency, and throughput.”She said developers want to know how a model was trained, where the training data came from and how they can evaluate it.",
          "feed_position": 1,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/3xUHPM8pE59D833CnZxxZJ/b8fe73f844b3383d940fda2c5906df94/crimedy7_illustration_of_the_nvidia_colors_in_a_digital_archi_31fa1654-c274-4f35-9673-8879080998cf_1.png?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/why-agentic-ai-needs-a-new-category-of-customer-data",
          "published_at": "Mon, 15 Dec 2025 05:00:00 GMT",
          "title": "Why agentic AI needs a new category of customer data",
          "standfirst": "Presented by TwilioThe customer data infrastructure powering most enterprises was architected for a world that no longer exists: one where marketing interactions could be captured and processed in batches, where campaign timing was measured in days (not milliseconds), and where \"personalization\" meant inserting a first name into an email template.Conversational AI has shattered those assumptions.AI agents need to know what a customer just said, the tone they used, their emotional state, and their complete history with a brand instantly to provide relevant guidance and effective resolution. This fast-moving stream of conversational signals (tone, urgency, intent, sentiment) represents a fundamentally different category of customer data. Yet the systems most enterprises rely on today were never designed to capture or deliver it at the speed modern customer experiences demand.The conversational AI context gapThe consequences of this architectural mismatch are already visible in customer satisfaction data. Twilio’s Inside the Conversational AI Revolution report reveals that more than half (54%) of consumers report AI rarely has context from their past interactions, and only 15% feel that human agents receive the full story after an AI handoff. The result: customer experiences defined by repetition, friction, and disjointed handoffs.The problem isn&#x27;t a lack of customer data. Enterprises are drowning in it. The problem is that conversational AI requires real-time, portable memory of customer interactions, and few organizations have infrastructure capable of delivering it. Traditional CRMs and CDPs excel at capturing static attributes but weren&#x27;t architected to handle the dynamic exchange of a conversation unfolding second by second.Solving this requires building conversational memory inside communications infrastructure itself, rather than attempting to bolt it onto legacy data systems through integrations.The agentic AI adoption wave and its limitsThis infrastructure gap is becoming critical as agentic AI moves from pilot to production. Nearly two-thirds of companies (63%) are already in late-stage development or fully deployed with conversational AI across sales and support functions.The reality check: While 90% of organizations believe customers are satisfied with their AI experiences, only 59% of consumers agree. The disconnect isn&#x27;t about conversational fluency or response speed. It&#x27;s about whether AI can demonstrate true understanding, respond with appropriate context, and actually solve problems rather than forcing escalation to human agents.Consider the gap: A customer calls about a delayed order. With proper conversational memory infrastructure, an AI agent could instantly recognize the customer, reference their previous order, details about a delay, proactively suggest solutions, and offer appropriate compensation, all without asking them to repeat information. Most enterprises can&#x27;t deliver this because the required data lives in separate systems that can&#x27;t be accessed quickly enough.Where enterprise data architecture breaks downEnterprise data systems built for marketing and support were optimized for structured data and batch processing, not the dynamic memory required for natural conversation. Three fundamental limitations prevent these systems from supporting conversational AI:Latency breaks the conversational contract. When customer data lives in one system and conversations happen in another, every interaction requires API calls that introduce 200-500 millisecond delays, transforming natural dialogue into robotic exchanges.Conversational nuance gets lost. The signals that make conversations meaningful (tone, urgency, emotional state, commitments made mid-conversation) rarely make it into traditional CRMs, which were designed to capture structured data, not the unstructured richness AI needs.Data fragmentation creates experience fragmentation. AI agents operate in one system, human agents in another, marketing automation in a third, and customer data in a fourth, creating fractured experiences where context evaporates at every handoff.Conversational memory requires infrastructure where conversations and customer data are unified by design.What unified conversational memory enablesOrganizations treating conversational memory as core infrastructure are seeing clear competitive advantages:Seamless handoffs: When conversational memory is unified, human agents inherit complete context instantly, eliminating the \"let me pull up your account\" dead time that signals wasted interactions.Personalization at scale: While 88% of consumers expect personalized experiences, over half of businesses cite this as a top challenge. When conversational memory is native to communications infrastructure, agents can personalize based on what customers are trying to accomplish right now.Operational intelligence: Unified conversational memory provides real-time visibility into conversation quality and key performance indicators, with insights feeding back into AI models to improve quality continuously.Agentic automation: Perhaps most significantly, conversational memory transforms AI from a transactional tool to a genuinely agentic system capable of nuanced decisions, like rebooking a frustrated customer&#x27;s flight while offering compensation calibrated to their loyalty tier.The infrastructure imperativeThe agentic AI wave is forcing a fundamental re-architecture of how enterprises think about customer data.The solution isn&#x27;t iterating on existing CDP or CRM architecture. It&#x27;s recognizing that conversational memory represents a distinct category requiring real-time capture, millisecond-level access, and preservation of conversational nuance that can only be met when data capabilities are embedded directly into communications infrastructure.Organizations approaching this as a systems integration challenge will find themselves at a disadvantage against competitors who treat conversational memory as foundational infrastructure. When memory is native to the platform powering every customer touchpoint, context travels with customers across channels, latency disappears, and continuous journeys become operationally feasible.The enterprises setting the pace aren&#x27;t those with the most sophisticated AI models. They&#x27;re the ones that solved the infrastructure problem first, recognizing that agentic AI can&#x27;t deliver on its promise without a new category of customer data purpose-built for the speed, nuance, and continuity that conversational experiences demand. Robin Grochol is SVP of Product, Data, Identity & Security at Twilio.Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.",
          "content": "Presented by TwilioThe customer data infrastructure powering most enterprises was architected for a world that no longer exists: one where marketing interactions could be captured and processed in batches, where campaign timing was measured in days (not milliseconds), and where \"personalization\" meant inserting a first name into an email template.Conversational AI has shattered those assumptions.AI agents need to know what a customer just said, the tone they used, their emotional state, and their complete history with a brand instantly to provide relevant guidance and effective resolution. This fast-moving stream of conversational signals (tone, urgency, intent, sentiment) represents a fundamentally different category of customer data. Yet the systems most enterprises rely on today were never designed to capture or deliver it at the speed modern customer experiences demand.The conversational AI context gapThe consequences of this architectural mismatch are already visible in customer satisfaction data. Twilio’s Inside the Conversational AI Revolution report reveals that more than half (54%) of consumers report AI rarely has context from their past interactions, and only 15% feel that human agents receive the full story after an AI handoff. The result: customer experiences defined by repetition, friction, and disjointed handoffs.The problem isn&#x27;t a lack of customer data. Enterprises are drowning in it. The problem is that conversational AI requires real-time, portable memory of customer interactions, and few organizations have infrastructure capable of delivering it. Traditional CRMs and CDPs excel at capturing static attributes but weren&#x27;t architected to handle the dynamic exchange of a conversation unfolding second by second.Solving this requires building conversational memory inside communications infrastructure itself, rather than attempting to bolt it onto legacy data systems through integrations.The agentic AI adoption wave and its limitsThis infrastructure gap is becoming critical as agentic AI moves from pilot to production. Nearly two-thirds of companies (63%) are already in late-stage development or fully deployed with conversational AI across sales and support functions.The reality check: While 90% of organizations believe customers are satisfied with their AI experiences, only 59% of consumers agree. The disconnect isn&#x27;t about conversational fluency or response speed. It&#x27;s about whether AI can demonstrate true understanding, respond with appropriate context, and actually solve problems rather than forcing escalation to human agents.Consider the gap: A customer calls about a delayed order. With proper conversational memory infrastructure, an AI agent could instantly recognize the customer, reference their previous order, details about a delay, proactively suggest solutions, and offer appropriate compensation, all without asking them to repeat information. Most enterprises can&#x27;t deliver this because the required data lives in separate systems that can&#x27;t be accessed quickly enough.Where enterprise data architecture breaks downEnterprise data systems built for marketing and support were optimized for structured data and batch processing, not the dynamic memory required for natural conversation. Three fundamental limitations prevent these systems from supporting conversational AI:Latency breaks the conversational contract. When customer data lives in one system and conversations happen in another, every interaction requires API calls that introduce 200-500 millisecond delays, transforming natural dialogue into robotic exchanges.Conversational nuance gets lost. The signals that make conversations meaningful (tone, urgency, emotional state, commitments made mid-conversation) rarely make it into traditional CRMs, which were designed to capture structured data, not the unstructured richness AI needs.Data fragmentation creates experience fragmentation. AI agents operate in one system, human agents in another, marketing automation in a third, and customer data in a fourth, creating fractured experiences where context evaporates at every handoff.Conversational memory requires infrastructure where conversations and customer data are unified by design.What unified conversational memory enablesOrganizations treating conversational memory as core infrastructure are seeing clear competitive advantages:Seamless handoffs: When conversational memory is unified, human agents inherit complete context instantly, eliminating the \"let me pull up your account\" dead time that signals wasted interactions.Personalization at scale: While 88% of consumers expect personalized experiences, over half of businesses cite this as a top challenge. When conversational memory is native to communications infrastructure, agents can personalize based on what customers are trying to accomplish right now.Operational intelligence: Unified conversational memory provides real-time visibility into conversation quality and key performance indicators, with insights feeding back into AI models to improve quality continuously.Agentic automation: Perhaps most significantly, conversational memory transforms AI from a transactional tool to a genuinely agentic system capable of nuanced decisions, like rebooking a frustrated customer&#x27;s flight while offering compensation calibrated to their loyalty tier.The infrastructure imperativeThe agentic AI wave is forcing a fundamental re-architecture of how enterprises think about customer data.The solution isn&#x27;t iterating on existing CDP or CRM architecture. It&#x27;s recognizing that conversational memory represents a distinct category requiring real-time capture, millisecond-level access, and preservation of conversational nuance that can only be met when data capabilities are embedded directly into communications infrastructure.Organizations approaching this as a systems integration challenge will find themselves at a disadvantage against competitors who treat conversational memory as foundational infrastructure. When memory is native to the platform powering every customer touchpoint, context travels with customers across channels, latency disappears, and continuous journeys become operationally feasible.The enterprises setting the pace aren&#x27;t those with the most sophisticated AI models. They&#x27;re the ones that solved the infrastructure problem first, recognizing that agentic AI can&#x27;t deliver on its promise without a new category of customer data purpose-built for the speed, nuance, and continuity that conversational experiences demand. Robin Grochol is SVP of Product, Data, Identity & Security at Twilio.Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.",
          "feed_position": 2,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/4ONhyEvpMjhV5KWf469Zcm/3534179a6a3df355dab84e929e0db853/AdobeStock_1389144486.jpeg?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/build-vs-buy-is-dead-ai-just-killed-it",
          "published_at": "Sun, 14 Dec 2025 19:00:00 GMT",
          "title": "Build vs buy is dead — AI just killed it",
          "standfirst": "Picture this: You&#x27;re sitting in a conference room, halfway through a vendor pitch. The demo looks solid, and pricing fits nicely under budget. The timeline seems reasonable too. Everyone’s nodding along.You’re literally minutes away from saying yes.Then someone from your finance team walks in. They see the deck and frown. A few minutes later, they shoot you a message on Slack: “Actually, I threw together a version of this last week. Took me 2 hours in Cursor. Wanna take a look?”Wait… what?This person doesn&#x27;t code. You know for a fact they&#x27;ve never written a line of JavaScript in their entire life. But here they are, showing you a working prototype on their laptop that does... pretty much exactly what the vendor pitched. Sure, it&#x27;s got some rough edges, but it works. And it didn’t cost six figures. Just two hours of their time.Suddenly, the assumptions you walked in with — about how software is developed, who makes it and how decisions are made around it — all start coming apart at the seams.The old frameworkFor decades, every growing company asked the same question: Should we build this ourselves, or should we buy it?And, for decades, the answer was pretty straightforward: Build if it&#x27;s core to your business; buy if it isn’t.The logic made sense, because building was expensive and meant borrowing time from overworked engineers, writing specs, planning sprints, managing infrastructure and bracing yourself for a long tail of maintenance. Buying was faster. Safer. You paid for the support and the peace of mind.But something fundamental has changed: AI has made building accessible to everyone. What used to take weeks now takes hours, and what used to require fluency in a programming language now requires fluency in plain English.When the cost and complexity of building collapse this dramatically, the old framework goes down with them. It’s not build versus buy anymore. It’s something stranger that we haven&#x27;t quite found the right words for.When the market doesn’t know what you need (yet)My company never planned to build so many of the tools we use. We just had to build because the things we needed didn’t exist. And, through that process, we developed this visceral understanding of what we actually wanted, what was useful and what it could or couldn&#x27;t do. Not what vendor decks told us we needed or what analyst reports said we should want, but what actually moved the needle in our business.We figured out which problems were worth solving, which ones weren’t, where AI created real leverage and where it was just noise. And only then, once we had that hard-earned clarity, did we start buying.By that point, we knew exactly what we were looking for and could tell the difference between substance and marketing in about five minutes. We asked questions that made vendors nervous because we&#x27;d already built some rudimentary version of what they were selling.When anyone can build in minutesLast week, someone on our CX team noticed some customer feedback about a bug in Slack. Just a minor customer complaint, nothing major. In another company, this would’ve kicked off a support ticket and they’d have waited for someone else to handle it, but that’s not what happened here. They opened Cursor, described the change and let AI write the fix. Then they submitted a pull request that engineering reviewed and merged.Just 15 minutes after that complaint popped up in Slack, the fix was live in production.The person who did this isn’t technical in the slightest. I doubt they could tell you the difference between Python and JavaScript, but they solved the problem anyway. And that’s the point.AI has gotten so good at cranking out relatively simple code that it handles 80% of the problems that used to require a sprint planning meeting and two weeks of engineering time. It’s erasing the boundary between technical and non-technical. Work that used to be bottlenecked by engineering is now being done by the people closest to the problem.This is happening right now in companies that are actually paying attention.The inversion that’s happeningHere&#x27;s where it gets fascinating for finance leaders, because AI has actually flipped the entire strategic logic of the build versus buy decision on its head.The old model went something like:Define the need.Decide whether to build or buy.But defining the need took forever and required deep technical expertise, or you&#x27;d burn through money through trial-and-error vendor implementations. You&#x27;d sit through countless demos, trying to picture whether this actually solved your problem. Then you’d negotiate, implement, move all your data and workflows to the new tool and six months and six figures later discover whether (or not) you were actually right.Now, the whole sequence gets turned around:Build something lightweight with AI.Use it to understand what you actually need.Then decide whether to buy (and you&#x27;ll know exactly why).This approach lets you run controlled experiments. You figure out whether the problem even matters. You discover which features deliver value and which just look good in demos. Then you go shopping. Instead of letting some external vendor sell you on what the need is, you get to figure out whether you even have that need in the first place.Think about how many software purchases you&#x27;ve made that, in hindsight, solved problems you didn&#x27;t actually have. How many times have you been three months into an implementation and thought, “Hang on, is this actually helping us, or are we just trying to justify what we spent?”Now, when you do buy, the question becomes “Does this solve the problem better than what we already proved we can build?”That one reframe changes the entire conversation. Now you show up to vendor calls informed. You ask sharper questions, and negotiate from a place of strength. Most importantly, you avoid the most expensive mistake in enterprise software, which is solving a problem you never really had.The trap you need to avoidAs this new capability emerges, I’m watching companies sprint in the wrong direction. They know they need to be AI native, so they go on a shopping spree. They look for AI-powered tools, filling their stack with products that have GPT integrations, chatbot UIs or “AI” slapped onto the marketing site. They think they’re transforming, but they’re not.Remember what physicist Richard Feynman called cargo cult science? After World War II, islanders in the South Pacific built fake airstrips and control towers, mimicking what they&#x27;d seen during the war, hoping planes full of cargo would return. They had all the outward forms of an airport: Towers, headsets, even people miming flight controllers. But no planes landed, because the form wasn’t the function.That’s exactly what’s happening with AI transformation in boardrooms everywhere. Leaders are buying AI tools without asking if they meaningfully change how work gets done, who they empower or what processes they unlock.They’ve built the airstrip, but the planes aren’t showing up.And the whole market&#x27;s basically set up to make you fall into this trap. Everything gets branded as AI now, but nobody seems to care what these products actually do. Every SaaS product has bolted on a chatbot or an auto-complete feature and slapped an AI label on it, and the label has lost all meaning. It’s just a checkbox vendors figure they need to tick, regardless of whether it creates actual value for customers.The finance team’s new superpowerThis is the part that gets me excited about what finance teams can do now. You don’t have to guess anymore. You don’t have to bet six figures on a sales deck. You can test things, and you can actually learn something before you spend.Here&#x27;s what I mean: If you’re evaluating vendor management software, prototype the core workflow with AI tools. Figure out whether you’re solving a tooling problem or a process problem. Figure out whether you need software at all.This doesn’t mean you’ll build everything internally — of course not. Most of the time, you’ll still end up buying, and that&#x27;s totally fine, because enterprise tools exist for good reasons (scale, support, security, and maintenance). But now you’ll buy with your eyes wide open.You’ll know what “good” looks like. You’ll show up to demos already understanding the edge cases, and know in about 5 minutes whether they actually get your specific problem. You’ll implement faster. You&#x27;ll negotiate better because you&#x27;re not completely dependent on the vendor&#x27;s solution. And you’ll choose it because it&#x27;s genuinely better than what you could build yourself.You&#x27;ll have already mapped out the shape of what you need, and you&#x27;ll just be looking for the best version of it.The new paradigmFor years, the mantra was: Build or buy.Now, it’s more elegant and way smarter: Build to learn what to buy.And it&#x27;s not some future state. This is already happening. Right now, somewhere, a customer rep is using AI to fix a product issue they spotted minutes ago. Somewhere else, a finance team is prototyping their own analytical tools because they&#x27;ve realized they can iterate faster than they can write up requirements for engineering. Somewhere, a team is realizing that the boundary between technical and non-technical was always more cultural than fundamental.The companies that embrace this shift will move faster and spend smarter. They’ll know their operations more deeply than any vendor ever could. They&#x27;ll make fewer expensive mistakes, and buy better tools because they actually understand what makes tools good.The companies that stick to the old playbook will keep sitting through vendor pitches, nodding along at budget-friendly proposals. They’ll debate timelines, and keep mistaking professional decks for actual solutions.Until someone on their own team pops open their laptop, says, “I built a version of this last night. Want to check it out?,” and shows them something they built in two hours that does 80% of what they’re about to pay six figures for.And, just like that, the rules change for good.Siqi Chen is co-founder and CEO of Runway. Read more from our guest writers. Or, consider submitting a post of your own! See our guidelines here.",
          "content": "Picture this: You&#x27;re sitting in a conference room, halfway through a vendor pitch. The demo looks solid, and pricing fits nicely under budget. The timeline seems reasonable too. Everyone’s nodding along.You’re literally minutes away from saying yes.Then someone from your finance team walks in. They see the deck and frown. A few minutes later, they shoot you a message on Slack: “Actually, I threw together a version of this last week. Took me 2 hours in Cursor. Wanna take a look?”Wait… what?This person doesn&#x27;t code. You know for a fact they&#x27;ve never written a line of JavaScript in their entire life. But here they are, showing you a working prototype on their laptop that does... pretty much exactly what the vendor pitched. Sure, it&#x27;s got some rough edges, but it works. And it didn’t cost six figures. Just two hours of their time.Suddenly, the assumptions you walked in with — about how software is developed, who makes it and how decisions are made around it — all start coming apart at the seams.The old frameworkFor decades, every growing company asked the same question: Should we build this ourselves, or should we buy it?And, for decades, the answer was pretty straightforward: Build if it&#x27;s core to your business; buy if it isn’t.The logic made sense, because building was expensive and meant borrowing time from overworked engineers, writing specs, planning sprints, managing infrastructure and bracing yourself for a long tail of maintenance. Buying was faster. Safer. You paid for the support and the peace of mind.But something fundamental has changed: AI has made building accessible to everyone. What used to take weeks now takes hours, and what used to require fluency in a programming language now requires fluency in plain English.When the cost and complexity of building collapse this dramatically, the old framework goes down with them. It’s not build versus buy anymore. It’s something stranger that we haven&#x27;t quite found the right words for.When the market doesn’t know what you need (yet)My company never planned to build so many of the tools we use. We just had to build because the things we needed didn’t exist. And, through that process, we developed this visceral understanding of what we actually wanted, what was useful and what it could or couldn&#x27;t do. Not what vendor decks told us we needed or what analyst reports said we should want, but what actually moved the needle in our business.We figured out which problems were worth solving, which ones weren’t, where AI created real leverage and where it was just noise. And only then, once we had that hard-earned clarity, did we start buying.By that point, we knew exactly what we were looking for and could tell the difference between substance and marketing in about five minutes. We asked questions that made vendors nervous because we&#x27;d already built some rudimentary version of what they were selling.When anyone can build in minutesLast week, someone on our CX team noticed some customer feedback about a bug in Slack. Just a minor customer complaint, nothing major. In another company, this would’ve kicked off a support ticket and they’d have waited for someone else to handle it, but that’s not what happened here. They opened Cursor, described the change and let AI write the fix. Then they submitted a pull request that engineering reviewed and merged.Just 15 minutes after that complaint popped up in Slack, the fix was live in production.The person who did this isn’t technical in the slightest. I doubt they could tell you the difference between Python and JavaScript, but they solved the problem anyway. And that’s the point.AI has gotten so good at cranking out relatively simple code that it handles 80% of the problems that used to require a sprint planning meeting and two weeks of engineering time. It’s erasing the boundary between technical and non-technical. Work that used to be bottlenecked by engineering is now being done by the people closest to the problem.This is happening right now in companies that are actually paying attention.The inversion that’s happeningHere&#x27;s where it gets fascinating for finance leaders, because AI has actually flipped the entire strategic logic of the build versus buy decision on its head.The old model went something like:Define the need.Decide whether to build or buy.But defining the need took forever and required deep technical expertise, or you&#x27;d burn through money through trial-and-error vendor implementations. You&#x27;d sit through countless demos, trying to picture whether this actually solved your problem. Then you’d negotiate, implement, move all your data and workflows to the new tool and six months and six figures later discover whether (or not) you were actually right.Now, the whole sequence gets turned around:Build something lightweight with AI.Use it to understand what you actually need.Then decide whether to buy (and you&#x27;ll know exactly why).This approach lets you run controlled experiments. You figure out whether the problem even matters. You discover which features deliver value and which just look good in demos. Then you go shopping. Instead of letting some external vendor sell you on what the need is, you get to figure out whether you even have that need in the first place.Think about how many software purchases you&#x27;ve made that, in hindsight, solved problems you didn&#x27;t actually have. How many times have you been three months into an implementation and thought, “Hang on, is this actually helping us, or are we just trying to justify what we spent?”Now, when you do buy, the question becomes “Does this solve the problem better than what we already proved we can build?”That one reframe changes the entire conversation. Now you show up to vendor calls informed. You ask sharper questions, and negotiate from a place of strength. Most importantly, you avoid the most expensive mistake in enterprise software, which is solving a problem you never really had.The trap you need to avoidAs this new capability emerges, I’m watching companies sprint in the wrong direction. They know they need to be AI native, so they go on a shopping spree. They look for AI-powered tools, filling their stack with products that have GPT integrations, chatbot UIs or “AI” slapped onto the marketing site. They think they’re transforming, but they’re not.Remember what physicist Richard Feynman called cargo cult science? After World War II, islanders in the South Pacific built fake airstrips and control towers, mimicking what they&#x27;d seen during the war, hoping planes full of cargo would return. They had all the outward forms of an airport: Towers, headsets, even people miming flight controllers. But no planes landed, because the form wasn’t the function.That’s exactly what’s happening with AI transformation in boardrooms everywhere. Leaders are buying AI tools without asking if they meaningfully change how work gets done, who they empower or what processes they unlock.They’ve built the airstrip, but the planes aren’t showing up.And the whole market&#x27;s basically set up to make you fall into this trap. Everything gets branded as AI now, but nobody seems to care what these products actually do. Every SaaS product has bolted on a chatbot or an auto-complete feature and slapped an AI label on it, and the label has lost all meaning. It’s just a checkbox vendors figure they need to tick, regardless of whether it creates actual value for customers.The finance team’s new superpowerThis is the part that gets me excited about what finance teams can do now. You don’t have to guess anymore. You don’t have to bet six figures on a sales deck. You can test things, and you can actually learn something before you spend.Here&#x27;s what I mean: If you’re evaluating vendor management software, prototype the core workflow with AI tools. Figure out whether you’re solving a tooling problem or a process problem. Figure out whether you need software at all.This doesn’t mean you’ll build everything internally — of course not. Most of the time, you’ll still end up buying, and that&#x27;s totally fine, because enterprise tools exist for good reasons (scale, support, security, and maintenance). But now you’ll buy with your eyes wide open.You’ll know what “good” looks like. You’ll show up to demos already understanding the edge cases, and know in about 5 minutes whether they actually get your specific problem. You’ll implement faster. You&#x27;ll negotiate better because you&#x27;re not completely dependent on the vendor&#x27;s solution. And you’ll choose it because it&#x27;s genuinely better than what you could build yourself.You&#x27;ll have already mapped out the shape of what you need, and you&#x27;ll just be looking for the best version of it.The new paradigmFor years, the mantra was: Build or buy.Now, it’s more elegant and way smarter: Build to learn what to buy.And it&#x27;s not some future state. This is already happening. Right now, somewhere, a customer rep is using AI to fix a product issue they spotted minutes ago. Somewhere else, a finance team is prototyping their own analytical tools because they&#x27;ve realized they can iterate faster than they can write up requirements for engineering. Somewhere, a team is realizing that the boundary between technical and non-technical was always more cultural than fundamental.The companies that embrace this shift will move faster and spend smarter. They’ll know their operations more deeply than any vendor ever could. They&#x27;ll make fewer expensive mistakes, and buy better tools because they actually understand what makes tools good.The companies that stick to the old playbook will keep sitting through vendor pitches, nodding along at budget-friendly proposals. They’ll debate timelines, and keep mistaking professional decks for actual solutions.Until someone on their own team pops open their laptop, says, “I built a version of this last night. Want to check it out?,” and shows them something they built in two hours that does 80% of what they’re about to pay six figures for.And, just like that, the rules change for good.Siqi Chen is co-founder and CEO of Runway. Read more from our guest writers. Or, consider submitting a post of your own! See our guidelines here.",
          "feed_position": 3,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/6kXpR2jU6eoKdeTMAngNL8/96b17bfd5d5ce4ea7285636538c2d054/DDM_build_versus_buy.png?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/why-most-enterprise-ai-coding-pilots-underperform-hint-its-not-the-model",
          "published_at": "Sat, 13 Dec 2025 20:00:00 GMT",
          "title": "Why most enterprise AI coding pilots underperform (Hint: It's not the model)",
          "standfirst": "Dhyey Mavani is accelerating generative AI at LinkedIn and is a guest author at VentureBeat.Gen AI in software engineering has moved well beyond autocomplete. The emerging frontier is agentic coding: AI systems capable of planning changes, executing them across multiple steps and iterating based on feedback. Yet despite the excitement around “AI agents that code,” most enterprise deployments underperform. The limiting factor is no longer the model. It’s context: The structure, history and intent surrounding the code being changed. In other words, enterprises are now facing a systems design problem: They have not yet engineered the environment these agents operate in.The shift from assistance to agencyThe past year has seen a rapid evolution from assistive coding tools to agentic workflows. Research has begun to formalize what agentic behavior means in practice: The ability to reason across design, testing, execution and validation rather than generate isolated snippets. Work such as dynamic action re-sampling shows that allowing agents to branch, reconsider and revise their own decisions significantly improves outcomes in large, interdependent codebases. At the platform level, providers like GitHub are now building dedicated agent orchestration environments, such as Copilot Agent and Agent HQ, to support multi-agent collaboration inside real enterprise pipelines.But early field results tell a cautionary story. When organizations introduce agentic tools without addressing workflow and environment, productivity can decline. A randomized control study this year showed that developers who used AI assistance in unchanged workflows completed tasks more slowly, largely due to verification, rework and confusion around intent. The lesson is straightforward: Autonomy without orchestration rarely yields efficiency.Why context engineering is the real unlockIn every unsuccessful deployment I’ve observed, the failure stemmed from context. When agents lack a structured understanding of a codebase, specifically its relevant modules, dependency graph, test harness, architectural conventions and change history. They often generate output that appears correct but is disconnected from reality. Too much information overwhelms the agent; too little forces it to guess. The goal is not to feed the model more tokens. The goal is to determine what should be visible to the agent, when and in what form.The teams seeing meaningful gains treat context as an engineering surface. They create tooling to snapshot, compact and version the agent’s working memory: What is persisted across turns, what is discarded, what is summarized and what is linked instead of inlined. They design deliberation steps rather than prompting sessions. They make the specification a first-class artifact, something reviewable, testable and owned, not a transient chat history. This shift aligns with a broader trend some researchers describe as “specs becoming the new source of truth.”Workflow must change alongside toolingBut context alone isn’t enough. Enterprises must re-architect the workflows around these agents. As McKinsey’s 2025 report “One Year of Agentic AI” noted, productivity gains arise not from layering AI onto existing processes but from rethinking the process itself. When teams simply drop an agent into an unaltered workflow, they invite friction: Engineers spend more time verifying AI-written code than they would have spent writing it themselves. The agents can only amplify what’s already structured: Well-tested, modular codebases with clear ownership and documentation. Without those foundations, autonomy becomes chaos.Security and governance, too, demand a shift in mindset. AI-generated code introduces new forms of risk: Unvetted dependencies, subtle license violations and undocumented modules that escape peer review. Mature teams are beginning to integrate agentic activity directly into their CI/CD pipelines, treating agents as autonomous contributors whose work must pass the same static analysis, audit logging and approval gates as any human developer. GitHub’s own documentation highlights this trajectory, positioning Copilot Agents not as replacements for engineers but as orchestrated participants in secure, reviewable workflows. The goal isn’t to let an AI “write everything,” but to ensure that when it acts, it does so inside defined guardrails.What enterprise decision-makers should focus on nowFor technical leaders, the path forward starts with readiness rather than hype. Monoliths with sparse tests rarely yield net gains; agents thrive where tests are authoritative and can drive iterative refinement. This is exactly the loop Anthropic calls out for coding agents. Pilots in tightly scoped domains (test generation, legacy modernization, isolated refactors); treat each deployment as an experiment with explicit metrics (defect escape rate, PR cycle time, change failure rate, security findings burned down). As your usage grows, treat agents as data infrastructure: Every plan, context snapshot, action log and test run is data that composes into a searchable memory of engineering intent, and a durable competitive advantage.Under the hood, agentic coding is less a tooling problem than a data problem. Every context snapshot, test iteration and code revision becomes a form of structured data that must be stored, indexed and reused. As these agents proliferate, enterprises will find themselves managing an entirely new data layer: One that captures not just what was built, but how it was reasoned about. This shift turns engineering logs into a knowledge graph of intent, decision-making and validation. In time, the organizations that can search and replay this contextual memory will outpace those who still treat code as static text.The coming year will likely determine whether agentic coding becomes a cornerstone of enterprise development or another inflated promise. The difference will hinge on context engineering: How intelligently teams design the informational substrate their agents rely on. The winners will be those who see autonomy not as magic, but as an extension of disciplined systems design:Clear workflows, measurable feedback, and rigorous governance.Bottom linePlatforms are converging on orchestration and guardrails, and research keeps improving context control at inference time. The winners over the next 12 to 24 months won’t be the teams with the flashiest model; they’ll be the ones that engineer context as an asset and treat workflow as the product. Do that, and autonomy compounds. Skip it, and the review queue does.Context + agent = leverage. Skip the first half, and the rest collapses.Dhyey Mavani is accelerating generative AI at LinkedIn.Read more from our guest writers. Or, consider submitting a post of your own! See our guidelines here.",
          "content": "Dhyey Mavani is accelerating generative AI at LinkedIn and is a guest author at VentureBeat.Gen AI in software engineering has moved well beyond autocomplete. The emerging frontier is agentic coding: AI systems capable of planning changes, executing them across multiple steps and iterating based on feedback. Yet despite the excitement around “AI agents that code,” most enterprise deployments underperform. The limiting factor is no longer the model. It’s context: The structure, history and intent surrounding the code being changed. In other words, enterprises are now facing a systems design problem: They have not yet engineered the environment these agents operate in.The shift from assistance to agencyThe past year has seen a rapid evolution from assistive coding tools to agentic workflows. Research has begun to formalize what agentic behavior means in practice: The ability to reason across design, testing, execution and validation rather than generate isolated snippets. Work such as dynamic action re-sampling shows that allowing agents to branch, reconsider and revise their own decisions significantly improves outcomes in large, interdependent codebases. At the platform level, providers like GitHub are now building dedicated agent orchestration environments, such as Copilot Agent and Agent HQ, to support multi-agent collaboration inside real enterprise pipelines.But early field results tell a cautionary story. When organizations introduce agentic tools without addressing workflow and environment, productivity can decline. A randomized control study this year showed that developers who used AI assistance in unchanged workflows completed tasks more slowly, largely due to verification, rework and confusion around intent. The lesson is straightforward: Autonomy without orchestration rarely yields efficiency.Why context engineering is the real unlockIn every unsuccessful deployment I’ve observed, the failure stemmed from context. When agents lack a structured understanding of a codebase, specifically its relevant modules, dependency graph, test harness, architectural conventions and change history. They often generate output that appears correct but is disconnected from reality. Too much information overwhelms the agent; too little forces it to guess. The goal is not to feed the model more tokens. The goal is to determine what should be visible to the agent, when and in what form.The teams seeing meaningful gains treat context as an engineering surface. They create tooling to snapshot, compact and version the agent’s working memory: What is persisted across turns, what is discarded, what is summarized and what is linked instead of inlined. They design deliberation steps rather than prompting sessions. They make the specification a first-class artifact, something reviewable, testable and owned, not a transient chat history. This shift aligns with a broader trend some researchers describe as “specs becoming the new source of truth.”Workflow must change alongside toolingBut context alone isn’t enough. Enterprises must re-architect the workflows around these agents. As McKinsey’s 2025 report “One Year of Agentic AI” noted, productivity gains arise not from layering AI onto existing processes but from rethinking the process itself. When teams simply drop an agent into an unaltered workflow, they invite friction: Engineers spend more time verifying AI-written code than they would have spent writing it themselves. The agents can only amplify what’s already structured: Well-tested, modular codebases with clear ownership and documentation. Without those foundations, autonomy becomes chaos.Security and governance, too, demand a shift in mindset. AI-generated code introduces new forms of risk: Unvetted dependencies, subtle license violations and undocumented modules that escape peer review. Mature teams are beginning to integrate agentic activity directly into their CI/CD pipelines, treating agents as autonomous contributors whose work must pass the same static analysis, audit logging and approval gates as any human developer. GitHub’s own documentation highlights this trajectory, positioning Copilot Agents not as replacements for engineers but as orchestrated participants in secure, reviewable workflows. The goal isn’t to let an AI “write everything,” but to ensure that when it acts, it does so inside defined guardrails.What enterprise decision-makers should focus on nowFor technical leaders, the path forward starts with readiness rather than hype. Monoliths with sparse tests rarely yield net gains; agents thrive where tests are authoritative and can drive iterative refinement. This is exactly the loop Anthropic calls out for coding agents. Pilots in tightly scoped domains (test generation, legacy modernization, isolated refactors); treat each deployment as an experiment with explicit metrics (defect escape rate, PR cycle time, change failure rate, security findings burned down). As your usage grows, treat agents as data infrastructure: Every plan, context snapshot, action log and test run is data that composes into a searchable memory of engineering intent, and a durable competitive advantage.Under the hood, agentic coding is less a tooling problem than a data problem. Every context snapshot, test iteration and code revision becomes a form of structured data that must be stored, indexed and reused. As these agents proliferate, enterprises will find themselves managing an entirely new data layer: One that captures not just what was built, but how it was reasoned about. This shift turns engineering logs into a knowledge graph of intent, decision-making and validation. In time, the organizations that can search and replay this contextual memory will outpace those who still treat code as static text.The coming year will likely determine whether agentic coding becomes a cornerstone of enterprise development or another inflated promise. The difference will hinge on context engineering: How intelligently teams design the informational substrate their agents rely on. The winners will be those who see autonomy not as magic, but as an extension of disciplined systems design:Clear workflows, measurable feedback, and rigorous governance.Bottom linePlatforms are converging on orchestration and guardrails, and research keeps improving context control at inference time. The winners over the next 12 to 24 months won’t be the teams with the flashiest model; they’ll be the ones that engineer context as an asset and treat workflow as the product. Do that, and autonomy compounds. Skip it, and the review queue does.Context + agent = leverage. Skip the first half, and the rest collapses.Dhyey Mavani is accelerating generative AI at LinkedIn.Read more from our guest writers. Or, consider submitting a post of your own! See our guidelines here.",
          "feed_position": 4,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/38oJ3yjcAb67Hunz4whpDs/5e419481c52a1deafddb26fc2af21f57/DDM_Coding.png?w=300&q=30"
        }
      ],
      "featured_image": "https://d29szjachogqwa.cloudfront.net/images/user-uploaded/Galaxy-XR-lead.jpg",
      "popularity_score": 2017.1404386111112
    },
    {
      "id": "cluster_34",
      "coverage": 2,
      "updated_at": "Mon, 15 Dec 2025 16:28:06 +0000",
      "title": "Netflix responds to concerns about WBD deal",
      "neutral_headline": "Netflix responds to concerns about WBD deal",
      "items": [
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2025/12/15/netflix-responds-to-concerns-about-wbd-deal/",
          "published_at": "Mon, 15 Dec 2025 16:28:06 +0000",
          "title": "Netflix responds to concerns about WBD deal",
          "standfirst": "Industry concerns over Netflix acquiring Warner Bros has led executives to address fears regarding jobs and theatrical releases.",
          "content": "Industry concerns over Netflix acquiring Warner Bros has led executives to address fears regarding jobs and theatrical releases.",
          "feed_position": 5
        },
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/251215/p17#a251215p17",
          "published_at": "Mon, 15 Dec 2025 08:00:53 -0500",
          "title": "Memo: Greg Peters and Ted Sarandos lay out Netflix's WBD offer, moving to quell concerns about job losses and the end of theater releases, after Paramount's bid (Rose Henderson/Bloomberg)",
          "standfirst": "Rose Henderson / Bloomberg: Memo: Greg Peters and Ted Sarandos lay out Netflix's WBD offer, moving to quell concerns about job losses and the end of theater releases, after Paramount's bid &mdash; The two chief executive officers of Netflix Inc. laid out the company's case for acquiring Warner Bros Discovery Inc. &hellip;",
          "content": "Rose Henderson / Bloomberg: Memo: Greg Peters and Ted Sarandos lay out Netflix's WBD offer, moving to quell concerns about job losses and the end of theater releases, after Paramount's bid &mdash; The two chief executive officers of Netflix Inc. laid out the company's case for acquiring Warner Bros Discovery Inc. &hellip;",
          "feed_position": 14,
          "image_url": "http://www.techmeme.com/251215/i17.jpg"
        }
      ],
      "featured_image": "http://www.techmeme.com/251215/i17.jpg",
      "popularity_score": 2017.1087719444445
    },
    {
      "id": "cluster_15",
      "coverage": 1,
      "updated_at": "Mon, 15 Dec 2025 18:13:24 +0000",
      "title": "Google will end dark web reports that alerted users to leaked data",
      "neutral_headline": "Google will end dark web reports that alerted users to leaked data",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2025/12/google-is-shutting-down-dark-web-reports-in-january-because-they-werent-helpful/",
          "published_at": "Mon, 15 Dec 2025 18:13:24 +0000",
          "title": "Google will end dark web reports that alerted users to leaked data",
          "standfirst": "Google says the reports lacked \"helpful next steps.\"",
          "content": "Google began offering “dark web reports” a while back, but the company has just announced the feature will be going away very soon. In an email to users of the service, Google says it will stop telling you about dark web data leaks in February. This probably won’t negatively impact your security or privacy because, as Google points out in its latest email, there’s really nothing you can do about the dark web. The dark web reports launched in March 2023 as a perk for Google One subscribers. The reports were expanded to general access in 2024. Now, barely a year later, Google has decided it doesn’t see the value in this type of alert for users. Dark web reports provide a list of partially redacted user data retrieved from shadowy forums and sites where such information is bought and sold. However, that’s all it is—a list. The dark web consists of so-called hidden services hosted inside the Tor network. You need a special browser or connection tools in order to access Tor hidden services, and its largely anonymous nature has made it a favorite hangout for online criminals. If a company with your personal data has been hacked, that data probably lives somewhere on the dark web.Read full article Comments",
          "feed_position": 0,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/08/google-logo-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/08/google-logo-1152x648.jpg",
      "popularity_score": 363.86377194444447
    },
    {
      "id": "cluster_30",
      "coverage": 1,
      "updated_at": "Mon, 15 Dec 2025 16:44:01 +0000",
      "title": "Oh look, yet another Starship clone has popped up in China",
      "neutral_headline": "Oh look, yet another Starship clone has popped up in China",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2025/12/oh-look-yet-another-starship-clone-has-popped-up-in-china/",
          "published_at": "Mon, 15 Dec 2025 16:44:01 +0000",
          "title": "Oh look, yet another Starship clone has popped up in China",
          "standfirst": "Chinese companies are no longer hiding their intent to clone SpaceX. They're advertising it.",
          "content": "Every other week, it seems, a new Chinese launch company pops up with a rocket design and a plan to reach orbit within a few years. For a long time, the majority of these companies revealed designs that looked a lot like SpaceX’s Falcon 9 rocket. The first of these copy cats, the medium-lift Zhuque-3 rocket built by LandSpace, launched earlier this month. Its primary mission was nominal, but the Zhuque-3 rocket failed its landing attempt, which is understandable for a first flight. Doubtless there will be more Chinese Falcon 9-like rockets making their debut in the near future. However, over the last year, there has been a distinct change in announcements from China when it comes to new launch technology. Just as SpaceX is seeking to transition from its workhorse Falcon 9 rocket—which has now been flying for a decade and a half—to the fully reusable Starship design, so too are Chinese companies modifying their visions.Read full article Comments",
          "feed_position": 1,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/china-rocket.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/china-rocket.jpg",
      "popularity_score": 340.3740497222222
    },
    {
      "id": "cluster_64",
      "coverage": 1,
      "updated_at": "Mon, 15 Dec 2025 12:30:44 +0000",
      "title": "Verizon refused to unlock man’s iPhone, so he sued the carrier and won",
      "neutral_headline": "Verizon refused to unlock man’s iPhone, so he sued the carrier and won",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/12/verizon-refused-to-unlock-mans-iphone-so-he-sued-the-carrier-and-won/",
          "published_at": "Mon, 15 Dec 2025 12:30:44 +0000",
          "title": "Verizon refused to unlock man’s iPhone, so he sued the carrier and won",
          "standfirst": "Verizon changed policy after he bought the phone, wouldn't unlock it despite FCC rule.",
          "content": "When Verizon refused to unlock an iPhone purchased by Kansas resident Patrick Roach, he had no intention of giving up without a fight. Roach sued the wireless carrier in small claims court and won. Roach bought a discounted iPhone 16e from Verizon’s Straight Talk brand on February 28, 2025, as a gift for his wife’s birthday. He intended to pay for one month of service, cancel, and then switch the phone to the US Mobile service plan that the couple uses. Under federal rules that apply to Verizon and a Verizon unlocking policy that was in place when Roach bought the phone, this strategy should have worked. “The best deals tend to be buying it from one of these MVNOs [Mobile Virtual Network Operators] and then activating it until it unlocks and then switching it to whatever you are planning to use it with. It usually saves you about half the value of the phone,” Roach said in a phone interview.Read full article Comments",
          "feed_position": 3,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/verizon-jerks-locked-phone-1152x648-1765486982.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/verizon-jerks-locked-phone-1152x648-1765486982.jpg",
      "popularity_score": 328.15266083333336
    },
    {
      "id": "cluster_4",
      "coverage": 1,
      "updated_at": "2025-12-15T19:21:33.427Z",
      "title": "Sharks and rays gain landmark protections as nations move to curb international trade",
      "neutral_headline": "Sharks and rays gain landmark protections as nations move to curb international trade",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2025/12/sharks-and-rays-gain-landmark-protections-as-nations-move-to-curb-international-trade/",
          "published_at": "2025-12-15T19:21:33.427Z",
          "title": "Sharks and rays gain landmark protections as nations move to curb international trade",
          "standfirst": "",
          "content": "",
          "feed_position": 4
        }
      ],
      "popularity_score": 327.99972388888887
    },
    {
      "id": "cluster_45",
      "coverage": 1,
      "updated_at": "Mon, 15 Dec 2025 15:24:41 +0000",
      "title": "Roomba maker iRobot swept into bankruptcy",
      "neutral_headline": "Roomba maker iRobot swept into bankruptcy",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/information-technology/2025/12/roomba-maker-irobot-swept-into-bankruptcy/",
          "published_at": "Mon, 15 Dec 2025 15:24:41 +0000",
          "title": "Roomba maker iRobot swept into bankruptcy",
          "standfirst": "Shenzhen-based Picea Robotics, its lender and primary supplier, will acquire all of iRobot’s shares.",
          "content": "Roomba maker iRobot has filed for bankruptcy and will be taken over by its Chinese supplier after the company that popularized the robot vacuum cleaner fell under the weight of competition from cheaper rivals. The US-listed group on Sunday said it had filed for Chapter 11 bankruptcy in Delaware as part of a restructuring agreement with Shenzhen-based Picea Robotics, its lender and primary supplier, which will acquire all of iRobot’s shares. The deal comes nearly two years after a proposed $1.5 billion acquisition by Amazon fell through over competition concerns from EU regulators.Read full article Comments",
          "feed_position": 2,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-633690806-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-633690806-1152x648.jpg",
      "popularity_score": 326.0518275
    },
    {
      "id": "cluster_5",
      "coverage": 1,
      "updated_at": "2025-12-15T19:21:33.427Z",
      "title": "OpenAI built an AI coding agent and uses it to improve the agent itself",
      "neutral_headline": "OpenAI built an AI coding agent and uses it to improve the agent itself",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/ai/2025/12/how-openai-is-using-gpt-5-codex-to-improve-the-ai-tool-itself/",
          "published_at": "2025-12-15T19:21:33.427Z",
          "title": "OpenAI built an AI coding agent and uses it to improve the agent itself",
          "standfirst": "",
          "content": "",
          "feed_position": 5
        }
      ],
      "popularity_score": 317.99972388888887
    },
    {
      "id": "cluster_7",
      "coverage": 1,
      "updated_at": "2025-12-15T19:21:33.427Z",
      "title": "Google Translate expands live translation to all earbuds on Android",
      "neutral_headline": "Google Translate expands live translation to all earbuds on Android",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/google/2025/12/google-translate-learns-slang-and-idioms-expands-live-translation-beyond-pixel-buds/",
          "published_at": "2025-12-15T19:21:33.427Z",
          "title": "Google Translate expands live translation to all earbuds on Android",
          "standfirst": "",
          "content": "",
          "feed_position": 7
        }
      ],
      "popularity_score": 294.99972388888887
    },
    {
      "id": "cluster_6",
      "coverage": 1,
      "updated_at": "2025-12-15T19:21:33.427Z",
      "title": "Reminder: Donate to win swag in our annual Charity Drive sweepstakes",
      "neutral_headline": "Reminder: Donate to win swag in our annual Charity Drive sweepstakes",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gaming/2025/12/reminder-donate-to-win-swag-in-our-annual-charity-drive-sweepstakes-17/",
          "published_at": "2025-12-15T19:21:33.427Z",
          "title": "Reminder: Donate to win swag in our annual Charity Drive sweepstakes",
          "standfirst": "",
          "content": "",
          "feed_position": 6
        }
      ],
      "popularity_score": 292.99972388888887
    },
    {
      "id": "cluster_8",
      "coverage": 1,
      "updated_at": "2025-12-15T19:21:33.427Z",
      "title": "Ukrainians sue US chip firms for powering Russian drones, missiles",
      "neutral_headline": "Ukrainians sue US chip firms for powering Russian drones, missiles",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/12/ukrainians-sue-us-chip-firms-for-powering-russian-drones-missiles/",
          "published_at": "2025-12-15T19:21:33.427Z",
          "title": "Ukrainians sue US chip firms for powering Russian drones, missiles",
          "standfirst": "",
          "content": "",
          "feed_position": 8
        }
      ],
      "popularity_score": 287.99972388888887
    },
    {
      "id": "cluster_9",
      "coverage": 1,
      "updated_at": "2025-12-15T19:21:33.427Z",
      "title": "Scientists built an AI co-pilot for prosthetic bionic hands",
      "neutral_headline": "Scientists built an AI co-pilot for prosthetic bionic hands",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/ai/2025/12/scientists-built-an-ai-co-pilot-for-prosthetic-bionic-hands/",
          "published_at": "2025-12-15T19:21:33.427Z",
          "title": "Scientists built an AI co-pilot for prosthetic bionic hands",
          "standfirst": "",
          "content": "",
          "feed_position": 9
        }
      ],
      "popularity_score": 277.99972388888887
    }
  ]
}