{
  "updated_at": "2026-01-21T07:29:05.064Z",
  "clusters": [
    {
      "id": "cluster_13",
      "coverage": 2,
      "updated_at": "Tue, 20 Jan 2026 23:55:30 -0500",
      "title": "China's Cyberspace Administration requires firms to file their AI tools in a public algorithm registry, creating a detailed map of the country's AI ecosystem (Yi-Ling Liu/Wired)",
      "neutral_headline": "Thousands of Companies Are Driving China’s AI Boom. A...",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/260120/p53#a260120p53",
          "published_at": "Tue, 20 Jan 2026 23:55:30 -0500",
          "title": "China's Cyberspace Administration requires firms to file their AI tools in a public algorithm registry, creating a detailed map of the country's AI ecosystem (Yi-Ling Liu/Wired)",
          "standfirst": "Yi-Ling Liu / Wired: China's Cyberspace Administration requires firms to file their AI tools in a public algorithm registry, creating a detailed map of the country's AI ecosystem &mdash; How the Cyberspace Administration of China inadvertently made a guide to the country's homegrown AI revolution.",
          "content": "Yi-Ling Liu / Wired: China's Cyberspace Administration requires firms to file their AI tools in a public algorithm registry, creating a detailed map of the country's AI ecosystem &mdash; How the Cyberspace Administration of China inadvertently made a guide to the country's homegrown AI revolution.",
          "feed_position": 7,
          "image_url": "http://www.techmeme.com/260120/i53.jpg"
        },
        {
          "source": "Wired Tech",
          "url": "https://www.wired.com/story/china-ai-boom-algorithm-registry/",
          "published_at": "Tue, 20 Jan 2026 11:00:00 +0000",
          "title": "Thousands of Companies Are Driving China’s AI Boom. A Government Registry Tracks Them All",
          "standfirst": "How the Cyberspace Administration of China inadvertently made a guide to the country’s homegrown AI revolution.",
          "content": "How the Cyberspace Administration of China inadvertently made a guide to the country’s homegrown AI revolution.",
          "feed_position": 12,
          "image_url": "https://media.wired.com/photos/696686610ad2360ba658ec17/master/pass/China_AI_registry_main.jpg"
        }
      ],
      "featured_image": "http://www.techmeme.com/260120/i53.jpg",
      "popularity_score": 2017.44026
    },
    {
      "id": "cluster_44",
      "coverage": 2,
      "updated_at": "2026-01-20T17:52:59-05:00",
      "title": "Trump admin admits DOGE employees had access to off-limits Social Security data",
      "neutral_headline": "Trump admin admits DOGE employees had access to off-limits Social Security data",
      "items": [
        {
          "source": "The Verge",
          "url": "https://www.theverge.com/news/864643/doge-social-security-data-misuse",
          "published_at": "2026-01-20T17:52:59-05:00",
          "title": "Trump admin admits DOGE employees had access to off-limits Social Security data",
          "standfirst": "Department of Government Efficiency (DOGE) staffers working at the Social Security Administration (SSA) broke protocols, had more access to sensitive data on Americans than previously disclosed, and were in touch with a political advocacy group hunting for election fraud, the Trump administration admitted in a recent court filing. Justice Department officials told a federal court [&#8230;]",
          "content": "Department of Government Efficiency (DOGE) staffers working at the Social Security Administration (SSA) broke protocols, had more access to sensitive data on Americans than previously disclosed, and were in touch with a political advocacy group hunting for election fraud, the Trump administration admitted in a recent court filing. Justice Department officials told a federal court in Maryland that the SSA had not fully complied with the court's prior order, and had made statements to the court that it later found out were not entirely true. The admission came in a document, reported earlier by Politico, correcting the record in a case filed … Read the full story at The Verge.",
          "feed_position": 3
        },
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2026/01/20/trump-administration-admits-doge-may-have-misused-americans-social-security-data/",
          "published_at": "Tue, 20 Jan 2026 20:56:25 +0000",
          "title": "Trump administration admits DOGE may have misused Americans&#8217; Social Security data",
          "standfirst": "The revelation comes as part of a series of corrections in a legal case over DOGE’s access to Social Security Administration data.",
          "content": "The revelation comes as part of a series of corrections in a legal case over DOGE’s access to Social Security Administration data.",
          "feed_position": 11
        }
      ],
      "popularity_score": 2011.3983155555557
    },
    {
      "id": "cluster_60",
      "coverage": 2,
      "updated_at": "Tue, 20 Jan 2026 20:30:00 GMT",
      "title": "MIT’s new ‘recursive’ framework lets LLMs process 10 million tokens without context rot",
      "neutral_headline": "X open sources its algorithm: 5 ways businesses can benefit",
      "items": [
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/orchestration/mits-new-recursive-framework-lets-llms-process-10-million-tokens-without",
          "published_at": "Tue, 20 Jan 2026 20:30:00 GMT",
          "title": "MIT’s new ‘recursive’ framework lets LLMs process 10 million tokens without context rot",
          "standfirst": "Recursive language models (RLMs) are an inference technique developed by researchers at MIT CSAIL that treat long prompts as an external environment to the model. Instead of forcing the entire prompt into the model&#x27;s context window, the framework allows the LLM to programmatically examine, decompose, and recursively call itself over snippets of the text.Rather than expanding context windows or summarizing old information, the MIT team reframes long-context reasoning as a systems problem. By letting models treat prompts as something they can inspect with code, recursive language models allow LLMs to reason over millions of tokens without retraining. This offers enterprises a practical path to long-horizon tasks like codebase analysis, legal review, and multi-step reasoning that routinely break today’s models.Because the framework is designed as a wrapper around existing models, it can serve as a drop-in replacement for applications that make direct calls to LLMs.The LLM context problemWhile frontier models are becoming increasingly sophisticated at reasoning, their ability to process massive amounts of information is not scaling at the same rate. This bottleneck is driven by two distinct limitations: the hard physical constraint on how much text a model can process at once (context length) and \"context rot.\"The challenge, the researchers argue, is whether it’s possible to scale the effective context size of general-purpose LLMs by orders of magnitude without retraining them. This capability is becoming increasingly important for enterprise applications, where LLMs are adopted for long-horizon tasks requiring the processing of millions of tokens — a challenge Zhang argues can’t be solved by simply expanding context windows.\"There is an entropy argument that implies you need exponentially more data samples as you increase the effective context window size,\" Alex Zhang, a co-author of the paper, told VentureBeat. Current approaches to extending context often rely on compaction, where the model summarizes older parts of the conversation to free up space. However, this method fails for tasks requiring random access to specific details located in earlier parts of the prompt.How RLMs workThe concept behind RLMs is drawn from \"out-of-core\" algorithms used in classical computing. These algorithms are designed to process datasets too large to fit into a computer&#x27;s main memory by keeping the data on a hard drive and fetching only the necessary chunks as needed.RLMs apply this logic to generative AI. Instead of feeding a long prompt directly into the neural network, the framework loads the text as a string variable inside a Python coding environment. The LLM is given general context about the data (such as the total character count) but does not \"see\" the text initially.Once the prompt is stored as a variable, the LLM acts as a programmer. It writes Python code to interact with the external variable, using standard commands to peek into the data. For example, the model might use regular expressions to search for specific keywords like \"Chapter 1\" or \"financial results.\"When the code execution finds a relevant snippet, the RLM pulls only that specific chunk into its active context window for analysis.For example, if the prompt is a massive book, the LLM might write a loop that identifies chapter boundaries and then triggers a sub-call to summarize each chapter individually.The architecture typically involves two agents. A \"root language model,\" often a capability-heavy model like GPT-5, acts as the orchestrator. It plans the approach, writes the code, and manages the data flow within the REPL environment. A \"recursive language model,\" often a faster and cheaper model, acts as the worker. The root LM calls this worker to process the specific text snippets isolated by the code.Because the prompt resides in the environment&#x27;s memory rather than the model&#x27;s context window, the system can handle inputs far larger than the model&#x27;s training limit. Importantly, to the end-user, the RLM behaves exactly like a standard model: It accepts a string and returns an answer. This allows enterprise teams to swap standard API calls for RLMs. For developers looking to experiment, the RLM code is currently available on GitHub. \"A key argument for RLMs is that most complex tasks can be decomposed into smaller, &#x27;local&#x27; sub-tasks,\" Zhang said. \"However, how to perform this context/problem decomposition is non-trivial, and the model must be capable of performing this.\"RLMs in actionTo validate the framework, the researchers tested RLMs against base models and other agentic approaches like CodeAct and summary agents across a variety of long-context tasks, including retrieval and multi-hop question answering.The results demonstrated strong performance gains at the 10 million+ token scale. On BrowseComp-Plus, a benchmark involving inputs of 6 to 11 million tokens, standard base models failed completely, scoring 0%. In contrast, the RLM powered by GPT-5 achieved a score of 91.33%, significantly outperforming the Summary Agent (70.47%) and CodeAct (51%).The framework also excelled at tasks with high computational complexity. On OOLONG-Pairs, an information-dense reasoning benchmark where the difficulty scales quadratically with input length, base GPT-5 models failed catastrophically with a score of just 0.04%. The RLM achieved an F1 score (a balanced measure of precision and recall) of 58%, demonstrating emergent capabilities to handle dense tasks that paralyze standard models. Similarly, on code understanding tasks (CodeQA benchmark), the RLM more than doubled the performance of the base GPT-5 model, jumping from 24% to 62%.Regarding the context rot problem, the data showed that while the base GPT-5 performance degrades rapidly as task complexity increases, RLM performance holds steady, consistently outperforming the base model on contexts longer than 16,000 tokens.Despite the increased complexity of the workflow, RLMs often maintained comparable or lower average costs than the baselines. On the BrowseComp-Plus benchmark, the RLM was up to three times cheaper than the summarization baseline. However, the researchers noted that while median costs are low, RLM trajectories are \"long-tailed.\" Outlier runs can become expensive if the model gets stuck in loops or performs redundant verifications. While GPT-5 was conservative in its sub-calls, the open-source Qwen3-Coder model sometimes attempted thousands of sub-calls for simple tasks.\"Today, you likely will have to implement your own guardrails and logic to control RLM behavior,\" Zhang said. However, he hypothesizes that future models could be trained to manage their own compute budgets more effectively. Companies like Prime Intellect are planning to integrate RLM into the training process of models, possibly addressing the edge cases where the model’s inference budget spikes.For enterprise architects deciding where to place their bets, the RLM framework offers a new tool for handling information-dense problems.\"I think RLMs are still extremely useful for chatbots (think long chat histories), but ultimately they argue for an alternative way of using LMs,\" Zhang said. \"I think RLMs work in tandem with standard retrieval methods like RAG; they do not serve as a replacement, and can be used in different settings or together.\"",
          "content": "Recursive language models (RLMs) are an inference technique developed by researchers at MIT CSAIL that treat long prompts as an external environment to the model. Instead of forcing the entire prompt into the model&#x27;s context window, the framework allows the LLM to programmatically examine, decompose, and recursively call itself over snippets of the text.Rather than expanding context windows or summarizing old information, the MIT team reframes long-context reasoning as a systems problem. By letting models treat prompts as something they can inspect with code, recursive language models allow LLMs to reason over millions of tokens without retraining. This offers enterprises a practical path to long-horizon tasks like codebase analysis, legal review, and multi-step reasoning that routinely break today’s models.Because the framework is designed as a wrapper around existing models, it can serve as a drop-in replacement for applications that make direct calls to LLMs.The LLM context problemWhile frontier models are becoming increasingly sophisticated at reasoning, their ability to process massive amounts of information is not scaling at the same rate. This bottleneck is driven by two distinct limitations: the hard physical constraint on how much text a model can process at once (context length) and \"context rot.\"The challenge, the researchers argue, is whether it’s possible to scale the effective context size of general-purpose LLMs by orders of magnitude without retraining them. This capability is becoming increasingly important for enterprise applications, where LLMs are adopted for long-horizon tasks requiring the processing of millions of tokens — a challenge Zhang argues can’t be solved by simply expanding context windows.\"There is an entropy argument that implies you need exponentially more data samples as you increase the effective context window size,\" Alex Zhang, a co-author of the paper, told VentureBeat. Current approaches to extending context often rely on compaction, where the model summarizes older parts of the conversation to free up space. However, this method fails for tasks requiring random access to specific details located in earlier parts of the prompt.How RLMs workThe concept behind RLMs is drawn from \"out-of-core\" algorithms used in classical computing. These algorithms are designed to process datasets too large to fit into a computer&#x27;s main memory by keeping the data on a hard drive and fetching only the necessary chunks as needed.RLMs apply this logic to generative AI. Instead of feeding a long prompt directly into the neural network, the framework loads the text as a string variable inside a Python coding environment. The LLM is given general context about the data (such as the total character count) but does not \"see\" the text initially.Once the prompt is stored as a variable, the LLM acts as a programmer. It writes Python code to interact with the external variable, using standard commands to peek into the data. For example, the model might use regular expressions to search for specific keywords like \"Chapter 1\" or \"financial results.\"When the code execution finds a relevant snippet, the RLM pulls only that specific chunk into its active context window for analysis.For example, if the prompt is a massive book, the LLM might write a loop that identifies chapter boundaries and then triggers a sub-call to summarize each chapter individually.The architecture typically involves two agents. A \"root language model,\" often a capability-heavy model like GPT-5, acts as the orchestrator. It plans the approach, writes the code, and manages the data flow within the REPL environment. A \"recursive language model,\" often a faster and cheaper model, acts as the worker. The root LM calls this worker to process the specific text snippets isolated by the code.Because the prompt resides in the environment&#x27;s memory rather than the model&#x27;s context window, the system can handle inputs far larger than the model&#x27;s training limit. Importantly, to the end-user, the RLM behaves exactly like a standard model: It accepts a string and returns an answer. This allows enterprise teams to swap standard API calls for RLMs. For developers looking to experiment, the RLM code is currently available on GitHub. \"A key argument for RLMs is that most complex tasks can be decomposed into smaller, &#x27;local&#x27; sub-tasks,\" Zhang said. \"However, how to perform this context/problem decomposition is non-trivial, and the model must be capable of performing this.\"RLMs in actionTo validate the framework, the researchers tested RLMs against base models and other agentic approaches like CodeAct and summary agents across a variety of long-context tasks, including retrieval and multi-hop question answering.The results demonstrated strong performance gains at the 10 million+ token scale. On BrowseComp-Plus, a benchmark involving inputs of 6 to 11 million tokens, standard base models failed completely, scoring 0%. In contrast, the RLM powered by GPT-5 achieved a score of 91.33%, significantly outperforming the Summary Agent (70.47%) and CodeAct (51%).The framework also excelled at tasks with high computational complexity. On OOLONG-Pairs, an information-dense reasoning benchmark where the difficulty scales quadratically with input length, base GPT-5 models failed catastrophically with a score of just 0.04%. The RLM achieved an F1 score (a balanced measure of precision and recall) of 58%, demonstrating emergent capabilities to handle dense tasks that paralyze standard models. Similarly, on code understanding tasks (CodeQA benchmark), the RLM more than doubled the performance of the base GPT-5 model, jumping from 24% to 62%.Regarding the context rot problem, the data showed that while the base GPT-5 performance degrades rapidly as task complexity increases, RLM performance holds steady, consistently outperforming the base model on contexts longer than 16,000 tokens.Despite the increased complexity of the workflow, RLMs often maintained comparable or lower average costs than the baselines. On the BrowseComp-Plus benchmark, the RLM was up to three times cheaper than the summarization baseline. However, the researchers noted that while median costs are low, RLM trajectories are \"long-tailed.\" Outlier runs can become expensive if the model gets stuck in loops or performs redundant verifications. While GPT-5 was conservative in its sub-calls, the open-source Qwen3-Coder model sometimes attempted thousands of sub-calls for simple tasks.\"Today, you likely will have to implement your own guardrails and logic to control RLM behavior,\" Zhang said. However, he hypothesizes that future models could be trained to manage their own compute budgets more effectively. Companies like Prime Intellect are planning to integrate RLM into the training process of models, possibly addressing the edge cases where the model’s inference budget spikes.For enterprise architects deciding where to place their bets, the RLM framework offers a new tool for handling information-dense problems.\"I think RLMs are still extremely useful for chatbots (think long chat histories), but ultimately they argue for an alternative way of using LMs,\" Zhang said. \"I think RLMs work in tandem with standard retrieval methods like RAG; they do not serve as a replacement, and can be used in different settings or together.\"",
          "feed_position": 0,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/5Ybr6R81VAO6yoTI3fLOlG/9a42cc8ccddba84b036b42d27f769789/Recursive_language_model.jpg?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/data/x-open-sources-its-algorithm-5-ways-businesses-can-benefit",
          "published_at": "Tue, 20 Jan 2026 18:33:00 GMT",
          "title": "X open sources its algorithm: 5 ways businesses can benefit",
          "standfirst": "Elon Musk&#x27;s social network X (formerly known as Twitter) last night released some of the code and architecture of its overhauled social recommendation algorithm under a permissive, enterprise-friendly open source license (Apache 2.0) on Github, allowing for commercial usage and modification. This is the algorithm that decides which X posts and accounts to show to which users on the social network.The new X algorithm, as opposed toto the manual heuristic rules and legacy models in the past, is based on a \"Transformer\" architecture powered by its parent company, xAI’s, Grok AI language model. This is a significant release for enterprises who have brand accounts on X, or whose leaders and employees use X to post company promotional messages, links, content, etc — as it now provides a look at how X evaluates posts and accounts on the platform, and what criteria go into it deciding to show a post or specific account to users. Therefore, it&#x27;s imperative for any businesses using X to post promotional and informational content to understand how the X algorithm works as best as they can, in order to maximize their usage of the platform. To analogize: imagine trying to navigate a hike through a massive woods without a map. You&#x27;d likely end up lost and waste time and energy (resources) trying to get to your destination. But with a map, you could plot your route, look for the appropriate landmarks, check your progress along the way, and revise your path as necessary to stay on track. X open sourcing its new transformer-based recommendation algorithm is in many ways just this — providing a \"map\" to all those who use the platform on how to achieve the best performance they (and their brands) can.Here is the technical breakdown of the new architecture and five data-backed strategies to leverage it for commercial growth.The \"Red Herring\" of 2023 vs. The \"Grok\" Reality of 2026In March 2023, shortly after it was acquired by Musk, X also open sourced its recommendation algorithm.However, the release revealed a tangled web of \"spaghetti code\" and manual heuristics and was criticized by outlets like Wired (where my wife works, full disclosure) and organizations including the Center for Democracy and Technology, as being too heavily redacted to be useful. It was seen as a static snapshot of a decaying system.The code released on January 19, 2026, confirms that the spaghetti is gone. X has replaced the manual filtering layers with a unified, AI-driven Transformer architecture. The system uses a RecsysBatch input model that ingests user history and action probabilities to output a raw score. It is cleaner, faster, and infinitely more ruthless.But there is a catch: The specific \"weighting constants\"—the magic numbers that tell us exactly how much a Like or Reply is worth—have been redacted from this release.Here are the five strategic imperatives for brands operating in this new, Grok-mediated environment.1. The \"Velocity\" Window: You Have 30 Minutes to Live or DieIn the 2023 legacy code, content drifted through complex clusters, often finding life hours after posting. The new Grok architecture is designed for immediate signal processing.Community analysis of the new Rust-based scoring functions reveals a strict \"Velocity\" mechanic. The lifecycle of a corporate post is determined in the first half-hour. If engagement signals (clicks, dwells, replies) fail to exceed a dynamic threshold in the first 15 minutes, the post is mathematically unlikely to breach the general \"For You\" pool.The architecture includes a specific scorer that penalizes multiple posts from the same user in a short window. Posting 10 times a day yields diminishing returns; the algorithm actively downranks your 3rd, 4th, and 5th posts to force variety into the feed. Space your announcements out.Thus, the takeaway for business data leads is to coordinate your internal comms and advocacy programs with military precision. \"Employee advocacy\" can no longer be asynchronous. If your employees or partners engage with a company announcement two hours later, the mathematical window has likely closed. You must front-load engagement in the first 10 minutes to artificially spike the velocity signal.2. The \"Reply\" Trap: Why Engagement Bait is DeadIn 2023, data suggested that an author replying to comments was a \"cheat code\" for visibility. In 2026, this strategy has become a trap.While early analysis circulated rumors of a \"75x\" boost for replies, developers examining the new repository have confirmed that the actual weighting constants are hidden. More importantly, X’s Head of Product, Nikita Bier, has explicitly stated that \"Replies don&#x27;t count anymore\" for revenue sharing, in a move designed to kill \"reply rings\" and spam farms.Bier clarified that replies only generate value if they are high-quality enough to generate \"Home Timeline impressions\" on their own merit.As this is the case, businesses should stop optimizing for \"reply volume\" and start optimizing for \"reply quality.\" The algorithm is actively hostile toward low-effort engagement rings. Businesses and individuals should not reply incessantly to every comment with emojis or generic thanks. They should only reply if the response adds enough value to stand alone as a piece of content in a user’s feed.With replies devalued, focus on the other positive signals visible in the code: dwell_time (how long a user freezes on your post) and share_via_dm. Long-form threads or visual data that force a user to stop scrolling are now mathematically safer bets than controversial questions.3. X Is Basically Pay-to-Play, NowThe 2023 algorithm used X paid subscription status as one of many variables. The 2026 architecture simplifies this into a brutal base-score reality.Code analysis reveals that before a post is evaluated for quality, the account is assigned a base score. X accounts that are \"verified\" by paying the monthly \"Premium\" subscription ($3 per month for individual account Premium Basic, $200/month for businesses) receive a significantly higher ceiling (up to +100) compared to unverified accounts, which are capped (max +55).Therefore, if your brand, executives, or key spokespeople are not verified (X Premium or Verified Organizations), you are competing with a handicap. For a business looking to acquire customers or leads via X, verification is a mandatory infrastructure cost to remove a programmatic throttle on your reach.4. The \"Report\" Penalty: Brand Safety Requires De-escalationThe Grok model has replaced complex \"toxicity\" rules with a simplified feedback loop. While the exact weight of someone filing a \"Report\" on your X post or account over objectionable or false material is hidden in the new config files, it remains the ultimate negative signal. But it isn&#x27;t the only one. The model also outputs probabilities for P(not_interested) and P(mute_author). Irrelevant clickbait doesn&#x27;t just get ignored; it actively trains the model to predict that users will mute you, permanently suppressing your future reach.In a system driven by AI probabilities, a \"Report\" or \"Block\" signal trains the model to permanently dissociate your brand from that user&#x27;s entire cluster.In practice, this means \"rage bait\" or controversial takes are now incredibly dangerous for brands. It takes only a tiny fraction of users utilizing the \"Report\" function to tank a post&#x27;s visibility entirely. Your content strategy must prioritize engagement that excites users enough to reply, but never enough to report.5. OSINT as a Competency: Watch the Execs, Not Just the RepoThe most significant takeaway from today&#x27;s release is what is missing. The repository provides the architecture (the \"car\"), but it hides the weights (the \"fuel\").As X user @Tenobrus noted, the repo is \"barebones\" regarding constants. This means you cannot rely solely on the code to dictate strategy. You must triangulate the code with executive communications. When Bier announces a change to \"revenue share\" logic, you must assume it mirrors a change in the \"ranking\" logic.Therefore, data decision makers should assign a technical lead to monitor both the xai-org/x-algorithm repository and the public statements of the Engineering team. The code tells you *how* the system thinks; the executives tell you *what* it is currently rewarding.Summary: The Code is the StrategyThe Grok-based transformer architecture is cleaner, faster, and more logical than its predecessor. It does not care about your legacy or your follower count. It cares about Velocity and Quality.The Winning Formula:1. Verify to secure the base score. 2. Front-load engagement to survive the 30-minute velocity check. 3. Avoid \"spammy\" replies; focus on standalone value. 4. Monitor executive comms to fill in the gaps left by the code.In the era of Grok, the algorithm is smarter. Your data and business strategy using X ought to be, too.",
          "content": "Elon Musk&#x27;s social network X (formerly known as Twitter) last night released some of the code and architecture of its overhauled social recommendation algorithm under a permissive, enterprise-friendly open source license (Apache 2.0) on Github, allowing for commercial usage and modification. This is the algorithm that decides which X posts and accounts to show to which users on the social network.The new X algorithm, as opposed toto the manual heuristic rules and legacy models in the past, is based on a \"Transformer\" architecture powered by its parent company, xAI’s, Grok AI language model. This is a significant release for enterprises who have brand accounts on X, or whose leaders and employees use X to post company promotional messages, links, content, etc — as it now provides a look at how X evaluates posts and accounts on the platform, and what criteria go into it deciding to show a post or specific account to users. Therefore, it&#x27;s imperative for any businesses using X to post promotional and informational content to understand how the X algorithm works as best as they can, in order to maximize their usage of the platform. To analogize: imagine trying to navigate a hike through a massive woods without a map. You&#x27;d likely end up lost and waste time and energy (resources) trying to get to your destination. But with a map, you could plot your route, look for the appropriate landmarks, check your progress along the way, and revise your path as necessary to stay on track. X open sourcing its new transformer-based recommendation algorithm is in many ways just this — providing a \"map\" to all those who use the platform on how to achieve the best performance they (and their brands) can.Here is the technical breakdown of the new architecture and five data-backed strategies to leverage it for commercial growth.The \"Red Herring\" of 2023 vs. The \"Grok\" Reality of 2026In March 2023, shortly after it was acquired by Musk, X also open sourced its recommendation algorithm.However, the release revealed a tangled web of \"spaghetti code\" and manual heuristics and was criticized by outlets like Wired (where my wife works, full disclosure) and organizations including the Center for Democracy and Technology, as being too heavily redacted to be useful. It was seen as a static snapshot of a decaying system.The code released on January 19, 2026, confirms that the spaghetti is gone. X has replaced the manual filtering layers with a unified, AI-driven Transformer architecture. The system uses a RecsysBatch input model that ingests user history and action probabilities to output a raw score. It is cleaner, faster, and infinitely more ruthless.But there is a catch: The specific \"weighting constants\"—the magic numbers that tell us exactly how much a Like or Reply is worth—have been redacted from this release.Here are the five strategic imperatives for brands operating in this new, Grok-mediated environment.1. The \"Velocity\" Window: You Have 30 Minutes to Live or DieIn the 2023 legacy code, content drifted through complex clusters, often finding life hours after posting. The new Grok architecture is designed for immediate signal processing.Community analysis of the new Rust-based scoring functions reveals a strict \"Velocity\" mechanic. The lifecycle of a corporate post is determined in the first half-hour. If engagement signals (clicks, dwells, replies) fail to exceed a dynamic threshold in the first 15 minutes, the post is mathematically unlikely to breach the general \"For You\" pool.The architecture includes a specific scorer that penalizes multiple posts from the same user in a short window. Posting 10 times a day yields diminishing returns; the algorithm actively downranks your 3rd, 4th, and 5th posts to force variety into the feed. Space your announcements out.Thus, the takeaway for business data leads is to coordinate your internal comms and advocacy programs with military precision. \"Employee advocacy\" can no longer be asynchronous. If your employees or partners engage with a company announcement two hours later, the mathematical window has likely closed. You must front-load engagement in the first 10 minutes to artificially spike the velocity signal.2. The \"Reply\" Trap: Why Engagement Bait is DeadIn 2023, data suggested that an author replying to comments was a \"cheat code\" for visibility. In 2026, this strategy has become a trap.While early analysis circulated rumors of a \"75x\" boost for replies, developers examining the new repository have confirmed that the actual weighting constants are hidden. More importantly, X’s Head of Product, Nikita Bier, has explicitly stated that \"Replies don&#x27;t count anymore\" for revenue sharing, in a move designed to kill \"reply rings\" and spam farms.Bier clarified that replies only generate value if they are high-quality enough to generate \"Home Timeline impressions\" on their own merit.As this is the case, businesses should stop optimizing for \"reply volume\" and start optimizing for \"reply quality.\" The algorithm is actively hostile toward low-effort engagement rings. Businesses and individuals should not reply incessantly to every comment with emojis or generic thanks. They should only reply if the response adds enough value to stand alone as a piece of content in a user’s feed.With replies devalued, focus on the other positive signals visible in the code: dwell_time (how long a user freezes on your post) and share_via_dm. Long-form threads or visual data that force a user to stop scrolling are now mathematically safer bets than controversial questions.3. X Is Basically Pay-to-Play, NowThe 2023 algorithm used X paid subscription status as one of many variables. The 2026 architecture simplifies this into a brutal base-score reality.Code analysis reveals that before a post is evaluated for quality, the account is assigned a base score. X accounts that are \"verified\" by paying the monthly \"Premium\" subscription ($3 per month for individual account Premium Basic, $200/month for businesses) receive a significantly higher ceiling (up to +100) compared to unverified accounts, which are capped (max +55).Therefore, if your brand, executives, or key spokespeople are not verified (X Premium or Verified Organizations), you are competing with a handicap. For a business looking to acquire customers or leads via X, verification is a mandatory infrastructure cost to remove a programmatic throttle on your reach.4. The \"Report\" Penalty: Brand Safety Requires De-escalationThe Grok model has replaced complex \"toxicity\" rules with a simplified feedback loop. While the exact weight of someone filing a \"Report\" on your X post or account over objectionable or false material is hidden in the new config files, it remains the ultimate negative signal. But it isn&#x27;t the only one. The model also outputs probabilities for P(not_interested) and P(mute_author). Irrelevant clickbait doesn&#x27;t just get ignored; it actively trains the model to predict that users will mute you, permanently suppressing your future reach.In a system driven by AI probabilities, a \"Report\" or \"Block\" signal trains the model to permanently dissociate your brand from that user&#x27;s entire cluster.In practice, this means \"rage bait\" or controversial takes are now incredibly dangerous for brands. It takes only a tiny fraction of users utilizing the \"Report\" function to tank a post&#x27;s visibility entirely. Your content strategy must prioritize engagement that excites users enough to reply, but never enough to report.5. OSINT as a Competency: Watch the Execs, Not Just the RepoThe most significant takeaway from today&#x27;s release is what is missing. The repository provides the architecture (the \"car\"), but it hides the weights (the \"fuel\").As X user @Tenobrus noted, the repo is \"barebones\" regarding constants. This means you cannot rely solely on the code to dictate strategy. You must triangulate the code with executive communications. When Bier announces a change to \"revenue share\" logic, you must assume it mirrors a change in the \"ranking\" logic.Therefore, data decision makers should assign a technical lead to monitor both the xai-org/x-algorithm repository and the public statements of the Engineering team. The code tells you *how* the system thinks; the executives tell you *what* it is currently rewarding.Summary: The Code is the StrategyThe Grok-based transformer architecture is cleaner, faster, and more logical than its predecessor. It does not care about your legacy or your follower count. It cares about Velocity and Quality.The Winning Formula:1. Verify to secure the base score. 2. Front-load engagement to survive the 30-minute velocity check. 3. Avoid \"spammy\" replies; focus on standalone value. 4. Monitor executive comms to fill in the gaps left by the code.In the era of Grok, the algorithm is smarter. Your data and business strategy using X ought to be, too.",
          "feed_position": 1,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/16AQsBHmZwamZhEThe81IE/c62d8f72d2a927091292807e176cbf7c/1c3ad1be-18a5-42ae-9e71-8e1407640d0f.jpg?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/computing/accessories/best-wireless-charger-140036359.html",
          "published_at": "Tue, 20 Jan 2026 10:01:26 +0000",
          "title": "The best wireless chargers for 2026",
          "standfirst": "If you’ve upgraded your phone recently, there’s a good chance it supports wireless charging. Battery life can be one of the first things to deteriorate as your phone ages, so you’ll need quick and easy ways to power up wherever you are. You may not always be able to have a cable on your person, but investing in a wireless phone charger (or a few) can make it more convenient to plop your phone down and know it’ll have more juice when you pick it back up again.While you’re not going to get the same charging speed with a wireless charger that you would with a power cable, the convenience of a power source like this is undeniable. Stick a wireless charger on your bedside, on your desk at work, in your kitchen or wherever you spend a good chunk of your time and you’ll have a reliable way to power up your phone, sans bulky, messy cables. Needless to say, there are a ton of options out there with different charging capabilities and price ranges. Below, we’ve collected the best wireless chargers we’ve tested to make your search a little easier. Table of contents Best wireless chargers for 2026 What to look for in a wireless charger Where and how will you use your charger? Wireless charging performance Quality and box contents Wireless chargers FAQs Best wireless chargers for 2026 What to look for in a wireless charger While it’s tempting to buy a wireless charging pad optimized for the specific phone you have now, resist that urge. Instead, think about the types of devices (phones included) that you could see yourself using in the near future. If you’re sure you’ll use iPhones for a long time, an Apple MagSafe-compatible magnetic wireless charger will be faster and more convenient. If you use Android phones or think you might switch sides, however, you’ll want a more universal design. If you have other accessories like wireless earbuds or a smartwatch that supports wireless charging, maybe you’d be better off with a 3-in-1 wireless charger or full wireless charging station. Where and how will you use your charger? Odds are that you have a specific use case in mind for your charger. You may want it by your bedside on your nightstand for a quick charge in the morning, or on your desk for at-a-glance notifications. You might even keep it in your bag for convenient travel charging instead of bulky portable chargers or power banks. Think about where you want to use this accessory and what you want to do with the device(s) it charges while it’s powering up. For example, a wireless charging pad might be better for bedside use if you just want to be able to drop your phone down at the end of a long day and know it’ll be powered up in the morning. However, a stand will be better if you have an iPhone and want to make use of the Standby feature during the nighttime hours. For a desk wireless charger, a stand lets you more easily glance at phone notifications throughout the day. For traveling, undoubtedly, a puck-style charging pad is best since it will take up much less space in your bag than a stand would. Many power banks also include wireless charging pads built in, so one of those might make even more sense for those who are always on the go. Some foldable chargers are also designed for travel, collapsing flat to take up less space. Wireless charging performance Although wireless charging is usually slower than its wired equivalent, speed and wattage are still important considerations. A fast charger can supply enough power for a long night out in the time it takes to change outfits. Look for options that promise faster charging and support standards like Qi2 certified charging for the best balance of efficiency and compatibility. In general, a 15W charger is more than quick enough for most situations, and you’ll need a MagSafe-compatible charger to extract that level of performance from an iPhone. With that said, even the slower 7.5W and 10W chargers are fast enough for an overnight power-up. If anything, you’ll want to worry more about support for cases. While many models can deliver power through a reasonably thick case (typically 3mm to 5mm), you’ll occasionally run into examples that only work with naked phones. There are some proprietary chargers that smash the 15W barrier if you have the right phone. Apple’s latest MagSafe charging pad can provide up to 25W of wireless power to compatible iPhones when paired with a 30W or 35W adapter — the latter being another component you’ll have to get right to make sure the whole equation works as fast as it possibly can. Quality and box contents Pay attention to what’s included in the box. Some wireless chargers don’t include power adapters, and others may even ask you to reuse your phone’s USB-C charging cable. What may seem to be a bargain may prove expensive if you have to buy extras just to use it properly. As mentioned above, you’ll want to make sure all of the components needed to use the wireless charger can provide the level of power you need — you’re only as strong (or in this case, fast) as your weakest link. Fit and finish is also worth considering. You’re likely going to use your wireless charger every day, so even small differences in build quality could make the difference between joy and frustration. If your charger doesn’t use MagSafe-compatible tech, textured surfaces like fabric or rubberized plastic are more likely to keep your phone in place. The base should be grippy or weighty enough that the charger won’t slide around. Also double check that the wireless charger you’re considering can support phones outfitted with cases — the specifications are usually listed in the charger’s description or specs. You’ll also want to think about the minor conveniences. Status lights are useful for indicating correct phone placement, but an overly bright light can be distracting. Ideally, the light dims or shuts off after a certain period of time. And while we caution against lips and trays that limit compatibility, you may still want some barriers to prevent your device falling off its perch on the charging station. Wireless chargers FAQs Do wireless chargers work if you have a phone case? Many wireless chargers do work if you leave the case on your phone. Generally, a case up to 3mm thick should be compatible with most wireless chargers. However, you should check the manufacturer’s guide to ensure a case is supported. How do I know if my phone supports wireless charging? Checking the phone’s specification should tell you if your phone is compatible with wireless charging. You might see words like “Qi wireless charging” or “wireless charging compatible.” Do cords charge your phone faster? Most often, wired charging will be faster than wireless charging. However, wired charging also depends on what the charging cable’s speed is and how much power it’s designed to carry. A quick-charging cable that can transmit up to 120W of power is going to be faster than a wireless charger.This article originally appeared on Engadget at https://www.engadget.com/computing/accessories/best-wireless-charger-140036359.html?src=rss",
          "content": "If you’ve upgraded your phone recently, there’s a good chance it supports wireless charging. Battery life can be one of the first things to deteriorate as your phone ages, so you’ll need quick and easy ways to power up wherever you are. You may not always be able to have a cable on your person, but investing in a wireless phone charger (or a few) can make it more convenient to plop your phone down and know it’ll have more juice when you pick it back up again.While you’re not going to get the same charging speed with a wireless charger that you would with a power cable, the convenience of a power source like this is undeniable. Stick a wireless charger on your bedside, on your desk at work, in your kitchen or wherever you spend a good chunk of your time and you’ll have a reliable way to power up your phone, sans bulky, messy cables. Needless to say, there are a ton of options out there with different charging capabilities and price ranges. Below, we’ve collected the best wireless chargers we’ve tested to make your search a little easier. Table of contents Best wireless chargers for 2026 What to look for in a wireless charger Where and how will you use your charger? Wireless charging performance Quality and box contents Wireless chargers FAQs Best wireless chargers for 2026 What to look for in a wireless charger While it’s tempting to buy a wireless charging pad optimized for the specific phone you have now, resist that urge. Instead, think about the types of devices (phones included) that you could see yourself using in the near future. If you’re sure you’ll use iPhones for a long time, an Apple MagSafe-compatible magnetic wireless charger will be faster and more convenient. If you use Android phones or think you might switch sides, however, you’ll want a more universal design. If you have other accessories like wireless earbuds or a smartwatch that supports wireless charging, maybe you’d be better off with a 3-in-1 wireless charger or full wireless charging station. Where and how will you use your charger? Odds are that you have a specific use case in mind for your charger. You may want it by your bedside on your nightstand for a quick charge in the morning, or on your desk for at-a-glance notifications. You might even keep it in your bag for convenient travel charging instead of bulky portable chargers or power banks. Think about where you want to use this accessory and what you want to do with the device(s) it charges while it’s powering up. For example, a wireless charging pad might be better for bedside use if you just want to be able to drop your phone down at the end of a long day and know it’ll be powered up in the morning. However, a stand will be better if you have an iPhone and want to make use of the Standby feature during the nighttime hours. For a desk wireless charger, a stand lets you more easily glance at phone notifications throughout the day. For traveling, undoubtedly, a puck-style charging pad is best since it will take up much less space in your bag than a stand would. Many power banks also include wireless charging pads built in, so one of those might make even more sense for those who are always on the go. Some foldable chargers are also designed for travel, collapsing flat to take up less space. Wireless charging performance Although wireless charging is usually slower than its wired equivalent, speed and wattage are still important considerations. A fast charger can supply enough power for a long night out in the time it takes to change outfits. Look for options that promise faster charging and support standards like Qi2 certified charging for the best balance of efficiency and compatibility. In general, a 15W charger is more than quick enough for most situations, and you’ll need a MagSafe-compatible charger to extract that level of performance from an iPhone. With that said, even the slower 7.5W and 10W chargers are fast enough for an overnight power-up. If anything, you’ll want to worry more about support for cases. While many models can deliver power through a reasonably thick case (typically 3mm to 5mm), you’ll occasionally run into examples that only work with naked phones. There are some proprietary chargers that smash the 15W barrier if you have the right phone. Apple’s latest MagSafe charging pad can provide up to 25W of wireless power to compatible iPhones when paired with a 30W or 35W adapter — the latter being another component you’ll have to get right to make sure the whole equation works as fast as it possibly can. Quality and box contents Pay attention to what’s included in the box. Some wireless chargers don’t include power adapters, and others may even ask you to reuse your phone’s USB-C charging cable. What may seem to be a bargain may prove expensive if you have to buy extras just to use it properly. As mentioned above, you’ll want to make sure all of the components needed to use the wireless charger can provide the level of power you need — you’re only as strong (or in this case, fast) as your weakest link. Fit and finish is also worth considering. You’re likely going to use your wireless charger every day, so even small differences in build quality could make the difference between joy and frustration. If your charger doesn’t use MagSafe-compatible tech, textured surfaces like fabric or rubberized plastic are more likely to keep your phone in place. The base should be grippy or weighty enough that the charger won’t slide around. Also double check that the wireless charger you’re considering can support phones outfitted with cases — the specifications are usually listed in the charger’s description or specs. You’ll also want to think about the minor conveniences. Status lights are useful for indicating correct phone placement, but an overly bright light can be distracting. Ideally, the light dims or shuts off after a certain period of time. And while we caution against lips and trays that limit compatibility, you may still want some barriers to prevent your device falling off its perch on the charging station. Wireless chargers FAQs Do wireless chargers work if you have a phone case? Many wireless chargers do work if you leave the case on your phone. Generally, a case up to 3mm thick should be compatible with most wireless chargers. However, you should check the manufacturer’s guide to ensure a case is supported. How do I know if my phone supports wireless charging? Checking the phone’s specification should tell you if your phone is compatible with wireless charging. You might see words like “Qi wireless charging” or “wireless charging compatible.” Do cords charge your phone faster? Most often, wired charging will be faster than wireless charging. However, wired charging also depends on what the charging cable’s speed is and how much power it’s designed to carry. A quick-charging cable that can transmit up to 120W of power is going to be faster than a wireless charger.This article originally appeared on Engadget at https://www.engadget.com/computing/accessories/best-wireless-charger-140036359.html?src=rss",
          "feed_position": 15,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2023-02/c9a24a20-b7a0-11ed-b3cd-16d5d607f8d1"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/infrastructure/claude-code-costs-up-to-usd200-a-month-goose-does-the-same-thing-for-free",
          "published_at": "Mon, 19 Jan 2026 14:00:00 GMT",
          "title": "Claude Code costs up to $200 a month. Goose does the same thing for free.",
          "standfirst": "The artificial intelligence coding revolution comes with a catch: it&#x27;s expensive.Claude Code, Anthropic&#x27;s terminal-based AI agent that can write, debug, and deploy code autonomously, has captured the imagination of software developers worldwide. But its pricing — ranging from $20 to $200 per month depending on usage — has sparked a growing rebellion among the very programmers it aims to serve.Now, a free alternative is gaining traction. Goose, an open-source AI agent developed by Block (the financial technology company formerly known as Square), offers nearly identical functionality to Claude Code but runs entirely on a user&#x27;s local machine. No subscription fees. No cloud dependency. No rate limits that reset every five hours.\"Your data stays with you, period,\" said Parth Sareen, a software engineer who demonstrated the tool during a recent livestream. The comment captures the core appeal: Goose gives developers complete control over their AI-powered workflow, including the ability to work offline — even on an airplane.The project has exploded in popularity. Goose now boasts more than 26,100 stars on GitHub, the code-sharing platform, with 362 contributors and 102 releases since its launch. The latest version, 1.20.1, shipped on January 19, 2026, reflecting a development pace that rivals commercial products.For developers frustrated by Claude Code&#x27;s pricing structure and usage caps, Goose represents something increasingly rare in the AI industry: a genuinely free, no-strings-attached option for serious work.Anthropic&#x27;s new rate limits spark a developer revoltTo understand why Goose matters, you need to understand the Claude Code pricing controversy.Anthropic, the San Francisco artificial intelligence company founded by former OpenAI executives, offers Claude Code as part of its subscription tiers. The free plan provides no access whatsoever. The Pro plan, at $17 per month with annual billing (or $20 monthly), limits users to just 10 to 40 prompts every five hours — a constraint that serious developers exhaust within minutes of intensive work.The Max plans, at $100 and $200 per month, offer more headroom: 50 to 200 prompts and 200 to 800 prompts respectively, plus access to Anthropic&#x27;s most powerful model, Claude 4.5 Opus. But even these premium tiers come with restrictions that have inflamed the developer community.In late July, Anthropic announced new weekly rate limits. Under the system, Pro users receive 40 to 80 hours of Sonnet 4 usage per week. Max users at the $200 tier get 240 to 480 hours of Sonnet 4, plus 24 to 40 hours of Opus 4. Nearly five months later, the frustration has not subsided.The problem? Those \"hours\" are not actual hours. They represent token-based limits that vary wildly depending on codebase size, conversation length, and the complexity of the code being processed. Independent analysis suggests the actual per-session limits translate to roughly 44,000 tokens for Pro users and 220,000 tokens for the $200 Max plan.\"It&#x27;s confusing and vague,\" one developer wrote in a widely shared analysis. \"When they say &#x27;24-40 hours of Opus 4,&#x27; that doesn&#x27;t really tell you anything useful about what you&#x27;re actually getting.\"The backlash on Reddit and developer forums has been fierce. Some users report hitting their daily limits within 30 minutes of intensive coding. Others have canceled their subscriptions entirely, calling the new restrictions \"a joke\" and \"unusable for real work.\"Anthropic has defended the changes, stating that the limits affect fewer than five percent of users and target people running Claude Code \"continuously in the background, 24/7.\" But the company has not clarified whether that figure refers to five percent of Max subscribers or five percent of all users — a distinction that matters enormously.How Block built a free AI coding agent that works offlineGoose takes a radically different approach to the same problem.Built by Block, the payments company led by Jack Dorsey, Goose is what engineers call an \"on-machine AI agent.\" Unlike Claude Code, which sends your queries to Anthropic&#x27;s servers for processing, Goose can run entirely on your local computer using open-source language models that you download and control yourself.The project&#x27;s documentation describes it as going \"beyond code suggestions\" to \"install, execute, edit, and test with any LLM.\" That last phrase — \"any LLM\" — is the key differentiator. Goose is model-agnostic by design.You can connect Goose to Anthropic&#x27;s Claude models if you have API access. You can use OpenAI&#x27;s GPT-5 or Google&#x27;s Gemini. You can route it through services like Groq or OpenRouter. Or — and this is where things get interesting — you can run it entirely locally using tools like Ollama, which let you download and execute open-source models on your own hardware.The practical implications are significant. With a local setup, there are no subscription fees, no usage caps, no rate limits, and no concerns about your code being sent to external servers. Your conversations with the AI never leave your machine.\"I use Ollama all the time on planes — it&#x27;s a lot of fun!\" Sareen noted during a demonstration, highlighting how local models free developers from the constraints of internet connectivity.What Goose can do that traditional code assistants can&#x27;tGoose operates as a command-line tool or desktop application that can autonomously perform complex development tasks. It can build entire projects from scratch, write and execute code, debug failures, orchestrate workflows across multiple files, and interact with external APIs — all without constant human oversight.The architecture relies on what the AI industry calls \"tool calling\" or \"function calling\" — the ability for a language model to request specific actions from external systems. When you ask Goose to create a new file, run a test suite, or check the status of a GitHub pull request, it doesn&#x27;t just generate text describing what should happen. It actually executes those operations.This capability depends heavily on the underlying language model. Claude 4 models from Anthropic currently perform best at tool calling, according to the Berkeley Function-Calling Leaderboard, which ranks models on their ability to translate natural language requests into executable code and system commands.But newer open-source models are catching up quickly. Goose&#x27;s documentation highlights several options with strong tool-calling support: Meta&#x27;s Llama series, Alibaba&#x27;s Qwen models, Google&#x27;s Gemma variants, and DeepSeek&#x27;s reasoning-focused architectures.The tool also integrates with the Model Context Protocol, or MCP, an emerging standard for connecting AI agents to external services. Through MCP, Goose can access databases, search engines, file systems, and third-party APIs — extending its capabilities far beyond what the base language model provides.Setting Up Goose with a Local ModelFor developers interested in a completely free, privacy-preserving setup, the process involves three main components: Goose itself, Ollama (a tool for running open-source models locally), and a compatible language model.Step 1: Install OllamaOllama is an open-source project that dramatically simplifies the process of running large language models on personal hardware. It handles the complex work of downloading, optimizing, and serving models through a simple interface.Download and install Ollama from ollama.com. Once installed, you can pull models with a single command. For coding tasks, Qwen 2.5 offers strong tool-calling support:ollama run qwen2.5The model downloads automatically and begins running on your machine.Step 2: Install GooseGoose is available as both a desktop application and a command-line interface. The desktop version provides a more visual experience, while the CLI appeals to developers who prefer working entirely in the terminal.Installation instructions vary by operating system but generally involve downloading from Goose&#x27;s GitHub releases page or using a package manager. Block provides pre-built binaries for macOS (both Intel and Apple Silicon), Windows, and Linux.Step 3: Configure the ConnectionIn Goose Desktop, navigate to Settings, then Configure Provider, and select Ollama. Confirm that the API Host is set to http://localhost:11434 (Ollama&#x27;s default port) and click Submit.For the command-line version, run goose configure, select \"Configure Providers,\" choose Ollama, and enter the model name when prompted.That&#x27;s it. Goose is now connected to a language model running entirely on your hardware, ready to execute complex coding tasks without any subscription fees or external dependencies.The RAM, processing power, and trade-offs you should know aboutThe obvious question: what kind of computer do you need?Running large language models locally requires substantially more computational resources than typical software. The key constraint is memory — specifically, RAM on most systems, or VRAM if using a dedicated graphics card for acceleration.Block&#x27;s documentation suggests that 32 gigabytes of RAM provides \"a solid baseline for larger models and outputs.\" For Mac users, this means the computer&#x27;s unified memory is the primary bottleneck. For Windows and Linux users with discrete NVIDIA graphics cards, GPU memory (VRAM) matters more for acceleration.But you don&#x27;t necessarily need expensive hardware to get started. Smaller models with fewer parameters run on much more modest systems. Qwen 2.5, for instance, comes in multiple sizes, and the smaller variants can operate effectively on machines with 16 gigabytes of RAM.\"You don&#x27;t need to run the largest models to get excellent results,\" Sareen emphasized. The practical recommendation: start with a smaller model to test your workflow, then scale up as needed.For context, Apple&#x27;s entry-level MacBook Air with 8 gigabytes of RAM would struggle with most capable coding models. But a MacBook Pro with 32 gigabytes — increasingly common among professional developers — handles them comfortably.Why keeping your code off the cloud matters more than everGoose with a local LLM is not a perfect substitute for Claude Code. The comparison involves real trade-offs that developers should understand.Model Quality: Claude 4.5 Opus, Anthropic&#x27;s flagship model, remains arguably the most capable AI for software engineering tasks. It excels at understanding complex codebases, following nuanced instructions, and producing high-quality code on the first attempt. Open-source models have improved dramatically, but a gap persists — particularly for the most challenging tasks.One developer who switched to the $200 Claude Code plan described the difference bluntly: \"When I say &#x27;make this look modern,&#x27; Opus knows what I mean. Other models give me Bootstrap circa 2015.\"Context Window: Claude Sonnet 4.5, accessible through the API, offers a massive one-million-token context window — enough to load entire large codebases without chunking or context management issues. Most local models are limited to 4,096 or 8,192 tokens by default, though many can be configured for longer contexts at the cost of increased memory usage and slower processing.Speed: Cloud-based services like Claude Code run on dedicated server hardware optimized for AI inference. Local models, running on consumer laptops, typically process requests more slowly. The difference matters for iterative workflows where you&#x27;re making rapid changes and waiting for AI feedback.Tooling Maturity: Claude Code benefits from Anthropic&#x27;s dedicated engineering resources. Features like prompt caching (which can reduce costs by up to 90 percent for repeated contexts) and structured outputs are polished and well-documented. Goose, while actively developed with 102 releases to date, relies on community contributions and may lack equivalent refinement in specific areas.How Goose stacks up against Cursor, GitHub Copilot, and the paid AI coding marketGoose enters a crowded market of AI coding tools, but occupies a distinctive position.Cursor, a popular AI-enhanced code editor, charges $20 per month for its Pro tier and $200 for Ultra—pricing that mirrors Claude Code&#x27;s Max plans. Cursor provides approximately 4,500 Sonnet 4 requests per month at the Ultra level, a substantially different allocation model than Claude Code&#x27;s hourly resets.Cline, Roo Code, and similar open-source projects offer AI coding assistance but with varying levels of autonomy and tool integration. Many focus on code completion rather than the agentic task execution that defines Goose and Claude Code.Amazon&#x27;s CodeWhisperer, GitHub Copilot, and enterprise offerings from major cloud providers target large organizations with complex procurement processes and dedicated budgets. They are less relevant to individual developers and small teams seeking lightweight, flexible tools.Goose&#x27;s combination of genuine autonomy, model agnosticism, local operation, and zero cost creates a unique value proposition. The tool is not trying to compete with commercial offerings on polish or model quality. It&#x27;s competing on freedom — both financial and architectural.The $200-a-month era for AI coding tools may be endingThe AI coding tools market is evolving quickly. Open-source models are improving at a pace that continually narrows the gap with proprietary alternatives. Moonshot AI&#x27;s Kimi K2 and z.ai&#x27;s GLM 4.5 now benchmark near Claude Sonnet 4 levels — and they&#x27;re freely available.If this trajectory continues, the quality advantage that justifies Claude Code&#x27;s premium pricing may erode. Anthropic would then face pressure to compete on features, user experience, and integration rather than raw model capability.For now, developers face a clear choice. Those who need the absolute best model quality, who can afford premium pricing, and who accept usage restrictions may prefer Claude Code. Those who prioritize cost, privacy, offline access, and flexibility have a genuine alternative in Goose.The fact that a $200-per-month commercial product has a zero-dollar open-source competitor with comparable core functionality is itself remarkable. It reflects both the maturation of open-source AI infrastructure and the appetite among developers for tools that respect their autonomy.Goose is not perfect. It requires more technical setup than commercial alternatives. It depends on hardware resources that not every developer possesses. Its model options, while improving rapidly, still trail the best proprietary offerings on complex tasks.But for a growing community of developers, those limitations are acceptable trade-offs for something increasingly rare in the AI landscape: a tool that truly belongs to them.Goose is available for download at github.com/block/goose. Ollama is available at ollama.com. Both projects are free and open source.",
          "content": "The artificial intelligence coding revolution comes with a catch: it&#x27;s expensive.Claude Code, Anthropic&#x27;s terminal-based AI agent that can write, debug, and deploy code autonomously, has captured the imagination of software developers worldwide. But its pricing — ranging from $20 to $200 per month depending on usage — has sparked a growing rebellion among the very programmers it aims to serve.Now, a free alternative is gaining traction. Goose, an open-source AI agent developed by Block (the financial technology company formerly known as Square), offers nearly identical functionality to Claude Code but runs entirely on a user&#x27;s local machine. No subscription fees. No cloud dependency. No rate limits that reset every five hours.\"Your data stays with you, period,\" said Parth Sareen, a software engineer who demonstrated the tool during a recent livestream. The comment captures the core appeal: Goose gives developers complete control over their AI-powered workflow, including the ability to work offline — even on an airplane.The project has exploded in popularity. Goose now boasts more than 26,100 stars on GitHub, the code-sharing platform, with 362 contributors and 102 releases since its launch. The latest version, 1.20.1, shipped on January 19, 2026, reflecting a development pace that rivals commercial products.For developers frustrated by Claude Code&#x27;s pricing structure and usage caps, Goose represents something increasingly rare in the AI industry: a genuinely free, no-strings-attached option for serious work.Anthropic&#x27;s new rate limits spark a developer revoltTo understand why Goose matters, you need to understand the Claude Code pricing controversy.Anthropic, the San Francisco artificial intelligence company founded by former OpenAI executives, offers Claude Code as part of its subscription tiers. The free plan provides no access whatsoever. The Pro plan, at $17 per month with annual billing (or $20 monthly), limits users to just 10 to 40 prompts every five hours — a constraint that serious developers exhaust within minutes of intensive work.The Max plans, at $100 and $200 per month, offer more headroom: 50 to 200 prompts and 200 to 800 prompts respectively, plus access to Anthropic&#x27;s most powerful model, Claude 4.5 Opus. But even these premium tiers come with restrictions that have inflamed the developer community.In late July, Anthropic announced new weekly rate limits. Under the system, Pro users receive 40 to 80 hours of Sonnet 4 usage per week. Max users at the $200 tier get 240 to 480 hours of Sonnet 4, plus 24 to 40 hours of Opus 4. Nearly five months later, the frustration has not subsided.The problem? Those \"hours\" are not actual hours. They represent token-based limits that vary wildly depending on codebase size, conversation length, and the complexity of the code being processed. Independent analysis suggests the actual per-session limits translate to roughly 44,000 tokens for Pro users and 220,000 tokens for the $200 Max plan.\"It&#x27;s confusing and vague,\" one developer wrote in a widely shared analysis. \"When they say &#x27;24-40 hours of Opus 4,&#x27; that doesn&#x27;t really tell you anything useful about what you&#x27;re actually getting.\"The backlash on Reddit and developer forums has been fierce. Some users report hitting their daily limits within 30 minutes of intensive coding. Others have canceled their subscriptions entirely, calling the new restrictions \"a joke\" and \"unusable for real work.\"Anthropic has defended the changes, stating that the limits affect fewer than five percent of users and target people running Claude Code \"continuously in the background, 24/7.\" But the company has not clarified whether that figure refers to five percent of Max subscribers or five percent of all users — a distinction that matters enormously.How Block built a free AI coding agent that works offlineGoose takes a radically different approach to the same problem.Built by Block, the payments company led by Jack Dorsey, Goose is what engineers call an \"on-machine AI agent.\" Unlike Claude Code, which sends your queries to Anthropic&#x27;s servers for processing, Goose can run entirely on your local computer using open-source language models that you download and control yourself.The project&#x27;s documentation describes it as going \"beyond code suggestions\" to \"install, execute, edit, and test with any LLM.\" That last phrase — \"any LLM\" — is the key differentiator. Goose is model-agnostic by design.You can connect Goose to Anthropic&#x27;s Claude models if you have API access. You can use OpenAI&#x27;s GPT-5 or Google&#x27;s Gemini. You can route it through services like Groq or OpenRouter. Or — and this is where things get interesting — you can run it entirely locally using tools like Ollama, which let you download and execute open-source models on your own hardware.The practical implications are significant. With a local setup, there are no subscription fees, no usage caps, no rate limits, and no concerns about your code being sent to external servers. Your conversations with the AI never leave your machine.\"I use Ollama all the time on planes — it&#x27;s a lot of fun!\" Sareen noted during a demonstration, highlighting how local models free developers from the constraints of internet connectivity.What Goose can do that traditional code assistants can&#x27;tGoose operates as a command-line tool or desktop application that can autonomously perform complex development tasks. It can build entire projects from scratch, write and execute code, debug failures, orchestrate workflows across multiple files, and interact with external APIs — all without constant human oversight.The architecture relies on what the AI industry calls \"tool calling\" or \"function calling\" — the ability for a language model to request specific actions from external systems. When you ask Goose to create a new file, run a test suite, or check the status of a GitHub pull request, it doesn&#x27;t just generate text describing what should happen. It actually executes those operations.This capability depends heavily on the underlying language model. Claude 4 models from Anthropic currently perform best at tool calling, according to the Berkeley Function-Calling Leaderboard, which ranks models on their ability to translate natural language requests into executable code and system commands.But newer open-source models are catching up quickly. Goose&#x27;s documentation highlights several options with strong tool-calling support: Meta&#x27;s Llama series, Alibaba&#x27;s Qwen models, Google&#x27;s Gemma variants, and DeepSeek&#x27;s reasoning-focused architectures.The tool also integrates with the Model Context Protocol, or MCP, an emerging standard for connecting AI agents to external services. Through MCP, Goose can access databases, search engines, file systems, and third-party APIs — extending its capabilities far beyond what the base language model provides.Setting Up Goose with a Local ModelFor developers interested in a completely free, privacy-preserving setup, the process involves three main components: Goose itself, Ollama (a tool for running open-source models locally), and a compatible language model.Step 1: Install OllamaOllama is an open-source project that dramatically simplifies the process of running large language models on personal hardware. It handles the complex work of downloading, optimizing, and serving models through a simple interface.Download and install Ollama from ollama.com. Once installed, you can pull models with a single command. For coding tasks, Qwen 2.5 offers strong tool-calling support:ollama run qwen2.5The model downloads automatically and begins running on your machine.Step 2: Install GooseGoose is available as both a desktop application and a command-line interface. The desktop version provides a more visual experience, while the CLI appeals to developers who prefer working entirely in the terminal.Installation instructions vary by operating system but generally involve downloading from Goose&#x27;s GitHub releases page or using a package manager. Block provides pre-built binaries for macOS (both Intel and Apple Silicon), Windows, and Linux.Step 3: Configure the ConnectionIn Goose Desktop, navigate to Settings, then Configure Provider, and select Ollama. Confirm that the API Host is set to http://localhost:11434 (Ollama&#x27;s default port) and click Submit.For the command-line version, run goose configure, select \"Configure Providers,\" choose Ollama, and enter the model name when prompted.That&#x27;s it. Goose is now connected to a language model running entirely on your hardware, ready to execute complex coding tasks without any subscription fees or external dependencies.The RAM, processing power, and trade-offs you should know aboutThe obvious question: what kind of computer do you need?Running large language models locally requires substantially more computational resources than typical software. The key constraint is memory — specifically, RAM on most systems, or VRAM if using a dedicated graphics card for acceleration.Block&#x27;s documentation suggests that 32 gigabytes of RAM provides \"a solid baseline for larger models and outputs.\" For Mac users, this means the computer&#x27;s unified memory is the primary bottleneck. For Windows and Linux users with discrete NVIDIA graphics cards, GPU memory (VRAM) matters more for acceleration.But you don&#x27;t necessarily need expensive hardware to get started. Smaller models with fewer parameters run on much more modest systems. Qwen 2.5, for instance, comes in multiple sizes, and the smaller variants can operate effectively on machines with 16 gigabytes of RAM.\"You don&#x27;t need to run the largest models to get excellent results,\" Sareen emphasized. The practical recommendation: start with a smaller model to test your workflow, then scale up as needed.For context, Apple&#x27;s entry-level MacBook Air with 8 gigabytes of RAM would struggle with most capable coding models. But a MacBook Pro with 32 gigabytes — increasingly common among professional developers — handles them comfortably.Why keeping your code off the cloud matters more than everGoose with a local LLM is not a perfect substitute for Claude Code. The comparison involves real trade-offs that developers should understand.Model Quality: Claude 4.5 Opus, Anthropic&#x27;s flagship model, remains arguably the most capable AI for software engineering tasks. It excels at understanding complex codebases, following nuanced instructions, and producing high-quality code on the first attempt. Open-source models have improved dramatically, but a gap persists — particularly for the most challenging tasks.One developer who switched to the $200 Claude Code plan described the difference bluntly: \"When I say &#x27;make this look modern,&#x27; Opus knows what I mean. Other models give me Bootstrap circa 2015.\"Context Window: Claude Sonnet 4.5, accessible through the API, offers a massive one-million-token context window — enough to load entire large codebases without chunking or context management issues. Most local models are limited to 4,096 or 8,192 tokens by default, though many can be configured for longer contexts at the cost of increased memory usage and slower processing.Speed: Cloud-based services like Claude Code run on dedicated server hardware optimized for AI inference. Local models, running on consumer laptops, typically process requests more slowly. The difference matters for iterative workflows where you&#x27;re making rapid changes and waiting for AI feedback.Tooling Maturity: Claude Code benefits from Anthropic&#x27;s dedicated engineering resources. Features like prompt caching (which can reduce costs by up to 90 percent for repeated contexts) and structured outputs are polished and well-documented. Goose, while actively developed with 102 releases to date, relies on community contributions and may lack equivalent refinement in specific areas.How Goose stacks up against Cursor, GitHub Copilot, and the paid AI coding marketGoose enters a crowded market of AI coding tools, but occupies a distinctive position.Cursor, a popular AI-enhanced code editor, charges $20 per month for its Pro tier and $200 for Ultra—pricing that mirrors Claude Code&#x27;s Max plans. Cursor provides approximately 4,500 Sonnet 4 requests per month at the Ultra level, a substantially different allocation model than Claude Code&#x27;s hourly resets.Cline, Roo Code, and similar open-source projects offer AI coding assistance but with varying levels of autonomy and tool integration. Many focus on code completion rather than the agentic task execution that defines Goose and Claude Code.Amazon&#x27;s CodeWhisperer, GitHub Copilot, and enterprise offerings from major cloud providers target large organizations with complex procurement processes and dedicated budgets. They are less relevant to individual developers and small teams seeking lightweight, flexible tools.Goose&#x27;s combination of genuine autonomy, model agnosticism, local operation, and zero cost creates a unique value proposition. The tool is not trying to compete with commercial offerings on polish or model quality. It&#x27;s competing on freedom — both financial and architectural.The $200-a-month era for AI coding tools may be endingThe AI coding tools market is evolving quickly. Open-source models are improving at a pace that continually narrows the gap with proprietary alternatives. Moonshot AI&#x27;s Kimi K2 and z.ai&#x27;s GLM 4.5 now benchmark near Claude Sonnet 4 levels — and they&#x27;re freely available.If this trajectory continues, the quality advantage that justifies Claude Code&#x27;s premium pricing may erode. Anthropic would then face pressure to compete on features, user experience, and integration rather than raw model capability.For now, developers face a clear choice. Those who need the absolute best model quality, who can afford premium pricing, and who accept usage restrictions may prefer Claude Code. Those who prioritize cost, privacy, offline access, and flexibility have a genuine alternative in Goose.The fact that a $200-per-month commercial product has a zero-dollar open-source competitor with comparable core functionality is itself remarkable. It reflects both the maturation of open-source AI infrastructure and the appetite among developers for tools that respect their autonomy.Goose is not perfect. It requires more technical setup than commercial alternatives. It depends on hardware resources that not every developer possesses. Its model options, while improving rapidly, still trail the best proprietary offerings on complex tasks.But for a growing community of developers, those limitations are acceptable trade-offs for something increasingly rare in the AI landscape: a tool that truly belongs to them.Goose is available for download at github.com/block/goose. Ollama is available at ollama.com. Both projects are free and open source.",
          "feed_position": 2,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/1U9H8GLIqoGqKpitVfgw3T/ba56292f99409eca709dac0b176ec245/nuneybits_Vector_art_of_white_goose_silhouette_flying_through_c_8100d5a7-9e36-4ed6-a188-016470e1d0e1.webp?w=300&q=30"
        }
      ],
      "featured_image": "https://images.ctfassets.net/jdtwqhzvc2n1/5Ybr6R81VAO6yoTI3fLOlG/9a42cc8ccddba84b036b42d27f769789/Recursive_language_model.jpg?w=300&q=30",
      "popularity_score": 2009.01526
    },
    {
      "id": "cluster_40",
      "coverage": 1,
      "updated_at": "Tue, 20 Jan 2026 23:33:57 +0000",
      "title": "Webb reveals a planetary nebula with phenomenal clarity, and it is spectacular",
      "neutral_headline": "Webb reveals a planetary nebula with phenomenal clarity, and it is spectacular",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2026/01/webb-has-given-us-with-a-stunning-new-view-of-a-well-known-planetary-nebula/",
          "published_at": "Tue, 20 Jan 2026 23:33:57 +0000",
          "title": "Webb reveals a planetary nebula with phenomenal clarity, and it is spectacular",
          "standfirst": "The colors show the star’s final breath transforming into the raw ingredients for new worlds.",
          "content": "The Helix Nebula is one of the most well-known and commonly photographed planetary nebulae because it resembles the \"Eye of Sauron.\" It is also one of the closest bright nebulae to Earth, located approximately 655 light-years from our Solar System. You may not know what this particular nebula looks like when reading its name, but the Hubble Space Telescope has taken some iconic images of it over the years. And almost certainly, you'll recognize a photograph of the Helix Nebula, shown below. Like many objects in astronomy, planetary nebulae have a confusing name, since they are formed not by planets but by stars like our own Sun, though a little larger. Near the end of their lives, these stars shed large amounts of gas in an expanding shell that, however briefly in cosmological time, put on a grand show.Read full article Comments",
          "feed_position": 0,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/STScI-01KCMAMZ8ZSMRTR9KKD8Y6EMRJ-1152x648.png"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/STScI-01KCMAMZ8ZSMRTR9KKD8Y6EMRJ-1152x648.png",
      "popularity_score": 351.08109333333334
    },
    {
      "id": "cluster_46",
      "coverage": 1,
      "updated_at": "Tue, 20 Jan 2026 22:35:32 +0000",
      "title": "Verizon starts requiring 365 days of paid service before it will unlock phones",
      "neutral_headline": "Verizon starts requiring 365 days of paid service before it will unlock phones",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2026/01/verizon-starts-requiring-365-days-of-paid-service-before-it-will-unlock-phones/",
          "published_at": "Tue, 20 Jan 2026 22:35:32 +0000",
          "title": "Verizon starts requiring 365 days of paid service before it will unlock phones",
          "standfirst": "Verizon changed prepaid brands' policy a week after FCC waived unlocking rule.",
          "content": "Verizon has started enforcing a 365-day lock period on phones purchased through its TracFone division, one week after the Federal Communications Commission waived a requirement that Verizon unlock handsets 60 days after they are activated on its network. Verizon was previously required to unlock phones automatically after 60 days due to restrictions imposed on its spectrum licenses and merger conditions that helped Verizon obtain approval of its purchase of TracFone. But an update applied today to the TracFone unlocking policy said new phones will be locked for at least a year and that each customer will have to request an unlock instead of getting it automatically. The \"new\" TracFone policy is basically a return to the yearlong locking it imposed before Verizon bought the company in 2021. TracFone first agreed to provide unlocking in a 2015 settlement with the Obama-era FCC, which alleged that TracFone failed to comply with a commitment to unlock phones for customers enrolled in the Lifeline subsidy program. TracFone later shortened the locking period from a year to 60 days as a condition of the Verizon merger.Read full article Comments",
          "feed_position": 2,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/07/verizon-1152x648-1752087139.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/07/verizon-1152x648-1752087139.jpg",
      "popularity_score": 339.10748222222225
    },
    {
      "id": "cluster_42",
      "coverage": 1,
      "updated_at": "Tue, 20 Jan 2026 23:22:26 +0000",
      "title": "Zuck stuck on Trump’s bad side: FTC appeals loss in Meta monopoly case",
      "neutral_headline": "Zuck stuck on Trump’s bad side: FTC appeals loss in Meta monopoly case",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2026/01/zuck-stuck-on-trumps-bad-side-ftc-appeals-loss-in-meta-monopoly-case/",
          "published_at": "Tue, 20 Jan 2026 23:22:26 +0000",
          "title": "Zuck stuck on Trump’s bad side: FTC appeals loss in Meta monopoly case",
          "standfirst": "FTC will appeal ruling that found Meta has no monopoly in social networking.",
          "content": "Still feeling uneasy about Meta's acquisition of Instagram in 2012 and WhatsApp in 2014, the Federal Trade Commission will be appealing a November ruling that cleared Meta of allegations that it holds an illegal monopoly in a market dubbed \"personal social networking.\" The FTC hopes the US Court of Appeals for the District of Columbia will agree that \"robust evidence at trial\" showed that Meta's acquisitions were improper. In the initial trial, the FTC sought a breakup of Meta's apps, with Meta risking forced divestments of Instagram or WhatsApp. In a press release Tuesday, the FTC confirmed that it \"continues to allege\" that \"for over a decade Meta has illegally maintained a monopoly in personal social networking services through anticompetitive conduct—by buying the significant competitive threats it identified in Instagram and WhatsApp.\"Read full article Comments",
          "feed_position": 1,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/05/zuck-the-monopolist-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/05/zuck-the-monopolist-1152x648.jpg",
      "popularity_score": 334.8891488888889
    },
    {
      "id": "cluster_50",
      "coverage": 1,
      "updated_at": "Tue, 20 Jan 2026 22:17:02 +0000",
      "title": "Google temporarily disabled YouTube's advanced captions without warning",
      "neutral_headline": "Google temporarily disabled YouTube's advanced captions without warning",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2026/01/google-temporarily-disabled-youtubes-advanced-captions-without-warning/",
          "published_at": "Tue, 20 Jan 2026 22:17:02 +0000",
          "title": "Google temporarily disabled YouTube's advanced captions without warning",
          "standfirst": "Google says SRV3 captions were causing playback errors, so it has \"temporarily\" disabled them.",
          "content": "YouTubers have been increasingly frustrated with Google's management of the platform, with disinformation welcomed back and an aggressive push for more AI (except where Google doesn't like it). So it's no surprise that creators have been up in arms over the suspicious removal of YouTube's advanced SRV3 caption format. You don't have to worry too much just yet—Google says this is only temporary, and it's working on a fix for the underlying bug. Google added support for this custom subtitle format around 2018, giving creators more customization options than with traditional captions. SRV3 (also known as YTT or YouTube Timed Text) allows for custom colors, transparency, animations, fonts, and precise positioning in videos. Uploaders using this format can color-code and position captions to help separate multiple speakers, create sing-along animations, or style them to match the video. Over the last several days, creators who've become accustomed to this level of control have been dismayed to see that YouTube is no longer accepting videos with this Google-created format. Many worried Google had ditched the format entirely, which could be problematic for all those previously uploaded videos.Read full article Comments",
          "feed_position": 3,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2024/06/youtube-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2024/06/youtube-1152x648.jpg",
      "popularity_score": 325.7991488888889
    },
    {
      "id": "cluster_52",
      "coverage": 1,
      "updated_at": "Tue, 20 Jan 2026 21:59:38 +0000",
      "title": "Flesh-eating flies are eating their way through Mexico, CDC warns",
      "neutral_headline": "Flesh-eating flies are eating their way through Mexico, CDC warns",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/health/2026/01/be-on-the-lookout-for-flesh-eating-flies-cdc-tells-clinicians-in-alert/",
          "published_at": "Tue, 20 Jan 2026 21:59:38 +0000",
          "title": "Flesh-eating flies are eating their way through Mexico, CDC warns",
          "standfirst": "Eight animal cases in Mexico's Tamaulipas spur CDC to warn doctors of festering wounds.",
          "content": "The US Centers for Disease Control and Prevention issued a health alert to clinicians Tuesday, warning that the savage, flesh-eating parasitic fly—the New World Screwworm—is not only approaching the Texas border, but also felling an increasing number of animals in the bordering Mexican state of Tamaulipas. The advisory, released through the agency's Health Alert Network, directs doctors, veterinarians, and other health workers to be on the lookout for patients with wounds teeming with ferocious maggots burrowing into their living flesh. The alert also provides guidance on what to do if any such festering wounds are encountered—namely, remove each and every maggot to prevent the patient from dying, and, under no circumstance allow any of the parasites to survive and escape. The New World Screwworm (NWS) is a fly that lays its eggs—up to 400 at a time—in the wounds, orifices, and mucus membranes of any warm-blooded animal. The eggs hatch into flesh-eating maggots, which look and act much like screws, twisting and boring into their victims while eating them alive.Read full article Comments",
          "feed_position": 4,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2016/11/CSIRO_ScienceImage_115_The_Tip_of_a_Screw_Worm_Fly_Larvae.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2016/11/CSIRO_ScienceImage_115_The_Tip_of_a_Screw_Worm_Fly_Larvae.jpg",
      "popularity_score": 303.5091488888889
    },
    {
      "id": "cluster_56",
      "coverage": 1,
      "updated_at": "Tue, 20 Jan 2026 21:25:30 +0000",
      "title": "Macaque facial gestures are more than just a reflex, study finds",
      "neutral_headline": "Macaque facial gestures are more than just a reflex, study finds",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2026/01/macaque-facial-gestures-are-more-than-just-a-reflex-study-finds/",
          "published_at": "Tue, 20 Jan 2026 21:25:30 +0000",
          "title": "Macaque facial gestures are more than just a reflex, study finds",
          "standfirst": "Study is first to implant micro-electrode arrays to record neurons as they produce facial gestures.",
          "content": "Recent advances in brain-computer interfaces have made it possible to more accurately extract speech from neural signals in humans, but language is just one of the tools we use to communicate. “When my young nephew asks for ice cream before dinner and I say ‘no,’ the meaning is entirely dictated by whether the word is punctuated with a smirk or a stern frown,” says Geena Ianni, a neuroscientist at the University of Pennsylvania. That’s why in the future, she thinks, neural prostheses meant for patients with a stroke or paralysis will decode facial gestures from brain signals in the same way they decode speech. To lay a foundation for these future facial gesture decoders, Ianni and her colleagues designed an experiment to find out how neural circuitry responsible for making faces really works. “Although in recent years neuroscience got a good handle on how the brain perceives facial expressions, we know relatively little about how they are generated,” Ianni says. And it turned out that a surprisingly large part of what neuroscientists assumed about facial gestures was wrong. The natural way For a long time, neuroscientists thought facial gestures in primates stemmed from a neat division of labor in the brain. “Case reports of patients with brain lesions suggested some brain regions were responsible for certain types of emotional expressions while other regions were responsible for volitional movements like speech,” Ianni explains. We’ve developed a clearer picture of speech by tracing the origin of these movements down to the level of individual neurons. But we’ve not done the same for facial expressions. To fill this gap, Ianni and her team designed a study using macaques—social primates that share most of their complex facial musculature with humans.Read full article Comments",
          "feed_position": 5,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2243934190-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2243934190-1152x648.jpg",
      "popularity_score": 292.94026
    },
    {
      "id": "cluster_67",
      "coverage": 1,
      "updated_at": "Tue, 20 Jan 2026 19:24:12 +0000",
      "title": "Netflix to pay all cash for Warner Bros. to fend off Paramount hostile takeover",
      "neutral_headline": "Netflix to pay all cash for Warner Bros. to fend off Paramount hostile takeover",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2026/01/netflix-to-pay-all-cash-for-warner-bros-to-fend-off-paramount-hostile-takeover/",
          "published_at": "Tue, 20 Jan 2026 19:24:12 +0000",
          "title": "Netflix to pay all cash for Warner Bros. to fend off Paramount hostile takeover",
          "standfirst": "Netflix and Warner seek quick shareholder vote as Paramount tries to upend deal.",
          "content": "Netflix agreed to pay all cash for Warner Bros. Discovery, amending its $72 billion deal in an attempt to fight off Paramount's hostile takeover bid. Netflix originally agreed to buy the company with a mix of cash and stock. To sweeten the offer for shareholders, Netflix and Warner Bros. today announced that Netflix will pay all cash instead. If successful, Netflix's purchase will include HBO Max, WB Studios, and other assets. The price is unchanged at $27.75 per share, and Warner Bros. is targeting an April 2026 shareholder vote. The original plan was for Netflix to buy each Warner Bros. share with $23.25 in cash and $4.50 in Netflix stock.Read full article Comments",
          "feed_position": 6,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/netflix-paramount-wb-icons-1152x648-1767815429.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/netflix-paramount-wb-icons-1152x648-1767815429.jpg",
      "popularity_score": 280.9185933333333
    },
    {
      "id": "cluster_69",
      "coverage": 1,
      "updated_at": "Tue, 20 Jan 2026 18:58:22 +0000",
      "title": "Sony is giving TCL control over its high-end Bravia TVs",
      "neutral_headline": "Sony is giving TCL control over its high-end Bravia TVs",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2026/01/tcl-to-gain-majority-ownership-over-sonys-bravia-tvs/",
          "published_at": "Tue, 20 Jan 2026 18:58:22 +0000",
          "title": "Sony is giving TCL control over its high-end Bravia TVs",
          "standfirst": "TCL will own 51 percent of the high-end TVs.",
          "content": "TCL is taking majority ownership of Sony’s Bravia series of TVs, the two companies announced today. The two firms said they have signed a memorandum of understanding and aim to sign binding agreements by the end of March. Pending “relevant regulatory approvals and other conditions,” the joint venture is expected to launch in April 2027. Under a new joint venture, Huizhou, China-headquartered TCL will own 51 percent of Tokyo, Japan-headquartered Sony’s “home entertainment business,” and Sony will own 49 percent, per an announcement today, adding:Read full article Comments",
          "feed_position": 7,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-1236174910-1152x648-1768934089.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-1236174910-1152x648-1768934089.jpg",
      "popularity_score": 270.4880377777778
    },
    {
      "id": "cluster_89",
      "coverage": 1,
      "updated_at": "Tue, 20 Jan 2026 15:00:04 +0000",
      "title": "The first commercial space station, Haven-1, is now undergoing assembly for launch",
      "neutral_headline": "The first commercial space station, Haven-1, is now undergoing assembly for launch",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2026/01/the-first-commercial-space-station-haven-1-is-now-undergoing-assembly-for-launch/",
          "published_at": "Tue, 20 Jan 2026 15:00:04 +0000",
          "title": "The first commercial space station, Haven-1, is now undergoing assembly for launch",
          "standfirst": "\"We have a very strong incentive to send a crew as quickly as we can safely do so.\"",
          "content": "As Ars reported last week, NASA's plan to replace the International Space Station with commercial space stations is running into a time crunch. The sprawling International Space Station is due to be decommissioned less than five years from now, and the US space agency has yet to formally publish rules and requirements for the follow-on stations being designed and developed by several different private companies. Although there are expected to be multiple bidders in \"phase two\" of NASA's commercial space station program, there are at present four main contenders: Voyager Technologies, Axiom Space, Blue Origin, and Vast Space. At some point later this year, the space agency is expected to select one, or more likely two, of these companies for larger contracts that will support their efforts to build their stations.Read full article Comments",
          "feed_position": 8,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/Vast_Haven-1_Integration_Phase-1_ISO8_Cleanroom_Spring_2026-01-15-DSC_1375-3-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/Vast_Haven-1_Integration_Phase-1_ISO8_Cleanroom_Spring_2026-01-15-DSC_1375-3-1152x648.jpg",
      "popularity_score": 264.5163711111111
    },
    {
      "id": "cluster_108",
      "coverage": 1,
      "updated_at": "Mon, 19 Jan 2026 22:01:52 +0000",
      "title": "The fastest human spaceflight mission in history crawls closer to liftoff",
      "neutral_headline": "The fastest human spaceflight mission in history crawls closer to liftoff",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2026/01/nasas-artemis-ii-rocket-rolls-to-launch-pad-but-key-test-looms-ahead/",
          "published_at": "Mon, 19 Jan 2026 22:01:52 +0000",
          "title": "The fastest human spaceflight mission in history crawls closer to liftoff",
          "standfirst": "After a remarkably smooth launch campaign, Artemis II reached its last stop before the Moon.",
          "content": "KENNEDY SPACE CENTER, Florida—Preparations for the first human spaceflight to the Moon in more than 50 years took a big step forward this weekend with the rollout of the Artemis II rocket to its launch pad. The rocket reached a top speed of just 1 mph on the four-mile, 12-hour journey from the Vehicle Assembly Building to Launch Complex 39B at NASA's Kennedy Space Center in Florida. At the end of its nearly 10-day tour through cislunar space, the Orion capsule on top of the rocket will exceed 25,000 mph as it plunges into the atmosphere to bring its four-person crew back to Earth. \"This is the start of a very long journey,\" said NASA Administrator Jared Isaacman. \"We ended our last human exploration of the moon on Apollo 17.\"Read full article Comments",
          "feed_position": 9,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/IMG_0127-1-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/IMG_0127-1-1152x648.jpg",
      "popularity_score": 256
    },
    {
      "id": "cluster_112",
      "coverage": 1,
      "updated_at": "Mon, 19 Jan 2026 19:04:18 +0000",
      "title": "Elon Musk accused of making up math to squeeze $134B from OpenAI, Microsoft",
      "neutral_headline": "Elon Musk accused of making up math to squeeze $134B from OpenAI, Microsoft",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2026/01/elon-musk-accused-of-making-up-math-to-squeeze-134b-from-openai-microsoft/",
          "published_at": "Mon, 19 Jan 2026 19:04:18 +0000",
          "title": "Elon Musk accused of making up math to squeeze $134B from OpenAI, Microsoft",
          "standfirst": "Musk's math reduced ChatGPT inventors' contributions to \"zero,\" OpenAI argued.",
          "content": "Elon Musk is going for some substantial damages in his lawsuit accusing OpenAI of abandoning its nonprofit mission and \"making a fool out of him\" as an early investor. On Friday, Musk filed a notice on remedies sought in the lawsuit, confirming that he's seeking damages between $79 billion and $134 billion from OpenAI and its largest backer, co-defendant Microsoft. Musk hired an expert he has never used before, C. Paul Wazzan, who reached this estimate by concluding that Musk's early contributions to OpenAI generated 50 to 75 percent of the nonprofit's current value. He got there by analyzing four factors: Musk's total financial contributions before he left OpenAI in 2018, Musk's proposed equity stake in OpenAI in 2017, Musk's current equity stake in xAI, and Musk's nonmonetary contributions to OpenAI (like investing time or lending his reputation).Read full article Comments",
          "feed_position": 12,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2087343447-1024x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2087343447-1024x648.jpg",
      "popularity_score": 160
    },
    {
      "id": "cluster_123",
      "coverage": 1,
      "updated_at": "Mon, 19 Jan 2026 12:00:45 +0000",
      "title": "10 things I learned from burning myself out with AI coding agents",
      "neutral_headline": "10 things I learned from burning myself out with AI coding agents",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/information-technology/2026/01/10-things-i-learned-from-burning-myself-out-with-ai-coding-agents/",
          "published_at": "Mon, 19 Jan 2026 12:00:45 +0000",
          "title": "10 things I learned from burning myself out with AI coding agents",
          "standfirst": "Opinion: As software power tools, AI agents may make people busier than ever before.",
          "content": "If you've ever used a 3D printer, you may recall the wondrous feeling when you first printed something you could have never sculpted or built yourself. Download a model file, load some plastic filament, push a button, and almost like magic, a three-dimensional object appears. But the result isn't polished and ready for mass production, and creating a novel shape requires more skills than just pushing a button. Interestingly, today's AI coding agents feel much the same way. Since November, I have used Claude Code and Claude Opus 4.5 through a personal Claude Max account to extensively experiment with AI-assisted software development (I have also used OpenAI's Codex in a similar way, though not as frequently). Fifty projects later, I'll be frank: I have not had this much fun with a computer since I learned BASIC on my Apple II Plus when I was 9 years old. This opinion comes not as an endorsement but as personal experience: I voluntarily undertook this project, and I paid out of pocket for both OpenAI and Anthropic's premium AI plans. Throughout my life, I have dabbled in programming as a utilitarian coder, writing small tools or scripts when needed. In my web development career, I wrote some small tools from scratch, but I primarily modified other people's code for my needs. Since 1990, I've programmed in BASIC, C, Visual Basic, PHP, ASP, Perl, Python, Ruby, MUSHcode, and some others. I am not an expert in any of these languages—I learned just enough to get the job done. I have developed my own hobby games over the years using BASIC, Torque Game Engine, and Godot, so I have some idea of what makes a good architecture for a modular program that can be expanded over time.Read full article Comments",
          "feed_position": 17,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/super-programmer-hes-heating-up-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/super-programmer-hes-heating-up-1152x648.jpg",
      "popularity_score": 148
    },
    {
      "id": "cluster_115",
      "coverage": 1,
      "updated_at": "Mon, 19 Jan 2026 17:16:38 +0000",
      "title": "Reports of ad-supported Xbox game streams show Microsoft's lack of imagination",
      "neutral_headline": "Reports of ad-supported Xbox game streams show Microsoft's lack of imagination",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gaming/2026/01/reports-of-ad-supported-xbox-game-streams-show-microsofts-lack-of-imagination/",
          "published_at": "Mon, 19 Jan 2026 17:16:38 +0000",
          "title": "Reports of ad-supported Xbox game streams show Microsoft's lack of imagination",
          "standfirst": "Xbox maker needs some fresher ideas for expanding access to cloud gaming.",
          "content": "Currently, Microsoft's long-running Cloud Gaming service is limited to players who have a Microsoft's Game Pass subscription. Now, new reporting suggests Microsoft is planning to offer non-subscribers access to game streams paid for by advertising in the near future, but only in extremely limited circumstances. The latest wave of rumors was set off late last week when The Verge's Tom Warren shared an Xbox Cloud Gaming loading screen with a message mentioning \"1 hour of ad supported playtime per session.\" That leaked message comes after Windows Central reported last summer that Microsoft has been \"exploring video ads for free games for quite some time,\" à la the two-minute sponsorships that appear before free-tier game streams on Nvidia's GeForce Now service. Don't get your hopes up for easy, free, ad-supported access to the entire Xbox Cloud Gaming library, though. Windows Central now reports that Microsoft will be using ads merely to slightly expand access to its \"Stream your own game\" program. That program currently offers subscribers to the Xbox Game Pass Essentials tier (or higher) the privilege of streaming versions of some of the Xbox games they've already purchased digitally. Windows Central's unnamed sources suggest that a \"session-based ad-supported access tier\" to stream those purchased games will be offered to non-subscribers as soon as \"this year.\"Read full article Comments",
          "feed_position": 14,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2021/06/Cloud-Gaming_iPadSurfaceiPhone-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2021/06/Cloud-Gaming_iPadSurfaceiPhone-1152x648.jpg",
      "popularity_score": 145
    },
    {
      "id": "cluster_109",
      "coverage": 1,
      "updated_at": "Mon, 19 Jan 2026 21:07:55 +0000",
      "title": "The first new Marathon game in decades will launch on March 5",
      "neutral_headline": "The first new Marathon game in decades will launch on March 5",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gaming/2026/01/bungies-delayed-marathon-revival-will-finally-launch-march-5/",
          "published_at": "Mon, 19 Jan 2026 21:07:55 +0000",
          "title": "The first new Marathon game in decades will launch on March 5",
          "standfirst": "Development hasn't exactly been smooth since the extraction shooter's 2023 announcement.",
          "content": "It has been nearly three years now since Destiny maker (and Sony subsidiary) Bungie formally announced a revival of the storied Marathon FPS franchise. And it has been about seven months since the game's originally announced release date of September 23, 2025, was pushed back indefinitely after a reportedly poor response to the game's first Alpha test. But today, in a post on the PlayStation Blog, Bungie revealed that the new Marathon would finally be hitting PS5, Windows, and Xbox Series X|S on March 5, narrowing down the monthlong March release window announced back in December. Today's preorder trailer revealing the Marathon release date. Unlike Destiny 2, which transitioned to a free-to-play model in 2019, the new Marathon sells a Standard Edition for $40 or a $60 Deluxe Edition that includes some digital rewards and cosmetics. That mirrors the pricing of the somewhat similar Arc Raiders, which recently hit 12 million sales in less than 12 weeks.Read full article Comments",
          "feed_position": 10,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/marathon-1152x648.png"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/marathon-1152x648.png",
      "popularity_score": 133
    },
    {
      "id": "cluster_110",
      "coverage": 1,
      "updated_at": "Mon, 19 Jan 2026 19:52:22 +0000",
      "title": "Signs point to a sooner-rather-than-later M5 MacBook Pro refresh",
      "neutral_headline": "Signs point to a sooner-rather-than-later M5 MacBook Pro refresh",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2026/01/signs-point-to-a-sooner-rather-than-later-m5-macbook-pro-refresh/",
          "published_at": "Mon, 19 Jan 2026 19:52:22 +0000",
          "title": "Signs point to a sooner-rather-than-later M5 MacBook Pro refresh",
          "standfirst": "Delayed shipping times for current models sometimes means an update is imminent.",
          "content": "Mac power users waiting on new high-end MacBook Pro models may have been disappointed last fall when Apple released an M5 upgrade for the low-end 14-inch MacBook Pro without touching the M4 Pro or Max versions of the laptop. But the wait for M5 Pro and M5 Max models may be nearing its end. The tea-leaf readers at MacRumors noticed that shipping times for a handful of high-end MacBook Pro configurations have slipped into mid-to-late February, rather than being available immediately as most Mac models are. This is often, though not always, a sign that Apple has slowed down or stopped production of an existing product in anticipation of an update. Currently, the shipping delays affect the M4 Max versions of both the 14-inch and 16-inch MacBook Pros. If you order them today, these models will arrive sometime between February 3 and February 24, depending on the configuration you choose; many M4 Pro versions are still available for same-day shipping, though adding a nano-texture display or upgrading RAM can still add a week or so to the shipping time.Read full article Comments",
          "feed_position": 11,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/IMG_6215-1152x648.jpeg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/IMG_6215-1152x648.jpeg",
      "popularity_score": 133
    },
    {
      "id": "cluster_113",
      "coverage": 1,
      "updated_at": "Mon, 19 Jan 2026 18:24:49 +0000",
      "title": "Asus confirms its smartphone business is on indefinite hiatus",
      "neutral_headline": "Asus confirms its smartphone business is on indefinite hiatus",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2026/01/asus-confirms-its-smartphone-business-is-on-indefinite-hiatus/",
          "published_at": "Mon, 19 Jan 2026 18:24:49 +0000",
          "title": "Asus confirms its smartphone business is on indefinite hiatus",
          "standfirst": "Asus chairman Jonney Shih sees AI applications as the company's main focus going forward.",
          "content": "An unconfirmed report early this month suggested Asus was pulling back on its smartphone plans, but the company declined to comment at the time. Asus chairman Jonney Shih has now confirmed during an event in Taiwan the wind-down of its smartphone business. Instead, Asus will focus on AI products like robots and smart glasses. Shih addressed the company's future plans during a 2026 kick-off event in Taiwan, as reported by Inside. \"Asus will no longer add new mobile phone models in the future,\" said Shih (machine translated). So don't expect a new Zenfone or ROG Phone from Asus in 2026. That said, very few phone buyers were keeping tabs on the latest Asus phones anyway, which is probably why Asus is throwing in the towel. Shih isn't saying Asus won't ever release a new phone, but the company will take an \"indefinite wait-and-see\" approach. Again, this is a translation and could be interpreted in multiple ways.Read full article Comments",
          "feed_position": 13,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/ROG-2-1-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/ROG-2-1-1152x648.jpg",
      "popularity_score": 133
    },
    {
      "id": "cluster_116",
      "coverage": 1,
      "updated_at": "Mon, 19 Jan 2026 17:06:28 +0000",
      "title": "The race to build a super-large ground telescope is likely down to two competitors",
      "neutral_headline": "The race to build a super-large ground telescope is likely down to two competitors",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2026/01/as-europes-large-ground-telescope-project-advances-how-is-its-us-competitor-faring/",
          "published_at": "Mon, 19 Jan 2026 17:06:28 +0000",
          "title": "The race to build a super-large ground telescope is likely down to two competitors",
          "standfirst": "Ars checks in with the new president of the Giant Magellan Telescope.",
          "content": "I have been writing about the Giant Magellan Telescope for a long time. Nearly two decades ago, for example, I wrote that time was \"running out\" in the race to build the next great optical telescope on the ground. At the time the proposed telescope was one of three contenders to make a giant leap in mirror size from the roughly 10-meter-diameter instruments that existed then, to approximately 30 meters. This represented a huge increase in light-gathering potential, allowing astronomers to see much further into the universe—and therefore back into time—with far greater clarity. Since then the projects have advanced at various rates. An international consortium to build the Thirty Meter Telescope in Hawaii ran into local protests that have bogged down development. Its future came further into question when the US National Science Foundation dropped support for the project in favor of the Giant Magellan Telescope. Meanwhile the European Extremely Large Telescope (ELT) has advanced on a faster schedule, and this 39.5-meter telescope could observe its first light in 2029.Read full article Comments",
          "feed_position": 15,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/GMT-Rendering-IDOM-Enclosure-2024-1-scaled-1-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/GMT-Rendering-IDOM-Enclosure-2024-1-scaled-1-1152x648.jpg",
      "popularity_score": 133
    },
    {
      "id": "cluster_118",
      "coverage": 1,
      "updated_at": "Mon, 19 Jan 2026 16:00:27 +0000",
      "title": "Meet Veronika, the tool-using cow",
      "neutral_headline": "Meet Veronika, the tool-using cow",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2026/01/meet-veronika-the-tool-using-cow/",
          "published_at": "Mon, 19 Jan 2026 16:00:27 +0000",
          "title": "Meet Veronika, the tool-using cow",
          "standfirst": "Veronika uses sticks to scratch herself, suggesting scientists have underestimated cow cognition.",
          "content": "Far Side fans might recall a classic 1982 cartoon called \"Cow Tools,\" featuring a cow standing next to a jumble of strange objects—the joke being that cows don't use tools. That's why a pet Swiss brown cow in Austria named Veronika has caused a bit of a sensation: she likes to pick up random sticks and use them to scratch herself. According to a new paper published in the journal Current Biology, this is a form of multipurpose tool use and suggests that the cognitive capabilities of cows have been underestimated by scientists. As previously reported, tool use was once thought to be one of the defining features of humans, but examples of it were eventually observed in primates and other mammals. Dolphins can toss objects as a form of play, which some scientists consider to be a type of tool use, particularly when it involves another member of the same species. Potential purposes include a means of communication, social bonding, or aggressiveness. (Octopuses have also been observed engaging in similar throwing behavior.) But the biggest surprise came when birds were observed using tools in the wild. After all, birds are the only surviving dinosaurs, and mammals and dinosaurs hadn’t shared a common ancestor for hundreds of millions of years. In the wild, observed tool use has been limited to the corvids (crows and jays), which show a variety of other complex behaviors—they’ll remember your face and recognize the passing of their dead.Read full article Comments",
          "feed_position": 16,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/cowtool1-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/cowtool1-1152x648.jpg",
      "popularity_score": 130
    }
  ]
}