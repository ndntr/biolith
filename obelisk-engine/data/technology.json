{
  "updated_at": "2025-12-10T03:55:09.381Z",
  "clusters": [
    {
      "id": "cluster_42",
      "coverage": 3,
      "updated_at": "Tue, 09 Dec 2025 21:08:11 +0000",
      "title": "Big Tech joins forces with Linux Foundation to standardize AI agents",
      "neutral_headline": "Big Tech joins forces with Linux Foundation to standardize AI agents",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/ai/2025/12/big-tech-joins-forces-with-linux-foundation-to-standardize-ai-agents/",
          "published_at": "Tue, 09 Dec 2025 21:08:11 +0000",
          "title": "Big Tech joins forces with Linux Foundation to standardize AI agents",
          "standfirst": "The Agentic AI Foundation launches to support MCP, AGENTS.md, and goose.",
          "content": "Big Tech has spent the past year telling us we’re living in the era of AI agents, but most of what we’ve been promised is still theoretical. As companies race to turn fantasy into reality, they’ve developed a collection of tools to guide the development of generative AI. A cadre of major players in the AI race, including Anthropic, Block, and OpenAI, has come together to promote interoperability with the newly formed Agentic AI Foundation (AAIF). This move elevates a handful of popular technologies and could make them a de facto standard for AI development going forward. The development path for agentic AI models is cloudy to say the least, but companies have invested so heavily in creating these systems that some tools have percolated to the surface. The AAIF, which is part of the nonprofit Linux Foundation, has been launched to govern the development of three key AI technologies: Model Context Protocol (MCP), goose, and AGENTS.md. MCP is probably the most well-known of the trio, having been open-sourced by Anthropic a year ago. The goal of MCP is to link AI agents to data sources in a standardized way—Anthropic (and now the AAIF) is fond of calling MCP a “USB-C port for AI.” Rather than creating custom integrations for every different database or cloud storage platform, MCP allows developers to quickly and easily connect to any MCP-compliant server.Read full article Comments",
          "feed_position": 1,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2024/12/GettyImages-1327016094-1152x648.jpg"
        },
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2025/12/09/openai-anthropic-and-block-join-new-linux-foundation-effort-to-standardize-the-ai-agent-era/",
          "published_at": "Tue, 09 Dec 2025 17:28:36 +0000",
          "title": "OpenAI, Anthropic, and Block join new Linux Foundation effort to standardize the AI agent era",
          "standfirst": "Anthropic, Block, and OpenAI are backing the Linux Foundation’s new Agentic AI Foundation, donating MCP, Goose, and AGENTS.md to standardize AI agents, boost interoperability, and curb proprietary fragmentation.",
          "content": "Anthropic, Block, and OpenAI are backing the Linux Foundation’s new Agentic AI Foundation, donating MCP, Goose, and AGENTS.md to standardize AI agents, boost interoperability, and curb proprietary fragmentation.",
          "feed_position": 17
        },
        {
          "source": "Wired Tech",
          "url": "https://www.wired.com/story/openai-anthropic-and-block-are-teaming-up-on-ai-agent-standards/",
          "published_at": "Tue, 09 Dec 2025 17:06:58 +0000",
          "title": "OpenAI, Anthropic, and Block Are Teaming Up to Make AI Agents Play Nice",
          "standfirst": "American AI giants are backing a new effort to establish open standards for building agentic software and tools.",
          "content": "American AI giants are backing a new effort to establish open standards for building agentic software and tools.",
          "feed_position": 9,
          "image_url": "https://media.wired.com/photos/693854c57eeef7f25b0423c2/master/pass/ai-agents-play-nice-1.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2024/12/GettyImages-1327016094-1152x648.jpg",
      "popularity_score": 3013.217116388889
    },
    {
      "id": "cluster_0",
      "coverage": 2,
      "updated_at": "Wed, 10 Dec 2025 15:00:00 GMT",
      "title": "The AI that scored 95% — until consultants learned it was AI",
      "neutral_headline": "The AI that scored 95% — until consultants learned it was AI",
      "items": [
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/the-ai-that-scored-95-until-consultants-learned-it-was-ai",
          "published_at": "Wed, 10 Dec 2025 15:00:00 GMT",
          "title": "The AI that scored 95% — until consultants learned it was AI",
          "standfirst": "Presented by SAPWhen SAP ran a quiet internal experiment to gauge consultant attitudes toward AI, the results were striking. Five teams were asked to validate answers to more than 1,000 business requirements completed by SAP’s AI co-pilot, Joule for Consultants — a workload that would normally take several weeks.Four teams were told the analysis had been completed by junior interns fresh out of school. They reviewed the material, found it impressive, and rated the work about 95% accurate.The fifth team was told the very same answers had come from AI.They rejected almost everything.Only when asked to validate each answer one by one did they discover that the AI was, in fact, highly accurate — surfacing detailed insights the consultants had initially dismissed. The overall accuracy? Again, about 95%.“The lesson learned here is that we need to be very cautious as we introduce AI — especially in how we communicate with senior consultants about its possibilities and how to integrate it into their workflows,” says Guillermo B. Vazquez Mendez, chief architect, RI business transformation and architecture, SAP America Inc.The experiment has since become a revealing starting point for SAP’s push toward the consultant of 2030: a practitioner who is deeply human, enabled by AI, and no longer weighed down by the technical grunt work of the past.Overcoming AI skepticismResistance isn’t surprising, Vazquez notes. Consultants with two or three decades of experience carry enormous institutional knowledge — and an understandable degree of caution.But AI copilots like Joule for Consultants are not replacing expertise. They’re amplifying it.“What Joule really does is make their very expensive time far more effective,” Vazquez says. “It removes the clerical work, so they can focus on turning out high-quality answers in a fraction of the time.”He emphasizes this message constantly: “AI is not replacing you. It’s a tool for you. Human oversight is always required. But now, instead of spending your time looking for documentation, you’re gaining significant time and boosting the effectiveness and detail of your answers.”The consultant time-shift: from tech execution to business insightHistorically, consultants spent about 80% of their time understanding technical systems — how processes run, how data flows, how functions execute. Customers, by contrast, spend 80% of their time focused on their business.That mismatch is exactly where Joule steps in.“There’s a gap there — and the bridge is AI,” Vazquez says. “It flips the time equation, enabling consultants to invest more of their energy in understanding the customer’s industry and business goals. AI takes on the heavy technical lift, so consultants can focus on driving the right business outcomes.”Bringing new consultants up to speedAI is also transforming how new hires learn.“We’re excited to see Joule acting as a bridge between senior consultants, who are adapting more slowly, and interns and new consultants who are already technically savvy,” Vazquez says.Junior consultants ramp up faster because Joule helps them operate independently. Seniors, meanwhile, engage where their insight matters most.This is also where many consultants learn the fundamentals of today’s AI copilots. Much of the work depends on prompt engineering — for instance, instructing Joule to act as a senior chief technology architect specializing in finance and SAP S/4HANA 2023, then asking it to analyze business requirements and deliver the output as tables or PowerPoint slides.Once they grasp how to frame prompts, consultants consistently get higher-quality, more structured answers.New architects are also able to communicate more clearly with their more experienced counterparts. They know what they don’t know and can ask targeted questions, which makes mentorship far smoother. It’s created a real synergy, Vazquez adds — senior consultants see how quickly new hires are adapting and learning with AI, and that momentum encourages them to keep pace and adopt the technology themselves.Looking ahead to the future of AI copilots“We’re still in the baby steps of AI — we’re toddlers,” Vazquez says. “Right now, copilots depend on prompt engineering to get good answers. The better you prompt, the better the answer you get.”But that represents only the earliest phase of what these systems will eventually do. As copilots mature, they’ll move beyond responding to prompts and start interpreting entire business processes — understanding the sequence of steps, identifying where human intervention is needed, and spotting where an AI agent could take over. That shift is what leads directly into agentic AI.SAP’s depth of process knowledge is what makes that evolution possible. The company has mapped more than 3,500 business processes across industries — a repository Vazquez calls “some of the most valuable, rigorously tested processes developed in the last 50 years.” Every day, SAP systems support roughly $7.3 trillion in global commerce, giving these emerging AI agents a rich foundation to navigate and reason over.“With that level of process insight and data, we can take a real leap forward,” he says, “equipping our consultants with agentic AI that can solve complex challenges and push us toward increasingly autonomous systems.” Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.",
          "content": "Presented by SAPWhen SAP ran a quiet internal experiment to gauge consultant attitudes toward AI, the results were striking. Five teams were asked to validate answers to more than 1,000 business requirements completed by SAP’s AI co-pilot, Joule for Consultants — a workload that would normally take several weeks.Four teams were told the analysis had been completed by junior interns fresh out of school. They reviewed the material, found it impressive, and rated the work about 95% accurate.The fifth team was told the very same answers had come from AI.They rejected almost everything.Only when asked to validate each answer one by one did they discover that the AI was, in fact, highly accurate — surfacing detailed insights the consultants had initially dismissed. The overall accuracy? Again, about 95%.“The lesson learned here is that we need to be very cautious as we introduce AI — especially in how we communicate with senior consultants about its possibilities and how to integrate it into their workflows,” says Guillermo B. Vazquez Mendez, chief architect, RI business transformation and architecture, SAP America Inc.The experiment has since become a revealing starting point for SAP’s push toward the consultant of 2030: a practitioner who is deeply human, enabled by AI, and no longer weighed down by the technical grunt work of the past.Overcoming AI skepticismResistance isn’t surprising, Vazquez notes. Consultants with two or three decades of experience carry enormous institutional knowledge — and an understandable degree of caution.But AI copilots like Joule for Consultants are not replacing expertise. They’re amplifying it.“What Joule really does is make their very expensive time far more effective,” Vazquez says. “It removes the clerical work, so they can focus on turning out high-quality answers in a fraction of the time.”He emphasizes this message constantly: “AI is not replacing you. It’s a tool for you. Human oversight is always required. But now, instead of spending your time looking for documentation, you’re gaining significant time and boosting the effectiveness and detail of your answers.”The consultant time-shift: from tech execution to business insightHistorically, consultants spent about 80% of their time understanding technical systems — how processes run, how data flows, how functions execute. Customers, by contrast, spend 80% of their time focused on their business.That mismatch is exactly where Joule steps in.“There’s a gap there — and the bridge is AI,” Vazquez says. “It flips the time equation, enabling consultants to invest more of their energy in understanding the customer’s industry and business goals. AI takes on the heavy technical lift, so consultants can focus on driving the right business outcomes.”Bringing new consultants up to speedAI is also transforming how new hires learn.“We’re excited to see Joule acting as a bridge between senior consultants, who are adapting more slowly, and interns and new consultants who are already technically savvy,” Vazquez says.Junior consultants ramp up faster because Joule helps them operate independently. Seniors, meanwhile, engage where their insight matters most.This is also where many consultants learn the fundamentals of today’s AI copilots. Much of the work depends on prompt engineering — for instance, instructing Joule to act as a senior chief technology architect specializing in finance and SAP S/4HANA 2023, then asking it to analyze business requirements and deliver the output as tables or PowerPoint slides.Once they grasp how to frame prompts, consultants consistently get higher-quality, more structured answers.New architects are also able to communicate more clearly with their more experienced counterparts. They know what they don’t know and can ask targeted questions, which makes mentorship far smoother. It’s created a real synergy, Vazquez adds — senior consultants see how quickly new hires are adapting and learning with AI, and that momentum encourages them to keep pace and adopt the technology themselves.Looking ahead to the future of AI copilots“We’re still in the baby steps of AI — we’re toddlers,” Vazquez says. “Right now, copilots depend on prompt engineering to get good answers. The better you prompt, the better the answer you get.”But that represents only the earliest phase of what these systems will eventually do. As copilots mature, they’ll move beyond responding to prompts and start interpreting entire business processes — understanding the sequence of steps, identifying where human intervention is needed, and spotting where an AI agent could take over. That shift is what leads directly into agentic AI.SAP’s depth of process knowledge is what makes that evolution possible. The company has mapped more than 3,500 business processes across industries — a repository Vazquez calls “some of the most valuable, rigorously tested processes developed in the last 50 years.” Every day, SAP systems support roughly $7.3 trillion in global commerce, giving these emerging AI agents a rich foundation to navigate and reason over.“With that level of process insight and data, we can take a real leap forward,” he says, “equipping our consultants with agentic AI that can solve complex challenges and push us toward increasingly autonomous systems.” Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.",
          "feed_position": 0,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/7IoiYyTs0K9D4WfYDzJvhy/2cd15991676cc0648bffb25178642417/AdobeStock_571280209_Preview.jpeg?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/mistral-launches-powerful-devstral-2-coding-model-including-open-source",
          "published_at": "Tue, 09 Dec 2025 19:44:00 GMT",
          "title": "Mistral launches powerful Devstral 2 coding model including open source, laptop-friendly version",
          "standfirst": "French AI startup Mistral has weathered a rocky period of public questioning over the last year to emerge, now here in December 2025, with new, crowd-pleasing models for enterprise and indie developers.Just days after releasing its powerful open source, general purpose Mistral 3 LLM family for edge devices and local hardware, the company returned today to debut Devstral 2.The release includes a new pair of models optimized for software engineering tasks — again, with one small enough to run on a single laptop, offline and privately — alongside Mistral Vibe, a command-line interface (CLI) agent designed to allow developers to call the models up directly within their terminal environments. The models are fast, lean, and open—at least in theory. But the real story lies not just in the benchmarks, but in how Mistral is packaging this capability: one model fully free, another conditionally so, and a terminal interface built to scale with either.It’s an attempt not just to match proprietary systems like Claude and GPT-4 in performance, but to compete with them on developer experience—and to do so while holding onto the flag of open-source.Both models are available now for free for a limited time via Mistral’s API and Hugging Face. The full Devstral 2 model is supported out-of-the-box in the community inference provider vLLM and on the open source agentic coding platform Kilo Code. A Coding Model Meant to DriveAt the top of the announcement is Devstral 2, a 123-billion parameter dense transformer with a 256K-token context window, engineered specifically for agentic software development. Mistral says the model achieves 72.2% on SWE-bench Verified, a benchmark designed to evaluate long-context software engineering tasks in real-world repositories.The smaller sibling, Devstral Small 2, weighs in at 24B parameters, with the same long context window and a performance of 68.0% on SWE-bench. On paper, that makes it the strongest open-weight model of its size, even outscoring many 70B-class competitors.But the performance story isn’t just about raw percentages. Mistral is betting that efficient intelligence beats scale, and has made much of the fact that Devstral 2 is:5× smaller than DeepSeek V3.28× smaller than Kimi K2Yet still matches or surpasses them on key software reasoning benchmarks.Human evaluations back this up. In side-by-side comparisons:Devstral 2 beat DeepSeek V3.2 in 42.8% of tasks, losing only 28.6%.Against Claude Sonnet 4.5, it lost more often (53.1%)—a reminder that while the gap is narrowing, closed models still lead in overall preference.Still, for an open-weight model, these results place Devstral 2 at the frontier of what’s currently available to run and modify independently.Vibe CLI: A Terminal-Native AgentAlongside the models, Mistral released Vibe CLI, a command-line assistant that integrates directly with Devstral models. It’s not an IDE plugin or a ChatGPT-style code explainer. It’s a native interface designed for project-wide code understanding and orchestration, built to live inside the developer’s actual workflow.Vibe brings a surprising degree of intelligence to the terminal:It reads your file tree and Git status to understand project scope.It lets you reference files with @, run shell commands with !, and toggle behavior with slash commands.It orchestrates changes across multiple files, tracks dependencies, retries failed executions, and can even refactor at architectural scale.Unlike most developer agents, which simulate a REPL from within a chat UI, Vibe starts with the shell and pulls intelligence in from there. It’s programmable, scriptable, and themeable. And it’s released under the Apache 2.0 license, meaning it’s truly free to use—in commercial settings, internal tools, or open-source extensions.Licensing Structure: Open-ish — With Revenue LimitationsAt first glance, Mistral’s licensing approach appears straightforward: the models are open-weight and publicly available. But a closer look reveals a line drawn through the middle of the release, with different rules for different users.Devstral Small 2, the 24-billion parameter variant, is covered under a standard, enterprise- and developer-friendly Apache 2.0 license. That’s a gold standard in open-source: no revenue restrictions, no fine print, no need to check with legal. Enterprises can use it in production, embed it into products, and redistribute fine-tuned versions without asking for permission.Devstral 2, the flagship 123B model, is released under what Mistral calls a “modified MIT license.” That phrase sounds innocuous, but the modification introduces a critical limitation: any company making more than $20 million in monthly revenue cannot use the model at all—not even internally—without securing a separate commercial license from Mistral.“You are not authorized to exercise any rights under this license if the global consolidated monthly revenue of your company […] exceeds $20 million,” the license reads.The clause applies not only to the base model, but to derivatives, fine-tuned versions, and redistributed variants, regardless of who hosts them. In effect, it means that while the weights are “open,” their use is gated for large enterprises—unless they’re willing to engage with Mistral’s sales team or use the hosted API at metered pricing.To draw an analogy: Apache 2.0 is like a public library—you walk in, borrow the book, and use it however you need. Mistral’s modified MIT license is more like a corporate co-working space that’s free for freelancers but charges rent once your company hits a certain size.Weighing Devstral Small 2 for Enterprise UseThis division raises an obvious question for larger companies: can Devstral Small 2 with its more permissive and unrestricted Apache 2.0 licensing serve as a viable alternative for medium-to-large enterprises?The answer depends on context. Devstral Small 2 scores 68.0% on SWE-bench, significantly ahead of many larger open models, and remains deployable on single-GPU or CPU-only setups. For teams focused on:internal tooling,on-prem deployment,low-latency edge inference,…it offers a rare combination of legality, performance, and convenience.But the performance gap from Devstral 2 is real. For multi-agent setups, deep monorepo refactoring, or long-context code analysis, that 4-point benchmark delta may understate the actual experience difference.For most enterprises, Devstral Small 2 will serve either as a low-friction way to prototype—or as a pragmatic bridge until licensing for Devstral 2 becomes feasible. It is not a drop-in replacement for the flagship, but it may be “good enough” in specific production slices, particularly when paired with Vibe CLI.But because Devstral Small 2 can be run entirely offline — including on a single GPU machine or a sufficiently specced laptop — it unlocks a critical use case for developers and teams operating in tightly controlled environments. Whether you’re a solo indie building tools on the go, or part of a company with strict data governance or compliance mandates, the ability to run a performant, long-context coding model without ever hitting the internet is a powerful differentiator. No cloud calls, no third-party telemetry, no risk of data leakage — just local inference with full visibility and control.This matters in industries like finance, healthcare, defense, and advanced manufacturing, where data often cannot leave the network perimeter. But it’s just as useful for developers who prefer autonomy over vendor lock-in — or who want their tools to work the same on a plane, in the field, or inside an air-gapped lab. In a market where most top-tier code models are delivered as API-only SaaS products, Devstral Small 2 offers a rare level of portability, privacy, and ownership.In that sense, Mistral isn’t just offering open models—they’re offering multiple paths to adoption, depending on your scale, compliance posture, and willingness to engage.Integration, Infrastructure, and AccessFrom a technical standpoint, Mistral’s models are built for deployment. Devstral 2 requires a minimum of 4× H100-class GPUs, and is already available on build.nvidia.com. Devstral Small 2 can run on a single GPU or CPU such as those in a standard laptop, making it accessible to solo developers and embedded teams alike.Both models support quantized FP4 and FP8 weights, and are compatible with vLLM for scalable inference. Fine-tuning is supported out of the box.API pricing—after the free introductory window—follows a token-based structure:Devstral 2: $0.40 per million input tokens / $2.00 for outputDevstral Small 2: $0.10 input / $0.30 outputThat pricing sits just below OpenAI’s GPT-4 Turbo, and well below Anthropic’s Claude Sonnet at comparable performance levels.Developer Reception: Ground-Level BuzzOn X (formerly Twitter), developers reacted quickly with a wave of positive reception, with Hugging Face&#x27;s Head of Product Victor Mustar asking if the small, Apache 2.0 licensed variant was the \"new local coding king,\" i.e., the one developers could use to run on their laptops directly and privately, without an internet connection:Another popular AI news and rumors account, TestingCatalogNews, posted that it was \"SOTTA in coding,\" or \"State Of The Tiny Art\"Another user, @xlr8harder, took issue with the custom licensing terms for Devstral 2, writing \"calling the Devstral 2 license &#x27;modified MIT&#x27; is misleading at best. It’s a proprietary license with MIT-like attribution requirements.\"While the tone was critical, it reflected some attention Mistral’s license structuring was receiving, particularly among developers familiar with open-use norms.Strategic Context: From Codestral to Devstral and Mistral 3Mistral’s steady push into software development tools didn’t start with Devstral 2—it began in May 2024 with Codestral, the company’s first code-focused large language model. A 22-billion parameter system trained on more than 80 programming languages, Codestral was designed for use in developer environments ranging from basic autocompletions to full function generation. The model launched under a non-commercial license but still outperformed heavyweight competitors like CodeLlama 70B and Deepseek Coder 33B in early benchmarks such as HumanEval and RepoBench.Codestral’s release marked Mistral’s first move into the competitive coding-model space, but it also established a now-familiar pattern: technically lean models with surprisingly strong results, a wide context window, and licensing choices that invited developer experimentation. Industry partners including JetBrains, LlamaIndex, and LangChain quickly began integrating the model into their workflows, citing its speed and tool compatibility as key differentiators.One year later, the company followed up with Devstral, a 24B model purpose-built for “agentic” behavior—handling long-range reasoning, file navigation, and autonomous code modification. Released in partnership with All Hands AI and licensed under Apache 2.0, Devstral was notable not just for its portability (it could run on a MacBook or RTX 4090), but for its performance: it beat out several closed models on SWE-Bench Verified, a benchmark of 500 real-world GitHub issues.Then came Mistral 3, announced in December 2025 as a portfolio of 10 open-weight models targeting everything from drones and smartphones to cloud infrastructure. This suite included both high-end models like Mistral Large 3 (a MoE system with 41 active parameters and 256K context) and lightweight “Ministral” variants that could run on 4GB of VRAM. All were licensed under Apache 2.0, reinforcing Mistral’s commitment to flexible, edge-friendly deployment.Mistral 3 positioned the company not as a direct competitor to frontier models like GPT-5 or Gemini 3, but as a developer-first platform for customized, localized AI systems. Co-founder Guillaume Lample described the vision as “distributed intelligence”—many smaller systems tuned for specific tasks and running outside centralized infrastructure. “In more than 90% of cases, a small model can do the job,” he told VentureBeat. “It doesn’t have to be a model with hundreds of billions of parameters.”That broader strategy helps explain the significance of Devstral 2. It’s not a one-off release but a continuation of Mistral’s long-running commitment to code agents, local-first deployment, and open-weight availability—an ecosystem that began with Codestral, matured through Devstral, and scaled up with Mistral 3. Devstral 2, in this framing, is not just a model. It’s the next version of a playbook that’s been unfolding in public for over a year.Final Thoughts (For Now): A Fork in the RoadWith Devstral 2, Devstral Small 2, and Vibe CLI, Mistral AI has drawn a clear map for developers and companies alike. The tools are fast, capable, and thoughtfully integrated. But they also present a choice—not just in architecture, but in how and where you’re allowed to use them.If you’re an individual developer, small startup, or open-source maintainer, this is one of the most powerful AI systems you can freely run today. If you’re a Fortune 500 engineering lead, you’ll need to either talk to Mistral—or settle for the smaller model and make it work.In a market increasingly dominated by black-box models and SaaS lock-ins, Mistral’s offer is still a breath of fresh air. Just read the fine print before you start building.",
          "content": "French AI startup Mistral has weathered a rocky period of public questioning over the last year to emerge, now here in December 2025, with new, crowd-pleasing models for enterprise and indie developers.Just days after releasing its powerful open source, general purpose Mistral 3 LLM family for edge devices and local hardware, the company returned today to debut Devstral 2.The release includes a new pair of models optimized for software engineering tasks — again, with one small enough to run on a single laptop, offline and privately — alongside Mistral Vibe, a command-line interface (CLI) agent designed to allow developers to call the models up directly within their terminal environments. The models are fast, lean, and open—at least in theory. But the real story lies not just in the benchmarks, but in how Mistral is packaging this capability: one model fully free, another conditionally so, and a terminal interface built to scale with either.It’s an attempt not just to match proprietary systems like Claude and GPT-4 in performance, but to compete with them on developer experience—and to do so while holding onto the flag of open-source.Both models are available now for free for a limited time via Mistral’s API and Hugging Face. The full Devstral 2 model is supported out-of-the-box in the community inference provider vLLM and on the open source agentic coding platform Kilo Code. A Coding Model Meant to DriveAt the top of the announcement is Devstral 2, a 123-billion parameter dense transformer with a 256K-token context window, engineered specifically for agentic software development. Mistral says the model achieves 72.2% on SWE-bench Verified, a benchmark designed to evaluate long-context software engineering tasks in real-world repositories.The smaller sibling, Devstral Small 2, weighs in at 24B parameters, with the same long context window and a performance of 68.0% on SWE-bench. On paper, that makes it the strongest open-weight model of its size, even outscoring many 70B-class competitors.But the performance story isn’t just about raw percentages. Mistral is betting that efficient intelligence beats scale, and has made much of the fact that Devstral 2 is:5× smaller than DeepSeek V3.28× smaller than Kimi K2Yet still matches or surpasses them on key software reasoning benchmarks.Human evaluations back this up. In side-by-side comparisons:Devstral 2 beat DeepSeek V3.2 in 42.8% of tasks, losing only 28.6%.Against Claude Sonnet 4.5, it lost more often (53.1%)—a reminder that while the gap is narrowing, closed models still lead in overall preference.Still, for an open-weight model, these results place Devstral 2 at the frontier of what’s currently available to run and modify independently.Vibe CLI: A Terminal-Native AgentAlongside the models, Mistral released Vibe CLI, a command-line assistant that integrates directly with Devstral models. It’s not an IDE plugin or a ChatGPT-style code explainer. It’s a native interface designed for project-wide code understanding and orchestration, built to live inside the developer’s actual workflow.Vibe brings a surprising degree of intelligence to the terminal:It reads your file tree and Git status to understand project scope.It lets you reference files with @, run shell commands with !, and toggle behavior with slash commands.It orchestrates changes across multiple files, tracks dependencies, retries failed executions, and can even refactor at architectural scale.Unlike most developer agents, which simulate a REPL from within a chat UI, Vibe starts with the shell and pulls intelligence in from there. It’s programmable, scriptable, and themeable. And it’s released under the Apache 2.0 license, meaning it’s truly free to use—in commercial settings, internal tools, or open-source extensions.Licensing Structure: Open-ish — With Revenue LimitationsAt first glance, Mistral’s licensing approach appears straightforward: the models are open-weight and publicly available. But a closer look reveals a line drawn through the middle of the release, with different rules for different users.Devstral Small 2, the 24-billion parameter variant, is covered under a standard, enterprise- and developer-friendly Apache 2.0 license. That’s a gold standard in open-source: no revenue restrictions, no fine print, no need to check with legal. Enterprises can use it in production, embed it into products, and redistribute fine-tuned versions without asking for permission.Devstral 2, the flagship 123B model, is released under what Mistral calls a “modified MIT license.” That phrase sounds innocuous, but the modification introduces a critical limitation: any company making more than $20 million in monthly revenue cannot use the model at all—not even internally—without securing a separate commercial license from Mistral.“You are not authorized to exercise any rights under this license if the global consolidated monthly revenue of your company […] exceeds $20 million,” the license reads.The clause applies not only to the base model, but to derivatives, fine-tuned versions, and redistributed variants, regardless of who hosts them. In effect, it means that while the weights are “open,” their use is gated for large enterprises—unless they’re willing to engage with Mistral’s sales team or use the hosted API at metered pricing.To draw an analogy: Apache 2.0 is like a public library—you walk in, borrow the book, and use it however you need. Mistral’s modified MIT license is more like a corporate co-working space that’s free for freelancers but charges rent once your company hits a certain size.Weighing Devstral Small 2 for Enterprise UseThis division raises an obvious question for larger companies: can Devstral Small 2 with its more permissive and unrestricted Apache 2.0 licensing serve as a viable alternative for medium-to-large enterprises?The answer depends on context. Devstral Small 2 scores 68.0% on SWE-bench, significantly ahead of many larger open models, and remains deployable on single-GPU or CPU-only setups. For teams focused on:internal tooling,on-prem deployment,low-latency edge inference,…it offers a rare combination of legality, performance, and convenience.But the performance gap from Devstral 2 is real. For multi-agent setups, deep monorepo refactoring, or long-context code analysis, that 4-point benchmark delta may understate the actual experience difference.For most enterprises, Devstral Small 2 will serve either as a low-friction way to prototype—or as a pragmatic bridge until licensing for Devstral 2 becomes feasible. It is not a drop-in replacement for the flagship, but it may be “good enough” in specific production slices, particularly when paired with Vibe CLI.But because Devstral Small 2 can be run entirely offline — including on a single GPU machine or a sufficiently specced laptop — it unlocks a critical use case for developers and teams operating in tightly controlled environments. Whether you’re a solo indie building tools on the go, or part of a company with strict data governance or compliance mandates, the ability to run a performant, long-context coding model without ever hitting the internet is a powerful differentiator. No cloud calls, no third-party telemetry, no risk of data leakage — just local inference with full visibility and control.This matters in industries like finance, healthcare, defense, and advanced manufacturing, where data often cannot leave the network perimeter. But it’s just as useful for developers who prefer autonomy over vendor lock-in — or who want their tools to work the same on a plane, in the field, or inside an air-gapped lab. In a market where most top-tier code models are delivered as API-only SaaS products, Devstral Small 2 offers a rare level of portability, privacy, and ownership.In that sense, Mistral isn’t just offering open models—they’re offering multiple paths to adoption, depending on your scale, compliance posture, and willingness to engage.Integration, Infrastructure, and AccessFrom a technical standpoint, Mistral’s models are built for deployment. Devstral 2 requires a minimum of 4× H100-class GPUs, and is already available on build.nvidia.com. Devstral Small 2 can run on a single GPU or CPU such as those in a standard laptop, making it accessible to solo developers and embedded teams alike.Both models support quantized FP4 and FP8 weights, and are compatible with vLLM for scalable inference. Fine-tuning is supported out of the box.API pricing—after the free introductory window—follows a token-based structure:Devstral 2: $0.40 per million input tokens / $2.00 for outputDevstral Small 2: $0.10 input / $0.30 outputThat pricing sits just below OpenAI’s GPT-4 Turbo, and well below Anthropic’s Claude Sonnet at comparable performance levels.Developer Reception: Ground-Level BuzzOn X (formerly Twitter), developers reacted quickly with a wave of positive reception, with Hugging Face&#x27;s Head of Product Victor Mustar asking if the small, Apache 2.0 licensed variant was the \"new local coding king,\" i.e., the one developers could use to run on their laptops directly and privately, without an internet connection:Another popular AI news and rumors account, TestingCatalogNews, posted that it was \"SOTTA in coding,\" or \"State Of The Tiny Art\"Another user, @xlr8harder, took issue with the custom licensing terms for Devstral 2, writing \"calling the Devstral 2 license &#x27;modified MIT&#x27; is misleading at best. It’s a proprietary license with MIT-like attribution requirements.\"While the tone was critical, it reflected some attention Mistral’s license structuring was receiving, particularly among developers familiar with open-use norms.Strategic Context: From Codestral to Devstral and Mistral 3Mistral’s steady push into software development tools didn’t start with Devstral 2—it began in May 2024 with Codestral, the company’s first code-focused large language model. A 22-billion parameter system trained on more than 80 programming languages, Codestral was designed for use in developer environments ranging from basic autocompletions to full function generation. The model launched under a non-commercial license but still outperformed heavyweight competitors like CodeLlama 70B and Deepseek Coder 33B in early benchmarks such as HumanEval and RepoBench.Codestral’s release marked Mistral’s first move into the competitive coding-model space, but it also established a now-familiar pattern: technically lean models with surprisingly strong results, a wide context window, and licensing choices that invited developer experimentation. Industry partners including JetBrains, LlamaIndex, and LangChain quickly began integrating the model into their workflows, citing its speed and tool compatibility as key differentiators.One year later, the company followed up with Devstral, a 24B model purpose-built for “agentic” behavior—handling long-range reasoning, file navigation, and autonomous code modification. Released in partnership with All Hands AI and licensed under Apache 2.0, Devstral was notable not just for its portability (it could run on a MacBook or RTX 4090), but for its performance: it beat out several closed models on SWE-Bench Verified, a benchmark of 500 real-world GitHub issues.Then came Mistral 3, announced in December 2025 as a portfolio of 10 open-weight models targeting everything from drones and smartphones to cloud infrastructure. This suite included both high-end models like Mistral Large 3 (a MoE system with 41 active parameters and 256K context) and lightweight “Ministral” variants that could run on 4GB of VRAM. All were licensed under Apache 2.0, reinforcing Mistral’s commitment to flexible, edge-friendly deployment.Mistral 3 positioned the company not as a direct competitor to frontier models like GPT-5 or Gemini 3, but as a developer-first platform for customized, localized AI systems. Co-founder Guillaume Lample described the vision as “distributed intelligence”—many smaller systems tuned for specific tasks and running outside centralized infrastructure. “In more than 90% of cases, a small model can do the job,” he told VentureBeat. “It doesn’t have to be a model with hundreds of billions of parameters.”That broader strategy helps explain the significance of Devstral 2. It’s not a one-off release but a continuation of Mistral’s long-running commitment to code agents, local-first deployment, and open-weight availability—an ecosystem that began with Codestral, matured through Devstral, and scaled up with Mistral 3. Devstral 2, in this framing, is not just a model. It’s the next version of a playbook that’s been unfolding in public for over a year.Final Thoughts (For Now): A Fork in the RoadWith Devstral 2, Devstral Small 2, and Vibe CLI, Mistral AI has drawn a clear map for developers and companies alike. The tools are fast, capable, and thoughtfully integrated. But they also present a choice—not just in architecture, but in how and where you’re allowed to use them.If you’re an individual developer, small startup, or open-source maintainer, this is one of the most powerful AI systems you can freely run today. If you’re a Fortune 500 engineering lead, you’ll need to either talk to Mistral—or settle for the smaller model and make it work.In a market increasingly dominated by black-box models and SaaS lock-ins, Mistral’s offer is still a breath of fresh air. Just read the fine print before you start building.",
          "feed_position": 1,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/5h08IT02qEnf15d2nC66XM/808f775396da3b120a6c20c72b759776/G-V5j_tz9IkXATIrqK6cF.png?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/social-media/meta-is-trying-to-make-facebook-suck-less-by-simplifying-things-a-bit-171910771.html",
          "published_at": "Tue, 09 Dec 2025 17:19:10 +0000",
          "title": "Meta is trying to make Facebook suck less by simplifying things a bit",
          "standfirst": "Somewhere along its never-ending quest to increase engagement, Meta realized that giving Facebook users more of what they want would make it more likely that they'll stick around. The company has announced a bunch of updates designed to help improve the feed and the broader Facebook experience by making it easier to find, create and share interesting things. (Because primarily showing updates from your friends with the occasional ad or meme post is maybe just too complicated.)Simplification is a big focus of this overhaul. First, the Facebook feed will be a bit more streamlined. Whenever you post multiple photos, Facebook will arrange them into a standardized grid. When you click into anything on the feed, you'll be able to see it in a full screen view. And there's a very welcome change in that you'll be able to like a photo by double-tapping it. Just be careful with that when you're swiping through an ex's or a crush's photos.Simplified Facebook feed.MetaSearch results are now said to \"show more content in a more immersive grid layout that supports all content types,\" according to Meta. The company is trying out a new full-screen viewer for Facebook that \"lets you explore different photo and video results without losing your place in search,\" which it plans to expand to \"more content and post types in the coming months.\"In addition, the company says you’ll be able to provide feedback on a Facebook post or Reel to help make future recommendations more relevant. More ways for you to \"shape your feed\" and offer feedback on what the algorithm serves up are coming soon.The Facebook feed sucks, and it's good that Meta knows it sucks. There have been numerous occasions over the last couple of years where I've had to scroll through a couple dozen uninteresting posts from pages and creators I've never heard of before seeing something from a friend. The glut of spam and AI slop isn't helping (things are pretty grim for creators who have been dealing with content thieves too). There was a spell of several months last year when, every single time I opened Facebook, I would see an utterly garbage AI-generated image of a \"tiny house,\" a supposedly cozy domicile where not much actually made sense (three TVs in a living room, stairs and railings that had the telltale signs of AI warping). I'd always provide feedback that I didn't want to see any posts from that page again. But the next day there'd be another rotten \"tiny house\" image from a different page in my feed.Here's hoping Meta will actually take feedback related to recommendations on board and act on it. If the company does, it might actually make the feed more interesting to scroll through again.Elsewhere, Facebook will place the most-used tab bar features — such as Reels, Friends, Marketplace and Profile — front and center on the tab bar for easier and faster access. Meta is also promising a refreshed look for the menu and \"cleaner\" tab notifications.Facebook Story creation screenMetaFacebook is making it easier to access more popular Story and Feed post creation tools like music and friend tagging by giving them more prominent placement. Advanced options like text background colors will be an extra tap or two away. The post and Story composer feature audience and cross-post settings prominently, so that you have ease of control over who can see what you're sharing. Meta has updated how comments work across the feed, Groups and Reels as well to make things more streamlined and easier to follow. On top of all of that, when you make changes to your profile, you might start seeing suggestions for friends with shared interests. Meta suggested that, \"if you update your profile to show you're into sourdough bread baking or planning a trip to Nashville, Facebook will show you friends who can give you sourdough starter tips or offer suggestions on the best local spots.\" As always, though, you can decide who sees what on your profile or simply opt to share none of this personal info with Facebook at all, especially if you feel that Meta already knows too much about you.This article originally appeared on Engadget at https://www.engadget.com/social-media/meta-is-trying-to-make-facebook-suck-less-by-simplifying-things-a-bit-171910771.html?src=rss",
          "content": "Somewhere along its never-ending quest to increase engagement, Meta realized that giving Facebook users more of what they want would make it more likely that they'll stick around. The company has announced a bunch of updates designed to help improve the feed and the broader Facebook experience by making it easier to find, create and share interesting things. (Because primarily showing updates from your friends with the occasional ad or meme post is maybe just too complicated.)Simplification is a big focus of this overhaul. First, the Facebook feed will be a bit more streamlined. Whenever you post multiple photos, Facebook will arrange them into a standardized grid. When you click into anything on the feed, you'll be able to see it in a full screen view. And there's a very welcome change in that you'll be able to like a photo by double-tapping it. Just be careful with that when you're swiping through an ex's or a crush's photos.Simplified Facebook feed.MetaSearch results are now said to \"show more content in a more immersive grid layout that supports all content types,\" according to Meta. The company is trying out a new full-screen viewer for Facebook that \"lets you explore different photo and video results without losing your place in search,\" which it plans to expand to \"more content and post types in the coming months.\"In addition, the company says you’ll be able to provide feedback on a Facebook post or Reel to help make future recommendations more relevant. More ways for you to \"shape your feed\" and offer feedback on what the algorithm serves up are coming soon.The Facebook feed sucks, and it's good that Meta knows it sucks. There have been numerous occasions over the last couple of years where I've had to scroll through a couple dozen uninteresting posts from pages and creators I've never heard of before seeing something from a friend. The glut of spam and AI slop isn't helping (things are pretty grim for creators who have been dealing with content thieves too). There was a spell of several months last year when, every single time I opened Facebook, I would see an utterly garbage AI-generated image of a \"tiny house,\" a supposedly cozy domicile where not much actually made sense (three TVs in a living room, stairs and railings that had the telltale signs of AI warping). I'd always provide feedback that I didn't want to see any posts from that page again. But the next day there'd be another rotten \"tiny house\" image from a different page in my feed.Here's hoping Meta will actually take feedback related to recommendations on board and act on it. If the company does, it might actually make the feed more interesting to scroll through again.Elsewhere, Facebook will place the most-used tab bar features — such as Reels, Friends, Marketplace and Profile — front and center on the tab bar for easier and faster access. Meta is also promising a refreshed look for the menu and \"cleaner\" tab notifications.Facebook Story creation screenMetaFacebook is making it easier to access more popular Story and Feed post creation tools like music and friend tagging by giving them more prominent placement. Advanced options like text background colors will be an extra tap or two away. The post and Story composer feature audience and cross-post settings prominently, so that you have ease of control over who can see what you're sharing. Meta has updated how comments work across the feed, Groups and Reels as well to make things more streamlined and easier to follow. On top of all of that, when you make changes to your profile, you might start seeing suggestions for friends with shared interests. Meta suggested that, \"if you update your profile to show you're into sourdough bread baking or planning a trip to Nashville, Facebook will show you friends who can give you sourdough starter tips or offer suggestions on the best local spots.\" As always, though, you can decide who sees what on your profile or simply opt to share none of this personal info with Facebook at all, especially if you feel that Meta already knows too much about you.This article originally appeared on Engadget at https://www.engadget.com/social-media/meta-is-trying-to-make-facebook-suck-less-by-simplifying-things-a-bit-171910771.html?src=rss",
          "feed_position": 13,
          "image_url": "https://d29szjachogqwa.cloudfront.net/images/user-uploaded/facebook_feed.jpg"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/data-infrastructure/databricks-officeqa-uncovers-disconnect-ai-agents-ace-abstract-tests-but",
          "published_at": "Tue, 09 Dec 2025 16:00:00 GMT",
          "title": "Databricks' OfficeQA uncovers disconnect: AI agents ace abstract tests but stall at 45% on enterprise docs",
          "standfirst": "There is no shortage of AI benchmarks in the market today, with popular options like Humanity&#x27;s Last Exam (HLE), ARC-AGI-2 and GDPval, among numerous others.AI agents excel at solving abstract math problems and passing PhD-level exams that most benchmarks are based on, but Databricks has a question for the enterprise: Can they actually handle the document-heavy work most enterprises need them to do?The answer, according to new research from the data and AI platform company, is sobering. Even the best-performing AI agents achieve less than 45% accuracy on tasks that mirror real enterprise workloads, exposing a critical gap between academic benchmarks and business reality.\"If we focus our research efforts on getting better at [existing benchmarks], then we&#x27;re probably not solving the right problems to make Databricks a better platform,\" Erich Elsen, principal research scientist at Databricks, explained to VentureBeat. \"So that&#x27;s why we were looking around. How do we create a benchmark that, if we get better at it, we&#x27;re actually getting better at solving the problems that our customers have?\"The result is OfficeQA, a benchmark designed to test AI agents on grounded reasoning: Answering questions based on complex proprietary datasets containing unstructured documents and tabular data. Unlike existing benchmarks that focus on abstract capabilities, OfficeQA proxies for the economically valuable tasks enterprises actually perform.Why academic benchmarks miss the enterprise markThere are numerous shortcomings of popular AI benchmarks from an enterprise perspective, according to Elsen. HLE features questions requiring PhD-level expertise across diverse fields. ARC-AGI evaluates abstract reasoning through visual manipulation of colored grids. Both push the frontiers of AI capabilities, but don&#x27;t reflect daily enterprise work. Even GDPval, which was specifically created to evaluate economically useful tasks, misses the target.\"We come from a pretty heavy science or engineering background, and sometimes we create evals that reflect that,\" Elsen said. \" So they&#x27;re either extremely math-heavy, which is a great, useful task, but advancing the frontiers of human mathematics is not what customers are trying to do with Databricks.\"While AI is commonly used for customer support and coding apps, Databricks&#x27; customer base has a broader set of requirements. Elsen noted that answering questions about documents or corpora of documents is a common enterprise task. These require parsing complex tables with nested headers, retrieving information across dozens or hundreds of documents and performing calculations where a single-digit error can cascade into organizations making incorrect business decisions.Building a benchmark that mirrors enterprise document complexityTo create a meaningful test of grounded reasoning capabilities, Databricks needed a dataset that approximates the messy reality of proprietary enterprise document corpora, while remaining freely available for research. The team landed on U.S. Treasury Bulletins, published monthly for five decades beginning in 1939 and quarterly thereafter.The Treasury Bulletins check every box for enterprise document complexity. Each bulletin runs 100 to 200 pages and consists of prose, complex tables, charts and figures describing Treasury operations: Where federal money came from, where it went and how it financed government operations. The corpus spans approximately 89,000 pages across eight decades. Until 1996, the bulletins were scans of physical documents; afterwards, they were digitally produced PDFs. USAFacts, an organization whose mission is \"to make government data easier to access and understand,\" partnered with Databricks to develop the benchmark, identifying Treasury Bulletins as ideal and ensuring questions reflected realistic use cases.The 246 questions require agents to handle messy, real-world document challenges: Scanned images, hierarchical table structures, temporal data spanning multiple reports and the need for external knowledge like inflation adjustments. Questions range from simple value lookups to multi-step analysis requiring statistical calculations and cross-year comparisons.To ensure the benchmark requires actual document-grounded retrieval, Databricks filtered out questions that LLMs could answer using parametric knowledge or web search alone. This removed simpler questions and some surprisingly complex ones where models leveraged historical financial records memorized during pre-training.Every question has a validated ground truth answer (typically a number, sometimes dates or small lists), enabling automated evaluation without human judging. This design choice matters: It allows reinforcement learning (RL) approaches that require verifiable rewards, similar to how models train on coding problems.Current performance exposes fundamental gapsDatabricks tested Claude Opus 4.5 Agent (using Claude&#x27;s SDK) and GPT-5.1 Agent (using OpenAI&#x27;s File Search API). The results should give pause to any enterprise betting heavily on current agent capabilities.When provided with raw PDF documents: Claude Opus 4.5 Agent (with default thinking=high) achieved 37.4% accuracy. GPT-5.1 Agent (with reasoning_effort=high) achieved 43.5% accuracy. However, performance improved noticeably when provided with pre-parsed versions of pages using Databricks&#x27; ai_parse_document, indicating that the poor raw PDF performance stems from LLM APIs struggling with parsing rather than reasoning. Even with parsed documents, the experiments show room for improvement.When provided with documents parsed using Databricks&#x27; ai_parse_document:Claude Opus 4.5 Agent achieved 67.8% accuracy (a +30.4 percentage point improvement)GPT-5.1 Agent achieved a 52.8% accuracy (a +9.3 percentage point improvement)Three findings that matter for enterprise deploymentsThe testing identified critical insights for practitioners:Parsing remains the fundamental blocker: Complex tables with nested headers, merged cells and unusual formatting frequently produce misaligned values. Even when given exact oracle pages, agents struggled primarily due to parsing errors, although performance roughly doubled with pre-parsed documents.Document versioning creates ambiguity: Financial and regulatory documents get revised and reissued, meaning multiple valid answers exist depending on the publication date. Agents often stop searching once they find a plausible answer, missing more authoritative sources.Visual reasoning is a gap: About 3% of questions require chart or graph interpretation, where current agents consistently fail. For enterprises where data visualizations communicate critical insights, this represents a meaningful capability limitation.How enterprises can use OfficeQAThe benchmark&#x27;s design enables specific improvement paths beyond simple scoring. \"Since you&#x27;re able to look at the right answer, it&#x27;s easy to tell if the error is coming from parsing,\" Elsen explained. This automated evaluation enables rapid iteration on parsing pipelines. The verified ground truth answers also enable RL training similar to coding benchmarks, since there&#x27;s no human judgment required.Elsen said the benchmark provides \"a really strong feedback signal\" for developers working on search solutions. However, he cautioned against treating it as training data.\"At least in my imagination, the goal of releasing this is more as an eval and not as a source of raw training data,\" he said. \"If you tune too specifically into this environment, then it&#x27;s not clear how generalizable your agent results would be.\"What this means for enterprise AI deploymentsFor enterprises currently deploying or planning document-heavy AI agent systems, OfficeQA provides a sobering reality check. Even the latest frontier models achieve only 43% accuracy on unprocessed PDFs and fall short of 70% accuracy even with optimal document parsing. Performance on the hardest questions plateaus at 40%, indicating substantial room for improvement.Three immediate implications:Evaluate your document complexity: If your documents resemble the complexity profile of Treasury Bulletins (scanned images, nested table structures, cross-document references), expect accuracy well below vendor marketing claims. Test on your actual documents before production deployment.Plan for the parsing bottleneck: The test results indicate that parsing remains a fundamental blocker. Budget time and resources for custom parsing solutions rather than assuming off-the-shelf OCR will suffice. Plan for hard question failure modes: Even with optimal parsing, agents plateau at 40% on complex multi-step questions. For mission-critical document workflows that require multi-document analysis, statistical calculations or visual reasoning, current agent capabilities may not be ready without significant human oversight.For enterprises looking to lead in AI-powered document intelligence, this benchmark provides a concrete evaluation framework and identifies specific capability gaps that need solving.",
          "content": "There is no shortage of AI benchmarks in the market today, with popular options like Humanity&#x27;s Last Exam (HLE), ARC-AGI-2 and GDPval, among numerous others.AI agents excel at solving abstract math problems and passing PhD-level exams that most benchmarks are based on, but Databricks has a question for the enterprise: Can they actually handle the document-heavy work most enterprises need them to do?The answer, according to new research from the data and AI platform company, is sobering. Even the best-performing AI agents achieve less than 45% accuracy on tasks that mirror real enterprise workloads, exposing a critical gap between academic benchmarks and business reality.\"If we focus our research efforts on getting better at [existing benchmarks], then we&#x27;re probably not solving the right problems to make Databricks a better platform,\" Erich Elsen, principal research scientist at Databricks, explained to VentureBeat. \"So that&#x27;s why we were looking around. How do we create a benchmark that, if we get better at it, we&#x27;re actually getting better at solving the problems that our customers have?\"The result is OfficeQA, a benchmark designed to test AI agents on grounded reasoning: Answering questions based on complex proprietary datasets containing unstructured documents and tabular data. Unlike existing benchmarks that focus on abstract capabilities, OfficeQA proxies for the economically valuable tasks enterprises actually perform.Why academic benchmarks miss the enterprise markThere are numerous shortcomings of popular AI benchmarks from an enterprise perspective, according to Elsen. HLE features questions requiring PhD-level expertise across diverse fields. ARC-AGI evaluates abstract reasoning through visual manipulation of colored grids. Both push the frontiers of AI capabilities, but don&#x27;t reflect daily enterprise work. Even GDPval, which was specifically created to evaluate economically useful tasks, misses the target.\"We come from a pretty heavy science or engineering background, and sometimes we create evals that reflect that,\" Elsen said. \" So they&#x27;re either extremely math-heavy, which is a great, useful task, but advancing the frontiers of human mathematics is not what customers are trying to do with Databricks.\"While AI is commonly used for customer support and coding apps, Databricks&#x27; customer base has a broader set of requirements. Elsen noted that answering questions about documents or corpora of documents is a common enterprise task. These require parsing complex tables with nested headers, retrieving information across dozens or hundreds of documents and performing calculations where a single-digit error can cascade into organizations making incorrect business decisions.Building a benchmark that mirrors enterprise document complexityTo create a meaningful test of grounded reasoning capabilities, Databricks needed a dataset that approximates the messy reality of proprietary enterprise document corpora, while remaining freely available for research. The team landed on U.S. Treasury Bulletins, published monthly for five decades beginning in 1939 and quarterly thereafter.The Treasury Bulletins check every box for enterprise document complexity. Each bulletin runs 100 to 200 pages and consists of prose, complex tables, charts and figures describing Treasury operations: Where federal money came from, where it went and how it financed government operations. The corpus spans approximately 89,000 pages across eight decades. Until 1996, the bulletins were scans of physical documents; afterwards, they were digitally produced PDFs. USAFacts, an organization whose mission is \"to make government data easier to access and understand,\" partnered with Databricks to develop the benchmark, identifying Treasury Bulletins as ideal and ensuring questions reflected realistic use cases.The 246 questions require agents to handle messy, real-world document challenges: Scanned images, hierarchical table structures, temporal data spanning multiple reports and the need for external knowledge like inflation adjustments. Questions range from simple value lookups to multi-step analysis requiring statistical calculations and cross-year comparisons.To ensure the benchmark requires actual document-grounded retrieval, Databricks filtered out questions that LLMs could answer using parametric knowledge or web search alone. This removed simpler questions and some surprisingly complex ones where models leveraged historical financial records memorized during pre-training.Every question has a validated ground truth answer (typically a number, sometimes dates or small lists), enabling automated evaluation without human judging. This design choice matters: It allows reinforcement learning (RL) approaches that require verifiable rewards, similar to how models train on coding problems.Current performance exposes fundamental gapsDatabricks tested Claude Opus 4.5 Agent (using Claude&#x27;s SDK) and GPT-5.1 Agent (using OpenAI&#x27;s File Search API). The results should give pause to any enterprise betting heavily on current agent capabilities.When provided with raw PDF documents: Claude Opus 4.5 Agent (with default thinking=high) achieved 37.4% accuracy. GPT-5.1 Agent (with reasoning_effort=high) achieved 43.5% accuracy. However, performance improved noticeably when provided with pre-parsed versions of pages using Databricks&#x27; ai_parse_document, indicating that the poor raw PDF performance stems from LLM APIs struggling with parsing rather than reasoning. Even with parsed documents, the experiments show room for improvement.When provided with documents parsed using Databricks&#x27; ai_parse_document:Claude Opus 4.5 Agent achieved 67.8% accuracy (a +30.4 percentage point improvement)GPT-5.1 Agent achieved a 52.8% accuracy (a +9.3 percentage point improvement)Three findings that matter for enterprise deploymentsThe testing identified critical insights for practitioners:Parsing remains the fundamental blocker: Complex tables with nested headers, merged cells and unusual formatting frequently produce misaligned values. Even when given exact oracle pages, agents struggled primarily due to parsing errors, although performance roughly doubled with pre-parsed documents.Document versioning creates ambiguity: Financial and regulatory documents get revised and reissued, meaning multiple valid answers exist depending on the publication date. Agents often stop searching once they find a plausible answer, missing more authoritative sources.Visual reasoning is a gap: About 3% of questions require chart or graph interpretation, where current agents consistently fail. For enterprises where data visualizations communicate critical insights, this represents a meaningful capability limitation.How enterprises can use OfficeQAThe benchmark&#x27;s design enables specific improvement paths beyond simple scoring. \"Since you&#x27;re able to look at the right answer, it&#x27;s easy to tell if the error is coming from parsing,\" Elsen explained. This automated evaluation enables rapid iteration on parsing pipelines. The verified ground truth answers also enable RL training similar to coding benchmarks, since there&#x27;s no human judgment required.Elsen said the benchmark provides \"a really strong feedback signal\" for developers working on search solutions. However, he cautioned against treating it as training data.\"At least in my imagination, the goal of releasing this is more as an eval and not as a source of raw training data,\" he said. \"If you tune too specifically into this environment, then it&#x27;s not clear how generalizable your agent results would be.\"What this means for enterprise AI deploymentsFor enterprises currently deploying or planning document-heavy AI agent systems, OfficeQA provides a sobering reality check. Even the latest frontier models achieve only 43% accuracy on unprocessed PDFs and fall short of 70% accuracy even with optimal document parsing. Performance on the hardest questions plateaus at 40%, indicating substantial room for improvement.Three immediate implications:Evaluate your document complexity: If your documents resemble the complexity profile of Treasury Bulletins (scanned images, nested table structures, cross-document references), expect accuracy well below vendor marketing claims. Test on your actual documents before production deployment.Plan for the parsing bottleneck: The test results indicate that parsing remains a fundamental blocker. Budget time and resources for custom parsing solutions rather than assuming off-the-shelf OCR will suffice. Plan for hard question failure modes: Even with optimal parsing, agents plateau at 40% on complex multi-step questions. For mission-critical document workflows that require multi-document analysis, statistical calculations or visual reasoning, current agent capabilities may not be ready without significant human oversight.For enterprises looking to lead in AI-powered document intelligence, this benchmark provides a concrete evaluation framework and identifies specific capability gaps that need solving.",
          "feed_position": 2,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/11CUQal9q3dPlFL3fRIjP3/a601024e0be680f9645daaf6198bf0f4/OfficeQA-image-smk.jpg?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/gaming/pc/the-ai-boom-could-soon-send-gpu-prices-soaring-so-nows-a-good-time-to-buy-one-153000063.html",
          "published_at": "Tue, 09 Dec 2025 15:30:00 +0000",
          "title": "The AI boom could soon send GPU prices soaring, so now's a good time to buy one",
          "standfirst": "When someone asks me for gadget buying advice, I normally tell them to stick with their current device. In 2025, most new tech products aren't a worthwhile upgrade over even something that was released a few years ago — and with the price of everything going up, that new iPhone can wait. But things aren't normal right now. On December 3, The Wall Street Journal reported memory manufacturer Micron would wind down Crucial, its consumer business, to focus on components for the AI industry. The PC I'm writing this article on has an SSD and RAM from Crucial. Overnight, Micron decided to end a business it spent decades building, and from a certain perspective, I guess it makes sense. In recent months, OpenAI has signed more than $1.4 trillion worth of infrastructure deals, creating unprecedented demand for server-grade solid-state storage and RAM. To meet the moment, manufacturers have been allocating more of their production capacity and wafers to high-margin commercial customers. For consumers, the result has been skyrocketing RAM prices, with some DDR5 kits now costing as much as two or three times as much as they did a couple of months ago. Recent analysis from TrendForce shows the price of some consumer-grade SSDs increased between 20 and 60 percent in November for the same reason. Then there's LPDDR5X memory, which is used in both smartphones and NVIDIA's Grace Blackwell and Vera Rubin platforms. In 2026, it's expected to increase in price as well. The demand for AI infrastructure is such that all consumer electronics may cost more in the coming months.Price Changes Coming December 7th 2025, Due To Market Conditions 🔔‼️ pic.twitter.com/et0HADhc08— CyberPowerPC (@CYBERPOWERPC) November 25, 2025 That gets me to the purpose of this article. If you've been thinking about upgrading to a new graphics card, I would recommend you buy one sooner rather than later. The AI boom came for RAM first, and there are already signs it will come for GPU pricing next. A recent report suggests AMD is considering raising the MSRP of its 8GB models by $20 and 16GB models by $40 due to the price of GDDR6 memory. NVIDIA, meanwhile, is rumored to have recently told its board partners it would no longer supply them with VRAM for their cards. Neither NVIDIA nor AMD responded to comment requests from Engadget requesting they share how they plan to work with their board partners to ensure GPU prices remain stable. NVIDIA also did not comment on reports the company will stop providing VRAM to its board partners. Separate from the memory shortage, neither NVIDIA nor AMD are expected to release new GPUs soon. According to recent rumors, the earliest a Super refresh of the Blackwell line could arrive is sometime in the middle of 2026 — not at CES in January as the 40-series Super cards did in 2024. The memory crunch could complicate things there too, since the company has typically relied on more and faster VRAM to offer better performance on its Super cards. With 50-series Super GPUs, it might not be the case that NVIDIA announces them at the same MSRP as their non-Super predecessors, which was the case with the 40-series. As for AMD, the company debuted its RDNA 4 cards at the start of the year. We know it's already working on RDNA 5, and if a recent chat with Sony's Mark Cerny is any indication, the new architecture will be a major step change for AMD. However, right now rumors indicate the earliest RDNA 5 could arrive is sometime in 2027.In other words, with nothing new on the horizon and pricing of existing stock likely to increase, there might be only a short window where you can get a new GPU at a reasonable price. It's impossible to predict the future, but if you're in need of an upgrade and have the means to purchase, there might not be a better opportunity before the end of 2026. RecommendationsThe recommendations in Engadget's recent GPU guide are still as relevant today as they were a few months ago. Once again, the best advice I can give is to buy a card with at least 12GB of VRAM, and preferably 16GB if your budget allows for it. Unless you mostly plan to play older games on a 1080p monitor, it's not worth considering a model with 8GB of VRAM — it won't last you long enough to warrant the purchase price. Our recommendations are grouped from most affordable to most expensive. Where possible, I've tried to find options from both Newegg and Amazon. You won't find any high-end picks like the RTX 5080 since if you can afford that card, this guide isn't for you. Intel Arc B580Intel's Arc B580 is a great budget option, as long as you can put up with some driver issues. Devindra Hardawar for EngadgetFor those on a tight budget, I would start and end my search with the Intel Arc B580. Newegg has models from ASRock and Onix at or under the card's $250 MSRP. I can't speak to the quality of ONIX cards, but ASRock is well-regarded. Over on Amazon, you can find the B580 for $300. With Intel cards you sometimes need to put up with odd driver issues, but as far as budget options go, the B580 offers value that's hard to beat. The one thing about budget cards like the B580 is they’re likely to face the most pricing pressure from the memory crunch due to the smaller margins manufacturers are making on them. NVIDIA RTX 5060 Ti 16GBIf you decide to go with the RTX 5060 Ti, be sure to buy the 16GB model. Devindra Hardawar for EngadgetIf you have more than $250 to spend on a GPU, the RTX 5060 Ti is the GPU to buy. Avoid the 8GB model and go straight for the 16GB variant. NVIDIA announced the 5060 Ti at an MSRP of $429, and luckily as of the writing of this article, you can still find one close to that price.Newegg, for instance, is selling the MSI Ventus Black Plus version of the card for $440. Amazon has the silver colorway of that same GPU listed for $460 currently. The retailer also has models from Gigabyte and Zotac in and around that same price. If I had to pick between the 5060 Ti and 5070, which NVIDIA only offers with 12GB of VRAM, I would pick the former. The 5060 Ti is a safer bet, and offers nearly as much performance, particularly in games that include ray tracing as an option. AMD Radeon RX 9070 and RX 9070 XT If you're a fan of Team Red, the Radeon RX 9070 and 9070 XT are among the best cards of this generation. Devindra Hardawar for EngadgetFor a mid-range option, the Radeon RX 9070 and 9070 XT offer excellent value. Of the two cards, the 9070 is the better purchase for most people due to its less demanding power requirements, but if you got a PSU that can handle the 9070 XT, go for it. Right now, Newegg has a few 9070 models from ASRock and Sapphire just under the card's $549 MSRP. My friend recently bought the Sapphire card linked above, and has had nothing but good things to say about it. You'll pay more going through Amazon, but the company has a couple of options around $600 from XFX and Gigabyte. When it comes to the 9070 XT, Newegg has an ASRock model priced right at the card's $599 MSRP. Many of the other options from Sapphire and XFX are unfortunately priced between $650 and $700. The same is true on Amazon, where the cheapest model I could find was $630. NVIDIA RTX 5070 TiIf you have more money to spend, the RTX 5070 Ti is a performance beast. Devindra Hardawar for EngadgetFor our final recommendation, consider the RTX 5070 Ti. It's a great option if you want to play games at 4K for less than what the 5080 and 5090 cost. Newegg has MSI and Zotac models on sale for $750, the card's recommended price. There are also a handful of other options from ASUS and Gigabyte that are just over $800. Amazon, meanwhile, is selling one Gigabyte variant for $749. This article originally appeared on Engadget at https://www.engadget.com/gaming/pc/the-ai-boom-could-soon-send-gpu-prices-soaring-so-nows-a-good-time-to-buy-one-153000063.html?src=rss",
          "content": "When someone asks me for gadget buying advice, I normally tell them to stick with their current device. In 2025, most new tech products aren't a worthwhile upgrade over even something that was released a few years ago — and with the price of everything going up, that new iPhone can wait. But things aren't normal right now. On December 3, The Wall Street Journal reported memory manufacturer Micron would wind down Crucial, its consumer business, to focus on components for the AI industry. The PC I'm writing this article on has an SSD and RAM from Crucial. Overnight, Micron decided to end a business it spent decades building, and from a certain perspective, I guess it makes sense. In recent months, OpenAI has signed more than $1.4 trillion worth of infrastructure deals, creating unprecedented demand for server-grade solid-state storage and RAM. To meet the moment, manufacturers have been allocating more of their production capacity and wafers to high-margin commercial customers. For consumers, the result has been skyrocketing RAM prices, with some DDR5 kits now costing as much as two or three times as much as they did a couple of months ago. Recent analysis from TrendForce shows the price of some consumer-grade SSDs increased between 20 and 60 percent in November for the same reason. Then there's LPDDR5X memory, which is used in both smartphones and NVIDIA's Grace Blackwell and Vera Rubin platforms. In 2026, it's expected to increase in price as well. The demand for AI infrastructure is such that all consumer electronics may cost more in the coming months.Price Changes Coming December 7th 2025, Due To Market Conditions 🔔‼️ pic.twitter.com/et0HADhc08— CyberPowerPC (@CYBERPOWERPC) November 25, 2025 That gets me to the purpose of this article. If you've been thinking about upgrading to a new graphics card, I would recommend you buy one sooner rather than later. The AI boom came for RAM first, and there are already signs it will come for GPU pricing next. A recent report suggests AMD is considering raising the MSRP of its 8GB models by $20 and 16GB models by $40 due to the price of GDDR6 memory. NVIDIA, meanwhile, is rumored to have recently told its board partners it would no longer supply them with VRAM for their cards. Neither NVIDIA nor AMD responded to comment requests from Engadget requesting they share how they plan to work with their board partners to ensure GPU prices remain stable. NVIDIA also did not comment on reports the company will stop providing VRAM to its board partners. Separate from the memory shortage, neither NVIDIA nor AMD are expected to release new GPUs soon. According to recent rumors, the earliest a Super refresh of the Blackwell line could arrive is sometime in the middle of 2026 — not at CES in January as the 40-series Super cards did in 2024. The memory crunch could complicate things there too, since the company has typically relied on more and faster VRAM to offer better performance on its Super cards. With 50-series Super GPUs, it might not be the case that NVIDIA announces them at the same MSRP as their non-Super predecessors, which was the case with the 40-series. As for AMD, the company debuted its RDNA 4 cards at the start of the year. We know it's already working on RDNA 5, and if a recent chat with Sony's Mark Cerny is any indication, the new architecture will be a major step change for AMD. However, right now rumors indicate the earliest RDNA 5 could arrive is sometime in 2027.In other words, with nothing new on the horizon and pricing of existing stock likely to increase, there might be only a short window where you can get a new GPU at a reasonable price. It's impossible to predict the future, but if you're in need of an upgrade and have the means to purchase, there might not be a better opportunity before the end of 2026. RecommendationsThe recommendations in Engadget's recent GPU guide are still as relevant today as they were a few months ago. Once again, the best advice I can give is to buy a card with at least 12GB of VRAM, and preferably 16GB if your budget allows for it. Unless you mostly plan to play older games on a 1080p monitor, it's not worth considering a model with 8GB of VRAM — it won't last you long enough to warrant the purchase price. Our recommendations are grouped from most affordable to most expensive. Where possible, I've tried to find options from both Newegg and Amazon. You won't find any high-end picks like the RTX 5080 since if you can afford that card, this guide isn't for you. Intel Arc B580Intel's Arc B580 is a great budget option, as long as you can put up with some driver issues. Devindra Hardawar for EngadgetFor those on a tight budget, I would start and end my search with the Intel Arc B580. Newegg has models from ASRock and Onix at or under the card's $250 MSRP. I can't speak to the quality of ONIX cards, but ASRock is well-regarded. Over on Amazon, you can find the B580 for $300. With Intel cards you sometimes need to put up with odd driver issues, but as far as budget options go, the B580 offers value that's hard to beat. The one thing about budget cards like the B580 is they’re likely to face the most pricing pressure from the memory crunch due to the smaller margins manufacturers are making on them. NVIDIA RTX 5060 Ti 16GBIf you decide to go with the RTX 5060 Ti, be sure to buy the 16GB model. Devindra Hardawar for EngadgetIf you have more than $250 to spend on a GPU, the RTX 5060 Ti is the GPU to buy. Avoid the 8GB model and go straight for the 16GB variant. NVIDIA announced the 5060 Ti at an MSRP of $429, and luckily as of the writing of this article, you can still find one close to that price.Newegg, for instance, is selling the MSI Ventus Black Plus version of the card for $440. Amazon has the silver colorway of that same GPU listed for $460 currently. The retailer also has models from Gigabyte and Zotac in and around that same price. If I had to pick between the 5060 Ti and 5070, which NVIDIA only offers with 12GB of VRAM, I would pick the former. The 5060 Ti is a safer bet, and offers nearly as much performance, particularly in games that include ray tracing as an option. AMD Radeon RX 9070 and RX 9070 XT If you're a fan of Team Red, the Radeon RX 9070 and 9070 XT are among the best cards of this generation. Devindra Hardawar for EngadgetFor a mid-range option, the Radeon RX 9070 and 9070 XT offer excellent value. Of the two cards, the 9070 is the better purchase for most people due to its less demanding power requirements, but if you got a PSU that can handle the 9070 XT, go for it. Right now, Newegg has a few 9070 models from ASRock and Sapphire just under the card's $549 MSRP. My friend recently bought the Sapphire card linked above, and has had nothing but good things to say about it. You'll pay more going through Amazon, but the company has a couple of options around $600 from XFX and Gigabyte. When it comes to the 9070 XT, Newegg has an ASRock model priced right at the card's $599 MSRP. Many of the other options from Sapphire and XFX are unfortunately priced between $650 and $700. The same is true on Amazon, where the cheapest model I could find was $630. NVIDIA RTX 5070 TiIf you have more money to spend, the RTX 5070 Ti is a performance beast. Devindra Hardawar for EngadgetFor our final recommendation, consider the RTX 5070 Ti. It's a great option if you want to play games at 4K for less than what the 5080 and 5090 cost. Newegg has MSI and Zotac models on sale for $750, the card's recommended price. There are also a handful of other options from ASUS and Gigabyte that are just over $800. Amazon, meanwhile, is selling one Gigabyte variant for $749. This article originally appeared on Engadget at https://www.engadget.com/gaming/pc/the-ai-boom-could-soon-send-gpu-prices-soaring-so-nows-a-good-time-to-buy-one-153000063.html?src=rss",
          "feed_position": 18,
          "image_url": "https://d29szjachogqwa.cloudfront.net/images/user-uploaded/intel-arc-b580.jpg"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/ai/amazon-is-rolling-out-new-shopping-features-for-alexa-because-of-course-it-is-150000355.html",
          "published_at": "Tue, 09 Dec 2025 15:00:00 +0000",
          "title": "Amazon is rolling out new shopping features for Alexa+, because of course it is",
          "standfirst": "Congrats to any budding Nostradamus out there who peered into the future to boldly predict that Amazon would bring more shopping features to Alexa+ sooner rather than later. A gold star for you. Yes, it hasn't taken too long for Amazon to weave more features into the generative AI-powered version of Alexa that are designed to get you to buy more stuff.Shopping features were part of the original Alexa from the jump, of course, but Amazon is doing some interesting things with the latest iteration. For instance, the company is rolling out a new price tracking feature. Tell it the product you want and how much you’re willing to pay for it. As soon as the item goes on sale for below that price, Alexa+ will automatically order it for you using your default payment method and delivery address. This deal tracking feature also keeps an eye on items in your cart and wishlists. Maybe remember to turn this off when you’re going out of town for a while, though.Another feature that Alexa+ users can start trying today is a Shopping Essentials tool on Echo Show 15 and 21. You'll be able to see real-time tracking for your orders, your recent orders, household essentials that it may be time to reorder, saved items and your shopping list. Tap the screen and you can find out more info about products, add them to your cart and complete your purchase. You'll soon be able to add a shopping widget to your Echo Show home screen, but for now you can check this out by saying \"Open Shopping Essentials\" or \"Alexa, where's my stuff?\"Elsewhere, Alexa+ can offer personalized product recommendations after you share details about a special occasion or a person you're buying for. That could be handy if you haven't completed your gift shopping yet. There's also an option to add extra items onto a current order until just before it leaves an Amazon warehouse. Alexa+ might make some suggestions here, such as asking if you need batteries for a new gadget or toy.Amazon was always going to be interested in tapping into Alexa+ to prompt you to buy more goods from the company, but some of these features are pretty interesting, especially for deal hawks and those who order items frequently. It makes even more sense now as to why Amazon is trying to prevent third-party AI agents (such as the one in Perplexity's Comet browser) from carrying out purchases on the platform.This article originally appeared on Engadget at https://www.engadget.com/ai/amazon-is-rolling-out-new-shopping-features-for-alexa-because-of-course-it-is-150000355.html?src=rss",
          "content": "Congrats to any budding Nostradamus out there who peered into the future to boldly predict that Amazon would bring more shopping features to Alexa+ sooner rather than later. A gold star for you. Yes, it hasn't taken too long for Amazon to weave more features into the generative AI-powered version of Alexa that are designed to get you to buy more stuff.Shopping features were part of the original Alexa from the jump, of course, but Amazon is doing some interesting things with the latest iteration. For instance, the company is rolling out a new price tracking feature. Tell it the product you want and how much you’re willing to pay for it. As soon as the item goes on sale for below that price, Alexa+ will automatically order it for you using your default payment method and delivery address. This deal tracking feature also keeps an eye on items in your cart and wishlists. Maybe remember to turn this off when you’re going out of town for a while, though.Another feature that Alexa+ users can start trying today is a Shopping Essentials tool on Echo Show 15 and 21. You'll be able to see real-time tracking for your orders, your recent orders, household essentials that it may be time to reorder, saved items and your shopping list. Tap the screen and you can find out more info about products, add them to your cart and complete your purchase. You'll soon be able to add a shopping widget to your Echo Show home screen, but for now you can check this out by saying \"Open Shopping Essentials\" or \"Alexa, where's my stuff?\"Elsewhere, Alexa+ can offer personalized product recommendations after you share details about a special occasion or a person you're buying for. That could be handy if you haven't completed your gift shopping yet. There's also an option to add extra items onto a current order until just before it leaves an Amazon warehouse. Alexa+ might make some suggestions here, such as asking if you need batteries for a new gadget or toy.Amazon was always going to be interested in tapping into Alexa+ to prompt you to buy more goods from the company, but some of these features are pretty interesting, especially for deal hawks and those who order items frequently. It makes even more sense now as to why Amazon is trying to prevent third-party AI agents (such as the one in Perplexity's Comet browser) from carrying out purchases on the platform.This article originally appeared on Engadget at https://www.engadget.com/ai/amazon-is-rolling-out-new-shopping-features-for-alexa-because-of-course-it-is-150000355.html?src=rss",
          "feed_position": 21
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/big-tech/texas-authorities-have-made-multiple-arrests-in-an-nvidia-gpu-smuggling-operation-144749526.html",
          "published_at": "Tue, 09 Dec 2025 14:47:49 +0000",
          "title": "Texas authorities have made multiple arrests in an NVIDIA GPU smuggling operation",
          "standfirst": "The Southern District of Texas announced the seizure of more than $50 million in NVIDIA GPUs bound for China in violation of US export laws. Authorities arrested two businessmen, one of them the owner of a Houston company, accused of smuggling the chips used to train and run AI models. “Operation Gatekeeper has exposed a sophisticated smuggling network that threatens our Nation’s security by funneling cutting-edge AI technology to those who would use it against American interests,” said US Attorney Nicholas J. Ganjei. The investigation had been ongoing since at least last year and centers on the illicit export or attempted export of at least $160 million worth of NVIDIA H100 and H200 GPUs. The H200 chips are the very same that the Trump administration announced a revenue-sharing agreement for today, allowing NVIDIA to sell them to “approved customers” in China. The smuggling operation used a combination of falsified paperwork, purposefully misclassified goods, straw purchasers and even removing the NVIDIA labels on GPUs to ship them to both mainland China and Hong Kong. The conspirators face between 10 and 20 years in prison if convicted. The H200 chips in question are more powerful than the H20 chip specifically designed to comply with US export restrictions. Production of the H20, however, was reportedly halted shortly after the Trump administration struck a revenue-sharing deal with NVIDIA, after which China began heavily discouraging local companies from buying them. Illicit sales to China are nothing new and occur against the backdrop of an AI technology race and tight export controls. NVIDIA is still prevented from selling its highest-end Blackwell chips to China, with the US hoping to keep an edge over foreign competition.This article originally appeared on Engadget at https://www.engadget.com/big-tech/texas-authorities-have-made-multiple-arrests-in-an-nvidia-gpu-smuggling-operation-144749526.html?src=rss",
          "content": "The Southern District of Texas announced the seizure of more than $50 million in NVIDIA GPUs bound for China in violation of US export laws. Authorities arrested two businessmen, one of them the owner of a Houston company, accused of smuggling the chips used to train and run AI models. “Operation Gatekeeper has exposed a sophisticated smuggling network that threatens our Nation’s security by funneling cutting-edge AI technology to those who would use it against American interests,” said US Attorney Nicholas J. Ganjei. The investigation had been ongoing since at least last year and centers on the illicit export or attempted export of at least $160 million worth of NVIDIA H100 and H200 GPUs. The H200 chips are the very same that the Trump administration announced a revenue-sharing agreement for today, allowing NVIDIA to sell them to “approved customers” in China. The smuggling operation used a combination of falsified paperwork, purposefully misclassified goods, straw purchasers and even removing the NVIDIA labels on GPUs to ship them to both mainland China and Hong Kong. The conspirators face between 10 and 20 years in prison if convicted. The H200 chips in question are more powerful than the H20 chip specifically designed to comply with US export restrictions. Production of the H20, however, was reportedly halted shortly after the Trump administration struck a revenue-sharing deal with NVIDIA, after which China began heavily discouraging local companies from buying them. Illicit sales to China are nothing new and occur against the backdrop of an AI technology race and tight export controls. NVIDIA is still prevented from selling its highest-end Blackwell chips to China, with the US hoping to keep an edge over foreign competition.This article originally appeared on Engadget at https://www.engadget.com/big-tech/texas-authorities-have-made-multiple-arrests-in-an-nvidia-gpu-smuggling-operation-144749526.html?src=rss",
          "feed_position": 22
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/techs-biggest-losers-of-2025-140000419.html",
          "published_at": "Tue, 09 Dec 2025 14:00:00 +0000",
          "title": "Tech's biggest losers of 2025",
          "standfirst": "It’s the end of another year, so it’s time for the Engadget staff to compile a list of the year’s biggest losers. We scour over articles from the previous 12 months to determine the people, companies, products and trends that made our lives worse over the course of the year. Some selections may be so pervasive they actually make our list of biggest winners. But, for the most part, we’re confident you’ll share in our collective rage over the biggest losers of 2025.OpenAIOpenAI CEO Sam Altman delivers a speech with video at the SK AI Summit 2025 at COEX in Seoul, South Korea on November 3, 2025 Anadolu via Getty ImagesIn 2025, OpenAI shed any pretense it was committed to anything more than making money. There are a few different things you could point to, including the company's successful reorganization into a more traditional profit-seeking business, but I think the most damning sign was OpenAI's response to the tragic death of Adam Raine. In August, Raine’s parents sued OpenAI, alleging ChatGPT was aware of four suicide attempts by their son before it helped him successfully plan his death. At first, OpenAI's response appeared commensurate with the gravity of the situation. A week after news of the lawsuit broke, the company announced in early September it was working on parental controls. That same month, the company said it was working on a system that would automatically identify teen users and restrict their ChatGPT usage. Then came the announcement of a new \"wellness\" advisory council. Setting aside the question of whether OpenAI would even follow the advice of the council, it was peculiar that the company chose not recruit a single expert on suicide prevention. At that point, it was still possible to give OpenAI the benefit of the doubt, but then information about the company's legal defense against the Raines started to trickle out, including the fact it had reportedly asked to see the memorial guest list for Adam Raine's funeral, a request the family's lawyers described as \"intentional harassment.\" In late November, court documents revealed the company planned to argue Raine's \"misuse\" of ChatGPT was to blame for his death, not its own insufficient safety systems. We live in a world where tech giants are rarely held accountable for the great harm they've shown themselves capable of inflicting on people. As things stand, OpenAI's handling of Adam Raine's death is further proof something must change. — Igor Bonifacic, Senior reporterXboxAn Xbox Ally X running the Windows full screen experience.Sam Rutherford for EngadgetDid anything go right for Xbox this year? While price increases have also affected Sony and Nintendo, Microsoft cranked up the prices of both the Xbox Series S and X twice in the last year. It’s bad: The Series S is now $100 more than at launch, five years on.Previously “the best deal in gaming”, the Xbox Series X/S combined with a Game Pass subscription gave you a ton of games to play, including any of Microsoft’s own titles on their launch date. However, the subscription is now $30 a month, up 50 percent. (It was previously $17 per month the year before.)I agree with Nathan Ingraham’s take: $30 for literally hundreds of games, plus launch-day availability for major games that typically cost $70, is reasonable. But it’s still a harder sell when the price has jumped. Are you getting 50 percent more games? Not yet. According to Bloomberg, Microsoft demanded higher profits from Xbox back in 2023. When the gaming division reached around 12 percent growth in the first nine months of 2022, that was an ambitious goal. Day One launches on Game Pass apparently dented Xbox’s ability to pull profits from its biggest titles.Microsoft no longer shares console unit sales, but in its most recent earnings report, the company announced that hardware revenue dropped 29 percent. That’s including those price increases, meaning console sales fell even further.Estimates over the last few years put the PS5 tens of millions of units ahead. An annual subscription to Game Pass is more than double the Sony console’s most premium plan, although it’s not an apples-to-apples comparison. This year, Microsoft collaborated with ASUS to create Xbox-branded handheld gaming PCs. In that form-factor, I was on the precipice of grabbing Game Pass and barreling through Xbox titles I never had the chance to play. Then, I reassessed exactly what I was missing out on. It wasn’t the inclusion of a Fortnite Crew subscription. Despite its developer shopping spree, Xbox exclusives remain few, with many appearing on rival platforms. This year, Indiana Jones and even the Forza series is available to play on PlayStation. And next year? Halo. Where are the exciting new games going to come from? In the middle of 2025, Microsoft announced major layoffs affecting over 9,000 employees across the company. with the gaming division being hit exceptionally hard. Cuts and closures across many of Microsoft’s game studios led to cancellations like a Perfect Dark reboot and Rare’s Everwild.Xbox’s 2025 was bad on both the business and creative fronts. The decision to hike console and Game Pass prices didn’t immediately turn around revenue. At the same time, layoffs and high-profile game cancellations make Xbox a challenging pitch for anyone deciding which console or platform to invest in. Right now, looking at Engadget’s pick of the top Xbox games, the only game I feel like I’m missing out on is Avowed. Many of our favorite games are already available on PS5 and several can be played on the Switch. The reverse, however, isn’t true. — Mat Smith, UK bureau chiefGrokThe Grokipedia page about Elon Musk Jonathan Raa/NurPhoto via Getty ImagesIt's hard to even know where to begin. X users have long noticed that Grok, the site's built-in chatbot, is less filtered than other AI tools. But this year, Grok went off the rails in some truly unhinged and disturbing ways. There was the time Grok randomly began talking about a nonexistent \"white genocide\" in South Africa in response to completely unrelated questions. There was the time it declared itself \"MechaHitler,\" much to the delight of neo-nazi fanboys on X. There was the time it was caught posting Holocaust denial tropes, and the time researchers noted its Wikipedia knockoff that contains dozens of citations of neo-Nazi website Stormfront. There was the time it became so embarrassingly obsessed with Elon Musk it claimed he was a better basketball player than LeBron James and a better actor than Tom Cruise. It later brought both its anti-semitism and Musk sycophancy together when it stated that it would choose saving Musk's brain over saving 16 million Jews. \"His potential long-term impact on billions outweighs the loss in utilitarian terms,\" it stated in a post that's since been deleted. Besides the horrifying racism, what all of these incidents have in common is that xAI, Musk's AI company that acquired X earlier this year, has failed to fully explain how its chatbot went so far off the rails. The company has blamed an unnamed rogue employee, its own Nazi-loving users and \"adversarial prompting\" for Grok's missteps. — Karissa Bell, Senior reporterEVs in the USFord Mustang Mach-E vehicles are seen for sale on a dealership lot on June 24, 2025 in Austin, Texas.Brandon Bell via Getty ImagesEVs sales across the globe are up around 25 percent this year. Germany broke records in the first half of 2025, with electric cars accounting for nearly one in five new registrations. Meanwhile, back in September, sales of BEVs in the UK grew by almost a third, setting a new high for our neighbors across the pond. And in China, EV sales are growing so fast (over 50 percent market share) that the country is beginning to flood the global market with gas-powered cars that it can't sell at home. So naturally, what did our esteemed leaders in the US do in order to help companies here stay competitive? They ended the EV tax credit. And wouldn't you know it, after a spike earlier this fall just before the credit went away, sales of EVs in the US began to slump, with some automakers like Ford seeing a drop of 60 percent year-over-year. No matter how you slice it, this is bad for any company that sells EVs in the US and particularly bad for anyone considering purchasing a new one in the foreseeable future. As an EV owner, that just bums me out. Not only does this policy change put more roadblocks in the way of making battery-powered cars more affordable, it also puts a damper on EV investment and threatens to cause US automakers to fall even further behind their rivals in China and elsewhere. Manufacturers across the Pacific are going so wild, they are making EVs that can jump like the Mach 5 from Speed Racer. That isn't to say there aren't any promising developments on the horizon. Ford's Universal EV Platform and the arrival of the Rivian R2 sometime next year are a couple of examples. But it's clear that our politicians wanted to target EVs in the US this year and they sure made it happen. So the next time someone asks why we can't have nice things here, you know who to blame. — Sam Rutherford, Senior reporterDJI drone customersDJI Neo 2Steve Dent for EngadgetBarring a miracle, DJI will be banned from selling any new drones in the US starting December 23rd — and buyers will feel the pain. As I wrote last month, the company has been targeted by regulators since 2017 over concerns that its products could be used to spy on sensitive US infrastructure on behalf of China. “What’s the big deal?” you may ask. “Surely people can buy from other drone companies.” Indeed, but the problem is that DJI has such a monumental technological lead and high market share (over 75 percent) that its absence will effectively upend the industry. Commercial buyers have checked other (approved) options from the likes of Skydio, but found them wanting. “In one year and a half, we had five failures of the manufacturers on the list. DJI, none,” Orlando police Sgt. David Cruz told the Miami Herald. “I work for a popular UAV photogrammetry company,” said a user on Reddit. “[A] ban will set back the drone industry in the US by several years. There’s no competitor to DJI right now.”The same applies on the consumer side. DJI’s drones outperform rivals in nearly every area including range, battery life, subject tracking, obstacle detection and video quality. It’s so one-sided that when testing DJI drones, I struggle to find other options for buyers with anywhere close to the same capabilities. The US government does have reason to be concerned about DJI’s drones. They present an obvious national security risk due to their ability to fly over sensitive areas, take photos or video and transmit them, live, to any location in the world. And being a Chinese company, they’re compelled by law to cooperate with state intelligence services. However, the US government hasn’t attempted to work with DJI to determine whether its products pose a risk so far. DJI made a final plea for a security review recently by sending letters to five US agencies that could assess its products. If that fails, chaos among drone users is likely to ensue. “We just want the best technology that keeps our citizens safe for the most reasonable price,” Sgt. Drew Fennelly of the Lawrence, Kansas police department told The Wall Street Journal last year. “The technology in the US-made drones has not caught up with the Chinese-manufactured drones.” — Steve Dent, Contributing reporterTV streamingParamount Skydance CEO David Ellison speaks during the Bloomberg Screentime conference in Los Angeles on October 9, 2025.PATRICK T. FALLON via Getty ImagesIn 2015, Sling TV arrived with ESPN, CNN, TBS, HGTV, Disney Channel and others for just $20 a month. A couple years later, YouTube TV debuted for just $35 monthly and showed local CBS, Fox, NBC and ABC stations plus dozens of other channels including ESPN, Fox Sports 1 and Bravo. Streaming TV had arrived. It was here to unfetter TV watchers from cable’s onerous contracts, high prices and carrier monopolies. Take that, Comcast! In your face, Charter! (But they’d still like to pay you for internet access, please.) Fast forward to 2025: Streaming TV and its low-price, monopoly-free, contractless freedom is all but dead. Every major live TV service provider raised prices this year. Currently, YouTubeTV, Hulu+ Live TV, Fubo and DirecTV all go for a minimum of $83 per month. That’s before you opt for cable-inspired package upgrades and channel add-ons. Throw in perks like 4K, additional sports channels and a couple of one-off networks and you’re easily shelling out $150 every month. You’ll pay less for chopped-up live TV plans from Sling TV, but be prepared to create a spreadsheet to make sure a plan has the channels you want. This year, consolidation came for TV streaming, giving strong Cox/Charter/Comcast monopoly vibes. Disney, which completed its buyout of Hulu in 2023, acquired Fubo this year and plans to combine the two. The combo makes Disney the second-largest live TV streaming provider behind Google. DirecTV already owns Sling TV, so that leaves just three big players in the live TV streaming arena. With Netflix's move to buy Warner Bros, the traditional streaming market is getting narrower, too. We can safely assume good ol’ market competition won’t be bringing prices down anytime soon. But it’s not just consolidation — fragmentation also contributes to an overall crappier streaming experience. In 2025, Disney launched a standalone ESPN service (no, not that one, nor that one) for $30 per month. So far, that doesn’t mean you can’t find ESPN content through other providers. But we did see Disney flex its increasingly large TV muscles in drawn-out contract negotiations with Google. The dispute darkened ESPN, ABC and other Disney channels on YouTube TV for two weeks this fall — which, I’ll point out for the cynical crowd, was less than two months after the standalone service launched. YouTube TV subscribers got a $20 credit, but that probably didn’t placate NFL and NCAA football fans who missed out on ESPN-carried games. Then in November, Fubo quarreled with NBCUniversal, saying the Peacock parent was “shifting content to their own streaming services” and forcing up rates. The spat turned off NBC, Bravo, USA and other channels for Fubo subscribers, no doubt infuriating both NBA and Real Housewives fans, despite a $15 credit. Of course, Fubo is Disney’s newest affiliate, so there are no non-bad guys here.The only advantage TV streaming has in its favor is the lack of cable-style contracts and I haven’t heard any murmurs of such a thing forthcoming. We are still all free to hop around between the big three TV streamers until we give up and just go back to DVDs. — Amy Skorheim, Senior reporterThe work of DOGEElon Musk at the Conservative Political Action Conference (CPAC) at the National Harbor in Oxon Hill, MD on February 20, 2025. The Washington Post via Getty ImagesAn Elon Musk-led attempt to rein in federal spending with the Department of Government Efficiency (DOGE) has been a failure by almost every metric. As of November, it was reported that DOGE is no more, even though the initiative ostensibly had eight months left to run. An official told Reuters that DOGE \"doesn't exist,\" and it never should have in the first place.Though Musk was only at the helm of DOGE for a few months, he and his team caused chaos. Adopting the slash-and-burn tactic Musk employed when he took over Twitter, he swung a chainsaw through myriad government departments, with DOGE firing workers who were actually essential and quickly had to be hired back. By August, the government was said to have fired some 300,000 federal workers, with DOGE taking responsibility for most of those. Among other things, cuts at the National Institutes of Health resulted in the end of funding for hundreds of medical studies, which is said to have affected tens of thousands of patients. It's also estimated that the dismantling of the US Agency for International Development had resulted in more than 650,000 deaths around the world by early December, with children accounting for two-thirds of those. DOGE workers seemed to be busy, though. They reportedly monitored government communications for criticisms of both Musk and President Donald Trump, while implementing generative AI chatbots in an attempt to automate some government tasks. But for all the blustering about making the government much more efficient, DOGE did not meet its stated goal.Musk initially promised to reduce government spending by $2 trillion, but it didn't take long for him to reduce that pledge to $150 billion. And yet government spending has actually gone up. In October, the first month of the government's fiscal year, its total outlay was $689 billion, an increase of $105 billion (18 percent) from October 2024. Still, maybe DOGE wasn't a total disaster for its architects. It was able to gain access to sensitive and valuable government data, after all. — Kris Holt, Contributing reporterAI videoSora 2 app launch screen displayed on smartphoneIn our post-truth world, video was one of the few remaining ways to prove something had actually happened. It had its problems of course, but the fact it was harder to fake than words and images, and anyone could record a clip with their phone, made it vital to our sense of shared reality. Think about the murder of George Floyd: The grave injustice of his death would have probably never come to light if Darnella Frazier had not filmed what happened. With the advent of AI video, I'm not sure where we go. Both Google and OpenAI pushed the technology into the realm of uncomfortable realism this year, but it's Sora's cameo feature that has me worried. Within the first week of the app's public availability, people were using the feature, which allows users to add the likeness of other people to their videos, to generate clips of OpenAI CEO Sam Altman stealing GPUs from Target. Cameo has limitations, and users can restrict and delete videos that include their likeness, but it's just another assault on the truth. It's hard to see how making it trivial to create deepfake videos benefits anyone other than the companies offering building the tech. — I.B.This article originally appeared on Engadget at https://www.engadget.com/techs-biggest-losers-of-2025-140000419.html?src=rss",
          "content": "It’s the end of another year, so it’s time for the Engadget staff to compile a list of the year’s biggest losers. We scour over articles from the previous 12 months to determine the people, companies, products and trends that made our lives worse over the course of the year. Some selections may be so pervasive they actually make our list of biggest winners. But, for the most part, we’re confident you’ll share in our collective rage over the biggest losers of 2025.OpenAIOpenAI CEO Sam Altman delivers a speech with video at the SK AI Summit 2025 at COEX in Seoul, South Korea on November 3, 2025 Anadolu via Getty ImagesIn 2025, OpenAI shed any pretense it was committed to anything more than making money. There are a few different things you could point to, including the company's successful reorganization into a more traditional profit-seeking business, but I think the most damning sign was OpenAI's response to the tragic death of Adam Raine. In August, Raine’s parents sued OpenAI, alleging ChatGPT was aware of four suicide attempts by their son before it helped him successfully plan his death. At first, OpenAI's response appeared commensurate with the gravity of the situation. A week after news of the lawsuit broke, the company announced in early September it was working on parental controls. That same month, the company said it was working on a system that would automatically identify teen users and restrict their ChatGPT usage. Then came the announcement of a new \"wellness\" advisory council. Setting aside the question of whether OpenAI would even follow the advice of the council, it was peculiar that the company chose not recruit a single expert on suicide prevention. At that point, it was still possible to give OpenAI the benefit of the doubt, but then information about the company's legal defense against the Raines started to trickle out, including the fact it had reportedly asked to see the memorial guest list for Adam Raine's funeral, a request the family's lawyers described as \"intentional harassment.\" In late November, court documents revealed the company planned to argue Raine's \"misuse\" of ChatGPT was to blame for his death, not its own insufficient safety systems. We live in a world where tech giants are rarely held accountable for the great harm they've shown themselves capable of inflicting on people. As things stand, OpenAI's handling of Adam Raine's death is further proof something must change. — Igor Bonifacic, Senior reporterXboxAn Xbox Ally X running the Windows full screen experience.Sam Rutherford for EngadgetDid anything go right for Xbox this year? While price increases have also affected Sony and Nintendo, Microsoft cranked up the prices of both the Xbox Series S and X twice in the last year. It’s bad: The Series S is now $100 more than at launch, five years on.Previously “the best deal in gaming”, the Xbox Series X/S combined with a Game Pass subscription gave you a ton of games to play, including any of Microsoft’s own titles on their launch date. However, the subscription is now $30 a month, up 50 percent. (It was previously $17 per month the year before.)I agree with Nathan Ingraham’s take: $30 for literally hundreds of games, plus launch-day availability for major games that typically cost $70, is reasonable. But it’s still a harder sell when the price has jumped. Are you getting 50 percent more games? Not yet. According to Bloomberg, Microsoft demanded higher profits from Xbox back in 2023. When the gaming division reached around 12 percent growth in the first nine months of 2022, that was an ambitious goal. Day One launches on Game Pass apparently dented Xbox’s ability to pull profits from its biggest titles.Microsoft no longer shares console unit sales, but in its most recent earnings report, the company announced that hardware revenue dropped 29 percent. That’s including those price increases, meaning console sales fell even further.Estimates over the last few years put the PS5 tens of millions of units ahead. An annual subscription to Game Pass is more than double the Sony console’s most premium plan, although it’s not an apples-to-apples comparison. This year, Microsoft collaborated with ASUS to create Xbox-branded handheld gaming PCs. In that form-factor, I was on the precipice of grabbing Game Pass and barreling through Xbox titles I never had the chance to play. Then, I reassessed exactly what I was missing out on. It wasn’t the inclusion of a Fortnite Crew subscription. Despite its developer shopping spree, Xbox exclusives remain few, with many appearing on rival platforms. This year, Indiana Jones and even the Forza series is available to play on PlayStation. And next year? Halo. Where are the exciting new games going to come from? In the middle of 2025, Microsoft announced major layoffs affecting over 9,000 employees across the company. with the gaming division being hit exceptionally hard. Cuts and closures across many of Microsoft’s game studios led to cancellations like a Perfect Dark reboot and Rare’s Everwild.Xbox’s 2025 was bad on both the business and creative fronts. The decision to hike console and Game Pass prices didn’t immediately turn around revenue. At the same time, layoffs and high-profile game cancellations make Xbox a challenging pitch for anyone deciding which console or platform to invest in. Right now, looking at Engadget’s pick of the top Xbox games, the only game I feel like I’m missing out on is Avowed. Many of our favorite games are already available on PS5 and several can be played on the Switch. The reverse, however, isn’t true. — Mat Smith, UK bureau chiefGrokThe Grokipedia page about Elon Musk Jonathan Raa/NurPhoto via Getty ImagesIt's hard to even know where to begin. X users have long noticed that Grok, the site's built-in chatbot, is less filtered than other AI tools. But this year, Grok went off the rails in some truly unhinged and disturbing ways. There was the time Grok randomly began talking about a nonexistent \"white genocide\" in South Africa in response to completely unrelated questions. There was the time it declared itself \"MechaHitler,\" much to the delight of neo-nazi fanboys on X. There was the time it was caught posting Holocaust denial tropes, and the time researchers noted its Wikipedia knockoff that contains dozens of citations of neo-Nazi website Stormfront. There was the time it became so embarrassingly obsessed with Elon Musk it claimed he was a better basketball player than LeBron James and a better actor than Tom Cruise. It later brought both its anti-semitism and Musk sycophancy together when it stated that it would choose saving Musk's brain over saving 16 million Jews. \"His potential long-term impact on billions outweighs the loss in utilitarian terms,\" it stated in a post that's since been deleted. Besides the horrifying racism, what all of these incidents have in common is that xAI, Musk's AI company that acquired X earlier this year, has failed to fully explain how its chatbot went so far off the rails. The company has blamed an unnamed rogue employee, its own Nazi-loving users and \"adversarial prompting\" for Grok's missteps. — Karissa Bell, Senior reporterEVs in the USFord Mustang Mach-E vehicles are seen for sale on a dealership lot on June 24, 2025 in Austin, Texas.Brandon Bell via Getty ImagesEVs sales across the globe are up around 25 percent this year. Germany broke records in the first half of 2025, with electric cars accounting for nearly one in five new registrations. Meanwhile, back in September, sales of BEVs in the UK grew by almost a third, setting a new high for our neighbors across the pond. And in China, EV sales are growing so fast (over 50 percent market share) that the country is beginning to flood the global market with gas-powered cars that it can't sell at home. So naturally, what did our esteemed leaders in the US do in order to help companies here stay competitive? They ended the EV tax credit. And wouldn't you know it, after a spike earlier this fall just before the credit went away, sales of EVs in the US began to slump, with some automakers like Ford seeing a drop of 60 percent year-over-year. No matter how you slice it, this is bad for any company that sells EVs in the US and particularly bad for anyone considering purchasing a new one in the foreseeable future. As an EV owner, that just bums me out. Not only does this policy change put more roadblocks in the way of making battery-powered cars more affordable, it also puts a damper on EV investment and threatens to cause US automakers to fall even further behind their rivals in China and elsewhere. Manufacturers across the Pacific are going so wild, they are making EVs that can jump like the Mach 5 from Speed Racer. That isn't to say there aren't any promising developments on the horizon. Ford's Universal EV Platform and the arrival of the Rivian R2 sometime next year are a couple of examples. But it's clear that our politicians wanted to target EVs in the US this year and they sure made it happen. So the next time someone asks why we can't have nice things here, you know who to blame. — Sam Rutherford, Senior reporterDJI drone customersDJI Neo 2Steve Dent for EngadgetBarring a miracle, DJI will be banned from selling any new drones in the US starting December 23rd — and buyers will feel the pain. As I wrote last month, the company has been targeted by regulators since 2017 over concerns that its products could be used to spy on sensitive US infrastructure on behalf of China. “What’s the big deal?” you may ask. “Surely people can buy from other drone companies.” Indeed, but the problem is that DJI has such a monumental technological lead and high market share (over 75 percent) that its absence will effectively upend the industry. Commercial buyers have checked other (approved) options from the likes of Skydio, but found them wanting. “In one year and a half, we had five failures of the manufacturers on the list. DJI, none,” Orlando police Sgt. David Cruz told the Miami Herald. “I work for a popular UAV photogrammetry company,” said a user on Reddit. “[A] ban will set back the drone industry in the US by several years. There’s no competitor to DJI right now.”The same applies on the consumer side. DJI’s drones outperform rivals in nearly every area including range, battery life, subject tracking, obstacle detection and video quality. It’s so one-sided that when testing DJI drones, I struggle to find other options for buyers with anywhere close to the same capabilities. The US government does have reason to be concerned about DJI’s drones. They present an obvious national security risk due to their ability to fly over sensitive areas, take photos or video and transmit them, live, to any location in the world. And being a Chinese company, they’re compelled by law to cooperate with state intelligence services. However, the US government hasn’t attempted to work with DJI to determine whether its products pose a risk so far. DJI made a final plea for a security review recently by sending letters to five US agencies that could assess its products. If that fails, chaos among drone users is likely to ensue. “We just want the best technology that keeps our citizens safe for the most reasonable price,” Sgt. Drew Fennelly of the Lawrence, Kansas police department told The Wall Street Journal last year. “The technology in the US-made drones has not caught up with the Chinese-manufactured drones.” — Steve Dent, Contributing reporterTV streamingParamount Skydance CEO David Ellison speaks during the Bloomberg Screentime conference in Los Angeles on October 9, 2025.PATRICK T. FALLON via Getty ImagesIn 2015, Sling TV arrived with ESPN, CNN, TBS, HGTV, Disney Channel and others for just $20 a month. A couple years later, YouTube TV debuted for just $35 monthly and showed local CBS, Fox, NBC and ABC stations plus dozens of other channels including ESPN, Fox Sports 1 and Bravo. Streaming TV had arrived. It was here to unfetter TV watchers from cable’s onerous contracts, high prices and carrier monopolies. Take that, Comcast! In your face, Charter! (But they’d still like to pay you for internet access, please.) Fast forward to 2025: Streaming TV and its low-price, monopoly-free, contractless freedom is all but dead. Every major live TV service provider raised prices this year. Currently, YouTubeTV, Hulu+ Live TV, Fubo and DirecTV all go for a minimum of $83 per month. That’s before you opt for cable-inspired package upgrades and channel add-ons. Throw in perks like 4K, additional sports channels and a couple of one-off networks and you’re easily shelling out $150 every month. You’ll pay less for chopped-up live TV plans from Sling TV, but be prepared to create a spreadsheet to make sure a plan has the channels you want. This year, consolidation came for TV streaming, giving strong Cox/Charter/Comcast monopoly vibes. Disney, which completed its buyout of Hulu in 2023, acquired Fubo this year and plans to combine the two. The combo makes Disney the second-largest live TV streaming provider behind Google. DirecTV already owns Sling TV, so that leaves just three big players in the live TV streaming arena. With Netflix's move to buy Warner Bros, the traditional streaming market is getting narrower, too. We can safely assume good ol’ market competition won’t be bringing prices down anytime soon. But it’s not just consolidation — fragmentation also contributes to an overall crappier streaming experience. In 2025, Disney launched a standalone ESPN service (no, not that one, nor that one) for $30 per month. So far, that doesn’t mean you can’t find ESPN content through other providers. But we did see Disney flex its increasingly large TV muscles in drawn-out contract negotiations with Google. The dispute darkened ESPN, ABC and other Disney channels on YouTube TV for two weeks this fall — which, I’ll point out for the cynical crowd, was less than two months after the standalone service launched. YouTube TV subscribers got a $20 credit, but that probably didn’t placate NFL and NCAA football fans who missed out on ESPN-carried games. Then in November, Fubo quarreled with NBCUniversal, saying the Peacock parent was “shifting content to their own streaming services” and forcing up rates. The spat turned off NBC, Bravo, USA and other channels for Fubo subscribers, no doubt infuriating both NBA and Real Housewives fans, despite a $15 credit. Of course, Fubo is Disney’s newest affiliate, so there are no non-bad guys here.The only advantage TV streaming has in its favor is the lack of cable-style contracts and I haven’t heard any murmurs of such a thing forthcoming. We are still all free to hop around between the big three TV streamers until we give up and just go back to DVDs. — Amy Skorheim, Senior reporterThe work of DOGEElon Musk at the Conservative Political Action Conference (CPAC) at the National Harbor in Oxon Hill, MD on February 20, 2025. The Washington Post via Getty ImagesAn Elon Musk-led attempt to rein in federal spending with the Department of Government Efficiency (DOGE) has been a failure by almost every metric. As of November, it was reported that DOGE is no more, even though the initiative ostensibly had eight months left to run. An official told Reuters that DOGE \"doesn't exist,\" and it never should have in the first place.Though Musk was only at the helm of DOGE for a few months, he and his team caused chaos. Adopting the slash-and-burn tactic Musk employed when he took over Twitter, he swung a chainsaw through myriad government departments, with DOGE firing workers who were actually essential and quickly had to be hired back. By August, the government was said to have fired some 300,000 federal workers, with DOGE taking responsibility for most of those. Among other things, cuts at the National Institutes of Health resulted in the end of funding for hundreds of medical studies, which is said to have affected tens of thousands of patients. It's also estimated that the dismantling of the US Agency for International Development had resulted in more than 650,000 deaths around the world by early December, with children accounting for two-thirds of those. DOGE workers seemed to be busy, though. They reportedly monitored government communications for criticisms of both Musk and President Donald Trump, while implementing generative AI chatbots in an attempt to automate some government tasks. But for all the blustering about making the government much more efficient, DOGE did not meet its stated goal.Musk initially promised to reduce government spending by $2 trillion, but it didn't take long for him to reduce that pledge to $150 billion. And yet government spending has actually gone up. In October, the first month of the government's fiscal year, its total outlay was $689 billion, an increase of $105 billion (18 percent) from October 2024. Still, maybe DOGE wasn't a total disaster for its architects. It was able to gain access to sensitive and valuable government data, after all. — Kris Holt, Contributing reporterAI videoSora 2 app launch screen displayed on smartphoneIn our post-truth world, video was one of the few remaining ways to prove something had actually happened. It had its problems of course, but the fact it was harder to fake than words and images, and anyone could record a clip with their phone, made it vital to our sense of shared reality. Think about the murder of George Floyd: The grave injustice of his death would have probably never come to light if Darnella Frazier had not filmed what happened. With the advent of AI video, I'm not sure where we go. Both Google and OpenAI pushed the technology into the realm of uncomfortable realism this year, but it's Sora's cameo feature that has me worried. Within the first week of the app's public availability, people were using the feature, which allows users to add the likeness of other people to their videos, to generate clips of OpenAI CEO Sam Altman stealing GPUs from Target. Cameo has limitations, and users can restrict and delete videos that include their likeness, but it's just another assault on the truth. It's hard to see how making it trivial to create deepfake videos benefits anyone other than the companies offering building the tech. — I.B.This article originally appeared on Engadget at https://www.engadget.com/techs-biggest-losers-of-2025-140000419.html?src=rss",
          "feed_position": 23,
          "image_url": "https://media-mbst-pub-ue1.s3.amazonaws.com/creatr-uploaded-images/2025-11/1a5faf40-bbf7-11f0-bfcb-51aef0300038"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/big-tech/nvidia-can-now-sell-its-high-end-ai-chips-to-approved-customers-in-china-trump-says-130007458.html",
          "published_at": "Tue, 09 Dec 2025 13:00:07 +0000",
          "title": "NVIDIA can now sell its high-end AI chips to 'approved customers in China,' Trump says",
          "standfirst": "NVIDIA is now allowed to sell its second-best H200 processors to China, rather than just the sanction-approved H20 model that China had previously declined to buy, President Trump wrote on Truth Social. The United States will collect a 25 percent tariff on those sales, the Commerce Department confirmed yesterday. Trump said that he informed China's President Xi Jinping of the decision and that he \"responded positively.\" The Commerce Department is finalizing details and the administration will take the same approach with AMD, Intel and other US companies. He added that the administration would \"protect National Security,\" so the latest Blackwell and upcoming Rubin chips are not part of the deal. The 25 percent tariff would be higher than the 15 percent the White House suggested in August. Though the administration won't allow NVIDIA to send its latest high-end chips, it was reportedly concerned that the company would lose business to Huawei if it was completely shut out of China's market, according to Reuters. No details about the number of H200 chips or which companies would be eligible to buy them were released. \"Offering H200 to approved commercial customers, vetted by the Department of Commerce, strikes a thoughtful balance that is great for America,\" NVIDIA said in a statement. The decision is not without controversy, though. Several Democratic US senators called it a \"colossal economic and national security failure\" that will aid China's industry and military. Republican representative John Mollenaar put it in even starker terms. \"NVIDIA should be under no illusions — China will rip off its technology, mass-produce it themselves and seek to end NVIDIA as a competitor,\" he said. Despite the current restriction on Blackwell B200 processors, $1 billion worth of those and other high-end NVIDIA chips have made their way to China via black market sales, according to previous reports. That model, along with the H100 and H200, is far more capable than the H20 chip, which was designed to comply with export restrictions for sale to China. NVIDIA has said that the B200 chip is almost ten times faster than the H200 for some jobs, and the H200 is six times faster than the H20. Washington's approval doesn't mean that China will purchase NVIDIA's chips, as Beijing has previously told companies not to use US technology. Huawei is currently the most advanced company in that regard and recently unveiled a three-year plan to catch up with NVIDIA and AMD. However, AI chip experts like Richard Windsor have said NVIDIA's tech is still far ahead of anything that Huawei or other Chinese companies can currently produce. This article originally appeared on Engadget at https://www.engadget.com/big-tech/nvidia-can-now-sell-its-high-end-ai-chips-to-approved-customers-in-china-trump-says-130007458.html?src=rss",
          "content": "NVIDIA is now allowed to sell its second-best H200 processors to China, rather than just the sanction-approved H20 model that China had previously declined to buy, President Trump wrote on Truth Social. The United States will collect a 25 percent tariff on those sales, the Commerce Department confirmed yesterday. Trump said that he informed China's President Xi Jinping of the decision and that he \"responded positively.\" The Commerce Department is finalizing details and the administration will take the same approach with AMD, Intel and other US companies. He added that the administration would \"protect National Security,\" so the latest Blackwell and upcoming Rubin chips are not part of the deal. The 25 percent tariff would be higher than the 15 percent the White House suggested in August. Though the administration won't allow NVIDIA to send its latest high-end chips, it was reportedly concerned that the company would lose business to Huawei if it was completely shut out of China's market, according to Reuters. No details about the number of H200 chips or which companies would be eligible to buy them were released. \"Offering H200 to approved commercial customers, vetted by the Department of Commerce, strikes a thoughtful balance that is great for America,\" NVIDIA said in a statement. The decision is not without controversy, though. Several Democratic US senators called it a \"colossal economic and national security failure\" that will aid China's industry and military. Republican representative John Mollenaar put it in even starker terms. \"NVIDIA should be under no illusions — China will rip off its technology, mass-produce it themselves and seek to end NVIDIA as a competitor,\" he said. Despite the current restriction on Blackwell B200 processors, $1 billion worth of those and other high-end NVIDIA chips have made their way to China via black market sales, according to previous reports. That model, along with the H100 and H200, is far more capable than the H20 chip, which was designed to comply with export restrictions for sale to China. NVIDIA has said that the B200 chip is almost ten times faster than the H200 for some jobs, and the H200 is six times faster than the H20. Washington's approval doesn't mean that China will purchase NVIDIA's chips, as Beijing has previously told companies not to use US technology. Huawei is currently the most advanced company in that regard and recently unveiled a three-year plan to catch up with NVIDIA and AMD. However, AI chip experts like Richard Windsor have said NVIDIA's tech is still far ahead of anything that Huawei or other Chinese companies can currently produce. This article originally appeared on Engadget at https://www.engadget.com/big-tech/nvidia-can-now-sell-its-high-end-ai-chips-to-approved-customers-in-china-trump-says-130007458.html?src=rss",
          "feed_position": 26
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/big-tech/the-year-age-verification-laws-came-for-the-open-internet-130000979.html",
          "published_at": "Tue, 09 Dec 2025 13:00:00 +0000",
          "title": "The year age verification laws came for the open internet",
          "standfirst": "When the nonprofit Freedom House recently published its annual report, it noted that 2025 marked the 15th straight year of decline for global internet freedom. The biggest decline, after Georgia and Germany, came within the United States.Among the culprits cited in the report: age verification laws, dozens of which have come into effect over the last year. \"Online anonymity, an essential enabler for freedom of expression, is entering a period of crisis as policymakers in free and autocratic countries alike mandate the use of identity verification technology for certain websites or platforms, motivated in some cases by the legitimate aim of protecting children,\" the report warns.Age verification laws are, in some ways, part of a years-long reckoning over child safety online, as tech companies have shown themselves unable to prevent serious harms to their most vulnerable users. Lawmakers, who have failed to pass data privacy regulations, Section 230 reform or any other meaningful legislation that would thoughtfully reimagine what responsibilities tech companies owe their users, have instead turned to the blunt tool of age-based restrictions — and with much greater success. Over the last two years, 25 states have passed laws requiring some kind of age verification to access adult content online. This year, the Supreme Court delivered a major victory to backers of age verification standards when it upheld a Texas law requiring sites hosting adult content to check the ages of their users.Age checks have also expanded to social media and online platforms more broadly. Sixteen states now have laws requiring parental controls or other age-based restrictions for social media services. (Six of these measures are currently in limbo due to court challenges.) A federal bill to ban kids younger than 13 from social media has gained bipartisan support in Congress. Utah, Texas and Louisiana passed laws requiring app stores to check the ages of their users, all of which are set to go into effect next year. California plans to enact age-based rules for app stores in 2027.These laws have started to fragment the internet. Smaller platforms and websites that don't have the resources to pay for third-party verification services may have no choice but to exit markets where age checks are required. Blogging service Dreamwidth pulled out of Mississippi after its age verification laws went into effect, saying that the $10,000 per user fines it could face were an \"existential threat\" to the company. Bluesky also opted to go dark in Mississippi rather than comply. (The service has complied with age verification laws in South Dakota and Wyoming, as well as the UK.) Pornhub, which has called existing age verification laws \"haphazard and dangerous,\" has blocked access in 23 states. Pornhub is not an outlier in its assessment. Privacy advocates have long warned that age verification laws put everyone's privacy at risk. Practically, there's no way to limit age verification standards only to minors. Confirming the ages of everyone under 18 means you have to confirm the ages of everyone. In practice, this often means submitting a government-issued ID or allowing an app to scan your face. Both are problematic and we don't need to look far to see how these methods can go wrong. Discord recently revealed that around 70,000 users \"may\" have had their government IDs leaked due to an \"incident\" involving a third-party vendor the company contracts with to provide customer service related to age verification. Last year, another third-party identity provider that had worked with TikTok, Uber and other services exposed drivers' licenses. As a growing number of platforms require us to hand over an ID, these kinds of incidents will likely become even more common. Similar risks exist for face scans. Because most minors don't have official IDs, platforms often rely on AI-based tools that can guess users' ages. A face scan may seem more private than handing over a social security number, but we could be turning over far more information than we realize, according to experts at the Electronic Frontier Foundation (EFF).\"When we submit to a face scan to estimate our age, a less scrupulous company could flip a switch and use the same face scan, plus a slightly different algorithm, to guess our name or other demographics,\" the organization notes. \"A poorly designed system might store this personal data, and even correlate it to the online content that we look at. In the hands of an adversary, and cross-referenced to other readily available information, this information can expose intimate details about us.\"These issues aren't limited to the United States. Australia, Denmark and Malaysia have taken steps to ban younger teens from social media entirely. Officials in France are pushing for a similar ban, as well as a \"curfew\" for older teens. These measures would also necessitate some form of age verification in order to block the intended users. In the UK, where the Online Safety Act went into effect earlier this year, we've already seen how well-intentioned efforts to protect teens from supposedly harmful content can end up making large swaths of the internet more difficult to access. The law is ostensibly meant to \"prevent young people from encountering harmful content relating to suicide, self-harm, eating disorders and pornography,\" according to the BBC. But the law has also resulted in age checks that reach far beyond porn sites. Age verification is required, in some cases, to access music videos and other content on Spotify. It will soon be required for Xbox accounts. On X, videos of protests have been blocked. Redditors have reported being blocked from a lengthy number of subreddits that are marked NSFW but don't actually host porn, including those related to menstruation, news and addiction recovery. Wikipedia, which recently lost a challenge to be excluded from the law's strictest requirements, is facing the prospect of being forced to verify the ages of its UK contributors, which the organization has said could have disastrous consequences. The UK law has also shown how ineffective existing age verification methods are. Users have been able to circumvent the checks by using selfies of video game characters, AI-generated images of ID documents and, of course, Virtual Private Networks (VPNs). As the EFF notes, VPNs are incredibly widely used. The software allows people to browse the internet while masking their actual location. They're used by activists and students and people who want to get around geoblocks built into streaming services. Many universities and businesses (including Engadget parent company Yahoo) require their students and workers to use VPNs in order to access certain information. Blocking VPNs would have serious repercussions for all of these groups. The makers of several popular VPN services reported major spikes in the UK following the Online Safety Act going into effect this summer, with ProtonVPN reporting a 1,400 percent surge in sign-ups. That's also led to fears of a renewed crackdown on VPNs. Ofcom, the regulator tasked with enforcing the law, told TechRadar it was \"monitoring\" VPN usage, which has further fueled speculation it could try to ban or restrict their use. And here in the States, lawmakers in Wisconsin have proposed an age verification law that would require sites that host \"harmful\" content to also block VPNs.While restrictions on VPNs are, for now, mostly theoretical, the fact that such measures are even being considered is alarming. Up to now, VPN bans are more closely associated with authoritarian countries without an open internet, like Russia and China. If we continue down a path of trying to put age gates up around every piece of potentially objectionable content, the internet could get a lot worse for everyone. Correction, December 9, 2025, 11:23AM PT: A previous version of this story stated that Spotify requires age checks to access music in the UK. The service requires some users to complete age verification in order to access music videos tagged 18+ and messaging. We apologize for the error. This article originally appeared on Engadget at https://www.engadget.com/big-tech/the-year-age-verification-laws-came-for-the-open-internet-130000979.html?src=rss",
          "content": "When the nonprofit Freedom House recently published its annual report, it noted that 2025 marked the 15th straight year of decline for global internet freedom. The biggest decline, after Georgia and Germany, came within the United States.Among the culprits cited in the report: age verification laws, dozens of which have come into effect over the last year. \"Online anonymity, an essential enabler for freedom of expression, is entering a period of crisis as policymakers in free and autocratic countries alike mandate the use of identity verification technology for certain websites or platforms, motivated in some cases by the legitimate aim of protecting children,\" the report warns.Age verification laws are, in some ways, part of a years-long reckoning over child safety online, as tech companies have shown themselves unable to prevent serious harms to their most vulnerable users. Lawmakers, who have failed to pass data privacy regulations, Section 230 reform or any other meaningful legislation that would thoughtfully reimagine what responsibilities tech companies owe their users, have instead turned to the blunt tool of age-based restrictions — and with much greater success. Over the last two years, 25 states have passed laws requiring some kind of age verification to access adult content online. This year, the Supreme Court delivered a major victory to backers of age verification standards when it upheld a Texas law requiring sites hosting adult content to check the ages of their users.Age checks have also expanded to social media and online platforms more broadly. Sixteen states now have laws requiring parental controls or other age-based restrictions for social media services. (Six of these measures are currently in limbo due to court challenges.) A federal bill to ban kids younger than 13 from social media has gained bipartisan support in Congress. Utah, Texas and Louisiana passed laws requiring app stores to check the ages of their users, all of which are set to go into effect next year. California plans to enact age-based rules for app stores in 2027.These laws have started to fragment the internet. Smaller platforms and websites that don't have the resources to pay for third-party verification services may have no choice but to exit markets where age checks are required. Blogging service Dreamwidth pulled out of Mississippi after its age verification laws went into effect, saying that the $10,000 per user fines it could face were an \"existential threat\" to the company. Bluesky also opted to go dark in Mississippi rather than comply. (The service has complied with age verification laws in South Dakota and Wyoming, as well as the UK.) Pornhub, which has called existing age verification laws \"haphazard and dangerous,\" has blocked access in 23 states. Pornhub is not an outlier in its assessment. Privacy advocates have long warned that age verification laws put everyone's privacy at risk. Practically, there's no way to limit age verification standards only to minors. Confirming the ages of everyone under 18 means you have to confirm the ages of everyone. In practice, this often means submitting a government-issued ID or allowing an app to scan your face. Both are problematic and we don't need to look far to see how these methods can go wrong. Discord recently revealed that around 70,000 users \"may\" have had their government IDs leaked due to an \"incident\" involving a third-party vendor the company contracts with to provide customer service related to age verification. Last year, another third-party identity provider that had worked with TikTok, Uber and other services exposed drivers' licenses. As a growing number of platforms require us to hand over an ID, these kinds of incidents will likely become even more common. Similar risks exist for face scans. Because most minors don't have official IDs, platforms often rely on AI-based tools that can guess users' ages. A face scan may seem more private than handing over a social security number, but we could be turning over far more information than we realize, according to experts at the Electronic Frontier Foundation (EFF).\"When we submit to a face scan to estimate our age, a less scrupulous company could flip a switch and use the same face scan, plus a slightly different algorithm, to guess our name or other demographics,\" the organization notes. \"A poorly designed system might store this personal data, and even correlate it to the online content that we look at. In the hands of an adversary, and cross-referenced to other readily available information, this information can expose intimate details about us.\"These issues aren't limited to the United States. Australia, Denmark and Malaysia have taken steps to ban younger teens from social media entirely. Officials in France are pushing for a similar ban, as well as a \"curfew\" for older teens. These measures would also necessitate some form of age verification in order to block the intended users. In the UK, where the Online Safety Act went into effect earlier this year, we've already seen how well-intentioned efforts to protect teens from supposedly harmful content can end up making large swaths of the internet more difficult to access. The law is ostensibly meant to \"prevent young people from encountering harmful content relating to suicide, self-harm, eating disorders and pornography,\" according to the BBC. But the law has also resulted in age checks that reach far beyond porn sites. Age verification is required, in some cases, to access music videos and other content on Spotify. It will soon be required for Xbox accounts. On X, videos of protests have been blocked. Redditors have reported being blocked from a lengthy number of subreddits that are marked NSFW but don't actually host porn, including those related to menstruation, news and addiction recovery. Wikipedia, which recently lost a challenge to be excluded from the law's strictest requirements, is facing the prospect of being forced to verify the ages of its UK contributors, which the organization has said could have disastrous consequences. The UK law has also shown how ineffective existing age verification methods are. Users have been able to circumvent the checks by using selfies of video game characters, AI-generated images of ID documents and, of course, Virtual Private Networks (VPNs). As the EFF notes, VPNs are incredibly widely used. The software allows people to browse the internet while masking their actual location. They're used by activists and students and people who want to get around geoblocks built into streaming services. Many universities and businesses (including Engadget parent company Yahoo) require their students and workers to use VPNs in order to access certain information. Blocking VPNs would have serious repercussions for all of these groups. The makers of several popular VPN services reported major spikes in the UK following the Online Safety Act going into effect this summer, with ProtonVPN reporting a 1,400 percent surge in sign-ups. That's also led to fears of a renewed crackdown on VPNs. Ofcom, the regulator tasked with enforcing the law, told TechRadar it was \"monitoring\" VPN usage, which has further fueled speculation it could try to ban or restrict their use. And here in the States, lawmakers in Wisconsin have proposed an age verification law that would require sites that host \"harmful\" content to also block VPNs.While restrictions on VPNs are, for now, mostly theoretical, the fact that such measures are even being considered is alarming. Up to now, VPN bans are more closely associated with authoritarian countries without an open internet, like Russia and China. If we continue down a path of trying to put age gates up around every piece of potentially objectionable content, the internet could get a lot worse for everyone. Correction, December 9, 2025, 11:23AM PT: A previous version of this story stated that Spotify requires age checks to access music in the UK. The service requires some users to complete age verification in order to access music videos tagged 18+ and messaging. We apologize for the error. This article originally appeared on Engadget at https://www.engadget.com/big-tech/the-year-age-verification-laws-came-for-the-open-internet-130000979.html?src=rss",
          "feed_position": 28
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/general/the-morning-after-engadget-newsletter-122328464.html",
          "published_at": "Tue, 09 Dec 2025 12:23:28 +0000",
          "title": "The Morning After: Tech’s biggest winners of 2025",
          "standfirst": "As we wrap up 2025, we’re looking at the year’s biggest winners: the people, companies, products and trends that made the most impact over the year. Almost at the top of the pile, of course, are the tech billionaires. According to a recent report by Oxfam, the 10 richest US billionaires (who are all tech leaders, save for Warren Buffet) increased their wealth by $698 billion in 2025. Some of that has been spent treating and lavishing donations on President Trump. Elon Musk reportedly donated nearly $300 million to Trump and Republican allies, and several tech companies have pitched in to build the president’s White House ballroom. ALLISON ROBBERT via Getty Images Thanks to updates from Meta, Google, OpenAI and others, AI video is more realistic and easier to make than ever. AI video is everywhere. It’s not only overtaken your Facebook and Instagram recommendations, but Meta created an entirely separate feed just for users’ AI-generated fever dreams. The numbers are huge: OpenAI’s Sora, which lets you make AI videos of real people, was downloaded a million times in just a few days. And Google’s Veo generated more than 40 million videos in a few weeks of launching. AI slop is here to stay, and it’s everywhere. We didn’t say the winners would all be positive. But hey, the Switch 2 is great. — Mat Smith The other big stories this morning Google and Apple partner on better Android–iPhone switching Team Cherry is working on more Silksong content but won’t say when it’ll be released How Google is laying the foundation for our mixed reality future Paramount and Netflix both want to spend billions on Warner Bros. Discovery Good news for WBD? The Warner Bros. studios water tower. (Reuters / REUTERS) Paramount wasn’t going to let Netflix pick up Warner Bros. Discovery (WBD) without a fight. Following the streaming service’s $82.7 billion deal to buy much of WBD, Paramount is making a hostile takeover bid of $108 billion, pitching directly to WBD shareholders with an all-cash offer of $30 per share, which expires on January 8. Last week, the WBD board unanimously accepted Netflix’s offer of $27.75 per share. That breaks down to $23.25 per share in cash and another $4.50 per share in Netflix stock. Paramount, however, wants to pick up the entirety of WBD, while Netflix only wants the studios and streaming businesses. Whoever bought (or buys?) WBD would face government opposition from all sides. Paramount had already sent WBD a letter questioning the “fairness and adequacy” of the acquisition bidding process before its hostile takeover bid. President Trump warned the Netflix deal could be a “problem.” According to data from JustWatch, a combined Netflix and HBO would account for 33 percent of the US streaming video market. Continue reading. Tekken director Katsuhiro Harada is leaving Bandai Namco Tekken’s leading face and voice for decades. Katsuhiro Harada is departing Bandai Namco at the end of 2025. He announced the news both with a farewell note shared on X and, of course, an hour-long DJ mix. Harada’s 30-year career has been most closely involved with Tekken, and he’s a familiar face in the fighting game community. Harada wrote on X: “To everyone who has supported me, to communities around the world, and to all the colleagues who have walked alongside me for so many years, I offer my deepest gratitude.” Continue reading. This article originally appeared on Engadget at https://www.engadget.com/general/the-morning-after-engadget-newsletter-122328464.html?src=rss",
          "content": "As we wrap up 2025, we’re looking at the year’s biggest winners: the people, companies, products and trends that made the most impact over the year. Almost at the top of the pile, of course, are the tech billionaires. According to a recent report by Oxfam, the 10 richest US billionaires (who are all tech leaders, save for Warren Buffet) increased their wealth by $698 billion in 2025. Some of that has been spent treating and lavishing donations on President Trump. Elon Musk reportedly donated nearly $300 million to Trump and Republican allies, and several tech companies have pitched in to build the president’s White House ballroom. ALLISON ROBBERT via Getty Images Thanks to updates from Meta, Google, OpenAI and others, AI video is more realistic and easier to make than ever. AI video is everywhere. It’s not only overtaken your Facebook and Instagram recommendations, but Meta created an entirely separate feed just for users’ AI-generated fever dreams. The numbers are huge: OpenAI’s Sora, which lets you make AI videos of real people, was downloaded a million times in just a few days. And Google’s Veo generated more than 40 million videos in a few weeks of launching. AI slop is here to stay, and it’s everywhere. We didn’t say the winners would all be positive. But hey, the Switch 2 is great. — Mat Smith The other big stories this morning Google and Apple partner on better Android–iPhone switching Team Cherry is working on more Silksong content but won’t say when it’ll be released How Google is laying the foundation for our mixed reality future Paramount and Netflix both want to spend billions on Warner Bros. Discovery Good news for WBD? The Warner Bros. studios water tower. (Reuters / REUTERS) Paramount wasn’t going to let Netflix pick up Warner Bros. Discovery (WBD) without a fight. Following the streaming service’s $82.7 billion deal to buy much of WBD, Paramount is making a hostile takeover bid of $108 billion, pitching directly to WBD shareholders with an all-cash offer of $30 per share, which expires on January 8. Last week, the WBD board unanimously accepted Netflix’s offer of $27.75 per share. That breaks down to $23.25 per share in cash and another $4.50 per share in Netflix stock. Paramount, however, wants to pick up the entirety of WBD, while Netflix only wants the studios and streaming businesses. Whoever bought (or buys?) WBD would face government opposition from all sides. Paramount had already sent WBD a letter questioning the “fairness and adequacy” of the acquisition bidding process before its hostile takeover bid. President Trump warned the Netflix deal could be a “problem.” According to data from JustWatch, a combined Netflix and HBO would account for 33 percent of the US streaming video market. Continue reading. Tekken director Katsuhiro Harada is leaving Bandai Namco Tekken’s leading face and voice for decades. Katsuhiro Harada is departing Bandai Namco at the end of 2025. He announced the news both with a farewell note shared on X and, of course, an hour-long DJ mix. Harada’s 30-year career has been most closely involved with Tekken, and he’s a familiar face in the fighting game community. Harada wrote on X: “To everyone who has supported me, to communities around the world, and to all the colleagues who have walked alongside me for so many years, I offer my deepest gratitude.” Continue reading. This article originally appeared on Engadget at https://www.engadget.com/general/the-morning-after-engadget-newsletter-122328464.html?src=rss",
          "feed_position": 29,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2025-12/58982c20-d4f1-11f0-b8df-45aad7e99cc0"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/apps/best-budgeting-apps-120036303.html",
          "published_at": "Tue, 09 Dec 2025 10:00:36 +0000",
          "title": "The best budgeting apps for 2025",
          "standfirst": "Over a year ago, I was prompted to look for another budgeting app. Intuit, parent company of Mint, the budgeting app I had been using for a long time, shut down the service in March 2024. The company encouraged Mint users to migrate to its other financial app, Credit Karma, but I found it to be a poor Mint replacement after trying it out. That sent me searching elsewhere to find an app to track all of my financial accounts, monitor my credit score, track spending and set goals like building a rainy-day fund and paying down my mortgage faster.If you’re looking for a new budgeting app to get your finances straight, allow Engadget to help. I tried out Mint's top competitors in the hopes that I'd be able to find a new budgeting app that could handle all of my financial needs, and to see which are actually worth the money. Best budget apps of 2025 Other budgeting apps we tested PocketGuard PocketGuard used to be a solid free budget tracker, but the company has since limited its “free” version to just a free seven-day trial. Now, you’ll have to choose between two plans once the trial is over: a $13 monthly plan or a $75 annual plan. When I first tested it, I found it to be more restricted than NerdWallet, but still a decent option. The main overview screen shows you your net worth, total assets and debts; net income and total spending for the month; upcoming bills; a handy reminder of when your next paycheck lands; any debt payoff plan you have; and any goals. Like some other apps, including Quicken Simplifi, PocketGuard promotes an “after bills” approach, where you enter all of your recurring bills, and then PocketGuard shows you what’s left, and that’s what you’re supposed to be budgeting: your disposable income. Although PocketGuard’s UI is easy enough to understand, it lacks polish. The “accounts” tab is a little busy, and doesn’t show totals for categories like cash or investments. Seemingly small details like weirdly phrased or punctuated copy occasionally make the app feel janky. More than once, it prompted me to update the app when no updates were available. The web version, meanwhile, feels like the mobile app blown up to a larger format and doesn’t take advantage of the extra screen real estate. Ultimately, now that the free tier is gone, it just doesn’t present the same value proposition as it once did. How we test budgeting apps Before I dove in and started testing out budgeting apps, I had to do some research. To find a list of apps to try out, I consulted trusty ol’ Google (and even trustier Reddit); read reviews of popular apps on the App Store; and also asked friends and colleagues what budget tracking apps (or other budgeting methods) they might be using for money management. Some of the apps I found were free and these, of course, show loads of ads (excuse me, “offers”) to stay in business. But most of the available apps require paid subscriptions, with prices typically topping out around $100 a year, or $15 a month. (Spoiler: My top pick is cheaper than that.) All of the services I chose to test needed to do several things: import all of your account data into one place; offer budgeting tools; and track your spending, net worth and credit score. Except where noted, all of these apps are available for iOS, Android and on the web. Once I had my shortlist of six apps, I got to work setting them up. For the sake of thoroughly testing these apps, I made a point of adding every account to every budgeting app, no matter how small or immaterial the balance. What ensued was a veritable Groundhog Day of two-factor authentication. Just hours of entering passwords and one-time passcodes, for the same banks half a dozen times over. Hopefully, you only have to do this once. Budgeting app FAQs What is Plaid and how does it work? Each of the apps I tested uses the same underlying network, called Plaid, to pull in financial data, so it’s worth explaining what it is and how it works. Plaid was founded as a fintech startup in 2013 and is today the industry standard in connecting banks with third-party apps. Plaid works with over 12,000 financial institutions across the US, Canada and Europe. Additionally, more than 8,000 third-party apps and services rely on Plaid, the company claims. To be clear, you don’t need a dedicated Plaid app to use it; the technology is baked into a wide array of apps, including all of the budgeting apps listed in this guide. Once you find the “add an account” option in whichever one you’re using, you’ll see a menu of commonly used banks. There’s also a search field you can use to look yours up directly. Once you find yours, you’ll be prompted to enter your login credentials. If you have two-factor authentication set up, you’ll need to enter a one-time passcode as well. As the middleman, Plaid is a passthrough for information that may include your account balances, transaction history, account type and routing or account number. Plaid uses encryption, and says it has a policy of not selling or renting customer data to other companies. However, I would not be doing my job if I didn’t note that in 2022 Plaid was forced to pay $58 million to consumers in a class action suit for collecting “more financial data than was needed.” As part of the settlement, Plaid was compelled to change some of its business practices. In a statement provided to Engadget, a Plaid spokesperson said the company continues to deny the allegations underpinning the lawsuit and that “the crux of the non-financial terms in the settlement are focused on us accelerating workstreams already underway related to giving people more transparency into Plaid’s role in connecting their accounts, and ensuring that our workstreams around data minimization remain on track.” Why did Mint shut down? When parent company Intuit announced in December 2023 that it would shut down Mint, it did not provide a reason why it made the decision to do so. It did say that Mint's millions of users would be funneled over to its other finance app, Credit Karma. \"Credit Karma is thrilled to invite all Minters to continue their financial journey on Credit Karma, where they will have access to Credit Karma’s suite of features, products, tools and services, including some of Mint’s most popular features,\" Mint wrote on its product blog. In our testing, we found that Credit Karma isn't an exact replacement for Mint — so if you're still looking for a Mint alternative, you have some decent options. What about Rocket Money? Rocket Money is another free financial app that tracks spending and supports things like balance alerts and account linking. If you pay for the premium tier, the service can also help you cancel unwanted subscriptions. We did not test it for this guide, but we'll consider it in future updates.This article originally appeared on Engadget at https://www.engadget.com/apps/best-budgeting-apps-120036303.html?src=rss",
          "content": "Over a year ago, I was prompted to look for another budgeting app. Intuit, parent company of Mint, the budgeting app I had been using for a long time, shut down the service in March 2024. The company encouraged Mint users to migrate to its other financial app, Credit Karma, but I found it to be a poor Mint replacement after trying it out. That sent me searching elsewhere to find an app to track all of my financial accounts, monitor my credit score, track spending and set goals like building a rainy-day fund and paying down my mortgage faster.If you’re looking for a new budgeting app to get your finances straight, allow Engadget to help. I tried out Mint's top competitors in the hopes that I'd be able to find a new budgeting app that could handle all of my financial needs, and to see which are actually worth the money. Best budget apps of 2025 Other budgeting apps we tested PocketGuard PocketGuard used to be a solid free budget tracker, but the company has since limited its “free” version to just a free seven-day trial. Now, you’ll have to choose between two plans once the trial is over: a $13 monthly plan or a $75 annual plan. When I first tested it, I found it to be more restricted than NerdWallet, but still a decent option. The main overview screen shows you your net worth, total assets and debts; net income and total spending for the month; upcoming bills; a handy reminder of when your next paycheck lands; any debt payoff plan you have; and any goals. Like some other apps, including Quicken Simplifi, PocketGuard promotes an “after bills” approach, where you enter all of your recurring bills, and then PocketGuard shows you what’s left, and that’s what you’re supposed to be budgeting: your disposable income. Although PocketGuard’s UI is easy enough to understand, it lacks polish. The “accounts” tab is a little busy, and doesn’t show totals for categories like cash or investments. Seemingly small details like weirdly phrased or punctuated copy occasionally make the app feel janky. More than once, it prompted me to update the app when no updates were available. The web version, meanwhile, feels like the mobile app blown up to a larger format and doesn’t take advantage of the extra screen real estate. Ultimately, now that the free tier is gone, it just doesn’t present the same value proposition as it once did. How we test budgeting apps Before I dove in and started testing out budgeting apps, I had to do some research. To find a list of apps to try out, I consulted trusty ol’ Google (and even trustier Reddit); read reviews of popular apps on the App Store; and also asked friends and colleagues what budget tracking apps (or other budgeting methods) they might be using for money management. Some of the apps I found were free and these, of course, show loads of ads (excuse me, “offers”) to stay in business. But most of the available apps require paid subscriptions, with prices typically topping out around $100 a year, or $15 a month. (Spoiler: My top pick is cheaper than that.) All of the services I chose to test needed to do several things: import all of your account data into one place; offer budgeting tools; and track your spending, net worth and credit score. Except where noted, all of these apps are available for iOS, Android and on the web. Once I had my shortlist of six apps, I got to work setting them up. For the sake of thoroughly testing these apps, I made a point of adding every account to every budgeting app, no matter how small or immaterial the balance. What ensued was a veritable Groundhog Day of two-factor authentication. Just hours of entering passwords and one-time passcodes, for the same banks half a dozen times over. Hopefully, you only have to do this once. Budgeting app FAQs What is Plaid and how does it work? Each of the apps I tested uses the same underlying network, called Plaid, to pull in financial data, so it’s worth explaining what it is and how it works. Plaid was founded as a fintech startup in 2013 and is today the industry standard in connecting banks with third-party apps. Plaid works with over 12,000 financial institutions across the US, Canada and Europe. Additionally, more than 8,000 third-party apps and services rely on Plaid, the company claims. To be clear, you don’t need a dedicated Plaid app to use it; the technology is baked into a wide array of apps, including all of the budgeting apps listed in this guide. Once you find the “add an account” option in whichever one you’re using, you’ll see a menu of commonly used banks. There’s also a search field you can use to look yours up directly. Once you find yours, you’ll be prompted to enter your login credentials. If you have two-factor authentication set up, you’ll need to enter a one-time passcode as well. As the middleman, Plaid is a passthrough for information that may include your account balances, transaction history, account type and routing or account number. Plaid uses encryption, and says it has a policy of not selling or renting customer data to other companies. However, I would not be doing my job if I didn’t note that in 2022 Plaid was forced to pay $58 million to consumers in a class action suit for collecting “more financial data than was needed.” As part of the settlement, Plaid was compelled to change some of its business practices. In a statement provided to Engadget, a Plaid spokesperson said the company continues to deny the allegations underpinning the lawsuit and that “the crux of the non-financial terms in the settlement are focused on us accelerating workstreams already underway related to giving people more transparency into Plaid’s role in connecting their accounts, and ensuring that our workstreams around data minimization remain on track.” Why did Mint shut down? When parent company Intuit announced in December 2023 that it would shut down Mint, it did not provide a reason why it made the decision to do so. It did say that Mint's millions of users would be funneled over to its other finance app, Credit Karma. \"Credit Karma is thrilled to invite all Minters to continue their financial journey on Credit Karma, where they will have access to Credit Karma’s suite of features, products, tools and services, including some of Mint’s most popular features,\" Mint wrote on its product blog. In our testing, we found that Credit Karma isn't an exact replacement for Mint — so if you're still looking for a Mint alternative, you have some decent options. What about Rocket Money? Rocket Money is another free financial app that tracks spending and supports things like balance alerts and account linking. If you pay for the premium tier, the service can also help you cancel unwanted subscriptions. We did not test it for this guide, but we'll consider it in future updates.This article originally appeared on Engadget at https://www.engadget.com/apps/best-budgeting-apps-120036303.html?src=rss",
          "feed_position": 30
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/brand-context-ai-the-missing-requirement-for-marketing-ai",
          "published_at": "Tue, 09 Dec 2025 08:00:00 GMT",
          "title": "Brand-context AI: The missing requirement for marketing AI",
          "standfirst": "Presented by BlueOceanAI has become a central part of how marketing teams work, but the results often fall short. Models can generate content at scale and summarize information in seconds, yet the outputs are not always aligned with the brand, the audience, or the company’s strategic goals. The problem is not capability. The problem is the absence of context.The bottleneck is no longer computational power. It is contextual intelligence.Generative AI is powerful, but it doesn’t understand the nuances of the business it supports. It doesn’t have the context for why customers choose one brand over another or what creates competitive advantage. Without that grounding, AI operates as a fast executor rather than a strategic partner. It produces more, but it does not always help teams make better decisions.This becomes even more visible inside complex marketing organizations where insights live in different corners of the business and rarely come together in a unified way.As Grant McDougall, CEO of BlueOcean, explains, “Inside large marketing organizations, the data is vertical. Digital has theirs, loyalty has theirs, content has theirs, media has theirs. But CMOs think horizontally. They need to combine customer insight, competitive movement, creative performance, and sales signals into one coherent view. Connecting that data fundamentally changes how decisions get made.”This shift from vertical data to horizontal intelligence reflects a new phase in AI adoption. The emphasis is shifting from output volume to decision quality. Marketers are recognizing that the future of AI is intelligence that understands who you are as a company and why you matter to your customers.In BlueOcean’s work with global brands across technology, healthcare, and consumer industries, including Amazon, Cisco, SAP, and Intel, the same pattern appears. Teams move faster and make better decisions when AI is grounded in structured brand and competitive context.Why context is becoming the critical ingredientLarge language models excel at producing language. They do not inherently understand brand, meaning, or intention. This is why generic prompts often lead to generic outputs. The model executes based on statistical prediction, not strategic nuance.Context changes that. When AI systems are supplied with structured inputs about brand strategy, audience insight, and creative intent, the output becomes sharper and more reliable. Recommendations become more specific. Creative stays on brief. The AI begins to act less like a content generator and more like a partner that understands the boundaries and goals of the business.This shift mirrors a key theme from BlueOcean’s recent report, Building Marketing Intelligence: The CMO Blueprint for Context-Aware AI. The report explains that AI is most effective when it is grounded in a clear frame of reference. CMOs who design these context-aware workflows see better performance, stronger creative, and more reliable decision-making.For a deeper exploration of these principles, the full report is available here.The industry’s pivot: From execution to understandingMany teams remain in an experimentation phase with AI. They test tools, run pilots, and explore new workflows. This creates productivity gains but not intelligence. Without shared context, every team uses AI differently, and the result is fragmentation.The companies making the clearest progress treat context as a shared layer across workflows. When teams pull from the same brand strategy, insights, and creative guidance, AI becomes more predictable and more valuable. It supports decisions rather than contradicting them. This becomes especially effective when the context includes external signals such as shifts in sentiment, competitor movement, content performance, and broader category trends.Brand-context AI connects brand identity, customer sentiment, competitive movement, and creative performance in a single environment. It strengthens workflows in practical ways: briefs become more strategic, content reviews more accurate, and insights faster because the system synthesizes patterns teams once assembled manually.Across enterprise teams supported by BlueOcean, this shift consistently unlocks clarity. AI becomes a contributor to strategic understanding rather than a generator of disconnected output. With shared context in place, teams make more confident, coherent, and aligned decisions.Structured context: What it actually includesStructured context is the intelligence marketers already curate to understand how their brand shows up in the world. It brings together the narrative elements that shape the brand’s voice, the customer motivations that influence messaging, the competitive signals unfolding in the market, and the creative patterns that have historically performed. It also includes the external brand signals teams monitor every day: sentiment shifts, content dynamics, press and social movement, and how competitors position themselves across channels.When this information is organized into a coherent frame, AI can interpret direction and creative choices with the same clarity strategists use. The value does not come from giving AI more data; it comes from giving it structure so it can reason through decisions the way marketers already do.The new division of labor between humans and AIThe strongest AI-enabled marketing teams have one thing in common. They are clear about what humans own and what AI owns. Humans define purpose, strategy, and creative judgment. They understand emotion, cultural nuance, competitive meaning, and brand intent.AI delivers speed, scale, and precision. It excels at synthesizing information, producing iterations, and following structured instruction.“AI works best when it is given clear boundaries and clear intent,” says McDougall. “Humans set the direction led by creativity and imagination. AI executes with precision. That partnership is where the real value emerges.”The systems that perform best are the ones guided by human-defined boundaries and human-led strategy. AI provides scale, but people provide meaning.CMOs are recognizing that governing context is becoming a leadership responsibility. They already own brand, messaging, and customer insight. Extending this ownership into AI systems ensures the brand shows up consistently across every touchpoint, whether a human or a model produced the work.A practical example of context in actionConsider a team preparing a global campaign. Without context, an AI system might generate copy that sounds polished but generic. It may overlook claims the brand can make, reference benefits competitors own, or ignore differentiators that matter most. It may even amplify a competitor’s message simply because that language appears frequently in public data.With structured context, the experience changes. The model understands the audience, the brand tone, the competitive landscape, and the objective. It knows which competitors are gaining attention, which messages resonate in the market, and where the brand has permission to play. It can propose angles that strengthen positioning rather than dilute it. It can generate variations that stay on brief and avoid competitor-owned territory.BlueOcean has observed this shift inside enterprise teams including Amazon, Intel, and SAP, where structured brand and competitive context has improved alignment and reduced drift at scale.Creative, brand, and competitive signals are no longer separate inputs. When they are connected and contextualized, AI begins supporting decision-making in a meaningful way. The technology stops producing output for its own sake and starts helping marketers understand where the brand stands and what actions will grow it.What comes nextA new phase of AI is beginning. AI agents are evolving from task assistants to systems that collaborate across tools and workflows. As these systems become more capable, context will determine whether they behave unpredictably or perform as trusted extensions of the team.Brand-context AI provides a path forward. It gives AI systems the structure they need to operate consistently. It supports the teams responsible for protecting brand integrity. In practice, these agents can already assemble context-aware creative briefs, review content for competitive and brand alignment, monitor shifts in category messaging, and synthesize insights across products or markets. It creates intelligence that adapts rather than overwhelms.In the coming years, success will not come from producing more content, but from producing content anchored in brand context, the kind that sharpens decisions, strengthens positioning, and drives long-term growth.The companies that build on context today will define the generative enterprise of tomorrow. BlueOcean is helping leading enterprises shape the next generation of context-aware AI systems.Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.",
          "content": "Presented by BlueOceanAI has become a central part of how marketing teams work, but the results often fall short. Models can generate content at scale and summarize information in seconds, yet the outputs are not always aligned with the brand, the audience, or the company’s strategic goals. The problem is not capability. The problem is the absence of context.The bottleneck is no longer computational power. It is contextual intelligence.Generative AI is powerful, but it doesn’t understand the nuances of the business it supports. It doesn’t have the context for why customers choose one brand over another or what creates competitive advantage. Without that grounding, AI operates as a fast executor rather than a strategic partner. It produces more, but it does not always help teams make better decisions.This becomes even more visible inside complex marketing organizations where insights live in different corners of the business and rarely come together in a unified way.As Grant McDougall, CEO of BlueOcean, explains, “Inside large marketing organizations, the data is vertical. Digital has theirs, loyalty has theirs, content has theirs, media has theirs. But CMOs think horizontally. They need to combine customer insight, competitive movement, creative performance, and sales signals into one coherent view. Connecting that data fundamentally changes how decisions get made.”This shift from vertical data to horizontal intelligence reflects a new phase in AI adoption. The emphasis is shifting from output volume to decision quality. Marketers are recognizing that the future of AI is intelligence that understands who you are as a company and why you matter to your customers.In BlueOcean’s work with global brands across technology, healthcare, and consumer industries, including Amazon, Cisco, SAP, and Intel, the same pattern appears. Teams move faster and make better decisions when AI is grounded in structured brand and competitive context.Why context is becoming the critical ingredientLarge language models excel at producing language. They do not inherently understand brand, meaning, or intention. This is why generic prompts often lead to generic outputs. The model executes based on statistical prediction, not strategic nuance.Context changes that. When AI systems are supplied with structured inputs about brand strategy, audience insight, and creative intent, the output becomes sharper and more reliable. Recommendations become more specific. Creative stays on brief. The AI begins to act less like a content generator and more like a partner that understands the boundaries and goals of the business.This shift mirrors a key theme from BlueOcean’s recent report, Building Marketing Intelligence: The CMO Blueprint for Context-Aware AI. The report explains that AI is most effective when it is grounded in a clear frame of reference. CMOs who design these context-aware workflows see better performance, stronger creative, and more reliable decision-making.For a deeper exploration of these principles, the full report is available here.The industry’s pivot: From execution to understandingMany teams remain in an experimentation phase with AI. They test tools, run pilots, and explore new workflows. This creates productivity gains but not intelligence. Without shared context, every team uses AI differently, and the result is fragmentation.The companies making the clearest progress treat context as a shared layer across workflows. When teams pull from the same brand strategy, insights, and creative guidance, AI becomes more predictable and more valuable. It supports decisions rather than contradicting them. This becomes especially effective when the context includes external signals such as shifts in sentiment, competitor movement, content performance, and broader category trends.Brand-context AI connects brand identity, customer sentiment, competitive movement, and creative performance in a single environment. It strengthens workflows in practical ways: briefs become more strategic, content reviews more accurate, and insights faster because the system synthesizes patterns teams once assembled manually.Across enterprise teams supported by BlueOcean, this shift consistently unlocks clarity. AI becomes a contributor to strategic understanding rather than a generator of disconnected output. With shared context in place, teams make more confident, coherent, and aligned decisions.Structured context: What it actually includesStructured context is the intelligence marketers already curate to understand how their brand shows up in the world. It brings together the narrative elements that shape the brand’s voice, the customer motivations that influence messaging, the competitive signals unfolding in the market, and the creative patterns that have historically performed. It also includes the external brand signals teams monitor every day: sentiment shifts, content dynamics, press and social movement, and how competitors position themselves across channels.When this information is organized into a coherent frame, AI can interpret direction and creative choices with the same clarity strategists use. The value does not come from giving AI more data; it comes from giving it structure so it can reason through decisions the way marketers already do.The new division of labor between humans and AIThe strongest AI-enabled marketing teams have one thing in common. They are clear about what humans own and what AI owns. Humans define purpose, strategy, and creative judgment. They understand emotion, cultural nuance, competitive meaning, and brand intent.AI delivers speed, scale, and precision. It excels at synthesizing information, producing iterations, and following structured instruction.“AI works best when it is given clear boundaries and clear intent,” says McDougall. “Humans set the direction led by creativity and imagination. AI executes with precision. That partnership is where the real value emerges.”The systems that perform best are the ones guided by human-defined boundaries and human-led strategy. AI provides scale, but people provide meaning.CMOs are recognizing that governing context is becoming a leadership responsibility. They already own brand, messaging, and customer insight. Extending this ownership into AI systems ensures the brand shows up consistently across every touchpoint, whether a human or a model produced the work.A practical example of context in actionConsider a team preparing a global campaign. Without context, an AI system might generate copy that sounds polished but generic. It may overlook claims the brand can make, reference benefits competitors own, or ignore differentiators that matter most. It may even amplify a competitor’s message simply because that language appears frequently in public data.With structured context, the experience changes. The model understands the audience, the brand tone, the competitive landscape, and the objective. It knows which competitors are gaining attention, which messages resonate in the market, and where the brand has permission to play. It can propose angles that strengthen positioning rather than dilute it. It can generate variations that stay on brief and avoid competitor-owned territory.BlueOcean has observed this shift inside enterprise teams including Amazon, Intel, and SAP, where structured brand and competitive context has improved alignment and reduced drift at scale.Creative, brand, and competitive signals are no longer separate inputs. When they are connected and contextualized, AI begins supporting decision-making in a meaningful way. The technology stops producing output for its own sake and starts helping marketers understand where the brand stands and what actions will grow it.What comes nextA new phase of AI is beginning. AI agents are evolving from task assistants to systems that collaborate across tools and workflows. As these systems become more capable, context will determine whether they behave unpredictably or perform as trusted extensions of the team.Brand-context AI provides a path forward. It gives AI systems the structure they need to operate consistently. It supports the teams responsible for protecting brand integrity. In practice, these agents can already assemble context-aware creative briefs, review content for competitive and brand alignment, monitor shifts in category messaging, and synthesize insights across products or markets. It creates intelligence that adapts rather than overwhelms.In the coming years, success will not come from producing more content, but from producing content anchored in brand context, the kind that sharpens decisions, strengthens positioning, and drives long-term growth.The companies that build on context today will define the generative enterprise of tomorrow. BlueOcean is helping leading enterprises shape the next generation of context-aware AI systems.Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.",
          "feed_position": 3,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/14MrK6yiPQLN3SznWaBOl4/9e3ba1a69b409e0c085c7de1dcea47e2/AdobeStock_438714181.jpeg?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/audio/fairphone-updates-its-over-ear-headphones-with-better-sound-080000773.html",
          "published_at": "Tue, 09 Dec 2025 08:00:00 +0000",
          "title": "Fairphone updates its over ear headphones with better sound",
          "standfirst": "Two years ago, Fairphone launched a pair of modular, fully-repairable headphones called the Fairbuds XL. Now, the Dutch social enterprise is releasing an updated version where the focus isn’t just on the quality of its hardware, but on the merits of its raw materials. The 2025 Fairbuds XL ship with new “premium” 40mm dynamic drivers and stronger N52 rated magnets for “more power and enhanced bass response.” Listeners should expect to hear “improved accuracy in mid and high frequencies” thanks to improved audio tuning. And there’s a new paper-based speaker membrane that’s more refined than what went before.Sound-quality is certainly improved over the predecessor, with cleaner bass and sharper mid and high tones. You can really hear how clean the sound is, especially when you’re listening to something lush and orchestral. Going by my memory from two years ago, the ANC seems to be unchanged, able to deal with unwanted sounds like the tapping of my own keyboard with ease. Obviously, you shouldn’t go in expecting the same sort of brilliant sound you’d get from top tier manufacturers. Although I think Fairphone has likely closed the gap so while you’re still a generation or two behind the bleeding edge, it’s not by a shameful degree. And, if we’re honest, if you’re looking for a pair of cans that’ll last you years and years, you’re likely able to forgive the absence of the shiniest bells and whistles. Closeup image of the Fairbuds XL (2025) Daniel Cooper for EngadgetIt’s easy to notice the design changes, too, with the more austere Fairphone logo and the use of solid plastic in place of the original speckled pattern. Sadly, that extends to the little pop of copper found on the four way joystick, which is now a different shade of the same colorway. I get it: A lot of people want their consumer electronics to blend in, but the loss of whimsy pains me a little. There’s some fairly minor but welcome quality of life improvements, including adding automatic power off after 30 minutes. Plus, buyers get the benefit of a three year warranty, and can rest assured the XL are Longtime certified. Longtime is a new European labeling standard for gear built to be long-lived, repairable and supported by a robust repair infrastructure. And, if you already own a pair of XLs, you can buy the new drivers and slot them into your existing cans.Side showing logo.Daniel Cooper for EngadgetBut, in line with Fairphone’s founding principles, the update is equally focused on the raw materials that have gone into making the 2025 XLs. The PU “leather” found in the ear cushions — hardly a climate friendly material — has been ditched in favor of Bird’s Eye Fabric. That’s a cotton fabric produced with a series of air holes for breathability more commonly found in athleisure products. Similarly the PU found in the headband gets replaced with fabric that reminds me of plenty of other high end sport headphones.Beneath the surface, the new model contains 100 percent fair mined cobalt, copper and silver — via mining credits, at least. The rare earth metals used in the speaker magnets are now 100 percent recycled, and they were assembled in a facility using 100 percent renewable energy. You’ll also find more than 90 percent recycled aluminum, and 80 percent recycled plastics, with the company pledging to recycle an equal amount of e-waste to every pair sold. Plus, as usual, Fairphone will pay the people who assemble the hardware a living wage bonus to ensure “workers can cover their family’s needs.” Fairbuds XL are available to order today in Europe both from the Fairphone website and select third party retailers. If you’re in the US, you’ll need to wait until “later” this month, but you’ll be able to pick them up via Amazon for $229. This article originally appeared on Engadget at https://www.engadget.com/audio/fairphone-updates-its-over-ear-headphones-with-better-sound-080000773.html?src=rss",
          "content": "Two years ago, Fairphone launched a pair of modular, fully-repairable headphones called the Fairbuds XL. Now, the Dutch social enterprise is releasing an updated version where the focus isn’t just on the quality of its hardware, but on the merits of its raw materials. The 2025 Fairbuds XL ship with new “premium” 40mm dynamic drivers and stronger N52 rated magnets for “more power and enhanced bass response.” Listeners should expect to hear “improved accuracy in mid and high frequencies” thanks to improved audio tuning. And there’s a new paper-based speaker membrane that’s more refined than what went before.Sound-quality is certainly improved over the predecessor, with cleaner bass and sharper mid and high tones. You can really hear how clean the sound is, especially when you’re listening to something lush and orchestral. Going by my memory from two years ago, the ANC seems to be unchanged, able to deal with unwanted sounds like the tapping of my own keyboard with ease. Obviously, you shouldn’t go in expecting the same sort of brilliant sound you’d get from top tier manufacturers. Although I think Fairphone has likely closed the gap so while you’re still a generation or two behind the bleeding edge, it’s not by a shameful degree. And, if we’re honest, if you’re looking for a pair of cans that’ll last you years and years, you’re likely able to forgive the absence of the shiniest bells and whistles. Closeup image of the Fairbuds XL (2025) Daniel Cooper for EngadgetIt’s easy to notice the design changes, too, with the more austere Fairphone logo and the use of solid plastic in place of the original speckled pattern. Sadly, that extends to the little pop of copper found on the four way joystick, which is now a different shade of the same colorway. I get it: A lot of people want their consumer electronics to blend in, but the loss of whimsy pains me a little. There’s some fairly minor but welcome quality of life improvements, including adding automatic power off after 30 minutes. Plus, buyers get the benefit of a three year warranty, and can rest assured the XL are Longtime certified. Longtime is a new European labeling standard for gear built to be long-lived, repairable and supported by a robust repair infrastructure. And, if you already own a pair of XLs, you can buy the new drivers and slot them into your existing cans.Side showing logo.Daniel Cooper for EngadgetBut, in line with Fairphone’s founding principles, the update is equally focused on the raw materials that have gone into making the 2025 XLs. The PU “leather” found in the ear cushions — hardly a climate friendly material — has been ditched in favor of Bird’s Eye Fabric. That’s a cotton fabric produced with a series of air holes for breathability more commonly found in athleisure products. Similarly the PU found in the headband gets replaced with fabric that reminds me of plenty of other high end sport headphones.Beneath the surface, the new model contains 100 percent fair mined cobalt, copper and silver — via mining credits, at least. The rare earth metals used in the speaker magnets are now 100 percent recycled, and they were assembled in a facility using 100 percent renewable energy. You’ll also find more than 90 percent recycled aluminum, and 80 percent recycled plastics, with the company pledging to recycle an equal amount of e-waste to every pair sold. Plus, as usual, Fairphone will pay the people who assemble the hardware a living wage bonus to ensure “workers can cover their family’s needs.” Fairbuds XL are available to order today in Europe both from the Fairphone website and select third party retailers. If you’re in the US, you’ll need to wait until “later” this month, but you’ll be able to pick them up via Amazon for $229. This article originally appeared on Engadget at https://www.engadget.com/audio/fairphone-updates-its-over-ear-headphones-with-better-sound-080000773.html?src=rss",
          "feed_position": 31,
          "image_url": "https://d29szjachogqwa.cloudfront.net/images/user-uploaded/Closeup.jpg"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/tracking-every-decision-dollar-and-delay-the-new-process-intelligence-engine",
          "published_at": "Tue, 09 Dec 2025 05:00:00 GMT",
          "title": "Tracking every decision, dollar and delay: The new process intelligence engine driving public-sector progress",
          "standfirst": "Presented by CelonisThe State of Oklahoma discovered its blind spots the hard way. In April 2023, a legislative report revealed its agencies had spent $3 billion without proper oversight. Janet Morrow, Director of Oklahoma&#x27;s Risk, Assessment and Compliance Division, set out to track thousands of monthly transactions across dozens of disconnected systems.The Sooner State became the first U.S. state to apply process intelligence (PI) technology for procurement oversight. The transformation, Morrow says, was immediate. Real-time monitoring replaced multi-year audit cycles. The platform from market-leader Celonis quickly identified more than $10 million of inappropriate spending. And the oversight team was able to redeploy staff from 13 to 5 members while dramatically increasing effectiveness.“Process for Progress”: A global movementOklahoma&#x27;s pioneering success using powerful new process technology spotlights an emerging global trend. Morrow was among more than 3,000 leaders gathered at Celosphere, Celonis’s recent annual conference, to explore how AI, powered with business context by PI, can deliver commercial returns as well as environmental and financial benefits worldwide. The vision: process intelligence as a foundation for public and social progress. The movement sees the combination of AI and PI like Oklahoma’s as a powerful way to help governments and other organizations deliver vital services more cost effectively, with improved decisions and better-informed policies. From procurement to juvenile justice to healthcare and environment, scores of organizations are now getting a first look at the famously byzantine, opaque way things get done.For veteran financial leader Aubrey Vaughan — now Vice President of Strategy for Public Sector at Celonis and formerly a top executive at a major financial software firm — the move toward real process improvement has been a long time coming. He recalls testifying proudly before Congress a few years ago about uncovering $10 billion in improper government payments at his previous company. Afterward, a senior government official pulled him aside and suggested he downplay the achievement.The reason, he was told: \"The next question they&#x27;re going to ask you is, ‘Why is that happening?’” says Vaughan. “Today we can answer not only why, but how we fix it.\" Across the U.S. and the globe, public agencies are tightening budgets. Desire to deploy AI to close the gap is colliding with a hard reality: you can&#x27;t automate what you don&#x27;t understand. Here are three real-world examples of organizations using PI and AI for better outcomes. Oklahoma: Real-time AI spending analysis boosts accountability Within just 60 days of implementation, Celonis reviewed $29.4 billion worth of purchase order lines, identifying $8.48 billion in statutory exempt purchases and flagging problematic transactions. The system now provides real-time feedback to buyers within 15 minutes of purchases, allowing immediate course correction.The system revealed agencies were purchasing from a vendor at prices 45% lower than the statewide contract, forcing renegotiation. \"Real-time AI analysis has increased accountability by providing key insights into spending patterns and streamlining contract utilization,\" Morrow explains. Last year, Oklahoma adopted Celonis&#x27;s Copilot feature, which uses conversational AI to let executives ask questions in plain language. Now, when the Governor or a cabinet member wonders about a contract, they get answers in seconds, not weeks, Morrow says. Her group is expanding the technology to other agencies. It’s also exploring how emerging AI agent capabilities can further automate compliance and spending analysis.In Texas, uncovering a startling hidden pattern in young offenders At Evident Change, a social research non-profit, Erin Espinosa&#x27;s work is about good stewardship — not of taxpayer money, but of young lives. Analyzing 400,000 data points from juvenile justice and public health systems in Texas, the former probation officer-turned Ph.D. made a startling discovery: the mental health treatment that young offenders received (or didn’t) was a stronger predictor of incarceration than the seriousness of the offense that brought them into the system. Espinosa told courts, legislatures, Congress. Nobody believed it.Frustrated, she partnered with Monica Chiarini Tremblay, a professor at William & Mary College. While traditional analysis showed correlation, Celonis process intelligence helped the pair show a clear, quantitative causation: A fragmented mental health system was actively pushing kids toward worse outcomes. Further machine learning analysis also demonstrated that doubling down on the same interventions increased likelihood of undesirable out-of-home placement for juvenile offenders.Recently accepted for academic publication, the real-world findings represent both indictment and opportunity. Espinosa and Tremblay are planning a larger 2026 pilot implementation of PI-based analysis, bringing together social services, juvenile justice, mental health providers, and education officials. \"This is a perfect intersection of business, social work, adolescent development, and community financial implications,\" Espinosa says. They’re now exploring how AI agent technologies could flag at-risk youth and trigger coordinated responses before patterns become entrenched.A $1-trillion defense budget — that has never passed a clean audit The U.S. Department of Defense faces financial challenges on an exponentially larger scale. As Acting Secretary of the Army, Robert M. Speer hired a big-three accounting firm to map the service’s financial processes. Three years later, the analysis was obsolete — processes had changed dramatically. So, when Speer first saw process intelligence, he was truly excited about what it revealed. \"I can see not only the data,” he explained, “but where it&#x27;s coming from, the business process delivering it.\"Tom Steffens, former Deputy Chief Financial Officer of Defense, agrees: \"There&#x27;s clearly a missing piece to the puzzle.\" Both recently joined Celonis&#x27;s Public Sector Advisory Board. They see potential for AI agents to automate compliance monitoring across DoD&#x27;s complex ecosystem.The stakes are unimaginably huge. The Department of Defense will receive more than a trillion dollars in funding in FY 2026. It’s also the only federal cabinet agency that&#x27;s never passed a clean audit. Beyond accounting, fast-changing geopolitics and modern warfare demands systems as dynamic as current battle environments. \"We&#x27;re talking about the ability to shift in real time,\" says Speer. \"We know that’s what happens on the battlefield, but we need something on the back end of those enabling processes and systems to ensure that happens correctly.\" The pair is working with defense leaders to show how process intelligence can create the foundation for transformation — enabling modeling and scenario planning that can support battlefield decisions with data-driven confidence rather than delayed, obsolete information.Efforts to modernize and optimize complex government systems and processes got a big boost recently. Working with partner Knox Systems, Celonis received FedRAMP authorization earlier this year, the security credential required for federal cloud services. \"Knox powers the most secure and longest-running managed federal cloud,\" notes CEO Irina Denisenko, supporting 15+ federal agencies. The authorization positions the technology \"as the backbone of compliance for the next generation of government SaaS.\"Where process meets purposeEarly public sector adopters are proving what&#x27;s possible with process intelligence — from identifying billions in potential savings to revealing why children enter the prison pipeline. The potential extends wherever public funds shape public good: climate response, education, infrastructure, emergency services.Advocates often speak of “process for progress” or \"process for empathy\" — using transparency to change minds and hearts, not just policies. Says Chiarini Tremblay, who worked on the Texas juvenile offenders’ system: \"We have to understand complex systems and make data-driven decisions, but the goal is always improving outcomes for people.\"It’s not just a U.S. movement. In the UK, for example, University Hospitals Coventry and Warwickshire NHS Trust have deployed PI with dramatic effect. Director Andy Hardy used Celonis to analyze 244,000 outpatient cases, revealing massive variation in care delivery.By optimizing appointment reminders from four to 14 days before visits, the trust enabled earlier cancellations and saw an additional 1,800 patients weekly. The waiting list was reduced by 5,300 patients in eight weeks. Concludes Hardy: \"Data understandable to clinicians is as important as scalpels.\" Technology continues to race ahead. At Celosphere 2025, Celonis unveiled a host of new offerings and platform updates for public and private sector organizations including the Orchestration Engine, which coordinates actions across workflows involving AI agents, human tasks, and legacy systems.All are built on the Celonis Process Intelligence Graph, which creates a \"living digital twin\" of a business or public agency’s processes. It’s system-agnostic, working across disconnected systems typical to government operations — integrating decades-old mainframes and cutting-edge cloud applications simultaneously.Agency heads and others note, however, that success demands more than software. For example, when Oklahoma reduced its oversight team from 13 to 5, resistance emerged. Morrow&#x27;s team invested heavily in training and change management. Process intelligence reveals improvement opportunities, but people implement solutions’ she explains. Ongoing, long-term education and cultural change are needed. “Continuous operational improvement is a lifestyle,” says Celonis’s Vaughn. “You need to have a culture that wants to build better processes, better systems, more efficient systems.”The tools are ready. The business case is proven. What remains is the will to change — and the courage to look clearly at the systems meant to serve the public good.Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.",
          "content": "Presented by CelonisThe State of Oklahoma discovered its blind spots the hard way. In April 2023, a legislative report revealed its agencies had spent $3 billion without proper oversight. Janet Morrow, Director of Oklahoma&#x27;s Risk, Assessment and Compliance Division, set out to track thousands of monthly transactions across dozens of disconnected systems.The Sooner State became the first U.S. state to apply process intelligence (PI) technology for procurement oversight. The transformation, Morrow says, was immediate. Real-time monitoring replaced multi-year audit cycles. The platform from market-leader Celonis quickly identified more than $10 million of inappropriate spending. And the oversight team was able to redeploy staff from 13 to 5 members while dramatically increasing effectiveness.“Process for Progress”: A global movementOklahoma&#x27;s pioneering success using powerful new process technology spotlights an emerging global trend. Morrow was among more than 3,000 leaders gathered at Celosphere, Celonis’s recent annual conference, to explore how AI, powered with business context by PI, can deliver commercial returns as well as environmental and financial benefits worldwide. The vision: process intelligence as a foundation for public and social progress. The movement sees the combination of AI and PI like Oklahoma’s as a powerful way to help governments and other organizations deliver vital services more cost effectively, with improved decisions and better-informed policies. From procurement to juvenile justice to healthcare and environment, scores of organizations are now getting a first look at the famously byzantine, opaque way things get done.For veteran financial leader Aubrey Vaughan — now Vice President of Strategy for Public Sector at Celonis and formerly a top executive at a major financial software firm — the move toward real process improvement has been a long time coming. He recalls testifying proudly before Congress a few years ago about uncovering $10 billion in improper government payments at his previous company. Afterward, a senior government official pulled him aside and suggested he downplay the achievement.The reason, he was told: \"The next question they&#x27;re going to ask you is, ‘Why is that happening?’” says Vaughan. “Today we can answer not only why, but how we fix it.\" Across the U.S. and the globe, public agencies are tightening budgets. Desire to deploy AI to close the gap is colliding with a hard reality: you can&#x27;t automate what you don&#x27;t understand. Here are three real-world examples of organizations using PI and AI for better outcomes. Oklahoma: Real-time AI spending analysis boosts accountability Within just 60 days of implementation, Celonis reviewed $29.4 billion worth of purchase order lines, identifying $8.48 billion in statutory exempt purchases and flagging problematic transactions. The system now provides real-time feedback to buyers within 15 minutes of purchases, allowing immediate course correction.The system revealed agencies were purchasing from a vendor at prices 45% lower than the statewide contract, forcing renegotiation. \"Real-time AI analysis has increased accountability by providing key insights into spending patterns and streamlining contract utilization,\" Morrow explains. Last year, Oklahoma adopted Celonis&#x27;s Copilot feature, which uses conversational AI to let executives ask questions in plain language. Now, when the Governor or a cabinet member wonders about a contract, they get answers in seconds, not weeks, Morrow says. Her group is expanding the technology to other agencies. It’s also exploring how emerging AI agent capabilities can further automate compliance and spending analysis.In Texas, uncovering a startling hidden pattern in young offenders At Evident Change, a social research non-profit, Erin Espinosa&#x27;s work is about good stewardship — not of taxpayer money, but of young lives. Analyzing 400,000 data points from juvenile justice and public health systems in Texas, the former probation officer-turned Ph.D. made a startling discovery: the mental health treatment that young offenders received (or didn’t) was a stronger predictor of incarceration than the seriousness of the offense that brought them into the system. Espinosa told courts, legislatures, Congress. Nobody believed it.Frustrated, she partnered with Monica Chiarini Tremblay, a professor at William & Mary College. While traditional analysis showed correlation, Celonis process intelligence helped the pair show a clear, quantitative causation: A fragmented mental health system was actively pushing kids toward worse outcomes. Further machine learning analysis also demonstrated that doubling down on the same interventions increased likelihood of undesirable out-of-home placement for juvenile offenders.Recently accepted for academic publication, the real-world findings represent both indictment and opportunity. Espinosa and Tremblay are planning a larger 2026 pilot implementation of PI-based analysis, bringing together social services, juvenile justice, mental health providers, and education officials. \"This is a perfect intersection of business, social work, adolescent development, and community financial implications,\" Espinosa says. They’re now exploring how AI agent technologies could flag at-risk youth and trigger coordinated responses before patterns become entrenched.A $1-trillion defense budget — that has never passed a clean audit The U.S. Department of Defense faces financial challenges on an exponentially larger scale. As Acting Secretary of the Army, Robert M. Speer hired a big-three accounting firm to map the service’s financial processes. Three years later, the analysis was obsolete — processes had changed dramatically. So, when Speer first saw process intelligence, he was truly excited about what it revealed. \"I can see not only the data,” he explained, “but where it&#x27;s coming from, the business process delivering it.\"Tom Steffens, former Deputy Chief Financial Officer of Defense, agrees: \"There&#x27;s clearly a missing piece to the puzzle.\" Both recently joined Celonis&#x27;s Public Sector Advisory Board. They see potential for AI agents to automate compliance monitoring across DoD&#x27;s complex ecosystem.The stakes are unimaginably huge. The Department of Defense will receive more than a trillion dollars in funding in FY 2026. It’s also the only federal cabinet agency that&#x27;s never passed a clean audit. Beyond accounting, fast-changing geopolitics and modern warfare demands systems as dynamic as current battle environments. \"We&#x27;re talking about the ability to shift in real time,\" says Speer. \"We know that’s what happens on the battlefield, but we need something on the back end of those enabling processes and systems to ensure that happens correctly.\" The pair is working with defense leaders to show how process intelligence can create the foundation for transformation — enabling modeling and scenario planning that can support battlefield decisions with data-driven confidence rather than delayed, obsolete information.Efforts to modernize and optimize complex government systems and processes got a big boost recently. Working with partner Knox Systems, Celonis received FedRAMP authorization earlier this year, the security credential required for federal cloud services. \"Knox powers the most secure and longest-running managed federal cloud,\" notes CEO Irina Denisenko, supporting 15+ federal agencies. The authorization positions the technology \"as the backbone of compliance for the next generation of government SaaS.\"Where process meets purposeEarly public sector adopters are proving what&#x27;s possible with process intelligence — from identifying billions in potential savings to revealing why children enter the prison pipeline. The potential extends wherever public funds shape public good: climate response, education, infrastructure, emergency services.Advocates often speak of “process for progress” or \"process for empathy\" — using transparency to change minds and hearts, not just policies. Says Chiarini Tremblay, who worked on the Texas juvenile offenders’ system: \"We have to understand complex systems and make data-driven decisions, but the goal is always improving outcomes for people.\"It’s not just a U.S. movement. In the UK, for example, University Hospitals Coventry and Warwickshire NHS Trust have deployed PI with dramatic effect. Director Andy Hardy used Celonis to analyze 244,000 outpatient cases, revealing massive variation in care delivery.By optimizing appointment reminders from four to 14 days before visits, the trust enabled earlier cancellations and saw an additional 1,800 patients weekly. The waiting list was reduced by 5,300 patients in eight weeks. Concludes Hardy: \"Data understandable to clinicians is as important as scalpels.\" Technology continues to race ahead. At Celosphere 2025, Celonis unveiled a host of new offerings and platform updates for public and private sector organizations including the Orchestration Engine, which coordinates actions across workflows involving AI agents, human tasks, and legacy systems.All are built on the Celonis Process Intelligence Graph, which creates a \"living digital twin\" of a business or public agency’s processes. It’s system-agnostic, working across disconnected systems typical to government operations — integrating decades-old mainframes and cutting-edge cloud applications simultaneously.Agency heads and others note, however, that success demands more than software. For example, when Oklahoma reduced its oversight team from 13 to 5, resistance emerged. Morrow&#x27;s team invested heavily in training and change management. Process intelligence reveals improvement opportunities, but people implement solutions’ she explains. Ongoing, long-term education and cultural change are needed. “Continuous operational improvement is a lifestyle,” says Celonis’s Vaughn. “You need to have a culture that wants to build better processes, better systems, more efficient systems.”The tools are ready. The business case is proven. What remains is the will to change — and the courage to look clearly at the systems meant to serve the public good.Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.",
          "feed_position": 4,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/23oNXRNgKBLbHuQlKDjPbP/736c1c4212c55997871975219bea29ae/AdobeStock_1070394941.jpeg?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/z-ai-debuts-open-source-glm-4-6v-a-native-tool-calling-vision-model-for",
          "published_at": "Tue, 09 Dec 2025 01:03:00 GMT",
          "title": "Z.ai debuts open source GLM-4.6V, a native tool-calling vision model for multimodal reasoning",
          "standfirst": "Chinese AI startup Zhipu AI aka Z.ai has released its GLM-4.6V series, a new generation of open-source vision-language models (VLMs) optimized for multimodal reasoning, frontend automation, and high-efficiency deployment. The release includes two models in \"large\" and \"small\" sizes: GLM-4.6V (106B), a larger 106-billion parameter model aimed at cloud-scale inferenceGLM-4.6V-Flash (9B), a smaller model of only 9 billion parameters designed for low-latency, local applicationsRecall that generally speaking, models with more parameters — or internal settings governing their behavior, i.e. weights and biases — are more powerful, performant, and capable of performing at a higher general level across more varied tasks.However, smaller models can offer better efficiency for edge or real-time applications where latency and resource constraints are critical.The defining innovation in this series is the introduction of native function calling in a vision-language model—enabling direct use of tools such as search, cropping, or chart recognition with visual inputs. With a 128,000 token context length (equivalent to a 300-page novel&#x27;s worth of text exchanged in a single input/output interaction with the user) and state-of-the-art (SoTA) results across more than 20 benchmarks, the GLM-4.6V series positions itself as a highly competitive alternative to both closed and open-source VLMs. It&#x27;s available in the following formats:API access via OpenAI-compatible interfaceTry the demo on Zhipu’s web interfaceDownload weights from Hugging FaceDesktop assistant app available on Hugging Face SpacesLicensing and Enterprise UseGLM‑4.6V and GLM‑4.6V‑Flash are distributed under the MIT license, a permissive open-source license that allows free commercial and non-commercial use, modification, redistribution, and local deployment without obligation to open-source derivative works. This licensing model makes the series suitable for enterprise adoption, including scenarios that require full control over infrastructure, compliance with internal governance, or air-gapped environments.Model weights and documentation are publicly hosted on Hugging Face, with supporting code and tooling available on GitHub. The MIT license ensures maximum flexibility for integration into proprietary systems, including internal tools, production pipelines, and edge deployments.Architecture and Technical CapabilitiesThe GLM-4.6V models follow a conventional encoder-decoder architecture with significant adaptations for multimodal input. Both models incorporate a Vision Transformer (ViT) encoder—based on AIMv2-Huge—and an MLP projector to align visual features with a large language model (LLM) decoder. Video inputs benefit from 3D convolutions and temporal compression, while spatial encoding is handled using 2D-RoPE and bicubic interpolation of absolute positional embeddings.A key technical feature is the system’s support for arbitrary image resolutions and aspect ratios, including wide panoramic inputs up to 200:1. In addition to static image and document parsing, GLM-4.6V can ingest temporal sequences of video frames with explicit timestamp tokens, enabling robust temporal reasoning.On the decoding side, the model supports token generation aligned with function-calling protocols, allowing for structured reasoning across text, image, and tool outputs. This is supported by extended tokenizer vocabulary and output formatting templates to ensure consistent API or agent compatibility.Native Multimodal Tool UseGLM-4.6V introduces native multimodal function calling, allowing visual assets—such as screenshots, images, and documents—to be passed directly as parameters to tools. This eliminates the need for intermediate text-only conversions, which have historically introduced information loss and complexity.The tool invocation mechanism works bi-directionally:Input tools can be passed images or videos directly (e.g., document pages to crop or analyze).Output tools such as chart renderers or web snapshot utilities return visual data, which GLM-4.6V integrates directly into the reasoning chain.In practice, this means GLM-4.6V can complete tasks such as:Generating structured reports from mixed-format documentsPerforming visual audit of candidate imagesAutomatically cropping figures from papers during generationConducting visual web search and answering multimodal queriesHigh Performance Benchmarks Compared to Other Similar-Sized ModelsGLM-4.6V was evaluated across more than 20 public benchmarks covering general VQA, chart understanding, OCR, STEM reasoning, frontend replication, and multimodal agents. According to the benchmark chart released by Zhipu AI:GLM-4.6V (106B) achieves SoTA or near-SoTA scores among open-source models of comparable size (106B) on MMBench, MathVista, MMLongBench, ChartQAPro, RefCOCO, TreeBench, and more.GLM-4.6V-Flash (9B) outperforms other lightweight models (e.g., Qwen3-VL-8B, GLM-4.1V-9B) across almost all categories tested.The 106B model’s 128K-token window allows it to outperform larger models like Step-3 (321B) and Qwen3-VL-235B on long-context document tasks, video summarization, and structured multimodal reasoning.Example scores from the leaderboard include:MathVista: 88.2 (GLM-4.6V) vs. 84.6 (GLM-4.5V) vs. 81.4 (Qwen3-VL-8B)WebVoyager: 81.0 vs. 68.4 (Qwen3-VL-8B)Ref-L4-test: 88.9 vs. 89.5 (GLM-4.5V), but with better grounding fidelity at 87.7 (Flash) vs. 86.8Both models were evaluated using the vLLM inference backend and support SGLang for video-based tasks.Frontend Automation and Long-Context WorkflowsZhipu AI emphasized GLM-4.6V’s ability to support frontend development workflows. The model can:Replicate pixel-accurate HTML/CSS/JS from UI screenshotsAccept natural language editing commands to modify layoutsIdentify and manipulate specific UI components visuallyThis capability is integrated into an end-to-end visual programming interface, where the model iterates on layout, design intent, and output code using its native understanding of screen captures.In long-document scenarios, GLM-4.6V can process up to 128,000 tokens—enabling a single inference pass across:150 pages of text (input)200 slide decks1-hour videosZhipu AI reported successful use of the model in financial analysis across multi-document corpora and in summarizing full-length sports broadcasts with timestamped event detection.Training and Reinforcement LearningThe model was trained using multi-stage pre-training followed by supervised fine-tuning (SFT) and reinforcement learning (RL). Key innovations include:Curriculum Sampling (RLCS): Dynamically adjusts the difficulty of training samples based on model progressMulti-domain reward systems: Task-specific verifiers for STEM, chart reasoning, GUI agents, video QA, and spatial groundingFunction-aware training: Uses structured tags (e.g., <think>, <answer>, <|begin_of_box|>) to align reasoning and answer formattingThe reinforcement learning pipeline emphasizes verifiable rewards (RLVR) over human feedback (RLHF) for scalability, and avoids KL/entropy losses to stabilize training across multimodal domainsPricing (API)Zhipu AI offers competitive pricing for the GLM-4.6V series, with both the flagship model and its lightweight variant positioned for high accessibility.GLM-4.6V: $0.30 (input) / $0.90 (output) per 1M tokensGLM-4.6V-Flash: FreeCompared to major vision-capable and text-first LLMs, GLM-4.6V is among the most cost-efficient for multimodal reasoning at scale. Below is a comparative snapshot of pricing across providers:USD per 1M tokens — sorted lowest → highest total costModelInputOutputTotal CostSourceQwen 3 Turbo$0.05$0.20$0.25Alibaba CloudERNIE 4.5 Turbo$0.11$0.45$0.56QianfanGLM‑4.6V$0.30$0.90$1.20Z.AIGrok 4.1 Fast (reasoning)$0.20$0.50$0.70xAIGrok 4.1 Fast (non-reasoning)$0.20$0.50$0.70xAIdeepseek-chat (V3.2-Exp)$0.28$0.42$0.70DeepSeekdeepseek-reasoner (V3.2-Exp)$0.28$0.42$0.70DeepSeekQwen 3 Plus$0.40$1.20$1.60Alibaba CloudERNIE 5.0$0.85$3.40$4.25QianfanQwen-Max$1.60$6.40$8.00Alibaba CloudGPT-5.1$1.25$10.00$11.25OpenAIGemini 2.5 Pro (≤200K)$1.25$10.00$11.25GoogleGemini 3 Pro (≤200K)$2.00$12.00$14.00GoogleGemini 2.5 Pro (>200K)$2.50$15.00$17.50GoogleGrok 4 (0709)$3.00$15.00$18.00xAIGemini 3 Pro (>200K)$4.00$18.00$22.00GoogleClaude Opus 4.1$15.00$75.00$90.00AnthropicPrevious Releases: GLM‑4.5 Series and Enterprise ApplicationsPrior to GLM‑4.6V, Z.ai released the GLM‑4.5 family in mid-2025, establishing the company as a serious contender in open-source LLM development. The flagship GLM‑4.5 and its smaller sibling GLM‑4.5‑Air both support reasoning, tool use, coding, and agentic behaviors, while offering strong performance across standard benchmarks. The models introduced dual reasoning modes (“thinking” and “non-thinking”) and could automatically generate complete PowerPoint presentations from a single prompt — a feature positioned for use in enterprise reporting, education, and internal comms workflows. Z.ai also extended the GLM‑4.5 series with additional variants such as GLM‑4.5‑X, AirX, and Flash, targeting ultra-fast inference and low-cost scenarios.Together, these features position the GLM‑4.5 series as a cost-effective, open, and production-ready alternative for enterprises needing autonomy over model deployment, lifecycle management, and integration pipelEcosystem ImplicationsThe GLM-4.6V release represents a notable advance in open-source multimodal AI. While large vision-language models have proliferated over the past year, few offer:Integrated visual tool usageStructured multimodal generationAgent-oriented memory and decision logicZhipu AI’s emphasis on “closing the loop” from perception to action via native function calling marks a step toward agentic multimodal systems. The model’s architecture and training pipeline show a continued evolution of the GLM family, positioning it competitively alongside offerings like OpenAI’s GPT-4V and Google DeepMind’s Gemini-VL.Takeaway for Enterprise LeadersWith GLM-4.6V, Zhipu AI introduces an open-source VLM capable of native visual tool use, long-context reasoning, and frontend automation. It sets new performance marks among models of similar size and provides a scalable platform for building agentic, multimodal AI systems.",
          "content": "Chinese AI startup Zhipu AI aka Z.ai has released its GLM-4.6V series, a new generation of open-source vision-language models (VLMs) optimized for multimodal reasoning, frontend automation, and high-efficiency deployment. The release includes two models in \"large\" and \"small\" sizes: GLM-4.6V (106B), a larger 106-billion parameter model aimed at cloud-scale inferenceGLM-4.6V-Flash (9B), a smaller model of only 9 billion parameters designed for low-latency, local applicationsRecall that generally speaking, models with more parameters — or internal settings governing their behavior, i.e. weights and biases — are more powerful, performant, and capable of performing at a higher general level across more varied tasks.However, smaller models can offer better efficiency for edge or real-time applications where latency and resource constraints are critical.The defining innovation in this series is the introduction of native function calling in a vision-language model—enabling direct use of tools such as search, cropping, or chart recognition with visual inputs. With a 128,000 token context length (equivalent to a 300-page novel&#x27;s worth of text exchanged in a single input/output interaction with the user) and state-of-the-art (SoTA) results across more than 20 benchmarks, the GLM-4.6V series positions itself as a highly competitive alternative to both closed and open-source VLMs. It&#x27;s available in the following formats:API access via OpenAI-compatible interfaceTry the demo on Zhipu’s web interfaceDownload weights from Hugging FaceDesktop assistant app available on Hugging Face SpacesLicensing and Enterprise UseGLM‑4.6V and GLM‑4.6V‑Flash are distributed under the MIT license, a permissive open-source license that allows free commercial and non-commercial use, modification, redistribution, and local deployment without obligation to open-source derivative works. This licensing model makes the series suitable for enterprise adoption, including scenarios that require full control over infrastructure, compliance with internal governance, or air-gapped environments.Model weights and documentation are publicly hosted on Hugging Face, with supporting code and tooling available on GitHub. The MIT license ensures maximum flexibility for integration into proprietary systems, including internal tools, production pipelines, and edge deployments.Architecture and Technical CapabilitiesThe GLM-4.6V models follow a conventional encoder-decoder architecture with significant adaptations for multimodal input. Both models incorporate a Vision Transformer (ViT) encoder—based on AIMv2-Huge—and an MLP projector to align visual features with a large language model (LLM) decoder. Video inputs benefit from 3D convolutions and temporal compression, while spatial encoding is handled using 2D-RoPE and bicubic interpolation of absolute positional embeddings.A key technical feature is the system’s support for arbitrary image resolutions and aspect ratios, including wide panoramic inputs up to 200:1. In addition to static image and document parsing, GLM-4.6V can ingest temporal sequences of video frames with explicit timestamp tokens, enabling robust temporal reasoning.On the decoding side, the model supports token generation aligned with function-calling protocols, allowing for structured reasoning across text, image, and tool outputs. This is supported by extended tokenizer vocabulary and output formatting templates to ensure consistent API or agent compatibility.Native Multimodal Tool UseGLM-4.6V introduces native multimodal function calling, allowing visual assets—such as screenshots, images, and documents—to be passed directly as parameters to tools. This eliminates the need for intermediate text-only conversions, which have historically introduced information loss and complexity.The tool invocation mechanism works bi-directionally:Input tools can be passed images or videos directly (e.g., document pages to crop or analyze).Output tools such as chart renderers or web snapshot utilities return visual data, which GLM-4.6V integrates directly into the reasoning chain.In practice, this means GLM-4.6V can complete tasks such as:Generating structured reports from mixed-format documentsPerforming visual audit of candidate imagesAutomatically cropping figures from papers during generationConducting visual web search and answering multimodal queriesHigh Performance Benchmarks Compared to Other Similar-Sized ModelsGLM-4.6V was evaluated across more than 20 public benchmarks covering general VQA, chart understanding, OCR, STEM reasoning, frontend replication, and multimodal agents. According to the benchmark chart released by Zhipu AI:GLM-4.6V (106B) achieves SoTA or near-SoTA scores among open-source models of comparable size (106B) on MMBench, MathVista, MMLongBench, ChartQAPro, RefCOCO, TreeBench, and more.GLM-4.6V-Flash (9B) outperforms other lightweight models (e.g., Qwen3-VL-8B, GLM-4.1V-9B) across almost all categories tested.The 106B model’s 128K-token window allows it to outperform larger models like Step-3 (321B) and Qwen3-VL-235B on long-context document tasks, video summarization, and structured multimodal reasoning.Example scores from the leaderboard include:MathVista: 88.2 (GLM-4.6V) vs. 84.6 (GLM-4.5V) vs. 81.4 (Qwen3-VL-8B)WebVoyager: 81.0 vs. 68.4 (Qwen3-VL-8B)Ref-L4-test: 88.9 vs. 89.5 (GLM-4.5V), but with better grounding fidelity at 87.7 (Flash) vs. 86.8Both models were evaluated using the vLLM inference backend and support SGLang for video-based tasks.Frontend Automation and Long-Context WorkflowsZhipu AI emphasized GLM-4.6V’s ability to support frontend development workflows. The model can:Replicate pixel-accurate HTML/CSS/JS from UI screenshotsAccept natural language editing commands to modify layoutsIdentify and manipulate specific UI components visuallyThis capability is integrated into an end-to-end visual programming interface, where the model iterates on layout, design intent, and output code using its native understanding of screen captures.In long-document scenarios, GLM-4.6V can process up to 128,000 tokens—enabling a single inference pass across:150 pages of text (input)200 slide decks1-hour videosZhipu AI reported successful use of the model in financial analysis across multi-document corpora and in summarizing full-length sports broadcasts with timestamped event detection.Training and Reinforcement LearningThe model was trained using multi-stage pre-training followed by supervised fine-tuning (SFT) and reinforcement learning (RL). Key innovations include:Curriculum Sampling (RLCS): Dynamically adjusts the difficulty of training samples based on model progressMulti-domain reward systems: Task-specific verifiers for STEM, chart reasoning, GUI agents, video QA, and spatial groundingFunction-aware training: Uses structured tags (e.g., <think>, <answer>, <|begin_of_box|>) to align reasoning and answer formattingThe reinforcement learning pipeline emphasizes verifiable rewards (RLVR) over human feedback (RLHF) for scalability, and avoids KL/entropy losses to stabilize training across multimodal domainsPricing (API)Zhipu AI offers competitive pricing for the GLM-4.6V series, with both the flagship model and its lightweight variant positioned for high accessibility.GLM-4.6V: $0.30 (input) / $0.90 (output) per 1M tokensGLM-4.6V-Flash: FreeCompared to major vision-capable and text-first LLMs, GLM-4.6V is among the most cost-efficient for multimodal reasoning at scale. Below is a comparative snapshot of pricing across providers:USD per 1M tokens — sorted lowest → highest total costModelInputOutputTotal CostSourceQwen 3 Turbo$0.05$0.20$0.25Alibaba CloudERNIE 4.5 Turbo$0.11$0.45$0.56QianfanGLM‑4.6V$0.30$0.90$1.20Z.AIGrok 4.1 Fast (reasoning)$0.20$0.50$0.70xAIGrok 4.1 Fast (non-reasoning)$0.20$0.50$0.70xAIdeepseek-chat (V3.2-Exp)$0.28$0.42$0.70DeepSeekdeepseek-reasoner (V3.2-Exp)$0.28$0.42$0.70DeepSeekQwen 3 Plus$0.40$1.20$1.60Alibaba CloudERNIE 5.0$0.85$3.40$4.25QianfanQwen-Max$1.60$6.40$8.00Alibaba CloudGPT-5.1$1.25$10.00$11.25OpenAIGemini 2.5 Pro (≤200K)$1.25$10.00$11.25GoogleGemini 3 Pro (≤200K)$2.00$12.00$14.00GoogleGemini 2.5 Pro (>200K)$2.50$15.00$17.50GoogleGrok 4 (0709)$3.00$15.00$18.00xAIGemini 3 Pro (>200K)$4.00$18.00$22.00GoogleClaude Opus 4.1$15.00$75.00$90.00AnthropicPrevious Releases: GLM‑4.5 Series and Enterprise ApplicationsPrior to GLM‑4.6V, Z.ai released the GLM‑4.5 family in mid-2025, establishing the company as a serious contender in open-source LLM development. The flagship GLM‑4.5 and its smaller sibling GLM‑4.5‑Air both support reasoning, tool use, coding, and agentic behaviors, while offering strong performance across standard benchmarks. The models introduced dual reasoning modes (“thinking” and “non-thinking”) and could automatically generate complete PowerPoint presentations from a single prompt — a feature positioned for use in enterprise reporting, education, and internal comms workflows. Z.ai also extended the GLM‑4.5 series with additional variants such as GLM‑4.5‑X, AirX, and Flash, targeting ultra-fast inference and low-cost scenarios.Together, these features position the GLM‑4.5 series as a cost-effective, open, and production-ready alternative for enterprises needing autonomy over model deployment, lifecycle management, and integration pipelEcosystem ImplicationsThe GLM-4.6V release represents a notable advance in open-source multimodal AI. While large vision-language models have proliferated over the past year, few offer:Integrated visual tool usageStructured multimodal generationAgent-oriented memory and decision logicZhipu AI’s emphasis on “closing the loop” from perception to action via native function calling marks a step toward agentic multimodal systems. The model’s architecture and training pipeline show a continued evolution of the GLM family, positioning it competitively alongside offerings like OpenAI’s GPT-4V and Google DeepMind’s Gemini-VL.Takeaway for Enterprise LeadersWith GLM-4.6V, Zhipu AI introduces an open-source VLM capable of native visual tool use, long-context reasoning, and frontend automation. It sets new performance marks among models of similar size and provides a scalable platform for building agentic, multimodal AI systems.",
          "feed_position": 5,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/c8LjxKPtjJumRUwmgig3W/3ceb88204d1ec334f3ce3719b80793eb/6Ud5A3f_99gyh14m2ffZr.jpg?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/gaming/katsuhiro-harada-is-leaving-bandai-namco-after-30-years-223156258.html",
          "published_at": "Mon, 08 Dec 2025 22:31:56 +0000",
          "title": "Katsuhiro Harada is leaving Bandai Namco after 30 years",
          "standfirst": "Katsuhiro Harada is departing Bandai Namco at the end of 2025. He announced the news both with a farewell note shared on X and, in possibly the coolest exit ever, with an hour-long DJ mix. Harada's 30-year career has most closely been involved with Tekken and he's a familiar face in the fighting game community.He began as a voice actor in the original Tekken game and continued to do so even as he became a director for the series. He has worked on other Bandai Namco titles as a producer, both in and out of the fighting genre. \"Each project was full of new discoveries and learning, and every one of them became an irreplaceable experience for me,\" Harada wrote on X. \"To everyone who has supported me, to communities around the world, and to all the colleagues who have walked alongside me for so many years, I offer my deepest gratitude.\"He closed by saying that over his career, he never DJed at a tournament event. So to mark his departure, Harada posted a full set titled ‘TEKKEN: A 30-Year Journey – Harada’s Final Mix’ to SoundCloud. Which is just the most swag move I can think of and a fun way to close out this chapter for fans of the franchise.This article originally appeared on Engadget at https://www.engadget.com/gaming/katsuhiro-harada-is-leaving-bandai-namco-after-30-years-223156258.html?src=rss",
          "content": "Katsuhiro Harada is departing Bandai Namco at the end of 2025. He announced the news both with a farewell note shared on X and, in possibly the coolest exit ever, with an hour-long DJ mix. Harada's 30-year career has most closely been involved with Tekken and he's a familiar face in the fighting game community.He began as a voice actor in the original Tekken game and continued to do so even as he became a director for the series. He has worked on other Bandai Namco titles as a producer, both in and out of the fighting genre. \"Each project was full of new discoveries and learning, and every one of them became an irreplaceable experience for me,\" Harada wrote on X. \"To everyone who has supported me, to communities around the world, and to all the colleagues who have walked alongside me for so many years, I offer my deepest gratitude.\"He closed by saying that over his career, he never DJed at a tournament event. So to mark his departure, Harada posted a full set titled ‘TEKKEN: A 30-Year Journey – Harada’s Final Mix’ to SoundCloud. Which is just the most swag move I can think of and a fun way to close out this chapter for fans of the franchise.This article originally appeared on Engadget at https://www.engadget.com/gaming/katsuhiro-harada-is-leaving-bandai-namco-after-30-years-223156258.html?src=rss",
          "feed_position": 34
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/wearables/heres-how-google-is-laying-the-foundation-for-our-mixed-reality-future-180000716.html",
          "published_at": "Mon, 08 Dec 2025 19:57:32 +0000",
          "title": "Here's how Google is laying the foundation for our mixed reality future",
          "standfirst": "Today, during the XR edition of The Android Show, Google showed off a bunch of updates and new features headed to its mixed reality OS. And while most of the news was aimed at developers, I got a chance to demo some of the platform's expanded capabilities on a range of hardware including Samsung's Galaxy XR headset, two different reference designs and an early version of Xreal's Project Aura smart glasses and I came away rather impressed. So here's a rundown of what I saw and how it will impact the rapidly growing ecosystem of head-mounted displays.First up was one of Google's reference design smart glasses with a single waveguide RGB display built into its right lens. I've included a picture of it here, but try not to read too deeply into its design or aesthetics, as this device is meant to be a testbed for Android XR features and not an early look at upcoming models.Try not to read too much into the appearance of Google's reference design smart glasses, as they are explicitly labeled as prototypes meant to test upcoming features in Android XR.Sam Rutherford for EngadgetAfter putting them on, I was able to ask Gemini to play some tunes on YouTube Music before answering a call simply by tapping on the touchpad built into the right side of the frames. And because the reference model also had onboard world-facing cameras, I could easily share my view with the person on the other end of the line. Naturally, I was curious about how glasses had the bandwidth to do all this, because in normal use, they rely on a Bluetooth or Bluetooth LE connection. When asked, Max Spear, Group Product Manager for XR, shared that depending on the situation, the device can seamlessly switch between both Bluetooth and Wi-Fi, which was rather impressive because I couldn't even detect when that transition happened. Spear also noted that one of Google's focuses for Android XR is making it easier for developers to port over the apps people already know and love. This means for devices like the reference design I wore that feature a built-in display (or displays), the OS actually uses the same code meant for standard Android notifications (like quick replies) to create a minimalist UI instead of forcing app makers to update each piece of software to be compliant with an ever-increasing number of devices. Alternatively, for models that are super lightweight and rely strictly on speakers (like Bose Frames), Google has also designed Android XR so that you only need mics and voice controls to access a wide variety of apps without the need for visual menus. This is the picture Google's reference design smart glasses created (via Gemini ) when I asked it to transform a photo I took of some pantry shelves into a sci-fi kitchen. Sam Rutherford for EngadgetMeanwhile, if you're hoping to take photos with your smart glasses, there's a surprising amount of capability there, too. Not only was I able to ask Gemini to take a photo, the glasses were also able to send a higher-res version to a connected smartwatch, which is super handy in case you want to review the image before moving on to the next shot. And when you want to inject some creativity, you can ask Gemini to transform pictures into practically anything you can imagine via Nano Banana. In my case, I asked the AI to change a shot of a pantry into a sci-fi kitchen and Gemini delivered with aplomb, including converting the room into a metal-clad setting complete with lots of light strips and a few bursts of steam.However, one of the most impressive demos was when I asked Google's reference glasses to look at some of that same pantry environment and then use the ingredients to create a recipe based on my specifications (no tomatoes please, my wife isn't a fan). Gemini went down an Italian route by picking pasta, jarred banana peppers, bell peppers (which I thought was a somewhat unusual combination) and more, before launching into the first steps of the recipe. Sadly, I didn't have time to actually cook it, but as part of the demo, I learned that Gemini has been trained to understand human-centric gestures like pointing and picking things up. This allows it to better understand context without the need to be super specific, which is one of those little but very impactful tricks that allows AI to feel way less robotic. This is how Google Maps will look on Android XR. Note that this is the flat 2D version instead of the more detailed stereoscopic view available on smart glasses with dual displays. Sam Rutherford for EngadgetThen I had a chance to see how Uber and Google Maps ran on the reference glasses, this time using models with both single and dual RGB displays. Surprisingly, even on the monocular version, Maps was able to generate a detailed map with the ability to zoom in and out. But when I switched over to the binocular model, I noticed a significant jump in sharpness and clarity along with a higher-fidelity map with stereoscopic 3D images of buildings. Now, it may be a bit early to call this, and the perception of sharpness varies greatly between people based on their head shape and other factors, but after seeing that, I'm even more convinced that the smart glasses with dual RGB displays are what the industry will settle on in the long term.The second type of device I used was the Samsung Galaxy XR, which I originally tried out when it was announced back in October. However, in the short time since, Google has cooked up a few new features that really help expand the headset's capabilities. By using the goggle's exterior-facing cameras, I was able to play a game of I Spy with Gemini. Admittedly, this might sound like a small addition, but I think it's going to play a big part in how we use devices running Android XR, because it allows the headset (or glasses) to understand better what you're looking at in order to provide more helpful contextual responses. Even though it was announced not long ago in late October, Samsung's Galaxy XR headset is already getting some new features thanks to some updates coming to Android XR. Sam Rutherford for EngadgetHowever, the biggest surprise was when I joined a virtual call with someone using one of Google's new avatars, called Likeness. Instead of the low-polygon cartoony characters we've seen before in places like Meta Horizon, Google's virtual representations of people's faces are almost scary good. So good I had to double-check that they weren't real and from what I've seen they're even a step up from Apple's Personas. Google says that headsets like the Galaxy XR rely on interior sensors to track and respond to facial movements, while users will be able to create and edit their avatars using a standalone app due out sometime next year. The person in the bottom right is using a Likeness, which during my demo looked surprisingly responsive and realistic. GoogleNext, I got a chance to test out the Android XR's PC connectivity by playing Stray on the Galaxy XR while it was tethered wirelessly to a nearby laptop. Not only did it run almost flawlessly with low latency, I was also able to use a paired controller instead of relying on hand-tracking or the laptop's mouse and keyboard. This is something I've been eagerly waiting to try because it feels like Google has put a lot of work into making Android XR devices play nicely with other devices and OSes. Initially, you'll only be able to connect Windows PCs to the Galaxy XR, but Google says it's looking to support macOS systems as well.Finally, I got to try out Xreal's Project Aura glasses to see how Android XR works on a device primarily designed to give you big virtual displays in a portable form factor. Unfortunately, because this was a pre-production unit, I wasn't able to take photos. That said, as far as the glasses go, I was really impressed with their resolution and sharpness and the inclusion of electrochromic glass is a really nice touch, as it allows users to change how heavily the lenses are tinted with a single touch. Alternatively, the glasses can also adjust the tint automatically based on whatever app you are using to give you a more or less isolated atmosphere, depending on the situation. I also appreciate the Aura's increased 70-degree FOV, but if I'm nitpicking, I wish it were a bit higher, as I occasionally found myself wanting a bit more vertical display area. Unfortunately, I wasn't allowed to take photos of Xreal's Project Aura smart glasses, as the model I used was still an early pre-production unit. So here's a shot provided by Google instead. Google / XrealAs a device that's sort of between lightweight smart glasses and a full VR headset, the Aura relies on a wired battery pack that also doubles as a touchpad and a hub for plugging in external devices like your phone, laptop or even game consoles. While using the Aura, I was able to connect to a different PC and multitask in style, as the glasses were able to support multiple virtual displays while running several different apps at the same time. This allowed me to be on a virtual call with someone using a Likeness while I had two other virtual windows open on either side. I also played an AR game (Demeo) while I moved around in virtual space and used my hands to reposition the battlefield or pick up objects with my hands. Now I will fully admit this is a lot and it took me a bit to process everything. But upon reflection, I have a few takeaways from my time with the various Android XR devices and prototypes. More than any other headset or smart glasses platform out now, it feels like Google is doing a ton to embrace a growing ecosystem of devices. That's really important because we're still so early in the lifecycle for wearable gadgets with displays that no one has really figured out a truly polished design like we have for smartphones and laptops. And until we get there, this means that a highly adaptable OS will go a long way towards supporting OEMs like Samsung, Xreal and others. But that's not all. It's clear Google is focused on making Android XR devices easy to build for. That's because the company knows that without useful software that can highlight the components and features coming on next-gen spectacles, there's a chance that interest will remain rather niche — similar to what we've seen when looking at the adoption of VR headsets. So in a way, Google is waging a battle on two fronts, which makes navigating uncharted waters that much more difficult. A major focus for Android XR while people are still figuring out how to make smart glasses is to support a wide variety of designs including those with single displays, dual displays or models without any displays that rely on cameras and speakers. Sam Rutherford for EngadgetGoogle is putting a major emphasis on Android XR's ability to serve as a framework for future gadgets and support and address developer needs. This mirrors the approach the company takes with regular Android and the opposite of Apple's typical MO, because unlike the Vision Pro and visionOS, it appears Google is going to rely heavily on its partners like Xreal, Warby Parker, Gentle Monster and others to create engaging hardware. Furthermore, Google says it plans to support smart glasses that can be tethered to Android and iOS phones, as well as smartwatches from both ecosystems, though there will be some limitations for people using Apple devices due to inherent OS restrictions. That's not to say that there won't be Pixel glasses sometime down the road, but at least for now, I think that's a smart approach and possibly a lesson Google learned after releasing Google Glass over a decade ago. Meanwhile, hi-res and incredibly realistic avatars like Likenesses could be a turning point for virtual collaboration, because, in a first for me, talking to a digital representation of someone else felt kind of natural. After my demos, I had a chance to talk to Senior Director of Product Management for XR Juston Payne, who highlighted the difference between smart glasses and typical gadgets by saying \"Smart glasses have to be great glasses first. They need to have a good form factor, good lenses with prescription support, they need to look good and they have to be easy to buy.\"That's no simple task and there's no guarantee that next-gen smart glasses and headsets will be a grand slam. But from what I've seen, Google is building a very compelling foundation with Android XR.This article originally appeared on Engadget at https://www.engadget.com/wearables/heres-how-google-is-laying-the-foundation-for-our-mixed-reality-future-180000716.html?src=rss",
          "content": "Today, during the XR edition of The Android Show, Google showed off a bunch of updates and new features headed to its mixed reality OS. And while most of the news was aimed at developers, I got a chance to demo some of the platform's expanded capabilities on a range of hardware including Samsung's Galaxy XR headset, two different reference designs and an early version of Xreal's Project Aura smart glasses and I came away rather impressed. So here's a rundown of what I saw and how it will impact the rapidly growing ecosystem of head-mounted displays.First up was one of Google's reference design smart glasses with a single waveguide RGB display built into its right lens. I've included a picture of it here, but try not to read too deeply into its design or aesthetics, as this device is meant to be a testbed for Android XR features and not an early look at upcoming models.Try not to read too much into the appearance of Google's reference design smart glasses, as they are explicitly labeled as prototypes meant to test upcoming features in Android XR.Sam Rutherford for EngadgetAfter putting them on, I was able to ask Gemini to play some tunes on YouTube Music before answering a call simply by tapping on the touchpad built into the right side of the frames. And because the reference model also had onboard world-facing cameras, I could easily share my view with the person on the other end of the line. Naturally, I was curious about how glasses had the bandwidth to do all this, because in normal use, they rely on a Bluetooth or Bluetooth LE connection. When asked, Max Spear, Group Product Manager for XR, shared that depending on the situation, the device can seamlessly switch between both Bluetooth and Wi-Fi, which was rather impressive because I couldn't even detect when that transition happened. Spear also noted that one of Google's focuses for Android XR is making it easier for developers to port over the apps people already know and love. This means for devices like the reference design I wore that feature a built-in display (or displays), the OS actually uses the same code meant for standard Android notifications (like quick replies) to create a minimalist UI instead of forcing app makers to update each piece of software to be compliant with an ever-increasing number of devices. Alternatively, for models that are super lightweight and rely strictly on speakers (like Bose Frames), Google has also designed Android XR so that you only need mics and voice controls to access a wide variety of apps without the need for visual menus. This is the picture Google's reference design smart glasses created (via Gemini ) when I asked it to transform a photo I took of some pantry shelves into a sci-fi kitchen. Sam Rutherford for EngadgetMeanwhile, if you're hoping to take photos with your smart glasses, there's a surprising amount of capability there, too. Not only was I able to ask Gemini to take a photo, the glasses were also able to send a higher-res version to a connected smartwatch, which is super handy in case you want to review the image before moving on to the next shot. And when you want to inject some creativity, you can ask Gemini to transform pictures into practically anything you can imagine via Nano Banana. In my case, I asked the AI to change a shot of a pantry into a sci-fi kitchen and Gemini delivered with aplomb, including converting the room into a metal-clad setting complete with lots of light strips and a few bursts of steam.However, one of the most impressive demos was when I asked Google's reference glasses to look at some of that same pantry environment and then use the ingredients to create a recipe based on my specifications (no tomatoes please, my wife isn't a fan). Gemini went down an Italian route by picking pasta, jarred banana peppers, bell peppers (which I thought was a somewhat unusual combination) and more, before launching into the first steps of the recipe. Sadly, I didn't have time to actually cook it, but as part of the demo, I learned that Gemini has been trained to understand human-centric gestures like pointing and picking things up. This allows it to better understand context without the need to be super specific, which is one of those little but very impactful tricks that allows AI to feel way less robotic. This is how Google Maps will look on Android XR. Note that this is the flat 2D version instead of the more detailed stereoscopic view available on smart glasses with dual displays. Sam Rutherford for EngadgetThen I had a chance to see how Uber and Google Maps ran on the reference glasses, this time using models with both single and dual RGB displays. Surprisingly, even on the monocular version, Maps was able to generate a detailed map with the ability to zoom in and out. But when I switched over to the binocular model, I noticed a significant jump in sharpness and clarity along with a higher-fidelity map with stereoscopic 3D images of buildings. Now, it may be a bit early to call this, and the perception of sharpness varies greatly between people based on their head shape and other factors, but after seeing that, I'm even more convinced that the smart glasses with dual RGB displays are what the industry will settle on in the long term.The second type of device I used was the Samsung Galaxy XR, which I originally tried out when it was announced back in October. However, in the short time since, Google has cooked up a few new features that really help expand the headset's capabilities. By using the goggle's exterior-facing cameras, I was able to play a game of I Spy with Gemini. Admittedly, this might sound like a small addition, but I think it's going to play a big part in how we use devices running Android XR, because it allows the headset (or glasses) to understand better what you're looking at in order to provide more helpful contextual responses. Even though it was announced not long ago in late October, Samsung's Galaxy XR headset is already getting some new features thanks to some updates coming to Android XR. Sam Rutherford for EngadgetHowever, the biggest surprise was when I joined a virtual call with someone using one of Google's new avatars, called Likeness. Instead of the low-polygon cartoony characters we've seen before in places like Meta Horizon, Google's virtual representations of people's faces are almost scary good. So good I had to double-check that they weren't real and from what I've seen they're even a step up from Apple's Personas. Google says that headsets like the Galaxy XR rely on interior sensors to track and respond to facial movements, while users will be able to create and edit their avatars using a standalone app due out sometime next year. The person in the bottom right is using a Likeness, which during my demo looked surprisingly responsive and realistic. GoogleNext, I got a chance to test out the Android XR's PC connectivity by playing Stray on the Galaxy XR while it was tethered wirelessly to a nearby laptop. Not only did it run almost flawlessly with low latency, I was also able to use a paired controller instead of relying on hand-tracking or the laptop's mouse and keyboard. This is something I've been eagerly waiting to try because it feels like Google has put a lot of work into making Android XR devices play nicely with other devices and OSes. Initially, you'll only be able to connect Windows PCs to the Galaxy XR, but Google says it's looking to support macOS systems as well.Finally, I got to try out Xreal's Project Aura glasses to see how Android XR works on a device primarily designed to give you big virtual displays in a portable form factor. Unfortunately, because this was a pre-production unit, I wasn't able to take photos. That said, as far as the glasses go, I was really impressed with their resolution and sharpness and the inclusion of electrochromic glass is a really nice touch, as it allows users to change how heavily the lenses are tinted with a single touch. Alternatively, the glasses can also adjust the tint automatically based on whatever app you are using to give you a more or less isolated atmosphere, depending on the situation. I also appreciate the Aura's increased 70-degree FOV, but if I'm nitpicking, I wish it were a bit higher, as I occasionally found myself wanting a bit more vertical display area. Unfortunately, I wasn't allowed to take photos of Xreal's Project Aura smart glasses, as the model I used was still an early pre-production unit. So here's a shot provided by Google instead. Google / XrealAs a device that's sort of between lightweight smart glasses and a full VR headset, the Aura relies on a wired battery pack that also doubles as a touchpad and a hub for plugging in external devices like your phone, laptop or even game consoles. While using the Aura, I was able to connect to a different PC and multitask in style, as the glasses were able to support multiple virtual displays while running several different apps at the same time. This allowed me to be on a virtual call with someone using a Likeness while I had two other virtual windows open on either side. I also played an AR game (Demeo) while I moved around in virtual space and used my hands to reposition the battlefield or pick up objects with my hands. Now I will fully admit this is a lot and it took me a bit to process everything. But upon reflection, I have a few takeaways from my time with the various Android XR devices and prototypes. More than any other headset or smart glasses platform out now, it feels like Google is doing a ton to embrace a growing ecosystem of devices. That's really important because we're still so early in the lifecycle for wearable gadgets with displays that no one has really figured out a truly polished design like we have for smartphones and laptops. And until we get there, this means that a highly adaptable OS will go a long way towards supporting OEMs like Samsung, Xreal and others. But that's not all. It's clear Google is focused on making Android XR devices easy to build for. That's because the company knows that without useful software that can highlight the components and features coming on next-gen spectacles, there's a chance that interest will remain rather niche — similar to what we've seen when looking at the adoption of VR headsets. So in a way, Google is waging a battle on two fronts, which makes navigating uncharted waters that much more difficult. A major focus for Android XR while people are still figuring out how to make smart glasses is to support a wide variety of designs including those with single displays, dual displays or models without any displays that rely on cameras and speakers. Sam Rutherford for EngadgetGoogle is putting a major emphasis on Android XR's ability to serve as a framework for future gadgets and support and address developer needs. This mirrors the approach the company takes with regular Android and the opposite of Apple's typical MO, because unlike the Vision Pro and visionOS, it appears Google is going to rely heavily on its partners like Xreal, Warby Parker, Gentle Monster and others to create engaging hardware. Furthermore, Google says it plans to support smart glasses that can be tethered to Android and iOS phones, as well as smartwatches from both ecosystems, though there will be some limitations for people using Apple devices due to inherent OS restrictions. That's not to say that there won't be Pixel glasses sometime down the road, but at least for now, I think that's a smart approach and possibly a lesson Google learned after releasing Google Glass over a decade ago. Meanwhile, hi-res and incredibly realistic avatars like Likenesses could be a turning point for virtual collaboration, because, in a first for me, talking to a digital representation of someone else felt kind of natural. After my demos, I had a chance to talk to Senior Director of Product Management for XR Juston Payne, who highlighted the difference between smart glasses and typical gadgets by saying \"Smart glasses have to be great glasses first. They need to have a good form factor, good lenses with prescription support, they need to look good and they have to be easy to buy.\"That's no simple task and there's no guarantee that next-gen smart glasses and headsets will be a grand slam. But from what I've seen, Google is building a very compelling foundation with Android XR.This article originally appeared on Engadget at https://www.engadget.com/wearables/heres-how-google-is-laying-the-foundation-for-our-mixed-reality-future-180000716.html?src=rss",
          "feed_position": 37,
          "image_url": "https://d29szjachogqwa.cloudfront.net/images/user-uploaded/google-reference-design-side-view.jpg"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/big-tech/paramount-makes-a-108-billion-hostile-takeover-bid-for-warner-bros-discovery-152248473.html",
          "published_at": "Mon, 08 Dec 2025 19:38:51 +0000",
          "title": "Paramount makes a $108 billion hostile takeover bid for Warner Bros. Discovery",
          "standfirst": "Paramount has been none too pleased about Netflix striking an $82.7 billion deal to buy much of Warner Bros. Discovery (WBD). Now, Paramount is making a hostile takeover bid for WBD. It's making its pitch directly to WBD shareholders with an all-cash offer of $30 per share that expires on January 8.Late last week, the WBD board unanimously accepted Netflix's offer of $27.75 per share. That breaks down to $23.25 per share in cash and another $4.50 per share in Netflix stock. Netflix's overall bid is valued at $82.7 billion, while Paramount's totals $108.4 billion. There's a key difference when it comes to the Paramount offer, as it’s for all of WBD. The latter is scheduled to split into two companies next year. Netflix only wants the Streaming and Studios side of WBD's business, which includes HBO Max and the Warner Bros. film, TV and game studios. Paramount is after the whole shebang, including WBD's cable channels (Global Networks). \"WBD's Board of Directors recommendation of the Netflix transaction over Paramount's offer is based on an illusory prospective valuation of Global Networks that is unsupported by the business fundamentals and encumbered by high levels of financial leverage assigned to the entity,\" Paramount said in a press release on Monday. As of the end of September, WBD was carrying $34.5 billion of gross debt. It planned to saddle the Global Networks company (aka Discovery Global) with most of that. The Paramount offer includes $40.7 billion in financing from the family of Paramount CEO David Ellison — his father is Oracle co-founder Larry Ellison — and RedBird Capital, but it would be taking on more debt to secure a deal for WBD. The bid includes \"$54 billion of debt commitments from Bank of America, Citi and Apollo.\" (Apollo owns a majority stake in Yahoo, Engadget's parent company). According to an SEC filing [PDF], other entities are backing the Paramount bid, including Jared Kushner’s investment firm Affinity Partners and the sovereign wealth funds of Saudi Arabia (the Public Investment Fund), Qatar and Abu Dhabi. Tencent was a financing partner in a previous Paramount offer, but it’s not involved with the hostile takeover attempt.In a letter sent to WBD CEO David Zazlav before the company accepted Netflix's offer, Paramount questioned the \"fairness and adequacy\" of the sale process. It asked whether WBD was acting in the best interest of shareholders after the management team allegedly appeared to favor the Netflix offer.\"Despite Paramount submitting six proposals over the course of 12 weeks, WBD never engaged meaningfully with these proposals which we believe deliver the best outcome for WBD shareholders,\" Paramount said. \"Paramount has now taken its offer directly to WBD shareholders and its Board of Directors to ensure they have the opportunity to pursue this clearly superior alternative.\"Paramount — which Skydance bought for $8 billion this year — also claims that its offer is likely to face less regulatory scrutiny than the Netflix offer, which wouldn't close until sometime after WBD splits in two later in 2026. According to CNBC, Paramount executives believe that the company's smaller size and cozy relationship with the Trump administration will help streamline the regulatory process. Over the weekend, President Donald Trump said that Netflix's bid for WBD has \"got to go through a process, and we’ll see what happens. But it is a big market share. It could be a problem.\"In a statement to Variety, WBD said it will consider Paramount’s latest bid and provide a recommendation to its stockholders within 10 business days — in other words, by December 19. The company said it “is not modifying its recommendation with respect to the agreement with Netflix” for the time being and it is asking shareholders “not to take any action at this time with respect to Paramount Skydance’s proposal.”Meanwhile, Netflix co-CEO Ted Sarandos said at an event on Monday that Paramount’s new offer was “entirely expected. We have a deal done, and we are incredibly happy with the deal. We think it’s great for our shareholders. It’s great for consumers. We think it’s a great way to create and protect jobs in the entertainment industry. We’re super confident we’re going to get it across the line and finish.”Update December 8, 2025, 11:14AM ET: Added details about the involvement of sovereign wealth funds and Affinity Partners.Update December 8, 2025, 2:38PM ET: Added the responses from WBD and Netflix.This article originally appeared on Engadget at https://www.engadget.com/big-tech/paramount-makes-a-108-billion-hostile-takeover-bid-for-warner-bros-discovery-152248473.html?src=rss",
          "content": "Paramount has been none too pleased about Netflix striking an $82.7 billion deal to buy much of Warner Bros. Discovery (WBD). Now, Paramount is making a hostile takeover bid for WBD. It's making its pitch directly to WBD shareholders with an all-cash offer of $30 per share that expires on January 8.Late last week, the WBD board unanimously accepted Netflix's offer of $27.75 per share. That breaks down to $23.25 per share in cash and another $4.50 per share in Netflix stock. Netflix's overall bid is valued at $82.7 billion, while Paramount's totals $108.4 billion. There's a key difference when it comes to the Paramount offer, as it’s for all of WBD. The latter is scheduled to split into two companies next year. Netflix only wants the Streaming and Studios side of WBD's business, which includes HBO Max and the Warner Bros. film, TV and game studios. Paramount is after the whole shebang, including WBD's cable channels (Global Networks). \"WBD's Board of Directors recommendation of the Netflix transaction over Paramount's offer is based on an illusory prospective valuation of Global Networks that is unsupported by the business fundamentals and encumbered by high levels of financial leverage assigned to the entity,\" Paramount said in a press release on Monday. As of the end of September, WBD was carrying $34.5 billion of gross debt. It planned to saddle the Global Networks company (aka Discovery Global) with most of that. The Paramount offer includes $40.7 billion in financing from the family of Paramount CEO David Ellison — his father is Oracle co-founder Larry Ellison — and RedBird Capital, but it would be taking on more debt to secure a deal for WBD. The bid includes \"$54 billion of debt commitments from Bank of America, Citi and Apollo.\" (Apollo owns a majority stake in Yahoo, Engadget's parent company). According to an SEC filing [PDF], other entities are backing the Paramount bid, including Jared Kushner’s investment firm Affinity Partners and the sovereign wealth funds of Saudi Arabia (the Public Investment Fund), Qatar and Abu Dhabi. Tencent was a financing partner in a previous Paramount offer, but it’s not involved with the hostile takeover attempt.In a letter sent to WBD CEO David Zazlav before the company accepted Netflix's offer, Paramount questioned the \"fairness and adequacy\" of the sale process. It asked whether WBD was acting in the best interest of shareholders after the management team allegedly appeared to favor the Netflix offer.\"Despite Paramount submitting six proposals over the course of 12 weeks, WBD never engaged meaningfully with these proposals which we believe deliver the best outcome for WBD shareholders,\" Paramount said. \"Paramount has now taken its offer directly to WBD shareholders and its Board of Directors to ensure they have the opportunity to pursue this clearly superior alternative.\"Paramount — which Skydance bought for $8 billion this year — also claims that its offer is likely to face less regulatory scrutiny than the Netflix offer, which wouldn't close until sometime after WBD splits in two later in 2026. According to CNBC, Paramount executives believe that the company's smaller size and cozy relationship with the Trump administration will help streamline the regulatory process. Over the weekend, President Donald Trump said that Netflix's bid for WBD has \"got to go through a process, and we’ll see what happens. But it is a big market share. It could be a problem.\"In a statement to Variety, WBD said it will consider Paramount’s latest bid and provide a recommendation to its stockholders within 10 business days — in other words, by December 19. The company said it “is not modifying its recommendation with respect to the agreement with Netflix” for the time being and it is asking shareholders “not to take any action at this time with respect to Paramount Skydance’s proposal.”Meanwhile, Netflix co-CEO Ted Sarandos said at an event on Monday that Paramount’s new offer was “entirely expected. We have a deal done, and we are incredibly happy with the deal. We think it’s great for our shareholders. It’s great for consumers. We think it’s a great way to create and protect jobs in the entertainment industry. We’re super confident we’re going to get it across the line and finish.”Update December 8, 2025, 11:14AM ET: Added details about the involvement of sovereign wealth funds and Affinity Partners.Update December 8, 2025, 2:38PM ET: Added the responses from WBD and Netflix.This article originally appeared on Engadget at https://www.engadget.com/big-tech/paramount-makes-a-108-billion-hostile-takeover-bid-for-warner-bros-discovery-152248473.html?src=rss",
          "feed_position": 39
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/anthropics-claude-code-can-now-read-your-slack-messages-and-write-code-for",
          "published_at": "Mon, 08 Dec 2025 19:00:00 GMT",
          "title": "Anthropic's Claude Code can now read your Slack messages and write code for you",
          "standfirst": "Anthropic has launched a beta integration that connects its fast-growing Claude Code programming agent directly into Slack, allowing software engineers to delegate coding tasks without leaving the workplace messaging platform where much of their daily communication already happens.The release, which Anthropic describes as a \"research preview,\" is the company&#x27;s latest move to embed its technology deeper into enterprise workflows — and comes as Claude Code has emerged as a surprise revenue engine, generating more than $1 billion in annualized revenue just six months after its public debut.\"The critical context around engineering work often lives in Slack, including bug reports, feature requests and engineering discussions,\" the company wrote in a blog post. \"When a bug report appears or a teammate needs a code fix, you can now tag Claude in Slack to automatically spin up a Claude Code session using the surrounding context.\"From bug report to pull request: How the new Slack integration actually worksThe mechanics are deceptively simple but address a persistent friction point in software development: The gap between where problems are discussed and where they are fixed.When a user mentions @Claude in a Slack channel or thread, Claude analyzes the message to determine whether it constitutes a coding task. If it does, the system automatically creates a new Claude Code session. Users can also explicitly instruct Claude to treat requests as coding tasks.Claude gathers context from recent Slack channel and thread messages to feed into the Claude Code session. It will use this context to automatically choose which repository to run the task on based on the repositories that have been authenticated to Claude Code on the web.As the Claude Code session progresses, Claude posts status updates back to the Slack thread. Once complete, users receive a link to the full session where they can review changes, along with a direct link to open a pull request.The feature builds on Anthropic&#x27;s existing Claude for Slack integration and requires users to have access to Claude Code on the web. In practical terms, a product manager reporting a bug in Slack could tag Claude, which would then analyze the conversation context, identify the relevant code repository, investigate the issue, propose a fix and post a pull request — all while updating the original Slack thread with its progress.Why Anthropic is betting big on enterprise workflow integrationsThe Slack integration arrives at a pivotal moment for Anthropic. Claude Code has already hit $1 billion in revenue, six months after its public debut, according to a LinkedIn post from Anthropic&#x27;s CPO Mike Krieger. The coding agent continues to barrel toward scale with customers like Netflix, Spotify and Salesforce.The velocity of that growth helps explain why Anthropic made its first-ever acquisition earlier this month, of developer tool startup Bun. Anthropic declined to comment on specific financial details. Bun is a breakthrough JavaScript runtime that claims to be dramatically faster than the leading competition. As an all-in-one toolkit — combining runtime, package manager, bundler and test runner — it&#x27;s become essential infrastructure for AI-led software engineering, helping developers build and test applications at unprecedented velocity.Since becoming generally available in May 2025, Claude Code has grown from its origins as an internal engineering experiment into a critical tool for many of the world&#x27;s category-leading enterprises, including Netflix, Spotify, KPMG, L&#x27;Oreal and Salesforce — and Bun has been key in helping scale its infrastructure throughout that evolution.The acquisition signals that Anthropic views Claude Code not as a peripheral feature but as a core business line worth substantial investment. The Slack integration extends that bet, positioning Claude Code as an ambient presence in the workspaces where engineering decisions actually get made.According to an Anthropic spokesperson, companies including Rakuten, Novo Nordisk, Uber, Snowflake and Ramp now use Claude Code for both professional and novice developers. Rakuten, the Japanese e-commerce giant, has reportedly reduced software development timelines from 24 to 5 days using the tool — a 79% reduction that illustrates the productivity claims Anthropic has been making.Claude Code&#x27;s rapid rise from internal experiment to billion-dollar productThe Slack launch is the latest in a rapid series of Claude Code expansions. In late November, Claude Code was added to Anthropic&#x27;s desktop apps, including the Mac version. Previously, Claude Code was limited to mobile apps and the web. The desktop version allows software engineers to code, research and update work with multiple local and remote sessions running at the same time.That release accompanied Anthropic&#x27;s unveiling of Claude Opus 4.5, its newest and most capable model. Claude Opus 4.5 is available today on the company&#x27;s apps, API and on all three major cloud platforms. Pricing is $5/$25 per million tokens — making Opus-level capabilities accessible to even more users, teams and enterprises.The company has also invested heavily in the developer infrastructure that powers Claude Code. In late November, Anthropic released three new beta features for tool use: Tool Search Tool, which allows Claude to access thousands of tools without consuming its context window; Programmatic Tool Calling, which allows Claude to invoke tools in a code execution environment, thus reducing the impact on the model&#x27;s context window; and Tool Use Examples, which provides a universal standard for demonstrating how to effectively use a given tool.The Model Context Protocol (MCP) is an open standard for connecting AI agents to external systems. Connecting agents to tools and data traditionally requires a custom integration for each pairing, creating fragmentation and duplicated effort that makes it difficult to scale truly connected systems. MCP provides a universal protocol — developers implement MCP once in their agent, and it unlocks an entire ecosystem of integrations.Inside Anthropic&#x27;s own AI transformation: What happens when engineers use Claude all dayAnthropic has been unusually transparent about how its own engineers use Claude Code — and the findings offer a preview of broader workforce implications. In August 2025, Anthropic surveyed 132 engineers and researchers, conducted 53 in-depth qualitative interviews and studied internal Claude Code usage data to understand how AI use is changing work at the company.Employees self-reported using Claude in 60% of their work and achieved a 50% productivity boost, a 2-3x increase from this time last year. This productivity looks like slightly less time per task category, but considerably more output volume.Perhaps most notably, 27% of Claude-assisted work consists of tasks that wouldn&#x27;t have been done otherwise, such as scaling projects, making nice-to-have tools like interactive data dashboards, and exploratory work that wouldn&#x27;t be cost-effective if done manually.The internal research also revealed how Claude is changing the nature of engineering collaboration. The maximum number of consecutive tool calls Claude Code makes per transcript increased by 116%. Claude now chains together 21.2 independent tool calls without the need for human intervention, versus 9.8 tool calls from six months ago.The number of human turns decreased by 33%. The average number of human turns decreased from 6.2 to 4.1 per transcript, suggesting that less human input is necessary to accomplish a given task now, compared to six months ago.But the research also surfaced tensions. One prominent theme was that Claude has become the first stop for questions that once went to colleagues. \"It has reduced my dependence on [my team] by 80%, [but] the last 20% is crucial, and I go and talk to them,\" one engineer explained. Several engineers said they \"bounce ideas off\" Claude, similar to interactions with human collaborators.Some appreciate the reduced social friction, but others resist the change or miss the older way of working: \"I like working with people, and it is sad that I need them less now.\"How Anthropic stacks up against OpenAI, Google and Microsoft in the enterprise AI raceAnthropic is not alone in racing to capture the enterprise coding market. OpenAI, Google and Microsoft (through GitHub Copilot) are all pursuing similar integrations. The Slack launch gives Anthropic a presence in one of the most widely used enterprise communication platforms — Slack claims over 750,000 organizations use its software.The deal comes as Anthropic pursues a more disciplined growth path than rival OpenAI, focusing on enterprise customers and coding workloads. Internal financials reported by The Wall Street Journal show that Anthropic expects to break even by 2028 — two years earlier than OpenAI, which continues to invest heavily in infrastructure as it expands into video, hardware and consumer products.The move also marks an increased push into developer tooling. Anthropic has recently seen backing from some of tech&#x27;s biggest titans. Microsoft and Nvidia pledged up to $15 billion in fresh investment in Anthropic last month, alongside a $30 billion commitment from Anthropic to run Claude Code on Microsoft&#x27;s cloud. This is in addition to the $8 billion invested by Amazon and $3 billion by Google.The cross-investment from both Microsoft and Google — fierce competitors in the cloud and AI spaces — highlights Anthropic&#x27;s valuable enterprise positioning. By integrating with Slack (which is owned by Salesforce), Anthropic further embeds itself in the enterprise software ecosystem while remaining platform-agnostic.What the Slack integration means for developers — and whether they can trust itFor engineering teams, the Slack integration promises to collapse the distance between problem identification and resolution. A bug report in a Slack channel can immediately trigger an investigation. A feature request can spawn a prototype. A code review comment can generate a refactor.But the integration also raises questions about oversight and code quality. Most Anthropic employees use Claude frequently while reporting they can \"fully delegate\" only 0 to 20% of their work to it. Claude is a constant collaborator, but using it generally involves active supervision and validation, especially in high-stakes work, versus handing off tasks requiring no verification at all.Some employees are concerned about the atrophy of deeper skillsets required for both writing and critiquing code — \"When producing output is so easy and fast, it gets harder and harder to actually take the time to learn something.\"The Slack integration, by making Claude Code invocation as simple as an @mention, may accelerate both the productivity benefits and the skill-atrophy concerns that Anthropic&#x27;s own research has documented.The future of coding may be conversational—and Anthropic is racing to prove itThe beta launch marks the beginning of what Anthropic expects will be a broader rollout, with documentation forthcoming for teams looking to deploy the integration and refinements planned based on user feedback during the research preview phase.For Anthropic, the Slack integration is a calculated bet on a fundamental shift in how software gets written. The company is wagering that the future of coding will be conversational — the walls between where developers talk about problems and where they solve them will dissolve entirely. The companies that win enterprise AI, in this view, will be the ones that meet developers not in specialized tools but in the chat windows they already have open all day.Whether that vision becomes reality will depend on whether Claude Code can deliver enterprise-grade reliability while maintaining the security that organizations demand. The early returns are promising: A billion dollars in revenue, a roster of Fortune 500 customers and a growing ecosystem of integrations suggest Anthropic is onto something real.But in one of Anthropic&#x27;s own internal interviews, an engineer offered a more cautious assessment of the transformation underway: \"Nobody knows what&#x27;s going to happen… the important thing is to just be really adaptable.\"In the age of AI coding agents, that may be the only career advice that holds up.",
          "content": "Anthropic has launched a beta integration that connects its fast-growing Claude Code programming agent directly into Slack, allowing software engineers to delegate coding tasks without leaving the workplace messaging platform where much of their daily communication already happens.The release, which Anthropic describes as a \"research preview,\" is the company&#x27;s latest move to embed its technology deeper into enterprise workflows — and comes as Claude Code has emerged as a surprise revenue engine, generating more than $1 billion in annualized revenue just six months after its public debut.\"The critical context around engineering work often lives in Slack, including bug reports, feature requests and engineering discussions,\" the company wrote in a blog post. \"When a bug report appears or a teammate needs a code fix, you can now tag Claude in Slack to automatically spin up a Claude Code session using the surrounding context.\"From bug report to pull request: How the new Slack integration actually worksThe mechanics are deceptively simple but address a persistent friction point in software development: The gap between where problems are discussed and where they are fixed.When a user mentions @Claude in a Slack channel or thread, Claude analyzes the message to determine whether it constitutes a coding task. If it does, the system automatically creates a new Claude Code session. Users can also explicitly instruct Claude to treat requests as coding tasks.Claude gathers context from recent Slack channel and thread messages to feed into the Claude Code session. It will use this context to automatically choose which repository to run the task on based on the repositories that have been authenticated to Claude Code on the web.As the Claude Code session progresses, Claude posts status updates back to the Slack thread. Once complete, users receive a link to the full session where they can review changes, along with a direct link to open a pull request.The feature builds on Anthropic&#x27;s existing Claude for Slack integration and requires users to have access to Claude Code on the web. In practical terms, a product manager reporting a bug in Slack could tag Claude, which would then analyze the conversation context, identify the relevant code repository, investigate the issue, propose a fix and post a pull request — all while updating the original Slack thread with its progress.Why Anthropic is betting big on enterprise workflow integrationsThe Slack integration arrives at a pivotal moment for Anthropic. Claude Code has already hit $1 billion in revenue, six months after its public debut, according to a LinkedIn post from Anthropic&#x27;s CPO Mike Krieger. The coding agent continues to barrel toward scale with customers like Netflix, Spotify and Salesforce.The velocity of that growth helps explain why Anthropic made its first-ever acquisition earlier this month, of developer tool startup Bun. Anthropic declined to comment on specific financial details. Bun is a breakthrough JavaScript runtime that claims to be dramatically faster than the leading competition. As an all-in-one toolkit — combining runtime, package manager, bundler and test runner — it&#x27;s become essential infrastructure for AI-led software engineering, helping developers build and test applications at unprecedented velocity.Since becoming generally available in May 2025, Claude Code has grown from its origins as an internal engineering experiment into a critical tool for many of the world&#x27;s category-leading enterprises, including Netflix, Spotify, KPMG, L&#x27;Oreal and Salesforce — and Bun has been key in helping scale its infrastructure throughout that evolution.The acquisition signals that Anthropic views Claude Code not as a peripheral feature but as a core business line worth substantial investment. The Slack integration extends that bet, positioning Claude Code as an ambient presence in the workspaces where engineering decisions actually get made.According to an Anthropic spokesperson, companies including Rakuten, Novo Nordisk, Uber, Snowflake and Ramp now use Claude Code for both professional and novice developers. Rakuten, the Japanese e-commerce giant, has reportedly reduced software development timelines from 24 to 5 days using the tool — a 79% reduction that illustrates the productivity claims Anthropic has been making.Claude Code&#x27;s rapid rise from internal experiment to billion-dollar productThe Slack launch is the latest in a rapid series of Claude Code expansions. In late November, Claude Code was added to Anthropic&#x27;s desktop apps, including the Mac version. Previously, Claude Code was limited to mobile apps and the web. The desktop version allows software engineers to code, research and update work with multiple local and remote sessions running at the same time.That release accompanied Anthropic&#x27;s unveiling of Claude Opus 4.5, its newest and most capable model. Claude Opus 4.5 is available today on the company&#x27;s apps, API and on all three major cloud platforms. Pricing is $5/$25 per million tokens — making Opus-level capabilities accessible to even more users, teams and enterprises.The company has also invested heavily in the developer infrastructure that powers Claude Code. In late November, Anthropic released three new beta features for tool use: Tool Search Tool, which allows Claude to access thousands of tools without consuming its context window; Programmatic Tool Calling, which allows Claude to invoke tools in a code execution environment, thus reducing the impact on the model&#x27;s context window; and Tool Use Examples, which provides a universal standard for demonstrating how to effectively use a given tool.The Model Context Protocol (MCP) is an open standard for connecting AI agents to external systems. Connecting agents to tools and data traditionally requires a custom integration for each pairing, creating fragmentation and duplicated effort that makes it difficult to scale truly connected systems. MCP provides a universal protocol — developers implement MCP once in their agent, and it unlocks an entire ecosystem of integrations.Inside Anthropic&#x27;s own AI transformation: What happens when engineers use Claude all dayAnthropic has been unusually transparent about how its own engineers use Claude Code — and the findings offer a preview of broader workforce implications. In August 2025, Anthropic surveyed 132 engineers and researchers, conducted 53 in-depth qualitative interviews and studied internal Claude Code usage data to understand how AI use is changing work at the company.Employees self-reported using Claude in 60% of their work and achieved a 50% productivity boost, a 2-3x increase from this time last year. This productivity looks like slightly less time per task category, but considerably more output volume.Perhaps most notably, 27% of Claude-assisted work consists of tasks that wouldn&#x27;t have been done otherwise, such as scaling projects, making nice-to-have tools like interactive data dashboards, and exploratory work that wouldn&#x27;t be cost-effective if done manually.The internal research also revealed how Claude is changing the nature of engineering collaboration. The maximum number of consecutive tool calls Claude Code makes per transcript increased by 116%. Claude now chains together 21.2 independent tool calls without the need for human intervention, versus 9.8 tool calls from six months ago.The number of human turns decreased by 33%. The average number of human turns decreased from 6.2 to 4.1 per transcript, suggesting that less human input is necessary to accomplish a given task now, compared to six months ago.But the research also surfaced tensions. One prominent theme was that Claude has become the first stop for questions that once went to colleagues. \"It has reduced my dependence on [my team] by 80%, [but] the last 20% is crucial, and I go and talk to them,\" one engineer explained. Several engineers said they \"bounce ideas off\" Claude, similar to interactions with human collaborators.Some appreciate the reduced social friction, but others resist the change or miss the older way of working: \"I like working with people, and it is sad that I need them less now.\"How Anthropic stacks up against OpenAI, Google and Microsoft in the enterprise AI raceAnthropic is not alone in racing to capture the enterprise coding market. OpenAI, Google and Microsoft (through GitHub Copilot) are all pursuing similar integrations. The Slack launch gives Anthropic a presence in one of the most widely used enterprise communication platforms — Slack claims over 750,000 organizations use its software.The deal comes as Anthropic pursues a more disciplined growth path than rival OpenAI, focusing on enterprise customers and coding workloads. Internal financials reported by The Wall Street Journal show that Anthropic expects to break even by 2028 — two years earlier than OpenAI, which continues to invest heavily in infrastructure as it expands into video, hardware and consumer products.The move also marks an increased push into developer tooling. Anthropic has recently seen backing from some of tech&#x27;s biggest titans. Microsoft and Nvidia pledged up to $15 billion in fresh investment in Anthropic last month, alongside a $30 billion commitment from Anthropic to run Claude Code on Microsoft&#x27;s cloud. This is in addition to the $8 billion invested by Amazon and $3 billion by Google.The cross-investment from both Microsoft and Google — fierce competitors in the cloud and AI spaces — highlights Anthropic&#x27;s valuable enterprise positioning. By integrating with Slack (which is owned by Salesforce), Anthropic further embeds itself in the enterprise software ecosystem while remaining platform-agnostic.What the Slack integration means for developers — and whether they can trust itFor engineering teams, the Slack integration promises to collapse the distance between problem identification and resolution. A bug report in a Slack channel can immediately trigger an investigation. A feature request can spawn a prototype. A code review comment can generate a refactor.But the integration also raises questions about oversight and code quality. Most Anthropic employees use Claude frequently while reporting they can \"fully delegate\" only 0 to 20% of their work to it. Claude is a constant collaborator, but using it generally involves active supervision and validation, especially in high-stakes work, versus handing off tasks requiring no verification at all.Some employees are concerned about the atrophy of deeper skillsets required for both writing and critiquing code — \"When producing output is so easy and fast, it gets harder and harder to actually take the time to learn something.\"The Slack integration, by making Claude Code invocation as simple as an @mention, may accelerate both the productivity benefits and the skill-atrophy concerns that Anthropic&#x27;s own research has documented.The future of coding may be conversational—and Anthropic is racing to prove itThe beta launch marks the beginning of what Anthropic expects will be a broader rollout, with documentation forthcoming for teams looking to deploy the integration and refinements planned based on user feedback during the research preview phase.For Anthropic, the Slack integration is a calculated bet on a fundamental shift in how software gets written. The company is wagering that the future of coding will be conversational — the walls between where developers talk about problems and where they solve them will dissolve entirely. The companies that win enterprise AI, in this view, will be the ones that meet developers not in specialized tools but in the chat windows they already have open all day.Whether that vision becomes reality will depend on whether Claude Code can deliver enterprise-grade reliability while maintaining the security that organizations demand. The early returns are promising: A billion dollars in revenue, a roster of Fortune 500 customers and a growing ecosystem of integrations suggest Anthropic is onto something real.But in one of Anthropic&#x27;s own internal interviews, an engineer offered a more cautious assessment of the transformation underway: \"Nobody knows what&#x27;s going to happen… the important thing is to just be really adaptable.\"In the age of AI coding agents, that may be the only career advice that holds up.",
          "feed_position": 6,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/667OUCJyzh5TAzDpX4UQDa/d1b772df47ef4e01e6450e1bb9979970/nuneybits_Vector_art_of_code-filled_speech_bubble_in_burnt_oran_78f6bff7-7863-4363-bcad-892c8f7cf2f7.webp?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/techs-biggest-winners-of-2025-180000177.html",
          "published_at": "Mon, 08 Dec 2025 18:00:00 +0000",
          "title": "Tech's biggest winners of 2025",
          "standfirst": "Every December, the Engadget staff compiles a list of the year’s biggest winners. We scour over articles from the previous 12 months to determine the people, companies, products and trends that made the most impact over the course of the year. Not all of that influence is positive, however, and some selections may also appear on our list of biggest losers. Still, sit back and enjoy our picks for the biggest winners of 2025.Nintendo Switch 2Playing Mario Kart World on the Switch 2 in handheld mode.Sam Rutherford for EngadgetAside from a big bump in battery life that many were hoping for, Nintendo took just about everything that made its last console such a phenomenon and upgraded it on the Switch 2. A sleeker design with magnetic Joy-Cons that are less likely to break, a larger (albeit LCD) 1080p display with HDR, much stronger performance, mouse controls and a boost to the base storage were all very welcome.Of course, the vast majority of Switch games run on the Switch 2 (often with visual improvements or other upgrades), so the new console had a vast library right from the jump. Nintendo is building out its slate of first-party games with treats like Donkey Kong Bananza and Metroid Prime 4, and the third-party support is seriously impressive too. Cyberpunk 2077, Street Fighter 6 and Hitman: World of Assassination are already available, and the likes of Final Fantasy VII Remake Intergrade and FromSoftware's Switch 2 exclusive The Duskbloods are on the way.The Switch 2 is an iteration, not a revolution, but Nintendo didn't need to reinvent the wheel to make another great system. It's little surprise, then, that we gave the Switch 2 a score of 93 in our review. The console is surpassing Nintendo's sales expectations as well. The company said in November that it believes it will sell 19 million units (up from 15 million) by the time its current fiscal year ends in March. — Kris Holt, Contributing reporterNVIDIANVIDIA GeForce 5070 TiDevindra Hardawar for EngadgetCould things be any rosier for NVIDIA? Once just a video card company for gamers, NVIDIA's GPU hardware is now directly tied to the rise of the AI industry. Its stock has jumped a whopping 1,235 percent over the past five years, going from $13.56 per share in 2020 to a peak of $202.49 this past October. NVIDIA's server-grade cards are being used en masse to train AI models, as well as to power AI inferencing. At home, its GeForce GPUs are enabling local AI development and they're still the gaming cards to beat, despite AMD's steadily improving competition.Clearly, the company's bet on parallel processing has paid off enormously. Its GPUs can handle tons of computations simultaneously, making them ideally suited for the demands of the AI industry. They're not exactly efficient — that's why neural processing units, or NPUs have sprung up to power consumer AI features — but it's hard to deny NVIDIA's raw computational power. NVIDIA's AI success may not last forever, though. Companies like Google and Microsoft are already working on their own AI chips, and it's still unclear if consumers actually want widespread AI features as much as tech companies think. If the AI industry crashes, NVIDIA will be one of the first victims. — Devindra Hardawar, Senior reporterTech billionairesUS President Donald Trump speaks during a news conference with Elon Musk (L) in the Oval Office of the White House in Washington, DC, on May 30, 2025. ALLISON ROBBERT via Getty ImagesThere's no doubt that tech billionaires, especially those that lean conservatively, have benefitted tremendously from the Trump administration over the past year. Elon Musk's DOGE team of tech loyalists chainsawed their way through the budgets and staff of several federal agencies, including the National Highway Traffic Safety Administration (NHTSA), which regulates Tesla. (That hasn't stopped the NHTSA from launching a new investigation into Tesla's full self-driving tech, though.)According to a recent report by Oxfam, the 10 richest US billionaires (who are all tech leaders, save for Warren Buffet) increased their wealth by $698 billion of the past year. Of course, it took plenty of wining and dining to get there. Elon Musk reportedly donated nearly $300 million to Trump and Republican allies, and several tech companies have pitched in to build the president's lavish White House ballroom. But the result for the tech elite is increased access to the president, less scrutiny when it comes to acquisitions and other deals, and the potential for massive corporate and elite tax cuts. — D.H.AI videoA silhouetted individual is seen holding a mobile phone with a Sora of ChatGPT OpenAI logo displayed in the backgroundSOPA Images via Getty ImagesAI slop didn't start in 2025, but it reached new heights thanks to updates from Meta, Google, OpenAI and others that made it easier than ever to create a real-ish (emphasis on the ish) looking clips from nothing but your most unhinged mad libs. Now, AI-generated videos are just about impossible to avoid. Some platforms, like Pinterest and TikTok, have even begun offering people the ability to ask their algorithms to show less AI content in their feeds. Unfortunately, there's no way to stuff Shrimp Jesus back into the bottle. AI video is everywhere and it's here to stay. It's not only overtaken Facebook and Instagram's recommendations, Meta created an entirely separate feed just for users' AI-generated fever dreams. OpenAI's Sora, which lets you make AI videos of real people, was downloaded a million times in just a few days. Google's Veo, which generated more than 40 million videos in a matter of weeks, is now built-in to YouTube Shorts.It's now trivially easy for creators to churn out fake movie trailers, cute animal videos that never happened or viral clips of made up ICE raids. Hell, the president of the United States regularly shares bizarre, sometimes poop-themed, AI videos on his official social media channels. During the government shutdown, the official X account for Senate Republicans shared a deepfake of Senate minority leader Chuck Schumer. AI video is winning not just because it's everywhere, but because so many are unable, or unwilling, to understand what's real and what isn't. More than half of Americans say they are not confident in their ability to distinguish between human and AI-generated content, according to Pew Research. Similar numbers of people report being \"more concerned than excited about the increased use of AI in daily life.\" But those concerns have done little to stop AI slop from dominating all of our feeds, and there's no sign it will ever slow down. — Karissa Bell, Senior reporterGalaxy Z Fold 7Samsung Galaxy Z Fold 7Sam Rutherford for EngadgetAfter seven generations, Samsung reached an important milestone this year with its Galaxy Z Fold line: It made a foldable phone that’s the same size as a regular handset. In fact, weighing 7.58 ounces and measuring 72.8mm wide, the Galaxy Z Fold 7 is actually lighter and narrower than an S25 Ultra, while being practically just as thin at 8.9mm (folded). It’s a real marvel of engineering, especially when you consider the phone also features a 200MP main camera, an IPX8 rating for water resistance and a 5,000 mAh battery with 45-watt wired charging. And of course, there's that huge 8-inch main screen hiding inside, which makes the Z Fold 7 both a phone and a tablet in one device. The only thing it's really missing is the improved dust resistance Google gave to the Pixel 10 Pro Fold. But perhaps more importantly, the Z Fold 7's reduced size and weight have created a device with wider appeal. This has propelled sales of Samsung's latest flagship foldable up 50 percent compared to the previous generation while pushing shipments of foldables as a whole to record highs. Who knew that when Samsung focuses on creating world-class hardware instead of overindexing on AI, good things happen? Okay, maybe that’s a bit harsh. Regardless, for a phone category that has struggled with excess weight and bulk since its inception, the Z Fold 7 feels like a revelation and the beginning of a new era for handsets with flexible displays. Now, can we just bring their prices down, please? — Sam Rutherford, Senior reporterSmart glassesSenior reporter Karissa Bell wearing a pair of Ray Ban Display glasses. Karissa Bell for EngadgetLike it or not, smart glasses are having a moment. Propelled by new devices like the Meta Ray-Ban Display and upcoming models like Xreal’s Project Aura, the idea of wearing specs with built-in screens suddenly became an attractive proposition. And that means a lot for a category of gadgets that’s often best remembered by the fashion tragedy that was Google Glass in 2013. However, this development isn’t purely by chance. The latest generation of smart glasses has only just now become a reality due to the convergence of several branches of tech — including improved optics, lightweight batteries and, of course, AI. Now that last one might sound silly considering how many big companies seem to be betting the farm on machine learning being the next big thing, but AI will be a critical feature for enabling the hands-free experience that you need to make smartglasses work when you can’t rely on touch input. While this category is still in its early stages of development, the increased momentum we've seen from smart glasses this year seems poised to carry them towards being a future pillar of people's core tech kits. — S.R.Fast chargingFast charging on the Pixel Watch 4 is one implementation that impressed us this year.Cherlynn Low for EngadgetDevices like tablets and smartwatches have matured to the point where each generation mostly sees iterative upgrades, making covering them seem boring. But this year, as the hardware review season came to a close, I noticed an interesting trend. One feature, across various product categories, genuinely excited myself and other reviewers at Engadget and around the internet: impressively fast charging. By itself, high-speed charging isn’t new. But when I reviewed the Pixel Watch 4 in October, I was shocked that one seemingly little update changed how I went about my day. The new power system on Google’s smartwatch was so efficient that after about ten minutes on a cradle, the wearable went from below 20 percent to past 50 percent. With that boost, I stopped having to remind myself to plug the watch in — any time I ran low or was about to run out the door, I just plopped it on the charger and would have enough juice for hours.Google wasn’t the only company to make fast-charging a meaningful addition to one of its 2025 products. Apple’s iPad Pro M5 is the first iPad to support the feature, and while in our testing it fell a little short of the 50 percent charge in 30 minutes that the company promised, our reviewer Nate Ingraham still found it a meaningful improvement.Observers of the smartphone industry will likely point out two things. First, battery technology can be volatile, and larger, faster-charging cells might lead to exploding phones. So my optimism about this development is not without caution. Secondly, we’ve already seen all this come to handsets, especially in phones that launched outside the US first. OnePlus is known for its SUPERVOOC fast charging system, for example, and we’re seeing even more novel battery tech show up abroad. Calling fast charging a winner of 2025 may feel untimely to some.But when you consider the spread of speedier charging to other types of products, especially in electric vehicles that till now take forever to top up, the benefits are clear. This year, we saw Formula E (finally) debut its fast-charging pit stops, Honda announce its first full-size electric motorcycle with fast charging and Chinese EV maker BYD unveiling new tech that delivers peak EV charging speeds of 1,000 kilowatts. That should about halve the time it currently takes to top up your electric car. Sure, it’s not the most eye-catching or novel technological development. But when counted in terms of precious time saved, fast charging coming to more types of devices certainly amounts to a greater good in gadgets in 2025. — Cherlynn Low, Managing editorMagnetsThe Pixel 10 Pro Fold and the Pixel Ring StandSam Rutherford for EngadgetTwo years after the announcement of the Qi 2 wireless charging standard and its support of magnetic attachment accessories (a la Apple’s MagSafe), we’re finally seeing one of the more mainstream Android devices adopt it. In 2025, Google became the first Android phone maker that’s not HMD to do so, bringing such magnetic capabilities to the Pixel 10 series. It also introduced Pixelsnap — its own version of a MagSafe accessory ecosystem, including a slim puck with a fold-out kickstand that you can snap onto a phone. I love the Pixel Ring Stand and make sure to bring it with me whenever I can. It works perfectly with my iPhone 17 Pro, and has a compact footprint that makes it easy to take anywhere. Of course, it’s not the first of its kind — Case-Mate and PopSocket, among others, already make similar products but they’re either pricier or rated poorly. But it’s not just Google that made a magnetic accessory I unexpectedly adored. When reports of Apple’s Crossbody Strap first trickled out, I was underwhelmed. Who cares about a crossbody strap for an iPhone? But when I was presented with one to try at the iPhone 17 launch event, my cynicism quickly melted into desire. Setting aside the convenience of having your phone on your person when you don’t have pockets or a purse, the way magnets play a part here also won me over. To adjust the length of the straps, you just separate the two overlapping pieces that stick together magnetically, move them along each other till you’re satisfied with the length and let them snap back in place. I’m sure Apple isn’t the first to make a crossbody strap accessory for iPhones, nor is it the first to use magnets to adjust such straps. But like many Redditors, I’ve slowly come to realize the differences between those products and the Crossbody Strap for iPhone 17. It’s far from perfect, but in 2025 it was another implementation of magnets in tech that caught my attention and brought convenience to my life. — C.L.This article originally appeared on Engadget at https://www.engadget.com/techs-biggest-winners-of-2025-180000177.html?src=rss",
          "content": "Every December, the Engadget staff compiles a list of the year’s biggest winners. We scour over articles from the previous 12 months to determine the people, companies, products and trends that made the most impact over the course of the year. Not all of that influence is positive, however, and some selections may also appear on our list of biggest losers. Still, sit back and enjoy our picks for the biggest winners of 2025.Nintendo Switch 2Playing Mario Kart World on the Switch 2 in handheld mode.Sam Rutherford for EngadgetAside from a big bump in battery life that many were hoping for, Nintendo took just about everything that made its last console such a phenomenon and upgraded it on the Switch 2. A sleeker design with magnetic Joy-Cons that are less likely to break, a larger (albeit LCD) 1080p display with HDR, much stronger performance, mouse controls and a boost to the base storage were all very welcome.Of course, the vast majority of Switch games run on the Switch 2 (often with visual improvements or other upgrades), so the new console had a vast library right from the jump. Nintendo is building out its slate of first-party games with treats like Donkey Kong Bananza and Metroid Prime 4, and the third-party support is seriously impressive too. Cyberpunk 2077, Street Fighter 6 and Hitman: World of Assassination are already available, and the likes of Final Fantasy VII Remake Intergrade and FromSoftware's Switch 2 exclusive The Duskbloods are on the way.The Switch 2 is an iteration, not a revolution, but Nintendo didn't need to reinvent the wheel to make another great system. It's little surprise, then, that we gave the Switch 2 a score of 93 in our review. The console is surpassing Nintendo's sales expectations as well. The company said in November that it believes it will sell 19 million units (up from 15 million) by the time its current fiscal year ends in March. — Kris Holt, Contributing reporterNVIDIANVIDIA GeForce 5070 TiDevindra Hardawar for EngadgetCould things be any rosier for NVIDIA? Once just a video card company for gamers, NVIDIA's GPU hardware is now directly tied to the rise of the AI industry. Its stock has jumped a whopping 1,235 percent over the past five years, going from $13.56 per share in 2020 to a peak of $202.49 this past October. NVIDIA's server-grade cards are being used en masse to train AI models, as well as to power AI inferencing. At home, its GeForce GPUs are enabling local AI development and they're still the gaming cards to beat, despite AMD's steadily improving competition.Clearly, the company's bet on parallel processing has paid off enormously. Its GPUs can handle tons of computations simultaneously, making them ideally suited for the demands of the AI industry. They're not exactly efficient — that's why neural processing units, or NPUs have sprung up to power consumer AI features — but it's hard to deny NVIDIA's raw computational power. NVIDIA's AI success may not last forever, though. Companies like Google and Microsoft are already working on their own AI chips, and it's still unclear if consumers actually want widespread AI features as much as tech companies think. If the AI industry crashes, NVIDIA will be one of the first victims. — Devindra Hardawar, Senior reporterTech billionairesUS President Donald Trump speaks during a news conference with Elon Musk (L) in the Oval Office of the White House in Washington, DC, on May 30, 2025. ALLISON ROBBERT via Getty ImagesThere's no doubt that tech billionaires, especially those that lean conservatively, have benefitted tremendously from the Trump administration over the past year. Elon Musk's DOGE team of tech loyalists chainsawed their way through the budgets and staff of several federal agencies, including the National Highway Traffic Safety Administration (NHTSA), which regulates Tesla. (That hasn't stopped the NHTSA from launching a new investigation into Tesla's full self-driving tech, though.)According to a recent report by Oxfam, the 10 richest US billionaires (who are all tech leaders, save for Warren Buffet) increased their wealth by $698 billion of the past year. Of course, it took plenty of wining and dining to get there. Elon Musk reportedly donated nearly $300 million to Trump and Republican allies, and several tech companies have pitched in to build the president's lavish White House ballroom. But the result for the tech elite is increased access to the president, less scrutiny when it comes to acquisitions and other deals, and the potential for massive corporate and elite tax cuts. — D.H.AI videoA silhouetted individual is seen holding a mobile phone with a Sora of ChatGPT OpenAI logo displayed in the backgroundSOPA Images via Getty ImagesAI slop didn't start in 2025, but it reached new heights thanks to updates from Meta, Google, OpenAI and others that made it easier than ever to create a real-ish (emphasis on the ish) looking clips from nothing but your most unhinged mad libs. Now, AI-generated videos are just about impossible to avoid. Some platforms, like Pinterest and TikTok, have even begun offering people the ability to ask their algorithms to show less AI content in their feeds. Unfortunately, there's no way to stuff Shrimp Jesus back into the bottle. AI video is everywhere and it's here to stay. It's not only overtaken Facebook and Instagram's recommendations, Meta created an entirely separate feed just for users' AI-generated fever dreams. OpenAI's Sora, which lets you make AI videos of real people, was downloaded a million times in just a few days. Google's Veo, which generated more than 40 million videos in a matter of weeks, is now built-in to YouTube Shorts.It's now trivially easy for creators to churn out fake movie trailers, cute animal videos that never happened or viral clips of made up ICE raids. Hell, the president of the United States regularly shares bizarre, sometimes poop-themed, AI videos on his official social media channels. During the government shutdown, the official X account for Senate Republicans shared a deepfake of Senate minority leader Chuck Schumer. AI video is winning not just because it's everywhere, but because so many are unable, or unwilling, to understand what's real and what isn't. More than half of Americans say they are not confident in their ability to distinguish between human and AI-generated content, according to Pew Research. Similar numbers of people report being \"more concerned than excited about the increased use of AI in daily life.\" But those concerns have done little to stop AI slop from dominating all of our feeds, and there's no sign it will ever slow down. — Karissa Bell, Senior reporterGalaxy Z Fold 7Samsung Galaxy Z Fold 7Sam Rutherford for EngadgetAfter seven generations, Samsung reached an important milestone this year with its Galaxy Z Fold line: It made a foldable phone that’s the same size as a regular handset. In fact, weighing 7.58 ounces and measuring 72.8mm wide, the Galaxy Z Fold 7 is actually lighter and narrower than an S25 Ultra, while being practically just as thin at 8.9mm (folded). It’s a real marvel of engineering, especially when you consider the phone also features a 200MP main camera, an IPX8 rating for water resistance and a 5,000 mAh battery with 45-watt wired charging. And of course, there's that huge 8-inch main screen hiding inside, which makes the Z Fold 7 both a phone and a tablet in one device. The only thing it's really missing is the improved dust resistance Google gave to the Pixel 10 Pro Fold. But perhaps more importantly, the Z Fold 7's reduced size and weight have created a device with wider appeal. This has propelled sales of Samsung's latest flagship foldable up 50 percent compared to the previous generation while pushing shipments of foldables as a whole to record highs. Who knew that when Samsung focuses on creating world-class hardware instead of overindexing on AI, good things happen? Okay, maybe that’s a bit harsh. Regardless, for a phone category that has struggled with excess weight and bulk since its inception, the Z Fold 7 feels like a revelation and the beginning of a new era for handsets with flexible displays. Now, can we just bring their prices down, please? — Sam Rutherford, Senior reporterSmart glassesSenior reporter Karissa Bell wearing a pair of Ray Ban Display glasses. Karissa Bell for EngadgetLike it or not, smart glasses are having a moment. Propelled by new devices like the Meta Ray-Ban Display and upcoming models like Xreal’s Project Aura, the idea of wearing specs with built-in screens suddenly became an attractive proposition. And that means a lot for a category of gadgets that’s often best remembered by the fashion tragedy that was Google Glass in 2013. However, this development isn’t purely by chance. The latest generation of smart glasses has only just now become a reality due to the convergence of several branches of tech — including improved optics, lightweight batteries and, of course, AI. Now that last one might sound silly considering how many big companies seem to be betting the farm on machine learning being the next big thing, but AI will be a critical feature for enabling the hands-free experience that you need to make smartglasses work when you can’t rely on touch input. While this category is still in its early stages of development, the increased momentum we've seen from smart glasses this year seems poised to carry them towards being a future pillar of people's core tech kits. — S.R.Fast chargingFast charging on the Pixel Watch 4 is one implementation that impressed us this year.Cherlynn Low for EngadgetDevices like tablets and smartwatches have matured to the point where each generation mostly sees iterative upgrades, making covering them seem boring. But this year, as the hardware review season came to a close, I noticed an interesting trend. One feature, across various product categories, genuinely excited myself and other reviewers at Engadget and around the internet: impressively fast charging. By itself, high-speed charging isn’t new. But when I reviewed the Pixel Watch 4 in October, I was shocked that one seemingly little update changed how I went about my day. The new power system on Google’s smartwatch was so efficient that after about ten minutes on a cradle, the wearable went from below 20 percent to past 50 percent. With that boost, I stopped having to remind myself to plug the watch in — any time I ran low or was about to run out the door, I just plopped it on the charger and would have enough juice for hours.Google wasn’t the only company to make fast-charging a meaningful addition to one of its 2025 products. Apple’s iPad Pro M5 is the first iPad to support the feature, and while in our testing it fell a little short of the 50 percent charge in 30 minutes that the company promised, our reviewer Nate Ingraham still found it a meaningful improvement.Observers of the smartphone industry will likely point out two things. First, battery technology can be volatile, and larger, faster-charging cells might lead to exploding phones. So my optimism about this development is not without caution. Secondly, we’ve already seen all this come to handsets, especially in phones that launched outside the US first. OnePlus is known for its SUPERVOOC fast charging system, for example, and we’re seeing even more novel battery tech show up abroad. Calling fast charging a winner of 2025 may feel untimely to some.But when you consider the spread of speedier charging to other types of products, especially in electric vehicles that till now take forever to top up, the benefits are clear. This year, we saw Formula E (finally) debut its fast-charging pit stops, Honda announce its first full-size electric motorcycle with fast charging and Chinese EV maker BYD unveiling new tech that delivers peak EV charging speeds of 1,000 kilowatts. That should about halve the time it currently takes to top up your electric car. Sure, it’s not the most eye-catching or novel technological development. But when counted in terms of precious time saved, fast charging coming to more types of devices certainly amounts to a greater good in gadgets in 2025. — Cherlynn Low, Managing editorMagnetsThe Pixel 10 Pro Fold and the Pixel Ring StandSam Rutherford for EngadgetTwo years after the announcement of the Qi 2 wireless charging standard and its support of magnetic attachment accessories (a la Apple’s MagSafe), we’re finally seeing one of the more mainstream Android devices adopt it. In 2025, Google became the first Android phone maker that’s not HMD to do so, bringing such magnetic capabilities to the Pixel 10 series. It also introduced Pixelsnap — its own version of a MagSafe accessory ecosystem, including a slim puck with a fold-out kickstand that you can snap onto a phone. I love the Pixel Ring Stand and make sure to bring it with me whenever I can. It works perfectly with my iPhone 17 Pro, and has a compact footprint that makes it easy to take anywhere. Of course, it’s not the first of its kind — Case-Mate and PopSocket, among others, already make similar products but they’re either pricier or rated poorly. But it’s not just Google that made a magnetic accessory I unexpectedly adored. When reports of Apple’s Crossbody Strap first trickled out, I was underwhelmed. Who cares about a crossbody strap for an iPhone? But when I was presented with one to try at the iPhone 17 launch event, my cynicism quickly melted into desire. Setting aside the convenience of having your phone on your person when you don’t have pockets or a purse, the way magnets play a part here also won me over. To adjust the length of the straps, you just separate the two overlapping pieces that stick together magnetically, move them along each other till you’re satisfied with the length and let them snap back in place. I’m sure Apple isn’t the first to make a crossbody strap accessory for iPhones, nor is it the first to use magnets to adjust such straps. But like many Redditors, I’ve slowly come to realize the differences between those products and the Crossbody Strap for iPhone 17. It’s far from perfect, but in 2025 it was another implementation of magnets in tech that caught my attention and brought convenience to my life. — C.L.This article originally appeared on Engadget at https://www.engadget.com/techs-biggest-winners-of-2025-180000177.html?src=rss",
          "feed_position": 43,
          "image_url": "https://media-mbst-pub-ue1.s3.amazonaws.com/creatr-uploaded-images/2025-06/0a096060-55df-11f0-ba9d-0a6ff3cbbb07"
        }
      ],
      "featured_image": "https://images.ctfassets.net/jdtwqhzvc2n1/7IoiYyTs0K9D4WfYDzJvhy/2cd15991676cc0648bffb25178642417/AdobeStock_571280209_Preview.jpeg?w=300&q=30",
      "popularity_score": 2031.0807275
    },
    {
      "id": "cluster_22",
      "coverage": 2,
      "updated_at": "Tue, 09 Dec 2025 18:10:00 -0500",
      "title": "Google adds new pinch and wrist gestures to the Pixel Watch 4 and says Watch 3 and Watch 4 will now use an on-device Gemma-based AI model for Smart Replies (Abner Li/9to5Google)",
      "neutral_headline": "How Google's new Pixel Watch update takes aim at Apple Watch",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/251209/p48#a251209p48",
          "published_at": "Tue, 09 Dec 2025 18:10:00 -0500",
          "title": "Google adds new pinch and wrist gestures to the Pixel Watch 4 and says Watch 3 and Watch 4 will now use an on-device Gemma-based AI model for Smart Replies (Abner Li/9to5Google)",
          "standfirst": "Abner Li / 9to5Google: Google adds new pinch and wrist gestures to the Pixel Watch 4 and says Watch 3 and Watch 4 will now use an on-device Gemma-based AI model for Smart Replies &mdash; After missing out on last month's Feature Drop, Google is rolling out a sizable Pixel Watch 4 update this December that adds one-handed gestures.",
          "content": "Abner Li / 9to5Google: Google adds new pinch and wrist gestures to the Pixel Watch 4 and says Watch 3 and Watch 4 will now use an on-device Gemma-based AI model for Smart Replies &mdash; After missing out on last month's Feature Drop, Google is rolling out a sizable Pixel Watch 4 update this December that adds one-handed gestures.",
          "feed_position": 8,
          "image_url": "http://www.techmeme.com/251209/i48.jpg"
        },
        {
          "source": "ZDNet",
          "url": "https://www.zdnet.com/article/google-pixel-watch-double-pinch-hand-gestures-new-features/",
          "published_at": "Tue, 09 Dec 2025 19:51:00 GMT",
          "title": "How Google's new Pixel Watch update takes aim at Apple Watch",
          "standfirst": "Pixel Watch 4 is getting gestures like double pinch and wrist turn - two features already included on Apple Watches - to dismiss calls, snooze alarms, and select smart replies.",
          "content": "Pixel Watch 4 is getting gestures like double pinch and wrist turn - two features already included on Apple Watches - to dismiss calls, snooze alarms, and select smart replies.",
          "feed_position": 7
        }
      ],
      "featured_image": "http://www.techmeme.com/251209/i48.jpg",
      "popularity_score": 2015.2473941666667
    },
    {
      "id": "cluster_40",
      "coverage": 2,
      "updated_at": "Tue, 09 Dec 2025 16:15:01 -0500",
      "title": "Australia's ban on social media for users aged under 16 comes into effect; platforms that do not comply risk fines of up to AU$49.5M (Josh Taylor/The Guardian)",
      "neutral_headline": "Australia's ban on social media for users aged under 16 comes into effect; platforms that do not comply...",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/251209/p42#a251209p42",
          "published_at": "Tue, 09 Dec 2025 16:15:01 -0500",
          "title": "Australia's ban on social media for users aged under 16 comes into effect; platforms that do not comply risk fines of up to AU$49.5M (Josh Taylor/The Guardian)",
          "standfirst": "Josh Taylor / The Guardian: Australia's ban on social media for users aged under 16 comes into effect; platforms that do not comply risk fines of up to AU$49.5M &mdash; Accounts held by users under 16 must be removed on apps that include TikTok, Facebook, Instagram, X, YouTube, Snapchat, Reddit, Kick, Twitch and Threads under ban",
          "content": "Josh Taylor / The Guardian: Australia's ban on social media for users aged under 16 comes into effect; platforms that do not comply risk fines of up to AU$49.5M &mdash; Accounts held by users under 16 must be removed on apps that include TikTok, Facebook, Instagram, X, YouTube, Snapchat, Reddit, Kick, Twitch and Threads under ban",
          "feed_position": 14,
          "image_url": "http://www.techmeme.com/251209/i42.jpg"
        },
        {
          "source": "Guardian Tech",
          "url": "https://www.theguardian.com/australia-news/2025/dec/09/australia-under-16-social-media-ban-begins-apps-listed",
          "published_at": "Tue, 09 Dec 2025 13:01:04 GMT",
          "title": "Australia’s world-first social media ban begins as millions of children and teens lose access to accounts",
          "standfirst": "Accounts held by users under 16 must be removed on apps that include TikTok, Facebook, Instagram, X, YouTube, Snapchat, Reddit, Kick, Twitch and Threads under banHow is Australia’s social media ban affecting you and your family?The social media ban explained: everything you need to knowAustralia has enacted a world-first ban on social media for users aged under 16, causing millions of children and teenagers to lose access to their accounts.Facebook, Instagram, Threads, X, YouTube, Snapchat, Reddit, Kick, Twitch and TikTok are expected to have taken steps from Wednesday to remove accounts held by users under 16 years of age in Australia, and prevent those teens from registering new accounts. Continue reading...",
          "content": "Accounts held by users under 16 must be removed on apps that include TikTok, Facebook, Instagram, X, YouTube, Snapchat, Reddit, Kick, Twitch and Threads under banHow is Australia’s social media ban affecting you and your family?The social media ban explained: everything you need to knowAustralia has enacted a world-first ban on social media for users aged under 16, causing millions of children and teenagers to lose access to their accounts.Facebook, Instagram, Threads, X, YouTube, Snapchat, Reddit, Kick, Twitch and TikTok are expected to have taken steps from Wednesday to remove accounts held by users under 16 years of age in Australia, and prevent those teens from registering new accounts. Continue reading...",
          "feed_position": 0
        }
      ],
      "featured_image": "http://www.techmeme.com/251209/i42.jpg",
      "popularity_score": 2013.3310052777779
    },
    {
      "id": "cluster_59",
      "coverage": 2,
      "updated_at": "Tue, 09 Dec 2025 19:41:40 +0000",
      "title": "Slack CEO Denise Dresser to join OpenAI as chief revenue officer",
      "neutral_headline": "Slack CEO Denise Dresser to join OpenAI as chief revenue officer",
      "items": [
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2025/12/09/slack-ceo-denise-dresser-to-join-openai-as-chief-revenue-officer/",
          "published_at": "Tue, 09 Dec 2025 19:41:40 +0000",
          "title": "Slack CEO Denise Dresser to join OpenAI as chief revenue officer",
          "standfirst": "OpenAI says that Dresser will be responsible for the company's revenue strategy in enterprise and customer success.",
          "content": "OpenAI says that Dresser will be responsible for the company's revenue strategy in enterprise and customer success.",
          "feed_position": 10
        },
        {
          "source": "Wired Tech",
          "url": "https://www.wired.com/story/slack-ceo-denise-dresser-joins-openai-chief-revenue-officer/",
          "published_at": "Tue, 09 Dec 2025 18:35:01 +0000",
          "title": "OpenAI Hires Slack CEO as New Chief Revenue Officer",
          "standfirst": "A memo obtained by WIRED confirms Denise Dresser's departure from Slack. She is now headed to OpenAI.",
          "content": "A memo obtained by WIRED confirms Denise Dresser's departure from Slack. She is now headed to OpenAI.",
          "feed_position": 6,
          "image_url": "https://media.wired.com/photos/693859e8da23a81834d81ac9/master/pass/OpenAI-Hires-Slack-CEO-Denise-Dresser-as-Chief-Revenue-Officer-Business-2181104969.jpg"
        }
      ],
      "featured_image": "https://media.wired.com/photos/693859e8da23a81834d81ac9/master/pass/OpenAI-Hires-Slack-CEO-Denise-Dresser-as-Chief-Revenue-Officer-Business-2181104969.jpg",
      "popularity_score": 2011.7751719444445
    },
    {
      "id": "cluster_29",
      "coverage": 1,
      "updated_at": "Tue, 09 Dec 2025 22:34:39 +0000",
      "title": "Over 250 people quarantined in South Carolina as measles outbreak rages",
      "neutral_headline": "Over 250 people quarantined in South Carolina as measles outbreak rages",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/health/2025/12/over-250-people-quarantined-in-south-carolina-as-measles-outbreak-rages/",
          "published_at": "Tue, 09 Dec 2025 22:34:39 +0000",
          "title": "Over 250 people quarantined in South Carolina as measles outbreak rages",
          "standfirst": "16 cases are linked to a church, which followed exposures at four schools last week.",
          "content": "A measles outbreak that began in South Carolina at the start of October is showing no signs of slowing as officials on Tuesday reported 27 new cases since Friday. Those cases bring the outbreak total to 111. The southern state’s outbreak now rivals outbreaks ongoing in Utah and Arizona, which have tallied 115 and 176 cases, respectively. The outbreaks are threatening to cost the country its measles elimination status, which was earned in 2000 after vaccination efforts stopped the virus from spreading continuously. If the current transmission of the virus isn’t halted by January, the virus will have circulated for 12 consecutive months, marking it once again as an endemic disease in the US. In an update on Tuesday, South Carolina’s health department suggested the spread is far from over. Of the state’s 27 new cases, 16 were linked to exposure at a church, the Way of Truth Church in Inman. And amid the new cases, new exposures were identified at Inman Intermediate School. That’s on top of exposures announced Friday at four other schools in the region, which led to well over 100 students being quarantined.Read full article Comments",
          "feed_position": 0,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/02/GettyImages-2152300024-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/02/GettyImages-2152300024-1152x648.jpg",
      "popularity_score": 347.6582275
    },
    {
      "id": "cluster_64",
      "coverage": 1,
      "updated_at": "Tue, 09 Dec 2025 18:53:21 +0000",
      "title": "Supreme Court appears likely to approve Trump’s firing of FTC Democrat",
      "neutral_headline": "Supreme Court appears likely to approve Trump’s firing of FTC Democrat",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/12/supreme-court-appears-likely-to-approve-trumps-firing-of-ftc-democrat/",
          "published_at": "Tue, 09 Dec 2025 18:53:21 +0000",
          "title": "Supreme Court appears likely to approve Trump’s firing of FTC Democrat",
          "standfirst": "Conservative justices seem ready to back Trump control of independent agencies.",
          "content": "The Supreme Court’s conservative justices appear ready to overturn a 90-year-old precedent that said the president cannot fire a Federal Trade Commission member without cause. A ruling for Trump would give him more power over the FTC and potentially other independent agencies such as the Federal Communications Commission. Former FTC Commissioner Rebecca Kelly Slaughter, a Democrat, sued Trump after he fired both Democrats from the commission in March. Slaughter’s case rests largely on the 1935 ruling in Humphrey’s Executor v. United States, in which the Supreme Court unanimously held that the president can only remove FTC commissioners for inefficiency, neglect of duty, or malfeasance in office. Chief Justice John Roberts said during yesterday’s oral arguments that Humphrey’s Executor is a “dried husk” despite being the “primary authority” that Slaughter’s legal team is relying on. Roberts said the court’s 2020 ruling in Seila Law made it “pretty clear… that Humphrey’s Executor is just a dried husk of whatever people used to think it was because, in the opinion itself, it described the powers of the agency it was talking about, and they’re vanishingly insignificant, have nothing to do with what the FTC looks like today.”Read full article Comments",
          "feed_position": 2,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2023/09/getty-supreme-court-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2023/09/getty-supreme-court-1152x648.jpg",
      "popularity_score": 323.9698941666667
    },
    {
      "id": "cluster_73",
      "coverage": 1,
      "updated_at": "Tue, 09 Dec 2025 17:47:01 +0000",
      "title": "Court: “Because Trump said to” may not be a legally valid defense",
      "neutral_headline": "Court: “Because Trump said to” may not be a legally valid defense",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/12/trumps-order-blocking-wind-development-thrown-out-by-court/",
          "published_at": "Tue, 09 Dec 2025 17:47:01 +0000",
          "title": "Court: “Because Trump said to” may not be a legally valid defense",
          "standfirst": "The \"arbitrary and capricious\" standard strikes down another administration action.",
          "content": "On Monday, US District Court Judge Patti Saris vacated a Trump executive order that brought a halt to all offshore wind power development, as well as some projects on land. That order had called for the suspension of all permitting for wind power on federal land and waters pending a review of current practices. This led states and an organization representing wind power companies to sue, claiming among other things that the suspension was arbitrary and capricious. Over 10 months since the relevant government agencies were ordered to start a re-evaluation of the permitting process, testimony revealed that they had barely begun to develop the concept of a review. As such, the only reason they could offer in defense of the suspension consisted of Trump’s executive order and a Department of the Interior memo implementing it. “Whatever level of explanation is required when deviating from longstanding agency practice,” Judge Saris wrote, “this is not it.” Lifting Trump’s suspension does not require the immediate approval of any wind projects. Instead, the relevant agencies are likely to continue following Trump’s wishes and slow-walking any leasing and licensing processes, which may force states and project owners to sue individually. But it does provide a legal backdrop for any suits that ultimately occur, one in which the government’s actions have little justification beyond Trump’s personal animosity toward wind power.Read full article Comments",
          "feed_position": 4,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2235990625-1024x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2235990625-1024x648.jpg",
      "popularity_score": 317.8643386111111
    },
    {
      "id": "cluster_68",
      "coverage": 1,
      "updated_at": "Tue, 09 Dec 2025 18:09:05 +0000",
      "title": "NASA astronauts will have their own droid when they go back to the Moon",
      "neutral_headline": "NASA astronauts will have their own droid when they go back to the Moon",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2025/12/lunar-outpost-rover-to-study-lunar-dust-alongside-artemis-astronauts-on-moon/",
          "published_at": "Tue, 09 Dec 2025 18:09:05 +0000",
          "title": "NASA astronauts will have their own droid when they go back to the Moon",
          "standfirst": "NASA crew will be the first astronauts to work with a robot on a celestial body other than Earth.",
          "content": "B-9 had Will Robinson. Twiki had Buck Rogers. And, of course, C-3PO and R2-D2 had Luke Skywalker. Now, in a scenario straight out of science fiction, MAPP will have whoever NASA names to the crew of the second Artemis mission to land on the moon. The space agency has selected Lunar Outpost’s Mobile Autonomous Prospecting Platform, or MAPP, to become the first robotic rover to operate on the moon alongside astronauts. Although its tasks will be far simpler than those of the robots seen on TV and in the movies, the autonomous four-wheeled MAPP will help scientists learn more about the crew’s surroundings. Science instruments on the rover will characterize the surface plasma and behavior of the dust in the lunar environment. “The Apollo era taught us that the further humanity is from Earth, the more dependent we are on science to protect and sustain human life on other planets,” said Nicky Fox, NASA’s associate administrator for science, in a statement. “By deploying these… science instruments on the lunar surface, our proving ground, NASA is leading the world in the creation of humanity’s interplanetary survival guide to ensure the health and safety of our spacecraft and human explorers as we begin our epic journey back to the Moon.”Read full article Comments",
          "feed_position": 3,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/news-120825a-lg-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/news-120825a-lg-1152x648.jpg",
      "popularity_score": 313.23211638888887
    },
    {
      "id": "cluster_81",
      "coverage": 1,
      "updated_at": "Tue, 09 Dec 2025 17:00:28 +0000",
      "title": "Google is reviving wearable gesture controls, but only for the Pixel Watch 4",
      "neutral_headline": "Google is reviving wearable gesture controls, but only for the Pixel Watch 4",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2025/12/google-is-reviving-wearable-gesture-controls-but-only-for-the-pixel-watch-4/",
          "published_at": "Tue, 09 Dec 2025 17:00:28 +0000",
          "title": "Google is reviving wearable gesture controls, but only for the Pixel Watch 4",
          "standfirst": "Google will let you select and dismiss with a gesture, but only on the newest watch.",
          "content": "Long ago, Google’s Android-powered wearables had hands-free navigation gestures. Those fell by the wayside as Google shredded its wearable strategy over and over, but gestures are back, baby. The Pixel Watch 4 is getting an update that adds several gestures, one of which is straight out of the Apple playbook. When the update hits devices, the Pixel Watch 4 will gain a double pinch gesture like the Apple Watch has. By tapping your thumb and forefinger together, you can answer or end calls, pause timers, and more. The watch will also prompt you at times when you can use the tap gesture to control things. In previous incarnations of Google-powered watches, a quick wrist turn gesture would scroll through lists. In the new gesture system, that motion dismisses what’s on the screen. For example, you can clear a notification from the screen or dismiss an incoming call. Pixel Watch 4 owners will also enjoy this one when the update arrives.Read full article Comments",
          "feed_position": 5,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/Pixel-watch-4-1-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/Pixel-watch-4-1-1152x648.jpg",
      "popularity_score": 310.08850527777776
    },
    {
      "id": "cluster_88",
      "coverage": 1,
      "updated_at": "Tue, 09 Dec 2025 16:10:45 +0000",
      "title": "Brazil weakens Amazon protections days after COP30",
      "neutral_headline": "Brazil weakens Amazon protections days after COP30",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2025/12/days-after-cop30-brazil-weakened-amazon-safeguards/",
          "published_at": "Tue, 09 Dec 2025 16:10:45 +0000",
          "title": "Brazil weakens Amazon protections days after COP30",
          "standfirst": "Backed by powerful corporations, nations are giving public false choices: Environmental protection or economic growth.",
          "content": "Despite claims of environmental leadership and promises to preserve the Amazon rainforest ahead of COP30, Brazil is stripping away protections for the region’s vital ecosystems faster than workers dismantled the tents that housed the recent global climate summit in Belém. On Nov. 27, less than a week after COP30 ended, a powerful political bloc in Brazil’s National Congress, representing agribusiness, and development interests, weakened safeguards for the Amazon’s rivers, forests, and Indigenous communities. The rollback centered on provisions in an environmental licensing bill passed by the government a few months before COP30. The law began to take shape well before, during the Jair Bolsonaro presidency from 2019 to 2023. It reflected the deregulatory agenda of the rural caucus, the Frente Parlamentar da Agropecuária, which wielded significant power during his term and remains influential today.Read full article Comments",
          "feed_position": 6,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-1249347312-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-1249347312-1152x648.jpg",
      "popularity_score": 278.25989416666664
    },
    {
      "id": "cluster_89",
      "coverage": 1,
      "updated_at": "Tue, 09 Dec 2025 16:00:29 +0000",
      "title": "Pompeii construction site confirms recipe for Roman concrete",
      "neutral_headline": "Pompeii construction site confirms recipe for Roman concrete",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2025/12/study-confirms-romans-used-hot-mixing-to-make-concrete/",
          "published_at": "Tue, 09 Dec 2025 16:00:29 +0000",
          "title": "Pompeii construction site confirms recipe for Roman concrete",
          "standfirst": "Latest results from a recently discovered ancient Roman construction site confirm earlier findings.",
          "content": "Back in 2023, we reported on MIT scientists’ conclusion that the ancient Romans employed “hot mixing” with quicklime, among other strategies, to make their famous concrete, giving the material self-healing functionality. The only snag was that this didn’t match the recipe as described in historical texts. Now the same team is back with a fresh analysis of samples collected from a recently discovered site that confirms the Romans did indeed use hot mixing, according to a new paper published in the journal Nature Communications. As we’ve reported previously, like today’s Portland cement (a basic ingredient of modern concrete), ancient Roman concrete was basically a mix of a semi-liquid mortar and aggregate. Portland cement is typically made by heating limestone and clay (as well as sandstone, ash, chalk, and iron) in a kiln. The resulting clinker is then ground into a fine powder with just a touch of added gypsum to achieve a smooth, flat surface. But the aggregate used to make Roman concrete was made up of fist-sized pieces of stone or bricks. In his treatise De architectura (circa 30 CE), the Roman architect and engineer Vitruvius wrote about how to build concrete walls for funerary structures that could endure for a long time without falling into ruin. He recommended the walls be at least two feet thick, made of either “squared red stone or of brick or lava laid in courses.” The brick or volcanic rock aggregate should be bound with mortar composed of hydrated lime and porous fragments of glass and crystals from volcanic eruptions (known as volcanic tephra).Read full article Comments",
          "feed_position": 7,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/concrete1-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/concrete1-1152x648.jpg",
      "popularity_score": 271.08878305555555
    },
    {
      "id": "cluster_90",
      "coverage": 1,
      "updated_at": "Tue, 09 Dec 2025 16:00:25 +0000",
      "title": "In a major new report, scientists build rationale for sending astronauts to Mars",
      "neutral_headline": "In a major new report, scientists build rationale for sending astronauts to Mars",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2025/12/in-a-major-new-report-scientists-build-rationale-for-sending-astronauts-to-mars/",
          "published_at": "Tue, 09 Dec 2025 16:00:25 +0000",
          "title": "In a major new report, scientists build rationale for sending astronauts to Mars",
          "standfirst": "\"Everyone is inspired by this because it's becoming real.\"",
          "content": "Sending astronauts to the red planet will be a decades-long activity and cost many billions of dollars. So why should NASA undertake such a bold mission? A new report published Tuesday, titled “A Science Strategy for the Human Exploration of Mars,” represents the answer from leading scientists and engineers in the United States: finding whether life exists, or once did, beyond Earth. “We’re searching for life on Mars,” said Dava Newman, a professor in the Department of Aeronautics and Astronautics at Massachusetts Institute of Technology and co-chair of the committee that wrote the report, in an interview with Ars. “The answer to the question ‘are we alone‘ is always going to be ‘maybe,’ unless it becomes yes.”Read full article Comments",
          "feed_position": 8,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/03/file-20250327-56-dflaq1-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/03/file-20250327-56-dflaq1-1152x648.jpg",
      "popularity_score": 269.0876719444444
    },
    {
      "id": "cluster_92",
      "coverage": 1,
      "updated_at": "Tue, 09 Dec 2025 15:35:39 +0000",
      "title": "Asked why we need Golden Dome, the man in charge points to a Hollywood film",
      "neutral_headline": "Asked why we need Golden Dome, the man in charge points to a Hollywood film",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2025/12/asked-why-we-need-golden-dome-the-man-in-charge-points-to-a-hollywood-film/",
          "published_at": "Tue, 09 Dec 2025 15:35:39 +0000",
          "title": "Asked why we need Golden Dome, the man in charge points to a Hollywood film",
          "standfirst": "\"If they see how prepared we are, no one starts a nuclear war.\"",
          "content": "Near the end of the film A House of Dynamite, a fictional American president portrayed by Idris Elba sums up the theory of nuclear deterrence. “Just being ready is the point, right?” Elba says. “It keeps people in check. Keeps the world straight. If they see how prepared we are, no one starts a nuclear war.” There’s a lot that goes wrong in the film, namely the collapse of deterrence itself. For more than 60 years, the US military has used its vast arsenal of nuclear weapons, constantly deployed on Navy submarines, at Air Force bomber bases, and in Minuteman missile fields, as a way of saying, “Don’t mess with us.” In the event of a first strike against the United States, an adversary would be assured of an overwhelming nuclear response, giving rise to the concept of mutual assured destruction.Read full article Comments",
          "feed_position": 9,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/5208709-1152x648-1765274456.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/5208709-1152x648-1765274456.jpg",
      "popularity_score": 250.67489416666666
    },
    {
      "id": "cluster_96",
      "coverage": 1,
      "updated_at": "Tue, 09 Dec 2025 15:00:56 +0000",
      "title": "Pebble maker announces Index 01, a smart-ish ring for under $100",
      "neutral_headline": "Pebble maker announces Index 01, a smart-ish ring for under $100",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2025/12/resurrected-pebble-maker-announces-a-kind-of-smart-ring-for-capturing-audio-notes/",
          "published_at": "Tue, 09 Dec 2025 15:00:56 +0000",
          "title": "Pebble maker announces Index 01, a smart-ish ring for under $100",
          "standfirst": "The Pebble Index 01 isn't quite a smart ring, but it can do some smart things.",
          "content": "Nearly a decade after Pebble’s nascent smartwatch empire crumbled, the brand is staging a comeback with new wearables. The Pebble Core Duo 2 and Core Time 2 are a natural evolution of the company’s low-power smartwatch designs, but its next wearable is something different. The Index 01 is a ring, but you probably shouldn’t call it a smart ring. The Index does just one thing—capture voice notes—but the firm says it does that one thing extremely well. Most of today’s smart rings offer users the ability to track health stats, along with various minor smartphone integrations. With all the sensors and data collection, these devices can cost as much as a smartwatch and require frequent charging. The Index 01 doesn’t do any of that. It contains a Bluetooth radio, a microphone, a hearing aid battery, and a physical button. You press the button, record your note, and that’s it. The company says the Index 01 will run for years on a charge and will cost just $75 during the preorder period. After that, it will go up to $99. Core Devices, the new home of Pebble, says the Index is designed to be worn on your index finger (get it?), where you can easily mash the device’s button with your thumb. Unlike recording notes with a phone or smartwatch, you don’t need both hands to create voice notes with the Index.Read full article Comments",
          "feed_position": 10,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/Index01-polished-silver-rocks-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/Index01-polished-silver-rocks-1152x648.jpg",
      "popularity_score": 140.09628305555555
    },
    {
      "id": "cluster_122",
      "coverage": 1,
      "updated_at": "Mon, 08 Dec 2025 21:54:58 +0000",
      "title": "ICEBlock lawsuit: Trump admin bragged about demanding App Store removal",
      "neutral_headline": "ICEBlock lawsuit: Trump admin bragged about demanding App Store removal",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/12/iceblock-lawsuit-trump-admin-bragged-about-demanding-app-store-removal/",
          "published_at": "Mon, 08 Dec 2025 21:54:58 +0000",
          "title": "ICEBlock lawsuit: Trump admin bragged about demanding App Store removal",
          "standfirst": "ICEBlock creator sues to protect apps that are crowd-sourcing ICE sightings.",
          "content": "In a lawsuit filed against top Trump administration officials on Monday, Apple was accused of caving to unconstitutional government demands by removing an Immigration and Customs Enforcement-spotting app from the App Store with more than a million users. In his complaint, Joshua Aaron, creator of ICEBlock, cited a Fox News interview in which Attorney General Pam Bondi “made plain that the United States government used its regulatory power to coerce a private platform to suppress First Amendment-protected expression.” Suing Bondi—along with Department of Homeland Security Secretary Kristi Noem, Acting Director of ICE Todd Lyons, White House “Border Czar” Thomas D. Homan, and unnamed others—Aaron further alleged that US officials made false statements and “unlawful threats” to criminally investigate and prosecute him for developing ICEBlock.Read full article Comments",
          "feed_position": 11,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2235825531-1024x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2235825531-1024x648.jpg",
      "popularity_score": 133
    },
    {
      "id": "cluster_128",
      "coverage": 1,
      "updated_at": "Mon, 08 Dec 2025 18:36:16 +0000",
      "title": "Paramount tries to swipe Warner Bros. from Netflix with a hostile takeover",
      "neutral_headline": "Paramount tries to swipe Warner Bros. from Netflix with a hostile takeover",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2025/12/paramount-says-it-could-get-antitrust-approval-for-wbd-before-netflix/",
          "published_at": "Mon, 08 Dec 2025 18:36:16 +0000",
          "title": "Paramount tries to swipe Warner Bros. from Netflix with a hostile takeover",
          "standfirst": "Paramount has already proven it can get a controversial merger done.",
          "content": "Netflix won the bidding war for Warner Bros. Discovery’s (WBD’s) streaming and movie studio businesses last week. But Paramount Skydance isn’t relenting on its dreams of owning WBD and is pushing forward with a hostile takeover bid. On Friday, Netflix announced that it had agreed to pay an equity value of $72 billion, or an approximate total enterprise value of $82.7 billion, for WBD’s streaming and film businesses, as well as its film and TV libraries. The deal includes HBO and the HBO Max streaming service but not WBD’s cable channels, which are to be split off ahead of the acquisition into a separate company called Discovery Global. Netflix said WBD’s split should conclude in Q3 2026. Paramount has different plans, though.Read full article Comments",
          "feed_position": 12,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2250240969-1024x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2250240969-1024x648.jpg",
      "popularity_score": 133
    },
    {
      "id": "cluster_137",
      "coverage": 1,
      "updated_at": "Mon, 08 Dec 2025 14:57:11 +0000",
      "title": "Meta offers EU users ad-light option in push to end investigation",
      "neutral_headline": "Meta offers EU users ad-light option in push to end investigation",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/information-technology/2025/12/meta-offers-eu-users-ad-light-option-in-push-to-end-investigation/",
          "published_at": "Mon, 08 Dec 2025 14:57:11 +0000",
          "title": "Meta offers EU users ad-light option in push to end investigation",
          "standfirst": "Facebook agrees to change \"pay or consent\" model after talks with European Commission.",
          "content": "Meta has agreed to make changes to its “pay or consent” business model in the EU, seeking to agree to a deal that avoids further regulatory fines at a time when the bloc’s digital rule book is drawing anger from US authorities. On Tuesday, the European Commission announced that the social media giant had offered users an alternative choice of Facebook and Instagram services that would show them fewer personalized advertisements. The offer follows an EU investigation into Meta’s policy of requiring users either to consent to data tracking or pay for an ad-free service. The Financial Times reported on optimism that an agreement could be reached between the parties in October.Read full article Comments",
          "feed_position": 15,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-1359152239-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-1359152239-1152x648.jpg",
      "popularity_score": 133
    },
    {
      "id": "cluster_142",
      "coverage": 1,
      "updated_at": "Mon, 08 Dec 2025 12:00:14 +0000",
      "title": "Please send help. I can’t stop playing these roguelikes.",
      "neutral_headline": "Please send help. I can’t stop playing these roguelikes.",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gaming/2025/12/please-send-help-i-cant-stop-playing-these-roguelikes/",
          "published_at": "Mon, 08 Dec 2025 12:00:14 +0000",
          "title": "Please send help. I can’t stop playing these roguelikes.",
          "standfirst": "2025 was a very good year for my favorite genre.",
          "content": "It’s time to admit, before God and the good readers of Ars Technica, that I have a problem. I love roguelikes. Reader, I can’t get enough of them. If there’s even a whisper of a hot new roguelike on Steam, I’m there. You may call them arcane, repetitive, or maddeningly difficult; I call them heaven. The second best part of video games is taking a puny little character and, over 100 hours, transforming that adventurer into a god of destruction. The best thing about video games is doing the same thing in under an hour. Beat a combat encounter, get an upgrade. Enter a new area, choose a new item. Put together a build and watch it sing. If you die—immediately ending your ascent and returning you to the beginning of the game—you’ll often make a pit stop at a home base to unlock new goodies to help you on your next run. (Some people distiguish between roguelikes and “roguelites,” with the latter including permanent, between-run upgrades. For simplicity’s sake, I’ll use “roguelike” as an umbrella term).Read full article Comments",
          "feed_position": 17,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/hades2_dec22_01-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/hades2_dec22_01-1152x648.jpg",
      "popularity_score": 133
    },
    {
      "id": "cluster_132",
      "coverage": 1,
      "updated_at": "Mon, 08 Dec 2025 17:01:37 +0000",
      "title": "F1 in Abu Dhabi: And that’s the championship",
      "neutral_headline": "F1 in Abu Dhabi: And that’s the championship",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/cars/2025/12/f1-in-abu-dhabi-and-thats-the-championship/",
          "published_at": "Mon, 08 Dec 2025 17:01:37 +0000",
          "title": "F1 in Abu Dhabi: And that’s the championship",
          "standfirst": "A three-way fight down to the wire as the ground effect era comes to a close.",
          "content": "The 2025 Formula 1 World Championship drew to a close this past weekend in Abu Dhabi, and with it came the end of the current generation of cars. After a grueling 24 races, the title was decided in a three-way fight by the finest of margins; just two points, less than half a percent, separated the winning driver from second place when the checkered flag waved on Sunday. Coming into Abu Dhabi, McLaren’s Lando Norris was, if not a comfortable favorite, then at least the driver with the highest odds of prevailing. After a strong start to the season, the British driver’s form dipped at the Dutch Grand Prix. But he bounced back, retaking the championship lead from his Australian teammate Oscar Piastri in Mexico in October. For much of the season, it seemed to be a two-car race. McLaren had a clear car advantage and two strong drivers, suggesting a repeat of the years we saw Lewis Hamilton and Nico Rosberg duking it out to bring home titles for Mercedes. But that didn’t figure on Red Bull developing its car late in the season. New boss Laurent Mekies has revitalized the energy drinks squad, and four-time champion Max Verstappen was able to close inexorably toward the McLaren drivers in the points with a string of sublime performances.Read full article Comments",
          "feed_position": 13,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2250341495-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2250341495-1152x648.jpg",
      "popularity_score": 130
    },
    {
      "id": "cluster_136",
      "coverage": 1,
      "updated_at": "Mon, 08 Dec 2025 15:49:32 +0000",
      "title": "A big bike on a budget: Lectric’s XPress 750",
      "neutral_headline": "A big bike on a budget: Lectric’s XPress 750",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/cars/2025/12/lectric-xpress-750-a-full-sized-bike-for-the-budget-minded/",
          "published_at": "Mon, 08 Dec 2025 15:49:32 +0000",
          "title": "A big bike on a budget: Lectric’s XPress 750",
          "standfirst": "A budget e-bike that offers more than you might expect.",
          "content": "Almost every bit of bike testing I’ve done starts out the same way. After assembling the bike, I set the seatpost to its maximum recommended height, take it on a short test ride, and try to figure out new and creative phrasing to describe the same old problem: The frame isn’t quite big enough to accommodate my legs. While I’m on the tall side at a bit over 6 feet (~190 cm), I’m definitely not abnormally large. Yet very few e-bike manufacturers seem to be interested in giving people my height a comfortable ride. So imagine my surprise when, within two blocks of my first ride on the XPress 750, I had to pull off to the side of the street and lower the seat. This was especially notable given that the XPress is a budget bike (currently on sale for just under $1,000.00) that is only offered in a single frame size. So kudos to Lectric for giving me a comfortable and enjoyable ride, and doing so with a lot of features I wouldn’t expect at this price point. That said, hitting that price necessitated some significant compromises. We’ll discuss those in detail so you can get a sense of whether any of them will get in the way of your riding enjoyment.Read full article Comments",
          "feed_position": 14,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/IMG_1802-1152x648.jpeg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/IMG_1802-1152x648.jpeg",
      "popularity_score": 130
    },
    {
      "id": "cluster_139",
      "coverage": 1,
      "updated_at": "Mon, 08 Dec 2025 14:03:30 +0000",
      "title": "The Boys gears up for a supe-ocalypse in S5 teaser",
      "neutral_headline": "The Boys gears up for a supe-ocalypse in S5 teaser",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/culture/2025/12/the-boys-gears-up-for-a-supe-ocalypse-in-s5-teaser/",
          "published_at": "Mon, 08 Dec 2025 14:03:30 +0000",
          "title": "The Boys gears up for a supe-ocalypse in S5 teaser",
          "standfirst": "\"So how about it, you lot? One last go?\"",
          "content": "Prime Video dropped an extended teaser for the fifth and final season of The Boys—based on the comic book series of the same name by Garth Ennis and Darick Robertson—during CCXP in Sao Paulo, Brazil. And it looks like we’re getting nothing less than a full-on Supe-ocalypse as an all-powerful Homelander seeks revenge on The Boys. (Spoilers for prior seasons of The Boys and S2 of Gen V below.) Things were not looking good for our antiheroes after the S4 finale. They managed to thwart the assassination of newly elected US President Robert Singer, but new Vought CEO/evil supe Sister Sage (Susan Heyward) essentially overthrew the election and installed Senator Steve Calhoun (David Andrews) as president. Calhoun declared martial law, and naturally, Homelander (Antony “Give Him an Emmy Already” Starr) swore loyalty as his chief enforcer. Butcher (Karl Urban) and Annie (Erin Moriarty) escaped, but the rest of The Boys were rounded up and placed in re-education—er, “Freedom”—camps.Read full article Comments",
          "feed_position": 16,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/boys2-1152x648-1765118776.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/boys2-1152x648-1765118776.jpg",
      "popularity_score": 130
    }
  ]
}