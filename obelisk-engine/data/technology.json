{
  "updated_at": "2025-11-05T03:43:51.171Z",
  "clusters": [
    {
      "id": "cluster_32",
      "coverage": 2,
      "updated_at": "Tue, 04 Nov 2025 21:54:45 +0000",
      "title": "Amazon and Perplexity are fighting over the future of AI shopping",
      "neutral_headline": "Amazon and Perplexity Dispute AI Shopping Browser Capabilities",
      "items": [
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/ai/amazon-and-perplexity-are-fighting-over-the-future-of-ai-shopping-215445479.html",
          "published_at": "Tue, 04 Nov 2025 21:54:45 +0000",
          "title": "Amazon and Perplexity are fighting over the future of AI shopping",
          "standfirst": "Amazon has sent a cease-and-desist letter to Perplexity that demands that the AI startup prevents its Comet browser from making purchases on Amazon, Bloomberg reports. In a blog post responding to Amazon's letter, Perplexity claims Amazon is \"bullying\" the company and that its demands pose \"a threat to all internet users.\"In Amazon's eyes, Comet's agent violates its terms of service, degrades the Amazon shopping experience and introduces privacy vulnerabilities, Bloomberg writes. Amazon's \"Conditions of Use\" for Amazon.com specifically prohibit \"any downloading, copying, or other use of account information for the benefit of any third party\" and \"any use of data mining, robots, or similar data gathering and extraction tools.\" Depending on your definition, the agentic capabilities Perplexity offers through Comet could violate both clauses. The browser securely stores log-in credentials for websites locally, and uses them to make purchases for customers on Amazon with a simple command. Perplexity and Amazon agreed to pause agentic shopping on Amazon in November 2024, according to the report, but when Comet was released, Perplexity allowed it again. By representing the Comet agent as a Chrome browser user rather than a bot, the company allegedly tried to get around the agreement, until Amazon found out and sent its cease-and-desist letter. Amazon posted the statement below on its blog, openly acknowledging the issues it has with Perplexity: We think it’s fairly straightforward that third-party applications that offer to make purchases on behalf of customers from other businesses should operate openly and respect service provider decisions whether or not to participate. This helps ensure a positive customer experience and it is how others operate, including food delivery apps and the restaurants they take orders for, delivery service apps and the stores they shop from, and online travel agencies and the airlines they book tickets with for customers. Agentic third-party applications such as Perplexity’s Comet have the same obligations, and we’ve repeatedly requested that Perplexity remove Amazon from the Comet experience, particularly in light of the significantly degraded shopping and customer service experience it provides.Complicating Amazon’s claims, Perplexity might be a future shopping rival. Amazon demoed its own AI shopping agent called “Buy for Me” in April 2025. But Perplexity also disagrees with the fundamentals of Amazon's argument. \"User agents are exactly that: agents of the user,\" Perplexity says. \"They're distinct from crawlers, scrapers, or bots.\" Perplexity believes the Comet agent shouldn't run afoul of Amazon's terms and conditions then because it acts on the users' behalf, with the users' permission. This isn't the first time Perplexity has been accused of misrepresenting its AI tools to access content. In August, Cloudflare claimed that the company's bots were accessing blocked websites by pretending to be a normal Chrome browser user on macOS. Reddit also sued Perplexity and three other companies earlier this month for accessing Reddit posts without paying for a license.This article originally appeared on Engadget at https://www.engadget.com/ai/amazon-and-perplexity-are-fighting-over-the-future-of-ai-shopping-215445479.html?src=rss",
          "content": "Amazon has sent a cease-and-desist letter to Perplexity that demands that the AI startup prevents its Comet browser from making purchases on Amazon, Bloomberg reports. In a blog post responding to Amazon's letter, Perplexity claims Amazon is \"bullying\" the company and that its demands pose \"a threat to all internet users.\"In Amazon's eyes, Comet's agent violates its terms of service, degrades the Amazon shopping experience and introduces privacy vulnerabilities, Bloomberg writes. Amazon's \"Conditions of Use\" for Amazon.com specifically prohibit \"any downloading, copying, or other use of account information for the benefit of any third party\" and \"any use of data mining, robots, or similar data gathering and extraction tools.\" Depending on your definition, the agentic capabilities Perplexity offers through Comet could violate both clauses. The browser securely stores log-in credentials for websites locally, and uses them to make purchases for customers on Amazon with a simple command. Perplexity and Amazon agreed to pause agentic shopping on Amazon in November 2024, according to the report, but when Comet was released, Perplexity allowed it again. By representing the Comet agent as a Chrome browser user rather than a bot, the company allegedly tried to get around the agreement, until Amazon found out and sent its cease-and-desist letter. Amazon posted the statement below on its blog, openly acknowledging the issues it has with Perplexity: We think it’s fairly straightforward that third-party applications that offer to make purchases on behalf of customers from other businesses should operate openly and respect service provider decisions whether or not to participate. This helps ensure a positive customer experience and it is how others operate, including food delivery apps and the restaurants they take orders for, delivery service apps and the stores they shop from, and online travel agencies and the airlines they book tickets with for customers. Agentic third-party applications such as Perplexity’s Comet have the same obligations, and we’ve repeatedly requested that Perplexity remove Amazon from the Comet experience, particularly in light of the significantly degraded shopping and customer service experience it provides.Complicating Amazon’s claims, Perplexity might be a future shopping rival. Amazon demoed its own AI shopping agent called “Buy for Me” in April 2025. But Perplexity also disagrees with the fundamentals of Amazon's argument. \"User agents are exactly that: agents of the user,\" Perplexity says. \"They're distinct from crawlers, scrapers, or bots.\" Perplexity believes the Comet agent shouldn't run afoul of Amazon's terms and conditions then because it acts on the users' behalf, with the users' permission. This isn't the first time Perplexity has been accused of misrepresenting its AI tools to access content. In August, Cloudflare claimed that the company's bots were accessing blocked websites by pretending to be a normal Chrome browser user on macOS. Reddit also sued Perplexity and three other companies earlier this month for accessing Reddit posts without paying for a license.This article originally appeared on Engadget at https://www.engadget.com/ai/amazon-and-perplexity-are-fighting-over-the-future-of-ai-shopping-215445479.html?src=rss",
          "feed_position": 2
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/databricks-research-reveals-that-building-better-ai-judges-isnt-just-a",
          "published_at": "Tue, 04 Nov 2025 20:00:00 GMT",
          "title": "Databricks research reveals that building better AI judges isn't just a technical concern, it's a people problem",
          "standfirst": "The intelligence of AI models isn&#x27;t what&#x27;s blocking enterprise deployments. It&#x27;s the inability to define and measure quality in the first place.That&#x27;s where AI judges are now playing an increasingly important role. In AI evaluation, a \"judge\" is an AI system that scores outputs from another AI system. Judge Builder is Databricks&#x27; framework for creating judges and was first deployed as part of the company&#x27;s Agent Bricks technology earlier this year. The framework has evolved significantly since its initial launch in response to direct user feedback and deployments.Early versions focused on technical implementation but customer feedback revealed the real bottleneck was organizational alignment. Databricks now offers a structured workshop process that guides teams through three core challenges: getting stakeholders to agree on quality criteria, capturing domain expertise from limited subject matter experts and deploying evaluation systems at scale.\"The intelligence of the model is typically not the bottleneck, the models are really smart,\" Jonathan Frankle, Databricks&#x27; chief AI scientist, told VentureBeat in an exclusive briefing. \"Instead, it&#x27;s really about asking, how do we get the models to do what we want, and how do we know if they did what we wanted?\"The &#x27;Ouroboros problem&#x27; of AI evaluationJudge Builder addresses what Pallavi Koppol, a Databricks research scientist who led the development, calls the \"Ouroboros problem.\" An Ouroboros is an ancient symbol that depicts a snake eating its own tail. Using AI systems to evaluate AI systems creates a circular validation challenge.\"You want a judge to see if your system is good, if your AI system is good, but then your judge is also an AI system,\" Koppol explained. \"And now you&#x27;re saying like, well, how do I know this judge is good?\"The solution is measuring \"distance to human expert ground truth\" as the primary scoring function. By minimizing the gap between how an AI judge scores outputs versus how domain experts would score them, organizations can trust these judges as scalable proxies for human evaluation.This approach differs fundamentally from traditional guardrail systems or single-metric evaluations. Rather than asking whether an AI output passed or failed on a generic quality check, Judge Builder creates highly specific evaluation criteria tailored to each organization&#x27;s domain expertise and business requirements.The technical implementation also sets it apart. Judge Builder integrates with Databricks&#x27; MLflow and prompt optimization tools and can work with any underlying model. Teams can version control their judges, track performance over time and deploy multiple judges simultaneously across different quality dimensions.Lessons learned: Building judges that actually workDatabricks&#x27; work with enterprise customers revealed three critical lessons that apply to anyone building AI judges.Lesson one: Your experts don&#x27;t agree as much as you think. When quality is subjective, organizations discover that even their own subject matter experts disagree on what constitutes acceptable output. A customer service response might be factually correct but use an inappropriate tone. A financial summary might be comprehensive but too technical for the intended audience.\"One of the biggest lessons of this whole process is that all problems become people problems,\" Frankle said. \"The hardest part is getting an idea out of a person&#x27;s brain and into something explicit. And the harder part is that companies are not one brain, but many brains.\"The fix is batched annotation with inter-rater reliability checks. Teams annotate examples in small groups, then measure agreement scores before proceeding. This catches misalignment early. In one case, three experts gave ratings of 1, 5 and neutral for the same output before discussion revealed they were interpreting the evaluation criteria differently.Companies using this approach achieve inter-rater reliability scores as high as 0.6 compared to typical scores of 0.3 from external annotation services. Higher agreement translates directly to better judge performance because the training data contains less noise.Lesson two: Break down vague criteria into specific judges. Instead of one judge evaluating whether a response is \"relevant, factual and concise,\" create three separate judges. Each targets a specific quality aspect. This granularity matters because a failing \"overall quality\" score reveals something is wrong but not what to fix.The best results come from combining top-down requirements such as regulatory constraints, stakeholder priorities, with bottom-up discovery of observed failure patterns. One customer built a top-down judge for correctness but discovered through data analysis that correct responses almost always cited the top two retrieval results. This insight became a new production-friendly judge that could proxy for correctness without requiring ground-truth labels.Lesson three: You need fewer examples than you think. Teams can create robust judges from just 20-30 well-chosen examples. The key is selecting edge cases that expose disagreement rather than obvious examples where everyone agrees.\"We&#x27;re able to run this process with some teams in as little as three hours, so it doesn&#x27;t really take that long to start getting a good judge,\" Koppol said.Production results: From pilots to seven-figure deploymentsFrankle shared three metrics Databricks uses to measure Judge Builder&#x27;s success: whether customers want to use it again, whether they increase AI spending and whether they progress further in their AI journey.On the first metric, one customer created more than a dozen judges after their initial workshop. \"This customer made more than a dozen judges after we walked them through doing this in a rigorous way for the first time with this framework,\" Frankle said. \"They really went to town on judges and are now measuring everything.\"For the second metric, the business impact is clear. \"There are multiple customers who have gone through this workshop and have become seven-figure spenders on GenAI at Databricks in a way that they weren&#x27;t before,\" Frankle said.The third metric reveals Judge Builder&#x27;s strategic value. Customers who previously hesitated to use advanced techniques like reinforcement learning now feel confident deploying them because they can measure whether improvements actually occurred.\"There are customers who have gone and done very advanced things after having had these judges where they were reluctant to do so before,\" Frankle said. \"They&#x27;ve moved from doing a little bit of prompt engineering to doing reinforcement learning with us. Why spend the money on reinforcement learning, and why spend the energy on reinforcement learning if you don&#x27;t know whether it actually made a difference?\"What enterprises should do nowThe teams successfully moving AI from pilot to production treat judges not as one-time artifacts but as evolving assets that grow with their systems.Databricks recommends three practical steps. First, focus on high-impact judges by identifying one critical regulatory requirement plus one observed failure mode. These become your initial judge portfolio.Second, create lightweight workflows with subject matter experts. A few hours reviewing 20-30 edge cases provides sufficient calibration for most judges. Use batched annotation and inter-rater reliability checks to denoise your data.Third, schedule regular judge reviews using production data. New failure modes will emerge as your system evolves. Your judge portfolio should evolve with them.\"A judge is a way to evaluate a model, it&#x27;s also a way to create guardrails, it&#x27;s also a way to have a metric against which you can do prompt optimization and it&#x27;s also a way to have a metric against which you can do reinforcement learning,\" Frankle said. \"Once you have a judge that you know represents your human taste in an empirical form that you can query as much as you want, you can use it in 10,000 different ways to measure or improve your agents.\"",
          "content": "The intelligence of AI models isn&#x27;t what&#x27;s blocking enterprise deployments. It&#x27;s the inability to define and measure quality in the first place.That&#x27;s where AI judges are now playing an increasingly important role. In AI evaluation, a \"judge\" is an AI system that scores outputs from another AI system. Judge Builder is Databricks&#x27; framework for creating judges and was first deployed as part of the company&#x27;s Agent Bricks technology earlier this year. The framework has evolved significantly since its initial launch in response to direct user feedback and deployments.Early versions focused on technical implementation but customer feedback revealed the real bottleneck was organizational alignment. Databricks now offers a structured workshop process that guides teams through three core challenges: getting stakeholders to agree on quality criteria, capturing domain expertise from limited subject matter experts and deploying evaluation systems at scale.\"The intelligence of the model is typically not the bottleneck, the models are really smart,\" Jonathan Frankle, Databricks&#x27; chief AI scientist, told VentureBeat in an exclusive briefing. \"Instead, it&#x27;s really about asking, how do we get the models to do what we want, and how do we know if they did what we wanted?\"The &#x27;Ouroboros problem&#x27; of AI evaluationJudge Builder addresses what Pallavi Koppol, a Databricks research scientist who led the development, calls the \"Ouroboros problem.\" An Ouroboros is an ancient symbol that depicts a snake eating its own tail. Using AI systems to evaluate AI systems creates a circular validation challenge.\"You want a judge to see if your system is good, if your AI system is good, but then your judge is also an AI system,\" Koppol explained. \"And now you&#x27;re saying like, well, how do I know this judge is good?\"The solution is measuring \"distance to human expert ground truth\" as the primary scoring function. By minimizing the gap between how an AI judge scores outputs versus how domain experts would score them, organizations can trust these judges as scalable proxies for human evaluation.This approach differs fundamentally from traditional guardrail systems or single-metric evaluations. Rather than asking whether an AI output passed or failed on a generic quality check, Judge Builder creates highly specific evaluation criteria tailored to each organization&#x27;s domain expertise and business requirements.The technical implementation also sets it apart. Judge Builder integrates with Databricks&#x27; MLflow and prompt optimization tools and can work with any underlying model. Teams can version control their judges, track performance over time and deploy multiple judges simultaneously across different quality dimensions.Lessons learned: Building judges that actually workDatabricks&#x27; work with enterprise customers revealed three critical lessons that apply to anyone building AI judges.Lesson one: Your experts don&#x27;t agree as much as you think. When quality is subjective, organizations discover that even their own subject matter experts disagree on what constitutes acceptable output. A customer service response might be factually correct but use an inappropriate tone. A financial summary might be comprehensive but too technical for the intended audience.\"One of the biggest lessons of this whole process is that all problems become people problems,\" Frankle said. \"The hardest part is getting an idea out of a person&#x27;s brain and into something explicit. And the harder part is that companies are not one brain, but many brains.\"The fix is batched annotation with inter-rater reliability checks. Teams annotate examples in small groups, then measure agreement scores before proceeding. This catches misalignment early. In one case, three experts gave ratings of 1, 5 and neutral for the same output before discussion revealed they were interpreting the evaluation criteria differently.Companies using this approach achieve inter-rater reliability scores as high as 0.6 compared to typical scores of 0.3 from external annotation services. Higher agreement translates directly to better judge performance because the training data contains less noise.Lesson two: Break down vague criteria into specific judges. Instead of one judge evaluating whether a response is \"relevant, factual and concise,\" create three separate judges. Each targets a specific quality aspect. This granularity matters because a failing \"overall quality\" score reveals something is wrong but not what to fix.The best results come from combining top-down requirements such as regulatory constraints, stakeholder priorities, with bottom-up discovery of observed failure patterns. One customer built a top-down judge for correctness but discovered through data analysis that correct responses almost always cited the top two retrieval results. This insight became a new production-friendly judge that could proxy for correctness without requiring ground-truth labels.Lesson three: You need fewer examples than you think. Teams can create robust judges from just 20-30 well-chosen examples. The key is selecting edge cases that expose disagreement rather than obvious examples where everyone agrees.\"We&#x27;re able to run this process with some teams in as little as three hours, so it doesn&#x27;t really take that long to start getting a good judge,\" Koppol said.Production results: From pilots to seven-figure deploymentsFrankle shared three metrics Databricks uses to measure Judge Builder&#x27;s success: whether customers want to use it again, whether they increase AI spending and whether they progress further in their AI journey.On the first metric, one customer created more than a dozen judges after their initial workshop. \"This customer made more than a dozen judges after we walked them through doing this in a rigorous way for the first time with this framework,\" Frankle said. \"They really went to town on judges and are now measuring everything.\"For the second metric, the business impact is clear. \"There are multiple customers who have gone through this workshop and have become seven-figure spenders on GenAI at Databricks in a way that they weren&#x27;t before,\" Frankle said.The third metric reveals Judge Builder&#x27;s strategic value. Customers who previously hesitated to use advanced techniques like reinforcement learning now feel confident deploying them because they can measure whether improvements actually occurred.\"There are customers who have gone and done very advanced things after having had these judges where they were reluctant to do so before,\" Frankle said. \"They&#x27;ve moved from doing a little bit of prompt engineering to doing reinforcement learning with us. Why spend the money on reinforcement learning, and why spend the energy on reinforcement learning if you don&#x27;t know whether it actually made a difference?\"What enterprises should do nowThe teams successfully moving AI from pilot to production treat judges not as one-time artifacts but as evolving assets that grow with their systems.Databricks recommends three practical steps. First, focus on high-impact judges by identifying one critical regulatory requirement plus one observed failure mode. These become your initial judge portfolio.Second, create lightweight workflows with subject matter experts. A few hours reviewing 20-30 edge cases provides sufficient calibration for most judges. Use batched annotation and inter-rater reliability checks to denoise your data.Third, schedule regular judge reviews using production data. New failure modes will emerge as your system evolves. Your judge portfolio should evolve with them.\"A judge is a way to evaluate a model, it&#x27;s also a way to create guardrails, it&#x27;s also a way to have a metric against which you can do prompt optimization and it&#x27;s also a way to have a metric against which you can do reinforcement learning,\" Frankle said. \"Once you have a judge that you know represents your human taste in an empirical form that you can query as much as you want, you can use it in 10,000 different ways to measure or improve your agents.\"",
          "feed_position": 0,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/75UWyAdt4L6TmmJjkbVDJu/96a54479b06fb94b7d13366ad4f046af/ouroboros-ai-smk.jpg?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/attention-isnt-all-you-need-new-qwen3-variant-brumby-14b-base-leverages",
          "published_at": "Tue, 04 Nov 2025 19:37:00 GMT",
          "title": "Attention ISN'T all you need?! New Qwen3 variant Brumby-14B-Base leverages Power Retention technique",
          "standfirst": "When the transformer architecture was introduced in 2017 in the now seminal Google paper \"Attention Is All You Need,\" it became an instant cornerstone of modern artificial intelligence. Every major large language model (LLM) — from OpenAI&#x27;s GPT series to Anthropic&#x27;s Claude, Google&#x27;s Gemini, and Meta&#x27;s Llama — has been built on some variation of its central mechanism: attention, the mathematical operation that allows a model to look back across its entire input and decide what information matters most.Eight years later, the same mechanism that defined AI’s golden age is now showing its limits. Attention is powerful, but it is also expensive — its computational and memory costs scale quadratically with context length, creating an increasingly unsustainable bottleneck for both research and industry. As models aim to reason across documents, codebases, or video streams lasting hours or days, attention becomes the architecture’s Achilles’ heel.On October 28, 2025, the little-known AI startup Manifest AI introduced a radical alternative. Their new model, Brumby-14B-Base, is a retrained variant of Qwen3-14B-Base, one of the leading open-source transformer models.But while many variants of Qwen have been trained already, Brumby-14B-Base is novel in that it abandons attention altogether. Instead, Brumby replaces those layers with a novel mechanism called Power Retention—a recurrent, hardware-efficient architecture that stores and updates information over arbitrarily long contexts without the exponential memory growth of attention.Trained at a stated cost of just $4,000, the 14-billion-parameter Brumby model performs on par with established transformer models like Qwen3-14B and GLM-4.5-Air, achieving near-state-of-the-art accuracy on a range of reasoning and comprehension benchmarks.From Attention to Retention: The Architectural ShiftThe core of Manifest AI’s innovation lies in what they call the Power Retention layer. In a traditional transformer, every token computes a set of queries (Q), keys (K), and values (V), then performs a matrix operation that measures the similarity between every token and every other token—essentially a full pairwise comparison across the sequence. This is what gives attention its flexibility, but also what makes it so costly: processing a sequence twice as long takes roughly four times the compute and memory.Power Retention keeps the same inputs (Q, K, V), but replaces the global similarity operation with a recurrent state update. Each layer maintains a memory matrix S, which is updated at each time step according to the incoming key, value, and a learned gating signal. The process looks more like an RNN (Recurrent Neural Network) than a transformer: instead of recomputing attention over the entire context, the model continuously compresses past information into a fixed-size latent state.This means the computational cost of Power Retention does not grow with context length. Whether the model is processing 1,000 or 1,000,000 tokens, the per-token cost remains constant. That property alone—constant-time per-token computation—marks a profound departure from transformer behavior.At the same time, Power Retention preserves the expressive power that made attention successful. Because the recurrence involves tensor powers of the input (hence the name “power retention”), it can represent higher-order dependencies between past and present tokens. The result is an architecture that can theoretically retain long-term dependencies indefinitely, while remaining as efficient as an RNN and as expressive as a transformer.Retraining, Not RebuildingPerhaps the most striking aspect of Brumby-14B’s training process is its efficiency. Manifest AI trained the model for only 60 hours on 32 Nvidia H100 GPUs, at a cost of roughly $4,000 — less than 2% of what a conventional model of this scale would cost to train from scratch.However, since it relied on a transformer-based model, it&#x27;s safe to say that this advance alone will not end the transformer AI-era.As Jacob Buckman, founder of Manifest AI, clarified in an email to VentureBeat: “The ability to train for $4,000 is indeed only possible when leveraging an existing transformer model,” he said. “Brumby could not be trained from scratch for that price.”Still, Buckman emphasized the significance of that result: “The reason this is important is that the ability to build on the weights of the previous generation of model architectures is a critical accelerant for the adoption of a new modeling paradigm.” He argues this demonstrates how attention-free systems can catch up to transformer performance “for orders-of-magnitude less” investment.In the loss curves released by Manifest AI, Brumby’s training loss quickly converges to that of the Qwen3 baseline within 3,000 training steps, even as the architecture diverges significantly from its transformer origins. Although Brumby-14B-Base began life as Qwen3-14B-Base, it did not remain identical for long. Manifest AI fundamentally altered Qwen3’s architecture by removing its attention layers—the mathematical engine that defines how a transformer model processes information—and replacing them with their new “power retention” mechanism. This change restructured the model’s internal wiring, effectively giving it a new brain while preserving much of its prior knowledge.Because of that architectural swap, the existing Qwen3 weights no longer fit perfectly. They were trained to operate within a transformer’s attention dynamics, not the new retention-based system. As a result, the Brumby model initially “forgot” how to apply some of its learned knowledge effectively. The retraining process—about 3,000 steps of additional learning—served to recalibrate those weights, aligning them with the power retention framework without having to start from zero.A helpful way to think about this is to imagine taking a world-class pianist and handing them a guitar. They already understand rhythm, harmony, and melody, but their hands must learn entirely new patterns to produce the same music. Similarly, Brumby had to relearn how to use its existing knowledge through a new computational instrument. Those 3,000 training steps were, in effect, its crash course in guitar lessons.By the end of this short retraining phase, Brumby had regained its full performance, reaching the same accuracy as the original Qwen3 model. That quick recovery is what makes the result so significant: it shows that an attention-free system can inherit and adapt the capabilities of a transformer model with only a fraction of the training time and cost.The benchmark progression plots show a similar trend: the model rapidly approaches its target accuracy on core evaluations like GSM8K, HellaSwag, and MMLU after only a few thousand steps, matching or even slightly surpassing Qwen3 on several tasks.Benchmarking the BrumbyAcross standard evaluation tasks, Brumby-14B-Base consistently performs at or near parity with transformer baselines of comparable scale.TaskBrumby-14BQwen3-14BGLM-4.5-AirNemotron Nano (12B)ARC0.890.940.920.93GSM8K0.880.840.830.84GSM8K (Platinum)0.870.880.850.87HellaSwag0.770.810.850.82MATH0.620.540.470.26MBPP0.570.750.730.71MMLU0.710.780.770.78MMLU (Pro)0.360.550.510.53While it lags slightly behind transformers on knowledge-heavy evaluations like MMLU-Pro, it matches or outperforms them on mathematical reasoning and long-context reasoning tasks—precisely where attention architectures tend to falter. This pattern reinforces the idea that recurrent or retention-based systems may hold a structural advantage for reasoning over extended temporal or logical dependencies.Hardware Efficiency and Inference PerformanceBrumby’s power retention design offers another major advantage: hardware efficiency.Because the state update involves only local matrix operations, inference can be implemented with linear complexity in sequence length. Manifest AI reports that their fastest kernels, developed through their in-house CUDA framework Vidrial, can deliver hundreds-fold speedups over attention on very long contexts.Buckman said the alpha-stage Power Retention kernels “achieve typical hardware utilization of 80–85%, which is higher than FlashAttention2’s 70–75% or Mamba’s 50–60%.” (Mamba is another emerging “post-transformer” architecture developed by Carnegie Mellon scientists back in 2023 that, like Power Retention, seeks to eliminate the computational bottleneck of attention. It replaces attention with a state-space mechanism that processes sequences linearly — updating an internal state over time rather than comparing every token to every other one. This makes it far more efficient for long inputs, though it typically achieves lower hardware utilization than Power Retention in early tests.)Both Power Retention and Mamba, he added, “expend meaningfully fewer total FLOPs than FlashAttention2 on long contexts, as well as far less memory.” According to Buckman, the reported 100× speedup comes from this combined improvement in utilization and computational efficiency, though he noted that “we have not yet stress-tested it on production-scale workloads.”Training and Scaling EconomicsPerhaps no statistic in the Brumby release generated more attention than the training cost.A 14-billion-parameter model, trained for $4,000, represents a two-order-of-magnitude reduction in the cost of foundation model development.Buckman confirmed that the low cost reflects a broader scaling pattern. “Far from diminishing returns, we have found that ease of retraining improves with scale,” he said. “The number of steps required to successfully retrain a model decreases with its parameter count.” Manifest has not yet validated the cost of retraining models at 700B parameters, but Buckman projected a range of $10,000–$20,000 for models of that magnitude—still far below transformer training budgets.He also reiterated that this approach could democratize large-scale experimentation by allowing smaller research groups or companies to retrain or repurpose existing transformer checkpoints without prohibitive compute costs.Integration and DeploymentAccording to Buckman, converting an existing transformer into a Power Retention model is designed to be simple. “It is straightforward for any company that is already retraining, post-training, or fine-tuning open-source models,” he said. “Simply pip install retention, change one line of your architecture code, and resume training where you left off.”He added that after only a small number of GPU-hours, the model typically recovers its original performance—at which point it gains the efficiency benefits of the attention-free design. “The resulting architecture will permit far faster long-context training and inference than previously,” Buckman noted.On infrastructure, Buckman said the main Brumby kernels are written in Triton, compatible with both NVIDIA and AMD accelerators. Specialized CUDA kernels are also available through the team’s in-house Vidrial framework. Integration with vLLM and other inference engines remains a work in progress: “We have not yet integrated Power Retention into inference engines, but doing so is a major ongoing initiative at Manifest.”As for distributed inference, Buckman dismissed concerns about instability: “We have not found this difficulty to be exacerbated in any way by our recurrent-state architecture. In fact, context-parallel training and GPU partitioning for multi-user inference both become significantly cleaner technically when using our approach.”Mission and Long-Term VisionBeyond the engineering details, Buckman also described Manifest’s broader mission. “Our mission is to train a neural network to model all human output,” he said. The team’s goal, he explained, is to move beyond modeling “artifacts of intelligence” toward modeling “the intelligent processes that generated them.” This shift, he argued, requires “fundamentally rethinking” how models are designed and trained—work that Power Retention represents only the beginning of.The Brumby-14B release, he said, is “one step forward in a long march” toward architectures that can model thought processes continuously and efficiently.Public Debate and Industry ReceptionThe launch of Brumby-14B sparked immediate discussion on X (formerly Twitter), where researchers debated the framing of Manifest AI’s announcement. Some, including Meta researcher Ariel (@redtachyon), argued that the “$4,000 foundation model” tagline was misleading, since the training involved reusing pretrained transformer weights rather than training from scratch.“They shuffled around the weights of Qwen, fine-tuned it a bit, and called it ‘training a foundation model for $4k,’” Ariel wrote.Buckman responded publicly, clarifying that the initial tweet had been part of a longer thread explaining the retraining approach. “It’s not like I was being deceptive about it,” he wrote. “I broke it up into separate tweets, and now everyone is mad about the first one.”In a follow-up email, Buckman took a measured view of the controversy. “The end of the transformer era is not yet here,” he reiterated, “but the march has begun.” He also acknowledged that the $4,000 claim, though technically accurate in context, had drawn attention precisely because it challenged expectations about what it costs to experiment at frontier scale.Conclusion: A Crack in the Transformer’s Wall?The release of Brumby-14B-Base is more than an engineering milestone; it is a proof of concept that the transformer’s dominance may finally face credible competition. By replacing attention with power retention, Manifest AI has demonstrated that performance parity with state-of-the-art transformers is possible at a fraction of the computational cost—and that the long-context bottleneck can be broken without exotic hardware.The broader implications are twofold. First, the economics of training and serving large models could shift dramatically, lowering the barrier to entry for open research and smaller organizations. Second, the architectural diversity of AI models may expand again, reigniting theoretical and empirical exploration after half a decade of transformer monoculture.As Buckman put it: “The end of the transformer era is not yet here. Our release is just one step forward in a long march toward the future.”",
          "content": "When the transformer architecture was introduced in 2017 in the now seminal Google paper \"Attention Is All You Need,\" it became an instant cornerstone of modern artificial intelligence. Every major large language model (LLM) — from OpenAI&#x27;s GPT series to Anthropic&#x27;s Claude, Google&#x27;s Gemini, and Meta&#x27;s Llama — has been built on some variation of its central mechanism: attention, the mathematical operation that allows a model to look back across its entire input and decide what information matters most.Eight years later, the same mechanism that defined AI’s golden age is now showing its limits. Attention is powerful, but it is also expensive — its computational and memory costs scale quadratically with context length, creating an increasingly unsustainable bottleneck for both research and industry. As models aim to reason across documents, codebases, or video streams lasting hours or days, attention becomes the architecture’s Achilles’ heel.On October 28, 2025, the little-known AI startup Manifest AI introduced a radical alternative. Their new model, Brumby-14B-Base, is a retrained variant of Qwen3-14B-Base, one of the leading open-source transformer models.But while many variants of Qwen have been trained already, Brumby-14B-Base is novel in that it abandons attention altogether. Instead, Brumby replaces those layers with a novel mechanism called Power Retention—a recurrent, hardware-efficient architecture that stores and updates information over arbitrarily long contexts without the exponential memory growth of attention.Trained at a stated cost of just $4,000, the 14-billion-parameter Brumby model performs on par with established transformer models like Qwen3-14B and GLM-4.5-Air, achieving near-state-of-the-art accuracy on a range of reasoning and comprehension benchmarks.From Attention to Retention: The Architectural ShiftThe core of Manifest AI’s innovation lies in what they call the Power Retention layer. In a traditional transformer, every token computes a set of queries (Q), keys (K), and values (V), then performs a matrix operation that measures the similarity between every token and every other token—essentially a full pairwise comparison across the sequence. This is what gives attention its flexibility, but also what makes it so costly: processing a sequence twice as long takes roughly four times the compute and memory.Power Retention keeps the same inputs (Q, K, V), but replaces the global similarity operation with a recurrent state update. Each layer maintains a memory matrix S, which is updated at each time step according to the incoming key, value, and a learned gating signal. The process looks more like an RNN (Recurrent Neural Network) than a transformer: instead of recomputing attention over the entire context, the model continuously compresses past information into a fixed-size latent state.This means the computational cost of Power Retention does not grow with context length. Whether the model is processing 1,000 or 1,000,000 tokens, the per-token cost remains constant. That property alone—constant-time per-token computation—marks a profound departure from transformer behavior.At the same time, Power Retention preserves the expressive power that made attention successful. Because the recurrence involves tensor powers of the input (hence the name “power retention”), it can represent higher-order dependencies between past and present tokens. The result is an architecture that can theoretically retain long-term dependencies indefinitely, while remaining as efficient as an RNN and as expressive as a transformer.Retraining, Not RebuildingPerhaps the most striking aspect of Brumby-14B’s training process is its efficiency. Manifest AI trained the model for only 60 hours on 32 Nvidia H100 GPUs, at a cost of roughly $4,000 — less than 2% of what a conventional model of this scale would cost to train from scratch.However, since it relied on a transformer-based model, it&#x27;s safe to say that this advance alone will not end the transformer AI-era.As Jacob Buckman, founder of Manifest AI, clarified in an email to VentureBeat: “The ability to train for $4,000 is indeed only possible when leveraging an existing transformer model,” he said. “Brumby could not be trained from scratch for that price.”Still, Buckman emphasized the significance of that result: “The reason this is important is that the ability to build on the weights of the previous generation of model architectures is a critical accelerant for the adoption of a new modeling paradigm.” He argues this demonstrates how attention-free systems can catch up to transformer performance “for orders-of-magnitude less” investment.In the loss curves released by Manifest AI, Brumby’s training loss quickly converges to that of the Qwen3 baseline within 3,000 training steps, even as the architecture diverges significantly from its transformer origins. Although Brumby-14B-Base began life as Qwen3-14B-Base, it did not remain identical for long. Manifest AI fundamentally altered Qwen3’s architecture by removing its attention layers—the mathematical engine that defines how a transformer model processes information—and replacing them with their new “power retention” mechanism. This change restructured the model’s internal wiring, effectively giving it a new brain while preserving much of its prior knowledge.Because of that architectural swap, the existing Qwen3 weights no longer fit perfectly. They were trained to operate within a transformer’s attention dynamics, not the new retention-based system. As a result, the Brumby model initially “forgot” how to apply some of its learned knowledge effectively. The retraining process—about 3,000 steps of additional learning—served to recalibrate those weights, aligning them with the power retention framework without having to start from zero.A helpful way to think about this is to imagine taking a world-class pianist and handing them a guitar. They already understand rhythm, harmony, and melody, but their hands must learn entirely new patterns to produce the same music. Similarly, Brumby had to relearn how to use its existing knowledge through a new computational instrument. Those 3,000 training steps were, in effect, its crash course in guitar lessons.By the end of this short retraining phase, Brumby had regained its full performance, reaching the same accuracy as the original Qwen3 model. That quick recovery is what makes the result so significant: it shows that an attention-free system can inherit and adapt the capabilities of a transformer model with only a fraction of the training time and cost.The benchmark progression plots show a similar trend: the model rapidly approaches its target accuracy on core evaluations like GSM8K, HellaSwag, and MMLU after only a few thousand steps, matching or even slightly surpassing Qwen3 on several tasks.Benchmarking the BrumbyAcross standard evaluation tasks, Brumby-14B-Base consistently performs at or near parity with transformer baselines of comparable scale.TaskBrumby-14BQwen3-14BGLM-4.5-AirNemotron Nano (12B)ARC0.890.940.920.93GSM8K0.880.840.830.84GSM8K (Platinum)0.870.880.850.87HellaSwag0.770.810.850.82MATH0.620.540.470.26MBPP0.570.750.730.71MMLU0.710.780.770.78MMLU (Pro)0.360.550.510.53While it lags slightly behind transformers on knowledge-heavy evaluations like MMLU-Pro, it matches or outperforms them on mathematical reasoning and long-context reasoning tasks—precisely where attention architectures tend to falter. This pattern reinforces the idea that recurrent or retention-based systems may hold a structural advantage for reasoning over extended temporal or logical dependencies.Hardware Efficiency and Inference PerformanceBrumby’s power retention design offers another major advantage: hardware efficiency.Because the state update involves only local matrix operations, inference can be implemented with linear complexity in sequence length. Manifest AI reports that their fastest kernels, developed through their in-house CUDA framework Vidrial, can deliver hundreds-fold speedups over attention on very long contexts.Buckman said the alpha-stage Power Retention kernels “achieve typical hardware utilization of 80–85%, which is higher than FlashAttention2’s 70–75% or Mamba’s 50–60%.” (Mamba is another emerging “post-transformer” architecture developed by Carnegie Mellon scientists back in 2023 that, like Power Retention, seeks to eliminate the computational bottleneck of attention. It replaces attention with a state-space mechanism that processes sequences linearly — updating an internal state over time rather than comparing every token to every other one. This makes it far more efficient for long inputs, though it typically achieves lower hardware utilization than Power Retention in early tests.)Both Power Retention and Mamba, he added, “expend meaningfully fewer total FLOPs than FlashAttention2 on long contexts, as well as far less memory.” According to Buckman, the reported 100× speedup comes from this combined improvement in utilization and computational efficiency, though he noted that “we have not yet stress-tested it on production-scale workloads.”Training and Scaling EconomicsPerhaps no statistic in the Brumby release generated more attention than the training cost.A 14-billion-parameter model, trained for $4,000, represents a two-order-of-magnitude reduction in the cost of foundation model development.Buckman confirmed that the low cost reflects a broader scaling pattern. “Far from diminishing returns, we have found that ease of retraining improves with scale,” he said. “The number of steps required to successfully retrain a model decreases with its parameter count.” Manifest has not yet validated the cost of retraining models at 700B parameters, but Buckman projected a range of $10,000–$20,000 for models of that magnitude—still far below transformer training budgets.He also reiterated that this approach could democratize large-scale experimentation by allowing smaller research groups or companies to retrain or repurpose existing transformer checkpoints without prohibitive compute costs.Integration and DeploymentAccording to Buckman, converting an existing transformer into a Power Retention model is designed to be simple. “It is straightforward for any company that is already retraining, post-training, or fine-tuning open-source models,” he said. “Simply pip install retention, change one line of your architecture code, and resume training where you left off.”He added that after only a small number of GPU-hours, the model typically recovers its original performance—at which point it gains the efficiency benefits of the attention-free design. “The resulting architecture will permit far faster long-context training and inference than previously,” Buckman noted.On infrastructure, Buckman said the main Brumby kernels are written in Triton, compatible with both NVIDIA and AMD accelerators. Specialized CUDA kernels are also available through the team’s in-house Vidrial framework. Integration with vLLM and other inference engines remains a work in progress: “We have not yet integrated Power Retention into inference engines, but doing so is a major ongoing initiative at Manifest.”As for distributed inference, Buckman dismissed concerns about instability: “We have not found this difficulty to be exacerbated in any way by our recurrent-state architecture. In fact, context-parallel training and GPU partitioning for multi-user inference both become significantly cleaner technically when using our approach.”Mission and Long-Term VisionBeyond the engineering details, Buckman also described Manifest’s broader mission. “Our mission is to train a neural network to model all human output,” he said. The team’s goal, he explained, is to move beyond modeling “artifacts of intelligence” toward modeling “the intelligent processes that generated them.” This shift, he argued, requires “fundamentally rethinking” how models are designed and trained—work that Power Retention represents only the beginning of.The Brumby-14B release, he said, is “one step forward in a long march” toward architectures that can model thought processes continuously and efficiently.Public Debate and Industry ReceptionThe launch of Brumby-14B sparked immediate discussion on X (formerly Twitter), where researchers debated the framing of Manifest AI’s announcement. Some, including Meta researcher Ariel (@redtachyon), argued that the “$4,000 foundation model” tagline was misleading, since the training involved reusing pretrained transformer weights rather than training from scratch.“They shuffled around the weights of Qwen, fine-tuned it a bit, and called it ‘training a foundation model for $4k,’” Ariel wrote.Buckman responded publicly, clarifying that the initial tweet had been part of a longer thread explaining the retraining approach. “It’s not like I was being deceptive about it,” he wrote. “I broke it up into separate tweets, and now everyone is mad about the first one.”In a follow-up email, Buckman took a measured view of the controversy. “The end of the transformer era is not yet here,” he reiterated, “but the march has begun.” He also acknowledged that the $4,000 claim, though technically accurate in context, had drawn attention precisely because it challenged expectations about what it costs to experiment at frontier scale.Conclusion: A Crack in the Transformer’s Wall?The release of Brumby-14B-Base is more than an engineering milestone; it is a proof of concept that the transformer’s dominance may finally face credible competition. By replacing attention with power retention, Manifest AI has demonstrated that performance parity with state-of-the-art transformers is possible at a fraction of the computational cost—and that the long-context bottleneck can be broken without exotic hardware.The broader implications are twofold. First, the economics of training and serving large models could shift dramatically, lowering the barrier to entry for open research and smaller organizations. Second, the architectural diversity of AI models may expand again, reigniting theoretical and empirical exploration after half a decade of transformer monoculture.As Buckman put it: “The end of the transformer era is not yet here. Our release is just one step forward in a long march toward the future.”",
          "feed_position": 1,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/4w8pJoJCpKW8g1eJxxgy3f/c8d0f3a8431956228510e551a2b474f1/aRNZNKxpXqqt3S_iXlQfh_71e40940f61b4bf89d6b5f5cbeafd63e.png?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/audio/speakers/amazon-echo-dot-max-review-disappointing-sound-but-alexa-is-a-star-190000721.html",
          "published_at": "Tue, 04 Nov 2025 19:00:00 +0000",
          "title": "Amazon Echo Dot Max review: Disappointing sound, but Alexa+ is a star",
          "standfirst": "There’s a lot more riding on Amazon’s latest batch of Echo hardware than usual. After all, these are the first devices arriving alongside Alexa+, the AI-powered revamp of the company’s signature voice assistant. And unlike the $220 Echo Studio, the new Echo Dot Max is a more affordable ($100) and compact entry point into using Alexa+. It’s also another example of how confusing Amazon’s hardware lineup can get: the older Echo Pop ($40) and Echo Dot ($50) are also getting Alexa+. The Echo Dot Max isn’t the cheapest Echo, nor is it the best-sounding speaker Amazon sells. So, where does it fit? The best way to think about this new speaker is that it occupies the same spot in Amazon’s lineup as the venerable and discontinued device simply named “Echo.” For years, the Amazon Echo was the company’s main speaker, but it no longer sells one with that pleasantly concise name. The Echo Dot Max steps in at the same price point, though. And while my testing has shown that Alexa+ is a solid step forward, the Dot Max hardware itself isn’t as good as what it replaces. Design The Echo Dot Max feels immediately familiar if you’ve seen any of Amazon’s spherical smart speakers from the last five years or so, but there are numerous design changes here. The 2020 Echo and current Echo Dot have buttons on top for adjusting volume and muting the speaker’s microphone. Those buttons are now on a front-facing panel that is surrounded by the Echo’s signature light ring. While I liked how the light ring encircled the bottom of the Echo, it’s definitely easier to see in this new front-facing position and it’s particularly better at displaying the volume now. Overall, it’s a fairly refined and subtle device, which is exactly what you want from a smart speaker. I tested the graphite model, but you can also get it in white or a much bolder purple. I prefer the fully spherical, globe-like appearance of the Echo Dot, but there’s little to complain about here visually. Amazon's Echo Dot Max speaker. Nathan Ingraham for Engadget I do have some quibbles about the front-facing volume and mute buttons, though. The Echo Dot Max is simply so light that if I press the controls I’ll push the speaker around the shelf it’s on. This is easily solved by putting your hand around it and pressing the buttons with your thumb, but if you reach out to it with your index finger it probably won’t stay in place. I guess you’re supposed to primarily interact with the Echo Dot Max with your voice, but top-mounted buttons would’ve avoided this problem. I imagine this isn’t as much of a problem with the similarly-designed Echo Studio, simply because it’s much larger and three times heavier. Audio quality I’m a pretty big music nerd and I’ve listened to many smart speakers over the years. I’ve come away impressed with the Echo devices I tested in 2018 as well as the 2020 Echo I spoke about earlier. At $100, that Echo punched well above its weight and sounded notably better than the identically-priced HomePod Mini and Nest Audio. Unfortunately, the Echo Dot Max does not match that older speaker’s bonafides. Don’t get me wrong, it sounds just fine — better than a standard Echo Dot and in line with what I’d expect from a $100 speaker. It’s a good bit louder than my HomePod Mini, with plenty of volume to fill a medium-sized room on its own. If you’re looking to really pump music through a bigger room, though, you’re better off looking at the Echo Studio or something like the Sonos Era 100. You can also pair two Echo Dot Max speakers together for stereo playback and increased volume, but I didn’t get to test this so can’t say how it’ll perform in a larger space. Side view of Amazon's Echo Dot Max speaker Nathan Ingraham for Engadget My biggest complaint with the Echo Dot Max is that frequencies often felt a bit smushed together, without a solid bass thump separating itself from the clarity in the mid- and high-range frequencies. It doesn’t provide the most dynamic listening experience. When you look at the difference in the Echo Dot Max’s speaker components compared to the fourth-generation Echo, that’s not a surprise. That older speaker paired a 3-inch woofer with dual 0.8-inch tweeters, while the Dot Max makes do with just one tweeter and a smaller 2.5-inch woofer. I want to reiterate that the Echo Dot Max still sounds good! The various beats, electronic glitches and vocals of Lorde’s “What Was That” came through clearly for the most part, and the modern disco vibes of “Jealous” by The Aces had a nice thump and the instruments were plenty punchy. The heavier, guitar-driven fury of albums like the new Deftones release Private Music and the 33-year-old Dirt by Alice In Chains came through loud and clear, while turning up the volume on the rave-esque vibes of “As Alive As You Need Me To Be” by Nine Inch Nails had plenty of life. But while the overall sound was pleasant enough, further listening made me realize that details like a good snap of a snare could get lost amidst a storm of guitars. It’s a disappointment that Amazon took an undeniable step backwards here. Much like the Echo Studio that we just reviewed, the Echo Dot Max isn’t bad — it’s just underwhelming. It’s a little easier to forgive here, since the Dot Max is more of an all-purpose speaker rather than something designed to provide an exceptional listening experience. Again, it sounds totally fine for a $100 smart speaker, but given Amazon’s past success in providing surprisingly excellent audio, I was hoping for a lot more here. Amazon's Echo Dot Max smart speaker. Nathan Ingraham for Engadget Alexa+ The other piece of the puzzle is, of course, Alexa+, Amazon’s long-awaited update to its digital assistant. Somehow, it’s already been two years since Amazon first showed off the improvements it was working on delivering with Alexa+. But with these new devices, “early access” to the service is pretty easy to come by now. I’ll admit that I’m not much of a voice assistant guy. I think a lot of that comes from not having a very smart home. I’ve been renting for the last eight months after owning a home for almost a decade, and there just hasn’t been much to do with Alexa (or any voice assistant) at the moment. But even without home-based routines to run, lights to control or a smart thermostat to adjust, it was evident how much more conversational and context-aware Alexa+ is now. I did a lot of testing during the World Series and I made it a habit to ask Alexa what was going on in the series. We had several “conversations” about what happened in the previous night’s game, when the next game was happening, who the starting pitchers were and so forth. It was probably the most natural experience I’ve ever had using a voice assistant, even though using my natural language with a speaker still feels awkward. (I definitely said please to Alexa more than once.) Once you’re set up with Alexa+ Early Access, you can use the same updated assistant in the Alexa app on your smartphone, either with your voice or in a chat interface. The chatbot-style Alexa experience is fine, but I actually prefer using my voice, because I felt like it was easier to have a conversation with it and just ask things as they popped into my head. Another good thing about Alexa+ is that it felt fast and responsive. There are short pauses while it thinks about a response, but it usually got back to me quickly enough that continuing that natural language conversation didn’t feel stilted or awkward. Speed is a crucial factor towards making a voice assistant feel responsive, and Alexa+ on the Echo Dot Max hits on that point. Some combo of Amazon’s AZ3 chip and whatever is happening up in its cloud is getting the job done here. As with any voice assistant, Alexa+ is, of course, not perfect. Most basic tasks like setting reminders, checking the weather and playing music all work reliably. But asking for specific songs or albums can sometimes go badly. Occasionally, Alexa wouldn’t be able to find a specific album I was looking for but it would play other songs by the artist; other times it would come up with completely unrelated music. This is an issue I’ve had with all assistants, but I was hoping Alexa+ might be smart enough to avoid getting too far off base. I asked it to “play the latest release by the band Now, Now.” It’s an EP entitled 01 so I had a feeling Alexa might struggle with that. Sure enough, it said “sure, here’s new music from Now, Now” and played one of the songs from that release. Not bad, but not quite right. I then followed up and said “can you play this entire album?” That did not work. Instead, I ended up with the song “Ain’t it Funky Now” performed by legendary jazz guitarist and composer Grant Green. An outstanding recording, sure, but not remotely close to what I was looking for. Even asking “play the album ‘01 EP’ by the band Now, Now” got me Drake’s “Laugh Now Cry Later.” Sigh. Worse than that were the times when Alexa+ just made things up. The Alexa app provides you with little suggestions for things to ask about, like “iconic music duets.” I tapped it, curious to see what it provided, and it pulled up a list of “iconic music duets that have left an indelible mark on the music industry.” Among those was “Smells Like Teen Spirit” performed by the late Kurt Cobain and his wife Courtney Love. This happened? News to me! I followed up and asked for more details and got a response noting that “there isn’t a formal duet of ‘Smells Like Teen Spirit.’” Best I can tell, it popped up because Love sang some unused lyrics from the legendary song on an episode of the 60 Songs That Explain the ‘90s podcast, back in 2023. This is a perfect example of the random inferences AI often draws, and it’s a good reminder that Alexa+, like all AI assistants, can make things up sometimes. This didn’t happen often, but it’s still something you’ll need to look out for. Amazon's Echo Dot Max smart speaker. Nathan Ingraham for Engadget Wrap-up The Echo Dot Max more or less delivers on Amazon’s promises. It sounds better than smaller speakers like the Echo Dot or Pop, and it’s significantly cheaper than the Echo Studio. If you’re at all interested in music, it’s worth stepping up to the Dot Max over the standard Dot. Beyond just better audio, it’s also significantly newer (the Dot was last updated in 2022). Its more modern processor means it should have a longer lifespan than the standard Dot, making it a better option for people who are eager to try out Alexa+. That freshly updated hardware is a reason to consider the Echo Dot Max over similarly priced speakers like the Nest Audio and HomePod Mini, both of which are five years old. And despite Alexa+ dealing with some growing pains, it’s a better option at this moment than the unproven Gemini for Home Google that is rolling out or the old, limited Siri that the HomePod Mini is still stuck with. But the Echo Dot Max still feels like a bit of a missed opportunity to me. The old Echo sounded so good, and this speaker is just not as exciting in comparison. It’s a fine way to interact with Alexa+ and enjoy some tunes, I just wish it sounded a little bit better. This article originally appeared on Engadget at https://www.engadget.com/audio/speakers/amazon-echo-dot-max-review-disappointing-sound-but-alexa-is-a-star-190000721.html?src=rss",
          "content": "There’s a lot more riding on Amazon’s latest batch of Echo hardware than usual. After all, these are the first devices arriving alongside Alexa+, the AI-powered revamp of the company’s signature voice assistant. And unlike the $220 Echo Studio, the new Echo Dot Max is a more affordable ($100) and compact entry point into using Alexa+. It’s also another example of how confusing Amazon’s hardware lineup can get: the older Echo Pop ($40) and Echo Dot ($50) are also getting Alexa+. The Echo Dot Max isn’t the cheapest Echo, nor is it the best-sounding speaker Amazon sells. So, where does it fit? The best way to think about this new speaker is that it occupies the same spot in Amazon’s lineup as the venerable and discontinued device simply named “Echo.” For years, the Amazon Echo was the company’s main speaker, but it no longer sells one with that pleasantly concise name. The Echo Dot Max steps in at the same price point, though. And while my testing has shown that Alexa+ is a solid step forward, the Dot Max hardware itself isn’t as good as what it replaces. Design The Echo Dot Max feels immediately familiar if you’ve seen any of Amazon’s spherical smart speakers from the last five years or so, but there are numerous design changes here. The 2020 Echo and current Echo Dot have buttons on top for adjusting volume and muting the speaker’s microphone. Those buttons are now on a front-facing panel that is surrounded by the Echo’s signature light ring. While I liked how the light ring encircled the bottom of the Echo, it’s definitely easier to see in this new front-facing position and it’s particularly better at displaying the volume now. Overall, it’s a fairly refined and subtle device, which is exactly what you want from a smart speaker. I tested the graphite model, but you can also get it in white or a much bolder purple. I prefer the fully spherical, globe-like appearance of the Echo Dot, but there’s little to complain about here visually. Amazon's Echo Dot Max speaker. Nathan Ingraham for Engadget I do have some quibbles about the front-facing volume and mute buttons, though. The Echo Dot Max is simply so light that if I press the controls I’ll push the speaker around the shelf it’s on. This is easily solved by putting your hand around it and pressing the buttons with your thumb, but if you reach out to it with your index finger it probably won’t stay in place. I guess you’re supposed to primarily interact with the Echo Dot Max with your voice, but top-mounted buttons would’ve avoided this problem. I imagine this isn’t as much of a problem with the similarly-designed Echo Studio, simply because it’s much larger and three times heavier. Audio quality I’m a pretty big music nerd and I’ve listened to many smart speakers over the years. I’ve come away impressed with the Echo devices I tested in 2018 as well as the 2020 Echo I spoke about earlier. At $100, that Echo punched well above its weight and sounded notably better than the identically-priced HomePod Mini and Nest Audio. Unfortunately, the Echo Dot Max does not match that older speaker’s bonafides. Don’t get me wrong, it sounds just fine — better than a standard Echo Dot and in line with what I’d expect from a $100 speaker. It’s a good bit louder than my HomePod Mini, with plenty of volume to fill a medium-sized room on its own. If you’re looking to really pump music through a bigger room, though, you’re better off looking at the Echo Studio or something like the Sonos Era 100. You can also pair two Echo Dot Max speakers together for stereo playback and increased volume, but I didn’t get to test this so can’t say how it’ll perform in a larger space. Side view of Amazon's Echo Dot Max speaker Nathan Ingraham for Engadget My biggest complaint with the Echo Dot Max is that frequencies often felt a bit smushed together, without a solid bass thump separating itself from the clarity in the mid- and high-range frequencies. It doesn’t provide the most dynamic listening experience. When you look at the difference in the Echo Dot Max’s speaker components compared to the fourth-generation Echo, that’s not a surprise. That older speaker paired a 3-inch woofer with dual 0.8-inch tweeters, while the Dot Max makes do with just one tweeter and a smaller 2.5-inch woofer. I want to reiterate that the Echo Dot Max still sounds good! The various beats, electronic glitches and vocals of Lorde’s “What Was That” came through clearly for the most part, and the modern disco vibes of “Jealous” by The Aces had a nice thump and the instruments were plenty punchy. The heavier, guitar-driven fury of albums like the new Deftones release Private Music and the 33-year-old Dirt by Alice In Chains came through loud and clear, while turning up the volume on the rave-esque vibes of “As Alive As You Need Me To Be” by Nine Inch Nails had plenty of life. But while the overall sound was pleasant enough, further listening made me realize that details like a good snap of a snare could get lost amidst a storm of guitars. It’s a disappointment that Amazon took an undeniable step backwards here. Much like the Echo Studio that we just reviewed, the Echo Dot Max isn’t bad — it’s just underwhelming. It’s a little easier to forgive here, since the Dot Max is more of an all-purpose speaker rather than something designed to provide an exceptional listening experience. Again, it sounds totally fine for a $100 smart speaker, but given Amazon’s past success in providing surprisingly excellent audio, I was hoping for a lot more here. Amazon's Echo Dot Max smart speaker. Nathan Ingraham for Engadget Alexa+ The other piece of the puzzle is, of course, Alexa+, Amazon’s long-awaited update to its digital assistant. Somehow, it’s already been two years since Amazon first showed off the improvements it was working on delivering with Alexa+. But with these new devices, “early access” to the service is pretty easy to come by now. I’ll admit that I’m not much of a voice assistant guy. I think a lot of that comes from not having a very smart home. I’ve been renting for the last eight months after owning a home for almost a decade, and there just hasn’t been much to do with Alexa (or any voice assistant) at the moment. But even without home-based routines to run, lights to control or a smart thermostat to adjust, it was evident how much more conversational and context-aware Alexa+ is now. I did a lot of testing during the World Series and I made it a habit to ask Alexa what was going on in the series. We had several “conversations” about what happened in the previous night’s game, when the next game was happening, who the starting pitchers were and so forth. It was probably the most natural experience I’ve ever had using a voice assistant, even though using my natural language with a speaker still feels awkward. (I definitely said please to Alexa more than once.) Once you’re set up with Alexa+ Early Access, you can use the same updated assistant in the Alexa app on your smartphone, either with your voice or in a chat interface. The chatbot-style Alexa experience is fine, but I actually prefer using my voice, because I felt like it was easier to have a conversation with it and just ask things as they popped into my head. Another good thing about Alexa+ is that it felt fast and responsive. There are short pauses while it thinks about a response, but it usually got back to me quickly enough that continuing that natural language conversation didn’t feel stilted or awkward. Speed is a crucial factor towards making a voice assistant feel responsive, and Alexa+ on the Echo Dot Max hits on that point. Some combo of Amazon’s AZ3 chip and whatever is happening up in its cloud is getting the job done here. As with any voice assistant, Alexa+ is, of course, not perfect. Most basic tasks like setting reminders, checking the weather and playing music all work reliably. But asking for specific songs or albums can sometimes go badly. Occasionally, Alexa wouldn’t be able to find a specific album I was looking for but it would play other songs by the artist; other times it would come up with completely unrelated music. This is an issue I’ve had with all assistants, but I was hoping Alexa+ might be smart enough to avoid getting too far off base. I asked it to “play the latest release by the band Now, Now.” It’s an EP entitled 01 so I had a feeling Alexa might struggle with that. Sure enough, it said “sure, here’s new music from Now, Now” and played one of the songs from that release. Not bad, but not quite right. I then followed up and said “can you play this entire album?” That did not work. Instead, I ended up with the song “Ain’t it Funky Now” performed by legendary jazz guitarist and composer Grant Green. An outstanding recording, sure, but not remotely close to what I was looking for. Even asking “play the album ‘01 EP’ by the band Now, Now” got me Drake’s “Laugh Now Cry Later.” Sigh. Worse than that were the times when Alexa+ just made things up. The Alexa app provides you with little suggestions for things to ask about, like “iconic music duets.” I tapped it, curious to see what it provided, and it pulled up a list of “iconic music duets that have left an indelible mark on the music industry.” Among those was “Smells Like Teen Spirit” performed by the late Kurt Cobain and his wife Courtney Love. This happened? News to me! I followed up and asked for more details and got a response noting that “there isn’t a formal duet of ‘Smells Like Teen Spirit.’” Best I can tell, it popped up because Love sang some unused lyrics from the legendary song on an episode of the 60 Songs That Explain the ‘90s podcast, back in 2023. This is a perfect example of the random inferences AI often draws, and it’s a good reminder that Alexa+, like all AI assistants, can make things up sometimes. This didn’t happen often, but it’s still something you’ll need to look out for. Amazon's Echo Dot Max smart speaker. Nathan Ingraham for Engadget Wrap-up The Echo Dot Max more or less delivers on Amazon’s promises. It sounds better than smaller speakers like the Echo Dot or Pop, and it’s significantly cheaper than the Echo Studio. If you’re at all interested in music, it’s worth stepping up to the Dot Max over the standard Dot. Beyond just better audio, it’s also significantly newer (the Dot was last updated in 2022). Its more modern processor means it should have a longer lifespan than the standard Dot, making it a better option for people who are eager to try out Alexa+. That freshly updated hardware is a reason to consider the Echo Dot Max over similarly priced speakers like the Nest Audio and HomePod Mini, both of which are five years old. And despite Alexa+ dealing with some growing pains, it’s a better option at this moment than the unproven Gemini for Home Google that is rolling out or the old, limited Siri that the HomePod Mini is still stuck with. But the Echo Dot Max still feels like a bit of a missed opportunity to me. The old Echo sounded so good, and this speaker is just not as exciting in comparison. It’s a fine way to interact with Alexa+ and enjoy some tunes, I just wish it sounded a little bit better. This article originally appeared on Engadget at https://www.engadget.com/audio/speakers/amazon-echo-dot-max-review-disappointing-sound-but-alexa-is-a-star-190000721.html?src=rss",
          "feed_position": 13,
          "image_url": "https://d29szjachogqwa.cloudfront.net/images/user-uploaded/echo-dot-max.jpg"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/ai/uk-high-court-sides-with-stability-ai-over-getty-in-copyright-case-180029461.html",
          "published_at": "Tue, 04 Nov 2025 18:00:29 +0000",
          "title": "UK High Court sides with Stability AI over Getty in copyright case",
          "standfirst": "Stability AI has partially succeeded in defending itself against accusations of copyright infringement. As reported by The Guardian, Stability AI prevailed in a high-profile UK High Court case, following Getty first suing the company in 2023 for allegedly using its copyright images to train its Stable Diffusion AI art tool without permission. Getty’s original claim was that Stability AI had unlawfully copied and processed millions of protected images for training purposes, therefore abusing the rights of the original creators. However, the Seattle-based company eventually withdrew its claims of primary copyright infringement as it reportedly could offer no evidence that unauthorized copying for the training of Stable Diffusion had taken place in the UK. Today’s ruling concerns claims of secondary infringement, to which the High Court judge, Justice Joanna Smith, ruled that \"an AI model such as Stable Diffusion which does not store or reproduce any copyright works (and has never done so) is not an 'infringing copy'\" under UK law. This was despite the ruling finding some evidence of Getty’s images being used by Stability, as evidenced by the presence of the former’s watermark. While the judge sided with Getty on some of its claims, she said that the evidence was \"both historic and extremely limited in scope.\" The High Court ruling likely won’t fill companies and creators concerned about AI-related copyright infringement with a huge amount of optimism, but unsurprisingly, both Getty and Stability AI have been quick to celebrate their respective victories. Getty's statement reads, in part: Today’s ruling confirms that Stable Diffusion’s inclusion of Getty Images’ trademarks in AI‑generated outputs infringed those trademarks. Crucially, the Court rejected Stability AI’s attempt to hold the user responsible for that infringement, confirming that responsibility for the presence of such trademarks lies with the model provider, who has control over the images used to train the model. This is a significant win for intellectual property owners. The ruling delivered another key finding; that, wherever the training and development did take place, Getty Images' copyright‑protected works were used to train Stable Diffusion. The ruling also established a powerful precedent that intangible articles, such as AI models, are subject to copyright infringement claims in the same way as tangible articles. We will be taking forward findings of fact from the UK ruling in our US case. The company added that it was \"deeply concerned\" that even \"well-resourced companies\" remain at risk of infringement due to a \"lack of transparent requirements.\" It also urged the UK government to build on the current laws around this issue. Christian Dowell, general counsel to Stability AI, said the final ruling from the court \"ultimately resolves the copyright concerns that were the core issue.\" The ruling comes just days after Getty announced a new agreement with Perplexity AI that permits the latter to access Getty’s huge media library as part of its search and discovery tools. In a press release, Getty said a condition of the licensing deal was Perplexity committing to \"making improvements on how it displays imagery, including image credit with link to source, to better educate users on how to use licensed imagery legally.\"This article originally appeared on Engadget at https://www.engadget.com/ai/uk-high-court-sides-with-stability-ai-over-getty-in-copyright-case-180029461.html?src=rss",
          "content": "Stability AI has partially succeeded in defending itself against accusations of copyright infringement. As reported by The Guardian, Stability AI prevailed in a high-profile UK High Court case, following Getty first suing the company in 2023 for allegedly using its copyright images to train its Stable Diffusion AI art tool without permission. Getty’s original claim was that Stability AI had unlawfully copied and processed millions of protected images for training purposes, therefore abusing the rights of the original creators. However, the Seattle-based company eventually withdrew its claims of primary copyright infringement as it reportedly could offer no evidence that unauthorized copying for the training of Stable Diffusion had taken place in the UK. Today’s ruling concerns claims of secondary infringement, to which the High Court judge, Justice Joanna Smith, ruled that \"an AI model such as Stable Diffusion which does not store or reproduce any copyright works (and has never done so) is not an 'infringing copy'\" under UK law. This was despite the ruling finding some evidence of Getty’s images being used by Stability, as evidenced by the presence of the former’s watermark. While the judge sided with Getty on some of its claims, she said that the evidence was \"both historic and extremely limited in scope.\" The High Court ruling likely won’t fill companies and creators concerned about AI-related copyright infringement with a huge amount of optimism, but unsurprisingly, both Getty and Stability AI have been quick to celebrate their respective victories. Getty's statement reads, in part: Today’s ruling confirms that Stable Diffusion’s inclusion of Getty Images’ trademarks in AI‑generated outputs infringed those trademarks. Crucially, the Court rejected Stability AI’s attempt to hold the user responsible for that infringement, confirming that responsibility for the presence of such trademarks lies with the model provider, who has control over the images used to train the model. This is a significant win for intellectual property owners. The ruling delivered another key finding; that, wherever the training and development did take place, Getty Images' copyright‑protected works were used to train Stable Diffusion. The ruling also established a powerful precedent that intangible articles, such as AI models, are subject to copyright infringement claims in the same way as tangible articles. We will be taking forward findings of fact from the UK ruling in our US case. The company added that it was \"deeply concerned\" that even \"well-resourced companies\" remain at risk of infringement due to a \"lack of transparent requirements.\" It also urged the UK government to build on the current laws around this issue. Christian Dowell, general counsel to Stability AI, said the final ruling from the court \"ultimately resolves the copyright concerns that were the core issue.\" The ruling comes just days after Getty announced a new agreement with Perplexity AI that permits the latter to access Getty’s huge media library as part of its search and discovery tools. In a press release, Getty said a condition of the licensing deal was Perplexity committing to \"making improvements on how it displays imagery, including image credit with link to source, to better educate users on how to use licensed imagery legally.\"This article originally appeared on Engadget at https://www.engadget.com/ai/uk-high-court-sides-with-stability-ai-over-getty-in-copyright-case-180029461.html?src=rss",
          "feed_position": 15
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/ar-vr/best-vr-headsets-140012529.html",
          "published_at": "Tue, 04 Nov 2025 17:00:36 +0000",
          "title": "The best VR headsets for 2025",
          "standfirst": "Choosing the best VR headset for you is the first step into some of the most immersive gaming, entertainment and virtual work experiences available today. Whether you’re exploring new worlds, working out in virtual gyms or collaborating in 3D environments, a great headset can make all the difference. The latest models offer high-resolution displays, improved FOV (field of view) and ergonomic head strap designs that keep things comfortable during extended sessions. Many also support Bluetooth for connecting external accessories like controllers or earbuds.As the tech has matured, you’ll now find headsets that balance performance with portability, and some even offer a wider field of view to enhance your sense of presence in virtual spaces. From beginner-friendly options to high-end devices built for enthusiasts, there's a VR headset out there for every type of user — and this guide will help you find the one that fits your needs best. Table of contents Best VR headsets for 2025 How we test VR headsets Other VR headsets we've tested VR headset FAQs Recent updates Best VR headsets for 2025 How we test VR headsets I tend to judge candidates for the best VR headset on a few basic criteria: Ergonomics, immersion and controls. It's not that hard to shove a mobile display into a plastic headset and strap some cheap elastic headbands onto it. But it takes real design skill to craft something that's well balanced, includes a supportive headstrap, and doesn't feel uncomfortable after 30 minutes. My test for ergonomics is fairly simple: How long can I wear a headset until I start to feel discomfort? For the most ergonomic devices, like the Quest 3, that could easily be an hour or two. But heavier PC hardware often feels cumbersome after just 15 minutes — you won’t find those kinds of devices in our list of the best VR headsets. Immersion, meanwhile, comes from having high resolution screens with fast refresh rates, like a 120Hz refresh rate, so everything looks sharp and smooth. Field of view is also a major element, as it describes how well VR screens can cover what you see. A narrow FOV makes it feel like you're peering through a pair of binoculars, which limits your sense of “presence.” The best VR headsets aim for a wider field of view, helping virtual environments feel more natural and fully surround you. A wide field of view, on the other hand, can make it seem like you’re actually flying over the globe in Google Earth. We look at a few popular video games, like Superhot, Beat Saber and Pistol Whip, on every headset to judge how immersed we feel and how enjoyable the gaming experience is overall. The best controllers fit naturally in your hands and offer accurate tracking. The industry has basically adopted the design of Meta’s excellent touch controllers, but we're also seeing intriguing leaps forward like Valve's finger tracking gamepads. We judge controllers based on how easy they are to hold, how they hold up to sweaty gameplay sessions and how easily headsets can track their position in space. However, it’s important to look at a virtual reality headset’s specs as a whole. Depending on what you’re looking for in yourVR headset, you’ll want to consider factors like your PC’s CPU and graphics card if you plan to use the headset to play the best VR games. You might not need a super powerful PC, but you should check the minimum requirements for the headset you’re looking to purchase. If you’re not looking to invest in a VR headset solely for gaming, features like head tracking allow you to explore your environment just by simply moving your head in the simulator. This often results in a more immersive and realistic experience. Other VR headsets we’ve tested HTC Vive Focus Vision The Vive Focus Vision is a sleek premium standalone VR headset that can also deliver solid PC VR. But it’s also running aging hardware, it’s riddled with software issues and it’s expensive compared to the Meta Quest 3. Meta Quest Pro As great as the Meta Quest 3 is, the Quest 2 is still a very good entry-level VR headset, and it’s worth considering if it’s on sale below its current $250 list price. The Meta Quest Pro, on the the hand, is an expensive boondoggle best ignored. HTC Vive Pro 2 Outside of Meta’s hardware, the HTC Vive Pro 2 remains a fantastic PC headset, but it’s far more expensive than the Valve Index, which is more comfortable and offers better audio. VR headset FAQs How do VR headsets work? At the most basic level, a VR headset is simply a high quality screen that you’re holding up to your face. For a wired headset, the actual work of rendering a game is done on either a PC or game console. For completely wireless devices, like the Meta Quest 3, that work is handled right on the headset. They rely on either external sensors, or sensors built into the headsets, to map your physical space. While you can use a traditional gamepad or keyboard and mouse in VR, they typically use motion tracking controllers to immerse you in their 3D environments. What VR headset is best for full body tracking? While we’re still waiting for a truly great haptic VR bodysuit to arrive, you can still achieve accurate body tracking with most Steam VR-compatible PC headsets. The Valve Index and HTC Vive Pro 2 both rely on room-tracking sensors that can map your body more effectively than the built-in sensors on competitors. You can also add HTC Vive Trackers to wrist and leg straps, as well as belts, for even better coverage. The Meta Quest 3 doesn’t have any easy body tracking solutions, but you can add Vive trackers when it’s plugged into your PC to mimic a Steam VR headset. Only a few experiences, like VRChat, take advantage of full body tracking at the moment. Currently there aren’t any body tracking solutions for the PlayStation VR and VR2, but we’re intrigued by the company’s Mocopi body trackers, which were really announced in Japan. What VR headsets are better than Oculus? Oculus is the previous name for Meta’s VR hardware. Currently, Meta only supports the Quest 3, Quest 3S and Quest Pro, all of which are wireless headsets. As we explain above, PC VR headsets can generally achieve better quality virtual reality, since they rely on more powerful graphics hardware. What VR headsets work with Xbox? Currently, Microsoft’s Xbox consoles don’t support any VR headsets. Recent updates November 2025: Updated to include the Apple Vision Pro M5. April 2025: Updated to include review scores for our top picks, where applicable. November 2024: Added the HTC Vive Focus Vision to the \"others we tested\" section. October 2024: Updated our \"best cheap VR headset\" top pick to be the Meta Quest 3S.This article originally appeared on Engadget at https://www.engadget.com/ar-vr/best-vr-headsets-140012529.html?src=rss",
          "content": "Choosing the best VR headset for you is the first step into some of the most immersive gaming, entertainment and virtual work experiences available today. Whether you’re exploring new worlds, working out in virtual gyms or collaborating in 3D environments, a great headset can make all the difference. The latest models offer high-resolution displays, improved FOV (field of view) and ergonomic head strap designs that keep things comfortable during extended sessions. Many also support Bluetooth for connecting external accessories like controllers or earbuds.As the tech has matured, you’ll now find headsets that balance performance with portability, and some even offer a wider field of view to enhance your sense of presence in virtual spaces. From beginner-friendly options to high-end devices built for enthusiasts, there's a VR headset out there for every type of user — and this guide will help you find the one that fits your needs best. Table of contents Best VR headsets for 2025 How we test VR headsets Other VR headsets we've tested VR headset FAQs Recent updates Best VR headsets for 2025 How we test VR headsets I tend to judge candidates for the best VR headset on a few basic criteria: Ergonomics, immersion and controls. It's not that hard to shove a mobile display into a plastic headset and strap some cheap elastic headbands onto it. But it takes real design skill to craft something that's well balanced, includes a supportive headstrap, and doesn't feel uncomfortable after 30 minutes. My test for ergonomics is fairly simple: How long can I wear a headset until I start to feel discomfort? For the most ergonomic devices, like the Quest 3, that could easily be an hour or two. But heavier PC hardware often feels cumbersome after just 15 minutes — you won’t find those kinds of devices in our list of the best VR headsets. Immersion, meanwhile, comes from having high resolution screens with fast refresh rates, like a 120Hz refresh rate, so everything looks sharp and smooth. Field of view is also a major element, as it describes how well VR screens can cover what you see. A narrow FOV makes it feel like you're peering through a pair of binoculars, which limits your sense of “presence.” The best VR headsets aim for a wider field of view, helping virtual environments feel more natural and fully surround you. A wide field of view, on the other hand, can make it seem like you’re actually flying over the globe in Google Earth. We look at a few popular video games, like Superhot, Beat Saber and Pistol Whip, on every headset to judge how immersed we feel and how enjoyable the gaming experience is overall. The best controllers fit naturally in your hands and offer accurate tracking. The industry has basically adopted the design of Meta’s excellent touch controllers, but we're also seeing intriguing leaps forward like Valve's finger tracking gamepads. We judge controllers based on how easy they are to hold, how they hold up to sweaty gameplay sessions and how easily headsets can track their position in space. However, it’s important to look at a virtual reality headset’s specs as a whole. Depending on what you’re looking for in yourVR headset, you’ll want to consider factors like your PC’s CPU and graphics card if you plan to use the headset to play the best VR games. You might not need a super powerful PC, but you should check the minimum requirements for the headset you’re looking to purchase. If you’re not looking to invest in a VR headset solely for gaming, features like head tracking allow you to explore your environment just by simply moving your head in the simulator. This often results in a more immersive and realistic experience. Other VR headsets we’ve tested HTC Vive Focus Vision The Vive Focus Vision is a sleek premium standalone VR headset that can also deliver solid PC VR. But it’s also running aging hardware, it’s riddled with software issues and it’s expensive compared to the Meta Quest 3. Meta Quest Pro As great as the Meta Quest 3 is, the Quest 2 is still a very good entry-level VR headset, and it’s worth considering if it’s on sale below its current $250 list price. The Meta Quest Pro, on the the hand, is an expensive boondoggle best ignored. HTC Vive Pro 2 Outside of Meta’s hardware, the HTC Vive Pro 2 remains a fantastic PC headset, but it’s far more expensive than the Valve Index, which is more comfortable and offers better audio. VR headset FAQs How do VR headsets work? At the most basic level, a VR headset is simply a high quality screen that you’re holding up to your face. For a wired headset, the actual work of rendering a game is done on either a PC or game console. For completely wireless devices, like the Meta Quest 3, that work is handled right on the headset. They rely on either external sensors, or sensors built into the headsets, to map your physical space. While you can use a traditional gamepad or keyboard and mouse in VR, they typically use motion tracking controllers to immerse you in their 3D environments. What VR headset is best for full body tracking? While we’re still waiting for a truly great haptic VR bodysuit to arrive, you can still achieve accurate body tracking with most Steam VR-compatible PC headsets. The Valve Index and HTC Vive Pro 2 both rely on room-tracking sensors that can map your body more effectively than the built-in sensors on competitors. You can also add HTC Vive Trackers to wrist and leg straps, as well as belts, for even better coverage. The Meta Quest 3 doesn’t have any easy body tracking solutions, but you can add Vive trackers when it’s plugged into your PC to mimic a Steam VR headset. Only a few experiences, like VRChat, take advantage of full body tracking at the moment. Currently there aren’t any body tracking solutions for the PlayStation VR and VR2, but we’re intrigued by the company’s Mocopi body trackers, which were really announced in Japan. What VR headsets are better than Oculus? Oculus is the previous name for Meta’s VR hardware. Currently, Meta only supports the Quest 3, Quest 3S and Quest Pro, all of which are wireless headsets. As we explain above, PC VR headsets can generally achieve better quality virtual reality, since they rely on more powerful graphics hardware. What VR headsets work with Xbox? Currently, Microsoft’s Xbox consoles don’t support any VR headsets. Recent updates November 2025: Updated to include the Apple Vision Pro M5. April 2025: Updated to include review scores for our top picks, where applicable. November 2024: Added the HTC Vive Focus Vision to the \"others we tested\" section. October 2024: Updated our \"best cheap VR headset\" top pick to be the Meta Quest 3S.This article originally appeared on Engadget at https://www.engadget.com/ar-vr/best-vr-headsets-140012529.html?src=rss",
          "feed_position": 18
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/cameras/why-dji-drones-might-be-banned-in-the-us-170030273.html",
          "published_at": "Tue, 04 Nov 2025 17:00:30 +0000",
          "title": "Why DJI drones might be banned in the US",
          "standfirst": "Since being placed on a Department of Commerce entity list in 2020 over national security fears, China’s DJI has faced the threat of a US ban on its hyper-popular drones. After exhausting its appeals and losing a lawsuit last month, DJI products like the Mini 4 Pro, Avata 2 and Neo may disappear from US shelves starting December 23. The situation could be even worse than initially expected. The FCC just gave itself the power to retroactively cut off products from companies on its “covered” list, including DJI. That gives the government the right to not just halt sales of future products, but enact rules preventing people from using drones they’ve already purchased. DJI dominates the consumer US drone market, so a ban would be terrible news for hobbyists and creators, along with industrial and public safety operators. However, the government’s concerns about the company’s drones as potential spying tools are very real. A brief history of DJI DJI, or Da-Jiang Innovations, is based in Shenzhen, China and introduced its ready-to-fly, now-iconic Phantom drone in 2013. It was $629 and offered a more user-friendly experience than other drones at the time, opening up aerial photography to creators and cinematographers. DJI Mavic 4 ProSteve Dent for Engadget The company followed with increasingly sophisticated products like the Mavic Pro, Mini 3 Pro and Avata, along with larger commercial drones. It continued to expand its range with the small but powerful Air 3, Neo and Flip. As of 2020, DJI had an estimated 77 percent of the US drone market (which accounts for 40 percent of its sales), leaving rivals to fight for scraps. Most observers attribute DJI’s dominance to its engineering-first culture. To give an idea of its technical progress, the latest 2025 Mavic 4 Pro can be flown from 25 miles away, compared to just 0.62 miles for the 2015 Phantom 3. Nearly every DJI drone feature, including video quality, battery life, range, tracking and obstacle detection, is superior to rivals. Catching the eye of the US government By 2016, the company had caught the attention of US regulators concerned about Chinese camera-equipped drones flying over sensitive facilities. While no one has uncovered a smoking gun proving that DJI drones spy for China, they undoubtedly pose a potential national security risk. The Cybersecurity and Infrastructure Security Agency (CISA) laid out the dangers last year in a guidance sheet: DJI is subject to China’s 2017 National Intelligence Law, which compels companies to cooperate with state intelligence services. The 2021 Cyber Vulnerability Reporting Law requires Chinese-based companies to disclose cyber vulnerabilities to PRC authorities prior to any public disclosure, which could allow them to exploit such flaws before they’re publicly known. UAS (unmanned aircraft system) devices controlled by smartphones provide a path for UAS data egress and storage, which could enable intelligence gathering on US critical infrastructure. Updates controlled by Chinese entities could introduce unknown data collection and transmission capabilities without the user’s awareness. When a UAS is incorporated into a network, the potential for data collection and transmission of sensitive imagery, surveying data and facility layouts increases. Photo taken by a $200 DJI Neo droneSteve Dent for Engadget In 2017, DJI’s drones were banned from use by the US Army. Later that year, the Department of Homeland Security (DHS) issued a memo stating “with moderate confidence” that DJI’s drones were “providing US critical infrastructure and law enforcement data to the Chinese government.” The agency never provided any direct proof, however, and DJI denied it. Then in 2020, DJI was added to the US Department of Commerce’s “entity list” over claims it “enabled wide-scale human rights abuses within China.” That meant the company could no longer buy parts or services from US manufacturers, like Amazon Web Services, Texas Instruments and Intel. In response, DJI said it was “disappointed” with the decision but customers could “continue to buy and use DJI products normally.” A year later, however, it was placed on the Treasury department's \"Chinese military-industrial complex companies\" list for its alleged involvement in the surveillance of Uyghur Muslim people in China. That banned US citizens from investing in the company. The US Department of Defense (DoD) piled on in October 2022, putting DJI on a list of “Chinese military companies” operating in the US. After the DoD refused DJI’s delisting petition in 2023, the company filed a lawsuit, arguing that it was “neither owned nor controlled by the Chinese military.” Nearly three years later, a court ruled against it, saying the DoD had substantial evidence that DJI contributed to the Chinese defense industry. DJI has since appealed that decision. In September 2024, the US House of Representatives passed the Countering CCP Drones Act. Though still pending approval in the US Senate, the law would allow the FCC to block DJI’s drones from accessing US radio waves, effectively making them unusable here. DJI denounced the action as “inaccurate and unsubstantiated.” Later that month, US Customs and Border Protection was reportedly blocking some DJI drone imports under the Uyghur Forced Labor Prevention Act. Moving closer to a ban Operating DJI's Avata 2Steve Dent for Engadget Near the end of last year, the US military’s annual defense spending bill (called the National Defense Authorization Act or NDAA) further boosted the possibility of a DJI ban. It required an “appropriate national security agency” to rule that a company’s products didn’t pose an “unacceptable risk” to US national security, lest it be placed on a covered list. The DoD offered DJI and other companies a year to obtain such a ruling. Because of the DoD’s requirements, DJI paused US sales and distribution in retail channels, citing regulatory uncertainty. However, some drones that originally couldn’t be purchased in the US, like the Mavic 4 Pro, can now be found on retailers like Amazon and B&H Photo Video — albeit at inflated prices compared to other regions. In March, DJI sent a formal letter to five national security agencies (DHS, DoD, FBI, NSA, and ODNI) requesting that any or all of them begin evaluating its products. In a June blog post, however, DJI stated that none of them had offered to perform such checks. “If no agency steps forward and completes the review by the December 2025 deadline, the NDAA provision could trigger an automatic ban on DJI… simply because no agency chose to take on the work of reviewing our products,” the company said. DJI further explained that it was “ready” for such an audit. Last week, the situation became potentially more dire for DJI. The FCC voted 3-0 to give itself the authority to ban devices and radio components previously approved for operation in the US. On top of the NDAA ban, the FCC would theoretically have the right to prevent DJI’s drones and other products from using US radio frequencies, effectively making them inoperable. The new regulations would also empower the FCC to bar any clones of products like the Mavic Air 3 created by alleged DJI shell companies like Anzu and Skyhigh Tech, as The Verge reported. The DJI Flip lightweight droneSteve Dent for Engadget The FCC did underline that it wasn’t planning to take away drones people have already purchased. “We emphasize that we are currently not requiring manufacturers to replace equipment in the hands of consumers,” it said in a fact sheet. “The continued use of such equipment… would remain authorized.” The FCC would be required to undertake a “public interest analysis” for each product to be banned while giving “particular weight” to national security concerns. It would also be required to allow the public to comment during a minimum 30 day period, according to a fact sheet. The possible outcomes Here are scenarios that could arise before the December 23 deadline: DJI passes its audit. In the best case scenario, which looks unlikely at this point, DJI would pass its audit and not be added to the FCC’s covered list. The company could fully resume sales of new products, rather than being stuck in limbo as it is now, and existing drones would remain legal with full support. DJI receives another extension. If this happens, the status quo would remain. New drones like the Mavic 4 Pro may still be hard to purchase, but you would likely be able to buy previously approved products like the Mavic 3 Pro. Existing drones would remain legal with full support. The FCC blocks new DJI certifications. New drone sales would not be approved in the US. Existing drones would remain legal but possibly lose long-term support. DJI drones are placed on the covered list. All drone sales for both new and previous models would cease. Current drones would be allowed to operate but may lose updates and future support. DJI drones are banned retroactively. All DJI drone sales are banned and existing drones grounded or severely restricted. The FCC has said this won’t happen. DJI Agras 50 agricultural dronepicture alliance via Getty Images DJI has reportedly spent over $17 million since 2016 on lobbying and launched the Drone Advocacy Alliance last year to enlist support from customers. It has some allies as well, like agricultural drone operators that formed a lobby last year. Law enforcement, search and rescue and other agencies have also expressed concerns about the higher costs, lower reliability and reduced performance of non-DJI drones. However, US politicians are largely unsympathetic. Senator Rick Scott (R-FL) refused to even take meetings with DJI’s lobbyists, calling the company part of a “despicable government” that wants to “spy on us.” The same sentiment appears on the other side of the aisle. “I simply won’t stand by and accept that risk, which is why I’ll continue to support DJI being added to the list of banned telecom technology,” said Representative Frank Pallone Jr. (D-NJ). So what’s liable to happen? Given the limited time left before the December 23 deadline, I believe the number three or four scenarios above are most likely: The FCC blocks new certifications and DJI drones are put on the covered list. DJI would then be forced to cease sales of new drones and possibly stop selling current models. Customers in the US would still be able to use their existing products, but may have trouble obtaining repairs and updates. If you’re a DJI drone owner in the states, you might want to formulate a contingency plan. DJI may be resigned to that scenario as well, hoping that a ban will create enough customer outcry to stimulate political action in its favor. The company’s only other hope is that the US and China miraculously strike a trade deal that includes DJI. Given the anti-China sentiment in Washington, that looks unlikely — but then again, with Trump as president, anything is possible.This article originally appeared on Engadget at https://www.engadget.com/cameras/why-dji-drones-might-be-banned-in-the-us-170030273.html?src=rss",
          "content": "Since being placed on a Department of Commerce entity list in 2020 over national security fears, China’s DJI has faced the threat of a US ban on its hyper-popular drones. After exhausting its appeals and losing a lawsuit last month, DJI products like the Mini 4 Pro, Avata 2 and Neo may disappear from US shelves starting December 23. The situation could be even worse than initially expected. The FCC just gave itself the power to retroactively cut off products from companies on its “covered” list, including DJI. That gives the government the right to not just halt sales of future products, but enact rules preventing people from using drones they’ve already purchased. DJI dominates the consumer US drone market, so a ban would be terrible news for hobbyists and creators, along with industrial and public safety operators. However, the government’s concerns about the company’s drones as potential spying tools are very real. A brief history of DJI DJI, or Da-Jiang Innovations, is based in Shenzhen, China and introduced its ready-to-fly, now-iconic Phantom drone in 2013. It was $629 and offered a more user-friendly experience than other drones at the time, opening up aerial photography to creators and cinematographers. DJI Mavic 4 ProSteve Dent for Engadget The company followed with increasingly sophisticated products like the Mavic Pro, Mini 3 Pro and Avata, along with larger commercial drones. It continued to expand its range with the small but powerful Air 3, Neo and Flip. As of 2020, DJI had an estimated 77 percent of the US drone market (which accounts for 40 percent of its sales), leaving rivals to fight for scraps. Most observers attribute DJI’s dominance to its engineering-first culture. To give an idea of its technical progress, the latest 2025 Mavic 4 Pro can be flown from 25 miles away, compared to just 0.62 miles for the 2015 Phantom 3. Nearly every DJI drone feature, including video quality, battery life, range, tracking and obstacle detection, is superior to rivals. Catching the eye of the US government By 2016, the company had caught the attention of US regulators concerned about Chinese camera-equipped drones flying over sensitive facilities. While no one has uncovered a smoking gun proving that DJI drones spy for China, they undoubtedly pose a potential national security risk. The Cybersecurity and Infrastructure Security Agency (CISA) laid out the dangers last year in a guidance sheet: DJI is subject to China’s 2017 National Intelligence Law, which compels companies to cooperate with state intelligence services. The 2021 Cyber Vulnerability Reporting Law requires Chinese-based companies to disclose cyber vulnerabilities to PRC authorities prior to any public disclosure, which could allow them to exploit such flaws before they’re publicly known. UAS (unmanned aircraft system) devices controlled by smartphones provide a path for UAS data egress and storage, which could enable intelligence gathering on US critical infrastructure. Updates controlled by Chinese entities could introduce unknown data collection and transmission capabilities without the user’s awareness. When a UAS is incorporated into a network, the potential for data collection and transmission of sensitive imagery, surveying data and facility layouts increases. Photo taken by a $200 DJI Neo droneSteve Dent for Engadget In 2017, DJI’s drones were banned from use by the US Army. Later that year, the Department of Homeland Security (DHS) issued a memo stating “with moderate confidence” that DJI’s drones were “providing US critical infrastructure and law enforcement data to the Chinese government.” The agency never provided any direct proof, however, and DJI denied it. Then in 2020, DJI was added to the US Department of Commerce’s “entity list” over claims it “enabled wide-scale human rights abuses within China.” That meant the company could no longer buy parts or services from US manufacturers, like Amazon Web Services, Texas Instruments and Intel. In response, DJI said it was “disappointed” with the decision but customers could “continue to buy and use DJI products normally.” A year later, however, it was placed on the Treasury department's \"Chinese military-industrial complex companies\" list for its alleged involvement in the surveillance of Uyghur Muslim people in China. That banned US citizens from investing in the company. The US Department of Defense (DoD) piled on in October 2022, putting DJI on a list of “Chinese military companies” operating in the US. After the DoD refused DJI’s delisting petition in 2023, the company filed a lawsuit, arguing that it was “neither owned nor controlled by the Chinese military.” Nearly three years later, a court ruled against it, saying the DoD had substantial evidence that DJI contributed to the Chinese defense industry. DJI has since appealed that decision. In September 2024, the US House of Representatives passed the Countering CCP Drones Act. Though still pending approval in the US Senate, the law would allow the FCC to block DJI’s drones from accessing US radio waves, effectively making them unusable here. DJI denounced the action as “inaccurate and unsubstantiated.” Later that month, US Customs and Border Protection was reportedly blocking some DJI drone imports under the Uyghur Forced Labor Prevention Act. Moving closer to a ban Operating DJI's Avata 2Steve Dent for Engadget Near the end of last year, the US military’s annual defense spending bill (called the National Defense Authorization Act or NDAA) further boosted the possibility of a DJI ban. It required an “appropriate national security agency” to rule that a company’s products didn’t pose an “unacceptable risk” to US national security, lest it be placed on a covered list. The DoD offered DJI and other companies a year to obtain such a ruling. Because of the DoD’s requirements, DJI paused US sales and distribution in retail channels, citing regulatory uncertainty. However, some drones that originally couldn’t be purchased in the US, like the Mavic 4 Pro, can now be found on retailers like Amazon and B&H Photo Video — albeit at inflated prices compared to other regions. In March, DJI sent a formal letter to five national security agencies (DHS, DoD, FBI, NSA, and ODNI) requesting that any or all of them begin evaluating its products. In a June blog post, however, DJI stated that none of them had offered to perform such checks. “If no agency steps forward and completes the review by the December 2025 deadline, the NDAA provision could trigger an automatic ban on DJI… simply because no agency chose to take on the work of reviewing our products,” the company said. DJI further explained that it was “ready” for such an audit. Last week, the situation became potentially more dire for DJI. The FCC voted 3-0 to give itself the authority to ban devices and radio components previously approved for operation in the US. On top of the NDAA ban, the FCC would theoretically have the right to prevent DJI’s drones and other products from using US radio frequencies, effectively making them inoperable. The new regulations would also empower the FCC to bar any clones of products like the Mavic Air 3 created by alleged DJI shell companies like Anzu and Skyhigh Tech, as The Verge reported. The DJI Flip lightweight droneSteve Dent for Engadget The FCC did underline that it wasn’t planning to take away drones people have already purchased. “We emphasize that we are currently not requiring manufacturers to replace equipment in the hands of consumers,” it said in a fact sheet. “The continued use of such equipment… would remain authorized.” The FCC would be required to undertake a “public interest analysis” for each product to be banned while giving “particular weight” to national security concerns. It would also be required to allow the public to comment during a minimum 30 day period, according to a fact sheet. The possible outcomes Here are scenarios that could arise before the December 23 deadline: DJI passes its audit. In the best case scenario, which looks unlikely at this point, DJI would pass its audit and not be added to the FCC’s covered list. The company could fully resume sales of new products, rather than being stuck in limbo as it is now, and existing drones would remain legal with full support. DJI receives another extension. If this happens, the status quo would remain. New drones like the Mavic 4 Pro may still be hard to purchase, but you would likely be able to buy previously approved products like the Mavic 3 Pro. Existing drones would remain legal with full support. The FCC blocks new DJI certifications. New drone sales would not be approved in the US. Existing drones would remain legal but possibly lose long-term support. DJI drones are placed on the covered list. All drone sales for both new and previous models would cease. Current drones would be allowed to operate but may lose updates and future support. DJI drones are banned retroactively. All DJI drone sales are banned and existing drones grounded or severely restricted. The FCC has said this won’t happen. DJI Agras 50 agricultural dronepicture alliance via Getty Images DJI has reportedly spent over $17 million since 2016 on lobbying and launched the Drone Advocacy Alliance last year to enlist support from customers. It has some allies as well, like agricultural drone operators that formed a lobby last year. Law enforcement, search and rescue and other agencies have also expressed concerns about the higher costs, lower reliability and reduced performance of non-DJI drones. However, US politicians are largely unsympathetic. Senator Rick Scott (R-FL) refused to even take meetings with DJI’s lobbyists, calling the company part of a “despicable government” that wants to “spy on us.” The same sentiment appears on the other side of the aisle. “I simply won’t stand by and accept that risk, which is why I’ll continue to support DJI being added to the list of banned telecom technology,” said Representative Frank Pallone Jr. (D-NJ). So what’s liable to happen? Given the limited time left before the December 23 deadline, I believe the number three or four scenarios above are most likely: The FCC blocks new certifications and DJI drones are put on the covered list. DJI would then be forced to cease sales of new drones and possibly stop selling current models. Customers in the US would still be able to use their existing products, but may have trouble obtaining repairs and updates. If you’re a DJI drone owner in the states, you might want to formulate a contingency plan. DJI may be resigned to that scenario as well, hoping that a ban will create enough customer outcry to stimulate political action in its favor. The company’s only other hope is that the US and China miraculously strike a trade deal that includes DJI. Given the anti-China sentiment in Washington, that looks unlikely — but then again, with Trump as president, anything is possible.This article originally appeared on Engadget at https://www.engadget.com/cameras/why-dji-drones-might-be-banned-in-the-us-170030273.html?src=rss",
          "feed_position": 19,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2025-05/5f943440-2f76-11f0-bfff-ecd8ef2e4605"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/black-friday-2025-the-best-early-tech-deals-on-apple-shark-lego-and-other-gear-plus-what-to-expect-during-the-sale-100052983.html",
          "published_at": "Tue, 04 Nov 2025 16:00:37 +0000",
          "title": "Black Friday 2025: The best early tech deals on Apple, Shark, Lego and other gear, plus what to expect during the sale",
          "standfirst": "November has turned into Black Friday and vice versa. What was once a one-day shopping sprint has turned into a month-long marathon, with retailers rolling out discounts week after week. Thanks to this, it can be easy to get deal fatigue after a while — but no one wants to miss out on a good discount, regardless of if you’re buying for yourself or someone else. We’re tracking all of the best Black Friday deals you can get right now so you don’t have to go searching for them.Engadget can help if you have tech on your shopping list this year. Here, we’ve curated the best Black Friday tech deals you can get right now, and we'll continue to update this post as we get closer to the big day at the end of November. Note that you probably have the best chance of snagging record-low prices when we get to about one week before Thanksgiving, but these deals available now are worth considering. Black Friday deals to shop now Apple iPad mini for $399 ($100 off): Apple's smallest tablet, the iPad mini is the best option for those who prefer their tablet be roughly the size of a paperback book. The latest model runs on the A17 Pro chipset and has an upgraded 128GB of storage in the base configuration. It also supports the Apple Pencil Pro. Apple Mac Mini M4 for $499 ($100 off): Desktop users looking for an upgrade should consider the latest Mac Mini, which runs on the M4 chip and 16GB of RAM as standard in the base configuration. This version has a smaller design that takes up less space, front-facing USB-C ports and a headphone jack, plus Thunderbolt 5 support. Bose QuietComfort headphones for $199 (43 percent off): These noise-cancelling headphones have a comfortable (albeit a bit boring) design, an \"Aware\" mode that lets you hear more of your surroundings when you need to and up to 24 hours of battery life. Also available at Best Buy. Apple Watch SE 3 for $200 ($50 off): The SE has been our top pick for the best Apple Watch for those on a budget, and the latest model only solidifies that further. It has the same chipset found in the latest flagship Apple Watches, fast-charging capabilities, an always-on display and most of the same activity-tracking features you'll find in more expensive model. Jisulife Life 7 handheld fan for $25 (14 percent off): This handy little fan is a must-have if you life in a warm climate or have a tropical vacation planned anytime soon. It can be used as a table or handheld fan and even be worn around the neck so you don't have to hold it at all. Its 5,000 mAh battery allows it to last hours on a single charge, and the small display in the middle of the fan's blades show its remaining battery level. Lego Disney advent calendar 2025 43253 for $39 (14 percent off): I probably don't need to tell you 'tis the season for advent calendars. Lego has a bunch, and this one in particular is on a good deal right now. Like most Lego advent calendars, this one includes a number of bricks and minifigs that, when put together, make a coherent, themed scene. Leebin 2025 electric spin scrubber for $40 (43 percent off, Prime exclusive): This weird little scrubber makes cleaning my bathroom and shower much less of a pain. Just choose the brush head you need for your job and the rotating head takes care of most of the hard work. I love the adjustable handle, which extends from 12 to 50 inches so you can get into hard-to-reach places without breaking a sweat. SanDisk microSD Express card (256GB) for $60 (12 percent off): If you have a Switch 2, no regular microSD card will do if you want to expand the console's storage. You need a newer microSD Express card, and currently there are only a handful on the market. We did some testing to find the best microSD Express card for the Switch 2 and found that performance was, in general, very similar amongst all the readily available cards. We recommend getting whichever fits within your budget at the capacity you want. Google TV Streamer 4K for $75 ($25 off): Our top pick for the best streaming device right now, the latest version of Google's streamer supports 4K video and an excellent, easy-to-use interface that will feel familiar to anyone who's seen a set with the Google TV technology built in. It provides access to all of the major streaming services including Netflix, Disney+, HBO Max, YouTube and more, plus it has a handy on-screen pop up that lets you control all compatible smart home devices right from your TV. Also available at Walmart. Cosori 9-in-1 air fryer for $90 (25 percent off): I personally have this air fryer, one of our top picks, in my house and I've used it for over a year with no issues. I love that it makes good use of vertical space so it doesn't take up too much space on my counter, and its rounded-square shape allows me to cook more food than you'd think in one go in the basket. It crisps all kinds of foods up well and generally takes a lot of the guess work (and time) out of making a good meal. Shark AI Ultra robot vacuum with 60-day self-emptying base for $300 (50 percent off): This is a version of one of our top picks for the best robot vacuums. We generally like Shark machines because they do a good job cleaning all types of flooring, produce accurate home maps and the companion app is pretty easy to use. This one in particularly comes with a self-emptying base that can hold up to 60 days worth of debris. Black Friday FAQs When is Black Friday 2025? Black Friday 2025 lands on November 28. Which stores have Black Friday deals? Many physical retail stores have Black Friday deals including Walmart, Target, Best Buy and others. Even more retailers have online Black Friday deals, including Amazon, GameStop, Costco and others. When do Black Friday sales start? Gone are the times when Black Friday sales were one-day-only affairs. Now, Black Friday deals are often available starting on Thanksgiving, or even earlier. Last year, we saw Black Friday deals online begin the week before Black Friday proper. When do Black Friday sales end? Black Friday and Cyber Monday have blended a lot over the past few years. Now, you can expect to see a good portion of Black Friday deals extend through the weekend and into Cyber Monday. It's not uncommon for Black Friday deals to expire at the end of Cyber Monday. Which retailers have the best Black Friday tech deals? The best Black Friday tech deals are typically available online at retailers like Amazon, Walmart, Best Buy and Target. It's also a good idea to check the store websites of the companies that make the products you want — for example, if you're looking for a Sonos speaker, check the Sonos website on Black Friday. Most of the time, you'll find the best Black Friday tech deals are matched at multiple retailers. Does Apple have Black Friday sales? No, you will usually not find Black Friday sales at Apple stores or on Apple's website. However, you can find Black Friday deals on Apple devices elsewhere; we recommend checking Amazon, Best Buy and other big retailers for discounts on iPads, Apple Watches and more on Black Friday. Does Amazon have Black Friday sales? Yes, Amazon has Black Friday sales. The online retailer's site will look similar to Prime Day on Black Friday, with discounts on all sorts of items from household essentials to fashion to tech.This article originally appeared on Engadget at https://www.engadget.com/deals/black-friday-2025-the-best-early-tech-deals-on-apple-shark-lego-and-other-gear-plus-what-to-expect-during-the-sale-100052983.html?src=rss",
          "content": "November has turned into Black Friday and vice versa. What was once a one-day shopping sprint has turned into a month-long marathon, with retailers rolling out discounts week after week. Thanks to this, it can be easy to get deal fatigue after a while — but no one wants to miss out on a good discount, regardless of if you’re buying for yourself or someone else. We’re tracking all of the best Black Friday deals you can get right now so you don’t have to go searching for them.Engadget can help if you have tech on your shopping list this year. Here, we’ve curated the best Black Friday tech deals you can get right now, and we'll continue to update this post as we get closer to the big day at the end of November. Note that you probably have the best chance of snagging record-low prices when we get to about one week before Thanksgiving, but these deals available now are worth considering. Black Friday deals to shop now Apple iPad mini for $399 ($100 off): Apple's smallest tablet, the iPad mini is the best option for those who prefer their tablet be roughly the size of a paperback book. The latest model runs on the A17 Pro chipset and has an upgraded 128GB of storage in the base configuration. It also supports the Apple Pencil Pro. Apple Mac Mini M4 for $499 ($100 off): Desktop users looking for an upgrade should consider the latest Mac Mini, which runs on the M4 chip and 16GB of RAM as standard in the base configuration. This version has a smaller design that takes up less space, front-facing USB-C ports and a headphone jack, plus Thunderbolt 5 support. Bose QuietComfort headphones for $199 (43 percent off): These noise-cancelling headphones have a comfortable (albeit a bit boring) design, an \"Aware\" mode that lets you hear more of your surroundings when you need to and up to 24 hours of battery life. Also available at Best Buy. Apple Watch SE 3 for $200 ($50 off): The SE has been our top pick for the best Apple Watch for those on a budget, and the latest model only solidifies that further. It has the same chipset found in the latest flagship Apple Watches, fast-charging capabilities, an always-on display and most of the same activity-tracking features you'll find in more expensive model. Jisulife Life 7 handheld fan for $25 (14 percent off): This handy little fan is a must-have if you life in a warm climate or have a tropical vacation planned anytime soon. It can be used as a table or handheld fan and even be worn around the neck so you don't have to hold it at all. Its 5,000 mAh battery allows it to last hours on a single charge, and the small display in the middle of the fan's blades show its remaining battery level. Lego Disney advent calendar 2025 43253 for $39 (14 percent off): I probably don't need to tell you 'tis the season for advent calendars. Lego has a bunch, and this one in particular is on a good deal right now. Like most Lego advent calendars, this one includes a number of bricks and minifigs that, when put together, make a coherent, themed scene. Leebin 2025 electric spin scrubber for $40 (43 percent off, Prime exclusive): This weird little scrubber makes cleaning my bathroom and shower much less of a pain. Just choose the brush head you need for your job and the rotating head takes care of most of the hard work. I love the adjustable handle, which extends from 12 to 50 inches so you can get into hard-to-reach places without breaking a sweat. SanDisk microSD Express card (256GB) for $60 (12 percent off): If you have a Switch 2, no regular microSD card will do if you want to expand the console's storage. You need a newer microSD Express card, and currently there are only a handful on the market. We did some testing to find the best microSD Express card for the Switch 2 and found that performance was, in general, very similar amongst all the readily available cards. We recommend getting whichever fits within your budget at the capacity you want. Google TV Streamer 4K for $75 ($25 off): Our top pick for the best streaming device right now, the latest version of Google's streamer supports 4K video and an excellent, easy-to-use interface that will feel familiar to anyone who's seen a set with the Google TV technology built in. It provides access to all of the major streaming services including Netflix, Disney+, HBO Max, YouTube and more, plus it has a handy on-screen pop up that lets you control all compatible smart home devices right from your TV. Also available at Walmart. Cosori 9-in-1 air fryer for $90 (25 percent off): I personally have this air fryer, one of our top picks, in my house and I've used it for over a year with no issues. I love that it makes good use of vertical space so it doesn't take up too much space on my counter, and its rounded-square shape allows me to cook more food than you'd think in one go in the basket. It crisps all kinds of foods up well and generally takes a lot of the guess work (and time) out of making a good meal. Shark AI Ultra robot vacuum with 60-day self-emptying base for $300 (50 percent off): This is a version of one of our top picks for the best robot vacuums. We generally like Shark machines because they do a good job cleaning all types of flooring, produce accurate home maps and the companion app is pretty easy to use. This one in particularly comes with a self-emptying base that can hold up to 60 days worth of debris. Black Friday FAQs When is Black Friday 2025? Black Friday 2025 lands on November 28. Which stores have Black Friday deals? Many physical retail stores have Black Friday deals including Walmart, Target, Best Buy and others. Even more retailers have online Black Friday deals, including Amazon, GameStop, Costco and others. When do Black Friday sales start? Gone are the times when Black Friday sales were one-day-only affairs. Now, Black Friday deals are often available starting on Thanksgiving, or even earlier. Last year, we saw Black Friday deals online begin the week before Black Friday proper. When do Black Friday sales end? Black Friday and Cyber Monday have blended a lot over the past few years. Now, you can expect to see a good portion of Black Friday deals extend through the weekend and into Cyber Monday. It's not uncommon for Black Friday deals to expire at the end of Cyber Monday. Which retailers have the best Black Friday tech deals? The best Black Friday tech deals are typically available online at retailers like Amazon, Walmart, Best Buy and Target. It's also a good idea to check the store websites of the companies that make the products you want — for example, if you're looking for a Sonos speaker, check the Sonos website on Black Friday. Most of the time, you'll find the best Black Friday tech deals are matched at multiple retailers. Does Apple have Black Friday sales? No, you will usually not find Black Friday sales at Apple stores or on Apple's website. However, you can find Black Friday deals on Apple devices elsewhere; we recommend checking Amazon, Best Buy and other big retailers for discounts on iPads, Apple Watches and more on Black Friday. Does Amazon have Black Friday sales? Yes, Amazon has Black Friday sales. The online retailer's site will look similar to Prime Day on Black Friday, with discounts on all sorts of items from household essentials to fashion to tech.This article originally appeared on Engadget at https://www.engadget.com/deals/black-friday-2025-the-best-early-tech-deals-on-apple-shark-lego-and-other-gear-plus-what-to-expect-during-the-sale-100052983.html?src=rss",
          "feed_position": 21
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/data-infrastructure/snowflake-builds-new-intelligence-that-goes-beyond-rag-to-query-and",
          "published_at": "Tue, 04 Nov 2025 16:00:00 GMT",
          "title": "Snowflake builds new intelligence that goes beyond RAG to query and aggregate thousands of documents at once",
          "standfirst": "Enterprise AI has a data problem. Despite billions in investment and increasingly capable language models, most organizations still can&#x27;t answer basic analytical questions about their document repositories. The culprit isn&#x27;t model quality but architecture: Traditional retrieval augmented generation (RAG) systems were designed to retrieve and summarize, not analyze and aggregate across large document sets.Snowflake is tackling this limitation head-on with a comprehensive platform strategy announced at its BUILD 2025 conference. The company unveiled Snowflake Intelligence, an enterprise intelligence agent platform designed to unify structured and unstructured data analysis, along with infrastructure improvements spanning data integration with Openflow, database consolidation with Snowflake Postgres and real-time analytics with interactive tables. The goal: Eliminate the data silos and architectural bottlenecks that prevent enterprises from operationalizing AI at scale.A key innovation is Agentic Document Analytics, a new capability within Snowflake Intelligence that can analyze thousands of documents simultaneously. This moves enterprises from basic lookups like \"What is our password reset policy?\" to complex analytical queries like \"Show me a count of weekly mentions by product area in my customer support tickets for the last six months.\"The RAG bottleneck: Why sampling fails for analyticsTraditional RAG systems work by embedding documents into vector representations, storing them in a vector database and retrieving the most semantically similar documents when a user asks a question.\"For RAG to work, it requires that all of the answers that you are searching for already exist in some published way today,\" Jeff Hollan, head of Cortex AI Agents at Snowflake explained to VentureBeat during a press briefing. \"The pattern I think about with RAG is it&#x27;s like a librarian, you get a question and it tells you, &#x27;This book has the answer on this specific page.&#x27;\"However, this architecture fundamentally breaks when organizations need to perform aggregate analysis. If, for example, an enterprise has 100,000 reports and wants to identify all of the reports that talk about a specific business entity and sum up all the revenue discussed in those reports, that&#x27;s a non-trivial task.\"That&#x27;s a much more complex thing than just traditional RAG,\" Hollan said.This limitation has typically forced enterprises to maintain separate analytics pipelines for structured data in data warehouses and unstructured data in vector databases or document stores. The result is data silos and governance challenges for enterprises.How Agentic Document Analytics works differentlySnowflake&#x27;s approach unifies structured and unstructured data analysis within its platform by treating documents as queryable data sources rather than retrieval targets. The system uses AI to extract, structure and index document content in ways that enable SQL-like analytical operations across thousands of documents.The capability leverages Snowflake&#x27;s existing architecture. Cortex AISQL handles document parsing and extraction. Interactive Tables and Warehouses deliver sub-second query performance on large datasets. By processing documents within the same governed data platform that houses structured data, enterprises can join document insights with transactional data, customer records and other business information.\"The value of AI, the power of AI, the productivity and disruptive potential of AI, is created and enabled by connecting with enterprise data,\" said Christian Kleinerman, EVP of product at Snowflake. The company&#x27;s architecture keeps all data processing within its security boundary, addressing governance concerns that have slowed enterprise AI adoption. The system works with documents across multiple sources. These include PDFs in SharePoint, Slack conversations, Microsoft Teams data and Salesforce records through Snowflake&#x27;s zero-copy integration capabilities. This eliminates the need to extract and move data into separate AI processing systems.Comparison with current market approachesThe announcement positions Snowflake differently from both traditional data warehouse vendors and AI-native startups. Companies like Databricks have focused on bringing AI capabilities to lakehouses, but typically still rely on vector databases and traditional RAG patterns for unstructured data. OpenAI&#x27;s Assistants API and Anthropic&#x27;s Claude both offer document analysis, but are limited by context window sizes.Vector database providers like Pinecone and Weaviate have built businesses around RAG use cases but sometimes face challenges when customers need analytical queries rather than retrieval-based ones. These systems excel at finding relevant documents but cannot easily aggregate information across large document sets.Among the key high-value use cases that were previously difficult with RAG-only architectures that Snowflow highlights for its approach is customer support analysis. Instead of manually reviewing support tickets, organizations can query patterns across thousands of interactions. Questions like \"What are the top 10 product issues mentioned in support tickets this quarter, broken down by customer segment?\" become answerable in seconds.What this means for enterprise AI strategyFor enterprises building AI strategies, Agentic Document Analytics represents a shift from the \"search and retrieve\" paradigm of RAG to a \"query and analyze\" paradigm more familiar from business intelligence tools. Rather than deploying separate vector databases and RAG systems for each use case, enterprises can consolidate document analytics into their existing data platform. This reduces infrastructure complexity while extending business intelligence practices to unstructured data.The capability also democratizes access. Making document analysis queryable through natural language means insights that previously required data science teams become available to business users.For enterprises looking to lead in AI, the competitive advantage comes not from having better language models, but from analyzing proprietary unstructured data at scale alongside structured business data. Organizations that can query their entire document corpus as easily as they query their data warehouse will gain insights competitors cannot easily replicate.\"AI is a reality today,\" Kleinerman said. \"We have lots of organizations already getting value out of AI, and if anyone is still waiting it out or sitting on the sidelines, our call to action is to start building now.\"",
          "content": "Enterprise AI has a data problem. Despite billions in investment and increasingly capable language models, most organizations still can&#x27;t answer basic analytical questions about their document repositories. The culprit isn&#x27;t model quality but architecture: Traditional retrieval augmented generation (RAG) systems were designed to retrieve and summarize, not analyze and aggregate across large document sets.Snowflake is tackling this limitation head-on with a comprehensive platform strategy announced at its BUILD 2025 conference. The company unveiled Snowflake Intelligence, an enterprise intelligence agent platform designed to unify structured and unstructured data analysis, along with infrastructure improvements spanning data integration with Openflow, database consolidation with Snowflake Postgres and real-time analytics with interactive tables. The goal: Eliminate the data silos and architectural bottlenecks that prevent enterprises from operationalizing AI at scale.A key innovation is Agentic Document Analytics, a new capability within Snowflake Intelligence that can analyze thousands of documents simultaneously. This moves enterprises from basic lookups like \"What is our password reset policy?\" to complex analytical queries like \"Show me a count of weekly mentions by product area in my customer support tickets for the last six months.\"The RAG bottleneck: Why sampling fails for analyticsTraditional RAG systems work by embedding documents into vector representations, storing them in a vector database and retrieving the most semantically similar documents when a user asks a question.\"For RAG to work, it requires that all of the answers that you are searching for already exist in some published way today,\" Jeff Hollan, head of Cortex AI Agents at Snowflake explained to VentureBeat during a press briefing. \"The pattern I think about with RAG is it&#x27;s like a librarian, you get a question and it tells you, &#x27;This book has the answer on this specific page.&#x27;\"However, this architecture fundamentally breaks when organizations need to perform aggregate analysis. If, for example, an enterprise has 100,000 reports and wants to identify all of the reports that talk about a specific business entity and sum up all the revenue discussed in those reports, that&#x27;s a non-trivial task.\"That&#x27;s a much more complex thing than just traditional RAG,\" Hollan said.This limitation has typically forced enterprises to maintain separate analytics pipelines for structured data in data warehouses and unstructured data in vector databases or document stores. The result is data silos and governance challenges for enterprises.How Agentic Document Analytics works differentlySnowflake&#x27;s approach unifies structured and unstructured data analysis within its platform by treating documents as queryable data sources rather than retrieval targets. The system uses AI to extract, structure and index document content in ways that enable SQL-like analytical operations across thousands of documents.The capability leverages Snowflake&#x27;s existing architecture. Cortex AISQL handles document parsing and extraction. Interactive Tables and Warehouses deliver sub-second query performance on large datasets. By processing documents within the same governed data platform that houses structured data, enterprises can join document insights with transactional data, customer records and other business information.\"The value of AI, the power of AI, the productivity and disruptive potential of AI, is created and enabled by connecting with enterprise data,\" said Christian Kleinerman, EVP of product at Snowflake. The company&#x27;s architecture keeps all data processing within its security boundary, addressing governance concerns that have slowed enterprise AI adoption. The system works with documents across multiple sources. These include PDFs in SharePoint, Slack conversations, Microsoft Teams data and Salesforce records through Snowflake&#x27;s zero-copy integration capabilities. This eliminates the need to extract and move data into separate AI processing systems.Comparison with current market approachesThe announcement positions Snowflake differently from both traditional data warehouse vendors and AI-native startups. Companies like Databricks have focused on bringing AI capabilities to lakehouses, but typically still rely on vector databases and traditional RAG patterns for unstructured data. OpenAI&#x27;s Assistants API and Anthropic&#x27;s Claude both offer document analysis, but are limited by context window sizes.Vector database providers like Pinecone and Weaviate have built businesses around RAG use cases but sometimes face challenges when customers need analytical queries rather than retrieval-based ones. These systems excel at finding relevant documents but cannot easily aggregate information across large document sets.Among the key high-value use cases that were previously difficult with RAG-only architectures that Snowflow highlights for its approach is customer support analysis. Instead of manually reviewing support tickets, organizations can query patterns across thousands of interactions. Questions like \"What are the top 10 product issues mentioned in support tickets this quarter, broken down by customer segment?\" become answerable in seconds.What this means for enterprise AI strategyFor enterprises building AI strategies, Agentic Document Analytics represents a shift from the \"search and retrieve\" paradigm of RAG to a \"query and analyze\" paradigm more familiar from business intelligence tools. Rather than deploying separate vector databases and RAG systems for each use case, enterprises can consolidate document analytics into their existing data platform. This reduces infrastructure complexity while extending business intelligence practices to unstructured data.The capability also democratizes access. Making document analysis queryable through natural language means insights that previously required data science teams become available to business users.For enterprises looking to lead in AI, the competitive advantage comes not from having better language models, but from analyzing proprietary unstructured data at scale alongside structured business data. Organizations that can query their entire document corpus as easily as they query their data warehouse will gain insights competitors cannot easily replicate.\"AI is a reality today,\" Kleinerman said. \"We have lots of organizations already getting value out of AI, and if anyone is still waiting it out or sitting on the sidelines, our call to action is to start building now.\"",
          "feed_position": 2,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/aNb5OOWRBwXwTcNyiXZd6/eea66c4d9359d596ed1345fc0e5ee004/snowflake-circuuits-code-e1687808581398.png?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/entertainment/music/alexa-comes-to-the-amazon-music-app-143234227.html",
          "published_at": "Tue, 04 Nov 2025 14:32:35 +0000",
          "title": "Alexa+ comes to the Amazon Music app",
          "standfirst": "Amazon has launched its new and improved AI assistant in the Amazon Music app. From today, anyone signed up to Alexa+ Early Access with the latest version of the app downloaded to their iOS or Android device can start using Amazon’s reimagined virtual assistant for music discovery and organizing their libraries. To access the chatbot, you tap the “A” button in the lower right corner of the screen when Amazon Music is open. You can then test its knowledge by asking it a range of questions, from something as basic as finding a recently released song by a particular artist, to more complex searches based on a single lyric or the name of the TV show the song you’re trying to find is featured in. Alexa+ is designed for more conversational interactions, so you can use natural language prompts and then ask follow-up questions as you would if you were talking to a friend, to narrow down its search results. Amazon says you can search for specific eras, moods and instruments, as well as telling Alexa what you don’t want it to serve up. Alexa+ can also be used for playlist creation, allowing you to request something as specific as a high-energy running playlist with songs from a particular decade that starts with a song from a certain artist. You can also be more vague, asking for something that fits your current mood or the time of day. Alexa+ in Amazon Music is being marketed not only as an AI tastemaker and personal DJ, but also a music expert, so you can ask it things like the inspiration for a song’s lyrics, where an album charted and questions about upcoming live performances. Alexa+ has been gradually rolling out in Amazon’s various smart devices since the beginning of the year, with mixed results. You’ll be using it in everything from new Ring devices, to the latest Kindles and Vega, Amazon’s new smart TV operating system. It’s also built into the new Echo Studio speaker, and Engadget’s Billy Steele was impressed by the AI assistant’s more human-like conversation skills, even if it’s still prone to basic errors right now, such as getting the day of the week wrong in a response. Alexa+ is currently available in Early Access for all tiers of Amazon Music. Eventually it’ll be free to all Prime members, and available to non-Prime members for $20 per month (more than an Amazon Prime subscription on its own).This article originally appeared on Engadget at https://www.engadget.com/entertainment/music/alexa-comes-to-the-amazon-music-app-143234227.html?src=rss",
          "content": "Amazon has launched its new and improved AI assistant in the Amazon Music app. From today, anyone signed up to Alexa+ Early Access with the latest version of the app downloaded to their iOS or Android device can start using Amazon’s reimagined virtual assistant for music discovery and organizing their libraries. To access the chatbot, you tap the “A” button in the lower right corner of the screen when Amazon Music is open. You can then test its knowledge by asking it a range of questions, from something as basic as finding a recently released song by a particular artist, to more complex searches based on a single lyric or the name of the TV show the song you’re trying to find is featured in. Alexa+ is designed for more conversational interactions, so you can use natural language prompts and then ask follow-up questions as you would if you were talking to a friend, to narrow down its search results. Amazon says you can search for specific eras, moods and instruments, as well as telling Alexa what you don’t want it to serve up. Alexa+ can also be used for playlist creation, allowing you to request something as specific as a high-energy running playlist with songs from a particular decade that starts with a song from a certain artist. You can also be more vague, asking for something that fits your current mood or the time of day. Alexa+ in Amazon Music is being marketed not only as an AI tastemaker and personal DJ, but also a music expert, so you can ask it things like the inspiration for a song’s lyrics, where an album charted and questions about upcoming live performances. Alexa+ has been gradually rolling out in Amazon’s various smart devices since the beginning of the year, with mixed results. You’ll be using it in everything from new Ring devices, to the latest Kindles and Vega, Amazon’s new smart TV operating system. It’s also built into the new Echo Studio speaker, and Engadget’s Billy Steele was impressed by the AI assistant’s more human-like conversation skills, even if it’s still prone to basic errors right now, such as getting the day of the week wrong in a response. Alexa+ is currently available in Early Access for all tiers of Amazon Music. Eventually it’ll be free to all Prime members, and available to non-Prime members for $20 per month (more than an Amazon Prime subscription on its own).This article originally appeared on Engadget at https://www.engadget.com/entertainment/music/alexa-comes-to-the-amazon-music-app-143234227.html?src=rss",
          "feed_position": 26
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/home/smart-home/best-smart-home-gadgets-125608958.html",
          "published_at": "Tue, 04 Nov 2025 10:00:35 +0000",
          "title": "The best smart home gadgets for 2025",
          "standfirst": "Turning your home into a smart home is easier than ever. Today’s gadgets can do everything from warming the house before you wake up to locking the door behind you at night. You can ask a voice assistant to dim the lights, set a reminder or play music while a robot vacuum takes care of the floors. With more devices working together across platforms, it’s never been simpler to make everyday life feel a little more connected.The best smart home gadgets save time and effort by automating the small things. You can control lighting, climate and security from your phone or with a quick voice command. Smart displays act as control hubs, video doorbells show who’s outside and sensors can trigger routines when you walk through the door.What once felt futuristic now just feels useful. Whether you’re starting small or expanding an existing setup, the latest smart home devices fit neatly into daily routines. From connected plugs to cleaning robots, they bring convenience, comfort and peace of mind to every room. Table of contents Best smart home gadgets for 2025: Smart speakers Best smart home gadgets for 2025: Smart displays Best smart home gadgets for 2025: Smart lights Best smart home gadgets for 2025: Security cameras Best smart home gadgets for 2025: IoT gear How to pick the right voice assistant before you buy smart gadgets Best smart home gadgets for 2025: Smart speakers Best smart home gadgets for 2025: Smart displays Best smart home gadgets for 2025: Smart lights Best smart home gadgets for 2025: Security cameras Best smart home gadgets for 2025: IoT gear How to pick the right voice assistant before you buy smart gadgets While plenty of the best smart home devices are platform agnostic, there are some — smart speakers and smart displays in particular — that require you to choose your voice control assistant. Currently, that means deciding if you’ll use the Google Assistant or Amazon’s Alexa on a regular basis (I’ll address Siri in a moment.) They’re both compatible with various smart home tech products from light switches and bulbs to robot vacuums, but there are certain devices that work best with either Google or Amazon. Nest products, for example, are more compatible and have more functionality with Google-powered speakers and displays. They can still work with Amazon devices, but certain features might be disabled. The same holds true with Amazon products: They work better if they’re in the same ecosystem. Amazon Alexa So how do you choose between Alexa and Google Assistant? It really depends on your personal preferences. Do you listen to Audible, watch Prime Video and tend to do a lot of shopping on Amazon? Then you might lean toward an Alexa-powered home automation setup. Alexa supports a wide range of devices — including smart locks, smart thermostats and motion sensors — and many of its speakers and displays include remote control functionality for lights, plugs and other smart gear. Google Assistant If you want a voice assistant that’s great at answering questions, Google Assistant tends to be better than Alexa. Amazon’s helper, on the other hand, currently supports more smart home products. The company’s smart speakers and displays also support the Zigbee smart home protocol, and some devices even have built-in smart home hubs. Both Google and Amazon devices can sync with your calendar, though Google’s tend to work better with Google services. Plus, if you already have an Android smartphone, you might be more comfortable with Google Assistant anyway. Siri But what about Siri? Apple’s assistant supports voice control as well, but it doesn't have as many compatible devices as Google or Amazon. The HomePod mini and the full-sized HomePod are the only Siri-compatible speakers on the market at the moment, too. That said, it’s not too hard to find Apple HomeKit-compatible gear as more third-party companies add support for it, but you currently have a smaller pool of devices to choose from.This article originally appeared on Engadget at https://www.engadget.com/home/smart-home/best-smart-home-gadgets-125608958.html?src=rss",
          "content": "Turning your home into a smart home is easier than ever. Today’s gadgets can do everything from warming the house before you wake up to locking the door behind you at night. You can ask a voice assistant to dim the lights, set a reminder or play music while a robot vacuum takes care of the floors. With more devices working together across platforms, it’s never been simpler to make everyday life feel a little more connected.The best smart home gadgets save time and effort by automating the small things. You can control lighting, climate and security from your phone or with a quick voice command. Smart displays act as control hubs, video doorbells show who’s outside and sensors can trigger routines when you walk through the door.What once felt futuristic now just feels useful. Whether you’re starting small or expanding an existing setup, the latest smart home devices fit neatly into daily routines. From connected plugs to cleaning robots, they bring convenience, comfort and peace of mind to every room. Table of contents Best smart home gadgets for 2025: Smart speakers Best smart home gadgets for 2025: Smart displays Best smart home gadgets for 2025: Smart lights Best smart home gadgets for 2025: Security cameras Best smart home gadgets for 2025: IoT gear How to pick the right voice assistant before you buy smart gadgets Best smart home gadgets for 2025: Smart speakers Best smart home gadgets for 2025: Smart displays Best smart home gadgets for 2025: Smart lights Best smart home gadgets for 2025: Security cameras Best smart home gadgets for 2025: IoT gear How to pick the right voice assistant before you buy smart gadgets While plenty of the best smart home devices are platform agnostic, there are some — smart speakers and smart displays in particular — that require you to choose your voice control assistant. Currently, that means deciding if you’ll use the Google Assistant or Amazon’s Alexa on a regular basis (I’ll address Siri in a moment.) They’re both compatible with various smart home tech products from light switches and bulbs to robot vacuums, but there are certain devices that work best with either Google or Amazon. Nest products, for example, are more compatible and have more functionality with Google-powered speakers and displays. They can still work with Amazon devices, but certain features might be disabled. The same holds true with Amazon products: They work better if they’re in the same ecosystem. Amazon Alexa So how do you choose between Alexa and Google Assistant? It really depends on your personal preferences. Do you listen to Audible, watch Prime Video and tend to do a lot of shopping on Amazon? Then you might lean toward an Alexa-powered home automation setup. Alexa supports a wide range of devices — including smart locks, smart thermostats and motion sensors — and many of its speakers and displays include remote control functionality for lights, plugs and other smart gear. Google Assistant If you want a voice assistant that’s great at answering questions, Google Assistant tends to be better than Alexa. Amazon’s helper, on the other hand, currently supports more smart home products. The company’s smart speakers and displays also support the Zigbee smart home protocol, and some devices even have built-in smart home hubs. Both Google and Amazon devices can sync with your calendar, though Google’s tend to work better with Google services. Plus, if you already have an Android smartphone, you might be more comfortable with Google Assistant anyway. Siri But what about Siri? Apple’s assistant supports voice control as well, but it doesn't have as many compatible devices as Google or Amazon. The HomePod mini and the full-sized HomePod are the only Siri-compatible speakers on the market at the moment, too. That said, it’s not too hard to find Apple HomeKit-compatible gear as more third-party companies add support for it, but you currently have a smaller pool of devices to choose from.This article originally appeared on Engadget at https://www.engadget.com/home/smart-home/best-smart-home-gadgets-125608958.html?src=rss",
          "feed_position": 34
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/computing/accessories/best-microsd-card-130038282.html",
          "published_at": "Tue, 04 Nov 2025 08:00:37 +0000",
          "title": "The best microSD cards in 2025",
          "standfirst": "Most microSD cards are fast enough for boosting storage space and making simple file transfers, but some provide a little more value than others. If you’ve got a device that still accepts microSD cards — whether it’s an older gaming handheld, the new Nintendo Switch 2, a dash cam, a drone or an Android tablet — we’ve scoured the market and put close to 20 top contenders through a number of benchmark tests. You can find our recommendations for the best microSD cards below, alongside some general shopping advice worth knowing before you buy. Table of contents Best microSD cards of 2025 Best microSD Express cards for the Nintendo Switch 2 Other notable microSD cards What to look for in a microSD card How we test microSD cards Recent updates Best microSD cards of 2025 Best microSD Express cards for the Nintendo Switch 2 Jeff Dunn for Engadget Read our full guide to the best microSD Express cards for the Nintendo Switch 2 Let’s be clear about this: Unless you plan to own a Nintendo Switch 2 in the near future, you do not need a high-speed microSD Express card just yet. Nintendo’s gaming handheld is the only popular device that natively supports this standard right now, and microSD Express cards themselves are highly expensive compared to more traditional options. Still, if you do want to increase a Switch 2’s storage, they’re your only choice. Fortunately, determining exactly which model to buy for the console is pretty straightforward: Get whichever one you can find in stock, in the capacity you want, at a price you can stomach. We benchmarked several microSD Express cards for a separate Switch 2 guide, and for the most part, the performance differences between them weren’t great enough to justify paying extra for any particular model. Loading times weren’t quite identical with every test we ran, but the cards were extremely close in most games and common scenarios. The few times when there was a notable gap — fast-traveling to a particularly resource-heavy region in Cyberpunk 2077, for instance — the gulf between the slowest and fastest card was only ever about four or five seconds at most. That’s not nothing, but it’s also not something most people are likely to fret over unless they have a stopwatch handy. The SanDisk microSD Express Card and Lexar Play Pro. Jeff Dunn for Engadget The only time you’d notice a major speed difference is if you transfer games to your Express card from the Switch 2’s internal storage (and vice versa). In that case, the SanDisk microSD Express Card and Lexar Play Pro were generally the quickest, while PNY’s microSD Express Flash Memory Card had particularly slow write speeds. Moving Mario Kart World to the SanDisk and Lexar models, for example, took around four minutes and 35 seconds on average; with the PNY card, it took a little over seven minutes. That said, the PNY model was the fastest when it came to moving games back to the system storage. Walmart’s Onn microSD Express Card was significantly slower to move games from the card to system storage, but it’s also the most affordable card we’ve seen by a good distance. Either way, most people aren’t constantly shuffling their games back and forth like this. Performance in actual games is more important, and in that regard the results were consistently much tighter. What matters most is getting the most space for your budget. Unfortunately, stock for all microSD Express cards has been spotty since the Switch 2’s launch. For your convenience, we’ll list out all of the models we’ve tested thus far and their respective list prices below. Note that some lower-capacity versions — the 128GB SanDisk card, for one — advertise slower speeds than their more spacious counterparts. SanDisk microSD Express Card: 128GB ($64), 256GB ($70), 512GB ($120) Lexar Play Pro: 256GB ($60), 512GB ($120), 1TB ($220) PNY microSD Express Flash Memory Card: 128GB ($45), 256GB ($62), 512GB ($120) Samsung microSD Express Card for Nintendo Switch 2: 256GB ($60) GameStop Express microSD Card for Nintendo Switch 2: 256GB ($60), 512GB ($100), 1TB ($190) Walmart Onn microSDXC Express Card: 256GB ($47), 512GB ($85) The Lexar Play Pro on top of Lexar's RW540 microSD Express card reader. Jeff Dunn for Engadget Broadly speaking, we recommend getting at least 256GB of storage, as Switch 2 games tend to have much larger file sizes than games for Nintendo’s previous handheld. But we also recommend holding off upgrading for as long as you can, if only because all of these cards should (tariff shenanigans aside) come down in price as time goes on. There’s no point in buying a microSD Express card for anything besides the Switch 2, but we did run the models above through our usual PC benchmarks as well. Unsurprisingly, they are miles faster than any traditional card on the market. With the 256GB SanDisk card, for instance, sequential read speeds checked in just under 900 MB/s in CrystalDiskMark and ATTO, while sequential writes topped out around 650 MB/s. Sustained writes speeds were slower (around 210 MB/s), but that was still fast enough to move our 12GB test file to the card in 52 seconds on average. It took a mere 20 seconds to read the file back to our PC. The write test with our smaller 1.15GB test folder, meanwhile, averaged just 4.5 seconds. It all adds up to performance that's at least twice as fast as the best UHS-I models we’ve tested in terms of sequential reads and writes, with three or four times the speeds in some cases. The gulf in random reads and writes is similar, and in some benchmarks even greater. But you need a pricey SD card reader to even see those increases on a PC, so only those with a Switch 2 in hand or serious cash to burn should consider one of these things. Other notable microSD cards Samsung Pro Ultimate The Samsung Pro Ultimate was the closest competitor to the Lexar Professional Silver Plus across our benchmark tests, but it's tangibly worse in terms of sequential write speeds, typically costs more and doesn’t offer a 1TB option. The Samsung Pro Plus is a bit slower for sequential reads, but it’s close enough otherwise and usually easier to find at a lower price. Lexar Professional Gold We haven't used it ourselves, but if you’re willing to pay for a more powerful UHS-II card built for heavy-duty video recording, the Lexar Professional Gold has tested well elsewhere and should deliver significantly faster sequential write speeds than our UHS-I picks above. It’s one of the few UHS-II cards we could actually find in stock, but it’s pricey, with a 128GB model normally priced in the $35 to $40 range. SanDisk Extreme The SanDisk Extreme effectively matched the Pro Plus in a few of our sequential tests, but that was partly due to us only being able to secure the 256GB model, which is higher-rated than the 128GB version. It’s a fine choice if you see it on sale at a reputable seller, but it’s broadly slower than our top pick and often costs more. SanDisk GamePlay The SanDisk GamePlay performs similarly to the SanDisk Extreme but costs a good bit extra as of our latest update. We couldn’t get it to reach its advertised speeds with the company’s own “Pro” card reader or other third-party options, so it fell short of our top picks. SanDisk Pokémon The SanDisk Pokémon does outperform its advertised read and write speeds, but not by enough to outpace the Lexar Silver Plus or Samsung Pro Plus. It essentially charges extra for having a picture of Pikachu (or Gengar, or Snorlax) on a product you’ll never look at. SanDisk Extreme Pro The SanDisk Extreme Pro is a close analog to the Samsung Pro Ultimate but, as of this writing, is either unavailable at most trusted retailers or priced too high by comparison. The Lexar Professional Silver Plus has faster sequential write speeds as well. PNY XLR8 Gaming The PNY XLR8 is an affordable card that comes with up to 512GB of space. Its sequential and random writes speeds checked in a little bit above those of Samsung’s Evo Select, plus it comes with a lifetime warranty. But its sequential reads were much, much slower, putting it out of contention. PNY Elite-X The PNY Elite-X often goes for cheap and wasn’t too far off the random read/write performance of Samsung's Pro Plus in CrystalDiskMark. Like the XLR8, it’s also slightly above the Evo Select in write speeds. But its sequential reads were too far behind all of our top picks, and it no longer appears to be available in capacities above 256GB. What to look for in a microSD card Capacity The first thing to figure out when buying a microSD card is how much storage space you need. Modern cards are commonly available in sizes ranging from 32GB to 512GB, with several models now available in 1TB or 1.5TB capacities as well. The first 2TB cards from major brands have started to arrive as well, which is exciting, but those are still fairly rare (and very expensive) by comparison. For many, a 128GB or 256GB model should be a sweet spot between price and storage space. But if you need more room — say, for stashing a bunch of games on a Steam Deck — a 512GB card or greater could make more sense and often provides a better cost-per-GB ratio. These days, you can find a decent 128GB card for around $15, a good 256GB card for less than $30 and a solid 512GB card for around $40 (with faster models priced a little higher). There’s a starker increase when you go up to 1TB cards, which often cost closer to $100, though we’ve seen some fall into the $70 to $80 range more frequently over the last year. The first 2TB cards are a bigger leap: the 2TB SanDisk Extreme, for example, now has a list price around $200, which is down a bit from its original MSRP but still far from cheap. Note that a microSD card’s performance may differ depending on what capacity you buy. SanDisk says its 128GB Extreme card delivers sequential write speeds up to 90 MB/s, for example, while the higher-capacity models in the same line offer up to 130 MB/s. When we talk about microSD cards today, we mostly refer to cards that use the microSDXC (eXtended Capacity) standard, which have a capacity between 32GB and 2TB. Your device needs to support this for it to work with a microSDXC card. This will almost never be an issue these days, but some older devices (a Nintendo 3DS, for instance) are only compatible with microSDHC (High Capacity) cards, which range from 2GB to 32GB. Read and write speeds MicroSD cards are primarily judged on their read and write speeds, which are usually measured in megabytes per second (MB/s). Generally, most microSD cards have faster read speeds than write speeds. These metrics can then be broken down into sequential and random performance. Sequential read and write speeds matter when you’re trying to access (read) or save (write) long, constant streams of data, such as opening a large video or copying a big batch of files from a PC. If you want to use a microSD card for media storage, this is particularly important. Random performance, meanwhile, is about how quickly a card can read and write small files scattered throughout the device. Since random read/write speeds are much lower than sequential ones, storage device makers tend not to advertise them as loudly. But they’re important if you use a card with a gaming device or a single-board computer like the Raspberry Pi, where it often has to rapidly save and access small bits of data in random locations. Speed ratings If you look at a microSD card, you’ll see a buffet of numbers, letters and symbols. Most of these refer to the card’s speed class and performance ratings, which are determined by the SD Association. A card’s Video Speed Class, or V-rating, details its minimum sequential write speed, which is especially important when recording video from a camera. It ranges from V6 to V90. Most of the cards we tested had a V30 rating, so they have a sequential write speed of at least 30 MB/s. This should be enough to support up to 4K video at lower bitrates. Higher-rated V60 and V90 cards are usually better for capturing 8K, but they come at a much higher cost. The UHS Speed Class, or U-rating, also refers to a card’s minimum sequential write speed. It comes in two varieties: U3, which mandates a minimum of 30 MB/s, and U1, which is rated for 10 MB/s. The older Speed Class rating overlaps with the other two systems. It’s signified by a C symbol and goes from Class 2 to Class 10, with the number (again) indicating minimum sequential write speed. This rating is less relevant nowadays, but you may still see a “C10” logo on some cards. The Application Performance spec, marked by an A symbol, is an indicator of random read/write speeds. This is measured in IOPS, or input/output operations per second, rather than MB/s. There are two categories here: A1 cards offer a minimum random read speed of 1,500 IOPS and a minimum random write speed of 500 IOPS, while A2 cards bump those up to 4,000 IOPS and 2,000 IOPS, respectively. Both ratings also guarantee sequential write speeds of at least 10 MB/s. To keep it simple, most people should look for a card with V30, U3 and A2 ratings. It’s totally possible to get a solid card without those: A U1 card might be worth it if you just need a cheap, high-capacity option, for example. V60 and V90 cards are worth a look if you’re serious about shooting high-resolution photos and video as well. But overall, cards with the certifications above should provide the best blend of price and performance today. It’s important to emphasize that these ratings are baselines. Most V30 cards offer significantly higher write speeds than 30 MB/s, for instance, and some A1 cards can outperform some A2 models in practice. The speeds advertised by manufacturers aren’t always 100 percent accurate, either: Sometimes the card will be slower in real-world use, other times it may actually be a bit faster. Samsung UHS bus speeds The other spec to note is the card’s bus interface. Most microSD cards available today are UHS-I, which has a theoretical maximum speed of 104 MB/s. There are also UHS-II cards, which have an extra row of pins on the back and can reach up to 312 MB/s. (A UHS-III standard technically exists as well but hasn’t seen wide adoption.) These are labeled on the card with a Roman numeral I or II. UHS-II cards are typically the ones with those higher V60 or V90 ratings. If you shoot lots of 4K to 8K video or frequently use burst mode to capture ultra high-res photos, the performance gains of a good UHS-II card can save you time. However, these are typically much more expensive than UHS-I cards: This 128GB Lexar Professional Gold model, for instance, is a relative bargain at $35. While that's less than many UHS-II models we’ve seen in the past, it's still more than double the common street price of our top pick above. You need a device that’s compatible with the UHS-II interface to see any benefits, too, and stock for UHS-II cards is often spottier. For now, the higher speeds aren’t worth the price premium for most people, so we stuck mostly to UHS-I cards with our recommendations. Unlike traditional UHS-I cards, a microSD Express card like the SanDisk model on the right comes with a second row of pins to enable its improved performance. Jeff Dunn for Engadget microSD Express and the Nintendo Switch 2 The absolute fastest microSD cards you can buy, however, are based on a different interface called SD Express. This has technically been around for several years and now includes its own subset of speed classes, but the gist is that it's much faster than UHS-I or UHS-II: SanDisk’s recently released microSD Express card, one of the first commercially available models, advertises sequential read speeds up to 880MB/s. That’s quicker than some older SSDs. It’s a substantial upgrade that has largely held up in our testing, but very few popular devices natively support SD Express today. The first major exception is the Nintendo Switch 2, which is only compatible with the newer standard, as it’s needed to keep up with the performance demands of new handheld games. Besides SanDisk, manufacturers like Samsung, Lexar and PNY have already launched microSD Express cards to go with the console. The presumption is that having such a popular device embrace the tech will only further its adoption and drive prices down. But it’s still early days, and right now all microSD Express cards cost much more than the best UHS-I or even UHS-II options. Beyond the Switch 2, it’s possible to take advantage of these cards’ superior speeds with a dedicated SD Express card reader, but that adds even more to the final cost. It’s also worth noting that SD Express cards are not backwards compatible with UHS-II (or UHS-III), so if you try to plug one into a device with a UHS-II slot, it’ll be limited to UHS-I speeds. Still, they’re a must for Nintendo fans going forward, and the tech should have plenty of appeal if more gadgets that support the interface do arrive. For more on how the first wave of Express cards perform on the Switch 2, we've put together a dedicated buying guide just for Nintendo's console. A note on card readers and reaching advertised speeds While the UHS-I interface has a theoretical maximum of 104 MB/s, some UHS-I cards can exceed that speed through proprietary extensions. However, you need a compatible card reader and host device to take advantage of that extra performance. If you find a UHS-I card advertising speeds higher than 104 MB/s, this is what’s going on. You can see these limits in action with an original Nintendo Switch or Steam Deck: Both of those gaming devices support the UHS-I interface but don’t go beyond its official speed, flattening any sequential gains some cards may have elsewhere. (Differences in random read and write speeds can still matter, though.) The same thing will happen if you plug a more powerful UHS-II or SD Express card into a device that doesn’t accept those interfaces. The takeaway: Your microSD card will only be as fast as the slowest link in your chain. Warranty Many microSD cards are designed to be durable, with protection from water, extreme temperatures, X-rays and drops. Still, in case of catastrophe, a long warranty is always good to have. Many manufacturers offer lifetime or 10-year limited warranties, though we’ve noticed that “endurance” cards marketed to withstand more hours of writing are usually covered for a shorter period of time. For example, Samsung's Pro Endurance, a model aimed at security cameras and other monitoring devices, comes with a five-year warranty. Avoiding counterfeits The memory card market has had a particular problem with scammers selling fake products. To guard against this, only buy from a known brand and a reputable retailer such as Best Buy, B&H Photo or Adorama. If you shop at Amazon, only buy if the shipper and seller is Amazon.com. (That said, a handful of users have reported receiving counterfeits even from Amazon directly in the past, so exercise caution.) Remember: If a price seems too good to be true, it probably is. Be wary of any retailer offering a significantly lower price than everyone else. Once you receive a card, check its packaging for any irregularities. You can run benchmark tests like CrystalDiskMark or BlackMagic Disk Speed Test to verify its speeds aren’t drastically lower than what’s advertised (or possible, given its specs). You can also use software that’s designed to verify the true capacity and performance of your card, such as H2testw and FakeFlashTest. A few of the microSD card readers we've used for testing. Jeff Dunn for Engadget How we test microSD cards We've put about 20 microSD cards through a series of tests to verify their sequential and random performance. These included benchmarks like CrystalDiskMark, BlackMagic Disk Speed Test, ATTO Disk Benchmark and AJA System Test, as well as a few “real-world” tests. We copied and pasted a small folder of photos about 1.15GB in size to and from each card, then did the same with a larger 12.2GB folder containing multiple file types and subfolders, timing the process each time. We also checked how each card performed on the Steam Deck, downloading games of varying sizes — including Stardew Valley, Aperture Desk Job, Metal Gear Rising: Revengeance and Apex Legends — then timing how long it took to launch each game and load save files. We do multiple runs of each test to verify our findings and account for potential outliers. Where applicable, we used a Kingston USB 3.2 UHS-II reader to test each card on both Windows 11 and macOS Sequoia. However, if a card could be bundled with (or is specifically advertised to use) a proprietary reader, we mainly tested with that, since we figure that’s the one most interested buyers will end up using. For Windows testing, we used an Alienware gaming PC with an Intel Core i9-10900F, Nvidia GeForce RTX 3080 GPU, 32GB of RAM and a 1TB SSD. For macOS, we used a 2021 16-inch MacBook Pro with an Apple M1 Pro chip, 16GB of RAM and a 512GB SSD. If a reader couldn’t connect over USB-C, we used CalDigit’s TS4 dock to test the corresponding card on the MacBook. We tested the 128GB version of each card whenever we could, though we were only able to test higher-capacity models for a few options. We also reformatted each card before testing with the SD Association’s Memory Card Formatter tool. For microSD Express cards, we ran each model through a series of tests specific to the Switch 2, since that is the only popular device that actually supports the tech. You can read more about that process in our separate buying guide linked above. Recent updates November 2025: We’ve tested Walmart’s Onn microSD Express Card and edited our notes on the best cards for the Nintendo Switch 2 accordingly. September 2025: We’ve fleshed out our advice on buying a microSD Express card for the Nintendo Switch 2. May 2025: We’ve updated this guide to ensure our advice is accurate and to reflect the new Nintendo Switch 2’s use of microSD Express cards. We’ve also added testing notes for the Lexar Play Pro microSD Express card and the more standard PNY XLR8. We’ve removed mentions of a few cards that have seemingly been discontinued, including the original Kingston Canvas Go Plus, the Lexar Professional 1066x and the 2021 Samsung Evo Select. We plan to test more new microSD Express cards for a future update. February 2025: We’ve updated this guide with a new top pick: the Lexar Professional Silver Plus. The Samsung Pro Plus, our previous recommendation, stays as a runner-up. We’ve also added SanDisk’s recently released microSD Express card as a pick for those who want the absolute fastest card possible, albeit with heavy caveats. This is one of the first microSD cards to use the SD Express bus interface, so we’ve included more details on what that entails in our “what to look for in a microSD card” section. Lastly, we’ve removed our write-up for SanDisk’s Apex Legends card, as it appears to have been discontinued, and included testing notes for a couple of the company’s more recent releases. November 2024: We've checked back with this guide to ensure our recommendations are still accurate and made light edits for clarity. August 2024: We’ve updated this guide to note the recently released 1TB models for three of our top picks: the Samsung Pro Plus, the Kingston Canvas Go Plus and the Samsung Evo Select. We’ve also made sure all pricing details are as up to date as they can be. This article originally appeared on Engadget at https://www.engadget.com/computing/accessories/best-microsd-card-130038282.html?src=rss",
          "content": "Most microSD cards are fast enough for boosting storage space and making simple file transfers, but some provide a little more value than others. If you’ve got a device that still accepts microSD cards — whether it’s an older gaming handheld, the new Nintendo Switch 2, a dash cam, a drone or an Android tablet — we’ve scoured the market and put close to 20 top contenders through a number of benchmark tests. You can find our recommendations for the best microSD cards below, alongside some general shopping advice worth knowing before you buy. Table of contents Best microSD cards of 2025 Best microSD Express cards for the Nintendo Switch 2 Other notable microSD cards What to look for in a microSD card How we test microSD cards Recent updates Best microSD cards of 2025 Best microSD Express cards for the Nintendo Switch 2 Jeff Dunn for Engadget Read our full guide to the best microSD Express cards for the Nintendo Switch 2 Let’s be clear about this: Unless you plan to own a Nintendo Switch 2 in the near future, you do not need a high-speed microSD Express card just yet. Nintendo’s gaming handheld is the only popular device that natively supports this standard right now, and microSD Express cards themselves are highly expensive compared to more traditional options. Still, if you do want to increase a Switch 2’s storage, they’re your only choice. Fortunately, determining exactly which model to buy for the console is pretty straightforward: Get whichever one you can find in stock, in the capacity you want, at a price you can stomach. We benchmarked several microSD Express cards for a separate Switch 2 guide, and for the most part, the performance differences between them weren’t great enough to justify paying extra for any particular model. Loading times weren’t quite identical with every test we ran, but the cards were extremely close in most games and common scenarios. The few times when there was a notable gap — fast-traveling to a particularly resource-heavy region in Cyberpunk 2077, for instance — the gulf between the slowest and fastest card was only ever about four or five seconds at most. That’s not nothing, but it’s also not something most people are likely to fret over unless they have a stopwatch handy. The SanDisk microSD Express Card and Lexar Play Pro. Jeff Dunn for Engadget The only time you’d notice a major speed difference is if you transfer games to your Express card from the Switch 2’s internal storage (and vice versa). In that case, the SanDisk microSD Express Card and Lexar Play Pro were generally the quickest, while PNY’s microSD Express Flash Memory Card had particularly slow write speeds. Moving Mario Kart World to the SanDisk and Lexar models, for example, took around four minutes and 35 seconds on average; with the PNY card, it took a little over seven minutes. That said, the PNY model was the fastest when it came to moving games back to the system storage. Walmart’s Onn microSD Express Card was significantly slower to move games from the card to system storage, but it’s also the most affordable card we’ve seen by a good distance. Either way, most people aren’t constantly shuffling their games back and forth like this. Performance in actual games is more important, and in that regard the results were consistently much tighter. What matters most is getting the most space for your budget. Unfortunately, stock for all microSD Express cards has been spotty since the Switch 2’s launch. For your convenience, we’ll list out all of the models we’ve tested thus far and their respective list prices below. Note that some lower-capacity versions — the 128GB SanDisk card, for one — advertise slower speeds than their more spacious counterparts. SanDisk microSD Express Card: 128GB ($64), 256GB ($70), 512GB ($120) Lexar Play Pro: 256GB ($60), 512GB ($120), 1TB ($220) PNY microSD Express Flash Memory Card: 128GB ($45), 256GB ($62), 512GB ($120) Samsung microSD Express Card for Nintendo Switch 2: 256GB ($60) GameStop Express microSD Card for Nintendo Switch 2: 256GB ($60), 512GB ($100), 1TB ($190) Walmart Onn microSDXC Express Card: 256GB ($47), 512GB ($85) The Lexar Play Pro on top of Lexar's RW540 microSD Express card reader. Jeff Dunn for Engadget Broadly speaking, we recommend getting at least 256GB of storage, as Switch 2 games tend to have much larger file sizes than games for Nintendo’s previous handheld. But we also recommend holding off upgrading for as long as you can, if only because all of these cards should (tariff shenanigans aside) come down in price as time goes on. There’s no point in buying a microSD Express card for anything besides the Switch 2, but we did run the models above through our usual PC benchmarks as well. Unsurprisingly, they are miles faster than any traditional card on the market. With the 256GB SanDisk card, for instance, sequential read speeds checked in just under 900 MB/s in CrystalDiskMark and ATTO, while sequential writes topped out around 650 MB/s. Sustained writes speeds were slower (around 210 MB/s), but that was still fast enough to move our 12GB test file to the card in 52 seconds on average. It took a mere 20 seconds to read the file back to our PC. The write test with our smaller 1.15GB test folder, meanwhile, averaged just 4.5 seconds. It all adds up to performance that's at least twice as fast as the best UHS-I models we’ve tested in terms of sequential reads and writes, with three or four times the speeds in some cases. The gulf in random reads and writes is similar, and in some benchmarks even greater. But you need a pricey SD card reader to even see those increases on a PC, so only those with a Switch 2 in hand or serious cash to burn should consider one of these things. Other notable microSD cards Samsung Pro Ultimate The Samsung Pro Ultimate was the closest competitor to the Lexar Professional Silver Plus across our benchmark tests, but it's tangibly worse in terms of sequential write speeds, typically costs more and doesn’t offer a 1TB option. The Samsung Pro Plus is a bit slower for sequential reads, but it’s close enough otherwise and usually easier to find at a lower price. Lexar Professional Gold We haven't used it ourselves, but if you’re willing to pay for a more powerful UHS-II card built for heavy-duty video recording, the Lexar Professional Gold has tested well elsewhere and should deliver significantly faster sequential write speeds than our UHS-I picks above. It’s one of the few UHS-II cards we could actually find in stock, but it’s pricey, with a 128GB model normally priced in the $35 to $40 range. SanDisk Extreme The SanDisk Extreme effectively matched the Pro Plus in a few of our sequential tests, but that was partly due to us only being able to secure the 256GB model, which is higher-rated than the 128GB version. It’s a fine choice if you see it on sale at a reputable seller, but it’s broadly slower than our top pick and often costs more. SanDisk GamePlay The SanDisk GamePlay performs similarly to the SanDisk Extreme but costs a good bit extra as of our latest update. We couldn’t get it to reach its advertised speeds with the company’s own “Pro” card reader or other third-party options, so it fell short of our top picks. SanDisk Pokémon The SanDisk Pokémon does outperform its advertised read and write speeds, but not by enough to outpace the Lexar Silver Plus or Samsung Pro Plus. It essentially charges extra for having a picture of Pikachu (or Gengar, or Snorlax) on a product you’ll never look at. SanDisk Extreme Pro The SanDisk Extreme Pro is a close analog to the Samsung Pro Ultimate but, as of this writing, is either unavailable at most trusted retailers or priced too high by comparison. The Lexar Professional Silver Plus has faster sequential write speeds as well. PNY XLR8 Gaming The PNY XLR8 is an affordable card that comes with up to 512GB of space. Its sequential and random writes speeds checked in a little bit above those of Samsung’s Evo Select, plus it comes with a lifetime warranty. But its sequential reads were much, much slower, putting it out of contention. PNY Elite-X The PNY Elite-X often goes for cheap and wasn’t too far off the random read/write performance of Samsung's Pro Plus in CrystalDiskMark. Like the XLR8, it’s also slightly above the Evo Select in write speeds. But its sequential reads were too far behind all of our top picks, and it no longer appears to be available in capacities above 256GB. What to look for in a microSD card Capacity The first thing to figure out when buying a microSD card is how much storage space you need. Modern cards are commonly available in sizes ranging from 32GB to 512GB, with several models now available in 1TB or 1.5TB capacities as well. The first 2TB cards from major brands have started to arrive as well, which is exciting, but those are still fairly rare (and very expensive) by comparison. For many, a 128GB or 256GB model should be a sweet spot between price and storage space. But if you need more room — say, for stashing a bunch of games on a Steam Deck — a 512GB card or greater could make more sense and often provides a better cost-per-GB ratio. These days, you can find a decent 128GB card for around $15, a good 256GB card for less than $30 and a solid 512GB card for around $40 (with faster models priced a little higher). There’s a starker increase when you go up to 1TB cards, which often cost closer to $100, though we’ve seen some fall into the $70 to $80 range more frequently over the last year. The first 2TB cards are a bigger leap: the 2TB SanDisk Extreme, for example, now has a list price around $200, which is down a bit from its original MSRP but still far from cheap. Note that a microSD card’s performance may differ depending on what capacity you buy. SanDisk says its 128GB Extreme card delivers sequential write speeds up to 90 MB/s, for example, while the higher-capacity models in the same line offer up to 130 MB/s. When we talk about microSD cards today, we mostly refer to cards that use the microSDXC (eXtended Capacity) standard, which have a capacity between 32GB and 2TB. Your device needs to support this for it to work with a microSDXC card. This will almost never be an issue these days, but some older devices (a Nintendo 3DS, for instance) are only compatible with microSDHC (High Capacity) cards, which range from 2GB to 32GB. Read and write speeds MicroSD cards are primarily judged on their read and write speeds, which are usually measured in megabytes per second (MB/s). Generally, most microSD cards have faster read speeds than write speeds. These metrics can then be broken down into sequential and random performance. Sequential read and write speeds matter when you’re trying to access (read) or save (write) long, constant streams of data, such as opening a large video or copying a big batch of files from a PC. If you want to use a microSD card for media storage, this is particularly important. Random performance, meanwhile, is about how quickly a card can read and write small files scattered throughout the device. Since random read/write speeds are much lower than sequential ones, storage device makers tend not to advertise them as loudly. But they’re important if you use a card with a gaming device or a single-board computer like the Raspberry Pi, where it often has to rapidly save and access small bits of data in random locations. Speed ratings If you look at a microSD card, you’ll see a buffet of numbers, letters and symbols. Most of these refer to the card’s speed class and performance ratings, which are determined by the SD Association. A card’s Video Speed Class, or V-rating, details its minimum sequential write speed, which is especially important when recording video from a camera. It ranges from V6 to V90. Most of the cards we tested had a V30 rating, so they have a sequential write speed of at least 30 MB/s. This should be enough to support up to 4K video at lower bitrates. Higher-rated V60 and V90 cards are usually better for capturing 8K, but they come at a much higher cost. The UHS Speed Class, or U-rating, also refers to a card’s minimum sequential write speed. It comes in two varieties: U3, which mandates a minimum of 30 MB/s, and U1, which is rated for 10 MB/s. The older Speed Class rating overlaps with the other two systems. It’s signified by a C symbol and goes from Class 2 to Class 10, with the number (again) indicating minimum sequential write speed. This rating is less relevant nowadays, but you may still see a “C10” logo on some cards. The Application Performance spec, marked by an A symbol, is an indicator of random read/write speeds. This is measured in IOPS, or input/output operations per second, rather than MB/s. There are two categories here: A1 cards offer a minimum random read speed of 1,500 IOPS and a minimum random write speed of 500 IOPS, while A2 cards bump those up to 4,000 IOPS and 2,000 IOPS, respectively. Both ratings also guarantee sequential write speeds of at least 10 MB/s. To keep it simple, most people should look for a card with V30, U3 and A2 ratings. It’s totally possible to get a solid card without those: A U1 card might be worth it if you just need a cheap, high-capacity option, for example. V60 and V90 cards are worth a look if you’re serious about shooting high-resolution photos and video as well. But overall, cards with the certifications above should provide the best blend of price and performance today. It’s important to emphasize that these ratings are baselines. Most V30 cards offer significantly higher write speeds than 30 MB/s, for instance, and some A1 cards can outperform some A2 models in practice. The speeds advertised by manufacturers aren’t always 100 percent accurate, either: Sometimes the card will be slower in real-world use, other times it may actually be a bit faster. Samsung UHS bus speeds The other spec to note is the card’s bus interface. Most microSD cards available today are UHS-I, which has a theoretical maximum speed of 104 MB/s. There are also UHS-II cards, which have an extra row of pins on the back and can reach up to 312 MB/s. (A UHS-III standard technically exists as well but hasn’t seen wide adoption.) These are labeled on the card with a Roman numeral I or II. UHS-II cards are typically the ones with those higher V60 or V90 ratings. If you shoot lots of 4K to 8K video or frequently use burst mode to capture ultra high-res photos, the performance gains of a good UHS-II card can save you time. However, these are typically much more expensive than UHS-I cards: This 128GB Lexar Professional Gold model, for instance, is a relative bargain at $35. While that's less than many UHS-II models we’ve seen in the past, it's still more than double the common street price of our top pick above. You need a device that’s compatible with the UHS-II interface to see any benefits, too, and stock for UHS-II cards is often spottier. For now, the higher speeds aren’t worth the price premium for most people, so we stuck mostly to UHS-I cards with our recommendations. Unlike traditional UHS-I cards, a microSD Express card like the SanDisk model on the right comes with a second row of pins to enable its improved performance. Jeff Dunn for Engadget microSD Express and the Nintendo Switch 2 The absolute fastest microSD cards you can buy, however, are based on a different interface called SD Express. This has technically been around for several years and now includes its own subset of speed classes, but the gist is that it's much faster than UHS-I or UHS-II: SanDisk’s recently released microSD Express card, one of the first commercially available models, advertises sequential read speeds up to 880MB/s. That’s quicker than some older SSDs. It’s a substantial upgrade that has largely held up in our testing, but very few popular devices natively support SD Express today. The first major exception is the Nintendo Switch 2, which is only compatible with the newer standard, as it’s needed to keep up with the performance demands of new handheld games. Besides SanDisk, manufacturers like Samsung, Lexar and PNY have already launched microSD Express cards to go with the console. The presumption is that having such a popular device embrace the tech will only further its adoption and drive prices down. But it’s still early days, and right now all microSD Express cards cost much more than the best UHS-I or even UHS-II options. Beyond the Switch 2, it’s possible to take advantage of these cards’ superior speeds with a dedicated SD Express card reader, but that adds even more to the final cost. It’s also worth noting that SD Express cards are not backwards compatible with UHS-II (or UHS-III), so if you try to plug one into a device with a UHS-II slot, it’ll be limited to UHS-I speeds. Still, they’re a must for Nintendo fans going forward, and the tech should have plenty of appeal if more gadgets that support the interface do arrive. For more on how the first wave of Express cards perform on the Switch 2, we've put together a dedicated buying guide just for Nintendo's console. A note on card readers and reaching advertised speeds While the UHS-I interface has a theoretical maximum of 104 MB/s, some UHS-I cards can exceed that speed through proprietary extensions. However, you need a compatible card reader and host device to take advantage of that extra performance. If you find a UHS-I card advertising speeds higher than 104 MB/s, this is what’s going on. You can see these limits in action with an original Nintendo Switch or Steam Deck: Both of those gaming devices support the UHS-I interface but don’t go beyond its official speed, flattening any sequential gains some cards may have elsewhere. (Differences in random read and write speeds can still matter, though.) The same thing will happen if you plug a more powerful UHS-II or SD Express card into a device that doesn’t accept those interfaces. The takeaway: Your microSD card will only be as fast as the slowest link in your chain. Warranty Many microSD cards are designed to be durable, with protection from water, extreme temperatures, X-rays and drops. Still, in case of catastrophe, a long warranty is always good to have. Many manufacturers offer lifetime or 10-year limited warranties, though we’ve noticed that “endurance” cards marketed to withstand more hours of writing are usually covered for a shorter period of time. For example, Samsung's Pro Endurance, a model aimed at security cameras and other monitoring devices, comes with a five-year warranty. Avoiding counterfeits The memory card market has had a particular problem with scammers selling fake products. To guard against this, only buy from a known brand and a reputable retailer such as Best Buy, B&H Photo or Adorama. If you shop at Amazon, only buy if the shipper and seller is Amazon.com. (That said, a handful of users have reported receiving counterfeits even from Amazon directly in the past, so exercise caution.) Remember: If a price seems too good to be true, it probably is. Be wary of any retailer offering a significantly lower price than everyone else. Once you receive a card, check its packaging for any irregularities. You can run benchmark tests like CrystalDiskMark or BlackMagic Disk Speed Test to verify its speeds aren’t drastically lower than what’s advertised (or possible, given its specs). You can also use software that’s designed to verify the true capacity and performance of your card, such as H2testw and FakeFlashTest. A few of the microSD card readers we've used for testing. Jeff Dunn for Engadget How we test microSD cards We've put about 20 microSD cards through a series of tests to verify their sequential and random performance. These included benchmarks like CrystalDiskMark, BlackMagic Disk Speed Test, ATTO Disk Benchmark and AJA System Test, as well as a few “real-world” tests. We copied and pasted a small folder of photos about 1.15GB in size to and from each card, then did the same with a larger 12.2GB folder containing multiple file types and subfolders, timing the process each time. We also checked how each card performed on the Steam Deck, downloading games of varying sizes — including Stardew Valley, Aperture Desk Job, Metal Gear Rising: Revengeance and Apex Legends — then timing how long it took to launch each game and load save files. We do multiple runs of each test to verify our findings and account for potential outliers. Where applicable, we used a Kingston USB 3.2 UHS-II reader to test each card on both Windows 11 and macOS Sequoia. However, if a card could be bundled with (or is specifically advertised to use) a proprietary reader, we mainly tested with that, since we figure that’s the one most interested buyers will end up using. For Windows testing, we used an Alienware gaming PC with an Intel Core i9-10900F, Nvidia GeForce RTX 3080 GPU, 32GB of RAM and a 1TB SSD. For macOS, we used a 2021 16-inch MacBook Pro with an Apple M1 Pro chip, 16GB of RAM and a 512GB SSD. If a reader couldn’t connect over USB-C, we used CalDigit’s TS4 dock to test the corresponding card on the MacBook. We tested the 128GB version of each card whenever we could, though we were only able to test higher-capacity models for a few options. We also reformatted each card before testing with the SD Association’s Memory Card Formatter tool. For microSD Express cards, we ran each model through a series of tests specific to the Switch 2, since that is the only popular device that actually supports the tech. You can read more about that process in our separate buying guide linked above. Recent updates November 2025: We’ve tested Walmart’s Onn microSD Express Card and edited our notes on the best cards for the Nintendo Switch 2 accordingly. September 2025: We’ve fleshed out our advice on buying a microSD Express card for the Nintendo Switch 2. May 2025: We’ve updated this guide to ensure our advice is accurate and to reflect the new Nintendo Switch 2’s use of microSD Express cards. We’ve also added testing notes for the Lexar Play Pro microSD Express card and the more standard PNY XLR8. We’ve removed mentions of a few cards that have seemingly been discontinued, including the original Kingston Canvas Go Plus, the Lexar Professional 1066x and the 2021 Samsung Evo Select. We plan to test more new microSD Express cards for a future update. February 2025: We’ve updated this guide with a new top pick: the Lexar Professional Silver Plus. The Samsung Pro Plus, our previous recommendation, stays as a runner-up. We’ve also added SanDisk’s recently released microSD Express card as a pick for those who want the absolute fastest card possible, albeit with heavy caveats. This is one of the first microSD cards to use the SD Express bus interface, so we’ve included more details on what that entails in our “what to look for in a microSD card” section. Lastly, we’ve removed our write-up for SanDisk’s Apex Legends card, as it appears to have been discontinued, and included testing notes for a couple of the company’s more recent releases. November 2024: We've checked back with this guide to ensure our recommendations are still accurate and made light edits for clarity. August 2024: We’ve updated this guide to note the recently released 1TB models for three of our top picks: the Samsung Pro Plus, the Kingston Canvas Go Plus and the Samsung Evo Select. We’ve also made sure all pricing details are as up to date as they can be. This article originally appeared on Engadget at https://www.engadget.com/computing/accessories/best-microsd-card-130038282.html?src=rss",
          "feed_position": 35,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2025-08/1b951760-737c-11f0-8ffe-35ba986a2495"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/98-of-market-researchers-use-ai-daily-but-4-in-10-say-it-makes-errors",
          "published_at": "Tue, 04 Nov 2025 08:00:00 GMT",
          "title": "98% of market researchers use AI daily, but 4 in 10 say it makes errors — revealing a major trust problem",
          "standfirst": "Market researchers have embraced artificial intelligence at a staggering pace, with 98% of professionals now incorporating AI tools into their work and 72% using them daily or more frequently, according to a new industry survey that reveals both the technology&#x27;s transformative promise and its persistent reliability problems.The findings, based on responses from 219 U.S. market research and insights professionals surveyed in August 2025 by QuestDIY, a research platform owned by The Harris Poll, paint a picture of an industry caught between competing pressures: the demand to deliver faster business insights and the burden of validating everything AI produces to ensure accuracy.While more than half of researchers — 56% — report saving at least five hours per week using AI tools, nearly four in ten say they&#x27;ve experienced \"increased reliance on technology that sometimes produces errors.\" An additional 37% report that AI has \"introduced new risks around data quality or accuracy,\" and 31% say the technology has \"led to more work re-checking or validating AI outputs.\"The disconnect between productivity gains and trustworthiness has created what amounts to a grand bargain in the research industry: professionals accept time savings and enhanced capabilities in exchange for constant vigilance over AI&#x27;s mistakes, a dynamic that may fundamentally reshape how insights work gets done.How market researchers went from AI skeptics to daily users in less than a yearThe numbers suggest AI has moved from experiment to infrastructure in record time. Among those using AI daily, 39% deploy it once per day, while 33% use it \"several times per day or more,\" according to the survey conducted between August 15-19, 2025. Adoption is accelerating: 80% of researchers say they&#x27;re using AI more than they were six months ago, and 71% expect to increase usage over the next six months. Only 8% anticipate their usage will decline.“While AI provides excellent assistance and opportunities, human judgment will remain vital,” Erica Parker, Managing Director Research Products at The Harris Poll, told VentureBeat. “The future is a teamwork dynamic where AI will accelerate tasks and quickly unearth findings, while researchers will ensure quality and provide high level consultative insights.”The top use cases reflect AI&#x27;s strength in handling data at scale: 58% of researchers use it for analyzing multiple data sources, 54% for analyzing structured data, 50% for automating insight reports, 49% for analyzing open-ended survey responses, and 48% for summarizing findings. These tasks—traditionally labor-intensive and time-consuming — now happen in minutes rather than hours.Beyond time savings, researchers report tangible quality improvements. Some 44% say AI improves accuracy, 43% report it helps surface insights they might otherwise have missed, 43% cite increased speed of insights delivery, and 39% say it sparks creativity. The overwhelming majority — 89% — say AI has made their work lives better, with 25% describing the improvement as \"significant.\"The productivity paradox: saving time while creating new validation workYet the same survey reveals deep unease about the technology&#x27;s reliability. The list of concerns is extensive: 39% of researchers report increased reliance on error-prone technology, 37% cite new risks around data quality or accuracy, 31% describe additional validation work, 29% report uncertainty about job security, and 28% say AI has raised concerns about data privacy and ethics.The report notes that \"accuracy is the biggest frustration with AI experienced by researchers when asked on an open-ended basis.\" One researcher captured the tension succinctly: \"The faster we move with AI, the more we need to check if we&#x27;re moving in the right direction.\"This paradox — saving time while simultaneously creating new work — reflects a fundamental characteristic of current AI systems, which can produce outputs that appear authoritative but contain what researchers call \"hallucinations,\" or fabricated information presented as fact. The challenge is particularly acute in a profession where credibility depends on methodological rigor and where incorrect data can lead clients to make costly business decisions.\"Researchers view AI as a junior analyst, capable of speed and breadth, but needing oversight and judgment,\" said Gary Topiol, Managing Director at QuestDIY, in the report.That metaphor — AI as junior analyst — captures the industry&#x27;s current operating model. Researchers treat AI outputs as drafts requiring senior review rather than finished products, a workflow that provides guardrails but also underscores the technology&#x27;s limitations.Why data privacy fears are the biggest obstacle to AI adoption in researchWhen asked what would limit AI use at work, researchers identified data privacy and security concerns as the greatest barrier, cited by 33% of respondents. This concern isn&#x27;t abstract: researchers handle sensitive customer data, proprietary business information, and personally identifiable information subject to regulations like GDPR and CCPA. Sharing that data with AI systems — particularly cloud-based large language models — raises legitimate questions about who controls the information and whether it might be used to train models accessible to competitors.Other significant barriers include time to experiment and learn new tools (32%), training (32%), integration challenges (28%), internal policy restrictions (25%), and cost (24%). An additional 31% cited lack of transparency in AI use as a concern, which could complicate explaining results to clients and stakeholders.The transparency issue is particularly thorny. When an AI system produces an analysis or insight, researchers often cannot trace how the system arrived at its conclusion — a problem that conflicts with the scientific method&#x27;s emphasis on replicability and clear methodology. Some clients have responded by including no-AI clauses in their contracts, forcing researchers to either avoid the technology entirely or use it in ways that don&#x27;t technically violate contractual terms but may blur ethical lines.\"Onboarding beats feature bloat,\" Parker said in the report. \"The biggest brakes are time to learn and train. Packaged workflows, templates, and guided setup all unlock usage faster than piling on capabilities.\"Inside the new workflow: treating AI like a junior analyst who needs constant supervisionDespite these challenges, researchers aren&#x27;t abandoning AI — they&#x27;re developing frameworks to use it responsibly. The consensus model, according to the survey, is \"human-led research supported by AI,\" where AI handles repetitive tasks like coding, data cleaning, and report generation while humans focus on interpretation, strategy, and business impact.About one-third of researchers (29%) describe their current workflow as \"human-led with significant AI support,\" while 31% characterize it as \"mostly human with some AI help.\" Looking ahead to 2030, 61% envision AI as a \"decision-support partner\" with expanded capabilities including generative features for drafting surveys and reports (56%), AI-driven synthetic data generation (53%), automation of core processes like project setup and coding (48%), predictive analytics (44%), and deeper cognitive insights (43%).The report describes an emerging division of labor where researchers become \"Insight Advocates\" — professionals who validate AI outputs, connect findings to stakeholder challenges, and translate machine-generated analysis into strategic narratives that drive business decisions. In this model, technical execution becomes less central to the researcher&#x27;s value proposition than judgment, context, and storytelling.\"AI can surface missed insights — but it still needs a human to judge what really matters,\" Topiol said in the report.What other knowledge workers can learn from the research industry&#x27;s AI experimentThe market research industry&#x27;s AI adoption may presage similar patterns in other knowledge work professions where the technology promises to accelerate analysis and synthesis. The experience of researchers — early AI adopters who have integrated the technology into daily workflows — offers lessons about both opportunities and pitfalls.First, speed genuinely matters. One boutique agency research lead quoted in the report described watching survey results accumulate in real-time after fielding: \"After submitting it for fielding, I literally watched the survey count climb and finish the same afternoon. It was a remarkable turnaround.\" That velocity enables researchers to respond to business questions within hours rather than weeks, making insights actionable while decisions are still being made rather than after the fact.Second, the productivity gains are real but uneven. Saving five hours per week represents meaningful efficiency for individual contributors, but those savings can disappear if spent validating AI outputs or correcting errors. The net benefit depends on the specific task, the quality of the AI tool, and the user&#x27;s skill in prompting and reviewing the technology&#x27;s work.Third, the skills required for research are changing. The report identifies future competencies including cultural fluency, strategic storytelling, ethical stewardship, and what it calls \"inquisitive insight advocacy\" — the ability to ask the right questions, validate AI outputs, and frame insights for maximum business impact. Technical execution, while still important, becomes less differentiating as AI handles more of the mechanical work.The strange phenomenon of using technology intensively while questioning its reliabilityThe survey&#x27;s most striking finding may be the persistence of trust issues despite widespread adoption. In most technology adoption curves, trust builds as users gain experience and tools mature. But with AI, researchers appear to be using tools intensively while simultaneously questioning their reliability — a dynamic driven by the technology&#x27;s pattern of performing well most of the time but failing unpredictably.This creates a verification burden that has no obvious endpoint. Unlike traditional software bugs that can be identified and fixed, AI systems&#x27; probabilistic nature means they may produce different outputs for the same inputs, making it difficult to develop reliable quality assurance processes.The data privacy concerns — cited by 33% as the biggest barrier to adoption — reflect a different dimension of trust. Researchers worry not just about whether AI produces accurate outputs but also about what happens to the sensitive data they feed into these systems. QuestDIY&#x27;s approach, according to the report, is to build AI directly into a research platform with ISO/IEC 27001 certification rather than requiring researchers to use general-purpose tools like ChatGPT that may store and learn from user inputs.\"The center of gravity is analysis at scale — fusing multiple sources, handling both structured and unstructured data, and automating reporting,\" Topiol said in the report, describing where AI delivers the most value.The future of research work: elevation or endless verification?The report positions 2026 as an inflection point when AI moves from being a tool researchers use to something more like a team member — what the authors call a \"co-analyst\" that participates in the research process rather than merely accelerating specific tasks.This vision assumes continued improvement in AI capabilities, particularly in areas where researchers currently see the technology as underdeveloped. While 41% currently use AI for survey design, 37% for programming, and 30% for proposal creation, most researchers consider these appropriate use cases, suggesting significant room for growth once the tools become more reliable or the workflows more structured.The human-led model appears likely to persist. \"The future is human-led, with AI as a trusted co-analyst,\" Parker said in the report. But what \"human-led\" means in practice may shift. If AI handles most analytical tasks and researchers focus on validation and strategic interpretation, the profession may come to resemble editorial work more than scientific analysis — curating and contextualizing machine-generated insights rather than producing them from scratch.\"AI gives researchers the space to move up the value chain – from data gatherers to Insight Advocates, focused on maximising business impact,\" Topiol said in the report.Whether this transformation marks an elevation of the profession or a deskilling depends partly on how the technology evolves. If AI systems become more transparent and reliable, the verification burden may decrease and researchers can focus on higher-order thinking. If they remain opaque and error-prone, researchers may find themselves trapped in an endless cycle of checking work produced by tools they cannot fully trust or explain.The survey data suggests researchers are navigating this uncertainty by developing a form of professional muscle memory — learning which tasks AI handles well, where it tends to fail, and how much oversight each type of output requires. This tacit knowledge, accumulated through daily use and occasional failures, may become as important to the profession as statistical literacy or survey design principles.Yet the fundamental tension remains unresolved. Researchers are moving faster than ever, delivering insights in hours instead of weeks, and handling analytical tasks that would have been impossible without AI. But they&#x27;re doing so while shouldering a new responsibility that previous generations never faced: serving as the quality control layer between powerful but unpredictable machines and business leaders making million-dollar decisions.The industry has made its bet. Now comes the harder part: proving that human judgment can keep pace with machine speed — and that the insights produced by this uneasy partnership are worth the trust clients place in them.",
          "content": "Market researchers have embraced artificial intelligence at a staggering pace, with 98% of professionals now incorporating AI tools into their work and 72% using them daily or more frequently, according to a new industry survey that reveals both the technology&#x27;s transformative promise and its persistent reliability problems.The findings, based on responses from 219 U.S. market research and insights professionals surveyed in August 2025 by QuestDIY, a research platform owned by The Harris Poll, paint a picture of an industry caught between competing pressures: the demand to deliver faster business insights and the burden of validating everything AI produces to ensure accuracy.While more than half of researchers — 56% — report saving at least five hours per week using AI tools, nearly four in ten say they&#x27;ve experienced \"increased reliance on technology that sometimes produces errors.\" An additional 37% report that AI has \"introduced new risks around data quality or accuracy,\" and 31% say the technology has \"led to more work re-checking or validating AI outputs.\"The disconnect between productivity gains and trustworthiness has created what amounts to a grand bargain in the research industry: professionals accept time savings and enhanced capabilities in exchange for constant vigilance over AI&#x27;s mistakes, a dynamic that may fundamentally reshape how insights work gets done.How market researchers went from AI skeptics to daily users in less than a yearThe numbers suggest AI has moved from experiment to infrastructure in record time. Among those using AI daily, 39% deploy it once per day, while 33% use it \"several times per day or more,\" according to the survey conducted between August 15-19, 2025. Adoption is accelerating: 80% of researchers say they&#x27;re using AI more than they were six months ago, and 71% expect to increase usage over the next six months. Only 8% anticipate their usage will decline.“While AI provides excellent assistance and opportunities, human judgment will remain vital,” Erica Parker, Managing Director Research Products at The Harris Poll, told VentureBeat. “The future is a teamwork dynamic where AI will accelerate tasks and quickly unearth findings, while researchers will ensure quality and provide high level consultative insights.”The top use cases reflect AI&#x27;s strength in handling data at scale: 58% of researchers use it for analyzing multiple data sources, 54% for analyzing structured data, 50% for automating insight reports, 49% for analyzing open-ended survey responses, and 48% for summarizing findings. These tasks—traditionally labor-intensive and time-consuming — now happen in minutes rather than hours.Beyond time savings, researchers report tangible quality improvements. Some 44% say AI improves accuracy, 43% report it helps surface insights they might otherwise have missed, 43% cite increased speed of insights delivery, and 39% say it sparks creativity. The overwhelming majority — 89% — say AI has made their work lives better, with 25% describing the improvement as \"significant.\"The productivity paradox: saving time while creating new validation workYet the same survey reveals deep unease about the technology&#x27;s reliability. The list of concerns is extensive: 39% of researchers report increased reliance on error-prone technology, 37% cite new risks around data quality or accuracy, 31% describe additional validation work, 29% report uncertainty about job security, and 28% say AI has raised concerns about data privacy and ethics.The report notes that \"accuracy is the biggest frustration with AI experienced by researchers when asked on an open-ended basis.\" One researcher captured the tension succinctly: \"The faster we move with AI, the more we need to check if we&#x27;re moving in the right direction.\"This paradox — saving time while simultaneously creating new work — reflects a fundamental characteristic of current AI systems, which can produce outputs that appear authoritative but contain what researchers call \"hallucinations,\" or fabricated information presented as fact. The challenge is particularly acute in a profession where credibility depends on methodological rigor and where incorrect data can lead clients to make costly business decisions.\"Researchers view AI as a junior analyst, capable of speed and breadth, but needing oversight and judgment,\" said Gary Topiol, Managing Director at QuestDIY, in the report.That metaphor — AI as junior analyst — captures the industry&#x27;s current operating model. Researchers treat AI outputs as drafts requiring senior review rather than finished products, a workflow that provides guardrails but also underscores the technology&#x27;s limitations.Why data privacy fears are the biggest obstacle to AI adoption in researchWhen asked what would limit AI use at work, researchers identified data privacy and security concerns as the greatest barrier, cited by 33% of respondents. This concern isn&#x27;t abstract: researchers handle sensitive customer data, proprietary business information, and personally identifiable information subject to regulations like GDPR and CCPA. Sharing that data with AI systems — particularly cloud-based large language models — raises legitimate questions about who controls the information and whether it might be used to train models accessible to competitors.Other significant barriers include time to experiment and learn new tools (32%), training (32%), integration challenges (28%), internal policy restrictions (25%), and cost (24%). An additional 31% cited lack of transparency in AI use as a concern, which could complicate explaining results to clients and stakeholders.The transparency issue is particularly thorny. When an AI system produces an analysis or insight, researchers often cannot trace how the system arrived at its conclusion — a problem that conflicts with the scientific method&#x27;s emphasis on replicability and clear methodology. Some clients have responded by including no-AI clauses in their contracts, forcing researchers to either avoid the technology entirely or use it in ways that don&#x27;t technically violate contractual terms but may blur ethical lines.\"Onboarding beats feature bloat,\" Parker said in the report. \"The biggest brakes are time to learn and train. Packaged workflows, templates, and guided setup all unlock usage faster than piling on capabilities.\"Inside the new workflow: treating AI like a junior analyst who needs constant supervisionDespite these challenges, researchers aren&#x27;t abandoning AI — they&#x27;re developing frameworks to use it responsibly. The consensus model, according to the survey, is \"human-led research supported by AI,\" where AI handles repetitive tasks like coding, data cleaning, and report generation while humans focus on interpretation, strategy, and business impact.About one-third of researchers (29%) describe their current workflow as \"human-led with significant AI support,\" while 31% characterize it as \"mostly human with some AI help.\" Looking ahead to 2030, 61% envision AI as a \"decision-support partner\" with expanded capabilities including generative features for drafting surveys and reports (56%), AI-driven synthetic data generation (53%), automation of core processes like project setup and coding (48%), predictive analytics (44%), and deeper cognitive insights (43%).The report describes an emerging division of labor where researchers become \"Insight Advocates\" — professionals who validate AI outputs, connect findings to stakeholder challenges, and translate machine-generated analysis into strategic narratives that drive business decisions. In this model, technical execution becomes less central to the researcher&#x27;s value proposition than judgment, context, and storytelling.\"AI can surface missed insights — but it still needs a human to judge what really matters,\" Topiol said in the report.What other knowledge workers can learn from the research industry&#x27;s AI experimentThe market research industry&#x27;s AI adoption may presage similar patterns in other knowledge work professions where the technology promises to accelerate analysis and synthesis. The experience of researchers — early AI adopters who have integrated the technology into daily workflows — offers lessons about both opportunities and pitfalls.First, speed genuinely matters. One boutique agency research lead quoted in the report described watching survey results accumulate in real-time after fielding: \"After submitting it for fielding, I literally watched the survey count climb and finish the same afternoon. It was a remarkable turnaround.\" That velocity enables researchers to respond to business questions within hours rather than weeks, making insights actionable while decisions are still being made rather than after the fact.Second, the productivity gains are real but uneven. Saving five hours per week represents meaningful efficiency for individual contributors, but those savings can disappear if spent validating AI outputs or correcting errors. The net benefit depends on the specific task, the quality of the AI tool, and the user&#x27;s skill in prompting and reviewing the technology&#x27;s work.Third, the skills required for research are changing. The report identifies future competencies including cultural fluency, strategic storytelling, ethical stewardship, and what it calls \"inquisitive insight advocacy\" — the ability to ask the right questions, validate AI outputs, and frame insights for maximum business impact. Technical execution, while still important, becomes less differentiating as AI handles more of the mechanical work.The strange phenomenon of using technology intensively while questioning its reliabilityThe survey&#x27;s most striking finding may be the persistence of trust issues despite widespread adoption. In most technology adoption curves, trust builds as users gain experience and tools mature. But with AI, researchers appear to be using tools intensively while simultaneously questioning their reliability — a dynamic driven by the technology&#x27;s pattern of performing well most of the time but failing unpredictably.This creates a verification burden that has no obvious endpoint. Unlike traditional software bugs that can be identified and fixed, AI systems&#x27; probabilistic nature means they may produce different outputs for the same inputs, making it difficult to develop reliable quality assurance processes.The data privacy concerns — cited by 33% as the biggest barrier to adoption — reflect a different dimension of trust. Researchers worry not just about whether AI produces accurate outputs but also about what happens to the sensitive data they feed into these systems. QuestDIY&#x27;s approach, according to the report, is to build AI directly into a research platform with ISO/IEC 27001 certification rather than requiring researchers to use general-purpose tools like ChatGPT that may store and learn from user inputs.\"The center of gravity is analysis at scale — fusing multiple sources, handling both structured and unstructured data, and automating reporting,\" Topiol said in the report, describing where AI delivers the most value.The future of research work: elevation or endless verification?The report positions 2026 as an inflection point when AI moves from being a tool researchers use to something more like a team member — what the authors call a \"co-analyst\" that participates in the research process rather than merely accelerating specific tasks.This vision assumes continued improvement in AI capabilities, particularly in areas where researchers currently see the technology as underdeveloped. While 41% currently use AI for survey design, 37% for programming, and 30% for proposal creation, most researchers consider these appropriate use cases, suggesting significant room for growth once the tools become more reliable or the workflows more structured.The human-led model appears likely to persist. \"The future is human-led, with AI as a trusted co-analyst,\" Parker said in the report. But what \"human-led\" means in practice may shift. If AI handles most analytical tasks and researchers focus on validation and strategic interpretation, the profession may come to resemble editorial work more than scientific analysis — curating and contextualizing machine-generated insights rather than producing them from scratch.\"AI gives researchers the space to move up the value chain – from data gatherers to Insight Advocates, focused on maximising business impact,\" Topiol said in the report.Whether this transformation marks an elevation of the profession or a deskilling depends partly on how the technology evolves. If AI systems become more transparent and reliable, the verification burden may decrease and researchers can focus on higher-order thinking. If they remain opaque and error-prone, researchers may find themselves trapped in an endless cycle of checking work produced by tools they cannot fully trust or explain.The survey data suggests researchers are navigating this uncertainty by developing a form of professional muscle memory — learning which tasks AI handles well, where it tends to fail, and how much oversight each type of output requires. This tacit knowledge, accumulated through daily use and occasional failures, may become as important to the profession as statistical literacy or survey design principles.Yet the fundamental tension remains unresolved. Researchers are moving faster than ever, delivering insights in hours instead of weeks, and handling analytical tasks that would have been impossible without AI. But they&#x27;re doing so while shouldering a new responsibility that previous generations never faced: serving as the quality control layer between powerful but unpredictable machines and business leaders making million-dollar decisions.The industry has made its bet. Now comes the harder part: proving that human judgment can keep pace with machine speed — and that the insights produced by this uneasy partnership are worth the trust clients place in them.",
          "feed_position": 3,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/69hFvbU4ambt5HRDl1GrZa/1a33a47f699ce8af5cf3783ee71cb52c/nuneybits_Vector_art_of_magnifying_glass_revealing_AI_errors_fed1833a-d173-4100-bd60-4e3416c7e83b.webp?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/forget-fine-tuning-saps-rpt-1-brings-ready-to-use-ai-for-business-tasks",
          "published_at": "Tue, 04 Nov 2025 05:00:00 GMT",
          "title": "Forget Fine-Tuning: SAP’s RPT-1 Brings Ready-to-Use AI for Business Tasks",
          "standfirst": "SAP aims to displace more general large language models with the release of its own foundational “tabular” model, which the company claims will reduce training requirements for enterprises. The model, called SAP RPT-1, is a pre-trained model with business and enterprise knowledge out of the box. SAP calls it a Relational Foundation Model, meaning it can do predictions based on relational databases even without fine-tuning or additional training.Walter Sun, SAP&#x27;s global head of AI, told VentureBeat in an interview that the value of the new model lies in its ability to perform various enterprise tasks, such as predictive analytics, out of the box. “Everyone knows about language models, and there’s a bunch of good ones that already exist,” Sun said. “But we trained the model on data on business transactions, basically Excel spreadsheets, and so we have a model that can do predictive analytics where the value is that it’s out of the box, meaning you don’t need to have specifics of a company to do tasks analogous to a language model.” Sun said that right out of the gate, RPT-1 can essentially build out a business model for enterprises based on its knowledge gained from data from SAP’s decades of information. Organizations can plug the model directly into applications, even without additional fine-tuning.RPT-1, SAP’s first large family of AI models, will be generally available in “Q4 of 2025” and be deployed via SAP’s AI Foundation. While RPT-1 is currently available, the company stated that additional models will be made available soon, including an open-source, state-of-the-art model. SAP will also release a no-code playground environment to experiment with the model. Tabular models vs LLMs Tabular or relational AI models learned from spreadsheets, unlike LLMs, which learned from text and code. RPT-1 not only understands numbers and the relationships between different cells, but it’s also able to provide more structured and precise answers. When enterprises decide to use RPT-1, they can add more direction to the model through a bit of context engineering, since the model is semantically aware and learns based on how it is being used. SAP researchers first proposed the idea that tabular models can both exhibit semantic awareness and learn from content through a paper published in June. It proposed ConTextTab introduced context-aware pretraining. It utilizes semantic signals, such as table headers or column types, to guide model training, enabling the model to build a relational structure with the data. It’s this architecture that makes the model work best for tasks with precise answers, such as for financial or enterprise use cases.The RPT models build on the ConTextTab work that lets it learn structured business data, say from SAP’s knowledge graph, and then be able to add more context through usage. SAP researchers did test ConTextTab against benchmarks, saying it “is competitive” against similar models like TabPFN and TabIFL. Industry-specific models continue to grow Many enterprises prefer to fine-tune general LLMs like GPT-5 or Claude, to basically retrain the model to answer only questions relevant to their business. However, a shift towards industry-specific models has begun to take root. Sun said that his experience at a previous company, building a very narrow, highly customized AI model for sentiment analysis, influenced a lot of what makes RPT-1 different. “It was a very customized model, a narrow model that takes specific feedback for specific products but it wasn’t scalable,” Sun said. “When LLMs came about, that one model measures sentiment. But there are use cases that we can do that LLMs cannot do.”He said these use cases include predictions, such as determining when a shopper will return to a grocery store, which may involve numerical analysis along with an understanding of the shopper’s buying habits. However, some LLMs have begun integrating into spreadsheets, and AI model providers encourage users to upload similar data to teach them context. Microsoft added new capabilities to Copilot, including the ability to work in Excel. Anthropic integrated its Claude model with Excel, complementing its Claude for Finance service. Chinese startup Manus also offers a data visualization tool that understands spreadsheets, and ChatGPT can create charts from uploaded spreadsheets and other data sources. However, SAP noted that it is more than just reading a spreadsheet; RPT-1 should stand out amongst its competitors because it requires fewer additional pieces of information about a business to provide its responses.",
          "content": "SAP aims to displace more general large language models with the release of its own foundational “tabular” model, which the company claims will reduce training requirements for enterprises. The model, called SAP RPT-1, is a pre-trained model with business and enterprise knowledge out of the box. SAP calls it a Relational Foundation Model, meaning it can do predictions based on relational databases even without fine-tuning or additional training.Walter Sun, SAP&#x27;s global head of AI, told VentureBeat in an interview that the value of the new model lies in its ability to perform various enterprise tasks, such as predictive analytics, out of the box. “Everyone knows about language models, and there’s a bunch of good ones that already exist,” Sun said. “But we trained the model on data on business transactions, basically Excel spreadsheets, and so we have a model that can do predictive analytics where the value is that it’s out of the box, meaning you don’t need to have specifics of a company to do tasks analogous to a language model.” Sun said that right out of the gate, RPT-1 can essentially build out a business model for enterprises based on its knowledge gained from data from SAP’s decades of information. Organizations can plug the model directly into applications, even without additional fine-tuning.RPT-1, SAP’s first large family of AI models, will be generally available in “Q4 of 2025” and be deployed via SAP’s AI Foundation. While RPT-1 is currently available, the company stated that additional models will be made available soon, including an open-source, state-of-the-art model. SAP will also release a no-code playground environment to experiment with the model. Tabular models vs LLMs Tabular or relational AI models learned from spreadsheets, unlike LLMs, which learned from text and code. RPT-1 not only understands numbers and the relationships between different cells, but it’s also able to provide more structured and precise answers. When enterprises decide to use RPT-1, they can add more direction to the model through a bit of context engineering, since the model is semantically aware and learns based on how it is being used. SAP researchers first proposed the idea that tabular models can both exhibit semantic awareness and learn from content through a paper published in June. It proposed ConTextTab introduced context-aware pretraining. It utilizes semantic signals, such as table headers or column types, to guide model training, enabling the model to build a relational structure with the data. It’s this architecture that makes the model work best for tasks with precise answers, such as for financial or enterprise use cases.The RPT models build on the ConTextTab work that lets it learn structured business data, say from SAP’s knowledge graph, and then be able to add more context through usage. SAP researchers did test ConTextTab against benchmarks, saying it “is competitive” against similar models like TabPFN and TabIFL. Industry-specific models continue to grow Many enterprises prefer to fine-tune general LLMs like GPT-5 or Claude, to basically retrain the model to answer only questions relevant to their business. However, a shift towards industry-specific models has begun to take root. Sun said that his experience at a previous company, building a very narrow, highly customized AI model for sentiment analysis, influenced a lot of what makes RPT-1 different. “It was a very customized model, a narrow model that takes specific feedback for specific products but it wasn’t scalable,” Sun said. “When LLMs came about, that one model measures sentiment. But there are use cases that we can do that LLMs cannot do.”He said these use cases include predictions, such as determining when a shopper will return to a grocery store, which may involve numerical analysis along with an understanding of the shopper’s buying habits. However, some LLMs have begun integrating into spreadsheets, and AI model providers encourage users to upload similar data to teach them context. Microsoft added new capabilities to Copilot, including the ability to work in Excel. Anthropic integrated its Claude model with Excel, complementing its Claude for Finance service. Chinese startup Manus also offers a data visualization tool that understands spreadsheets, and ChatGPT can create charts from uploaded spreadsheets and other data sources. However, SAP noted that it is more than just reading a spreadsheet; RPT-1 should stand out amongst its competitors because it requires fewer additional pieces of information about a business to provide its responses.",
          "feed_position": 4,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/5g7xV36aNl7PqllpWpReld/6ffe5f74c51111a0fcac396e36cbe565/crimedy7_illustration_of_excel_sheets_but_digital_and_electro_5a6d1d66-2e06-4305-bd78-4ccb05874d5d_3.png?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/inside-zendesks-dual-ai-leap-from-reliable-agents-to-real-time-intelligence",
          "published_at": "Tue, 04 Nov 2025 04:00:00 GMT",
          "title": "Inside Zendesk’s dual AI leap: From reliable agents to real-time intelligence with GPT-5 and HyperArc",
          "standfirst": "Presented by ZendeskAgentic AI is currently transforming three key areas of work — creative, coding, and support — says Shashi Upadhyay, president of engineering, AI, and product at Zendesk. But he notes that support presents a distinct challenge. \"Support is special because you’re putting an autonomous AI agent right in front of your customer,\" Upadhyay says. \"You have to be confident that it’s going to do the right thing for the customer and by the customer. Every step forward in AI should make service more dependable for both customers and human agents.\" Zendesk, recently named a Leader in the 2025 Gartner Magic Quadrant for the CRM Customer Engagement Center, started implementing AI agents about a year and a half ago. Since then, they&#x27;ve seen that AI agents can solve almost 80% of all incoming customer requests on their own. For the remaining 20%, the AI agent can hand it over to a human to help solve the more complex problems. \"Autonomous AI agents work 24/7, with no wait or queue time. You have a problem; they provide an answer right away. All of that adds up,\" he says. \"Not only do you get higher resolutions, higher automation, but you can also improve the CSAT at the same time. Because 80% is such a promising number, and the results are so solid, we believe it’s only a matter of time before everyone adopts this technology. We already see that across the board.\"The company&#x27;s efforts to advance its standard of usability, depth of insight, and time to value for organizations of all sizes require continuous testing, integration of advanced models like ChatGPT-5, and a major upgrade of its analytics capabilities and real-time, gen AI–powered insights with the acquisition of HyperArc, an AI-native analytics platform.Designing, testing, and deploying a better agent\"In a support context especially, it’s important AI agents behave consistently with the brand of the company, policies, and regulatory requirements you may have,\" Upadhyay says. \"We test every agent, every model continuously across all our customers. We do it before we release it and we do it after we release it, across five categories.\" Those categories — automation rate, execution, precision, latency, and safety — form the foundation of Zendesk’s ongoing benchmarking program. Each model is scored on how accurately it resolves issues, how well it follows instructions, how fast it responds, and whether it stays within clearly defined guardrails. The goal isn’t just to make AI faster — it’s to make it dependable, accountable, and aligned with the standards that define great customer service.That testing is reinforced by Zendesk’s QA agent — an automated monitor that keeps a constant eye on every conversation. If an exchange starts to drift off course, whether in tone or accuracy, the system immediately flags it and alerts a human agent to step in. It’s an added layer of assurance that keeps the customer experience on track, even when AI is running the first line of support.GPT-5 for next-level agentsIn the world of support and service, the move from simple chatbots that answer basic queries or solve uncomplicated problems, to agents that actually take action, is groundbreaking. An agent that can understand that a customer wants to return an item, confirm whether it&#x27;s eligible for a return, process the return, and issue a refund, is a powerful upgrade. With the introduction of ChatGPT-5, Zendesk recognized an opportunity to integrate that ability into its Resolution Platform.\"We worked very closely with OpenAI because GPT-5 was a pretty big improvement in model capabilities, going from being able to answer questions, to being able to reason and take action,\" Upadhyay says. \"First, it does a much better job at solving problems autonomously. Secondly, it&#x27;s much better at understanding your intent, which improves the customer experience because you feel understood. Last but not least, it has 95%-plus reliability on executing correctly.\"Those gains ripple across Zendesk’s AI agents, Copilot, and App Builder. GPT-5 cuts workflow failures by 30%, thanks to its ability to adapt to unexpected complexity without losing context, and reduces fallback escalations by more than 20%, with more complete and accurate responses. The result: faster resolutions, fewer hand-offs, and AI that behaves more like a seasoned support professional than a scripted assistant.Plus, GPT-5 is better at handling ambiguity, and able to clarify vague customer input, which improves routing and increases automated workflows in over 65% of conversations. It has greater accuracy across five languages, and makes agents more productive with more concise, contextually relevant answers that align with tone guidelines.And in App Builder, GPT-5 delivered 25% to 30% faster overall performance, with more prompt iterations per minute, speeding app builder development workflows.Filling in the analytics gapTraditionally, support analytics has focused on structured data — the kind that fits neatly into a table: when a ticket was opened, who handled it, how long it took to resolve, and when it was closed. But the most valuable insights often live in unstructured data — the conversations themselves, spread across email, chat, voice, and messaging apps like WhatsApp.\"Customers often don’t realize how much intelligence sits in their support interactions,\" Upadhyay says. \"What we’re pushing for with analytics is ways in which we can improve the entire company with the insights that are sitting in support data.\"To surface those deeper insights, Zendesk turned to HyperArc, an AI-native analytics company known for its proprietary HyperGraph engine and generative-AI-powered insights. The acquisition gave new life to Explore, Zendesk’s analytics platform, transforming it into a modern solution capable of merging structured and unstructured data, supporting conversational interfaces, and drawing on persistent memory to use past interactions as context for new queries.\"Your support interactions are telling you everything that’s not working in your business today, all that information is sitting in these millions of tickets that you’ve collected over time,\" Upadhyay says. \"We wanted to make that completely visible. Now we have this genius AI agent that can analyze it all and come back with explicit recommendations. That doesn’t just improve support. It improves the entire company.\"That visibility now translates into actionable intelligence. The system can pinpoint where issues are most persistent, identify the patterns behind them, and suggest ways to resolve them. It can even anticipate problems before they happen. During high-pressure events like Black Friday, for example, it can analyze historical data to flag recurring issues, predict where new bottlenecks might appear, and recommend preventive measures — turning reactive support into proactive strategy.\"That’s where HyperArc shines,\" Upadhyay says. It doesn’t just help you understand the past — it helps you plan better for the future.\"By integrating HyperArc’s AI-native intelligence, Zendesk is moving customer service toward continuous learning — where every interaction builds trust and sharpens performance, setting the stage for AI that can see what’s coming next.Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.",
          "content": "Presented by ZendeskAgentic AI is currently transforming three key areas of work — creative, coding, and support — says Shashi Upadhyay, president of engineering, AI, and product at Zendesk. But he notes that support presents a distinct challenge. \"Support is special because you’re putting an autonomous AI agent right in front of your customer,\" Upadhyay says. \"You have to be confident that it’s going to do the right thing for the customer and by the customer. Every step forward in AI should make service more dependable for both customers and human agents.\" Zendesk, recently named a Leader in the 2025 Gartner Magic Quadrant for the CRM Customer Engagement Center, started implementing AI agents about a year and a half ago. Since then, they&#x27;ve seen that AI agents can solve almost 80% of all incoming customer requests on their own. For the remaining 20%, the AI agent can hand it over to a human to help solve the more complex problems. \"Autonomous AI agents work 24/7, with no wait or queue time. You have a problem; they provide an answer right away. All of that adds up,\" he says. \"Not only do you get higher resolutions, higher automation, but you can also improve the CSAT at the same time. Because 80% is such a promising number, and the results are so solid, we believe it’s only a matter of time before everyone adopts this technology. We already see that across the board.\"The company&#x27;s efforts to advance its standard of usability, depth of insight, and time to value for organizations of all sizes require continuous testing, integration of advanced models like ChatGPT-5, and a major upgrade of its analytics capabilities and real-time, gen AI–powered insights with the acquisition of HyperArc, an AI-native analytics platform.Designing, testing, and deploying a better agent\"In a support context especially, it’s important AI agents behave consistently with the brand of the company, policies, and regulatory requirements you may have,\" Upadhyay says. \"We test every agent, every model continuously across all our customers. We do it before we release it and we do it after we release it, across five categories.\" Those categories — automation rate, execution, precision, latency, and safety — form the foundation of Zendesk’s ongoing benchmarking program. Each model is scored on how accurately it resolves issues, how well it follows instructions, how fast it responds, and whether it stays within clearly defined guardrails. The goal isn’t just to make AI faster — it’s to make it dependable, accountable, and aligned with the standards that define great customer service.That testing is reinforced by Zendesk’s QA agent — an automated monitor that keeps a constant eye on every conversation. If an exchange starts to drift off course, whether in tone or accuracy, the system immediately flags it and alerts a human agent to step in. It’s an added layer of assurance that keeps the customer experience on track, even when AI is running the first line of support.GPT-5 for next-level agentsIn the world of support and service, the move from simple chatbots that answer basic queries or solve uncomplicated problems, to agents that actually take action, is groundbreaking. An agent that can understand that a customer wants to return an item, confirm whether it&#x27;s eligible for a return, process the return, and issue a refund, is a powerful upgrade. With the introduction of ChatGPT-5, Zendesk recognized an opportunity to integrate that ability into its Resolution Platform.\"We worked very closely with OpenAI because GPT-5 was a pretty big improvement in model capabilities, going from being able to answer questions, to being able to reason and take action,\" Upadhyay says. \"First, it does a much better job at solving problems autonomously. Secondly, it&#x27;s much better at understanding your intent, which improves the customer experience because you feel understood. Last but not least, it has 95%-plus reliability on executing correctly.\"Those gains ripple across Zendesk’s AI agents, Copilot, and App Builder. GPT-5 cuts workflow failures by 30%, thanks to its ability to adapt to unexpected complexity without losing context, and reduces fallback escalations by more than 20%, with more complete and accurate responses. The result: faster resolutions, fewer hand-offs, and AI that behaves more like a seasoned support professional than a scripted assistant.Plus, GPT-5 is better at handling ambiguity, and able to clarify vague customer input, which improves routing and increases automated workflows in over 65% of conversations. It has greater accuracy across five languages, and makes agents more productive with more concise, contextually relevant answers that align with tone guidelines.And in App Builder, GPT-5 delivered 25% to 30% faster overall performance, with more prompt iterations per minute, speeding app builder development workflows.Filling in the analytics gapTraditionally, support analytics has focused on structured data — the kind that fits neatly into a table: when a ticket was opened, who handled it, how long it took to resolve, and when it was closed. But the most valuable insights often live in unstructured data — the conversations themselves, spread across email, chat, voice, and messaging apps like WhatsApp.\"Customers often don’t realize how much intelligence sits in their support interactions,\" Upadhyay says. \"What we’re pushing for with analytics is ways in which we can improve the entire company with the insights that are sitting in support data.\"To surface those deeper insights, Zendesk turned to HyperArc, an AI-native analytics company known for its proprietary HyperGraph engine and generative-AI-powered insights. The acquisition gave new life to Explore, Zendesk’s analytics platform, transforming it into a modern solution capable of merging structured and unstructured data, supporting conversational interfaces, and drawing on persistent memory to use past interactions as context for new queries.\"Your support interactions are telling you everything that’s not working in your business today, all that information is sitting in these millions of tickets that you’ve collected over time,\" Upadhyay says. \"We wanted to make that completely visible. Now we have this genius AI agent that can analyze it all and come back with explicit recommendations. That doesn’t just improve support. It improves the entire company.\"That visibility now translates into actionable intelligence. The system can pinpoint where issues are most persistent, identify the patterns behind them, and suggest ways to resolve them. It can even anticipate problems before they happen. During high-pressure events like Black Friday, for example, it can analyze historical data to flag recurring issues, predict where new bottlenecks might appear, and recommend preventive measures — turning reactive support into proactive strategy.\"That’s where HyperArc shines,\" Upadhyay says. It doesn’t just help you understand the past — it helps you plan better for the future.\"By integrating HyperArc’s AI-native intelligence, Zendesk is moving customer service toward continuous learning — where every interaction builds trust and sharpens performance, setting the stage for AI that can see what’s coming next.Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.",
          "feed_position": 5,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/3A1V6wmdmtvqCufzm5ImTe/d6992106fc91da6b10e9dc7a63a264d5/AdobeStock_1768576457.jpeg?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/entertainment/streaming/youtube-tv-blackout-with-disney-how-to-watch-espn-abc-and-more-as-a-youtube-tv-subscriber-173330635.html",
          "published_at": "Mon, 03 Nov 2025 23:36:24 +0000",
          "title": "YouTube TV blackout with Disney: How to watch ESPN, ABC and more as a YouTube TV subscriber",
          "standfirst": "SOPA Images via Getty Images It doesn't look like Disney-owned channels including ABC and ESPN will be returning to YouTube TV anytime soon. The Walt Disney Co. pulled its channels from YouTube TV as of midnight on Oct. 30 after the two companies failed to reach new terms on their latest carriage agreement. While big sporting events are often where the rubber meets the road on these channel blackouts, YouTube TV subscribers were unable to see any college football games on ABC or ESPN all weekend, and it looks like anyone hoping to watch tonight's Monday Night Football game between the Arizona Cardinals and the Dallas Cowboys will suffer the same fate: YouTube TV management has officially rebuffed Disney's request for a 24-hour restoration of its channels in a blog post — ostensibly to offer coverage of Tuesday's elections — proposing instead that Disney reactivate the feeds for ABC and ESPN while negotiations continue. YouTube TV had previously stated that if Disney’s channels remain off the platform for an extended period, customers will receive a $20 monthly credit. That's all fine and good, but if you're looking to watch tonight's game or your favorite shows — including Abbott Elementary, Grey's Anatomy and Dancing with the Stars, or Wednesday's NBA games — you'll need to seek out alternative viewing methods. And unfortunately for YouTube TV's negotiating position, there are plenty of options. One of the cheapest ways to watch ESPN is with a Sling Day Pass — for just $5/day, you can tune into any and all ESPN programming, including Monday Night Football, with no other commitments. If you want a full switch from YouTube TV, there's Hulu + Live TV, DirecTV, or Fubo, where you can watch all the Disney-owned channels. (Remember, unlike a lot of cable plans, you can easily pause or cancel YouTube TV or any of these alternatives, so long as you have month-to-month subscriptions.) If you're looking for a workaround to watch ESPN, the Disney Channel, ABC and more, here's are the best options so you won't miss a moment of sports, news, or entertainment, all pulled from our list of best live TV streaming services to cut cable. Grab an ESPN bundle so you won't miss the NFL, NBA or any other games Get Hulu + Live TV at a great price Try Fubo free for a week and get $30 your first month Try DirecTV free for 5 days, and get $30 off your first month What about Sling \"day passes\"? You may have heard that Sling offers day, weekend and week passes to its streaming programming for as little as $5 per day. That is an option if you're looking for just some of the ESPN channels (the Sling Orange tier), but ABC isn't included. (If you're just looking to catch one of this week's big games, like Monday Night Football on ESPN, it's a great short-term solution.) If you want a longer-term solution, you can get both ESPN and ABC with Sling's Orange and Blue package ($30 a month to start, $61 thereafter), but you'll need to add on the Sports Extra package for ESPNU, which requires an additional charge. Get your local Disney/ABC programming for free Need your local ABC programming? Your station may have its own free local streaming news channel (many do), you can see if The Roku Channel carries your local station's news, or download your local news station app if it's a Nexstar channel. The other alternative — if you're within the broadcast radius of a local ABC affiliate — is to get an over-the-air antenna. You can plug in your ZIP code at antennaweb.org to see what channels are in your area. This off-brand unit has worked very well in our initial testing — it's under $30, and the channels are truly free. What games are on ESPN/ABC this week? If you're wondering what games you might miss as a result of the YouTubeTV/Disney blackout, here's a list of some upcoming sports you may not want to miss: Monday, Nov. 3 Monday Night Football: Arizona Cardinals vs. Dallas Cowboys, 8:15 p.m. ET (ESPN/ABC) Wednesday, Nov. 5 NBA: Minnesota Timberwolves vs. New York Knicks, 7:30 p.m. ET (ESPN) NBA: San Antonio Spurs vs. Los Angeles Lakers, 10 p.m. ET (ESPN) Update Nov. 3 2025, 6:36PM ET: This story has been updated to include YouTube TV's latest response to Disney's request to restore its channels for just 24 hours.This article originally appeared on Engadget at https://www.engadget.com/entertainment/streaming/youtube-tv-blackout-with-disney-how-to-watch-espn-abc-and-more-as-a-youtube-tv-subscriber-173330635.html?src=rss",
          "content": "SOPA Images via Getty Images It doesn't look like Disney-owned channels including ABC and ESPN will be returning to YouTube TV anytime soon. The Walt Disney Co. pulled its channels from YouTube TV as of midnight on Oct. 30 after the two companies failed to reach new terms on their latest carriage agreement. While big sporting events are often where the rubber meets the road on these channel blackouts, YouTube TV subscribers were unable to see any college football games on ABC or ESPN all weekend, and it looks like anyone hoping to watch tonight's Monday Night Football game between the Arizona Cardinals and the Dallas Cowboys will suffer the same fate: YouTube TV management has officially rebuffed Disney's request for a 24-hour restoration of its channels in a blog post — ostensibly to offer coverage of Tuesday's elections — proposing instead that Disney reactivate the feeds for ABC and ESPN while negotiations continue. YouTube TV had previously stated that if Disney’s channels remain off the platform for an extended period, customers will receive a $20 monthly credit. That's all fine and good, but if you're looking to watch tonight's game or your favorite shows — including Abbott Elementary, Grey's Anatomy and Dancing with the Stars, or Wednesday's NBA games — you'll need to seek out alternative viewing methods. And unfortunately for YouTube TV's negotiating position, there are plenty of options. One of the cheapest ways to watch ESPN is with a Sling Day Pass — for just $5/day, you can tune into any and all ESPN programming, including Monday Night Football, with no other commitments. If you want a full switch from YouTube TV, there's Hulu + Live TV, DirecTV, or Fubo, where you can watch all the Disney-owned channels. (Remember, unlike a lot of cable plans, you can easily pause or cancel YouTube TV or any of these alternatives, so long as you have month-to-month subscriptions.) If you're looking for a workaround to watch ESPN, the Disney Channel, ABC and more, here's are the best options so you won't miss a moment of sports, news, or entertainment, all pulled from our list of best live TV streaming services to cut cable. Grab an ESPN bundle so you won't miss the NFL, NBA or any other games Get Hulu + Live TV at a great price Try Fubo free for a week and get $30 your first month Try DirecTV free for 5 days, and get $30 off your first month What about Sling \"day passes\"? You may have heard that Sling offers day, weekend and week passes to its streaming programming for as little as $5 per day. That is an option if you're looking for just some of the ESPN channels (the Sling Orange tier), but ABC isn't included. (If you're just looking to catch one of this week's big games, like Monday Night Football on ESPN, it's a great short-term solution.) If you want a longer-term solution, you can get both ESPN and ABC with Sling's Orange and Blue package ($30 a month to start, $61 thereafter), but you'll need to add on the Sports Extra package for ESPNU, which requires an additional charge. Get your local Disney/ABC programming for free Need your local ABC programming? Your station may have its own free local streaming news channel (many do), you can see if The Roku Channel carries your local station's news, or download your local news station app if it's a Nexstar channel. The other alternative — if you're within the broadcast radius of a local ABC affiliate — is to get an over-the-air antenna. You can plug in your ZIP code at antennaweb.org to see what channels are in your area. This off-brand unit has worked very well in our initial testing — it's under $30, and the channels are truly free. What games are on ESPN/ABC this week? If you're wondering what games you might miss as a result of the YouTubeTV/Disney blackout, here's a list of some upcoming sports you may not want to miss: Monday, Nov. 3 Monday Night Football: Arizona Cardinals vs. Dallas Cowboys, 8:15 p.m. ET (ESPN/ABC) Wednesday, Nov. 5 NBA: Minnesota Timberwolves vs. New York Knicks, 7:30 p.m. ET (ESPN) NBA: San Antonio Spurs vs. Los Angeles Lakers, 10 p.m. ET (ESPN) Update Nov. 3 2025, 6:36PM ET: This story has been updated to include YouTube TV's latest response to Disney's request to restore its channels for just 24 hours.This article originally appeared on Engadget at https://www.engadget.com/entertainment/streaming/youtube-tv-blackout-with-disney-how-to-watch-espn-abc-and-more-as-a-youtube-tv-subscriber-173330635.html?src=rss",
          "feed_position": 38,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2025-02/f18a9a20-ea15-11ef-b5d3-3ae5ab47679d"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/ai/google-removes-ai-model-after-it-allegedly-accused-a-senator-of-sexual-assault-170235679.html",
          "published_at": "Mon, 03 Nov 2025 17:02:35 +0000",
          "title": "Google removes AI model after it allegedly accused a senator of sexual assault",
          "standfirst": "Google has pulled the AI model Gemma from its Studio platform after a Republican senator said it \"fabricated serious criminal allegations\" against her, as reported by The Verge. Senator Marsha Blackburn (R-TN) sent a letter to CEO Sundar Pichai to accuse the company of defamation after the model allegedly created a story about her committing sexual assault. The model was reportedly asked if Blackburn had ever \"been accused of rape\" and it reportedly answered in the affirmative, going so far as to provide a list of fake news articles to support the accusation. The chatbot said the senator “was accused of having a sexual relationship with a state trooper” during a campaign for state senate. This officer reportedly said “she pressured him to obtain prescription drugs for her and that the relationship involved non-consensual acts.” Gemma is available via an API and was also available via AI Studio, which is a developer tool (in fact to use it you need to attest you're a developer). We’ve now seen reports of non-developers trying to use Gemma in AI Studio and ask it factual questions. We never intended this…— News from Google (@NewsFromGoogle) November 1, 2025 None of this happened, of course. The chatbot said this transgression occurred during Blackburn's 1987 campaign, but she didn't run for state senate until 1998. She has never been accused of anything like that. \"The links lead to error pages and unrelated news articles. There has never been such an accusation, there is no such individual and there are no such news stories. This is not a harmless 'hallucination.' It is an act of defamation produced and distributed by a Google-owned AI model,\" she wrote to Pichai. There's one major caveat here. The chatbot in question, Gemma, is designed for developers and not for mass market queries. There are Gemma variants for medical use, coding and more. Google says it was never meant as a consumer tool or to be used to answer factual questions. It's still pulling the model from AI Studio to \"prevent this confusion.\" It'll still be available to developers through the API. Google has reportedly removed Gemma from its AI studio after I demanded the company take it down for smearing conservatives with manufactured criminal allegations. Google owes the American people answers, and I will be eagerly awaiting their response to my letter. pic.twitter.com/n8ye5ZKBu1— Sen. Marsha Blackburn (@MarshaBlackburn) November 3, 2025 Blackburn went a step further, accusing Google's AI platform of engaging in a \"consistent pattern of bias against conservative figures.\" I encounter multiple hallucinations every day. Chatbots have lied about all kinds of stuff about my life and what I write about online. AI chatbots are famous for making stuff up, conservative or not. Not everything is a political witch hunt. Sometimes tech just does what tech does.This article originally appeared on Engadget at https://www.engadget.com/ai/google-removes-ai-model-after-it-allegedly-accused-a-senator-of-sexual-assault-170235679.html?src=rss",
          "content": "Google has pulled the AI model Gemma from its Studio platform after a Republican senator said it \"fabricated serious criminal allegations\" against her, as reported by The Verge. Senator Marsha Blackburn (R-TN) sent a letter to CEO Sundar Pichai to accuse the company of defamation after the model allegedly created a story about her committing sexual assault. The model was reportedly asked if Blackburn had ever \"been accused of rape\" and it reportedly answered in the affirmative, going so far as to provide a list of fake news articles to support the accusation. The chatbot said the senator “was accused of having a sexual relationship with a state trooper” during a campaign for state senate. This officer reportedly said “she pressured him to obtain prescription drugs for her and that the relationship involved non-consensual acts.” Gemma is available via an API and was also available via AI Studio, which is a developer tool (in fact to use it you need to attest you're a developer). We’ve now seen reports of non-developers trying to use Gemma in AI Studio and ask it factual questions. We never intended this…— News from Google (@NewsFromGoogle) November 1, 2025 None of this happened, of course. The chatbot said this transgression occurred during Blackburn's 1987 campaign, but she didn't run for state senate until 1998. She has never been accused of anything like that. \"The links lead to error pages and unrelated news articles. There has never been such an accusation, there is no such individual and there are no such news stories. This is not a harmless 'hallucination.' It is an act of defamation produced and distributed by a Google-owned AI model,\" she wrote to Pichai. There's one major caveat here. The chatbot in question, Gemma, is designed for developers and not for mass market queries. There are Gemma variants for medical use, coding and more. Google says it was never meant as a consumer tool or to be used to answer factual questions. It's still pulling the model from AI Studio to \"prevent this confusion.\" It'll still be available to developers through the API. Google has reportedly removed Gemma from its AI studio after I demanded the company take it down for smearing conservatives with manufactured criminal allegations. Google owes the American people answers, and I will be eagerly awaiting their response to my letter. pic.twitter.com/n8ye5ZKBu1— Sen. Marsha Blackburn (@MarshaBlackburn) November 3, 2025 Blackburn went a step further, accusing Google's AI platform of engaging in a \"consistent pattern of bias against conservative figures.\" I encounter multiple hallucinations every day. Chatbots have lied about all kinds of stuff about my life and what I write about online. AI chatbots are famous for making stuff up, conservative or not. Not everything is a political witch hunt. Sometimes tech just does what tech does.This article originally appeared on Engadget at https://www.engadget.com/ai/google-removes-ai-model-after-it-allegedly-accused-a-senator-of-sexual-assault-170235679.html?src=rss",
          "feed_position": 45
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/data-infrastructure/ai-coding-transforms-data-engineering-how-dlthubs-open-source-python-library",
          "published_at": "Mon, 03 Nov 2025 15:00:00 GMT",
          "title": "AI coding transforms data engineering: How dltHub's open-source Python library helps developers create data pipelines for AI in minutes",
          "standfirst": "A quiet revolution is reshaping enterprise data engineering. Python developers are building production data pipelines in minutes using tools that would have required entire specialized teams just months ago.The catalyst is dlt, an open-source Python library that automates complex data engineering tasks. The tool has reached 3 million monthly downloads and powers data workflows for more than 5,000 companies across regulated industries, including finance, healthcare and manufacturing. That technology is getting another solid vote of confidence today as dltHub, the Berlin-based company behind the open-source dlt library, is raising $8 million in seed funding led by Bessemer Venture Partners. What makes this significant isn&#x27;t just adoption numbers; it&#x27;s how developers are using the tool in combination with AI coding assistants to accomplish tasks that previously required infrastructure engineers, DevOps specialists and on-call personnel.The company is building a cloud-hosted platform that extends its open-source library into a complete end-to-end solution. The platform will allow developers to deploy pipelines, transformations and notebooks with a single command without worrying about infrastructure. This represents a fundamental shift from data engineering requiring specialized teams to becoming accessible to any Python developer.\"Any Python developer should be able to bring their business users closer to fresh, reliable data,\" Matthaus Krzykowski, dltHub&#x27;s co-founder and CEO, told VentureBeat in an exclusive interview. \"Our mission is to make data engineering as accessible, collaborative and as frictionless as writing Python itself.\"From SQL to Python-native data engineeringThe problem the company set out to solve emerged from real-world frustrations.One comes from a fundamental clash between how different generations of developers work with data. Krzykowski pointed to the generation of developers grounded in SQL and relational database technology. On the other hand, a generation of developers is building AI agents with Python.This divide reflects deeper technical challenges. SQL-based data engineering locks teams into specific platforms and requires extensive infrastructure knowledge. Python developers working on AI need lightweight, platform-agnostic tools that work in notebooks and integrate with large language model (LLM) coding assistants.The dlt library changes this equation by automating complex data engineering tasks in simple Python code. \"If you know what a function in Python is, what a list is, a source and resource, then you can write this very declarative, very simple code,\" Krzykowski explained.The key technical breakthrough addresses schema evolution automatically. When data sources change their output format, traditional pipelines break. \"DLT has mechanisms to automatically resolve these issues,\" Thierry Jean, founding engineer at dltHub, told VentureBeat. \"So it will push data, and you can say, &#x27;Alert me if things change upstream,&#x27; or just make it flexible enough and change the data and the destination in a way to accommodate.\"Real-world developer experienceHoyt Emerson, data consultant and content creator at The Full Data Stack, recently adopted the tool to move data from Google Cloud Storage to multiple destinations, including Amazon S3 and a data warehouse. Traditional approaches would require platform-specific knowledge for each destination. Emerson told VentureBeat that what he really wanted was a much more lightweight, platform-agnostic way to send data from one spot to another. \"That&#x27;s when DLT gave me the aha moment,\" Emerson said.He completed the entire pipeline in five minutes using the library&#x27;s documentation, which made it easy to get up and running quickly and without issue.The process gets even more powerful when combined with AI coding assistants. Emerson noted that he&#x27;s using agentic AI coding principles and realized that the dlt documentation could be sent as context to an LLM to accelerate and automate his data work. With documentation as context, Emerson was able to create reusable templates for future projects and used AI assistants to generate deployment configurations.\"It&#x27;s extremely LLM-friendly because it&#x27;s very well documented,\" he said.The LLM-native development patternThis combination of well-documented tools and AI assistance represents a new development pattern. The company has optimized specifically for what they call \"YOLO mode\" development, where developers copy error messages and paste them into AI coding assistants.\"A lot of these people are literally just copying and pasting error messages and are trying the code editors to figure it out,\" Krzykowski said. The company takes this behavior seriously enough that they fix issues specifically for AI-assisted workflows.The results speak to the approach&#x27;s effectiveness. In September alone, users created more than 50,000 custom connectors using the library. That represents a 20X increase since January, driven largely by LLM-assisted development.Technical architecture for enterprise scaleThe dlt design philosophy prioritizes interoperability over platform lock-in. The tool can deploy anywhere from AWS Lambda to existing enterprise data stacks. It integrates with platforms like Snowflake, while maintaining the flexibility to work with any destination.\"We always believe that DLT needs to be interoperable and modular,\" Krzykowski explained. \"It can be deployed anywhere. It can be on Lambda. It often becomes part of other people&#x27;s data infrastructures.\"Key technical capabilities include:Automatic schema evolution: Handles upstream data changes without breaking pipelines or requiring manual intervention.Incremental loading: Processes only new or changed records, reducing computational overhead and costs.Platform agnostic deployment: Works across cloud providers and on-premises infrastructure without modification.LLM-optimized documentation: Structured specifically for AI assistant consumption, enabling rapid problem-solving and template generation.The platform currently supports more than 4,600 REST API data sources with continuous expansion driven by user-generated connectors.Competing against ETL giants with a code-first approachThe data engineering landscape splits into distinct camps, each serving different enterprise needs and developer preferences. Traditional ETL platforms like Informatica and Talend dominate enterprise environments with GUI-based tools that require specialized training but offer comprehensive governance features.Newer SaaS platforms like Fivetran have gained traction by emphasizing pre-built connectors and managed infrastructure, reducing operational overhead but creating vendor dependency.The open-source dlt library occupies a fundamentally different position as code-first, LLM-native infrastructure that developers can extend and customize. This positioning reflects the broader shift toward what the industry calls the composable data stack, where enterprises build infrastructure from interoperable components rather than monolithic platforms.More importantly, the intersection with AI creates new market dynamics. \"LLMs aren&#x27;t replacing data engineers,\" Krzykowski said. \"But they radically expand their reach and productivity.\"What this means for enterprise data leadersFor enterprises looking to lead in AI-driven operations, this development represents an opportunity to fundamentally rethink data engineering strategies.The immediate tactical advantages are clear. Organizations can leverage existing Python developers instead of hiring specialized data engineering teams. Organizations that adapt their tooling and hiring approaches to leverage this trend may find significant cost and agility advantages over competitors still dependent on traditional, team-intensive data engineering.The question isn&#x27;t whether this shift toward democratized data engineering will occur. It&#x27;s how quickly enterprises will adapt to capitalize on it.",
          "content": "A quiet revolution is reshaping enterprise data engineering. Python developers are building production data pipelines in minutes using tools that would have required entire specialized teams just months ago.The catalyst is dlt, an open-source Python library that automates complex data engineering tasks. The tool has reached 3 million monthly downloads and powers data workflows for more than 5,000 companies across regulated industries, including finance, healthcare and manufacturing. That technology is getting another solid vote of confidence today as dltHub, the Berlin-based company behind the open-source dlt library, is raising $8 million in seed funding led by Bessemer Venture Partners. What makes this significant isn&#x27;t just adoption numbers; it&#x27;s how developers are using the tool in combination with AI coding assistants to accomplish tasks that previously required infrastructure engineers, DevOps specialists and on-call personnel.The company is building a cloud-hosted platform that extends its open-source library into a complete end-to-end solution. The platform will allow developers to deploy pipelines, transformations and notebooks with a single command without worrying about infrastructure. This represents a fundamental shift from data engineering requiring specialized teams to becoming accessible to any Python developer.\"Any Python developer should be able to bring their business users closer to fresh, reliable data,\" Matthaus Krzykowski, dltHub&#x27;s co-founder and CEO, told VentureBeat in an exclusive interview. \"Our mission is to make data engineering as accessible, collaborative and as frictionless as writing Python itself.\"From SQL to Python-native data engineeringThe problem the company set out to solve emerged from real-world frustrations.One comes from a fundamental clash between how different generations of developers work with data. Krzykowski pointed to the generation of developers grounded in SQL and relational database technology. On the other hand, a generation of developers is building AI agents with Python.This divide reflects deeper technical challenges. SQL-based data engineering locks teams into specific platforms and requires extensive infrastructure knowledge. Python developers working on AI need lightweight, platform-agnostic tools that work in notebooks and integrate with large language model (LLM) coding assistants.The dlt library changes this equation by automating complex data engineering tasks in simple Python code. \"If you know what a function in Python is, what a list is, a source and resource, then you can write this very declarative, very simple code,\" Krzykowski explained.The key technical breakthrough addresses schema evolution automatically. When data sources change their output format, traditional pipelines break. \"DLT has mechanisms to automatically resolve these issues,\" Thierry Jean, founding engineer at dltHub, told VentureBeat. \"So it will push data, and you can say, &#x27;Alert me if things change upstream,&#x27; or just make it flexible enough and change the data and the destination in a way to accommodate.\"Real-world developer experienceHoyt Emerson, data consultant and content creator at The Full Data Stack, recently adopted the tool to move data from Google Cloud Storage to multiple destinations, including Amazon S3 and a data warehouse. Traditional approaches would require platform-specific knowledge for each destination. Emerson told VentureBeat that what he really wanted was a much more lightweight, platform-agnostic way to send data from one spot to another. \"That&#x27;s when DLT gave me the aha moment,\" Emerson said.He completed the entire pipeline in five minutes using the library&#x27;s documentation, which made it easy to get up and running quickly and without issue.The process gets even more powerful when combined with AI coding assistants. Emerson noted that he&#x27;s using agentic AI coding principles and realized that the dlt documentation could be sent as context to an LLM to accelerate and automate his data work. With documentation as context, Emerson was able to create reusable templates for future projects and used AI assistants to generate deployment configurations.\"It&#x27;s extremely LLM-friendly because it&#x27;s very well documented,\" he said.The LLM-native development patternThis combination of well-documented tools and AI assistance represents a new development pattern. The company has optimized specifically for what they call \"YOLO mode\" development, where developers copy error messages and paste them into AI coding assistants.\"A lot of these people are literally just copying and pasting error messages and are trying the code editors to figure it out,\" Krzykowski said. The company takes this behavior seriously enough that they fix issues specifically for AI-assisted workflows.The results speak to the approach&#x27;s effectiveness. In September alone, users created more than 50,000 custom connectors using the library. That represents a 20X increase since January, driven largely by LLM-assisted development.Technical architecture for enterprise scaleThe dlt design philosophy prioritizes interoperability over platform lock-in. The tool can deploy anywhere from AWS Lambda to existing enterprise data stacks. It integrates with platforms like Snowflake, while maintaining the flexibility to work with any destination.\"We always believe that DLT needs to be interoperable and modular,\" Krzykowski explained. \"It can be deployed anywhere. It can be on Lambda. It often becomes part of other people&#x27;s data infrastructures.\"Key technical capabilities include:Automatic schema evolution: Handles upstream data changes without breaking pipelines or requiring manual intervention.Incremental loading: Processes only new or changed records, reducing computational overhead and costs.Platform agnostic deployment: Works across cloud providers and on-premises infrastructure without modification.LLM-optimized documentation: Structured specifically for AI assistant consumption, enabling rapid problem-solving and template generation.The platform currently supports more than 4,600 REST API data sources with continuous expansion driven by user-generated connectors.Competing against ETL giants with a code-first approachThe data engineering landscape splits into distinct camps, each serving different enterprise needs and developer preferences. Traditional ETL platforms like Informatica and Talend dominate enterprise environments with GUI-based tools that require specialized training but offer comprehensive governance features.Newer SaaS platforms like Fivetran have gained traction by emphasizing pre-built connectors and managed infrastructure, reducing operational overhead but creating vendor dependency.The open-source dlt library occupies a fundamentally different position as code-first, LLM-native infrastructure that developers can extend and customize. This positioning reflects the broader shift toward what the industry calls the composable data stack, where enterprises build infrastructure from interoperable components rather than monolithic platforms.More importantly, the intersection with AI creates new market dynamics. \"LLMs aren&#x27;t replacing data engineers,\" Krzykowski said. \"But they radically expand their reach and productivity.\"What this means for enterprise data leadersFor enterprises looking to lead in AI-driven operations, this development represents an opportunity to fundamentally rethink data engineering strategies.The immediate tactical advantages are clear. Organizations can leverage existing Python developers instead of hiring specialized data engineering teams. Organizations that adapt their tooling and hiring approaches to leverage this trend may find significant cost and agility advantages over competitors still dependent on traditional, team-intensive data engineering.The question isn&#x27;t whether this shift toward democratized data engineering will occur. It&#x27;s how quickly enterprises will adapt to capitalize on it.",
          "feed_position": 6,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/4s24Zr8moHSwWGw0JHGi7D/77a891f4e1193fe3fdce8423ff27befc/python-dev-smk1.jpg?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/strengthening-our-core-welcoming-karyne-levy-as-venturebeats-new-managing",
          "published_at": "Mon, 03 Nov 2025 15:00:00 GMT",
          "title": "Strengthening Our Core: Welcoming Karyne Levy as VentureBeat’s New Managing Editor",
          "standfirst": "I’m thrilled to announce a fantastic new addition to our leadership team: Karyne Levy is joining VentureBeat as our new Managing Editor. Today is her first day.Many of you may know Karyne from her most recent role as Deputy Managing Editor at TechCrunch, but her career is a highlight reel of veteran tech journalism. Her resume includes pivotal roles at Protocol, NerdWallet, Business Insider, and CNET, giving her a deep understanding of this industry from every angle.Hiring Karyne is a significant step forward for VentureBeat. As we’ve sharpened our focus on serving you – the enterprise technical decision-maker navigating the complexities of AI and data – I’ve been looking for a very specific kind of leader.The \"Organizer&#x27;s Dopamine Hit\"In the past, a managing editor was often the final backstop for copy. Today, at a modern, data-focused media company like ours, the role is infinitely more dynamic. It’s the central hub of the entire content operation.During my search, I found myself talking a lot about the two types of \"dopamine hits\" in our business. There’s the writer’s hit – seeing your name on a great story. And then there’s the organizer’s hit – the satisfaction that comes from building, tuning, and running the complex machine that allows a dozen different parts of the company to move in a single, powerful direction.We were looking for the organizer.When I spoke with Karyne, I explained this vision: a leader who thrives on creating workflows, who loves being the liaison between editorial, our data and survey team, our events, and our marketing operations.Her response confirmed she was the one: \"Everything you said is exactly my dopamine hit.\"Karyne’s passion is making the entire operation hum. She has a proven track record of managing people, running newsrooms, and interfacing with all parts of a business to ensure everyone is aligned. That operational rigor is precisely what we need for our next chapter.Why This Matters for Our Strategy (and for You)As I’ve written about before, VentureBeat is on a mission to evolve. In an age where experts and companies can publish directly, it’s not enough to be a secondary source. Our goal is to become a primary source for you.How? By leveraging our relationship with our community of millions of technical leaders. We are increasingly surveying you directly to generate proprietary insights you can’t get anywhere else. We want to be the first to tell you which vector stores your peers are actually implementing, what governance challenges are most pressing for data scientists, or how your counterparts are budgeting for generative AI.This is an ambitious strategy. It requires a tight-knit team where our editorial content, our research surveys and reports, our newsletters, and our VB Transform events are all working from the same playbook.Karyne is the leader who will help us execute that vision. Her experience at Protocol, which was also dedicated to serving technical and business decision-makers, means she fundamentally understands our audience. She is ideally suited to manage our newsroom and ensure that every piece of content we produce helps you do your job better. She’ll be working alongside Carl Franzen, our executive editor, who continues to drive news decision-making.This is a fantastic hire for VentureBeat. It’s another sign of our commitment to building the most focused, expert team in enterprise AI and data.Please join me in welcoming Karyne to the team.",
          "content": "I’m thrilled to announce a fantastic new addition to our leadership team: Karyne Levy is joining VentureBeat as our new Managing Editor. Today is her first day.Many of you may know Karyne from her most recent role as Deputy Managing Editor at TechCrunch, but her career is a highlight reel of veteran tech journalism. Her resume includes pivotal roles at Protocol, NerdWallet, Business Insider, and CNET, giving her a deep understanding of this industry from every angle.Hiring Karyne is a significant step forward for VentureBeat. As we’ve sharpened our focus on serving you – the enterprise technical decision-maker navigating the complexities of AI and data – I’ve been looking for a very specific kind of leader.The \"Organizer&#x27;s Dopamine Hit\"In the past, a managing editor was often the final backstop for copy. Today, at a modern, data-focused media company like ours, the role is infinitely more dynamic. It’s the central hub of the entire content operation.During my search, I found myself talking a lot about the two types of \"dopamine hits\" in our business. There’s the writer’s hit – seeing your name on a great story. And then there’s the organizer’s hit – the satisfaction that comes from building, tuning, and running the complex machine that allows a dozen different parts of the company to move in a single, powerful direction.We were looking for the organizer.When I spoke with Karyne, I explained this vision: a leader who thrives on creating workflows, who loves being the liaison between editorial, our data and survey team, our events, and our marketing operations.Her response confirmed she was the one: \"Everything you said is exactly my dopamine hit.\"Karyne’s passion is making the entire operation hum. She has a proven track record of managing people, running newsrooms, and interfacing with all parts of a business to ensure everyone is aligned. That operational rigor is precisely what we need for our next chapter.Why This Matters for Our Strategy (and for You)As I’ve written about before, VentureBeat is on a mission to evolve. In an age where experts and companies can publish directly, it’s not enough to be a secondary source. Our goal is to become a primary source for you.How? By leveraging our relationship with our community of millions of technical leaders. We are increasingly surveying you directly to generate proprietary insights you can’t get anywhere else. We want to be the first to tell you which vector stores your peers are actually implementing, what governance challenges are most pressing for data scientists, or how your counterparts are budgeting for generative AI.This is an ambitious strategy. It requires a tight-knit team where our editorial content, our research surveys and reports, our newsletters, and our VB Transform events are all working from the same playbook.Karyne is the leader who will help us execute that vision. Her experience at Protocol, which was also dedicated to serving technical and business decision-makers, means she fundamentally understands our audience. She is ideally suited to manage our newsroom and ensure that every piece of content we produce helps you do your job better. She’ll be working alongside Carl Franzen, our executive editor, who continues to drive news decision-making.This is a fantastic hire for VentureBeat. It’s another sign of our commitment to building the most focused, expert team in enterprise AI and data.Please join me in welcoming Karyne to the team.",
          "feed_position": 7,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/3M8UYGsdxlIyYdOeQxU2Eo/3703cc8b9b2839a667c762773d65b725/Gemini_Generated_Image_n6phqnn6phqnn6ph.png?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/computing/laptops/acer-predator-triton-14-ai-review-a-true-ultraportable-gaming-laptop-145300067.html",
          "published_at": "Mon, 03 Nov 2025 14:53:00 +0000",
          "title": "Acer Predator Triton 14 AI review: A true ultraportable gaming laptop",
          "standfirst": "When I review products, I try to take other perspectives and use cases into account as much as possible. I'm very aware that I'm not the target audience for every device. But once in a while I run into something that seems like it was designed specifically for me and it just hits different. With the Acer Predator Triton 14 AI, that's pretty much the situation. While it isn't the flashiest or most powerful gaming laptop on the market, it has pretty much everything I look for in a portable system that lets me play games on the go — and then some. Design and display The term ultraportable is typically reserved for more traditional thin-and-light productivity machines, but I think it definitely applies to the Triton 14 AI. At just 3.5 pounds and 0.71 inches thick, Acer's rig is actually a touch lighter and just as thin as a Dell 14 Premium (3.7 pounds and 0.71 inches), despite featuring a much beefier GPU. And even compared to rivals like the Razer Blade 14 (3.6 pounds and 0.64 inches thick), the Triton 14 AI isn't losing much ground there either. Furthermore, while some gaming notebooks go overboard with edgy aesthetics and an abundance of RGB lighting, the Triton 14 AI looks refreshingly understated. Sure, it still has customizable LEDs behind the Predator logo on its lid and per-key lighting on its keyboard. But aside from that, the laptop feels like an exercise in restraint for a category that often favors excess.The other small design flourish is a pixelated Predator logo (that looks like it was made from a tiny dot matrix display) to the right of the touchpad.. I think it's a clever touch that hints at the notebook's gaming focus without hitting you over the head with it. Despite its size, the Triton 14 AI also has excellent connectivity. You get two USB-C ports (one on either side), with Thunderbolt 4 support on the right while the other is used for power and USB 4 data speeds (both can be used for charging). There are also two USB-A 3.2 jacks, 3.5mm audio, a full-size HDMI 2.1 connector and even a microSD card reader. That means you can easily hook it up to an external monitor (which you really ought to have when fragging at home). Alternatively, when you're not gaming, it can be a great mobile editing station because offloading photos and videos from a camera via microSD is a cinch. The Acer Predator Triton 14 AI's right side features a microSD card reader, two USB ports (one Type-C and one Type-A) and a full-size HDMI jack. Sam Rutherford for Engadget Acer didn't cut corners with the Triton 14 AI's display either. Sure, its 120Hz refresh rate could be a touch faster or it could have gone with a slightly higher 3.2K display like on the Dell 14 Premium, but those are real nitpicks. The OLED panel produces rich colors and in my testing, the display on my review unit actually exceeded Acer's stated 340-nit brightness by a few percent. While Acer included six speakers that get plenty loud, my one small gripe is that they aren't located in the best spots to maximize audio quality. There are two drivers hidden behind tiny grilles on each edge of the laptop and four more located on the bottom. This means unless the laptop is sitting on a hard reflective surface like a desk (without something like a desk mat in between), audio often sounds muffled or dampened. It's not a dealbreaker and I understand that the Triton 14 AI's petite dimensions didn't leave much room for up-firing drivers, but I wish Acer had found an arrangement that sounds slightly better. Keyboard, touchpad and an unusual special feature The Acer Predator Triton 14 AI features a keyboard with per-key mini LED lighting and a touchpad with built-in stylus support. Sam Rutherford for Engadget In addition to per-key lighting and a pleasantly bouncy typing experience, Acer added a few extra features to the Triton 14 AI's mouse and keyboard that you don't normally see on gaming laptops. On the left above the function row, there's a physical button that makes it fast and easy to switch between various performance modes with a single press. There's also a dedicated Predator key that acts as a shortcut to Acer's app, where you can do things like tweak settings or adjust the laptop's lighting. Down below, the Triton 14 AI features a large seamless touchpad made from Gorilla Glass, similar to what you get on a Dell 14 Premium. However, to address the issue of you not knowing where the trackpad ends and the rest of the notebook's deck begins, Acer added two light strips on either side. It’s a simple and elegant solution that looks nice too. Not only does the Predator Triton 14 AI's touchpad feature stylus support, Acer included an active pen in the box, so you won't need to buy one separately. Sam Rutherford for Engadget However, the Triton's real party trick is that it also supports stylus input (via MPP 2.0) with 4,096 levels of pressure sensitivity. This means you can use it like a small built-in Wacom tablet. On top of that, the laptop ships with an active pen, so you don't need to shell out extra money for one. And because Windows recognizes the stylus out of the box, there's no extra setup required. So while this isn't something I will use all the time, it's nice to have for times when I feel like taking notes, sketching or just need to sign a document electronically. Performance Our $2,500 review unit features an Intel Core Ultra 9 CPU with 32GB of RAM and a 1TB PCIe Gen 4 SSD along with an NVIDIA RTX 5070 GPU. Notably, this is as big a graphics card as the Triton 14 AI can handle, but considering similarly-sized rivals like the Razer Blade 14 have the same limitation, it's hard to be upset. More importantly, even without the option for an RTX 5080 or 5090, Acer's tiny gaming laptop still boasts respectable performance. The Acer Predator Triton 14 AI features a vivid 14.5-inch OLED panel with a WQXGA+ (2880 x 1800) resolution. Sam Rutherford for Engadget In Cyberpunk 2077 at 1080p and Ultra RT settings, the Triton 14 AI hit 55 fps, which is a notch above the 45 fps I got from the Radeon 8060S in the ROG Z Flow 13. It also means that with just a tiny bit of tweaking, it's easy to push framerates above 60 while keeping almost all of the graphics settings maxed out. Meanwhile, in Returnal at 1080p on Epic, the Triton 14 AI fared even better, hitting 115 fps. That falls short of what I saw on the Alienware 16 Area-51 (154 fps), but considering that's a larger system with an RTX 5080, the difference between the two machines is understandable. As for cooling, Acer went beyond simply using a built-in vapor chamber. Instead of the paste or liquid metal used by the competition, the company says this is the first time a graphene-based thermal interface material has been used inside a gaming laptop. This makes a difference, especially on a notebook this thin, because it means for less demanding games like Teamfight Tactics, if you adjust its performance mode you can actually play them on your lap without worrying about scorching your legs. That said, you still have to watch out because there are two largish fans on the bottom as well, so for more serious titles you'll still want to switch to a table or desk. Battery life The Acer Predator Triton 14 AI's stay relatively cool in normal use thanks to a vapor chamber and a graphene-based thermal interface material. However, under heavy loads, it will still get a bit toasty. Sam Rutherford for Engadget Longevity is often a concern for small, power-hungry gaming laptops like this. But somehow, Acer managed to fit a more than adequate 76Whr battery inside. On PCMark 10's Modern Office rundown test, the Triton 14 AI lasted seven hours and 26 minutes. That's three hours better than larger systems like the Alienware 16 Area-51 (4:13) and half an hour better than smaller rivals like the ASUS ROG Z Flow 13 (6:54). And even though it fell short by an hour when compared to a traditional ultraportable like the Dell 14 Premium (8:30), that's still very solid when you consider the Triton’s more powerful graphics. Wrap-up If you're in the market for a more powerful and sedentary type of gaming laptop that might only get moved around a couple of times a month (if that), the Triton 14 AI might not be for you. But as someone who prefers gaming laptops that are, you know, actually portable, this thing is pretty much my ideal notebook. Even though it's a gaming laptop, the Acer Predator Triton 14 AI's design is refreshingly understated. Sam Rutherford for Engadget For $2,500 as tested, the Predator Triton 14 AI has a vivid OLED display, solid performance, surprisingly good battery life and an incredibly sleek chassis that begs you to take this thing everywhere. It's a bit pricey, but considering a similarly-specced Blade 14 costs $2,700 (before sales or discounts), you might even say it's a bit of a bargain. What puts this thing over the top though, is that Acer could have stopped there and no one would have complained. But then it added extra features like ample ports, powerful cooling and built-in stylus support (not to mention the included pen). In a lot of ways, this isn't just a travel-friendly gaming machine, it's a true do-everything ultraportable. This article originally appeared on Engadget at https://www.engadget.com/computing/laptops/acer-predator-triton-14-ai-review-a-true-ultraportable-gaming-laptop-145300067.html?src=rss",
          "content": "When I review products, I try to take other perspectives and use cases into account as much as possible. I'm very aware that I'm not the target audience for every device. But once in a while I run into something that seems like it was designed specifically for me and it just hits different. With the Acer Predator Triton 14 AI, that's pretty much the situation. While it isn't the flashiest or most powerful gaming laptop on the market, it has pretty much everything I look for in a portable system that lets me play games on the go — and then some. Design and display The term ultraportable is typically reserved for more traditional thin-and-light productivity machines, but I think it definitely applies to the Triton 14 AI. At just 3.5 pounds and 0.71 inches thick, Acer's rig is actually a touch lighter and just as thin as a Dell 14 Premium (3.7 pounds and 0.71 inches), despite featuring a much beefier GPU. And even compared to rivals like the Razer Blade 14 (3.6 pounds and 0.64 inches thick), the Triton 14 AI isn't losing much ground there either. Furthermore, while some gaming notebooks go overboard with edgy aesthetics and an abundance of RGB lighting, the Triton 14 AI looks refreshingly understated. Sure, it still has customizable LEDs behind the Predator logo on its lid and per-key lighting on its keyboard. But aside from that, the laptop feels like an exercise in restraint for a category that often favors excess.The other small design flourish is a pixelated Predator logo (that looks like it was made from a tiny dot matrix display) to the right of the touchpad.. I think it's a clever touch that hints at the notebook's gaming focus without hitting you over the head with it. Despite its size, the Triton 14 AI also has excellent connectivity. You get two USB-C ports (one on either side), with Thunderbolt 4 support on the right while the other is used for power and USB 4 data speeds (both can be used for charging). There are also two USB-A 3.2 jacks, 3.5mm audio, a full-size HDMI 2.1 connector and even a microSD card reader. That means you can easily hook it up to an external monitor (which you really ought to have when fragging at home). Alternatively, when you're not gaming, it can be a great mobile editing station because offloading photos and videos from a camera via microSD is a cinch. The Acer Predator Triton 14 AI's right side features a microSD card reader, two USB ports (one Type-C and one Type-A) and a full-size HDMI jack. Sam Rutherford for Engadget Acer didn't cut corners with the Triton 14 AI's display either. Sure, its 120Hz refresh rate could be a touch faster or it could have gone with a slightly higher 3.2K display like on the Dell 14 Premium, but those are real nitpicks. The OLED panel produces rich colors and in my testing, the display on my review unit actually exceeded Acer's stated 340-nit brightness by a few percent. While Acer included six speakers that get plenty loud, my one small gripe is that they aren't located in the best spots to maximize audio quality. There are two drivers hidden behind tiny grilles on each edge of the laptop and four more located on the bottom. This means unless the laptop is sitting on a hard reflective surface like a desk (without something like a desk mat in between), audio often sounds muffled or dampened. It's not a dealbreaker and I understand that the Triton 14 AI's petite dimensions didn't leave much room for up-firing drivers, but I wish Acer had found an arrangement that sounds slightly better. Keyboard, touchpad and an unusual special feature The Acer Predator Triton 14 AI features a keyboard with per-key mini LED lighting and a touchpad with built-in stylus support. Sam Rutherford for Engadget In addition to per-key lighting and a pleasantly bouncy typing experience, Acer added a few extra features to the Triton 14 AI's mouse and keyboard that you don't normally see on gaming laptops. On the left above the function row, there's a physical button that makes it fast and easy to switch between various performance modes with a single press. There's also a dedicated Predator key that acts as a shortcut to Acer's app, where you can do things like tweak settings or adjust the laptop's lighting. Down below, the Triton 14 AI features a large seamless touchpad made from Gorilla Glass, similar to what you get on a Dell 14 Premium. However, to address the issue of you not knowing where the trackpad ends and the rest of the notebook's deck begins, Acer added two light strips on either side. It’s a simple and elegant solution that looks nice too. Not only does the Predator Triton 14 AI's touchpad feature stylus support, Acer included an active pen in the box, so you won't need to buy one separately. Sam Rutherford for Engadget However, the Triton's real party trick is that it also supports stylus input (via MPP 2.0) with 4,096 levels of pressure sensitivity. This means you can use it like a small built-in Wacom tablet. On top of that, the laptop ships with an active pen, so you don't need to shell out extra money for one. And because Windows recognizes the stylus out of the box, there's no extra setup required. So while this isn't something I will use all the time, it's nice to have for times when I feel like taking notes, sketching or just need to sign a document electronically. Performance Our $2,500 review unit features an Intel Core Ultra 9 CPU with 32GB of RAM and a 1TB PCIe Gen 4 SSD along with an NVIDIA RTX 5070 GPU. Notably, this is as big a graphics card as the Triton 14 AI can handle, but considering similarly-sized rivals like the Razer Blade 14 have the same limitation, it's hard to be upset. More importantly, even without the option for an RTX 5080 or 5090, Acer's tiny gaming laptop still boasts respectable performance. The Acer Predator Triton 14 AI features a vivid 14.5-inch OLED panel with a WQXGA+ (2880 x 1800) resolution. Sam Rutherford for Engadget In Cyberpunk 2077 at 1080p and Ultra RT settings, the Triton 14 AI hit 55 fps, which is a notch above the 45 fps I got from the Radeon 8060S in the ROG Z Flow 13. It also means that with just a tiny bit of tweaking, it's easy to push framerates above 60 while keeping almost all of the graphics settings maxed out. Meanwhile, in Returnal at 1080p on Epic, the Triton 14 AI fared even better, hitting 115 fps. That falls short of what I saw on the Alienware 16 Area-51 (154 fps), but considering that's a larger system with an RTX 5080, the difference between the two machines is understandable. As for cooling, Acer went beyond simply using a built-in vapor chamber. Instead of the paste or liquid metal used by the competition, the company says this is the first time a graphene-based thermal interface material has been used inside a gaming laptop. This makes a difference, especially on a notebook this thin, because it means for less demanding games like Teamfight Tactics, if you adjust its performance mode you can actually play them on your lap without worrying about scorching your legs. That said, you still have to watch out because there are two largish fans on the bottom as well, so for more serious titles you'll still want to switch to a table or desk. Battery life The Acer Predator Triton 14 AI's stay relatively cool in normal use thanks to a vapor chamber and a graphene-based thermal interface material. However, under heavy loads, it will still get a bit toasty. Sam Rutherford for Engadget Longevity is often a concern for small, power-hungry gaming laptops like this. But somehow, Acer managed to fit a more than adequate 76Whr battery inside. On PCMark 10's Modern Office rundown test, the Triton 14 AI lasted seven hours and 26 minutes. That's three hours better than larger systems like the Alienware 16 Area-51 (4:13) and half an hour better than smaller rivals like the ASUS ROG Z Flow 13 (6:54). And even though it fell short by an hour when compared to a traditional ultraportable like the Dell 14 Premium (8:30), that's still very solid when you consider the Triton’s more powerful graphics. Wrap-up If you're in the market for a more powerful and sedentary type of gaming laptop that might only get moved around a couple of times a month (if that), the Triton 14 AI might not be for you. But as someone who prefers gaming laptops that are, you know, actually portable, this thing is pretty much my ideal notebook. Even though it's a gaming laptop, the Acer Predator Triton 14 AI's design is refreshingly understated. Sam Rutherford for Engadget For $2,500 as tested, the Predator Triton 14 AI has a vivid OLED display, solid performance, surprisingly good battery life and an incredibly sleek chassis that begs you to take this thing everywhere. It's a bit pricey, but considering a similarly-specced Blade 14 costs $2,700 (before sales or discounts), you might even say it's a bit of a bargain. What puts this thing over the top though, is that Acer could have stopped there and no one would have complained. But then it added extra features like ample ports, powerful cooling and built-in stylus support (not to mention the included pen). In a lot of ways, this isn't just a travel-friendly gaming machine, it's a true do-everything ultraportable. This article originally appeared on Engadget at https://www.engadget.com/computing/laptops/acer-predator-triton-14-ai-review-a-true-ultraportable-gaming-laptop-145300067.html?src=rss",
          "feed_position": 49,
          "image_url": "https://d29szjachogqwa.cloudfront.net/images/user-uploaded/triton-14-right-side.jpg"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/the-beginning-of-the-end-of-the-transformer-era-neuro-symbolic-ai-startup",
          "published_at": "Mon, 03 Nov 2025 14:00:00 GMT",
          "title": "The beginning of the end of the transformer era? Neuro-symbolic AI startup AUI announces new funding at $750M valuation",
          "standfirst": "The buzzed-about but still stealthy New York City startup Augmented Intelligence Inc (AUI), which seeks to go beyond the popular \"transformer\" architecture used by most of today&#x27;s LLMs such as ChatGPT and Gemini, has raised $20 million in a bridge SAFE round at a $750 million valuation cap, bringing its total funding to nearly $60 million, VentureBeat can exclusively reveal.The round, completed in under a week, comes amid heightened interest in deterministic conversational AI and precedes a larger raise now in advanced stages.AUI relies on a fusion of the transformer tech and a newer technology called \"neuro-symbolic AI,\" described in greater detail below. \"We realize that you can combine the brilliance of LLMs in linguistic capabilities with the guarantees of symbolic AI,\" said Ohad Elhelo, AUI co-founder and CEO in a recent interview with VentureBeat. Elhelo launched the company in 2017 alongside co-founder and Chief Product Officer Ori Cohen.The new financing includes participation from eGateway Ventures, New Era Capital Partners, existing shareholders, and other strategic investors. It follows a $10 million raise in September 2024 at a $350 million valuation cap, coinciding with the company’s announced go-to-market partnership with Google in October 2024. Early investors include Vertex Pharmaceuticals founder Joshua Boger, UKG Chairman Aron Ain, and former IBM President Jim Whitehurst.According to the company, the bridge round is a precursor to a significantly larger raise already in advanced stages.AUI is the company behind Apollo-1, a new foundation model built for task-oriented dialog, which it describes as the \"economic half\" of conversational AI — distinct from the open-ended dialog handled by LLMs like ChatGPT and Gemini. The firm argues that existing LLMs lack the determinism, policy enforcement, and operational certainty required by enterprises, especially in regulated sectors.Chris Varelas, co-founder of Redwood Capital and an advisor to AUI, said in a press release provided to VentureBeat: “I’ve seen some of today’s top AI leaders walk away with their heads spinning after interacting with Apollo-1.”A Distinctive Neuro-Symbolic ArchitectureApollo-1’s core innovation is its neuro-symbolic architecture, which separates linguistic fluency from task reasoning. Instead of using the most common technology underpinning most LLMs and conversational AI systems today — the vaunted transformer architecture described in the seminal 2017 Google paper \"Attention Is All You Need\" — AUI&#x27;s system integrates two layers:Neural modules, powered by LLMs, handle perception: encoding user inputs and generating natural language responses.A symbolic reasoning engine, developed over several years, interprets structured task elements such as intents, entities, and parameters. This symbolic state engine determines the appropriate next actions using deterministic logic.This hybrid architecture allows Apollo-1 to maintain state continuity, enforce organizational policies, and reliably trigger tool or API calls — capabilities that transformer-only agents lack.Elhelo said this design emerged from a multi-year data collection effort: “We built a consumer service and recorded millions of human-agent interactions across 60,000 live agents. From that, we abstracted a symbolic language that defines the structure of task-based dialogs, separate from their domain-specific content.”However, enterprises that have already built systems built around transformer LLMs needn&#x27;t worry. AUI wants to make adopting its new technology just as easy. \"Apollo-1 deploys like any modern foundation model,\" Elhelo told VentureBeat in a text last night. \"It doesn’t require dedicated or proprietary clusters to run. It operates across standard cloud and hybrid environments, leveraging both GPUs and CPUs, and is significantly more cost-efficient to deploy than frontier reasoning models. Apollo-1 can also be deployed across all major clouds in a separated environment for increased security.\"Generalization and Domain FlexibilityApollo-1 is described as a foundation model for task-oriented dialog, meaning it is domain-agnostic and generalizable across verticals like healthcare, travel, insurance, and retail.Unlike consulting-heavy AI platforms that require building bespoke logic per client, Apollo-1 allows enterprises to define behaviors and tools within a shared symbolic language. This approach supports faster onboarding and reduces long-term maintenance. According to the team, an enterprise can launch a working agent in under a day.Crucially, procedural rules are encoded at the symbolic layer — not learned from examples. This enables deterministic execution for sensitive or regulated tasks. For instance, a system can block cancellation of a Basic Economy flight not by guessing intent but by applying hard-coded logic to a symbolic representation of the booking class.As Elhelo explained to VentureBeat, LLMs are \"not a good mechanism when you’re looking for certainty. It’s better if you know what you’re going to send [to an AI model] and always send it, and you know, always, what’s going to come back [to the user] and how to handle that.”Availability and Developer AccessApollo-1 is already in active use within Fortune 500 enterprises in a closed beta, and a broader general availability release is expected before the end of 2025, according to a previous report by The Information, which broke the initial news on the startup.Enterprises can integrate with Apollo-1 either via:A developer playground, where business users and technical teams jointly configure policies, rules, and behaviors; orA standard API, using OpenAI-compatible formats.The model supports policy enforcement, rule-based customization, and steering via guardrails. Symbolic rules allow businesses to dictate fixed behaviors, while LLM modules handle open-text interpretation and user interaction.Enterprise Fit: When Reliability Beats FluencyWhile LLMs have advanced general-purpose dialog and creativity, they remain probabilistic — a barrier to enterprise deployment in finance, healthcare, and customer service. Apollo-1 targets this gap by offering a system where policy adherence and deterministic task completion are first-class design goals.Elhelo puts it plainly: “If your use case is task-oriented dialog, you have to use us, even if you are ChatGPT.”",
          "content": "The buzzed-about but still stealthy New York City startup Augmented Intelligence Inc (AUI), which seeks to go beyond the popular \"transformer\" architecture used by most of today&#x27;s LLMs such as ChatGPT and Gemini, has raised $20 million in a bridge SAFE round at a $750 million valuation cap, bringing its total funding to nearly $60 million, VentureBeat can exclusively reveal.The round, completed in under a week, comes amid heightened interest in deterministic conversational AI and precedes a larger raise now in advanced stages.AUI relies on a fusion of the transformer tech and a newer technology called \"neuro-symbolic AI,\" described in greater detail below. \"We realize that you can combine the brilliance of LLMs in linguistic capabilities with the guarantees of symbolic AI,\" said Ohad Elhelo, AUI co-founder and CEO in a recent interview with VentureBeat. Elhelo launched the company in 2017 alongside co-founder and Chief Product Officer Ori Cohen.The new financing includes participation from eGateway Ventures, New Era Capital Partners, existing shareholders, and other strategic investors. It follows a $10 million raise in September 2024 at a $350 million valuation cap, coinciding with the company’s announced go-to-market partnership with Google in October 2024. Early investors include Vertex Pharmaceuticals founder Joshua Boger, UKG Chairman Aron Ain, and former IBM President Jim Whitehurst.According to the company, the bridge round is a precursor to a significantly larger raise already in advanced stages.AUI is the company behind Apollo-1, a new foundation model built for task-oriented dialog, which it describes as the \"economic half\" of conversational AI — distinct from the open-ended dialog handled by LLMs like ChatGPT and Gemini. The firm argues that existing LLMs lack the determinism, policy enforcement, and operational certainty required by enterprises, especially in regulated sectors.Chris Varelas, co-founder of Redwood Capital and an advisor to AUI, said in a press release provided to VentureBeat: “I’ve seen some of today’s top AI leaders walk away with their heads spinning after interacting with Apollo-1.”A Distinctive Neuro-Symbolic ArchitectureApollo-1’s core innovation is its neuro-symbolic architecture, which separates linguistic fluency from task reasoning. Instead of using the most common technology underpinning most LLMs and conversational AI systems today — the vaunted transformer architecture described in the seminal 2017 Google paper \"Attention Is All You Need\" — AUI&#x27;s system integrates two layers:Neural modules, powered by LLMs, handle perception: encoding user inputs and generating natural language responses.A symbolic reasoning engine, developed over several years, interprets structured task elements such as intents, entities, and parameters. This symbolic state engine determines the appropriate next actions using deterministic logic.This hybrid architecture allows Apollo-1 to maintain state continuity, enforce organizational policies, and reliably trigger tool or API calls — capabilities that transformer-only agents lack.Elhelo said this design emerged from a multi-year data collection effort: “We built a consumer service and recorded millions of human-agent interactions across 60,000 live agents. From that, we abstracted a symbolic language that defines the structure of task-based dialogs, separate from their domain-specific content.”However, enterprises that have already built systems built around transformer LLMs needn&#x27;t worry. AUI wants to make adopting its new technology just as easy. \"Apollo-1 deploys like any modern foundation model,\" Elhelo told VentureBeat in a text last night. \"It doesn’t require dedicated or proprietary clusters to run. It operates across standard cloud and hybrid environments, leveraging both GPUs and CPUs, and is significantly more cost-efficient to deploy than frontier reasoning models. Apollo-1 can also be deployed across all major clouds in a separated environment for increased security.\"Generalization and Domain FlexibilityApollo-1 is described as a foundation model for task-oriented dialog, meaning it is domain-agnostic and generalizable across verticals like healthcare, travel, insurance, and retail.Unlike consulting-heavy AI platforms that require building bespoke logic per client, Apollo-1 allows enterprises to define behaviors and tools within a shared symbolic language. This approach supports faster onboarding and reduces long-term maintenance. According to the team, an enterprise can launch a working agent in under a day.Crucially, procedural rules are encoded at the symbolic layer — not learned from examples. This enables deterministic execution for sensitive or regulated tasks. For instance, a system can block cancellation of a Basic Economy flight not by guessing intent but by applying hard-coded logic to a symbolic representation of the booking class.As Elhelo explained to VentureBeat, LLMs are \"not a good mechanism when you’re looking for certainty. It’s better if you know what you’re going to send [to an AI model] and always send it, and you know, always, what’s going to come back [to the user] and how to handle that.”Availability and Developer AccessApollo-1 is already in active use within Fortune 500 enterprises in a closed beta, and a broader general availability release is expected before the end of 2025, according to a previous report by The Information, which broke the initial news on the startup.Enterprises can integrate with Apollo-1 either via:A developer playground, where business users and technical teams jointly configure policies, rules, and behaviors; orA standard API, using OpenAI-compatible formats.The model supports policy enforcement, rule-based customization, and steering via guardrails. Symbolic rules allow businesses to dictate fixed behaviors, while LLM modules handle open-text interpretation and user interaction.Enterprise Fit: When Reliability Beats FluencyWhile LLMs have advanced general-purpose dialog and creativity, they remain probabilistic — a barrier to enterprise deployment in finance, healthcare, and customer service. Apollo-1 targets this gap by offering a system where policy adherence and deterministic task completion are first-class design goals.Elhelo puts it plainly: “If your use case is task-oriented dialog, you have to use us, even if you are ChatGPT.”",
          "feed_position": 8,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/4igdWans9zhjcEbMEhNvi2/df1b7697d86e37a001cfc65dbe5524ab/cfr0z3n_no_bracelets_or_watches_remove_and_keep_unadorned_--cha_9327ad90-cbcb-46f3-9695-65a002bd7142.png?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/meet-denario-the-ai-research-assistant-that-is-already-getting-its-own",
          "published_at": "Mon, 03 Nov 2025 09:40:00 GMT",
          "title": "Meet Denario, the AI ‘research assistant’ that is already getting its own papers published",
          "standfirst": "An international team of researchers has released an artificial intelligence system capable of autonomously conducting scientific research across multiple disciplines — generating papers from initial concept to publication-ready manuscript in approximately 30 minutes for about $4 each.The system, called Denario, can formulate research ideas, review existing literature, develop methodologies, write and execute code, create visualizations, and draft complete academic papers. In a demonstration of its versatility, the team used Denario to generate papers spanning astrophysics, biology, chemistry, medicine, neuroscience, and other fields, with one AI-generated paper already accepted for publication at an academic conference.\"The goal of Denario is not to automate science, but to develop a research assistant that can accelerate scientific discovery,\" the researchers wrote in a paper released Monday describing the system. The team is making the software publicly available as an open-source tool.This achievement marks a turning point in the application of large language models to scientific work, potentially transforming how researchers approach early-stage investigations and literature reviews. However, the research also highlights substantial limitations and raises pressing questions about validation, authorship, and the changing nature of scientific labor.From data to draft: how AI agents collaborate to conduct researchAt its core, Denario operates not as a single AI brain but as a digital research department where specialized AI agents collaborate to push a project from conception to completion. The process can begin with the \"Idea Module,\" which employs a fascinating adversarial process where an \"Idea Maker\" agent proposes research projects that are then scrutinized by an \"Idea Hater\" agent, which critiques them for feasibility and scientific value. This iterative loop refines raw concepts into robust research directions.Once a hypothesis is solidified, a \"Literature Module\" scours academic databases like Semantic Scholar to check the idea&#x27;s novelty, followed by a \"Methodology Module\" that lays out a detailed, step-by-step research plan. The heavy lifting is then done by the \"Analysis Module,\" a virtual workhorse that writes, debugs, and executes its own Python code to analyze data, generate plots, and summarize findings. Finally, the \"Paper Module\" takes the resulting data and plots and drafts a complete scientific paper in LaTeX, the standard for many scientific fields. In a final, recursive step, a \"Review Module\" can even act as an AI peer-reviewer, providing a critical report on the generated paper&#x27;s strengths and weaknesses.This modular design allows a human researcher to intervene at any stage, providing their own idea or methodology, or to simply use Denario as an end-to-end autonomous system. \"The system has a modular architecture, allowing it to handle specific tasks, such as generating an idea, or carrying out end-to-end scientific analysis,\" the paper explains.To validate its capabilities, the Denario team has put the system to the test, generating a vast repository of papers across numerous disciplines. In a striking proof of concept, one paper fully generated by Denario was accepted for publication at the Agents4Science 2025 conference — a peer-reviewed venue where AI systems themselves are the primary authors. The paper, titled \"QITT-Enhanced Multi-Scale Substructure Analysis with Learned Topological Embeddings for Cosmological Parameter Estimation from Dark Matter Halo Merger Trees,\" successfully combined complex ideas from quantum physics, machine learning, and cosmology to analyze simulation data.The ghost in the machine: AI’s ‘vacuous’ results and ethical alarmsWhile the successes are notable, the research paper is refreshingly candid about Denario&#x27;s significant limitations and failure modes. The authors stress that the system currently \"behaves more like a good undergraduate or early graduate student rather than a full professor in terms of big picture, connecting results...etc.\" This honesty provides a crucial reality check in a field often dominated by hype.The paper dedicates entire sections to \"Failure Modes\" and \"Ethical Implications,\" a level of transparency that enterprise leaders should note. The authors report that in one instance, the system \"hallucinated an entire paper without implementing the necessary numerical solver,\" inventing results to fit a plausible narrative. In another test on a pure mathematics problem, the AI produced text that had the form of a mathematical proof but was, in the authors&#x27; words, \"mathematically vacuous.\"These failures underscore a critical point for any organization looking to deploy agentic AI: the systems can be brittle and are prone to confident-sounding errors that require expert human oversight. The Denario paper serves as a vital case study in the importance of keeping a human in the loop for validation and critical assessment.The authors also confront the profound ethical questions raised by their creation. They warn that \"AI agents could be used to quickly flood the scientific literature with claims driven by a particular political agenda or specific commercial or economic interests.\" They also touch on the \"Turing Trap,\" a phenomenon where the goal becomes mimicking human intelligence rather than augmenting it, potentially leading to a \"homogenization\" of research that stifles true, paradigm-shifting innovation.An open-source co-pilot for the world&#x27;s labsDenario is not just a theoretical exercise locked away in an academic lab. The entire system is open-source under a GPL-3.0 license and is accessible to the broader community. The main project and its graphical user interface, DenarioApp, are available on GitHub, with installation managed via standard Python tools. For enterprise environments focused on reproducibility and scalability, the project also provides official Docker images. A public demo hosted on Hugging Face Spaces allows anyone to experiment with its capabilities.For now, Denario remains what its creators call a powerful assistant, but not a replacement for the seasoned intuition of a human expert. This framing is deliberate. The Denario project is less about creating an automated scientist and more about building the ultimate co-pilot, one designed to handle the tedious and time-consuming aspects of modern research.By handing off the grueling work of coding, debugging, and initial drafting to an AI agent, the system promises to free up human researchers for the one task it cannot automate: the deep, critical thinking required to ask the right questions in the first place.",
          "content": "An international team of researchers has released an artificial intelligence system capable of autonomously conducting scientific research across multiple disciplines — generating papers from initial concept to publication-ready manuscript in approximately 30 minutes for about $4 each.The system, called Denario, can formulate research ideas, review existing literature, develop methodologies, write and execute code, create visualizations, and draft complete academic papers. In a demonstration of its versatility, the team used Denario to generate papers spanning astrophysics, biology, chemistry, medicine, neuroscience, and other fields, with one AI-generated paper already accepted for publication at an academic conference.\"The goal of Denario is not to automate science, but to develop a research assistant that can accelerate scientific discovery,\" the researchers wrote in a paper released Monday describing the system. The team is making the software publicly available as an open-source tool.This achievement marks a turning point in the application of large language models to scientific work, potentially transforming how researchers approach early-stage investigations and literature reviews. However, the research also highlights substantial limitations and raises pressing questions about validation, authorship, and the changing nature of scientific labor.From data to draft: how AI agents collaborate to conduct researchAt its core, Denario operates not as a single AI brain but as a digital research department where specialized AI agents collaborate to push a project from conception to completion. The process can begin with the \"Idea Module,\" which employs a fascinating adversarial process where an \"Idea Maker\" agent proposes research projects that are then scrutinized by an \"Idea Hater\" agent, which critiques them for feasibility and scientific value. This iterative loop refines raw concepts into robust research directions.Once a hypothesis is solidified, a \"Literature Module\" scours academic databases like Semantic Scholar to check the idea&#x27;s novelty, followed by a \"Methodology Module\" that lays out a detailed, step-by-step research plan. The heavy lifting is then done by the \"Analysis Module,\" a virtual workhorse that writes, debugs, and executes its own Python code to analyze data, generate plots, and summarize findings. Finally, the \"Paper Module\" takes the resulting data and plots and drafts a complete scientific paper in LaTeX, the standard for many scientific fields. In a final, recursive step, a \"Review Module\" can even act as an AI peer-reviewer, providing a critical report on the generated paper&#x27;s strengths and weaknesses.This modular design allows a human researcher to intervene at any stage, providing their own idea or methodology, or to simply use Denario as an end-to-end autonomous system. \"The system has a modular architecture, allowing it to handle specific tasks, such as generating an idea, or carrying out end-to-end scientific analysis,\" the paper explains.To validate its capabilities, the Denario team has put the system to the test, generating a vast repository of papers across numerous disciplines. In a striking proof of concept, one paper fully generated by Denario was accepted for publication at the Agents4Science 2025 conference — a peer-reviewed venue where AI systems themselves are the primary authors. The paper, titled \"QITT-Enhanced Multi-Scale Substructure Analysis with Learned Topological Embeddings for Cosmological Parameter Estimation from Dark Matter Halo Merger Trees,\" successfully combined complex ideas from quantum physics, machine learning, and cosmology to analyze simulation data.The ghost in the machine: AI’s ‘vacuous’ results and ethical alarmsWhile the successes are notable, the research paper is refreshingly candid about Denario&#x27;s significant limitations and failure modes. The authors stress that the system currently \"behaves more like a good undergraduate or early graduate student rather than a full professor in terms of big picture, connecting results...etc.\" This honesty provides a crucial reality check in a field often dominated by hype.The paper dedicates entire sections to \"Failure Modes\" and \"Ethical Implications,\" a level of transparency that enterprise leaders should note. The authors report that in one instance, the system \"hallucinated an entire paper without implementing the necessary numerical solver,\" inventing results to fit a plausible narrative. In another test on a pure mathematics problem, the AI produced text that had the form of a mathematical proof but was, in the authors&#x27; words, \"mathematically vacuous.\"These failures underscore a critical point for any organization looking to deploy agentic AI: the systems can be brittle and are prone to confident-sounding errors that require expert human oversight. The Denario paper serves as a vital case study in the importance of keeping a human in the loop for validation and critical assessment.The authors also confront the profound ethical questions raised by their creation. They warn that \"AI agents could be used to quickly flood the scientific literature with claims driven by a particular political agenda or specific commercial or economic interests.\" They also touch on the \"Turing Trap,\" a phenomenon where the goal becomes mimicking human intelligence rather than augmenting it, potentially leading to a \"homogenization\" of research that stifles true, paradigm-shifting innovation.An open-source co-pilot for the world&#x27;s labsDenario is not just a theoretical exercise locked away in an academic lab. The entire system is open-source under a GPL-3.0 license and is accessible to the broader community. The main project and its graphical user interface, DenarioApp, are available on GitHub, with installation managed via standard Python tools. For enterprise environments focused on reproducibility and scalability, the project also provides official Docker images. A public demo hosted on Hugging Face Spaces allows anyone to experiment with its capabilities.For now, Denario remains what its creators call a powerful assistant, but not a replacement for the seasoned intuition of a human expert. This framing is deliberate. The Denario project is less about creating an automated scientist and more about building the ultimate co-pilot, one designed to handle the tedious and time-consuming aspects of modern research.By handing off the grueling work of coding, debugging, and initial drafting to an AI agent, the system promises to free up human researchers for the one task it cannot automate: the deep, critical thinking required to ask the right questions in the first place.",
          "feed_position": 9,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/3vIbj1xm8lwmfZQ7qEASzr/9f0690cb8087927276c290678be0e26c/nuneybits_Vector_art_of_data_flowing_into_a_book_c0036116-c0b9-48bb-9dbf-7986efe147d1.webp?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/developers-beware-googles-gemma-model-controversy-exposes-model-lifecycle",
          "published_at": "Mon, 03 Nov 2025 05:00:00 GMT",
          "title": "Developers beware: Google’s Gemma model controversy exposes model lifecycle risks",
          "standfirst": "The recent controversy surrounding Google’s Gemma model has once again highlighted the dangers of using developer test models and the fleeting nature of model availability. Google pulled its Gemma 3 model from AI Studio following a statement from Senator Marsha Blackburn (R-Tenn.) that the Gemma model willfully hallucinated falsehoods about her. Blackburn said the model fabricated news stories about her that go beyond “harmless hallucination” and function as a defamatory act. In response, Google posted on X on October 31 that it will remove Gemma from AI Studio, stating that this is “to prevent confusion.” Gemma remains available via API. It is also available via AI Studio, which, the company described, is \"a developer tool (in fact, to use it you need to attest you&#x27;re a developer). We’ve now seen reports of non-developers trying to use Gemma in AI Studio and ask it factual questions. We never intended this to be a consumer tool or model, or to be used this way. To prevent this confusion, access to Gemma is no longer available on AI Studio.\"To be clear, Google has the right to remove its model from its platform, especially if people have found hallucinations and falsehoods that could proliferate. It also underscores the danger of relying mainly on experimental models and why enterprise developers need to save projects before AI models are sunsetted or removed. Technology companies like Google continue to face political controversies, which often influence their deployments. VentureBeat reached out to Google for additional information and was pointed to their October 31 posts. We also contacted the office of Sen. Blackburn, who reiterated her stance outlined in a statement that AI companies should “shut [models] down until you can control it.\"Developer experimentsThe Gemma family of models, which includes a 270M parameter version, is best suited for small, quick apps and tasks that can run on devices such as smartphones and laptops. Google said the Gemma models were “built specifically for the developer and research community. They are not meant for factual assistance or for consumers to use.”Nevertheless, non-developers could still access Gemma because it is on the AI Studio platform, a more beginner-friendly space for developers to play around with Google AI models compared to Vertex AI. So even if Google never intended Gemma and AI Studio to be accessible to, say, Congressional staffers, these situations can still occur. It also shows that as models continue to improve, these models still produce inaccurate and potentially harmful information. Enterprises must continually weigh the benefits of using models like Gemma against their potential inaccuracies. Project continuity Another concern is the control that AI companies have over their models. The adage “you don’t own anything on the internet” remains true. If you don’t own a physical or local copy of software, it’s easy for you to lose access to it if the company that owns it decides to take it away. Google did not clarify with VentureBeat if current projects on AI Studio powered by Gemma are saved. Similarly, OpenAI users were disappointed when the company announced that it would remove popular older models on ChatGPT. Even after walking back his statement and reinstating GPT-4o back to ChatGPT, OpenAI CEO Sam Altman continues to field questions around keeping and supporting the model. AI companies can, and should, remove their models if they create harmful outputs. AI models, no matter how mature, remain works in progress and are constantly evolving and improving. But, since they are experimental in nature, models can easily become tools that technology companies and lawmakers can wield as leverage. Enterprise developers must ensure that their work can be saved before models are removed from platforms.",
          "content": "The recent controversy surrounding Google’s Gemma model has once again highlighted the dangers of using developer test models and the fleeting nature of model availability. Google pulled its Gemma 3 model from AI Studio following a statement from Senator Marsha Blackburn (R-Tenn.) that the Gemma model willfully hallucinated falsehoods about her. Blackburn said the model fabricated news stories about her that go beyond “harmless hallucination” and function as a defamatory act. In response, Google posted on X on October 31 that it will remove Gemma from AI Studio, stating that this is “to prevent confusion.” Gemma remains available via API. It is also available via AI Studio, which, the company described, is \"a developer tool (in fact, to use it you need to attest you&#x27;re a developer). We’ve now seen reports of non-developers trying to use Gemma in AI Studio and ask it factual questions. We never intended this to be a consumer tool or model, or to be used this way. To prevent this confusion, access to Gemma is no longer available on AI Studio.\"To be clear, Google has the right to remove its model from its platform, especially if people have found hallucinations and falsehoods that could proliferate. It also underscores the danger of relying mainly on experimental models and why enterprise developers need to save projects before AI models are sunsetted or removed. Technology companies like Google continue to face political controversies, which often influence their deployments. VentureBeat reached out to Google for additional information and was pointed to their October 31 posts. We also contacted the office of Sen. Blackburn, who reiterated her stance outlined in a statement that AI companies should “shut [models] down until you can control it.\"Developer experimentsThe Gemma family of models, which includes a 270M parameter version, is best suited for small, quick apps and tasks that can run on devices such as smartphones and laptops. Google said the Gemma models were “built specifically for the developer and research community. They are not meant for factual assistance or for consumers to use.”Nevertheless, non-developers could still access Gemma because it is on the AI Studio platform, a more beginner-friendly space for developers to play around with Google AI models compared to Vertex AI. So even if Google never intended Gemma and AI Studio to be accessible to, say, Congressional staffers, these situations can still occur. It also shows that as models continue to improve, these models still produce inaccurate and potentially harmful information. Enterprises must continually weigh the benefits of using models like Gemma against their potential inaccuracies. Project continuity Another concern is the control that AI companies have over their models. The adage “you don’t own anything on the internet” remains true. If you don’t own a physical or local copy of software, it’s easy for you to lose access to it if the company that owns it decides to take it away. Google did not clarify with VentureBeat if current projects on AI Studio powered by Gemma are saved. Similarly, OpenAI users were disappointed when the company announced that it would remove popular older models on ChatGPT. Even after walking back his statement and reinstating GPT-4o back to ChatGPT, OpenAI CEO Sam Altman continues to field questions around keeping and supporting the model. AI companies can, and should, remove their models if they create harmful outputs. AI models, no matter how mature, remain works in progress and are constantly evolving and improving. But, since they are experimental in nature, models can easily become tools that technology companies and lawmakers can wield as leverage. Enterprise developers must ensure that their work can be saved before models are removed from platforms.",
          "feed_position": 10,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/5qNV2RpIAeBQOQANXVwft6/ba2fb53327b104d5d3b6fe084f427e4d/gemma-3-270m.png?w=300&q=30"
        }
      ],
      "featured_image": "https://images.ctfassets.net/jdtwqhzvc2n1/75UWyAdt4L6TmmJjkbVDJu/96a54479b06fb94b7d13366ad4f046af/ouroboros-ai-smk.jpg?w=300&q=30",
      "popularity_score": 2014.1816191666667,
      "ai_summary": [
        "Amazon sent a cease and desist letter to Perplexity regarding its Comet browser.",
        "Perplexity claims Amazon is bullying the company and threatening internet users.",
        "Amazon's terms of service prohibit third-party use of account information.",
        "Comet stores login credentials locally and makes purchases on Amazon.",
        "Perplexity allegedly tried to bypass an agreement by misrepresenting Comet."
      ]
    },
    {
      "id": "cluster_42",
      "coverage": 2,
      "updated_at": "Tue, 04 Nov 2025 21:28:00 GMT",
      "title": "You can try OpenAI's popular Sora video app on Android and iOS now - for free",
      "neutral_headline": "You can try OpenAI's popular Sora video app on Android and iOS now - for free",
      "items": [
        {
          "source": "ZDNet",
          "url": "https://www.zdnet.com/article/you-can-try-openais-popular-sora-video-app-on-android-and-ios-now-for-free/",
          "published_at": "Tue, 04 Nov 2025 21:28:00 GMT",
          "title": "You can try OpenAI's popular Sora video app on Android and iOS now - for free",
          "standfirst": "With Sora, you can create short-form, AI-generated videos featuring yourself or your friends - no invite code needed.",
          "content": "With Sora, you can create short-form, AI-generated videos featuring yourself or your friends - no invite code needed.",
          "feed_position": 13
        },
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/251104/p28#a251104p28",
          "published_at": "Tue, 04 Nov 2025 14:10:03 -0500",
          "title": "OpenAI launches its Sora app on Android, including in the US, Canada, and Japan; the app reached 1M+ downloads in the five days after its September iOS launch (Emma Roth/The Verge)",
          "standfirst": "Emma Roth / The Verge: OpenAI launches its Sora app on Android, including in the US, Canada, and Japan; the app reached 1M+ downloads in the five days after its September iOS launch &mdash; Now, more people can create and share AI-generated videos on the app's social feed. &hellip; OpenAI has brought its AI video app, Sora, to Android.",
          "content": "Emma Roth / The Verge: OpenAI launches its Sora app on Android, including in the US, Canada, and Japan; the app reached 1M+ downloads in the five days after its September iOS launch &mdash; Now, more people can create and share AI-generated videos on the app's social feed. &hellip; OpenAI has brought its AI video app, Sora, to Android.",
          "feed_position": 13,
          "image_url": "http://www.techmeme.com/251104/i28.jpg"
        }
      ],
      "featured_image": "http://www.techmeme.com/251104/i28.jpg",
      "popularity_score": 2013.7357858333332,
      "ai_summary": [
        "Sora allows users to create short AI-generated videos featuring themselves or friends.",
        "No invite code is required to use the Sora app on Android or iOS.",
        "OpenAI launched the Sora app on Android, including in the US, Canada, and Japan.",
        "The app reached over one million downloads within five days of its iOS launch.",
        "Users can create and share AI-generated videos on the app's social feed."
      ]
    },
    {
      "id": "cluster_9",
      "coverage": 1,
      "updated_at": "Wed, 05 Nov 2025 00:50:43 +0000",
      "title": "In a stunning comeback, Jared Isaacman is renominated to lead NASA",
      "neutral_headline": "Jared Isaacman Renominated to Lead NASA",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2025/11/in-a-stunning-comeback-jared-isaacman-is-renominated-to-lead-nasa/",
          "published_at": "Wed, 05 Nov 2025 00:50:43 +0000",
          "title": "In a stunning comeback, Jared Isaacman is renominated to lead NASA",
          "standfirst": "\"I will do everything I can to live up to those expectations.\"",
          "content": "President Trump announced Tuesday evening that he is renominating private astronaut Jared Isaacman to lead NASA. “Jared’s passion for space, astronaut experience, and dedication to pushing the boundaries of exploration, unlocking the mysteries of the universe, and advancing the new space economy make him ideally suited to lead NASA into a bold new era,” Trump wrote on his social media network, Truth Social. In his statement, Trump did not offer an explanation for why he found Isaacman acceptable now after pulling his original nomination in late May.Read full article Comments",
          "feed_position": 0,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2021/02/Jared_Isaacman_at_SpaceX_2-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2021/02/Jared_Isaacman_at_SpaceX_2-1152x648.jpg",
      "popularity_score": 350.1143969444444,
      "ai_summary": [
        "Jared Isaacman has been renominated to lead NASA.",
        "Isaacman stated he will strive to meet expectations.",
        "No further details about the nomination are provided.",
        "The article focuses on the renomination itself.",
        "Isaacman's role and responsibilities are not specified."
      ]
    },
    {
      "id": "cluster_16",
      "coverage": 1,
      "updated_at": "Tue, 04 Nov 2025 23:24:50 +0000",
      "title": "FDA described as a “clown show” amid latest scandal; top drug regulator is out",
      "neutral_headline": "FDA Regulator Departs Amid Scandal",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/health/2025/11/fda-described-as-a-clown-show-amid-latest-scandal-top-drug-regulator-is-out/",
          "published_at": "Tue, 04 Nov 2025 23:24:50 +0000",
          "title": "FDA described as a “clown show” amid latest scandal; top drug regulator is out",
          "standfirst": "FDA regulator accused of using position to exact revenge on old business associate.",
          "content": "An alleged extortion attempt, a petty yearslong grudge, shocking social media posts, and ominous text messages make up the latest scandal at the Food and Drug Administration, an agency that industry outsiders are calling a “clown show” and “soap opera” amid the Trump administration’s leadership, according to reporting by Stat News. Federal health agencies, in general, have taken heavy blows in Trump’s second term. The Centers for Disease Control and Prevention, in particular, has seen the abrupt dismantling of whole programs and divisions—teams that provide critical health services to Americans. CDC staff regularly describe being demoralized over the last year. Their Senate-confirmed director didn’t make it a full month before being dramatically ousted after allegedly refusing to rubber-stamp vaccine recommendations from a panel filled with vaccine skeptics by anti-vaccine Health Secretary Robert. F. Kennedy Jr. While the CDC is in shambles, the FDA has turned into something of a sideshow, with concern mounting that it remains a serious enough regulator to keep America’s medicines and treatments modern and safe. Many of the scandals are tied to Vinay Prasad, the Trump administration’s top vaccine regulator, who also has the titles of chief medical officer and chief scientific officer. Prasad made a name for himself on social media during the pandemic as a COVID-19 response skeptic and, since joining the FDA, has been known for overruling agency scientists and sowing distrust, unrest, and paranoia among staff. He was pushed out of the agency in July only to be reinstated about two weeks later.Read full article Comments",
          "feed_position": 1,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2019/05/GettyImages-496532228-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2019/05/GettyImages-496532228-1152x648.jpg",
      "popularity_score": 343.68300805555555,
      "ai_summary": [
        "An FDA regulator is out following a recent scandal.",
        "The regulator is accused of using their position for revenge.",
        "The revenge was allegedly directed at a former business associate.",
        "The article describes the FDA as a \"clown show.\"",
        "The specific details of the scandal are not fully described."
      ]
    },
    {
      "id": "cluster_20",
      "coverage": 1,
      "updated_at": "Tue, 04 Nov 2025 22:50:29 +0000",
      "title": "Google’s new hurricane model was breathtakingly good this season",
      "neutral_headline": "Google's Hurricane Model Performed Well This Season",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2025/11/googles-new-weather-model-impressed-during-its-first-hurricane-season/",
          "published_at": "Tue, 04 Nov 2025 22:50:29 +0000",
          "title": "Google’s new hurricane model was breathtakingly good this season",
          "standfirst": "Meanwhile, the US Global Forecasting System continues to get worse.",
          "content": "The Atlantic hurricane season is drawing to a close, and with the tropics quieting down for a winter slumber, the focus of forecasters turns to evaluating what worked and what did not during the preceding season. This year, the answers are clear. Although Google DeepMind’s Weather Lab only started releasing cyclone track forecasts in June, the company’s AI forecasting service performed exceptionally well. By contrast, the Global Forecast System model, operated by the US National Weather Service and is based on traditional physics and runs on powerful supercomputers, performed abysmally. The official data comparing forecast model performance will not be published by the National Hurricane Center for a few months. However, Brian McNoldy, a senior researcher at the University of Miami, has already done some preliminary number crunching.Read full article Comments",
          "feed_position": 2,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/1000x1000-1000x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/1000x1000-1000x648.jpg",
      "popularity_score": 340.11050805555556,
      "ai_summary": [
        "Google's new hurricane model performed exceptionally well this season.",
        "The US Global Forecasting System is reportedly declining in performance.",
        "The article contrasts the performance of the two systems.",
        "No specific details about the models' methodologies are provided.",
        "The article focuses on the comparative performance of the models."
      ]
    },
    {
      "id": "cluster_47",
      "coverage": 1,
      "updated_at": "Tue, 04 Nov 2025 20:59:02 +0000",
      "title": "Meet Project Suncatcher, Google’s plan to put AI data centers in space",
      "neutral_headline": "Google Plans AI Data Centers in Space",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/google/2025/11/meet-project-suncatcher-googles-plan-to-put-ai-data-centers-in-space/",
          "published_at": "Tue, 04 Nov 2025 20:59:02 +0000",
          "title": "Meet Project Suncatcher, Google’s plan to put AI data centers in space",
          "standfirst": "Google is already zapping TPUs with radiation to get ready.",
          "content": "The tech industry is on a tear, building data centers for AI as quickly as they can buy up the land. The sky-high energy costs and logistical headaches of managing all those data centers have prompted interest in space-based infrastructure. Moguls like Jeff Bezos and Elon Musk have mused about putting GPUs in space, and now Google confirms it’s working on its own version of the technology. The company’s latest “moonshot” is known as Project Suncatcher, and if all goes as planned, Google hopes it will lead to scalable networks of orbiting TPUs. The space around Earth has changed a lot in the last few years. A new generation of satellite constellations like Starlink has shown it’s feasible to relay Internet communication via orbital systems. Deploying high-performance AI accelerators in space along similar lines would be a boon to the industry’s never-ending build-out. Google notes that space may be “the best place to scale AI compute.” Google’s vision for scalable orbiting data centers relies on solar-powered satellites with free-space optical links connecting the nodes into a distributed network. Naturally, there are numerous engineering challenges to solve before Project Suncatcher is real. As a reference, Google points to the long road from its first moonshot self-driving cars 15 years ago to the Waymo vehicles that are almost fully autonomous today.Read full article Comments",
          "feed_position": 5,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/Suncatcher_hero-1152x648.png"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/Suncatcher_hero-1152x648.png",
      "popularity_score": 331.25300805555554,
      "ai_summary": [
        "Google is developing Project Suncatcher, AI data centers in space.",
        "Google is testing TPUs with radiation to prepare for the project.",
        "The article focuses on Google's plans for space-based data centers.",
        "No specific details about the project's timeline are provided.",
        "The article highlights Google's initiative in space computing."
      ]
    },
    {
      "id": "cluster_27",
      "coverage": 1,
      "updated_at": "Tue, 04 Nov 2025 22:14:24 +0000",
      "title": "New HDR10+ Advanced standard will try to fix the soap opera effect",
      "neutral_headline": "New HDR10+ Standard Aims to Improve Motion Smoothing",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2025/11/hdr10-advanced-joins-dolby-vision-2-in-trying-to-make-you-like-motion-smoothing/",
          "published_at": "Tue, 04 Nov 2025 22:14:24 +0000",
          "title": "New HDR10+ Advanced standard will try to fix the soap opera effect",
          "standfirst": "Can more creator control fix motion smoothing?",
          "content": "Motion smoothing has a bad reputation among most cinephiles, as well as many home theater enthusiasts and content creators. Also known as motion or video interpolation, motion smoothing is available in virtually every modern TV today. It’s supposed to remove judder from films and TV shows that are shot with 24p (24 frames per second) or 25p film and displayed on 60Hz or 120Hz TVs. But motion smoothing often results in the dreaded soap opera effect and unwanted visual artifacts. Two upcoming HDR standards, HDR10+ Advanced and Dolby Vision 2, are looking to change how we perceive motion smoothing and more closely align motion interpolation with a creator’s vision. However, it’s unclear if these standards can pull that off. HDR10+ Advanced’s Intelligent FRC Today, Samsung provided details about the next version of the HDR10 format, which introduces six new features. Among HDR10+ Advanced’s most interesting features is HDR10+ Intelligent FRC (frame rate conversion), which is supposed to improve motion smoothing.Read full article Comments",
          "feed_position": 3,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/mission-impossible-fallout-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/mission-impossible-fallout-1152x648.jpg",
      "popularity_score": 317.50911916666666,
      "ai_summary": [
        "A new HDR10+ Advanced standard is being introduced.",
        "The standard aims to address the \"soap opera effect.\"",
        "The article questions if creator control can fix motion smoothing.",
        "The focus is on the new standard and its goals.",
        "No specific technical details about the standard are provided."
      ]
    },
    {
      "id": "cluster_43",
      "coverage": 1,
      "updated_at": "Tue, 04 Nov 2025 21:26:03 +0000",
      "title": "US gives local police a face-scanning app similar to one used by ICE agents",
      "neutral_headline": "Police Receive Face-Scanning App Similar to ICE's",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/11/us-gives-local-police-a-face-scanning-app-similar-to-one-used-by-ice-agents/",
          "published_at": "Tue, 04 Nov 2025 21:26:03 +0000",
          "title": "US gives local police a face-scanning app similar to one used by ICE agents",
          "standfirst": "Mobile Identify app on Google Play helps police perform tasks delegated by ICE.",
          "content": "US Customs and Border Protection (CBP) launched a face-scanning app for local law enforcement agencies that assist the federal government with immigration-enforcement operations. The Mobile Identify app was released on the Google Play store on October 30. “This app facilitates functions authorized by Section 287(g) of the Immigration and Nationality Act (INA),” a US law that lets Immigration and Customs Enforcement (ICE) delegate immigration-officer duties to state and local law enforcement, according to the Mobile Identify app’s description on the Google Play store. “Through a formal agreement, or Memorandum of Agreement (MOA), with DHS [Department of Homeland Security], participating agencies like your Sheriff’s Department can have designated officers who are trained, certified, and authorized to perform certain immigration enforcement functions, helping to identify and process individuals who may be in the country unlawfully. This tool is built to streamline those responsibilities securely and efficiently, directly in the field.” A screenshot of the app on the Google Play listing shows it requires camera access “to take photos of subjects.” More information on how it works was reported today by 404 Media. “A source with knowledge of the app told 404 Media the app doesn’t return names after a face search. Instead it tells users to contact ICE and provides a reference number, or to not detain the person depending on the result,” the news report said.Read full article Comments",
          "feed_position": 4,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/ice-badge-1152x648-1762290384.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/ice-badge-1152x648-1762290384.jpg",
      "popularity_score": 306.7032858333333,
      "ai_summary": [
        "US police are using a face-scanning app.",
        "The app is similar to one used by ICE agents.",
        "The app is called Mobile Identify and is on Google Play.",
        "The app helps police perform tasks delegated by ICE.",
        "The article highlights the app's functionality and use."
      ]
    },
    {
      "id": "cluster_71",
      "coverage": 1,
      "updated_at": "Tue, 04 Nov 2025 17:58:23 +0000",
      "title": "Some stinkbugs’ legs carry a mobile fungal garden",
      "neutral_headline": "Some Stinkbugs Have Mobile Fungal Gardens",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2025/11/some-stinkbugs-legs-carry-a-mobile-fungal-garden/",
          "published_at": "Tue, 04 Nov 2025 17:58:23 +0000",
          "title": "Some stinkbugs’ legs carry a mobile fungal garden",
          "standfirst": "A dedicated organ grows the fungus, which deters parasitic wasps.",
          "content": "Many insect species hear using tympanal organs, membranes roughly resembling our eardrums but located on their legs. Grasshoppers, mantises, and moths all have them, and for decades, we thought that female stinkbugs of the Dinidoridae family have them, too, although located a bit unusually on their hind rather than front legs. Suspecting that they use their hind leg tympanal organs to listen to male courtship songs, a team of Japanese researchers took a closer look at the organs in Megymenum gracilicorne, a Dinidoridae stinkbug species native to Japan. They discovered that these “tympanal organs” were not what they seemed. They’re actually mobile fungal nurseries of a kind we’ve never seen before. Portable gardens Dinidoridae is a small stinkbug family that lives exclusively in Asia. The bug did attract some scientific attention, but not nearly as much as its larger relatives like Pentatomidae. Prior work looking specifically into organs growing on the hind legs of Dinidoridae females was thus somewhat limited. “Most research relied on taxonomic and morphological approaches. Some taxonomists did describe that female Dinidoridae stinkbugs have an enlarged part on the hind legs that looks like the tympanal organ you can find, for example, in crickets,” said Takema Fukatsu, an evolutionary biologist at the National Institute of Advanced Industrial Science and Technology in Tokyo.Read full article Comments",
          "feed_position": 6,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-1654488784-1024x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-1654488784-1024x648.jpg",
      "popularity_score": 280.2421747222222,
      "ai_summary": [
        "Certain stinkbugs have fungal gardens on their legs.",
        "A dedicated organ grows the fungus on the stinkbugs.",
        "The fungus deters parasitic wasps from attacking the stinkbugs.",
        "The article focuses on the stinkbugs' defense mechanism.",
        "The specific type of stinkbug is not mentioned."
      ]
    },
    {
      "id": "cluster_111",
      "coverage": 1,
      "updated_at": "Tue, 04 Nov 2025 10:00:27 +0000",
      "title": "Dune driving with Mercedes-Benz as it tests off-road systems",
      "neutral_headline": "Mercedes-Benz Tests Off-Road Systems in Dunes",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/cars/2025/11/dune-driving-with-mercedes-benz-as-it-tests-off-road-systems/",
          "published_at": "Tue, 04 Nov 2025 10:00:27 +0000",
          "title": "Dune driving with Mercedes-Benz as it tests off-road systems",
          "standfirst": "The low-traction surfaces of the Dumont Dunes are a perfect test location.",
          "content": "Mercedes-Benz provided flights from Washington, DC, to Las Vegas and accommodation so Ars could drive the prototype GLC. Ars does not accept paid editorial content. LAS VEGAS—About 100 miles from Las Vegas, at the northern end of the Mojave Desert, you’ll find a pretty large pile of sand. The Dumont Dunes formed thousands of years ago as sand from recently dried-up lakes blew in on the wind through gaps in the mountains. We’re talking real Lawrence of Arabia stuff—some dunes hundreds of feet high, and a large amount of it is run as a recreational area by the Bureau of Land Management for activities that include a bit of off-roading. Which is why we found a trailer full of Mercedes-Benz engineers and some preproduction prototype electric GLCs at work out there. When we last saw the next GLC, it was earlier this year at a German test track, wearing one of those black-and-white digital camouflage wraps that obscure the finer details of a new design. The automaker is dealing with the finishing touches ahead of the model going into production next year. The hardware is signed off on, but there’s plenty of code to tweak and calibrations to perform, including making sure that even when the terrain gets loose, the handling won’t. While I’m sure that the vast majority of GLC customers’ experience with rough surfaces won’t extend past the odd, particularly bad pothole, the car is being engineered to cope with much more. When fitted with the optional air suspension, the ground clearance can increase to up to 8.1 inches (206 mm) at low speed and 7.2 inches (183 mm) even up to highway speed, as long as the car is in the more extreme of the two Terrain modes. That allows for approach and departure angles as much as 21.4 degrees (approach) and 22.6 degrees (departure).Read full article Comments",
          "feed_position": 7,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/25C0285_011-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/25C0285_011-1152x648.jpg",
      "popularity_score": 270.27661916666665,
      "ai_summary": [
        "Mercedes-Benz is testing off-road systems.",
        "The testing is taking place in the Dumont Dunes.",
        "The low-traction surfaces of the dunes are ideal for testing.",
        "The article focuses on the testing location and purpose.",
        "No specific details about the systems being tested are provided."
      ]
    },
    {
      "id": "cluster_117",
      "coverage": 1,
      "updated_at": "Tue, 04 Nov 2025 02:08:26 +0000",
      "title": "Apple releases iOS 26.1, macOS 26.1, other updates with Liquid Glass controls and more",
      "neutral_headline": "Apple Releases iOS 26.1, macOS 26.1, and Other Updates",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2025/11/apple-releases-ios-26-1-macos-26-1-other-updates-with-liquid-glass-controls-and-more/",
          "published_at": "Tue, 04 Nov 2025 02:08:26 +0000",
          "title": "Apple releases iOS 26.1, macOS 26.1, other updates with Liquid Glass controls and more",
          "standfirst": "New updates include a small but noteworthy batch of fixes and features.",
          "content": "After several weeks of testing, Apple has released the final versions of the 26.1 update to its various operating systems. Those include iOS, iPadOS, macOS, watchOS, tvOS, visionOS, and the HomePod operating system, all of which switched to a new unified year-based version numbering system this fall. This isn’t the first update that these operating systems have gotten since they were released in September, but it is the first to add significant changes and tweaks to existing features, addressing the early complaints and bugs that inevitably come with any major operating system update. One of the biggest changes across most of the platforms is a new translucency control for Liquid Glass that tones it down without totally disabling the effect. Users can stay with the default Clear look to see the clearer, glassier look that allows more of the contents underneath Liquid Glass to show through, or the new Tinted look to get a more opaque background that shows only vague shapes and colors to improve readability.Read full article Comments",
          "feed_position": 8,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/06/apple-os-beta-26-2025-1152x648-1750706565.jpeg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/06/apple-os-beta-26-2025-1152x648-1750706565.jpeg",
      "popularity_score": 265,
      "ai_summary": [
        "Apple released iOS 26.1, macOS 26.1, and other updates.",
        "The updates include a batch of fixes and new features.",
        "The article highlights the new updates from Apple.",
        "The specific features and fixes are not detailed.",
        "The focus is on the release of the updates."
      ]
    },
    {
      "id": "cluster_120",
      "coverage": 1,
      "updated_at": "Mon, 03 Nov 2025 22:13:53 +0000",
      "title": "A commercial space station startup now has a foothold in space",
      "neutral_headline": "Commercial Space Station Startup Gains Foothold in Space",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2025/11/a-commercial-space-station-startup-now-has-a-foothold-in-space/",
          "published_at": "Mon, 03 Nov 2025 22:13:53 +0000",
          "title": "A commercial space station startup now has a foothold in space",
          "standfirst": "Vast differs from its space station cohorts by flying a series of progressively more complex demos.",
          "content": "A pathfinder mission for Vast’s privately owned space station launched into orbit Sunday and promptly extended its solar panel, kicking off a shakedown cruise to prove the company’s designs can meet the demands of spaceflight. Vast’s Haven Demo mission lifted off just after midnight Sunday from Cape Canaveral Space Force Station, Florida, and rode a SpaceX Falcon 9 rocket into orbit. Haven Demo was one of 18 satellites sharing a ride on SpaceX’s Bandwagon 4 mission, launching alongside a South Korean spy satellite and a small testbed for Starcloud, a startup working with Nvidia to build an orbital data center. After release from the Falcon 9, the half-ton Haven Demo spacecraft stabilized itself and extended its power-generating solar array. The satellite captured 4K video of the solar array deployment, and Vast shared the beauty shot on social media.Read full article Comments",
          "feed_position": 9,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/haven_demo_array-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/haven_demo_array-1152x648.jpg",
      "popularity_score": 251,
      "ai_summary": [
        "A commercial space station startup has a presence in space.",
        "Vast differs from other space station companies.",
        "Vast will fly a series of progressively complex demos.",
        "The article focuses on Vast's approach to space stations.",
        "No specific details about the demos are provided."
      ]
    },
    {
      "id": "cluster_132",
      "coverage": 1,
      "updated_at": "Mon, 03 Nov 2025 18:28:22 +0000",
      "title": "Google removes Gemma models from AI Studio after GOP senator’s complaint",
      "neutral_headline": "Google Removes Gemma Models After Senator's Complaint",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/google/2025/11/google-removes-gemma-models-from-ai-studio-after-gop-senators-complaint/",
          "published_at": "Mon, 03 Nov 2025 18:28:22 +0000",
          "title": "Google removes Gemma models from AI Studio after GOP senator’s complaint",
          "standfirst": "Sen. Marsha Blackburn says Gemma concocted sexual misconduct allegations against her.",
          "content": "You may be disappointed if you go looking for Google’s open Gemma AI model in AI Studio today. Google announced late on Friday that it was pulling Gemma from the platform, but it was vague about the reasoning. The abrupt change appears to be tied to a letter from Sen. Marsha Blackburn (R-Tenn.), who claims the Gemma model generated false accusations of sexual misconduct against her. Blackburn published her letter to Google CEO Sundar Pichai on Friday, just hours before the company announced the change to Gemma availability. She demanded Google explain how the model could fail in this way, tying the situation to ongoing hearings that accuse Google and others of creating bots that defame conservatives. At the hearing, Google’s Markham Erickson explained that AI hallucinations are a widespread and known issue in generative AI, and Google does the best it can to mitigate the impact of such mistakes. Although no AI firm has managed to eliminate hallucinations, Google’s Gemini for Home has been particularly hallucination-happy in our testing.Read full article Comments",
          "feed_position": 14,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/Gemma-social-share.width-1300-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/Gemma-social-share.width-1300-1152x648.jpg",
      "popularity_score": 160,
      "ai_summary": [
        "Google removed Gemma models from AI Studio.",
        "The removal followed a complaint from Senator Marsha Blackburn.",
        "Blackburn said Gemma fabricated sexual misconduct allegations.",
        "The article focuses on the removal and the reason.",
        "No details about the models' functionality are provided."
      ]
    },
    {
      "id": "cluster_136",
      "coverage": 1,
      "updated_at": "Mon, 03 Nov 2025 17:23:11 +0000",
      "title": "OpenAI signs massive AI compute deal with Amazon",
      "neutral_headline": "OpenAI Signs Massive AI Compute Deal with Amazon",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/ai/2025/11/openai-signs-massive-ai-compute-deal-with-amazon/",
          "published_at": "Mon, 03 Nov 2025 17:23:11 +0000",
          "title": "OpenAI signs massive AI compute deal with Amazon",
          "standfirst": "Deal will provide access to hundreds of thousands of Nvidia chips that power ChatGPT.",
          "content": "On Monday, OpenAI announced it has signed a seven-year, $38 billion deal to buy cloud services from Amazon Web Services to power products like ChatGPT and Sora. It’s the company’s first big computing deal after a fundamental restructuring last week that gave OpenAI more operational and financial freedom from Microsoft. The agreement gives OpenAI access to hundreds of thousands of Nvidia graphics processors to train and run its AI models. “Scaling frontier AI requires massive, reliable compute,” OpenAI CEO Sam Altman said in a statement. “Our partnership with AWS strengthens the broad compute ecosystem that will power this next era and bring advanced AI to everyone.” OpenAI will reportedly use Amazon Web Services immediately, with all planned capacity set to come online by the end of 2026 and room to expand further in 2027 and beyond. Amazon plans to roll out hundreds of thousands of chips, including Nvidia’s GB200 and GB300 AI accelerators, in data clusters built to power ChatGPT’s responses, generate AI videos, and train OpenAI’s next wave of models.Read full article Comments",
          "feed_position": 15,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2023/08/hiding_hero_1-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2023/08/hiding_hero_1-1152x648.jpg",
      "popularity_score": 145,
      "ai_summary": [
        "OpenAI signed a large AI compute deal with Amazon.",
        "The deal provides access to Nvidia chips.",
        "The chips will power ChatGPT.",
        "The article focuses on the deal and its implications.",
        "The number of chips is in the hundreds of thousands."
      ]
    },
    {
      "id": "cluster_121",
      "coverage": 1,
      "updated_at": "Mon, 03 Nov 2025 21:01:21 +0000",
      "title": "Real humans don’t stream Drake songs 23 hours a day, rapper suing Spotify says",
      "neutral_headline": "Real humans don’t stream Drake songs 23 hours a day, rapper suing Spotify says",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/11/real-humans-dont-stream-drake-songs-23-hours-a-day-rapper-suing-spotify-says/",
          "published_at": "Mon, 03 Nov 2025 21:01:21 +0000",
          "title": "Real humans don’t stream Drake songs 23 hours a day, rapper suing Spotify says",
          "standfirst": "Proposed class action may force Spotify to pay back artists harmed by streaming fraud.",
          "content": "Spotify profits off fake Drake streams that rob other artists of perhaps hundreds of millions in revenue shares, a lawsuit filed Sunday alleged—hoping to force Spotify to reimburse every artist impacted. The lawsuit was filed by an American rapper known as RBX, who may be best known for cameos on two of the 1990s’ biggest hip-hop records, Dr. Dre’s The Chronic and Snoop Dogg’s Doggystyle. The problem goes beyond Drake, RBX’s lawsuit alleged. It claims Spotify ignores “billions of fraudulent streams” each month, selfishly benefiting from bot networks that artificially inflate user numbers to help Spotify attract significantly higher ad revenue.Read full article Comments",
          "feed_position": 10,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-2182712179-1152x648-1762197922.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-2182712179-1152x648-1762197922.jpg",
      "popularity_score": 133,
      "ai_summary": [
        "Drake is suing Spotify over alleged streaming fraud.",
        "The lawsuit claims real humans do not stream Drake's songs constantly.",
        "A proposed class action may force Spotify to pay artists.",
        "The artists may have been harmed by streaming fraud.",
        "The article focuses on the lawsuit and its potential impact."
      ]
    },
    {
      "id": "cluster_123",
      "coverage": 1,
      "updated_at": "Mon, 03 Nov 2025 20:21:36 +0000",
      "title": "After confusing driver release, AMD says old GPUs are still actively supported",
      "neutral_headline": "AMD Supports Older GPUs Despite Driver Release Confusion",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2025/11/amd-says-that-its-not-pulling-driver-support-for-older-radeon-gpus-afterall/",
          "published_at": "Mon, 03 Nov 2025 20:21:36 +0000",
          "title": "After confusing driver release, AMD says old GPUs are still actively supported",
          "standfirst": "Re-using old silicon means that dropping \"old\" GPUs can affect \"new\" products.",
          "content": "Last week, AMD released version 25.10.2 of its Adrenalin driver package for Radeon GPUs. It seemed like a relatively routine driver release with a typical list of bug fixes and game performance improvements, except for one accompanying announcement: AMD said at the time that it would be moving support for Radeon RX 5000-series and 6000-series GPUs (and their RDNA 1 and RDNA 2 architectures) to “maintenance mode.” That meant that a bunch of GPUs, including some dedicated graphics cards launched as recently as 2022, would no longer get fresh fixes and performance optimizations for newly launched games. As reported by Tom’s Hardware, AMD released several clarifying statements to address the ensuing backlash, saying that these older GPUs would still get “new features, bug fixes, and game optimizations” based on “market needs.” That must not have quieted the complaints, because AMD then made an entirely separate post to confirm that the 25.10.2 driver release “is not the end of support for RDNA 1 and RDNA 2,” and that integrated and dedicated GPUs based on these architectures would continue to receive “game support for new releases,” “stability and game optimizations,” and “security and bug fixes.” AMD confirmed that these older GPU architectures had been moved to a separate driver path, but the company says this is meant to keep fixes and features intended for newer RDNA 3 and RDNA 4-based GPUs from inadvertently breaking things for RDNA 1 and RDNA 2 GPUs.Read full article Comments",
          "feed_position": 11,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/lisa-su-6900xt-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/lisa-su-6900xt-1152x648.jpg",
      "popularity_score": 133,
      "ai_summary": [
        "AMD's driver release caused confusion regarding support for older graphics cards.",
        "Reusing old silicon can impact support for newer products, according to AMD.",
        "Dropping support for older GPUs could affect the functionality of newer products.",
        "AMD aims to maintain active support for older graphics processing units.",
        "The company's strategy involves balancing support for old and new hardware."
      ]
    },
    {
      "id": "cluster_125",
      "coverage": 1,
      "updated_at": "Mon, 03 Nov 2025 20:08:40 +0000",
      "title": "LLMs show a “highly unreliable” capacity to describe their own internal processes",
      "neutral_headline": "LLMs Show Unreliable Capacity Describing Internal Processes",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/ai/2025/11/llms-show-a-highly-unreliable-capacity-to-describe-their-own-internal-processes/",
          "published_at": "Mon, 03 Nov 2025 20:08:40 +0000",
          "title": "LLMs show a “highly unreliable” capacity to describe their own internal processes",
          "standfirst": "Anthropic finds some LLM \"self-awareness,\" but \"failures of introspection remain the norm.\"",
          "content": "If you ask an LLM to explain its own reasoning process, it may well simply confabulate a plausible-sounding explanation for its actions based on text found in its training data. To get around this problem, Anthropic is expanding on its previous research into AI interpretability with a new study that aims to measure LLMs’ actual so-called “introspective awareness” of their own inference processes. The full paper on “Emergent Introspective Awareness in Large Language Models” uses some interesting methods to separate out the metaphorical “thought process” represented by an LLM’s artificial neurons from simple text output that purports to represent that process. In the end, though, the research finds that current AI models are “highly unreliable” at describing their own inner workings and that “failures of introspection remain the norm.” Inception, but for AI Anthropic’s new research is centered on a process it calls “concept injection.” The method starts by comparing the model’s internal activation states following both a control prompt and an experimental prompt (e.g. an “ALL CAPS” prompt versus the same prompt in lower case). Calculating the differences between those activations across billions of internal neurons creates what Anthropic calls a “vector” that in some sense represents how that concept is modeled in the LLM’s internal state.Read full article Comments",
          "feed_position": 12,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-514162112-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-514162112-1152x648.jpg",
      "popularity_score": 133,
      "ai_summary": [
        "Anthropic found that some LLMs exhibit self-awareness, but it is unreliable.",
        "Failures of introspection remain the norm for large language models.",
        "LLMs struggle to accurately describe their own internal processes.",
        "The research highlights limitations in current LLM self-understanding.",
        "The study suggests a need for improvements in LLM introspection capabilities."
      ]
    },
    {
      "id": "cluster_127",
      "coverage": 1,
      "updated_at": "Mon, 03 Nov 2025 19:15:21 +0000",
      "title": "Trump on why he pardoned Binance CEO: “Are you ready? I don’t know who he is.”",
      "neutral_headline": "Trump Pardoned Binance CEO, Citing Lack of Knowledge",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/11/trump-on-why-he-pardoned-binance-ceo-are-you-ready-i-dont-know-who-he-is/",
          "published_at": "Mon, 03 Nov 2025 19:15:21 +0000",
          "title": "Trump on why he pardoned Binance CEO: “Are you ready? I don’t know who he is.”",
          "standfirst": "Trump family business could benefit from pardon of crypto ex-con Changpeng Zhao.",
          "content": "President Trump says he still doesn’t know who Binance founder and former CEO Changpeng Zhao is, despite having pardoned Zhao last month. CBS correspondent Norah O’Donnell asked Trump about the pardon in a 60 Minutes interview that aired yesterday, noting that Zhao pleaded guilty to violating anti-money laundering laws. “The government at the time said that C.Z. had caused ‘significant harm to US national security,’ essentially by allowing terrorist groups like Hamas to move millions of dollars around. Why did you pardon him?” O’Donnell asked. “Okay, are you ready? I don’t know who he is. I know he got a four-month sentence or something like that. And I heard it was a Biden witch hunt,” answered Trump, who has criticized his predecessor for signing pardons with an autopen.Read full article Comments",
          "feed_position": 13,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2024/04/Changpeng-Zhao-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2024/04/Changpeng-Zhao-1152x648.jpg",
      "popularity_score": 133,
      "ai_summary": [
        "Donald Trump pardoned Binance CEO Changpeng Zhao.",
        "Trump stated he did not know who Zhao was when issuing the pardon.",
        "The Trump family business could potentially benefit from the pardon.",
        "Zhao was previously convicted of a crime related to cryptocurrency.",
        "The pardon has raised questions about potential conflicts of interest."
      ]
    },
    {
      "id": "cluster_138",
      "coverage": 1,
      "updated_at": "Mon, 03 Nov 2025 16:53:45 +0000",
      "title": "Capitol Hill is abuzz with talk of the “Athena” plan for NASA",
      "neutral_headline": "NASA's \"Athena\" Plan Discussed on Capitol Hill",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2025/11/capitol-hill-is-abuzz-with-talk-of-the-athena-plan-for-nasa/",
          "published_at": "Mon, 03 Nov 2025 16:53:45 +0000",
          "title": "Capitol Hill is abuzz with talk of the “Athena” plan for NASA",
          "standfirst": "The Athena plan lays out a blueprint for Isaacman's tenure at NASA.",
          "content": "In recent weeks, copies of an intriguing policy document have started to spread among space lobbyists on Capitol Hill in Washington, DC. The document bears the title “Athena,” and it purports to summarize the actions that private astronaut Jared Isaacman would have taken, were his nomination to become NASA administrator confirmed. The 62-page plan is notable both for the ideas to remake NASA that it espouses as well as the manner in which it has been leaked to the space community. After receiving a copy of this plan from an industry official, I spoke with multiple sources over the weekend to understand what is happening. Based upon this reporting there are clearly multiple layers to the story, which I want to unpack.Read full article Comments",
          "feed_position": 16,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/08/news-080625d-lg-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/08/news-080625d-lg-1152x648.jpg",
      "popularity_score": 133,
      "ai_summary": [
        "The \"Athena\" plan is being discussed on Capitol Hill.",
        "The plan outlines a blueprint for Isaacman's tenure at NASA.",
        "The plan's details are currently under review and discussion.",
        "The plan could influence future NASA initiatives and projects.",
        "The plan's impact on NASA's direction is a subject of debate."
      ]
    },
    {
      "id": "cluster_142",
      "coverage": 1,
      "updated_at": "Mon, 03 Nov 2025 14:45:11 +0000",
      "title": "Disruption to science will last longer than the US government shutdown",
      "neutral_headline": "Government Shutdown's Impact on Science Will Be Prolonged",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2025/11/disruption-to-science-will-last-longer-than-the-us-government-shutdown/",
          "published_at": "Mon, 03 Nov 2025 14:45:11 +0000",
          "title": "Disruption to science will last longer than the US government shutdown",
          "standfirst": "The consequences of the shutdown extend far beyond a lapse in funding.",
          "content": "US science always suffers during government shutdowns. Funding lapses send government scientists home without pay. Federal agencies suspend new grant opportunities, place expert review panels on hold, and stop collecting and analyzing critical public datasets that tell us about the economy, the environment, and public health. In 2025, the stakes are higher than in past shutdowns. This shutdown arrives at a time of massive upheaval to American science and innovation driven by President Donald Trump’s ongoing attempts to extend executive power and assert political control of scientific institutions.Read full article Comments",
          "feed_position": 17,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2018/12/getty-us-capitol-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2018/12/getty-us-capitol-1152x648.jpg",
      "popularity_score": 133,
      "ai_summary": [
        "The consequences of the government shutdown extend beyond funding lapses.",
        "The shutdown's effects on science will last longer than the shutdown itself.",
        "Research projects and scientific progress are being negatively affected.",
        "The shutdown has disrupted ongoing scientific investigations.",
        "The long-term implications for scientific advancement are significant."
      ]
    },
    {
      "id": "cluster_148",
      "coverage": 1,
      "updated_at": "Mon, 03 Nov 2025 12:00:51 +0000",
      "title": "Internet Archive’s legal fights are over, but its founder mourns what was lost",
      "neutral_headline": "Internet Archive Founder Reflects on Legal Battles",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/11/the-internet-archive-survived-major-copyright-losses-whats-next/",
          "published_at": "Mon, 03 Nov 2025 12:00:51 +0000",
          "title": "Internet Archive’s legal fights are over, but its founder mourns what was lost",
          "standfirst": "\"We survived, but it wiped out the library,\" Internet Archive's founder says.",
          "content": "Last month, the Internet Archive’s Wayback Machine archived its trillionth webpage, and the nonprofit invited its more than 1,200 library partners and 800,000 daily users to join a celebration of the moment. To honor “three decades of safeguarding the world’s online heritage,” the city of San Francisco declared October 22 to be “Internet Archive Day.” The Archive was also recently designated a federal depository library by Sen. Alex Padilla (D-Calif.), who proclaimed the organization a “perfect fit” to expand “access to federal government publications amid an increasingly digital landscape.” The Internet Archive might sound like a thriving organization, but it only recently emerged from years of bruising copyright battles that threatened to bankrupt the beloved library project. In the end, the fight led to more than 500,000 books being removed from the Archive’s “Open Library.” “We survived,” Internet Archive founder Brewster Kahle told Ars. “But it wiped out the Library.”Read full article Comments",
          "feed_position": 18,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/2025-10-22-Internet-Archive-1-Trillion-web-pages-375-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/2025-10-22-Internet-Archive-1-Trillion-web-pages-375-1152x648.jpg",
      "popularity_score": 133,
      "ai_summary": [
        "The Internet Archive's legal fights have concluded.",
        "The founder mourns the loss of the library's original scope.",
        "The Internet Archive survived the legal challenges.",
        "The founder believes the legal battles wiped out the library.",
        "The legal battles significantly altered the archive's operations."
      ]
    }
  ]
}