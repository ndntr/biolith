{
  "updated_at": "2025-11-12T03:44:18.615Z",
  "clusters": [
    {
      "id": "cluster_59",
      "coverage": 4,
      "updated_at": "Tue, 11 Nov 2025 12:15:01 -0500",
      "title": "Google rolls out Nano Banana image editing upgrades in Photos, including a \"help me edit\" feature that lets users make edits using text or voice prompts (Elyse Betters Picaro/ZDNET)",
      "neutral_headline": "Google rolls out Nano Banana image editing upgrades in Photos, including a \"help me edit\" feature that lets users make edits using text or voice prompts (Elyse Betters Picaro/ZDNET)",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/251111/p28#a251111p28",
          "published_at": "Tue, 11 Nov 2025 12:15:01 -0500",
          "title": "Google rolls out Nano Banana image editing upgrades in Photos, including a \"help me edit\" feature that lets users make edits using text or voice prompts (Elyse Betters Picaro/ZDNET)",
          "standfirst": "Elyse Betters Picaro / ZDNET: Google rolls out Nano Banana image editing upgrades in Photos, including a &ldquo;help me edit&rdquo; feature that lets users make edits using text or voice prompts &mdash; ZDNET's key takeaways &mdash; Photos has added several new AI-powered editing and search tools.",
          "content": "Elyse Betters Picaro / ZDNET: Google rolls out Nano Banana image editing upgrades in Photos, including a &ldquo;help me edit&rdquo; feature that lets users make edits using text or voice prompts &mdash; ZDNET's key takeaways &mdash; Photos has added several new AI-powered editing and search tools.",
          "feed_position": 13,
          "image_url": "http://www.techmeme.com/251111/i28.jpg"
        },
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/google/2025/11/googles-nano-banana-ai-image-editing-is-finally-coming-to-google-photos/",
          "published_at": "Tue, 11 Nov 2025 17:00:11 +0000",
          "title": "Google announces even more AI in Photos app, powered by Nano Banana",
          "standfirst": "Google's Nano Banana is powering a raft of new features in the app.",
          "content": "We’re running out of ways to tell you that Google is releasing more generative AI features, but that’s what’s happening in Google Photos today. The Big G is finally making good on its promise to add its market-leading Nano Banana image-editing model to the app. The model powers a couple of features, and it’s not just for Google’s Android platform. Nano Banana edits are also coming to the iOS version of the app. Nano Banana started making waves when it appeared earlier this year as an unbranded demo. You simply feed the model an image and tell it what edits you want to see. Google said Nano Banana was destined for the Photos app back in October, but it’s only now beginning the rollout. The Photos app already had conversational editing in the “Help Me Edit” feature, but it was running an older non-fruit model that produced inferior results. Nano Banana editing will produce AI slop, yes, but it’s better slop. Nano Banana in Help me edit Nano Banana in Help me edit Google says the updated Help Me Edit feature has access to your private face groups, so you can use names in your instructions. For example, you could type “Remove Riley’s sunglasses,” and Nano Banana will identify Riley in the photo (assuming you have a person of that name saved) and make the edit without further instructions. You can also ask for more fantastical edits in Help Me Edit, changing the style of the image from top to bottom.Read full article Comments",
          "feed_position": 5,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/GP_Nov-AI-Feature_Blog-Post-Hero-1152x648.png"
        },
        {
          "source": "The Verge",
          "url": "https://www.theverge.com/news/818346/google-photos-ios-help-me-edit-update",
          "published_at": "2025-11-11T12:00:00-05:00",
          "title": "Google Photos lets iPhone users edit images by describing changes",
          "standfirst": "Google is rolling out several AI updates to its Google Photos app, including iOS support for conversational edits, more accurate facial editing, and new AI tools. This means the “Help me edit” feature that lets you describe how you want Google’s AI to change your photos will be available to iPhone users, following its release [&#8230;]",
          "content": "iPhone users can now make edits in Google Photos by typing or verbally describing them. Google is rolling out several AI updates to its Google Photos app, including iOS support for conversational edits, more accurate facial editing, and new AI tools. This means the “Help me edit” feature that lets you describe how you want Google’s AI to change your photos will be available to iPhone users, following its release on Pixel and other Android devices. “Starting to roll out on iOS in the US, you can simply describe the edits you want using your voice or text and watch Google Photos bring your vision to life,” Google said in its announcement blog. This update also brings the redesigned editor UI for Google Photos to iPhones, making it easy to adjust images with simple gestures and one-tap suggestions. New personalized editing capabilities should make changing something about you or your friends&#8217; faces — such as removing glasses, opening blinked eyes, or inserting a smile — more accurate by referencing images from your private face groups. Google’s Nano Banana AI model is also being integrated into Google Photos, giving users more options for transforming images into new styles like paintings, mosaics, and illustrations. Other updates include a new “Ask” button for Android and iOS users, which launches a chatbot-style interface to edit images and answer questions about their contents, and ready-made AI templates for Android users that let you instantly edit photos using popular prompts, such as “put me in a high fashion photoshoot.” The “Ask Photos” tool that makes it easier to find specific images in your gallery is also being expanded to more than 100 new regions and 17 new languages.",
          "feed_position": 5
        },
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2025/11/11/google-photos-adds-new-ai-features-for-editing-expands-ai-powered-search-to-over-100-countries/",
          "published_at": "Tue, 11 Nov 2025 17:00:00 +0000",
          "title": "Google Photos adds new AI features for editing, expands AI-powered search to over 100 countries",
          "standfirst": "Google Photos is getting an image edit feature powered by the Nano Banana model.",
          "content": "Google Photos is getting an image edit feature powered by the Nano Banana model.",
          "feed_position": 6
        }
      ],
      "featured_image": "http://www.techmeme.com/251111/i28.jpg",
      "popularity_score": 4009.511773611111,
      "ai_summary": [
        "Google Photos is adding new AI-powered editing and search tools.",
        "The updates are powered by Google's Nano Banana model.",
        "The \"Help me edit\" feature allows edits via text or voice.",
        "Conversational edits are now supported on iOS devices.",
        "The new AI tools include more accurate facial editing."
      ]
    },
    {
      "id": "cluster_5",
      "coverage": 2,
      "updated_at": "Wed, 12 Nov 2025 00:00:00 GMT",
      "title": "Baidu just dropped an open-source multimodal AI that it claims beats GPT-5 and Gemini",
      "neutral_headline": "Baidu just dropped an open-source multimodal AI that it claims beats GPT-5 and Gemini",
      "items": [
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/baidu-just-dropped-an-open-source-multimodal-ai-that-it-claims-beats-gpt-5",
          "published_at": "Wed, 12 Nov 2025 00:00:00 GMT",
          "title": "Baidu just dropped an open-source multimodal AI that it claims beats GPT-5 and Gemini",
          "standfirst": "Baidu Inc., China&#x27;s largest search engine company, released a new artificial intelligence model on Monday that its developers claim outperforms competitors from Google and OpenAI on several vision-related benchmarks despite using a fraction of the computing resources typically required for such systems.The model, dubbed ERNIE-4.5-VL-28B-A3B-Thinking, is the latest salvo in an escalating competition among technology companies to build AI systems that can understand and reason about images, videos, and documents alongside traditional text — capabilities increasingly critical for enterprise applications ranging from automated document processing to industrial quality control.What sets Baidu&#x27;s release apart is its efficiency: the model activates just 3 billion parameters during operation while maintaining 28 billion total parameters through a sophisticated routing architecture. According to documentation released with the model, this design allows it to match or exceed the performance of much larger competing systems on tasks involving document understanding, chart analysis, and visual reasoning while consuming significantly less computational power and memory.\"Built upon the powerful ERNIE-4.5-VL-28B-A3B architecture, the newly upgraded ERNIE-4.5-VL-28B-A3B-Thinking achieves a remarkable leap forward in multimodal reasoning capabilities,\" Baidu wrote in the model&#x27;s technical documentation on Hugging Face, the AI model repository where the system was released.The company said the model underwent \"an extensive mid-training phase\" that incorporated \"a vast and highly diverse corpus of premium visual-language reasoning data,\" dramatically boosting its ability to align visual and textual information semantically.How the model mimics human visual problem-solving through dynamic image analysisPerhaps the model&#x27;s most distinctive feature is what Baidu calls \"Thinking with Images\" — a capability that allows the AI to dynamically zoom in and out of images to examine fine-grained details, mimicking how humans approach visual problem-solving tasks.\"The model thinks like a human, capable of freely zooming in and out of images to grasp every detail and uncover all information,\" according to the model card. When paired with tools like image search, Baidu claims this feature \"dramatically elevates the model&#x27;s ability to process fine-grained details and handle long-tail visual knowledge.\"This approach marks a departure from traditional vision-language models, which typically process images at a fixed resolution. By allowing dynamic image examination, the system can theoretically handle scenarios requiring both broad context and granular detail—such as analyzing complex technical diagrams or detecting subtle defects in manufacturing quality control.The model also supports what Baidu describes as enhanced \"visual grounding\" capabilities with \"more precise grounding and flexible instruction execution, easily triggering grounding functions in complex industrial scenarios,\" suggesting potential applications in robotics, warehouse automation, and other settings where AI systems must identify and locate specific objects in visual scenes.Baidu&#x27;s performance claims draw scrutiny as independent testing remains pendingBaidu&#x27;s assertion that the model outperforms Google&#x27;s Gemini 2.5 Pro and OpenAI&#x27;s GPT-5-High on various document and chart understanding benchmarks has drawn attention across social media, though independent verification of these claims remains pending.The company released the model under the permissive Apache 2.0 license, allowing unrestricted commercial use—a strategic decision that contrasts with the more restrictive licensing approaches of some competitors and could accelerate enterprise adoption.\"Apache 2.0 is smart,\" wrote one X user responding to Baidu&#x27;s announcement, highlighting the competitive advantage of open licensing in the enterprise market.According to Baidu&#x27;s documentation, the model demonstrates six core capabilities beyond traditional text processing. In visual reasoning, the system can perform what Baidu describes as \"multi-step reasoning, chart analysis, and causal reasoning capabilities in complex visual tasks,\" aided by what the company characterizes as \"large-scale reinforcement learning.\" For STEM problem solving, Baidu claims that \"leveraging its powerful visual abilities, the model achieves a leap in performance on STEM tasks like solving problems from photos.\" The visual grounding capability allows the model to identify and locate objects within images with what Baidu characterizes as industrial-grade precision. Through tool integration, the system can invoke external functions including image search capabilities to access information beyond its training data.For video understanding, Baidu claims the model possesses \"outstanding temporal awareness and event localization abilities, accurately identifying content changes across different time segments in a video.\" Finally, the thinking with images feature enables the dynamic zoom functionality that distinguishes this model from competitors.Inside the mixture-of-experts architecture that powers efficient multimodal processingUnder the hood, ERNIE-4.5-VL-28B-A3B-Thinking employs a Mixture-of-Experts (MoE) architecture — a design pattern that has become increasingly popular for building efficient large-scale AI systems. Rather than activating all 28 billion parameters for every task, the model uses a routing mechanism to selectively activate only the 3 billion parameters most relevant to each specific input.This approach offers substantial practical advantages for enterprise deployments. According to Baidu&#x27;s documentation, the model can run on a single 80GB GPU — hardware readily available in many corporate data centers — making it significantly more accessible than competing systems that may require multiple high-end accelerators.The technical documentation reveals that Baidu employed several advanced training techniques to achieve the model&#x27;s capabilities. The company used \"cutting-edge multimodal reinforcement learning techniques on verifiable tasks, integrating GSPO and IcePop strategies to stabilize MoE training combined with dynamic difficulty sampling for exceptional learning efficiency.\"Baidu also notes that in response to \"strong community demand,\" the company \"significantly strengthened the model&#x27;s grounding performance with improved instruction-following capabilities.\"The new model fits into Baidu&#x27;s ambitious multimodal AI ecosystemThe new release is one component of Baidu&#x27;s broader ERNIE 4.5 model family, which the company unveiled in June 2025. That family comprises 10 distinct variants, including Mixture-of-Experts models ranging from the flagship ERNIE-4.5-VL-424B-A47B with 424 billion total parameters down to a compact 0.3 billion parameter dense model.According to Baidu&#x27;s technical report on the ERNIE 4.5 family, the models incorporate \"a novel heterogeneous modality structure, which supports parameter sharing across modalities while also allowing dedicated parameters for each individual modality.\"This architectural choice addresses a longstanding challenge in multimodal AI development: training systems on both visual and textual data without one modality degrading the performance of the other. Baidu claims this design \"has the advantage to enhance multimodal understanding without compromising, and even improving, performance on text-related tasks.\"The company reported achieving 47% Model FLOPs Utilization (MFU) — a measure of training efficiency — during pre-training of its largest ERNIE 4.5 language model, using the PaddlePaddle deep learning framework developed in-house.Comprehensive developer tools aim to simplify enterprise deployment and integrationFor organizations looking to deploy the model, Baidu has released a comprehensive suite of development tools through ERNIEKit, what the company describes as an \"industrial-grade training and compression development toolkit.\"The model offers full compatibility with popular open-source frameworks including Hugging Face Transformers, vLLM (a high-performance inference engine), and Baidu&#x27;s own FastDeploy toolkit. This multi-platform support could prove critical for enterprise adoption, allowing organizations to integrate the model into existing AI infrastructure without wholesale platform changes.Sample code released by Baidu shows a relatively straightforward implementation path. Using the Transformers library, developers can load and run the model with approximately 30 lines of Python code, according to the documentation on Hugging Face.For production deployments requiring higher throughput, Baidu provides vLLM integration with specialized support for the model&#x27;s \"reasoning-parser\" and \"tool-call-parser\" capabilities — features that enable the dynamic image examination and external tool integration that distinguish this model from earlier systems.The company also offers FastDeploy, a proprietary inference toolkit that Baidu claims delivers \"production-ready, easy-to-use multi-hardware deployment solutions\" with support for various quantization schemes that can reduce memory requirements and increase inference speed.Why this release matters for the enterprise AI market at a critical inflection pointThe release comes at a pivotal moment in the enterprise AI market. As organizations move beyond experimental chatbot deployments toward production systems that process documents, analyze visual data, and automate complex workflows, demand for capable and cost-effective vision-language models has intensified.Several enterprise use cases appear particularly well-suited to the model&#x27;s capabilities. Document processing — extracting information from invoices, contracts, and forms — represents a massive market where accurate chart and table understanding directly translates to cost savings through automation. Manufacturing quality control, where AI systems must detect visual defects, could benefit from the model&#x27;s grounding capabilities. Customer service applications that handle images from users could leverage the multi-step visual reasoning.The model&#x27;s efficiency profile may prove especially attractive to mid-market organizations and startups that lack the computing budgets of large technology companies. By fitting on a single 80GB GPU — hardware costing roughly $10,000 to $30,000 depending on the specific model — the system becomes economically viable for a much broader range of organizations than models requiring multi-GPU setups costing hundreds of thousands of dollars.\"With all these new models, where&#x27;s the best place to actually build and scale? Access to compute is everything,\" wrote one X user in response to Baidu&#x27;s announcement, highlighting the persistent infrastructure challenges facing organizations attempting to deploy advanced AI systems.The Apache 2.0 licensing further lowers barriers to adoption. Unlike models released under more restrictive licenses that may limit commercial use or require revenue sharing, organizations can deploy ERNIE-4.5-VL-28B-A3B-Thinking in production applications without ongoing licensing fees or usage restrictions.Competition intensifies as Chinese tech giant takes aim at Google and OpenAIBaidu&#x27;s release intensifies competition in the vision-language model space, where Google, OpenAI, Anthropic, and Chinese companies including Alibaba and ByteDance have all released capable systems in recent months.The company&#x27;s performance claims — if validated by independent testing — would represent a significant achievement. Google&#x27;s Gemini 2.5 Pro and OpenAI&#x27;s GPT-5-High are substantially larger models backed by the deep resources of two of the world&#x27;s most valuable technology companies. That a more compact, openly available model could match or exceed their performance on specific tasks would suggest the field is advancing more rapidly than some analysts anticipated.\"Impressive that ERNIE is outperforming Gemini 2.5 Pro,\" wrote one social media commenter, expressing surprise at the claimed results.However, some observers counseled caution about benchmark comparisons. \"It&#x27;s fascinating to see how multimodal models are evolving, especially with features like &#x27;Thinking with Images,&#x27;\" wrote one X user. \"That said, I&#x27;m curious if ERNIE-4.5&#x27;s edge over competitors like Gemini-2.5-Pro and GPT-5-High primarily lies in specific use cases like document and chart\" understanding rather than general-purpose vision tasks.Industry analysts note that benchmark performance often fails to capture real-world behavior across the diverse scenarios enterprises encounter. A model that excels at document understanding may struggle with creative visual tasks or real-time video analysis. Organizations evaluating these systems typically conduct extensive internal testing on representative workloads before committing to production deployments.Technical limitations and infrastructure requirements that enterprises must considerDespite its capabilities, the model faces several technical challenges common to large vision-language systems. The minimum requirement of 80GB of GPU memory, while more accessible than some competitors, still represents a significant infrastructure investment. Organizations without existing GPU infrastructure would need to procure specialized hardware or rely on cloud computing services, introducing ongoing operational costs.The model&#x27;s context window — the amount of text and visual information it can process simultaneously — is listed as 128K tokens in Baidu&#x27;s documentation. While substantial, this may prove limiting for some document processing scenarios involving very long technical manuals or extensive video content.Questions also remain about the model&#x27;s behavior on adversarial inputs, out-of-distribution data, and edge cases. Baidu&#x27;s documentation does not provide detailed information about safety testing, bias mitigation, or failure modes — considerations increasingly important for enterprise deployments where errors could have financial or safety implications.What technical decision-makers need to evaluate beyond the benchmark numbersFor technical decision-makers evaluating the model, several implementation factors warrant consideration beyond raw performance metrics.The model&#x27;s MoE architecture, while efficient during inference, adds complexity to deployment and optimization. Organizations must ensure their infrastructure can properly route inputs to the appropriate expert subnetworks — a capability not universally supported across all deployment platforms.The \"Thinking with Images\" feature, while innovative, requires integration with image manipulation tools to achieve its full potential. Baidu&#x27;s documentation suggests this capability works best \"when paired with tools like image zooming and image search,\" implying that organizations may need to build additional infrastructure to fully leverage this functionality.The model&#x27;s video understanding capabilities, while highlighted in marketing materials, come with practical constraints. Processing video requires substantially more computational resources than static images, and the documentation does not specify maximum video length or optimal frame rates.Organizations considering deployment should also evaluate Baidu&#x27;s ongoing commitment to the model. Open-source AI models require continuing maintenance, security updates, and potential retraining as data distributions shift over time. While the Apache 2.0 license ensures the model remains available, future improvements and support depend on Baidu&#x27;s strategic priorities.Developer community responds with enthusiasm tempered by practical requestsEarly response from the AI research and development community has been cautiously optimistic. Developers have requested versions of the model in additional formats including GGUF (a quantization format popular for local deployment) and MNN (a mobile neural network framework), suggesting interest in running the system on resource-constrained devices.\"Release MNN and GGUF so I can run it on my phone,\" wrote one developer, highlighting demand for mobile deployment options.Other developers praised Baidu&#x27;s technical choices while requesting additional resources. \"Fantastic model! Did you use discoveries from PaddleOCR?\" asked one user, referencing Baidu&#x27;s open-source optical character recognition toolkit.The model&#x27;s lengthy name—ERNIE-4.5-VL-28B-A3B-Thinking—drew lighthearted commentary. \"ERNIE-4.5-VL-28B-A3B-Thinking might be the longest model name in history,\" joked one observer. \"But hey, if you&#x27;re outperforming Gemini-2.5-Pro with only 3B active params, you&#x27;ve earned the right to a dramatic name!\"Baidu plans to showcase the ERNIE lineup during its Baidu World 2025 conference on November 13, where the company is expected to provide additional details about the model&#x27;s development, performance validation, and future roadmap.The release marks a strategic move by Baidu to establish itself as a major player in the global AI infrastructure market. While Chinese AI companies have historically focused primarily on domestic markets, the open-source release under a permissive license signals ambitions to compete internationally with Western AI giants.For enterprises, the release adds another capable option to a rapidly expanding menu of AI models. Organizations no longer face a binary choice between building proprietary systems or licensing closed-source models from a handful of vendors. The proliferation of capable open-source alternatives like ERNIE-4.5-VL-28B-A3B-Thinking is reshaping the economics of AI deployment and accelerating adoption across industries.Whether the model delivers on its performance promises in real-world deployments remains to be seen. But for organizations seeking powerful, cost-effective tools for visual understanding and reasoning, one thing is certain. As one developer succinctly summarized: \"Open source plus commercial use equals chef&#x27;s kiss. Baidu not playing around.\"",
          "content": "Baidu Inc., China&#x27;s largest search engine company, released a new artificial intelligence model on Monday that its developers claim outperforms competitors from Google and OpenAI on several vision-related benchmarks despite using a fraction of the computing resources typically required for such systems.The model, dubbed ERNIE-4.5-VL-28B-A3B-Thinking, is the latest salvo in an escalating competition among technology companies to build AI systems that can understand and reason about images, videos, and documents alongside traditional text — capabilities increasingly critical for enterprise applications ranging from automated document processing to industrial quality control.What sets Baidu&#x27;s release apart is its efficiency: the model activates just 3 billion parameters during operation while maintaining 28 billion total parameters through a sophisticated routing architecture. According to documentation released with the model, this design allows it to match or exceed the performance of much larger competing systems on tasks involving document understanding, chart analysis, and visual reasoning while consuming significantly less computational power and memory.\"Built upon the powerful ERNIE-4.5-VL-28B-A3B architecture, the newly upgraded ERNIE-4.5-VL-28B-A3B-Thinking achieves a remarkable leap forward in multimodal reasoning capabilities,\" Baidu wrote in the model&#x27;s technical documentation on Hugging Face, the AI model repository where the system was released.The company said the model underwent \"an extensive mid-training phase\" that incorporated \"a vast and highly diverse corpus of premium visual-language reasoning data,\" dramatically boosting its ability to align visual and textual information semantically.How the model mimics human visual problem-solving through dynamic image analysisPerhaps the model&#x27;s most distinctive feature is what Baidu calls \"Thinking with Images\" — a capability that allows the AI to dynamically zoom in and out of images to examine fine-grained details, mimicking how humans approach visual problem-solving tasks.\"The model thinks like a human, capable of freely zooming in and out of images to grasp every detail and uncover all information,\" according to the model card. When paired with tools like image search, Baidu claims this feature \"dramatically elevates the model&#x27;s ability to process fine-grained details and handle long-tail visual knowledge.\"This approach marks a departure from traditional vision-language models, which typically process images at a fixed resolution. By allowing dynamic image examination, the system can theoretically handle scenarios requiring both broad context and granular detail—such as analyzing complex technical diagrams or detecting subtle defects in manufacturing quality control.The model also supports what Baidu describes as enhanced \"visual grounding\" capabilities with \"more precise grounding and flexible instruction execution, easily triggering grounding functions in complex industrial scenarios,\" suggesting potential applications in robotics, warehouse automation, and other settings where AI systems must identify and locate specific objects in visual scenes.Baidu&#x27;s performance claims draw scrutiny as independent testing remains pendingBaidu&#x27;s assertion that the model outperforms Google&#x27;s Gemini 2.5 Pro and OpenAI&#x27;s GPT-5-High on various document and chart understanding benchmarks has drawn attention across social media, though independent verification of these claims remains pending.The company released the model under the permissive Apache 2.0 license, allowing unrestricted commercial use—a strategic decision that contrasts with the more restrictive licensing approaches of some competitors and could accelerate enterprise adoption.\"Apache 2.0 is smart,\" wrote one X user responding to Baidu&#x27;s announcement, highlighting the competitive advantage of open licensing in the enterprise market.According to Baidu&#x27;s documentation, the model demonstrates six core capabilities beyond traditional text processing. In visual reasoning, the system can perform what Baidu describes as \"multi-step reasoning, chart analysis, and causal reasoning capabilities in complex visual tasks,\" aided by what the company characterizes as \"large-scale reinforcement learning.\" For STEM problem solving, Baidu claims that \"leveraging its powerful visual abilities, the model achieves a leap in performance on STEM tasks like solving problems from photos.\" The visual grounding capability allows the model to identify and locate objects within images with what Baidu characterizes as industrial-grade precision. Through tool integration, the system can invoke external functions including image search capabilities to access information beyond its training data.For video understanding, Baidu claims the model possesses \"outstanding temporal awareness and event localization abilities, accurately identifying content changes across different time segments in a video.\" Finally, the thinking with images feature enables the dynamic zoom functionality that distinguishes this model from competitors.Inside the mixture-of-experts architecture that powers efficient multimodal processingUnder the hood, ERNIE-4.5-VL-28B-A3B-Thinking employs a Mixture-of-Experts (MoE) architecture — a design pattern that has become increasingly popular for building efficient large-scale AI systems. Rather than activating all 28 billion parameters for every task, the model uses a routing mechanism to selectively activate only the 3 billion parameters most relevant to each specific input.This approach offers substantial practical advantages for enterprise deployments. According to Baidu&#x27;s documentation, the model can run on a single 80GB GPU — hardware readily available in many corporate data centers — making it significantly more accessible than competing systems that may require multiple high-end accelerators.The technical documentation reveals that Baidu employed several advanced training techniques to achieve the model&#x27;s capabilities. The company used \"cutting-edge multimodal reinforcement learning techniques on verifiable tasks, integrating GSPO and IcePop strategies to stabilize MoE training combined with dynamic difficulty sampling for exceptional learning efficiency.\"Baidu also notes that in response to \"strong community demand,\" the company \"significantly strengthened the model&#x27;s grounding performance with improved instruction-following capabilities.\"The new model fits into Baidu&#x27;s ambitious multimodal AI ecosystemThe new release is one component of Baidu&#x27;s broader ERNIE 4.5 model family, which the company unveiled in June 2025. That family comprises 10 distinct variants, including Mixture-of-Experts models ranging from the flagship ERNIE-4.5-VL-424B-A47B with 424 billion total parameters down to a compact 0.3 billion parameter dense model.According to Baidu&#x27;s technical report on the ERNIE 4.5 family, the models incorporate \"a novel heterogeneous modality structure, which supports parameter sharing across modalities while also allowing dedicated parameters for each individual modality.\"This architectural choice addresses a longstanding challenge in multimodal AI development: training systems on both visual and textual data without one modality degrading the performance of the other. Baidu claims this design \"has the advantage to enhance multimodal understanding without compromising, and even improving, performance on text-related tasks.\"The company reported achieving 47% Model FLOPs Utilization (MFU) — a measure of training efficiency — during pre-training of its largest ERNIE 4.5 language model, using the PaddlePaddle deep learning framework developed in-house.Comprehensive developer tools aim to simplify enterprise deployment and integrationFor organizations looking to deploy the model, Baidu has released a comprehensive suite of development tools through ERNIEKit, what the company describes as an \"industrial-grade training and compression development toolkit.\"The model offers full compatibility with popular open-source frameworks including Hugging Face Transformers, vLLM (a high-performance inference engine), and Baidu&#x27;s own FastDeploy toolkit. This multi-platform support could prove critical for enterprise adoption, allowing organizations to integrate the model into existing AI infrastructure without wholesale platform changes.Sample code released by Baidu shows a relatively straightforward implementation path. Using the Transformers library, developers can load and run the model with approximately 30 lines of Python code, according to the documentation on Hugging Face.For production deployments requiring higher throughput, Baidu provides vLLM integration with specialized support for the model&#x27;s \"reasoning-parser\" and \"tool-call-parser\" capabilities — features that enable the dynamic image examination and external tool integration that distinguish this model from earlier systems.The company also offers FastDeploy, a proprietary inference toolkit that Baidu claims delivers \"production-ready, easy-to-use multi-hardware deployment solutions\" with support for various quantization schemes that can reduce memory requirements and increase inference speed.Why this release matters for the enterprise AI market at a critical inflection pointThe release comes at a pivotal moment in the enterprise AI market. As organizations move beyond experimental chatbot deployments toward production systems that process documents, analyze visual data, and automate complex workflows, demand for capable and cost-effective vision-language models has intensified.Several enterprise use cases appear particularly well-suited to the model&#x27;s capabilities. Document processing — extracting information from invoices, contracts, and forms — represents a massive market where accurate chart and table understanding directly translates to cost savings through automation. Manufacturing quality control, where AI systems must detect visual defects, could benefit from the model&#x27;s grounding capabilities. Customer service applications that handle images from users could leverage the multi-step visual reasoning.The model&#x27;s efficiency profile may prove especially attractive to mid-market organizations and startups that lack the computing budgets of large technology companies. By fitting on a single 80GB GPU — hardware costing roughly $10,000 to $30,000 depending on the specific model — the system becomes economically viable for a much broader range of organizations than models requiring multi-GPU setups costing hundreds of thousands of dollars.\"With all these new models, where&#x27;s the best place to actually build and scale? Access to compute is everything,\" wrote one X user in response to Baidu&#x27;s announcement, highlighting the persistent infrastructure challenges facing organizations attempting to deploy advanced AI systems.The Apache 2.0 licensing further lowers barriers to adoption. Unlike models released under more restrictive licenses that may limit commercial use or require revenue sharing, organizations can deploy ERNIE-4.5-VL-28B-A3B-Thinking in production applications without ongoing licensing fees or usage restrictions.Competition intensifies as Chinese tech giant takes aim at Google and OpenAIBaidu&#x27;s release intensifies competition in the vision-language model space, where Google, OpenAI, Anthropic, and Chinese companies including Alibaba and ByteDance have all released capable systems in recent months.The company&#x27;s performance claims — if validated by independent testing — would represent a significant achievement. Google&#x27;s Gemini 2.5 Pro and OpenAI&#x27;s GPT-5-High are substantially larger models backed by the deep resources of two of the world&#x27;s most valuable technology companies. That a more compact, openly available model could match or exceed their performance on specific tasks would suggest the field is advancing more rapidly than some analysts anticipated.\"Impressive that ERNIE is outperforming Gemini 2.5 Pro,\" wrote one social media commenter, expressing surprise at the claimed results.However, some observers counseled caution about benchmark comparisons. \"It&#x27;s fascinating to see how multimodal models are evolving, especially with features like &#x27;Thinking with Images,&#x27;\" wrote one X user. \"That said, I&#x27;m curious if ERNIE-4.5&#x27;s edge over competitors like Gemini-2.5-Pro and GPT-5-High primarily lies in specific use cases like document and chart\" understanding rather than general-purpose vision tasks.Industry analysts note that benchmark performance often fails to capture real-world behavior across the diverse scenarios enterprises encounter. A model that excels at document understanding may struggle with creative visual tasks or real-time video analysis. Organizations evaluating these systems typically conduct extensive internal testing on representative workloads before committing to production deployments.Technical limitations and infrastructure requirements that enterprises must considerDespite its capabilities, the model faces several technical challenges common to large vision-language systems. The minimum requirement of 80GB of GPU memory, while more accessible than some competitors, still represents a significant infrastructure investment. Organizations without existing GPU infrastructure would need to procure specialized hardware or rely on cloud computing services, introducing ongoing operational costs.The model&#x27;s context window — the amount of text and visual information it can process simultaneously — is listed as 128K tokens in Baidu&#x27;s documentation. While substantial, this may prove limiting for some document processing scenarios involving very long technical manuals or extensive video content.Questions also remain about the model&#x27;s behavior on adversarial inputs, out-of-distribution data, and edge cases. Baidu&#x27;s documentation does not provide detailed information about safety testing, bias mitigation, or failure modes — considerations increasingly important for enterprise deployments where errors could have financial or safety implications.What technical decision-makers need to evaluate beyond the benchmark numbersFor technical decision-makers evaluating the model, several implementation factors warrant consideration beyond raw performance metrics.The model&#x27;s MoE architecture, while efficient during inference, adds complexity to deployment and optimization. Organizations must ensure their infrastructure can properly route inputs to the appropriate expert subnetworks — a capability not universally supported across all deployment platforms.The \"Thinking with Images\" feature, while innovative, requires integration with image manipulation tools to achieve its full potential. Baidu&#x27;s documentation suggests this capability works best \"when paired with tools like image zooming and image search,\" implying that organizations may need to build additional infrastructure to fully leverage this functionality.The model&#x27;s video understanding capabilities, while highlighted in marketing materials, come with practical constraints. Processing video requires substantially more computational resources than static images, and the documentation does not specify maximum video length or optimal frame rates.Organizations considering deployment should also evaluate Baidu&#x27;s ongoing commitment to the model. Open-source AI models require continuing maintenance, security updates, and potential retraining as data distributions shift over time. While the Apache 2.0 license ensures the model remains available, future improvements and support depend on Baidu&#x27;s strategic priorities.Developer community responds with enthusiasm tempered by practical requestsEarly response from the AI research and development community has been cautiously optimistic. Developers have requested versions of the model in additional formats including GGUF (a quantization format popular for local deployment) and MNN (a mobile neural network framework), suggesting interest in running the system on resource-constrained devices.\"Release MNN and GGUF so I can run it on my phone,\" wrote one developer, highlighting demand for mobile deployment options.Other developers praised Baidu&#x27;s technical choices while requesting additional resources. \"Fantastic model! Did you use discoveries from PaddleOCR?\" asked one user, referencing Baidu&#x27;s open-source optical character recognition toolkit.The model&#x27;s lengthy name—ERNIE-4.5-VL-28B-A3B-Thinking—drew lighthearted commentary. \"ERNIE-4.5-VL-28B-A3B-Thinking might be the longest model name in history,\" joked one observer. \"But hey, if you&#x27;re outperforming Gemini-2.5-Pro with only 3B active params, you&#x27;ve earned the right to a dramatic name!\"Baidu plans to showcase the ERNIE lineup during its Baidu World 2025 conference on November 13, where the company is expected to provide additional details about the model&#x27;s development, performance validation, and future roadmap.The release marks a strategic move by Baidu to establish itself as a major player in the global AI infrastructure market. While Chinese AI companies have historically focused primarily on domestic markets, the open-source release under a permissive license signals ambitions to compete internationally with Western AI giants.For enterprises, the release adds another capable option to a rapidly expanding menu of AI models. Organizations no longer face a binary choice between building proprietary systems or licensing closed-source models from a handful of vendors. The proliferation of capable open-source alternatives like ERNIE-4.5-VL-28B-A3B-Thinking is reshaping the economics of AI deployment and accelerating adoption across industries.Whether the model delivers on its performance promises in real-world deployments remains to be seen. But for organizations seeking powerful, cost-effective tools for visual understanding and reasoning, one thing is certain. As one developer succinctly summarized: \"Open source plus commercial use equals chef&#x27;s kiss. Baidu not playing around.\"",
          "feed_position": 0,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/6PAfmxLhH2Yv7BpN8vczIt/f33baa764e279c49d01c5a10da8eef61/nuneybits_Vector_art_of_a_GPU_made_out_of_computer_code_and_the_59f97a50-f492-452f-bd5d-1d6e6e904c4a.webp?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/metas-spice-framework-lets-ai-systems-teach-themselves-to-reason",
          "published_at": "Tue, 11 Nov 2025 22:21:00 GMT",
          "title": "Meta’s SPICE framework lets AI systems teach themselves to reason",
          "standfirst": "Researchers at Meta FAIR and the National University of Singapore have developed a new reinforcement learning framework for self-improving AI systems. Called Self-Play In Corpus Environments (SPICE), the framework pits two AI agents against each other, creating its own challenges and gradually improving without human supervision.While currently a proof-of-concept, this self-play mechanism could provide a basis for future AI systems that can dynamically adapt to their environments, making them more robust against the unpredictability of real-world applications.The challenge of self-improving AIThe goal of self-improving AI is to create systems that can enhance their capabilities by interacting with their environment. A common approach is reinforcement learning with verifiable rewards (RLVR), where models are rewarded for providing the correct answers to problems. This is often limited by its reliance on human-curated problem sets and domain-specific reward engineering, which makes it difficult to scale.Self-play, where a model improves by competing against itself, is another promising paradigm. But existing self-play methods for language models are often limited by two critical factors. Factual errors in generated questions and answers compound, leading to a feedback loop of hallucinations. When the problem generator and solver have information symmetry (i.e., share the same knowledge base) they fail to generate genuinely new challenges and fall into repetitive patterns. As the researchers note in their paper, “These systematic empirical failures indicate that self-improvement requires interaction with an external source providing diverse, verifiable feedback, rather than closed-loop pure introspection.”How SPICE worksSPICE is a self-play framework where a single model acts in two distinct roles. A \"Challenger\" constructs a curriculum of challenging problems from a large corpus of documents. A \"Reasoner\" then attempts to solve these problems without access to the source documents. This setup breaks the information symmetry that limits other self-play methods, as the Reasoner does not have access to the documents and knowledge that the Challenger uses to generate the problems.Grounding the tasks in a vast and diverse corpus of documents prevents hallucination by anchoring questions and answers in real-world content. This is important because for AI systems to reliably self-improve, they need external grounding sources. Therefore, LLM agents should learn from interactions with humans and the real world, not just their own outputs, to avoid compounding errors.The adversarial dynamic between the two roles creates an automatic curriculum. The Challenger is rewarded for generating problems that are both diverse and at the frontier of the Reasoner&#x27;s capability (not too easy and also not impossible). The Reasoner is rewarded for answering correctly. This symbiotic interaction pushes both agents to continuously discover and overcome new challenges. Because the system uses raw documents instead of pre-defined question-answer pairs, it can generate diverse task formats, such as multiple-choice and free-form questions. This flexibility allows SPICE to be applied to any domain, breaking the bottleneck that has confined previous methods to narrow fields like math and code. It also reduces dependence on expensive human-curated datasets for specialized domains like legal or medical analysis.SPICE in actionThe researchers evaluated SPICE on several base models, including Qwen3-4B-Base and OctoThinker-3B-Hybrid-Base. They compared its performance against baselines such as the base model with no training, a Reasoner model trained with a fixed \"Strong Challenger\" (Qwen3-32B-Instruct), and pure self-play methods like R-Zero and Absolute Zero. The evaluation covered a wide range of mathematical and general reasoning benchmarks.Across all models, SPICE consistently outperformed the baselines, delivering significant improvements in both mathematical and general reasoning tasks. The results show that the reasoning capabilities developed through corpus-grounded self-play transfer broadly across different models, thanks to the diverse external knowledge corpus they used.A key finding is that the adversarial dynamic creates an effective automatic curriculum. As training progresses, the Challenger learns to generate increasingly difficult problems. In one experiment, the Reasoner&#x27;s pass rate on a fixed set of problems increased from 55% to 85% over time, showing its improved capabilities. Meanwhile, later versions of the Challenger were able to generate questions that dropped the pass rate of an early-stage Reasoner from 55% to 35%, confirming that both roles co-evolve successfully.The researchers conclude that this approach presents a paradigm shift in self-improving reasoning methods from “closed-loop self-play that often stagnates due to hallucination drift, to open-ended improvement through interaction with the vast, verifiable knowledge embedded in web document corpora.”Currently, the corpus used for SPICE represents human experience captured in text. The ultimate goal is for self-improving systems to generate questions based on interactions with reality, including the physical world, the internet, and human interactions across multiple modalities like video, audio, and sensor data.",
          "content": "Researchers at Meta FAIR and the National University of Singapore have developed a new reinforcement learning framework for self-improving AI systems. Called Self-Play In Corpus Environments (SPICE), the framework pits two AI agents against each other, creating its own challenges and gradually improving without human supervision.While currently a proof-of-concept, this self-play mechanism could provide a basis for future AI systems that can dynamically adapt to their environments, making them more robust against the unpredictability of real-world applications.The challenge of self-improving AIThe goal of self-improving AI is to create systems that can enhance their capabilities by interacting with their environment. A common approach is reinforcement learning with verifiable rewards (RLVR), where models are rewarded for providing the correct answers to problems. This is often limited by its reliance on human-curated problem sets and domain-specific reward engineering, which makes it difficult to scale.Self-play, where a model improves by competing against itself, is another promising paradigm. But existing self-play methods for language models are often limited by two critical factors. Factual errors in generated questions and answers compound, leading to a feedback loop of hallucinations. When the problem generator and solver have information symmetry (i.e., share the same knowledge base) they fail to generate genuinely new challenges and fall into repetitive patterns. As the researchers note in their paper, “These systematic empirical failures indicate that self-improvement requires interaction with an external source providing diverse, verifiable feedback, rather than closed-loop pure introspection.”How SPICE worksSPICE is a self-play framework where a single model acts in two distinct roles. A \"Challenger\" constructs a curriculum of challenging problems from a large corpus of documents. A \"Reasoner\" then attempts to solve these problems without access to the source documents. This setup breaks the information symmetry that limits other self-play methods, as the Reasoner does not have access to the documents and knowledge that the Challenger uses to generate the problems.Grounding the tasks in a vast and diverse corpus of documents prevents hallucination by anchoring questions and answers in real-world content. This is important because for AI systems to reliably self-improve, they need external grounding sources. Therefore, LLM agents should learn from interactions with humans and the real world, not just their own outputs, to avoid compounding errors.The adversarial dynamic between the two roles creates an automatic curriculum. The Challenger is rewarded for generating problems that are both diverse and at the frontier of the Reasoner&#x27;s capability (not too easy and also not impossible). The Reasoner is rewarded for answering correctly. This symbiotic interaction pushes both agents to continuously discover and overcome new challenges. Because the system uses raw documents instead of pre-defined question-answer pairs, it can generate diverse task formats, such as multiple-choice and free-form questions. This flexibility allows SPICE to be applied to any domain, breaking the bottleneck that has confined previous methods to narrow fields like math and code. It also reduces dependence on expensive human-curated datasets for specialized domains like legal or medical analysis.SPICE in actionThe researchers evaluated SPICE on several base models, including Qwen3-4B-Base and OctoThinker-3B-Hybrid-Base. They compared its performance against baselines such as the base model with no training, a Reasoner model trained with a fixed \"Strong Challenger\" (Qwen3-32B-Instruct), and pure self-play methods like R-Zero and Absolute Zero. The evaluation covered a wide range of mathematical and general reasoning benchmarks.Across all models, SPICE consistently outperformed the baselines, delivering significant improvements in both mathematical and general reasoning tasks. The results show that the reasoning capabilities developed through corpus-grounded self-play transfer broadly across different models, thanks to the diverse external knowledge corpus they used.A key finding is that the adversarial dynamic creates an effective automatic curriculum. As training progresses, the Challenger learns to generate increasingly difficult problems. In one experiment, the Reasoner&#x27;s pass rate on a fixed set of problems increased from 55% to 85% over time, showing its improved capabilities. Meanwhile, later versions of the Challenger were able to generate questions that dropped the pass rate of an early-stage Reasoner from 55% to 35%, confirming that both roles co-evolve successfully.The researchers conclude that this approach presents a paradigm shift in self-improving reasoning methods from “closed-loop self-play that often stagnates due to hallucination drift, to open-ended improvement through interaction with the vast, verifiable knowledge embedded in web document corpora.”Currently, the corpus used for SPICE represents human experience captured in text. The ultimate goal is for self-improving systems to generate questions based on interactions with reality, including the physical world, the internet, and human interactions across multiple modalities like video, audio, and sensor data.",
          "feed_position": 1,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/UAYtncQWQ2a2g1rzWOCHR/19d3e60c8a6eef26b99a5a492491bbec/Adversarial_AI_training.jpg?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/only-9-of-developers-think-ai-code-can-be-used-without-human-oversight",
          "published_at": "Tue, 11 Nov 2025 19:43:00 GMT",
          "title": "Only 9% of developers think AI code can be used without human oversight, BairesDev survey reveals",
          "standfirst": "Senior software developers are preparing for a major shift in how they work as artificial intelligence becomes central to their workflows, according to BairesDev’s latest Dev Barometer report published today. VentureBeat was given an exclusive early look and the findings below come directly from that report. The quarterly global survey, which polled 501 developers and 19 project managers across 92 software initiatives, finds that nearly two-thirds (65%) of senior developers expect their roles to be redefined by AI in 2026. The data highlights a transformation underway in software development: fewer routine coding tasks, more emphasis on design and strategy, and a rising need for AI fluency.From Coders to StrategistsAmong those anticipating change, 74% say they expect to shift from hands-on coding to designing solutions. Another 61% plan to integrate AI-generated code into their workflows, and half foresee spending more time on system strategy and architecture.“It’s not about lines of code anymore,” said Justice Erolin, Chief Technology Officer at BairesDev, in a recent interview with VentureBeat conducted over video call. “It’s about the quality and type of code, and the kind of work developers are doing.”Erolin said the company is watching developers evolve from individual contributors into system thinkers.“AI is great at code scaffolding and generating unit tests, saving developers around eight hours a week,” he explained. “That time can now be used for solution architecture and strategy work—areas where AI still falls short.”The survey’s data reflects this shift. Developers are moving toward higher-value tasks while automation takes over much of the repetitive coding that once occupied junior engineers.Erolin noted that BairesDev’s internal data mirrors these findings. “We’re seeing a shift where senior engineers with AI tools are outperforming, and even replacing, the traditional senior-plus-junior team setup,” he said.Realism About AI’s LimitsDespite widespread enthusiasm, developers remain cautious about AI’s reliability.Over half (56%) describe AI-generated code as “somewhat reliable,” saying it still requires validation for accuracy and security. Only 9% trust it enough to use without human oversight.Erolin agreed with that sentiment. “AI doesn’t replace human oversight,” he said. “Even as tools improve, developers still need to understand how individual components fit into the bigger system.” He added that the biggest constraint in large language models today is “their context window”—the limited ability to retain and reason across entire systems. “Engineers need to think holistically about architecture, not just individual lines of code,” he said.The CTO described 2025 as a turning point for how engineers use AI tools like GitHub Copilot, Cursor, Claude, and OpenAI’s models. “We’re tracking what tools and models our engineers use,” he said. “But the bigger story is how those tools impact learning, productivity, and oversight.”That tempered optimism aligns with BairesDev’s previous Dev Barometer findings, which reported that 92% of developers were already using AI-assisted coding by Q3 2025, saving an average of 7.3 hours per week.A Year of UpskillingIn 2025, AI integration already brought tangible professional benefits. 74% of developers said the technology strengthened their technical skills, 50% reported better work-life balance, and 37% said AI tools expanded their career opportunities.Erolin said the company is seeing AI emerge as “a top use case for upskilling.” Developers use it to “learn new technologies faster and fill knowledge gaps,” he noted. “When developers understand how AI works and its limitations, they can use it to enhance—not replace—their critical thinking. They prompt better and learn more efficiently.”Still, he warned of a potential long-term risk in the industry’s current trajectory. “If junior engineers are being replaced or not hired, we’ll face a shortage of qualified senior engineers in ten years as current ones retire,” Erolin said.The Dev Barometer findings echo that concern. Developers expect leaner teams, but many also worry that fewer entry-level opportunities could lead to long-term talent pipeline issues.Leaner Teams, New PrioritiesDevelopers expect 2026 to bring smaller, more specialized teams. 58% say automation will reduce entry-level tasks, while 63% expect new career paths to emerge as AI redefines team structures. 59% anticipate that AI will create entirely new specialized roles.According to BairesDev’s data, developers currently divide their time between writing code (48%), debugging (42%), and documentation (35%). Only 19% report focusing primarily on creative problem-solving and innovation—a share that’s expected to grow as AI removes lower-level coding tasks.The report also highlights where developers see the fastest-growing areas for 2026: AI/ML (67%), data analytics (46%), and cybersecurity (45%). In parallel, 63% of project managers said developers will need more training in AI, cloud, and security.Erolin described the next generation of developers as “T-shaped engineers”—people with broad system knowledge and deep expertise in one or more areas. “The most important developer moving forward will be the T-shaped engineer,” he said. “Broad in understanding, deep in skill.”AI as an Industry StandardThe Q4 Dev Barometer frames AI not as an experiment but as a foundation for how teams will operate in 2026. Developers are moving beyond using AI as a coding shortcut and instead incorporating it into architecture, validation, and design decisions.Erolin emphasized that BairesDev is already adapting its internal teams to this new reality. “Our engineers are full-time with us, and we staff them out where they’re needed,” he said. “Some clients need help for six months to a year; others outsource their entire dev team to us.”He said BairesDev provides “about 5,000 software engineers from Latin America, offering clients timezone-aligned, culturally aligned, and highly fluent English-speaking talent.”As developers integrate AI deeper into their daily work, Erolin believes the competitive advantage will belong to those who understand both the technology’s capabilities and its constraints. “When developers learn to collaborate with AI instead of compete against it, that’s when the real productivity and creativity gains happen,” he said.Background: Who BairesDev IsFounded in Buenos Aires in 2009 by Nacho De Marco and Paul Azorin, BairesDev began with a mission to connect what it describes as the “top 1%” of Latin American developers with global companies seeking high-quality software solutions. The company grew from those early roots into a major nearshore software development and staffing provider, offering everything from individual developer placements to full end-to-end project outsourcing.Today, BairesDev claims to have delivered more than 1,200 projects across 130+ industries, serving hundreds of clients ranging from startups to Fortune 500 firms such as Google, Adobe, and Rolls-Royce. It operates with a remote-first model and a workforce of over 4,000 professionals across more than 40 countries, aligning its teams to North American time zones.The company emphasizes three core advantages: access to elite technical talent across 100+ technologies, rapid scalability for project needs, and nearshore proximity for real-time collaboration. It reports client relationships averaging over three years and a satisfaction rate around 91%.BairesDev’s unique position—bridging Latin American talent with global enterprise clients—gives it an unusually data-rich perspective on how AI is transforming software development at scale.The TakeawayThe Dev Barometer’s Q4 2025 results suggest 2026 will mark a turning point for software engineering. Developers are becoming system architects rather than pure coders, AI literacy is becoming a baseline requirement, and traditional entry-level roles may give way to new, specialized positions.As AI becomes embedded in every stage of development—from design to testing—developers who can combine technical fluency with strategic thinking are set to lead the next era of software creation.",
          "content": "Senior software developers are preparing for a major shift in how they work as artificial intelligence becomes central to their workflows, according to BairesDev’s latest Dev Barometer report published today. VentureBeat was given an exclusive early look and the findings below come directly from that report. The quarterly global survey, which polled 501 developers and 19 project managers across 92 software initiatives, finds that nearly two-thirds (65%) of senior developers expect their roles to be redefined by AI in 2026. The data highlights a transformation underway in software development: fewer routine coding tasks, more emphasis on design and strategy, and a rising need for AI fluency.From Coders to StrategistsAmong those anticipating change, 74% say they expect to shift from hands-on coding to designing solutions. Another 61% plan to integrate AI-generated code into their workflows, and half foresee spending more time on system strategy and architecture.“It’s not about lines of code anymore,” said Justice Erolin, Chief Technology Officer at BairesDev, in a recent interview with VentureBeat conducted over video call. “It’s about the quality and type of code, and the kind of work developers are doing.”Erolin said the company is watching developers evolve from individual contributors into system thinkers.“AI is great at code scaffolding and generating unit tests, saving developers around eight hours a week,” he explained. “That time can now be used for solution architecture and strategy work—areas where AI still falls short.”The survey’s data reflects this shift. Developers are moving toward higher-value tasks while automation takes over much of the repetitive coding that once occupied junior engineers.Erolin noted that BairesDev’s internal data mirrors these findings. “We’re seeing a shift where senior engineers with AI tools are outperforming, and even replacing, the traditional senior-plus-junior team setup,” he said.Realism About AI’s LimitsDespite widespread enthusiasm, developers remain cautious about AI’s reliability.Over half (56%) describe AI-generated code as “somewhat reliable,” saying it still requires validation for accuracy and security. Only 9% trust it enough to use without human oversight.Erolin agreed with that sentiment. “AI doesn’t replace human oversight,” he said. “Even as tools improve, developers still need to understand how individual components fit into the bigger system.” He added that the biggest constraint in large language models today is “their context window”—the limited ability to retain and reason across entire systems. “Engineers need to think holistically about architecture, not just individual lines of code,” he said.The CTO described 2025 as a turning point for how engineers use AI tools like GitHub Copilot, Cursor, Claude, and OpenAI’s models. “We’re tracking what tools and models our engineers use,” he said. “But the bigger story is how those tools impact learning, productivity, and oversight.”That tempered optimism aligns with BairesDev’s previous Dev Barometer findings, which reported that 92% of developers were already using AI-assisted coding by Q3 2025, saving an average of 7.3 hours per week.A Year of UpskillingIn 2025, AI integration already brought tangible professional benefits. 74% of developers said the technology strengthened their technical skills, 50% reported better work-life balance, and 37% said AI tools expanded their career opportunities.Erolin said the company is seeing AI emerge as “a top use case for upskilling.” Developers use it to “learn new technologies faster and fill knowledge gaps,” he noted. “When developers understand how AI works and its limitations, they can use it to enhance—not replace—their critical thinking. They prompt better and learn more efficiently.”Still, he warned of a potential long-term risk in the industry’s current trajectory. “If junior engineers are being replaced or not hired, we’ll face a shortage of qualified senior engineers in ten years as current ones retire,” Erolin said.The Dev Barometer findings echo that concern. Developers expect leaner teams, but many also worry that fewer entry-level opportunities could lead to long-term talent pipeline issues.Leaner Teams, New PrioritiesDevelopers expect 2026 to bring smaller, more specialized teams. 58% say automation will reduce entry-level tasks, while 63% expect new career paths to emerge as AI redefines team structures. 59% anticipate that AI will create entirely new specialized roles.According to BairesDev’s data, developers currently divide their time between writing code (48%), debugging (42%), and documentation (35%). Only 19% report focusing primarily on creative problem-solving and innovation—a share that’s expected to grow as AI removes lower-level coding tasks.The report also highlights where developers see the fastest-growing areas for 2026: AI/ML (67%), data analytics (46%), and cybersecurity (45%). In parallel, 63% of project managers said developers will need more training in AI, cloud, and security.Erolin described the next generation of developers as “T-shaped engineers”—people with broad system knowledge and deep expertise in one or more areas. “The most important developer moving forward will be the T-shaped engineer,” he said. “Broad in understanding, deep in skill.”AI as an Industry StandardThe Q4 Dev Barometer frames AI not as an experiment but as a foundation for how teams will operate in 2026. Developers are moving beyond using AI as a coding shortcut and instead incorporating it into architecture, validation, and design decisions.Erolin emphasized that BairesDev is already adapting its internal teams to this new reality. “Our engineers are full-time with us, and we staff them out where they’re needed,” he said. “Some clients need help for six months to a year; others outsource their entire dev team to us.”He said BairesDev provides “about 5,000 software engineers from Latin America, offering clients timezone-aligned, culturally aligned, and highly fluent English-speaking talent.”As developers integrate AI deeper into their daily work, Erolin believes the competitive advantage will belong to those who understand both the technology’s capabilities and its constraints. “When developers learn to collaborate with AI instead of compete against it, that’s when the real productivity and creativity gains happen,” he said.Background: Who BairesDev IsFounded in Buenos Aires in 2009 by Nacho De Marco and Paul Azorin, BairesDev began with a mission to connect what it describes as the “top 1%” of Latin American developers with global companies seeking high-quality software solutions. The company grew from those early roots into a major nearshore software development and staffing provider, offering everything from individual developer placements to full end-to-end project outsourcing.Today, BairesDev claims to have delivered more than 1,200 projects across 130+ industries, serving hundreds of clients ranging from startups to Fortune 500 firms such as Google, Adobe, and Rolls-Royce. It operates with a remote-first model and a workforce of over 4,000 professionals across more than 40 countries, aligning its teams to North American time zones.The company emphasizes three core advantages: access to elite technical talent across 100+ technologies, rapid scalability for project needs, and nearshore proximity for real-time collaboration. It reports client relationships averaging over three years and a satisfaction rate around 91%.BairesDev’s unique position—bridging Latin American talent with global enterprise clients—gives it an unusually data-rich perspective on how AI is transforming software development at scale.The TakeawayThe Dev Barometer’s Q4 2025 results suggest 2026 will mark a turning point for software engineering. Developers are becoming system architects rather than pure coders, AI literacy is becoming a baseline requirement, and traditional entry-level roles may give way to new, specialized positions.As AI becomes embedded in every stage of development—from design to testing—developers who can combine technical fluency with strategic thinking are set to lead the next era of software creation.",
          "feed_position": 2,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/5CiQuierRq34dYmuxJUNHb/a86df483038c29162f381f3fc96559f7/cfr0z3n_aerial_view_extended_view_of_hundreds_of_software_dev_51f077e9-5eef-4c86-bf4e-2dabd7ab231b_1.png?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/get-half-off-our-favorite-budgeting-app-for-black-friday-140025864.html",
          "published_at": "Tue, 11 Nov 2025 17:01:26 +0000",
          "title": "Black Friday deals include half off our favorite budgeting app",
          "standfirst": "Now's the time of year you might be reconsidering how you budget your finances, or establishing a plan if you don't have one already. While it's possible to do it all yourself, budgeting apps can automate some processes and make it easier to see where your money is going and patterns, both good and bad, that might be occurring. For Black Friday, you can get 50 percent off our favorite budgeting app, Quicken Simplifi. The Quicken Simplifi app is down to $3 monthly from $6 monthly, adding up to $36 for the year. Quicken Classic, the company's \"original desktop software\" for \"experienced investors\" is also half off at $6 monthly, down from $12 monthly. The sale starts today and is available until Wednesday, December 3. One of the many things that sets Quicken Simplifi apart from its competitors is its sleek, easy to use interface. The setup is pretty straightforward and it allows for your spouse or financial advisor to act as co-manager of the account. It also clearly shows figures like net worth, recent spending, upcoming recurring payments and more. Plus, there's an option to say if you're expecting a refund. Quicken Simplifi unfortunately doesn't offer a free trial so testing it out with a discount means less money invested if it's not for you. This article originally appeared on Engadget at https://www.engadget.com/deals/get-half-off-our-favorite-budgeting-app-for-black-friday-140025864.html?src=rss",
          "content": "Now's the time of year you might be reconsidering how you budget your finances, or establishing a plan if you don't have one already. While it's possible to do it all yourself, budgeting apps can automate some processes and make it easier to see where your money is going and patterns, both good and bad, that might be occurring. For Black Friday, you can get 50 percent off our favorite budgeting app, Quicken Simplifi. The Quicken Simplifi app is down to $3 monthly from $6 monthly, adding up to $36 for the year. Quicken Classic, the company's \"original desktop software\" for \"experienced investors\" is also half off at $6 monthly, down from $12 monthly. The sale starts today and is available until Wednesday, December 3. One of the many things that sets Quicken Simplifi apart from its competitors is its sleek, easy to use interface. The setup is pretty straightforward and it allows for your spouse or financial advisor to act as co-manager of the account. It also clearly shows figures like net worth, recent spending, upcoming recurring payments and more. Plus, there's an option to say if you're expecting a refund. Quicken Simplifi unfortunately doesn't offer a free trial so testing it out with a discount means less money invested if it's not for you. This article originally appeared on Engadget at https://www.engadget.com/deals/get-half-off-our-favorite-budgeting-app-for-black-friday-140025864.html?src=rss",
          "feed_position": 10
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/apples-airpods-4-drop-to-85-ahead-of-black-friday-162917226.html",
          "published_at": "Tue, 11 Nov 2025 16:16:28 +0000",
          "title": "Apple's AirPods 4 drop to $85 ahead of Black Friday",
          "standfirst": "Apple's AirPods 4 have dropped back down to a record low price in an early Black Friday deal on Amazon. If you aren't looking for active noise cancellation, the model without ANC is a steal at 34 percent off, bringing it down to just $85 from its usual price of $130. The Apple AirPods 4 are the best budget AirPods you can get in 2025, with Apple's H2 audio chip to support some of the more advanced audio features from more expensive models. They offer Voice Isolation, Personalized Spatial Audio with dynamic head tracking and more. If you get the model without active noise cancellation, you won't have features like Transparency Mode and Conversation Awareness, or Apple's hearing health tools. But, the entry-level model still offers great sound quality for the price. This model also features the redesigned shape, which makes for a more comfortable and secure fit so you don't have to worry about them falling out of your ears. A force sensor on the stem allows for basic touch controls, including play and pause, play next track, previous track and answer a call. You can also summon Siri by pressing and holding the stem. You can expect to get up to 5 hours of battery life on a charge with the non-ANC model, and up to 30 hours using the USB-C charging case. This article originally appeared on Engadget at https://www.engadget.com/deals/apples-airpods-4-drop-to-85-ahead-of-black-friday-162917226.html?src=rss",
          "content": "Apple's AirPods 4 have dropped back down to a record low price in an early Black Friday deal on Amazon. If you aren't looking for active noise cancellation, the model without ANC is a steal at 34 percent off, bringing it down to just $85 from its usual price of $130. The Apple AirPods 4 are the best budget AirPods you can get in 2025, with Apple's H2 audio chip to support some of the more advanced audio features from more expensive models. They offer Voice Isolation, Personalized Spatial Audio with dynamic head tracking and more. If you get the model without active noise cancellation, you won't have features like Transparency Mode and Conversation Awareness, or Apple's hearing health tools. But, the entry-level model still offers great sound quality for the price. This model also features the redesigned shape, which makes for a more comfortable and secure fit so you don't have to worry about them falling out of your ears. A force sensor on the stem allows for basic touch controls, including play and pause, play next track, previous track and answer a call. You can also summon Siri by pressing and holding the stem. You can expect to get up to 5 hours of battery life on a charge with the non-ANC model, and up to 30 hours using the USB-C charging case. This article originally appeared on Engadget at https://www.engadget.com/deals/apples-airpods-4-drop-to-85-ahead-of-black-friday-162917226.html?src=rss",
          "feed_position": 12
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/nintendo-announces-its-black-friday-and-cyber-monday-2025-sale-switch-2-bundles-switch-game-deals-and-more-155223898.html",
          "published_at": "Tue, 11 Nov 2025 16:00:35 +0000",
          "title": "Nintendo announces its Black Friday and Cyber Monday 2025 sale: Switch 2 bundles, Switch game deals and more",
          "standfirst": "When it comes to holiday video game gifts, Nintendo gear is often at the top of the list for kids and adults like. This year likely more so than ever thanks to the launch of the Switch 2 back in the spring. But fi you were hoping to save money on the console with Black Friday deals, you may be disappointed. The Nintendo Black Friday sale was just announced, and unsurprisingly, there are a scant few real \"deals\" to be had. This is typical of Nintendo, though — actual Nintendo Black Friday deals are few and far between. However, there are ways to at least get the best value for your money if you're going to pick up a Switch 2 before this year is over. As has been the case for many years, the marquee Nintendo deals for the holidays come in the form of console bundles. When the Switch 2 launched earlier this year, it was available as just the console only for $449 or bundled with Mario Kart World for $499. Both options are still available now, but there's a new bundle to consider as well — the console with the new Pokémon Legends: Z-A game, which also costs $499. Considering the games by themselves cost $70 each, you do save a bit by picking up a console bundle. you can pick up the console and its bundles at most retailers including Amazon, Walmart, Best Buy and others. When it comes to deals on Nintendo Switch 2 games, the Nintendo eShop will have Cyber Deals starting on November 20, running through December 3. The shop will feature \"holiday offers on select games,\" so it appears we'll all just have to go to the online store on November 20 to see the games on offer. Starting on November 23, select retailers will have discounts on some physical Switch games including Princess Peach: Showtime!, The Legend of Zelda: Echoes of Wisdom, Luigi’s Mansion 3 and Kirby’s Return to Dream Land Deluxe. Those will each be $40, while other games like Super Mario Odyssey, Nintendo Switch Sports, Paper Mario: The Thousand-Year Door and Splatoon 3 will be $30. Even if you can't get huge discounts on Nintendo consoles or new games this year, that doesn't mean you can't find decent deals on other Nintendo gear. There are plenty of great ideas for gifts for the Nintendo fan in your life, and Engadget's Sam Rutherford got to see a bunch of them in person when he attended Nintendo's holiday showcase. From collectibles to clothing to plushies and holiday decor, there's really a ton to choose from — but you may want to pace yourself if you're also a Nintendo fan finding things that you want to pick up for yourself in the process of looking for good gifts. Here are just some of the best Nintendo gift ideas that you can look out for during Black Friday and Cyber Monday. This article originally appeared on Engadget at https://www.engadget.com/deals/nintendo-announces-its-black-friday-and-cyber-monday-2025-sale-switch-2-bundles-switch-game-deals-and-more-155223898.html?src=rss",
          "content": "When it comes to holiday video game gifts, Nintendo gear is often at the top of the list for kids and adults like. This year likely more so than ever thanks to the launch of the Switch 2 back in the spring. But fi you were hoping to save money on the console with Black Friday deals, you may be disappointed. The Nintendo Black Friday sale was just announced, and unsurprisingly, there are a scant few real \"deals\" to be had. This is typical of Nintendo, though — actual Nintendo Black Friday deals are few and far between. However, there are ways to at least get the best value for your money if you're going to pick up a Switch 2 before this year is over. As has been the case for many years, the marquee Nintendo deals for the holidays come in the form of console bundles. When the Switch 2 launched earlier this year, it was available as just the console only for $449 or bundled with Mario Kart World for $499. Both options are still available now, but there's a new bundle to consider as well — the console with the new Pokémon Legends: Z-A game, which also costs $499. Considering the games by themselves cost $70 each, you do save a bit by picking up a console bundle. you can pick up the console and its bundles at most retailers including Amazon, Walmart, Best Buy and others. When it comes to deals on Nintendo Switch 2 games, the Nintendo eShop will have Cyber Deals starting on November 20, running through December 3. The shop will feature \"holiday offers on select games,\" so it appears we'll all just have to go to the online store on November 20 to see the games on offer. Starting on November 23, select retailers will have discounts on some physical Switch games including Princess Peach: Showtime!, The Legend of Zelda: Echoes of Wisdom, Luigi’s Mansion 3 and Kirby’s Return to Dream Land Deluxe. Those will each be $40, while other games like Super Mario Odyssey, Nintendo Switch Sports, Paper Mario: The Thousand-Year Door and Splatoon 3 will be $30. Even if you can't get huge discounts on Nintendo consoles or new games this year, that doesn't mean you can't find decent deals on other Nintendo gear. There are plenty of great ideas for gifts for the Nintendo fan in your life, and Engadget's Sam Rutherford got to see a bunch of them in person when he attended Nintendo's holiday showcase. From collectibles to clothing to plushies and holiday decor, there's really a ton to choose from — but you may want to pace yourself if you're also a Nintendo fan finding things that you want to pick up for yourself in the process of looking for good gifts. Here are just some of the best Nintendo gift ideas that you can look out for during Black Friday and Cyber Monday. This article originally appeared on Engadget at https://www.engadget.com/deals/nintendo-announces-its-black-friday-and-cyber-monday-2025-sale-switch-2-bundles-switch-game-deals-and-more-155223898.html?src=rss",
          "feed_position": 13
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/black-friday-deals-for-2025-are-here-early-we-found-the-best-tech-sales-from-apple-amazon-lego-anker-and-others-100052831.html",
          "published_at": "Tue, 11 Nov 2025 15:30:35 +0000",
          "title": "Black Friday deals for 2025 are here early: We found the best tech sales from Apple, Amazon, Lego, Anker and others",
          "standfirst": "Black Friday has become the time to buy the hottest tech of the year. Whether you're shopping for yourself or stocking up on gifts for the holidays, Black Friday deals are sure to bring the best prices of the year to things like headphones, game consoles, robot vacuums, phone accessories and everything in between. You don't even have to wait until Black Friday proper to save a ton of money. Over the past few years, we've seen Black Friday tech deals start earlier and earlier — to the point where the entire month of November is packed with discounts.If you're on the hunt for solid tech deals, Engadget has you covered. We've collected the best Black Friday deals on tech you can get right now, and we'll continue to update this post as we get closer to the big day at the end of November. Note that you probably have the best chance of snagging record-low prices when we get to about one week before Thanksgiving, but these deals available now are worth considering. Black Friday deals to shop now Apple AirPods 4 for $85 (34 percent off): These are the most affordable AirPods you can get, and the latest model has been substantially improved over the previous. They have a better fit and noticeably better sound quality than their predecessor, plus some advanced features previously only found on pricer models. Apple AirTags (four pack) for $65 (34 percent off): iPhone users who frequently misplace things should invest in a few AirTags. Slip them into your wallet, bag, jacket and other belongings to keep track of their locations in the Find My app. Just make sure that, if you're going to attach one to your keys, you also pick up an AirTag holder to go along with it. LEGO Star Wars Millennium Falcon A New Hope 25th Anniversary Collectable 75375 for $68 (20 percent off): This is a set that any Star Wars fan will love to build and then love to display once it's complete. The 921-piece set features a fully-detailed Millennium Falcone, buildable stand and nameplate. It's one of many Lego Black Friday deals you can get right now. Dyson 360 Vis Nav robot vacuum for $400 ($600 off): This is one of the best robot vacuums you can get, period. It doesn't have a self-emptying base, but its superior suction power almost makes up for that. It's one of the strongest robot vacuums I've ever tested, and it has excellent obstacle avoidance. The latter means you will rarely, if ever, have to attend to it getting caught on the edge of a carpet or getting stuck under a piece of furniture. If a cordless stick vacuum is what you're looking for, don't forget to check out all of the other Dyson Black Friday deals. Apple Watch SE 3 for $200 ($50 off): The SE has been our top pick for the best Apple Watch for those on a budget, and the latest model only solidifies that further. It has the same chipset found in the latest flagship Apple Watches, fast-charging capabilities, an always-on display and most of the same activity-tracking features you'll find in more expensive model. Apple Mac Mini M4 for $499 ($100 off): Desktop users looking for an upgrade should consider the latest Mac Mini, which runs on the M4 chip and 16GB of RAM as standard in the base configuration. This version has a smaller design that takes up less space, front-facing USB-C ports and a headphone jack, plus Thunderbolt 5 support. Jisulife Life 7 handheld fan for $25 (14 percent off): This handy little fan is a must-have if you life in a warm climate or have a tropical vacation planned anytime soon. It can be used as a table or handheld fan and even be worn around the neck so you don't have to hold it at all. Its 5,000 mAh battery allows it to last hours on a single charge, and the small display in the middle of the fan's blades show its remaining battery level. Leebin 2025 electric spin scrubber for $40 (43 percent off, Prime exclusive): This weird little scrubber makes cleaning my bathroom and shower much less of a pain. Just choose the brush head you need for your job and the rotating head takes care of most of the hard work. I love the adjustable handle, which extends from 12 to 50 inches so you can get into hard-to-reach places without breaking a sweat. Monarch Money budgeting app (one year) for $50 (50 percent off with code MONARCHVIP): One of our favorite budgeting apps, Monarch Money gives you a lot of control over the organization of your funds. There's a helpful goals feature for when you're planning out big purchases or financial milestones you want to hit, and we found the month-in-review recap it provides to be more thorough than other budgeting apps we tried. There's even Zillow integration for folks looking to buy a home. SanDisk microSD Express card (256GB) for $60 (12 percent off): If you have a Switch 2, no regular microSD card will do if you want to expand the console's storage. You need a newer microSD Express card, and currently there are only a handful on the market. We did some testing to find the best microSD Express card for the Switch 2 and found that performance was, in general, very similar amongst all the readily available cards. We recommend getting whichever fits within your budget at the capacity you want. Google TV Streamer 4K for $75 ($25 off): Our top pick for the best streaming device right now, the latest version of Google's streamer supports 4K video and an excellent, easy-to-use interface that will feel familiar to anyone who's seen a set with the Google TV technology built in. It provides access to all of the major streaming services including Netflix, Disney+, HBO Max, YouTube and more, plus it has a handy on-screen pop up that lets you control all compatible smart home devices right from your TV. Also available at Walmart. Cosori 9-in-1 air fryer for $90 (25 percent off): I personally have this air fryer, one of our top picks, in my house and I've used it for over a year with no issues. I love that it makes good use of vertical space so it doesn't take up too much space on my counter, and its rounded-square shape allows me to cook more food than you'd think in one go in the basket. It crisps all kinds of foods up well and generally takes a lot of the guess work (and time) out of making a good meal. EcoFlow Black Friday deals — get up to 80 percent off: Portable power stations are an investment, but they can be crucial pieces of tech during emergencies. The top pick from our friends at Yahoo Tech has been heavily discounted in this early Black Friday sale. You can pick up the EcoFlow Delta Pro 3 for $1,400 off, down to $2,299, or the power station with an extra battery bundled in for $2,699 off, down to $3,599. Black Friday FAQs When is Black Friday 2025? Black Friday 2025 lands on November 28. Which stores have Black Friday deals? Many physical retail stores have Black Friday deals including Walmart, Target, Best Buy and others. Even more retailers have online Black Friday deals, including Amazon, GameStop, Costco and others. When do Black Friday sales start? Gone are the times when Black Friday sales were one-day-only affairs. Now, Black Friday deals are often available starting on Thanksgiving, or even earlier. Last year, we saw Black Friday deals online begin the week before Black Friday proper. When do Black Friday sales end? Black Friday and Cyber Monday have blended a lot over the past few years. Now, you can expect to see a good portion of Black Friday deals extend through the weekend and into Cyber Monday. It's not uncommon for Black Friday deals to expire at the end of Cyber Monday. Which retailers have the best Black Friday tech deals? The best Black Friday tech deals are typically available online at retailers like Amazon, Walmart, Best Buy and Target. It's also a good idea to check the store websites of the companies that make the products you want — for example, if you're looking for a Sonos speaker, check the Sonos website on Black Friday. Most of the time, you'll find the best Black Friday tech deals are matched at multiple retailers. Does Apple have Black Friday sales? No, you will usually not find Black Friday sales at Apple stores or on Apple's website. However, you can find Black Friday deals on Apple devices elsewhere; we recommend checking Amazon, Best Buy and other big retailers for discounts on iPads, Apple Watches and more on Black Friday. Does Amazon have Black Friday sales? Yes, Amazon has Black Friday sales. The online retailer's site will look similar to Prime Day on Black Friday, with discounts on all sorts of items from household essentials to fashion to tech.This article originally appeared on Engadget at https://www.engadget.com/deals/black-friday-deals-for-2025-are-here-early-we-found-the-best-tech-sales-from-apple-amazon-lego-anker-and-others-100052831.html?src=rss",
          "content": "Black Friday has become the time to buy the hottest tech of the year. Whether you're shopping for yourself or stocking up on gifts for the holidays, Black Friday deals are sure to bring the best prices of the year to things like headphones, game consoles, robot vacuums, phone accessories and everything in between. You don't even have to wait until Black Friday proper to save a ton of money. Over the past few years, we've seen Black Friday tech deals start earlier and earlier — to the point where the entire month of November is packed with discounts.If you're on the hunt for solid tech deals, Engadget has you covered. We've collected the best Black Friday deals on tech you can get right now, and we'll continue to update this post as we get closer to the big day at the end of November. Note that you probably have the best chance of snagging record-low prices when we get to about one week before Thanksgiving, but these deals available now are worth considering. Black Friday deals to shop now Apple AirPods 4 for $85 (34 percent off): These are the most affordable AirPods you can get, and the latest model has been substantially improved over the previous. They have a better fit and noticeably better sound quality than their predecessor, plus some advanced features previously only found on pricer models. Apple AirTags (four pack) for $65 (34 percent off): iPhone users who frequently misplace things should invest in a few AirTags. Slip them into your wallet, bag, jacket and other belongings to keep track of their locations in the Find My app. Just make sure that, if you're going to attach one to your keys, you also pick up an AirTag holder to go along with it. LEGO Star Wars Millennium Falcon A New Hope 25th Anniversary Collectable 75375 for $68 (20 percent off): This is a set that any Star Wars fan will love to build and then love to display once it's complete. The 921-piece set features a fully-detailed Millennium Falcone, buildable stand and nameplate. It's one of many Lego Black Friday deals you can get right now. Dyson 360 Vis Nav robot vacuum for $400 ($600 off): This is one of the best robot vacuums you can get, period. It doesn't have a self-emptying base, but its superior suction power almost makes up for that. It's one of the strongest robot vacuums I've ever tested, and it has excellent obstacle avoidance. The latter means you will rarely, if ever, have to attend to it getting caught on the edge of a carpet or getting stuck under a piece of furniture. If a cordless stick vacuum is what you're looking for, don't forget to check out all of the other Dyson Black Friday deals. Apple Watch SE 3 for $200 ($50 off): The SE has been our top pick for the best Apple Watch for those on a budget, and the latest model only solidifies that further. It has the same chipset found in the latest flagship Apple Watches, fast-charging capabilities, an always-on display and most of the same activity-tracking features you'll find in more expensive model. Apple Mac Mini M4 for $499 ($100 off): Desktop users looking for an upgrade should consider the latest Mac Mini, which runs on the M4 chip and 16GB of RAM as standard in the base configuration. This version has a smaller design that takes up less space, front-facing USB-C ports and a headphone jack, plus Thunderbolt 5 support. Jisulife Life 7 handheld fan for $25 (14 percent off): This handy little fan is a must-have if you life in a warm climate or have a tropical vacation planned anytime soon. It can be used as a table or handheld fan and even be worn around the neck so you don't have to hold it at all. Its 5,000 mAh battery allows it to last hours on a single charge, and the small display in the middle of the fan's blades show its remaining battery level. Leebin 2025 electric spin scrubber for $40 (43 percent off, Prime exclusive): This weird little scrubber makes cleaning my bathroom and shower much less of a pain. Just choose the brush head you need for your job and the rotating head takes care of most of the hard work. I love the adjustable handle, which extends from 12 to 50 inches so you can get into hard-to-reach places without breaking a sweat. Monarch Money budgeting app (one year) for $50 (50 percent off with code MONARCHVIP): One of our favorite budgeting apps, Monarch Money gives you a lot of control over the organization of your funds. There's a helpful goals feature for when you're planning out big purchases or financial milestones you want to hit, and we found the month-in-review recap it provides to be more thorough than other budgeting apps we tried. There's even Zillow integration for folks looking to buy a home. SanDisk microSD Express card (256GB) for $60 (12 percent off): If you have a Switch 2, no regular microSD card will do if you want to expand the console's storage. You need a newer microSD Express card, and currently there are only a handful on the market. We did some testing to find the best microSD Express card for the Switch 2 and found that performance was, in general, very similar amongst all the readily available cards. We recommend getting whichever fits within your budget at the capacity you want. Google TV Streamer 4K for $75 ($25 off): Our top pick for the best streaming device right now, the latest version of Google's streamer supports 4K video and an excellent, easy-to-use interface that will feel familiar to anyone who's seen a set with the Google TV technology built in. It provides access to all of the major streaming services including Netflix, Disney+, HBO Max, YouTube and more, plus it has a handy on-screen pop up that lets you control all compatible smart home devices right from your TV. Also available at Walmart. Cosori 9-in-1 air fryer for $90 (25 percent off): I personally have this air fryer, one of our top picks, in my house and I've used it for over a year with no issues. I love that it makes good use of vertical space so it doesn't take up too much space on my counter, and its rounded-square shape allows me to cook more food than you'd think in one go in the basket. It crisps all kinds of foods up well and generally takes a lot of the guess work (and time) out of making a good meal. EcoFlow Black Friday deals — get up to 80 percent off: Portable power stations are an investment, but they can be crucial pieces of tech during emergencies. The top pick from our friends at Yahoo Tech has been heavily discounted in this early Black Friday sale. You can pick up the EcoFlow Delta Pro 3 for $1,400 off, down to $2,299, or the power station with an extra battery bundled in for $2,699 off, down to $3,599. Black Friday FAQs When is Black Friday 2025? Black Friday 2025 lands on November 28. Which stores have Black Friday deals? Many physical retail stores have Black Friday deals including Walmart, Target, Best Buy and others. Even more retailers have online Black Friday deals, including Amazon, GameStop, Costco and others. When do Black Friday sales start? Gone are the times when Black Friday sales were one-day-only affairs. Now, Black Friday deals are often available starting on Thanksgiving, or even earlier. Last year, we saw Black Friday deals online begin the week before Black Friday proper. When do Black Friday sales end? Black Friday and Cyber Monday have blended a lot over the past few years. Now, you can expect to see a good portion of Black Friday deals extend through the weekend and into Cyber Monday. It's not uncommon for Black Friday deals to expire at the end of Cyber Monday. Which retailers have the best Black Friday tech deals? The best Black Friday tech deals are typically available online at retailers like Amazon, Walmart, Best Buy and Target. It's also a good idea to check the store websites of the companies that make the products you want — for example, if you're looking for a Sonos speaker, check the Sonos website on Black Friday. Most of the time, you'll find the best Black Friday tech deals are matched at multiple retailers. Does Apple have Black Friday sales? No, you will usually not find Black Friday sales at Apple stores or on Apple's website. However, you can find Black Friday deals on Apple devices elsewhere; we recommend checking Amazon, Best Buy and other big retailers for discounts on iPads, Apple Watches and more on Black Friday. Does Amazon have Black Friday sales? Yes, Amazon has Black Friday sales. The online retailer's site will look similar to Prime Day on Black Friday, with discounts on all sorts of items from household essentials to fashion to tech.This article originally appeared on Engadget at https://www.engadget.com/deals/black-friday-deals-for-2025-are-here-early-we-found-the-best-tech-sales-from-apple-amazon-lego-anker-and-others-100052831.html?src=rss",
          "feed_position": 15
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/black-friday-apple-deals-include-the-apple-watch-se-3-on-sale-for-200-133057960.html",
          "published_at": "Tue, 11 Nov 2025 14:45:38 +0000",
          "title": "Black Friday Apple deals include the Apple Watch SE 3 on sale for $200",
          "standfirst": "Black Friday will be here before you know it, but you can already save on some sought-after tech. Case in point: the new Apple Watch SE 3. Apple's most affordable smartwatch is even cheaper right now, down to just $200. We consider this to be the best budget Apple Watch, and arguably the best smartwatch for folks who have never owned one before. The latest version runs on the same chipset found in the new flagship models, and it has most of the same fitness and workout tracking features you'll find in those more expensive devices as well. The SE 3 also now has an always-on display, making it easier to glance down throughout the day to check the time or see activity stats without moving your wrist, and fast-charging support makes it a more viable sleep tracker. Just plop it down on its charger for a bit at the end of the day and put it back on to monitor your sleep overnight. Also discounted is the high-end Apple Watch Ultra 3, which you can snag for $100 off. The sale model comes with 64GB of storage, a 49mm screen and GPS and cellular service. Notably, it's also only available with the one size, adjustable band and in two colors: a Black titanium case with Black Ocean band and a natural titanium Case with Anchor Blue Ocean band. The Apple Watch Ultra 3 came out in early September and is one of the first smartwatches to support satellite communications. This feature means you can call, send messages or share your location with emergency services through the watch — even if you don't have a connection. The new Ultra 3 also has a larger screen thanks to thinner bezels and a battery that can last for up to 42 hours. This article originally appeared on Engadget at https://www.engadget.com/deals/black-friday-apple-deals-include-the-apple-watch-se-3-on-sale-for-200-133057960.html?src=rss",
          "content": "Black Friday will be here before you know it, but you can already save on some sought-after tech. Case in point: the new Apple Watch SE 3. Apple's most affordable smartwatch is even cheaper right now, down to just $200. We consider this to be the best budget Apple Watch, and arguably the best smartwatch for folks who have never owned one before. The latest version runs on the same chipset found in the new flagship models, and it has most of the same fitness and workout tracking features you'll find in those more expensive devices as well. The SE 3 also now has an always-on display, making it easier to glance down throughout the day to check the time or see activity stats without moving your wrist, and fast-charging support makes it a more viable sleep tracker. Just plop it down on its charger for a bit at the end of the day and put it back on to monitor your sleep overnight. Also discounted is the high-end Apple Watch Ultra 3, which you can snag for $100 off. The sale model comes with 64GB of storage, a 49mm screen and GPS and cellular service. Notably, it's also only available with the one size, adjustable band and in two colors: a Black titanium case with Black Ocean band and a natural titanium Case with Anchor Blue Ocean band. The Apple Watch Ultra 3 came out in early September and is one of the first smartwatches to support satellite communications. This feature means you can call, send messages or share your location with emergency services through the watch — even if you don't have a connection. The new Ultra 3 also has a larger screen thanks to thinner bezels and a battery that can last for up to 42 hours. This article originally appeared on Engadget at https://www.engadget.com/deals/black-friday-apple-deals-include-the-apple-watch-se-3-on-sale-for-200-133057960.html?src=rss",
          "feed_position": 17
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/lego-black-friday-deals-star-wars-and-disney-sets-are-up-to-37-percent-off-with-these-early-sales-155007667.html",
          "published_at": "Tue, 11 Nov 2025 14:30:36 +0000",
          "title": "Lego Black Friday deals: Star Wars and Disney sets are up to 37 percent off with these early sales",
          "standfirst": "Lego sets are probably at the top of your kid's wish list, and maybe they're at the top of your personal list, too. With so many to choose from, you'll be able to find one that makes a great gift for anyone who you know loves these little building bricks. Black Friday Lego deals are what to look for this time of year, because you can typically save at least 20 percent on a good number of sets. Yes, that often includes the most popular ones from the Star Wars, Super Mario, Harry Potter and other collections. In general, we always recommend using a price tracker when determining if a Lego deal is in fact a good one. Below, we've collected the best Lego Black Friday deals we could find right now. You'll find Lego deals across the board this holiday season at retailers like Amazon and Walmart, but don't overlook Lego's own site. If you join the free Lego Insiders program, you'll build up points with each purchase that you can redeem in the future, get special discounts and sometimes get exclusive gifts when you buy. While not a deal, arguably the hottest Lego for Black Friday will be the brand new Star Trek USS Enterprise set, which was announced recently. It has a whopping 3,600 pieces and will be a must-have for any Star Trek fans. The set will be available starting November 28 for $400. Best Lego Black Friday deals LEGO Disney Frozen Advent Calendar 2025 43273 for $31 (32 percent off) Lego Harry Potter Advent Calendar 2025 76456 for $39 (13 percent off) LEGO Star Wars Brick-Built Star Wars Logo 75407 for $48 (20 percent off) LEGO Star Wars Grogu with Hover Pram Building Toy Set 75403 for $63 (37 percent off) LEGO Star Wars Millennium Falcon A New Hope 25th Anniversary Collectable Model 75375 for $68 (20 percent off) LEGO Star Wars R2-D2 Building Toy Set 75379 for $80 (20 percent off) LEGO Harry Potter Hogwarts Castle and Grounds 76419 for $136 (20 percent off) LEGO Creator 3 in 1 Magical Unicorn Toy 31140 for $7 (32 percent off) LEGO City Donut Truck Toy 60452 for $16 (20 percent off) LEGO Speed Champions 2 Fast 2 Furious Nissan Skyline GT-R (R34) Race Car 76917 for $18 (28 percent off) LEGO Botanicals Happy Plants Building Toys 10349 for $20 (13 percent off) LEGO Botanicals Mini Orchid Building Set 10343 for $24 (20 percent off) LEGO Art Hokusai The Great Wave Framed Japanese Wall Art Building Set 31208 for $85 (15 percent off) This article originally appeared on Engadget at https://www.engadget.com/deals/lego-black-friday-deals-star-wars-and-disney-sets-are-up-to-37-percent-off-with-these-early-sales-155007667.html?src=rss",
          "content": "Lego sets are probably at the top of your kid's wish list, and maybe they're at the top of your personal list, too. With so many to choose from, you'll be able to find one that makes a great gift for anyone who you know loves these little building bricks. Black Friday Lego deals are what to look for this time of year, because you can typically save at least 20 percent on a good number of sets. Yes, that often includes the most popular ones from the Star Wars, Super Mario, Harry Potter and other collections. In general, we always recommend using a price tracker when determining if a Lego deal is in fact a good one. Below, we've collected the best Lego Black Friday deals we could find right now. You'll find Lego deals across the board this holiday season at retailers like Amazon and Walmart, but don't overlook Lego's own site. If you join the free Lego Insiders program, you'll build up points with each purchase that you can redeem in the future, get special discounts and sometimes get exclusive gifts when you buy. While not a deal, arguably the hottest Lego for Black Friday will be the brand new Star Trek USS Enterprise set, which was announced recently. It has a whopping 3,600 pieces and will be a must-have for any Star Trek fans. The set will be available starting November 28 for $400. Best Lego Black Friday deals LEGO Disney Frozen Advent Calendar 2025 43273 for $31 (32 percent off) Lego Harry Potter Advent Calendar 2025 76456 for $39 (13 percent off) LEGO Star Wars Brick-Built Star Wars Logo 75407 for $48 (20 percent off) LEGO Star Wars Grogu with Hover Pram Building Toy Set 75403 for $63 (37 percent off) LEGO Star Wars Millennium Falcon A New Hope 25th Anniversary Collectable Model 75375 for $68 (20 percent off) LEGO Star Wars R2-D2 Building Toy Set 75379 for $80 (20 percent off) LEGO Harry Potter Hogwarts Castle and Grounds 76419 for $136 (20 percent off) LEGO Creator 3 in 1 Magical Unicorn Toy 31140 for $7 (32 percent off) LEGO City Donut Truck Toy 60452 for $16 (20 percent off) LEGO Speed Champions 2 Fast 2 Furious Nissan Skyline GT-R (R34) Race Car 76917 for $18 (28 percent off) LEGO Botanicals Happy Plants Building Toys 10349 for $20 (13 percent off) LEGO Botanicals Mini Orchid Building Set 10343 for $24 (20 percent off) LEGO Art Hokusai The Great Wave Framed Japanese Wall Art Building Set 31208 for $85 (15 percent off) This article originally appeared on Engadget at https://www.engadget.com/deals/lego-black-friday-deals-star-wars-and-disney-sets-are-up-to-37-percent-off-with-these-early-sales-155007667.html?src=rss",
          "feed_position": 18
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/dyson-black-friday-deals-save-hundreds-on-cordless-and-robot-vacuums-173533593.html",
          "published_at": "Tue, 11 Nov 2025 14:15:41 +0000",
          "title": "Dyson Black Friday deals: Save hundreds on cordless and robot vacuums",
          "standfirst": "Dyson is holding an early Black Friday sale on vacuums and related products. Sure, Black Friday isn't for another month, but who are we to turn down a fantastic deal? To that end, the Dyson 360 Vis Nav robot vacuum is a whopping $600 off and down to $400 right now. That's $100 less than its previous all-time low and the cheapest we've seen it. Dyson was pretty late to the robot-vacuum party, but its entry was (and remains) one of the strongest in the category. It doesn't have a lot of bells and whistles like a self-emptying base or mopping capabilities, but it makes up for that by having probably the best suction power of any robovac we've tested. All kinds of debris will fall in its path: dirt, dust, food crumbs, pet hair and more. It also has excellent obstacle avoidance, so you'll rarely — if ever — have to dislodge it from getting stuck on the edge of a carpet or wedged in between furniture. Dyson's mobile app is easy to use as well, so if you're looking for a robot vacuum that does its main job incredibly well and you don't mind skipping on some extras, the 360 Vis Nav is a great option. Cordless vacuums are also a part of the sale. Take the Dyson V9 Motorbar cordless vacuum on sale for just $270, which is a discount of $330. That's more than half off. Dyson devices are all over our list of the best cordless vacuums, and for good reason. The company makes effective products. The V9 Motorbar has been designed to clean all floor types, in addition to upholstery. It's also been engineered to squeeze into tight spots, which is great for hitting those oft-neglected parts of the home. The suction power is on point and the battery lasts for 40 minutes before requiring a charge. That's just enough time to vacuum a standard-sized home if you don't stop for too many breaks. The V9 is getting a bit long-in-the-tooth. If you want a newer model, the V11 Extra is on sale for $400, which is a discount of $260. This one boosts the suction power and increases the battery life to 60 minutes. This article originally appeared on Engadget at https://www.engadget.com/deals/dyson-black-friday-deals-save-hundreds-on-cordless-and-robot-vacuums-173533593.html?src=rss",
          "content": "Dyson is holding an early Black Friday sale on vacuums and related products. Sure, Black Friday isn't for another month, but who are we to turn down a fantastic deal? To that end, the Dyson 360 Vis Nav robot vacuum is a whopping $600 off and down to $400 right now. That's $100 less than its previous all-time low and the cheapest we've seen it. Dyson was pretty late to the robot-vacuum party, but its entry was (and remains) one of the strongest in the category. It doesn't have a lot of bells and whistles like a self-emptying base or mopping capabilities, but it makes up for that by having probably the best suction power of any robovac we've tested. All kinds of debris will fall in its path: dirt, dust, food crumbs, pet hair and more. It also has excellent obstacle avoidance, so you'll rarely — if ever — have to dislodge it from getting stuck on the edge of a carpet or wedged in between furniture. Dyson's mobile app is easy to use as well, so if you're looking for a robot vacuum that does its main job incredibly well and you don't mind skipping on some extras, the 360 Vis Nav is a great option. Cordless vacuums are also a part of the sale. Take the Dyson V9 Motorbar cordless vacuum on sale for just $270, which is a discount of $330. That's more than half off. Dyson devices are all over our list of the best cordless vacuums, and for good reason. The company makes effective products. The V9 Motorbar has been designed to clean all floor types, in addition to upholstery. It's also been engineered to squeeze into tight spots, which is great for hitting those oft-neglected parts of the home. The suction power is on point and the battery lasts for 40 minutes before requiring a charge. That's just enough time to vacuum a standard-sized home if you don't stop for too many breaks. The V9 is getting a bit long-in-the-tooth. If you want a newer model, the V11 Extra is on sale for $400, which is a discount of $260. This one boosts the suction power and increases the battery life to 60 minutes. This article originally appeared on Engadget at https://www.engadget.com/deals/dyson-black-friday-deals-save-hundreds-on-cordless-and-robot-vacuums-173533593.html?src=rss",
          "feed_position": 19
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/this-256gb-switch-2-compatible-microsd-express-card-is-cheaper-than-ever-before-black-friday-151331728.html",
          "published_at": "Tue, 11 Nov 2025 14:15:35 +0000",
          "title": "This 256GB Switch 2-compatible microSD Express card is cheaper than ever before Black Friday",
          "standfirst": "The SanDisk 256GB microSD Express Card for the Switch 2 is down to $60 via Amazon, which is a record-low price. This model is also available in storage sizes of 128GB and 512GB, but neither version is on sale. This particular model easily made our list of the best microSD cards for the Nintendo Switch 2. It even made our list of the best Switch 2 accessories. It just gets the job done. We loved the speed on offer here. It was the fastest of all the cards we tested when transferring games and loading games. We also found that it performed admirably at just about every test we threw at it. The card was always consistently right near the top, thanks to outstanding sequential read and write performance. This was backed up by benchmark testing with PC tools like CrystalDiskMark. The Switch 2 only works with SD Express cards, so this covers that. Luckily, this card isn't just for Nintendo's latest console. It'll work with just about everything, if you ever find it outstays its usefulness as a storage container for Mario and friends. Elsewhere when it comes to microSD Express cards on sale: PNY's 128GB card is down to $40. This article originally appeared on Engadget at https://www.engadget.com/deals/this-256gb-switch-2-compatible-microsd-express-card-is-cheaper-than-ever-before-black-friday-151331728.html?src=rss",
          "content": "The SanDisk 256GB microSD Express Card for the Switch 2 is down to $60 via Amazon, which is a record-low price. This model is also available in storage sizes of 128GB and 512GB, but neither version is on sale. This particular model easily made our list of the best microSD cards for the Nintendo Switch 2. It even made our list of the best Switch 2 accessories. It just gets the job done. We loved the speed on offer here. It was the fastest of all the cards we tested when transferring games and loading games. We also found that it performed admirably at just about every test we threw at it. The card was always consistently right near the top, thanks to outstanding sequential read and write performance. This was backed up by benchmark testing with PC tools like CrystalDiskMark. The Switch 2 only works with SD Express cards, so this covers that. Luckily, this card isn't just for Nintendo's latest console. It'll work with just about everything, if you ever find it outstays its usefulness as a storage container for Mario and friends. Elsewhere when it comes to microSD Express cards on sale: PNY's 128GB card is down to $40. This article originally appeared on Engadget at https://www.engadget.com/deals/this-256gb-switch-2-compatible-microsd-express-card-is-cheaper-than-ever-before-black-friday-151331728.html?src=rss",
          "feed_position": 20
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/get-three-months-of-audible-for-3-in-this-black-friday-deal-140010983.html",
          "published_at": "Tue, 11 Nov 2025 14:00:10 +0000",
          "title": "Get three months of Audible for $3 in this Black Friday deal",
          "standfirst": "Amazon is selling three months of Audible for just $3 in honor of Black Friday. The company typically reserves this sale for Prime Day, so it's nice to see the deal make an unexpected return. This breaks down to $1 per month for the first three months, which is a boon for audiobook fans. Just make sure to cancel before the 90 days are up, as the subscription will auto-renew at $15 per month. That's not the worst deal in the world, given the vast number of titles available on the platform, but still. Audible has a diverse catalog that goes beyond audiobooks. It also hosts podcasts and Audible Originals. Subscribers get to choose one audiobook each month to keep in their collection for free, including best-sellers or new releases. Users also get unlimited access to the Plus Catalog, which houses thousands of audiobooks. Finally, active members get discounts on many audiobooks when looking to purchase. Winter is coming and this is a good way to make sure you have plenty to listen to throughout the next three months. This deal does have a time limit. It expires on December 16.This article originally appeared on Engadget at https://www.engadget.com/deals/get-three-months-of-audible-for-3-in-this-black-friday-deal-140010983.html?src=rss",
          "content": "Amazon is selling three months of Audible for just $3 in honor of Black Friday. The company typically reserves this sale for Prime Day, so it's nice to see the deal make an unexpected return. This breaks down to $1 per month for the first three months, which is a boon for audiobook fans. Just make sure to cancel before the 90 days are up, as the subscription will auto-renew at $15 per month. That's not the worst deal in the world, given the vast number of titles available on the platform, but still. Audible has a diverse catalog that goes beyond audiobooks. It also hosts podcasts and Audible Originals. Subscribers get to choose one audiobook each month to keep in their collection for free, including best-sellers or new releases. Users also get unlimited access to the Plus Catalog, which houses thousands of audiobooks. Finally, active members get discounts on many audiobooks when looking to purchase. Winter is coming and this is a good way to make sure you have plenty to listen to throughout the next three months. This deal does have a time limit. It expires on December 16.This article originally appeared on Engadget at https://www.engadget.com/deals/get-three-months-of-audible-for-3-in-this-black-friday-deal-140010983.html?src=rss",
          "feed_position": 21
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/apples-macbook-air-m4-hits-an-all-time-low-before-black-friday-183808288.html",
          "published_at": "Tue, 11 Nov 2025 13:51:01 +0000",
          "title": "Apple's MacBook Air M4 hits an all-time low before Black Friday",
          "standfirst": "The Apple MacBook Air M4 laptop has long hovered at $799 at Amazon — a full $200 below its Apple Store price. But it just dropped down to $749, which is the lowest price we've seen since this model was introduced in March. This sale is for the model with 16GB of RAM and 256GB of internal storage, across all four colorways, but the 512GB model is also down to $949 — another all-time low versus the Apple Store price of $1,199. We ranked this as our favorite Apple laptop in our list of the best MacBook computers. Heck, it's even our very favorite laptop. Full stop. The performance is exceptionally snappy, thanks to the M4 chip. We appreciated the upgraded battery life, which now lasts for around 18 hours per charge. That's well beyond a full day of work. The design is lightweight, but sturdy. This has become a hallmark for modern MacBook Air computers. The screen is both gorgeous and roomy, even though it's technically just a 13-inch panel. There's support for the P3 wide color gamut and it can reach up to 500 nits of brightness. This is a near-perfect laptop, but there are a couple of nitpicks. There's no USB-C port on the right side, limiting how users can arrange accessories on a desk. Also, the screen is capped with a 60Hz refresh rate. Another potential complication is the looming specter of the M5 chip. The company has already released the MacBook Pro M5, so a new MacBook Air is likely coming in the nearish future. (Read: sometime in early 2026). If you need more screen space, you'll find a similar discount on the 15-inch MacBook Air on Amazon, too. Most color options are $250 off and down to $949 for the base model (you guessed it — another all-time low). This article originally appeared on Engadget at https://www.engadget.com/deals/apples-macbook-air-m4-hits-an-all-time-low-before-black-friday-183808288.html?src=rss",
          "content": "The Apple MacBook Air M4 laptop has long hovered at $799 at Amazon — a full $200 below its Apple Store price. But it just dropped down to $749, which is the lowest price we've seen since this model was introduced in March. This sale is for the model with 16GB of RAM and 256GB of internal storage, across all four colorways, but the 512GB model is also down to $949 — another all-time low versus the Apple Store price of $1,199. We ranked this as our favorite Apple laptop in our list of the best MacBook computers. Heck, it's even our very favorite laptop. Full stop. The performance is exceptionally snappy, thanks to the M4 chip. We appreciated the upgraded battery life, which now lasts for around 18 hours per charge. That's well beyond a full day of work. The design is lightweight, but sturdy. This has become a hallmark for modern MacBook Air computers. The screen is both gorgeous and roomy, even though it's technically just a 13-inch panel. There's support for the P3 wide color gamut and it can reach up to 500 nits of brightness. This is a near-perfect laptop, but there are a couple of nitpicks. There's no USB-C port on the right side, limiting how users can arrange accessories on a desk. Also, the screen is capped with a 60Hz refresh rate. Another potential complication is the looming specter of the M5 chip. The company has already released the MacBook Pro M5, so a new MacBook Air is likely coming in the nearish future. (Read: sometime in early 2026). If you need more screen space, you'll find a similar discount on the 15-inch MacBook Air on Amazon, too. Most color options are $250 off and down to $949 for the base model (you guessed it — another all-time low). This article originally appeared on Engadget at https://www.engadget.com/deals/apples-macbook-air-m4-hits-an-all-time-low-before-black-friday-183808288.html?src=rss",
          "feed_position": 22
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/8bitdos-ultimate-controller-drops-to-a-record-low-price-in-early-black-friday-sales-133023804.html",
          "published_at": "Tue, 11 Nov 2025 13:30:23 +0000",
          "title": "8Bitdo's Ultimate Controller drops to a record low price in early Black Friday sales",
          "standfirst": "The holiday season is swiftly approaching and it's officially the time of year to ask: What presents am I going to get everyone? Thankfully, there are a ton of early Black Friday deals for the tech lover in your life. Take the 39 percent discount currently available on the 8Bitdo Ultimate Bluetooth Controller and Charging Dock. It's a great option for the gamer in your life, and a steal down from $70 to $43 — a new all-time low price. The controller comes with perks such as the charging dock, 22 hours of battery per charge and compatibility with everything from Steam Deck to Switch. The deal is only for the white model. Earlier this year, 8Bitdo released a new version of this $70 controller, aptly called the Ultimate 2 Bluetooth. We rated it as one of the best controllers for the Nintendo Switch 2. While the upgraded model brings you features like more precise and sensitive joysticks, this sale brings the original Ultimate Controller back into view — and our shopping carts. Plus, it also works well with the Nintendo Switch 2. This article originally appeared on Engadget at https://www.engadget.com/deals/8bitdos-ultimate-controller-drops-to-a-record-low-price-in-early-black-friday-sales-133023804.html?src=rss",
          "content": "The holiday season is swiftly approaching and it's officially the time of year to ask: What presents am I going to get everyone? Thankfully, there are a ton of early Black Friday deals for the tech lover in your life. Take the 39 percent discount currently available on the 8Bitdo Ultimate Bluetooth Controller and Charging Dock. It's a great option for the gamer in your life, and a steal down from $70 to $43 — a new all-time low price. The controller comes with perks such as the charging dock, 22 hours of battery per charge and compatibility with everything from Steam Deck to Switch. The deal is only for the white model. Earlier this year, 8Bitdo released a new version of this $70 controller, aptly called the Ultimate 2 Bluetooth. We rated it as one of the best controllers for the Nintendo Switch 2. While the upgraded model brings you features like more precise and sensitive joysticks, this sale brings the original Ultimate Controller back into view — and our shopping carts. Plus, it also works well with the Nintendo Switch 2. This article originally appeared on Engadget at https://www.engadget.com/deals/8bitdos-ultimate-controller-drops-to-a-record-low-price-in-early-black-friday-sales-133023804.html?src=rss",
          "feed_position": 23
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/gaming/the-12-best-retro-gaming-gifts-for-the-2025-holidays-140016618.html",
          "published_at": "Tue, 11 Nov 2025 13:01:27 +0000",
          "title": "The 12 best retro gaming gifts for the 2025 holidays",
          "standfirst": "The stream of new video games never ends, but for some of us, nothing beats the classics. If you don’t feel like hunting through eBay and local game shops for old cartridges to add to your loved one’s collection, we’ve picked out a few other gift ideas for the nostalgic gamer in your life — from video upscalers for old consoles to retro-themed books and artwork. Best retro gaming gifts FAQs Why do people buy retro games? Because they’re fun! Or because video game companies have generally had a spotty record of preserving their own history — and (legally) saving art, even in a minuscule way, is important. Or because, deep down, collectors just want to stave off the ceaseless march of time and hang onto any way to relive their youth before it dissipates for good. Or because they’re jaded with modern game design and crave shorter, more distinct or altogether different experiences that aren’t being served by today’s market. Or because they want to flip the games they collect for a quick buck on eBay. Or because… well, you get the idea. — J.D. Why is retro gaming so expensive? To put it simply: supply and demand. Companies aren’t making old games and consoles any more, yet a growing number of gaming enthusiasts want them. And as retro game collecting has grown more popular, sellers have become more acutely aware of how high they can price their goods. Not every retro game costs an arm and a leg, however: Popular games from relatively recent consoles are usually more affordable than lesser-selling titles for older hardware, and you can still find a good bargain every now and then by digging through local yard sales, individual eBay sellers and the like. — J.D. Are retro games a good investment? It depends on how you define “good.” Is it a good idea to buy a bunch of old games in the hopes that their value will skyrocket and make you a tidy profit? No, there’s little rhyme or reason to determining exactly which games will shoot up in value and by how much. There are much safer ways to invest if all you care about are financial returns. Is it a good idea to drop a bunch of cash on 40-year-old video games if you have pressing financial responsibilities? Probably not! But hey, it’s your life. If collecting retro games makes you happy, and you can budget for them within reason, that’s a good thing. Have fun. — J.D. What qualifies as a retro game? There’s no set definition for when a video game becomes “retro.” Personally, I think of it as any game that’s at least 10 years old and was originally released on a console that’s two or more generations old (or, for PC games, during that generation). But many others would stretch the timeline back farther, and the growing advent of “live service” games has complicated things. For instance, Grand Theft Auto V was released in 2013, while World of Warcraft arrived in 2004 — are those “retro games” when millions of people still play them today? Maybe not. With games from the ‘90s or earlier, though, the distinction is clearer. — J.D. Check out the rest of our gift ideas here.This article originally appeared on Engadget at https://www.engadget.com/gaming/the-12-best-retro-gaming-gifts-for-the-2025-holidays-140016618.html?src=rss",
          "content": "The stream of new video games never ends, but for some of us, nothing beats the classics. If you don’t feel like hunting through eBay and local game shops for old cartridges to add to your loved one’s collection, we’ve picked out a few other gift ideas for the nostalgic gamer in your life — from video upscalers for old consoles to retro-themed books and artwork. Best retro gaming gifts FAQs Why do people buy retro games? Because they’re fun! Or because video game companies have generally had a spotty record of preserving their own history — and (legally) saving art, even in a minuscule way, is important. Or because, deep down, collectors just want to stave off the ceaseless march of time and hang onto any way to relive their youth before it dissipates for good. Or because they’re jaded with modern game design and crave shorter, more distinct or altogether different experiences that aren’t being served by today’s market. Or because they want to flip the games they collect for a quick buck on eBay. Or because… well, you get the idea. — J.D. Why is retro gaming so expensive? To put it simply: supply and demand. Companies aren’t making old games and consoles any more, yet a growing number of gaming enthusiasts want them. And as retro game collecting has grown more popular, sellers have become more acutely aware of how high they can price their goods. Not every retro game costs an arm and a leg, however: Popular games from relatively recent consoles are usually more affordable than lesser-selling titles for older hardware, and you can still find a good bargain every now and then by digging through local yard sales, individual eBay sellers and the like. — J.D. Are retro games a good investment? It depends on how you define “good.” Is it a good idea to buy a bunch of old games in the hopes that their value will skyrocket and make you a tidy profit? No, there’s little rhyme or reason to determining exactly which games will shoot up in value and by how much. There are much safer ways to invest if all you care about are financial returns. Is it a good idea to drop a bunch of cash on 40-year-old video games if you have pressing financial responsibilities? Probably not! But hey, it’s your life. If collecting retro games makes you happy, and you can budget for them within reason, that’s a good thing. Have fun. — J.D. What qualifies as a retro game? There’s no set definition for when a video game becomes “retro.” Personally, I think of it as any game that’s at least 10 years old and was originally released on a console that’s two or more generations old (or, for PC games, during that generation). But many others would stretch the timeline back farther, and the growing advent of “live service” games has complicated things. For instance, Grand Theft Auto V was released in 2013, while World of Warcraft arrived in 2004 — are those “retro games” when millions of people still play them today? Maybe not. With games from the ‘90s or earlier, though, the distinction is clearer. — J.D. Check out the rest of our gift ideas here.This article originally appeared on Engadget at https://www.engadget.com/gaming/the-12-best-retro-gaming-gifts-for-the-2025-holidays-140016618.html?src=rss",
          "feed_position": 24
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/get-75-percent-off-proton-vpn-two-year-plans-before-black-friday-153737772.html",
          "published_at": "Tue, 11 Nov 2025 13:00:37 +0000",
          "title": "Get 75 percent off Proton VPN two-year plans before Black Friday",
          "standfirst": "A VPN subscription can make for a good holiday gift. Everyone could do with having a VPN to help protect their online activity from prying eyes (and, perhaps, access more things to watch from their favorite streaming services). It's something useful that a gift recipient may have never realized they needed. Of course, you may be looking for a great deal on a VPN yourself, and the Black Friday discount for Proton VPN is nothing to sniff at. You can get two years of access to the VPN Plus tier of the service for $59.76, which works out to $2.49 per month. That's a discount of 75 percent compared with the regular price of $10 per month. Overall, you'd save $180. Proton VPN is our pick for the best VPN overall because it checks all of the boxes it needs to. There is a free plan with unlimited data, but with that you can only connect to servers in a few countries and the connection might not be fast enough for you to watch anything from your preferred streaming service's library in that locale. The VPN Plus tier unlocks a lot more options, such as the ability to connect to 15,000 servers across more than 120 countries and simultaneous protection for up to 15 devices. The apps are well-designed — Proton has clients for Windows, Mac, iOS and Android — and it's easy to find a feature or setting you're looking for. In our testing, Proton VPN Plus had a relatively small impact on browsing speeds. Our download speeds dropped by 12 percent and uploads by 4 percent, while the global average ping remained below 300 ms (which is especially impressive if you're connecting to a server on the other side of the planet). Perhaps, most importantly, though, it's Proton's commitment to privacy that helps make its VPN an easy recommendation. There's a no-logs policy, meaning it does not log user activity or any identifiable characteristics of devices that connect to the VPN. Proton's servers use full-disk encryption to bolster privacy as well.This article originally appeared on Engadget at https://www.engadget.com/deals/get-75-percent-off-proton-vpn-two-year-plans-before-black-friday-153737772.html?src=rss",
          "content": "A VPN subscription can make for a good holiday gift. Everyone could do with having a VPN to help protect their online activity from prying eyes (and, perhaps, access more things to watch from their favorite streaming services). It's something useful that a gift recipient may have never realized they needed. Of course, you may be looking for a great deal on a VPN yourself, and the Black Friday discount for Proton VPN is nothing to sniff at. You can get two years of access to the VPN Plus tier of the service for $59.76, which works out to $2.49 per month. That's a discount of 75 percent compared with the regular price of $10 per month. Overall, you'd save $180. Proton VPN is our pick for the best VPN overall because it checks all of the boxes it needs to. There is a free plan with unlimited data, but with that you can only connect to servers in a few countries and the connection might not be fast enough for you to watch anything from your preferred streaming service's library in that locale. The VPN Plus tier unlocks a lot more options, such as the ability to connect to 15,000 servers across more than 120 countries and simultaneous protection for up to 15 devices. The apps are well-designed — Proton has clients for Windows, Mac, iOS and Android — and it's easy to find a feature or setting you're looking for. In our testing, Proton VPN Plus had a relatively small impact on browsing speeds. Our download speeds dropped by 12 percent and uploads by 4 percent, while the global average ping remained below 300 ms (which is especially impressive if you're connecting to a server on the other side of the planet). Perhaps, most importantly, though, it's Proton's commitment to privacy that helps make its VPN an easy recommendation. There's a no-logs policy, meaning it does not log user activity or any identifiable characteristics of devices that connect to the VPN. Proton's servers use full-disk encryption to bolster privacy as well.This article originally appeared on Engadget at https://www.engadget.com/deals/get-75-percent-off-proton-vpn-two-year-plans-before-black-friday-153737772.html?src=rss",
          "feed_position": 25
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/general/the-morning-after-engadget-newsletter-121537568.html",
          "published_at": "Tue, 11 Nov 2025 12:15:37 +0000",
          "title": "The Morning After: Is the Apple Watch SE 3 the best smartwatch for (almost) everyone?",
          "standfirst": "Apple’s entry-level smartwatch has improved so much it delivers an experience comparable to pricier Apple Watches. The most important upgrades in the SE 3 are the always-on display, faster charging speeds and on-device Siri. This brings a lot of parity to other Apple Watch devices in the most crucial areas. It doesn’t have everything: The SE 3’s optical sensor is older and lacks the capabilities of Series 10 and 11 smartwatches, which feature electrical heart rate sensors. You’ll still get notifications for irregular rhythms and low cardio fitness as well as high and low heart rate notifications. All of this for $259? (Or $200 if you’re quick.) It’s the most tempting first step into Apple’s wearable yet. It might be time for me to upgrade from my Series 8. — Mat Smith Get Engadget's newsletter delivered direct to your inbox. Subscribe right here! The news you might have missed Apple is reportedly working on more satellite features for iPhone Rodecaster Video S review: Compact and comprehensive video production Guillermo del Toro delivers a Frankenstein for the tech-bro era Nintendo announces its Black Friday deals Switch 2 bundles, Switch game deals and more. The Nintendo Switch 2 is the console launch of 2025, and it will undoubtedly be at the top of many wish lists for both kids and adults. If you were hoping to save a bit on the console during Black Friday, you may be disappointed. However, the marquee Nintendo deals for the holidays come as console bundles. When the Switch 2 launched earlier this year, it was available as just the console only for $449 or bundled with Mario Kart World for $499. Both options are still available, but there’s a new bundle to consider as well, the console with the new Pokémon Legends: Z-A game, which also costs $499. The Nintendo eShop will have cyber deals on games November 20 through December 3. The shop will feature “holiday offers on select games,” but we don’t know which games they will be. Meanwhile, other retailers will have discounts on some physical Switch games, including Princess Peach: Showtime!, The Legend of Zelda: Echoes of Wisdom, Luigi’s Mansion 3 and Kirby’s Return to Dream Land Deluxe. Those will each be $40, while other games like Super Mario Odyssey, Nintendo Switch Sports, Paper Mario: The Thousand-Year Door and Splatoon 3 will be $30. Continue reading. The latest Death Stranding collab is an actual exoskeleton Get those steps in even easier. Kojima Productions has a new collaboration for anyone who wishes to enhance their lower-body strength and stability — or simply prepare for cosplay in 2026. The studio has teamed up with exoskeleton maker Dnsys for a limited run of a model based on Death Stranding 2: On The Beach. It’s said to mimic the game’s load-balancing system by offloading up to 200 percent of the wearer’s body weight from their knees to help protect the joints. Dnsys said it adds 50 percent more power to steps and “intelligent gait control” to improve balance on stairs and uneven terrain. Continue reading. Renders show off Samsung’s upcoming Galaxy S26+ Maybe no S25 Edge successor. According to a report from Android Headlines, Samsung may stick with its Plus models of Galaxy phone — and ditch the just-got-here Edge line. A leaker has given us a glimpse at the forthcoming S26+, and it looks similar to the company’s other smartphones. The small, raised camera island has been tweaked a bit, and the report suggests the S26+ will be 7.35mm thick, which is in line with the S25+. It also seems to be decidedly chunkier than the S25 Edge, so this is likely not a swap: The next Plus device won’t be the Edge. Continue reading.This article originally appeared on Engadget at https://www.engadget.com/general/the-morning-after-engadget-newsletter-121537568.html?src=rss",
          "content": "Apple’s entry-level smartwatch has improved so much it delivers an experience comparable to pricier Apple Watches. The most important upgrades in the SE 3 are the always-on display, faster charging speeds and on-device Siri. This brings a lot of parity to other Apple Watch devices in the most crucial areas. It doesn’t have everything: The SE 3’s optical sensor is older and lacks the capabilities of Series 10 and 11 smartwatches, which feature electrical heart rate sensors. You’ll still get notifications for irregular rhythms and low cardio fitness as well as high and low heart rate notifications. All of this for $259? (Or $200 if you’re quick.) It’s the most tempting first step into Apple’s wearable yet. It might be time for me to upgrade from my Series 8. — Mat Smith Get Engadget's newsletter delivered direct to your inbox. Subscribe right here! The news you might have missed Apple is reportedly working on more satellite features for iPhone Rodecaster Video S review: Compact and comprehensive video production Guillermo del Toro delivers a Frankenstein for the tech-bro era Nintendo announces its Black Friday deals Switch 2 bundles, Switch game deals and more. The Nintendo Switch 2 is the console launch of 2025, and it will undoubtedly be at the top of many wish lists for both kids and adults. If you were hoping to save a bit on the console during Black Friday, you may be disappointed. However, the marquee Nintendo deals for the holidays come as console bundles. When the Switch 2 launched earlier this year, it was available as just the console only for $449 or bundled with Mario Kart World for $499. Both options are still available, but there’s a new bundle to consider as well, the console with the new Pokémon Legends: Z-A game, which also costs $499. The Nintendo eShop will have cyber deals on games November 20 through December 3. The shop will feature “holiday offers on select games,” but we don’t know which games they will be. Meanwhile, other retailers will have discounts on some physical Switch games, including Princess Peach: Showtime!, The Legend of Zelda: Echoes of Wisdom, Luigi’s Mansion 3 and Kirby’s Return to Dream Land Deluxe. Those will each be $40, while other games like Super Mario Odyssey, Nintendo Switch Sports, Paper Mario: The Thousand-Year Door and Splatoon 3 will be $30. Continue reading. The latest Death Stranding collab is an actual exoskeleton Get those steps in even easier. Kojima Productions has a new collaboration for anyone who wishes to enhance their lower-body strength and stability — or simply prepare for cosplay in 2026. The studio has teamed up with exoskeleton maker Dnsys for a limited run of a model based on Death Stranding 2: On The Beach. It’s said to mimic the game’s load-balancing system by offloading up to 200 percent of the wearer’s body weight from their knees to help protect the joints. Dnsys said it adds 50 percent more power to steps and “intelligent gait control” to improve balance on stairs and uneven terrain. Continue reading. Renders show off Samsung’s upcoming Galaxy S26+ Maybe no S25 Edge successor. According to a report from Android Headlines, Samsung may stick with its Plus models of Galaxy phone — and ditch the just-got-here Edge line. A leaker has given us a glimpse at the forthcoming S26+, and it looks similar to the company’s other smartphones. The small, raised camera island has been tweaked a bit, and the report suggests the S26+ will be 7.35mm thick, which is in line with the S25+. It also seems to be decidedly chunkier than the S25 Edge, so this is likely not a swap: The next Plus device won’t be the Edge. Continue reading.This article originally appeared on Engadget at https://www.engadget.com/general/the-morning-after-engadget-newsletter-121537568.html?src=rss",
          "feed_position": 28,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2025-11/080b1b31-bee1-11f0-b7fe-51929c1cc375"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/mobile/tablets/best-e-ink-tablet-130037939.html",
          "published_at": "Tue, 11 Nov 2025 10:00:36 +0000",
          "title": "The best E Ink tablets for 2025",
          "standfirst": "E Ink tablets have always been intriguing to me because I’m a longtime lover of pen and paper. I’ve had probably hundreds of notebooks over the years, serving as repositories for my story ideas, to-do lists, meeting notes and everything in between. However, I turned away from physical notebooks at a certain point because it was just easier to store everything digitally so I always had my most important information at my fingertips.E Ink tablets seem to provide the best of both worlds: the tactile satisfaction of regular notebooks with many of the conveniences found in digital tools, plus easy-on-the-eyes E Ink screens. These devices have come a long way in the past few years, and we’re just starting to see more color E Ink tablets become more widely available. I tested out a number of different E Ink tablets to see how well they work, how convenient they really are and which are the best tablets using E Ink screens available today.Editor's note: Amazon announced a revamped family of Kindle Scribe E Ink tablets in September 2025. The Kindle Scribe 3 is thinner and lighter than its predecessor with faster page-turning and writing experiences. The Kindle Scribe Colorsoft is the first full-color addition to the lineup, with a pen that will support writing in 10 colors and highlighting in five different shades. Both new Scribe tablets will be available in the US \"later this year.\" You can read our Kindle Scribe Colorsoft hands on to get a first look, but we'll update this guide once we've had the chance to test out both new E Ink tablets. Table of contents Best E Ink tablets for 2025 Are E Ink tablets worth it? What to look for in an E Ink tablet Other E Ink tablets we've tested Best E Ink tablets for 2025 Are E Ink tablets worth it? An E Ink tablet will be a worthwhile purchase to a very select group of people. If you prefer the look and feel of an e paper display to LCD panels found on traditional tablets, it makes a lot of sense. They’re also good options for those who want a more paper-like writing experience (although you can get that kind of functionality on a regular tablet with the right screen protector) or a more distraction-free device overall. The final note is key here. Many E Ink tablets don’t run on the same operating systems as regular tablets, so you’re automatically going to be limited in what you can do. And even with those that do allow you to download traditional apps like Chrome, Instagram and Facebook, E Ink tablets are not designed to give you the best casual-browsing experience. This is mostly due to the nature of E Ink displays, which have noticeable refreshes, a lack of vibrant colors and lower picture quality than the panels you’ll find on even the cheapest iPad. Arguably the biggest reason why you wouldn’t want to go with an iPad (all models of which support stylus input, a plethora of reading apps, etc) is because it’s much easier to get distracted by email, social media and other Internet-related temptations. What to look for in an E Ink tablet Writing and latency Arguably the most important thing to consider when looking for an E Ink tablet is the writing experience. How good it is will depend a lot on the display’s refresh rate (does it refresh after every time you put pen to “paper,” or at a different regular interval) and the stylus’ latency. Most of the tablets I’ve tested have little to no latency, but some are certainly better than others. Finally, you should double check before buying that your preferred E Ink tablet comes with a stylus, or if you need to purchase one separately. Reading How much will you be reading books, documents and other things on this tablet? E Ink tablets come in many sizes, but most of them tend to be larger than your standard e-reader because it makes writing much easier. Having a larger display isn’t a bad thing, but it might make holding it for long periods slightly more uncomfortable. (Most e-readers are roughly the size of a paperback book, giving you a similar feeling to analog reading). The supported file types for e-books can also make a big difference. It’s hard to make a blanket statement here because this varies so much among E Ink tablets. The TL;DR is that you’ll have a much better reading experience if you go with one made by a company that already has a history in e-book sales (i.e. Amazon or Kobo). All of the titles you bought via the Kindle or Kobo store should automatically be available to you on your Kindle or Kobo E Ink tablet. Also with Kindle titles, specifically, since they are protected by DRM, it’s not necessarily the best idea to try to bring those titles over to a third-party device. Unless the tablet runs an operating system like Android that supports downloads for apps like Kindle and Kobo, you’ll be limited to supported file types, like ePUB, PDF, MOBI, JPEG, PNG and others. Search functionality Most E Ink tablets have some on-device search features, but they can vary widely between models. You’ll want to consider how important it is to you to be able to search through all your handwritten notes and markups. I noticed in my testing that Amazon’s and Kobo’s E Ink tablets made it easy to refer back to notes made in books and files because they automatically save to the specific pages on which you took notes, made highlights and more. Searching is less standardized on E Ink tablets that have different supported file types, but their features can be quite powerful in their own right. For example, a few devices I tested supported text search in handwritten notes along with handwriting recognition, the latter of which allows you to translate your scribbles into typed text. Sharing and connectivity While we established that E Ink tablets can be great distraction-free devices, most manufacturers understand that your notes and doodles aren’t created in a vacuum. You may want to access them elsewhere, and that requires some form of connectivity. All of the E Ink tablets I tried have Wi-Fi support, and some support cloud syncing, companion mobile apps and the ability to export notes via email so you can access them elsewhere. None of them, however, integrate directly with a digital note taking system like Evernote or OneNote, so these devices will always be somewhat supplementary if you use apps like that, too. I’d argue that, if you already lean heavily on apps like OneNote, a standard tablet with a stylus and screen protector might be the best way to go. Ultimately, you should think about what you will want to do with the documents you’ll interact with on your E Ink tablet after the tablet portion is done. Price E Ink tablets aren’t known for being cheap. They generally fall into the $300-$800 price range, which is what you can expect to pay for a solid regular tablet, too. A key factor in price is size: cheaper devices with E Ink displays are likely to have smaller screens, and stylus support isn’t as much of a given. Also, those types of devices are generally considered e-readers because of their size and may not be the best for note-taking, doodling and the like. E Ink tablets have gone up in price recently. Supernote and Onyx Boox increased prices, as did reMarkable. The former said it was due to \"increased costs,” and a reMarkable representative confirmed this to Engadget and provided the following statement: \"We regularly review our pricing based on market conditions and operational costs. We've communicated an upcoming adjustment for the US market effective in May to provide transparency to our customers. Multiple factors influence our pricing decisions, including supply chain dynamics and overall operational costs in specific markets.” As a result, the reMarkable Paper Pro jumped from $579 to $629 (that's for the bundle with the standard Marker and no Folio). This isn't great, considering the Paper Pro was already on the expensive side of the spectrum for E Ink tablets. It's also worth noting that Supernote and Onyx Boox have raised prices in the past few months as well. Other E Ink tablets we've tested Onyx Boox Tab X C The Boox Tab X C is a color-screened version of the Tab X, the company’s all-purpose e-paper Android tablet. The Tab X C has a lovely 13.3-inch Kaleido 3 E Ink color display, an octa-core processor, 6GB of RAM and it runs on Android 13, making it one of the most powerful tablets in Boox’s lineup. I’ve used the Tab X in the past and this color version runs similarly, if not better, and at 5.3mm thick, it’s impressively svelte even when you pair it with its folio keyboard case. As someone who loves legal-pad sized things to write on, I also like how the Tab X C is most akin to A4-size paper. But at $820 for the bundle with the standard case (or a whopping $970 for the tablet and its keyboard case), it’s really only best for those who are ready to go all-in on a premium E Ink tablet. Lenovo Smart Paper Lenovo made a solid E Ink tablet in the Smart Paper, but it's too pricey and too married to the company's companion cloud service to warrant a spot on our top picks list. The hardware is great, but the software isn't as flexible as those of competitors like the reMarkable 2. It has good Google Drive integration, but you must pair it with Lenovo's cloud service to really get the most use out of it — and in the UK, the service costs £9 per month for three months, which is quite expensive. Onyx Boox Tab Ultra The Boox Tab Ultra has a lot of the same features we like in the Note Air 2 Plus, but it’s designed to be a true, all-purpose tablet with an E Ink screen. Running Android 11 and compatible with a magnetic keyboard case, you can use it like a standard 2-in-1 laptop, albeit a low-powered one. You can browse the web, check email and even watch YouTube videos on this thing — but that doesn’t mean you should. A standard 2-in-1 laptop with a more responsive screen and better overall performance would be a better fit for most people who even have the slightest desire to have an all-in-one device. Like the rest of Onyx’s devices, the Tab Ultra is specifically for those who put reading and eye comfort above all else.This article originally appeared on Engadget at https://www.engadget.com/mobile/tablets/best-e-ink-tablet-130037939.html?src=rss",
          "content": "E Ink tablets have always been intriguing to me because I’m a longtime lover of pen and paper. I’ve had probably hundreds of notebooks over the years, serving as repositories for my story ideas, to-do lists, meeting notes and everything in between. However, I turned away from physical notebooks at a certain point because it was just easier to store everything digitally so I always had my most important information at my fingertips.E Ink tablets seem to provide the best of both worlds: the tactile satisfaction of regular notebooks with many of the conveniences found in digital tools, plus easy-on-the-eyes E Ink screens. These devices have come a long way in the past few years, and we’re just starting to see more color E Ink tablets become more widely available. I tested out a number of different E Ink tablets to see how well they work, how convenient they really are and which are the best tablets using E Ink screens available today.Editor's note: Amazon announced a revamped family of Kindle Scribe E Ink tablets in September 2025. The Kindle Scribe 3 is thinner and lighter than its predecessor with faster page-turning and writing experiences. The Kindle Scribe Colorsoft is the first full-color addition to the lineup, with a pen that will support writing in 10 colors and highlighting in five different shades. Both new Scribe tablets will be available in the US \"later this year.\" You can read our Kindle Scribe Colorsoft hands on to get a first look, but we'll update this guide once we've had the chance to test out both new E Ink tablets. Table of contents Best E Ink tablets for 2025 Are E Ink tablets worth it? What to look for in an E Ink tablet Other E Ink tablets we've tested Best E Ink tablets for 2025 Are E Ink tablets worth it? An E Ink tablet will be a worthwhile purchase to a very select group of people. If you prefer the look and feel of an e paper display to LCD panels found on traditional tablets, it makes a lot of sense. They’re also good options for those who want a more paper-like writing experience (although you can get that kind of functionality on a regular tablet with the right screen protector) or a more distraction-free device overall. The final note is key here. Many E Ink tablets don’t run on the same operating systems as regular tablets, so you’re automatically going to be limited in what you can do. And even with those that do allow you to download traditional apps like Chrome, Instagram and Facebook, E Ink tablets are not designed to give you the best casual-browsing experience. This is mostly due to the nature of E Ink displays, which have noticeable refreshes, a lack of vibrant colors and lower picture quality than the panels you’ll find on even the cheapest iPad. Arguably the biggest reason why you wouldn’t want to go with an iPad (all models of which support stylus input, a plethora of reading apps, etc) is because it’s much easier to get distracted by email, social media and other Internet-related temptations. What to look for in an E Ink tablet Writing and latency Arguably the most important thing to consider when looking for an E Ink tablet is the writing experience. How good it is will depend a lot on the display’s refresh rate (does it refresh after every time you put pen to “paper,” or at a different regular interval) and the stylus’ latency. Most of the tablets I’ve tested have little to no latency, but some are certainly better than others. Finally, you should double check before buying that your preferred E Ink tablet comes with a stylus, or if you need to purchase one separately. Reading How much will you be reading books, documents and other things on this tablet? E Ink tablets come in many sizes, but most of them tend to be larger than your standard e-reader because it makes writing much easier. Having a larger display isn’t a bad thing, but it might make holding it for long periods slightly more uncomfortable. (Most e-readers are roughly the size of a paperback book, giving you a similar feeling to analog reading). The supported file types for e-books can also make a big difference. It’s hard to make a blanket statement here because this varies so much among E Ink tablets. The TL;DR is that you’ll have a much better reading experience if you go with one made by a company that already has a history in e-book sales (i.e. Amazon or Kobo). All of the titles you bought via the Kindle or Kobo store should automatically be available to you on your Kindle or Kobo E Ink tablet. Also with Kindle titles, specifically, since they are protected by DRM, it’s not necessarily the best idea to try to bring those titles over to a third-party device. Unless the tablet runs an operating system like Android that supports downloads for apps like Kindle and Kobo, you’ll be limited to supported file types, like ePUB, PDF, MOBI, JPEG, PNG and others. Search functionality Most E Ink tablets have some on-device search features, but they can vary widely between models. You’ll want to consider how important it is to you to be able to search through all your handwritten notes and markups. I noticed in my testing that Amazon’s and Kobo’s E Ink tablets made it easy to refer back to notes made in books and files because they automatically save to the specific pages on which you took notes, made highlights and more. Searching is less standardized on E Ink tablets that have different supported file types, but their features can be quite powerful in their own right. For example, a few devices I tested supported text search in handwritten notes along with handwriting recognition, the latter of which allows you to translate your scribbles into typed text. Sharing and connectivity While we established that E Ink tablets can be great distraction-free devices, most manufacturers understand that your notes and doodles aren’t created in a vacuum. You may want to access them elsewhere, and that requires some form of connectivity. All of the E Ink tablets I tried have Wi-Fi support, and some support cloud syncing, companion mobile apps and the ability to export notes via email so you can access them elsewhere. None of them, however, integrate directly with a digital note taking system like Evernote or OneNote, so these devices will always be somewhat supplementary if you use apps like that, too. I’d argue that, if you already lean heavily on apps like OneNote, a standard tablet with a stylus and screen protector might be the best way to go. Ultimately, you should think about what you will want to do with the documents you’ll interact with on your E Ink tablet after the tablet portion is done. Price E Ink tablets aren’t known for being cheap. They generally fall into the $300-$800 price range, which is what you can expect to pay for a solid regular tablet, too. A key factor in price is size: cheaper devices with E Ink displays are likely to have smaller screens, and stylus support isn’t as much of a given. Also, those types of devices are generally considered e-readers because of their size and may not be the best for note-taking, doodling and the like. E Ink tablets have gone up in price recently. Supernote and Onyx Boox increased prices, as did reMarkable. The former said it was due to \"increased costs,” and a reMarkable representative confirmed this to Engadget and provided the following statement: \"We regularly review our pricing based on market conditions and operational costs. We've communicated an upcoming adjustment for the US market effective in May to provide transparency to our customers. Multiple factors influence our pricing decisions, including supply chain dynamics and overall operational costs in specific markets.” As a result, the reMarkable Paper Pro jumped from $579 to $629 (that's for the bundle with the standard Marker and no Folio). This isn't great, considering the Paper Pro was already on the expensive side of the spectrum for E Ink tablets. It's also worth noting that Supernote and Onyx Boox have raised prices in the past few months as well. Other E Ink tablets we've tested Onyx Boox Tab X C The Boox Tab X C is a color-screened version of the Tab X, the company’s all-purpose e-paper Android tablet. The Tab X C has a lovely 13.3-inch Kaleido 3 E Ink color display, an octa-core processor, 6GB of RAM and it runs on Android 13, making it one of the most powerful tablets in Boox’s lineup. I’ve used the Tab X in the past and this color version runs similarly, if not better, and at 5.3mm thick, it’s impressively svelte even when you pair it with its folio keyboard case. As someone who loves legal-pad sized things to write on, I also like how the Tab X C is most akin to A4-size paper. But at $820 for the bundle with the standard case (or a whopping $970 for the tablet and its keyboard case), it’s really only best for those who are ready to go all-in on a premium E Ink tablet. Lenovo Smart Paper Lenovo made a solid E Ink tablet in the Smart Paper, but it's too pricey and too married to the company's companion cloud service to warrant a spot on our top picks list. The hardware is great, but the software isn't as flexible as those of competitors like the reMarkable 2. It has good Google Drive integration, but you must pair it with Lenovo's cloud service to really get the most use out of it — and in the UK, the service costs £9 per month for three months, which is quite expensive. Onyx Boox Tab Ultra The Boox Tab Ultra has a lot of the same features we like in the Note Air 2 Plus, but it’s designed to be a true, all-purpose tablet with an E Ink screen. Running Android 11 and compatible with a magnetic keyboard case, you can use it like a standard 2-in-1 laptop, albeit a low-powered one. You can browse the web, check email and even watch YouTube videos on this thing — but that doesn’t mean you should. A standard 2-in-1 laptop with a more responsive screen and better overall performance would be a better fit for most people who even have the slightest desire to have an all-in-one device. Like the rest of Onyx’s devices, the Tab Ultra is specifically for those who put reading and eye comfort above all else.This article originally appeared on Engadget at https://www.engadget.com/mobile/tablets/best-e-ink-tablet-130037939.html?src=rss",
          "feed_position": 31
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/general/rodecaster-video-s-review-compact-and-comprehensive-video-production-230000221.html",
          "published_at": "Mon, 10 Nov 2025 23:41:42 +0000",
          "title": "Rodecaster Video S review: Compact and comprehensive video production",
          "standfirst": "The Rodecaster series of podcast mixers have become a mainstay among audio creatives. Last year, the company expanded the line with its first multimedia model — the Rodecaster Video. It was a more niche proposition, aimed at streamers, video podcasters and live producers, and, at $1,200, people with deeper pockets. Today, Rode returns with the Rodecaster Video S (RCV-S), a streamlined version that offers much of the same functionality for less than half the cost of the original ($499). If you currently use Ecamm Live, OBS, a Stream Deck, a Rodecaster Pro/Duo, a video capture card or Blackmagic Atem type switcher, then there’s a good chance the RCV-S does something, or many things, useful to you. Maybe it can replace all those products, some of them or enhance your workflow. It’s a little complicated, but within that complication is a lot of versatility. With three HDMI inputs, one USB webcam/mic input and up to four NDI/wireless camera inputs, video is clearly the focus here. But there are also two XLR/Combo ports for studio microphones and instruments and the option to connect up to two of Rode’s wireless lav mics (such as the Wireless Go) directly without the need for a receiver. The RCV-S, like Blackmagic’s popular Atem Mini, outputs over HDMI at 1080p/60 (no 4K) or can be used over USB as a virtual camera just like you’d find in Zoom or Teams. Unlike the Rodecaster Pro or Duo, there are no faders; instead, there are two rows of pads for switching between video sources and scenes. There’s 20GB of onboard storage for media — such as overlays, graphics and audio/video clips — and you can record shows directly onto USB hard-drives or stream natively to Twitch, YouTube and other platforms via RTMP/S. There’s a lot going on, and how you want to use it all is largely up to you. While the RCV-S leans more toward “live” productions such as streaming or podcasts, you can use it for conventional YouTube-style video production, depending on your workflow or video preferences. Rodecaster Video versus Rodecaster Video S The huge price difference between the original Rodecaster Video and the new S model might lead you to expect some serious feature trimming this time around, but Rode’s garden shears have been fairly kind. Most notably, the number of sources you can connect has been reduced from six to four. Meanwhile, the number of “scenes” you can create is now five, down from seven. As mentioned above, there are three HDMI inputs, not four, and only one USB webcam/mic input, down from two on the original. There are still nine channels of audio with two headphone outputs, but the line outputs on the original are no longer here. The SD card slot for media has also been removed in favor of internal storage. In short, there’s a little bit less connectivity, but not enough for it to feel hobbled, especially for the more casual users this is clearly aimed at. In-use At its most basic, the RCV-S is an all-in-one video switcher. You don’t need to connect it to a PC, just plug in your video and audio sources and you can jump between them in real time with a choice of transitions. For a more polished production you can create “scenes” via the menu with a selection of layouts for multiple cameras, picture-in-picture and so on. That could be two talking heads for a podcast, overlaying your web camera on top of your gameplay or a top-down camera mixed with a different perspective for cooking tutorials or presentations. You can create scenes directly on the device in advance or live via templates, but things get a lot easier and more creative when you use Rode’s companion Rodecaster app. Here, you’ll find a “scene creator” tool that’s both visual and intuitive, plus it has the option to make custom layouts where you can freely resize and place all your different media wherever you want, add borders or rounded corners and so on. While the main pads are primarily for switching between sources and scenes, they can also trigger media — pre-made video clips, overlays and graphics, for example. You can assign media to them without reducing the number of sources/scenes that are available to switch between, just tap the Media/Overlay button to temporarily change the functionality of the pads. You can also set up chromakey/green screening directly on the device or via the Rodecaster App, too. Once you have everything set up, you can directly record output onto a USB SSD, again, without the need for a PC. There’s even the option to record only the main output (“program”) or a multi-track version which will export a recording of every input as a separate file that you can edit as you see fit. This is helpful if you’re using apps like Adobe Premier or Davinci Resolve. If you shoot multi-camera dance tutorials, for example, you can jump between your main and wide camera and then to a close up without having to hunt-down the right part from each file you recorded on different memory cards. Rode recently updated the Rodecaster Video firmware to add support for up to four “NDI” wireless cameras as additional sources. Typically these are remote or security-style cameras, but Rode’s Capture mobile app also supports NDI streaming, meaning you can use your phone wirelessly as another camera that you can switch to, which is particularly useful. Though do note, the Rodecaster Video will need to be hard-wired to your router on the same network as your phone for this to work. Audio The Rodecaster Video S handles audio and video with ease. James Trew for Engadget While the RCV-S offers nine audio tracks, it’s fair to say it isn’t really designed for live audio mixing in the pure sense since there are no faders. Instead, you have to access different channels via the menu on a small display and a dial — It’s a lot of hunting and pecking. If you’re connected to a PC, the Rodecaster app does offer a software mixer with faders though, which makes adjusting levels on the fly much easier. The good news is that some voice enhancement tools, such as Depth, Sparkle and Punch have been carried over from the Rodecaster Pro/Duo, giving you a little more fine-grained control over how you or your guests sound. There’s a slight drawback when using the RCV-S as a standalone unit, in that you need to physically push the buttons to change the scene, which might not be ideal if you’re trying to make a polished recording or can’t always be near the console. Thankfully, Rode has a solution in “auto switching.” I’m mentioning it here, in the audio section, as the RCV-S can switch cameras based on which one has the strongest audio or based on user-defined priority. Typically, that would be whoever is talking in a podcast, but it could also be in-game sound or when you switch to playing a musical instrument. Auto switching works well, but it’s not quite dependable enough to rely on for full autonomy in a professional environment — say while recording a panel at a conference — but it solves a problem if you’re on your own, and it could at least save some time in a casual podcast situation. Things to consider The Rodecaster Video S (top) vs. The Rodecaster Video (bottom) James Trew for Engadget It’s clear that the RCV-S, despite having fewer inputs than the original Rodecaster Video, is still quite a complex, open-ended tool. If you already have a streaming setup you like and the software and interfaces you need, then the appeal of the RCV-S will be down to whether it can do what you need more efficiently. For live video production, it’s an easier sell, as there aren’t many all-in-one devices to compete with it that can be used standalone. For example, Blackmagic’s Atem series is incredibly popular, and until now, had the advantage on price with the Mini Pro costing just $330. Even the Mini Pro ISO ($550) that exports multi-track video was half the price of the original Rodecaster Video. The RCV-S now offers comparable connectivity, onboard recording, more expansive audio features and multi-track export for $50 less. For general content creators it’s a little trickier. As someone that makes YouTube videos, the appeal to me is the chance to consolidate a few devices into one and remove some friction in my recordings. Right now, I have a Rodecaster Duo handling my audio, a capture card for my main camera and I swap out that camera for a smaller one if I am shooting top-down footage or need a second angle. That means I have several different devices on my desk, and I’m constantly doing a dance of unplugging things and juggling media or different recordings before I get everything ready to edit. The Rodecaster Video S has a tiny display for menus and settings. James Trew for Engadget With the RCV-S I can ditch my standalone capture card and permanently leave multiple cameras connected to the Rodecaster so that they are ready to go at any time. I can even remove the Rodecaster Duo and bring it back out again on the occasions I need more immediate control over multiple sources of audio (which is sometimes, but not often). The appeal in my case is fewer devices on my desk, and the ability to record multi-camera video without having to set up every shot, every single time, which saves significant time that I can then use to actually get more work done. There are, of course, some limitations. Not least of all is the lack of 4K. I’m still inclined to record on camera for my primary shot to ensure I still have a 4K copy for YouTube and then use the Rodecaster Video S for everything else, but as a small creator, convenience and flexibility is very appealing. For streamers and live video production, the Rodecaster Video S is a very capable tool that offers a wide range of functionality for an accessible price that will no doubt become the central hub for many creators. The real kicker here is the price. Let’s be clear, $500 is still significant money. But at less than half the price of the original Rodecaster with decent connectivity and basically the same functionality, it's an easy recommendation to those who were holding off based on price alone. Likewise, if you’re just starting out with content creation and need something with solid video credentials and audio chops, Rode makes a good case for itself with the Rodecaster Video S.This article originally appeared on Engadget at https://www.engadget.com/general/rodecaster-video-s-review-compact-and-comprehensive-video-production-230000221.html?src=rss",
          "content": "The Rodecaster series of podcast mixers have become a mainstay among audio creatives. Last year, the company expanded the line with its first multimedia model — the Rodecaster Video. It was a more niche proposition, aimed at streamers, video podcasters and live producers, and, at $1,200, people with deeper pockets. Today, Rode returns with the Rodecaster Video S (RCV-S), a streamlined version that offers much of the same functionality for less than half the cost of the original ($499). If you currently use Ecamm Live, OBS, a Stream Deck, a Rodecaster Pro/Duo, a video capture card or Blackmagic Atem type switcher, then there’s a good chance the RCV-S does something, or many things, useful to you. Maybe it can replace all those products, some of them or enhance your workflow. It’s a little complicated, but within that complication is a lot of versatility. With three HDMI inputs, one USB webcam/mic input and up to four NDI/wireless camera inputs, video is clearly the focus here. But there are also two XLR/Combo ports for studio microphones and instruments and the option to connect up to two of Rode’s wireless lav mics (such as the Wireless Go) directly without the need for a receiver. The RCV-S, like Blackmagic’s popular Atem Mini, outputs over HDMI at 1080p/60 (no 4K) or can be used over USB as a virtual camera just like you’d find in Zoom or Teams. Unlike the Rodecaster Pro or Duo, there are no faders; instead, there are two rows of pads for switching between video sources and scenes. There’s 20GB of onboard storage for media — such as overlays, graphics and audio/video clips — and you can record shows directly onto USB hard-drives or stream natively to Twitch, YouTube and other platforms via RTMP/S. There’s a lot going on, and how you want to use it all is largely up to you. While the RCV-S leans more toward “live” productions such as streaming or podcasts, you can use it for conventional YouTube-style video production, depending on your workflow or video preferences. Rodecaster Video versus Rodecaster Video S The huge price difference between the original Rodecaster Video and the new S model might lead you to expect some serious feature trimming this time around, but Rode’s garden shears have been fairly kind. Most notably, the number of sources you can connect has been reduced from six to four. Meanwhile, the number of “scenes” you can create is now five, down from seven. As mentioned above, there are three HDMI inputs, not four, and only one USB webcam/mic input, down from two on the original. There are still nine channels of audio with two headphone outputs, but the line outputs on the original are no longer here. The SD card slot for media has also been removed in favor of internal storage. In short, there’s a little bit less connectivity, but not enough for it to feel hobbled, especially for the more casual users this is clearly aimed at. In-use At its most basic, the RCV-S is an all-in-one video switcher. You don’t need to connect it to a PC, just plug in your video and audio sources and you can jump between them in real time with a choice of transitions. For a more polished production you can create “scenes” via the menu with a selection of layouts for multiple cameras, picture-in-picture and so on. That could be two talking heads for a podcast, overlaying your web camera on top of your gameplay or a top-down camera mixed with a different perspective for cooking tutorials or presentations. You can create scenes directly on the device in advance or live via templates, but things get a lot easier and more creative when you use Rode’s companion Rodecaster app. Here, you’ll find a “scene creator” tool that’s both visual and intuitive, plus it has the option to make custom layouts where you can freely resize and place all your different media wherever you want, add borders or rounded corners and so on. While the main pads are primarily for switching between sources and scenes, they can also trigger media — pre-made video clips, overlays and graphics, for example. You can assign media to them without reducing the number of sources/scenes that are available to switch between, just tap the Media/Overlay button to temporarily change the functionality of the pads. You can also set up chromakey/green screening directly on the device or via the Rodecaster App, too. Once you have everything set up, you can directly record output onto a USB SSD, again, without the need for a PC. There’s even the option to record only the main output (“program”) or a multi-track version which will export a recording of every input as a separate file that you can edit as you see fit. This is helpful if you’re using apps like Adobe Premier or Davinci Resolve. If you shoot multi-camera dance tutorials, for example, you can jump between your main and wide camera and then to a close up without having to hunt-down the right part from each file you recorded on different memory cards. Rode recently updated the Rodecaster Video firmware to add support for up to four “NDI” wireless cameras as additional sources. Typically these are remote or security-style cameras, but Rode’s Capture mobile app also supports NDI streaming, meaning you can use your phone wirelessly as another camera that you can switch to, which is particularly useful. Though do note, the Rodecaster Video will need to be hard-wired to your router on the same network as your phone for this to work. Audio The Rodecaster Video S handles audio and video with ease. James Trew for Engadget While the RCV-S offers nine audio tracks, it’s fair to say it isn’t really designed for live audio mixing in the pure sense since there are no faders. Instead, you have to access different channels via the menu on a small display and a dial — It’s a lot of hunting and pecking. If you’re connected to a PC, the Rodecaster app does offer a software mixer with faders though, which makes adjusting levels on the fly much easier. The good news is that some voice enhancement tools, such as Depth, Sparkle and Punch have been carried over from the Rodecaster Pro/Duo, giving you a little more fine-grained control over how you or your guests sound. There’s a slight drawback when using the RCV-S as a standalone unit, in that you need to physically push the buttons to change the scene, which might not be ideal if you’re trying to make a polished recording or can’t always be near the console. Thankfully, Rode has a solution in “auto switching.” I’m mentioning it here, in the audio section, as the RCV-S can switch cameras based on which one has the strongest audio or based on user-defined priority. Typically, that would be whoever is talking in a podcast, but it could also be in-game sound or when you switch to playing a musical instrument. Auto switching works well, but it’s not quite dependable enough to rely on for full autonomy in a professional environment — say while recording a panel at a conference — but it solves a problem if you’re on your own, and it could at least save some time in a casual podcast situation. Things to consider The Rodecaster Video S (top) vs. The Rodecaster Video (bottom) James Trew for Engadget It’s clear that the RCV-S, despite having fewer inputs than the original Rodecaster Video, is still quite a complex, open-ended tool. If you already have a streaming setup you like and the software and interfaces you need, then the appeal of the RCV-S will be down to whether it can do what you need more efficiently. For live video production, it’s an easier sell, as there aren’t many all-in-one devices to compete with it that can be used standalone. For example, Blackmagic’s Atem series is incredibly popular, and until now, had the advantage on price with the Mini Pro costing just $330. Even the Mini Pro ISO ($550) that exports multi-track video was half the price of the original Rodecaster Video. The RCV-S now offers comparable connectivity, onboard recording, more expansive audio features and multi-track export for $50 less. For general content creators it’s a little trickier. As someone that makes YouTube videos, the appeal to me is the chance to consolidate a few devices into one and remove some friction in my recordings. Right now, I have a Rodecaster Duo handling my audio, a capture card for my main camera and I swap out that camera for a smaller one if I am shooting top-down footage or need a second angle. That means I have several different devices on my desk, and I’m constantly doing a dance of unplugging things and juggling media or different recordings before I get everything ready to edit. The Rodecaster Video S has a tiny display for menus and settings. James Trew for Engadget With the RCV-S I can ditch my standalone capture card and permanently leave multiple cameras connected to the Rodecaster so that they are ready to go at any time. I can even remove the Rodecaster Duo and bring it back out again on the occasions I need more immediate control over multiple sources of audio (which is sometimes, but not often). The appeal in my case is fewer devices on my desk, and the ability to record multi-camera video without having to set up every shot, every single time, which saves significant time that I can then use to actually get more work done. There are, of course, some limitations. Not least of all is the lack of 4K. I’m still inclined to record on camera for my primary shot to ensure I still have a 4K copy for YouTube and then use the Rodecaster Video S for everything else, but as a small creator, convenience and flexibility is very appealing. For streamers and live video production, the Rodecaster Video S is a very capable tool that offers a wide range of functionality for an accessible price that will no doubt become the central hub for many creators. The real kicker here is the price. Let’s be clear, $500 is still significant money. But at less than half the price of the original Rodecaster with decent connectivity and basically the same functionality, it's an easy recommendation to those who were holding off based on price alone. Likewise, if you’re just starting out with content creation and need something with solid video credentials and audio chops, Rode makes a good case for itself with the Rodecaster Video S.This article originally appeared on Engadget at https://www.engadget.com/general/rodecaster-video-s-review-compact-and-comprehensive-video-production-230000221.html?src=rss",
          "feed_position": 33,
          "image_url": "https://d29szjachogqwa.cloudfront.net/images/user-uploaded/DSCF1856.jpg"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/meta-returns-to-open-source-ai-with-omnilingual-asr-models-that-can",
          "published_at": "Mon, 10 Nov 2025 20:27:00 GMT",
          "title": "Meta returns to open source AI with Omnilingual ASR models that can transcribe 1,600+ languages natively",
          "standfirst": "Meta has just released a new multilingual automatic speech recognition (ASR) system supporting 1,600+ languages — dwarfing OpenAI’s open source Whisper model, which supports just 99. Is architecture also allows developers to extend that support to thousands more. Through a feature called zero-shot in-context learning, users can provide a few paired examples of audio and text in a new language at inference time, enabling the model to transcribe additional utterances in that language without any retraining.In practice, this expands potential coverage to more than 5,400 languages — roughly every spoken language with a known script.It’s a shift from static model capabilities to a flexible framework that communities can adapt themselves. So while the 1,600 languages reflect official training coverage, the broader figure represents Omnilingual ASR’s capacity to generalize on demand, making it the most extensible speech recognition system released to date.Best of all: it&#x27;s been open sourced under a plain Apache 2.0 license — not a restrictive, quasi open-source Llama license like the company&#x27;s prior releases, which limited use by larger enterprises unless they paid licensing fees — meaning researchers and developers are free to take and implement it right away, for free, without restrictions, even in commercial and enterprise-grade projects!Released on November 10 on Meta&#x27;s website, Github, along with a demo space on Hugging Face and technical paper, Meta’s Omnilingual ASR suite includes a family of speech recognition models, a 7-billion parameter multilingual audio representation model, and a massive speech corpus spanning over 350 previously underserved languages. All resources are freely available under open licenses, and the models support speech-to-text transcription out of the box.“By open sourcing these models and dataset, we aim to break down language barriers, expand digital access, and empower communities worldwide,” Meta posted on its @AIatMeta account on XDesigned for Speech-to-Text TranscriptionAt its core, Omnilingual ASR is a speech-to-text system. The models are trained to convert spoken language into written text, supporting applications like voice assistants, transcription tools, subtitles, oral archive digitization, and accessibility features for low-resource languages.Unlike earlier ASR models that required extensive labeled training data, Omnilingual ASR includes a zero-shot variant. This version can transcribe languages it has never seen before—using just a few paired examples of audio and corresponding text. This lowers the barrier for adding new or endangered languages dramatically, removing the need for large corpora or retraining.Model Family and Technical DesignThe Omnilingual ASR suite includes multiple model families trained on more than 4.3 million hours of audio from 1,600+ languages:wav2vec 2.0 models for self-supervised speech representation learning (300M–7B parameters)CTC-based ASR models for efficient supervised transcriptionLLM-ASR models combining a speech encoder with a Transformer-based text decoder for state-of-the-art transcriptionLLM-ZeroShot ASR model, enabling inference-time adaptation to unseen languagesAll models follow an encoder–decoder design: raw audio is converted into a language-agnostic representation, then decoded into written text.Why the Scale MattersWhile Whisper and similar models have advanced ASR capabilities for global languages, they fall short on the long tail of human linguistic diversity. Whisper supports 99 languages. Meta’s system:Directly supports 1,600+ languagesCan generalize to 5,400+ languages using in-context learningAchieves character error rates (CER) under 10% in 78% of supported languagesAmong those supported are more than 500 languages never previously covered by any ASR model, according to Meta’s research paper.This expansion opens new possibilities for communities whose languages are often excluded from digital toolsHere’s the revised and expanded background section, integrating the broader context of Meta’s 2025 AI strategy, leadership changes, and Llama 4’s reception, complete with in-text citations and links:Background: Meta’s AI Overhaul and a Rebound from Llama 4The release of Omnilingual ASR arrives at a pivotal moment in Meta’s AI strategy, following a year marked by organizational turbulence, leadership changes, and uneven product execution. Omnilingual ASR is the first major open-source model release since the rollout of Llama 4, Meta’s latest large language model, which debuted in April 2025 to mixed and ultimately poor reviews, with scant enterprise adoption compared to Chinese open source model competitors.The failure led Meta founder and CEO Mark Zuckerberg to appoint Alexandr Wang, co-founder and prior CEO of AI data supplier Scale AI, as Chief AI Officer, and embark on an extensive and costly hiring spree that shocked the AI and business communities with eye-watering pay packages for top AI researchers.In contrast, Omnilingual ASR represents a strategic and reputational reset. It returns Meta to a domain where the company has historically led — multilingual AI — and offers a truly extensible, community-oriented stack with minimal barriers to entry. The system’s support for 1,600+ languages and its extensibility to over 5,000 more via zero-shot in-context learning reassert Meta’s engineering credibility in language technology. Importantly, it does so through a free and permissively licensed release, under Apache 2.0, with transparent dataset sourcing and reproducible training protocols.This shift aligns with broader themes in Meta’s 2025 strategy. The company has refocused its narrative around a “personal superintelligence” vision, investing heavily in infrastructure (including a September release of custom AI accelerators and Arm-based inference stacks) source while downplaying the metaverse in favor of foundational AI capabilities. The return to public training data in Europe after a regulatory pause also underscores its intention to compete globally, despite privacy scrutiny source.Omnilingual ASR, then, is more than a model release — it’s a calculated move to reassert control of the narrative: from the fragmented rollout of Llama 4 to a high-utility, research-grounded contribution that aligns with Meta’s long-term AI platform strategy.Community-Centered Dataset CollectionTo achieve this scale, Meta partnered with researchers and community organizations in Africa, Asia, and elsewhere to create the Omnilingual ASR Corpus, a 3,350-hour dataset across 348 low-resource languages. Contributors were compensated local speakers, and recordings were gathered in collaboration with groups like:African Next Voices: A Gates Foundation–supported consortium including Maseno University (Kenya), University of Pretoria, and Data Science NigeriaMozilla Foundation’s Common Voice, supported through the Open Multilingual Speech FundLanfrica / NaijaVoices, which created data for 11 African languages including Igala, Serer, and UrhoboThe data collection focused on natural, unscripted speech. Prompts were designed to be culturally relevant and open-ended, such as “Is it better to have a few close friends or many casual acquaintances? Why?” Transcriptions used established writing systems, with quality assurance built into every step.Performance and Hardware ConsiderationsThe largest model in the suite, the omniASR_LLM_7B, requires ~17GB of GPU memory for inference, making it suitable for deployment on high-end hardware. Smaller models (300M–1B) can run on lower-power devices and deliver real-time transcription speeds.Performance benchmarks show strong results even in low-resource scenarios:CER <10% in 95% of high-resource and mid-resource languagesCER <10% in 36% of low-resource languagesRobustness in noisy conditions and unseen domains, especially with fine-tuningThe zero-shot system, omniASR_LLM_7B_ZS, can transcribe new languages with minimal setup. Users provide a few sample audio–text pairs, and the model generates transcriptions for new utterances in the same language.Open Access and Developer ToolingAll models and the dataset are licensed under permissive terms:Apache 2.0 for models and codeCC-BY 4.0 for the Omnilingual ASR Corpus on HuggingFaceInstallation is supported via PyPI and uv:pip install omnilingual-asrMeta also provides:A HuggingFace dataset integrationPre-built inference pipelinesLanguage-code conditioning for improved accuracyDevelopers can view the full list of supported languages using the API:from omnilingual_asr.models.wav2vec2_llama.lang_ids import supported_langsprint(len(supported_langs)) print(supported_langs)Broader ImplicationsOmnilingual ASR reframes language coverage in ASR from a fixed list to an extensible framework. It enables:Community-driven inclusion of underrepresented languagesDigital access for oral and endangered languagesResearch on speech tech in linguistically diverse contextsCrucially, Meta emphasizes ethical considerations throughout—advocating for open-source participation and collaboration with native-speaking communities.“No model can ever anticipate and include all of the world’s languages in advance,” the Omnilingual ASR paper states, “but Omnilingual ASR makes it possible for communities to extend recognition with their own data.”Access the ToolsAll resources are now available at:Code + Models: github.com/facebookresearch/omnilingual-asrDataset: huggingface.co/datasets/facebook/omnilingual-asr-corpusBlogpost: ai.meta.com/blog/omnilingual-asrWhat This Means for EnterprisesFor enterprise developers, especially those operating in multilingual or international markets, Omnilingual ASR significantly lowers the barrier to deploying speech-to-text systems across a broader range of customers and geographies. Instead of relying on commercial ASR APIs that support only a narrow set of high-resource languages, teams can now integrate an open-source pipeline that covers over 1,600 languages out of the box—with the option to extend it to thousands more via zero-shot learning.This flexibility is especially valuable for enterprises working in sectors like voice-based customer support, transcription services, accessibility, education, or civic technology, where local language coverage can be a competitive or regulatory necessity. Because the models are released under the permissive Apache 2.0 license, businesses can fine-tune, deploy, or integrate them into proprietary systems without restrictive terms.It also represents a shift in the ASR landscape—from centralized, cloud-gated offerings to community-extendable infrastructure. By making multilingual speech recognition more accessible, customizable, and cost-effective, Omnilingual ASR opens the door to a new generation of enterprise speech applications built around linguistic inclusion rather than linguistic limitation.",
          "content": "Meta has just released a new multilingual automatic speech recognition (ASR) system supporting 1,600+ languages — dwarfing OpenAI’s open source Whisper model, which supports just 99. Is architecture also allows developers to extend that support to thousands more. Through a feature called zero-shot in-context learning, users can provide a few paired examples of audio and text in a new language at inference time, enabling the model to transcribe additional utterances in that language without any retraining.In practice, this expands potential coverage to more than 5,400 languages — roughly every spoken language with a known script.It’s a shift from static model capabilities to a flexible framework that communities can adapt themselves. So while the 1,600 languages reflect official training coverage, the broader figure represents Omnilingual ASR’s capacity to generalize on demand, making it the most extensible speech recognition system released to date.Best of all: it&#x27;s been open sourced under a plain Apache 2.0 license — not a restrictive, quasi open-source Llama license like the company&#x27;s prior releases, which limited use by larger enterprises unless they paid licensing fees — meaning researchers and developers are free to take and implement it right away, for free, without restrictions, even in commercial and enterprise-grade projects!Released on November 10 on Meta&#x27;s website, Github, along with a demo space on Hugging Face and technical paper, Meta’s Omnilingual ASR suite includes a family of speech recognition models, a 7-billion parameter multilingual audio representation model, and a massive speech corpus spanning over 350 previously underserved languages. All resources are freely available under open licenses, and the models support speech-to-text transcription out of the box.“By open sourcing these models and dataset, we aim to break down language barriers, expand digital access, and empower communities worldwide,” Meta posted on its @AIatMeta account on XDesigned for Speech-to-Text TranscriptionAt its core, Omnilingual ASR is a speech-to-text system. The models are trained to convert spoken language into written text, supporting applications like voice assistants, transcription tools, subtitles, oral archive digitization, and accessibility features for low-resource languages.Unlike earlier ASR models that required extensive labeled training data, Omnilingual ASR includes a zero-shot variant. This version can transcribe languages it has never seen before—using just a few paired examples of audio and corresponding text. This lowers the barrier for adding new or endangered languages dramatically, removing the need for large corpora or retraining.Model Family and Technical DesignThe Omnilingual ASR suite includes multiple model families trained on more than 4.3 million hours of audio from 1,600+ languages:wav2vec 2.0 models for self-supervised speech representation learning (300M–7B parameters)CTC-based ASR models for efficient supervised transcriptionLLM-ASR models combining a speech encoder with a Transformer-based text decoder for state-of-the-art transcriptionLLM-ZeroShot ASR model, enabling inference-time adaptation to unseen languagesAll models follow an encoder–decoder design: raw audio is converted into a language-agnostic representation, then decoded into written text.Why the Scale MattersWhile Whisper and similar models have advanced ASR capabilities for global languages, they fall short on the long tail of human linguistic diversity. Whisper supports 99 languages. Meta’s system:Directly supports 1,600+ languagesCan generalize to 5,400+ languages using in-context learningAchieves character error rates (CER) under 10% in 78% of supported languagesAmong those supported are more than 500 languages never previously covered by any ASR model, according to Meta’s research paper.This expansion opens new possibilities for communities whose languages are often excluded from digital toolsHere’s the revised and expanded background section, integrating the broader context of Meta’s 2025 AI strategy, leadership changes, and Llama 4’s reception, complete with in-text citations and links:Background: Meta’s AI Overhaul and a Rebound from Llama 4The release of Omnilingual ASR arrives at a pivotal moment in Meta’s AI strategy, following a year marked by organizational turbulence, leadership changes, and uneven product execution. Omnilingual ASR is the first major open-source model release since the rollout of Llama 4, Meta’s latest large language model, which debuted in April 2025 to mixed and ultimately poor reviews, with scant enterprise adoption compared to Chinese open source model competitors.The failure led Meta founder and CEO Mark Zuckerberg to appoint Alexandr Wang, co-founder and prior CEO of AI data supplier Scale AI, as Chief AI Officer, and embark on an extensive and costly hiring spree that shocked the AI and business communities with eye-watering pay packages for top AI researchers.In contrast, Omnilingual ASR represents a strategic and reputational reset. It returns Meta to a domain where the company has historically led — multilingual AI — and offers a truly extensible, community-oriented stack with minimal barriers to entry. The system’s support for 1,600+ languages and its extensibility to over 5,000 more via zero-shot in-context learning reassert Meta’s engineering credibility in language technology. Importantly, it does so through a free and permissively licensed release, under Apache 2.0, with transparent dataset sourcing and reproducible training protocols.This shift aligns with broader themes in Meta’s 2025 strategy. The company has refocused its narrative around a “personal superintelligence” vision, investing heavily in infrastructure (including a September release of custom AI accelerators and Arm-based inference stacks) source while downplaying the metaverse in favor of foundational AI capabilities. The return to public training data in Europe after a regulatory pause also underscores its intention to compete globally, despite privacy scrutiny source.Omnilingual ASR, then, is more than a model release — it’s a calculated move to reassert control of the narrative: from the fragmented rollout of Llama 4 to a high-utility, research-grounded contribution that aligns with Meta’s long-term AI platform strategy.Community-Centered Dataset CollectionTo achieve this scale, Meta partnered with researchers and community organizations in Africa, Asia, and elsewhere to create the Omnilingual ASR Corpus, a 3,350-hour dataset across 348 low-resource languages. Contributors were compensated local speakers, and recordings were gathered in collaboration with groups like:African Next Voices: A Gates Foundation–supported consortium including Maseno University (Kenya), University of Pretoria, and Data Science NigeriaMozilla Foundation’s Common Voice, supported through the Open Multilingual Speech FundLanfrica / NaijaVoices, which created data for 11 African languages including Igala, Serer, and UrhoboThe data collection focused on natural, unscripted speech. Prompts were designed to be culturally relevant and open-ended, such as “Is it better to have a few close friends or many casual acquaintances? Why?” Transcriptions used established writing systems, with quality assurance built into every step.Performance and Hardware ConsiderationsThe largest model in the suite, the omniASR_LLM_7B, requires ~17GB of GPU memory for inference, making it suitable for deployment on high-end hardware. Smaller models (300M–1B) can run on lower-power devices and deliver real-time transcription speeds.Performance benchmarks show strong results even in low-resource scenarios:CER <10% in 95% of high-resource and mid-resource languagesCER <10% in 36% of low-resource languagesRobustness in noisy conditions and unseen domains, especially with fine-tuningThe zero-shot system, omniASR_LLM_7B_ZS, can transcribe new languages with minimal setup. Users provide a few sample audio–text pairs, and the model generates transcriptions for new utterances in the same language.Open Access and Developer ToolingAll models and the dataset are licensed under permissive terms:Apache 2.0 for models and codeCC-BY 4.0 for the Omnilingual ASR Corpus on HuggingFaceInstallation is supported via PyPI and uv:pip install omnilingual-asrMeta also provides:A HuggingFace dataset integrationPre-built inference pipelinesLanguage-code conditioning for improved accuracyDevelopers can view the full list of supported languages using the API:from omnilingual_asr.models.wav2vec2_llama.lang_ids import supported_langsprint(len(supported_langs)) print(supported_langs)Broader ImplicationsOmnilingual ASR reframes language coverage in ASR from a fixed list to an extensible framework. It enables:Community-driven inclusion of underrepresented languagesDigital access for oral and endangered languagesResearch on speech tech in linguistically diverse contextsCrucially, Meta emphasizes ethical considerations throughout—advocating for open-source participation and collaboration with native-speaking communities.“No model can ever anticipate and include all of the world’s languages in advance,” the Omnilingual ASR paper states, “but Omnilingual ASR makes it possible for communities to extend recognition with their own data.”Access the ToolsAll resources are now available at:Code + Models: github.com/facebookresearch/omnilingual-asrDataset: huggingface.co/datasets/facebook/omnilingual-asr-corpusBlogpost: ai.meta.com/blog/omnilingual-asrWhat This Means for EnterprisesFor enterprise developers, especially those operating in multilingual or international markets, Omnilingual ASR significantly lowers the barrier to deploying speech-to-text systems across a broader range of customers and geographies. Instead of relying on commercial ASR APIs that support only a narrow set of high-resource languages, teams can now integrate an open-source pipeline that covers over 1,600 languages out of the box—with the option to extend it to thousands more via zero-shot learning.This flexibility is especially valuable for enterprises working in sectors like voice-based customer support, transcription services, accessibility, education, or civic technology, where local language coverage can be a competitive or regulatory necessity. Because the models are released under the permissive Apache 2.0 license, businesses can fine-tune, deploy, or integrate them into proprietary systems without restrictive terms.It also represents a shift in the ASR landscape—from centralized, cloud-gated offerings to community-extendable infrastructure. By making multilingual speech recognition more accessible, customizable, and cost-effective, Omnilingual ASR opens the door to a new generation of enterprise speech applications built around linguistic inclusion rather than linguistic limitation.",
          "feed_position": 3,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/5WF8w75sB7wnAYEK5pYnPD/16059c6ac3f6b853b3b301bc66f950d3/cfr0z3n_graphic_novel_abstract_expressionist_outline_style_show_390da294-e50e-424d-ab36-20b02133d3d9.png?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/mobile/smartphones/renders-show-off-samsungs-upcoming-galaxy-s26-194014359.html",
          "published_at": "Mon, 10 Nov 2025 19:40:14 +0000",
          "title": "Renders show off Samsung's upcoming Galaxy S26+",
          "standfirst": "It looks like Samsung is sticking with its Plus line of Galaxy phones, according to a report by Android Headlines. This comes after rumors swirled that the company would be dropping the Plus line in favor of the slim Edge handset. This report indicates that not only is Samsung keeping the Plus, but it's ditching the well-reviewed Edge entirely. A leaker has given us a peek at the forthcoming S26+ and it looks similar to the company's other modern smartphones. The camera island has been tweaked a bit. The leak shows a small, raised camera island, which is different from the design of the S25+. Exclusive: Samsung Galaxy S26 Plus Official CAD Renders & Rumorshttps://t.co/gkh378hnYW #GalaxyS26Plus #Samsung #Android— Androidheadline (@Androidheadline) November 10, 2025 Also, the leaked images seem to suggest that the S26+ won't have the full-width camera that was rumored to be coming with the S26 Edge. That phone is now reportedly cancelled, so we don't know when we'll get that camera. The publication suggests that the S26+ will be 7.35mm thick, which is in line with the S25+. It also seems to be decidedly chunkier than the S25 Edge, so this is likely not a covert rebrand. The Verge has suggested that the company is also ditching the long-rumored S26 Pro. In other words, we'll likely be getting a standard S26, the S26+ and the S26 Ultra next year. That's the same naming convention as this year. We won't have too long to wait for actual details from Samsung. The company typically reveals new Galaxy phones sometime in January. It's been reported that Samsung is dropping the Edge due to poor sales. However, Dutch fansite Galaxy Club has reported that the company is still developing a slim handset for release sometime in the coming years.This article originally appeared on Engadget at https://www.engadget.com/mobile/smartphones/renders-show-off-samsungs-upcoming-galaxy-s26-194014359.html?src=rss",
          "content": "It looks like Samsung is sticking with its Plus line of Galaxy phones, according to a report by Android Headlines. This comes after rumors swirled that the company would be dropping the Plus line in favor of the slim Edge handset. This report indicates that not only is Samsung keeping the Plus, but it's ditching the well-reviewed Edge entirely. A leaker has given us a peek at the forthcoming S26+ and it looks similar to the company's other modern smartphones. The camera island has been tweaked a bit. The leak shows a small, raised camera island, which is different from the design of the S25+. Exclusive: Samsung Galaxy S26 Plus Official CAD Renders & Rumorshttps://t.co/gkh378hnYW #GalaxyS26Plus #Samsung #Android— Androidheadline (@Androidheadline) November 10, 2025 Also, the leaked images seem to suggest that the S26+ won't have the full-width camera that was rumored to be coming with the S26 Edge. That phone is now reportedly cancelled, so we don't know when we'll get that camera. The publication suggests that the S26+ will be 7.35mm thick, which is in line with the S25+. It also seems to be decidedly chunkier than the S25 Edge, so this is likely not a covert rebrand. The Verge has suggested that the company is also ditching the long-rumored S26 Pro. In other words, we'll likely be getting a standard S26, the S26+ and the S26 Ultra next year. That's the same naming convention as this year. We won't have too long to wait for actual details from Samsung. The company typically reveals new Galaxy phones sometime in January. It's been reported that Samsung is dropping the Edge due to poor sales. However, Dutch fansite Galaxy Club has reported that the company is still developing a slim handset for release sometime in the coming years.This article originally appeared on Engadget at https://www.engadget.com/mobile/smartphones/renders-show-off-samsungs-upcoming-galaxy-s26-194014359.html?src=rss",
          "feed_position": 40
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/use-this-hack-to-get-one-year-of-peacock-for-49-before-black-friday-192739041.html",
          "published_at": "Mon, 10 Nov 2025 19:27:39 +0000",
          "title": "Use this hack to get one year of Peacock for $49 before Black Friday",
          "standfirst": "Walmart is offering its Walmart+ subscription at half off for new sign-ups, and it includes a choice of either Peacock Premium or Paramount+ Essential. The deal for new subscribers is just $49 for the first year, marked down from $98. The real value is in selecting Peacock Premium, which would normally run you $110 per year on its own. With the current discount on a Walmart+ subscription you are essentially getting half off on your streaming subscription for that year. Just about every major streaming service has raised its prices in the last year, including HBO Max, Disney+, Netflix, Apple TV and YouTube TV, so saving some money on one of them just might be worth the effort. Cord cutting is not nearly as affordable as it used to be, so finding a deal like this is pretty helpful. Walmart+ itself offers myriad additional benefits like early access to Black Friday deals, free shipping on orders over $35, discounts on gas, free online veterinary care and more. Earlier this year, Walmart+ subscribers got first dibs on the Nintendo Switch 2 at the retailer. You can also use that free shipping to take advantage of Walmart's drone delivery program in a handful of select cities.This article originally appeared on Engadget at https://www.engadget.com/deals/use-this-hack-to-get-one-year-of-peacock-for-49-before-black-friday-192739041.html?src=rss",
          "content": "Walmart is offering its Walmart+ subscription at half off for new sign-ups, and it includes a choice of either Peacock Premium or Paramount+ Essential. The deal for new subscribers is just $49 for the first year, marked down from $98. The real value is in selecting Peacock Premium, which would normally run you $110 per year on its own. With the current discount on a Walmart+ subscription you are essentially getting half off on your streaming subscription for that year. Just about every major streaming service has raised its prices in the last year, including HBO Max, Disney+, Netflix, Apple TV and YouTube TV, so saving some money on one of them just might be worth the effort. Cord cutting is not nearly as affordable as it used to be, so finding a deal like this is pretty helpful. Walmart+ itself offers myriad additional benefits like early access to Black Friday deals, free shipping on orders over $35, discounts on gas, free online veterinary care and more. Earlier this year, Walmart+ subscribers got first dibs on the Nintendo Switch 2 at the retailer. You can also use that free shipping to take advantage of Walmart's drone delivery program in a handful of select cities.This article originally appeared on Engadget at https://www.engadget.com/deals/use-this-hack-to-get-one-year-of-peacock-for-49-before-black-friday-192739041.html?src=rss",
          "feed_position": 41
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/chronosphere-takes-on-datadog-with-ai-that-explains-itself-not-just-outages",
          "published_at": "Mon, 10 Nov 2025 19:00:00 GMT",
          "title": "Chronosphere takes on Datadog with AI that explains itself, not just outages",
          "standfirst": "Chronosphere, a New York-based observability startup valued at $1.6 billion, announced Monday it will launch AI-Guided Troubleshooting capabilities designed to help engineers diagnose and fix production software failures — a problem that has intensified as artificial intelligence tools accelerate code creation while making systems harder to debug.The new features combine AI-driven analysis with what Chronosphere calls a Temporal Knowledge Graph, a continuously updated map of an organization&#x27;s services, infrastructure dependencies, and system changes over time. The technology aims to address a mounting challenge in enterprise software: developers are writing code faster than ever with AI assistance, but troubleshooting remains largely manual, creating bottlenecks when applications fail.\"For AI to be effective in observability, it needs more than pattern recognition and summarization,\" said Martin Mao, Chronosphere&#x27;s CEO and co-founder, in an exclusive interview with VentureBeat. \"Chronosphere has spent years building the data foundation and analytical depth needed for AI to actually help engineers. With our Temporal Knowledge Graph and advanced analytics capabilities, we&#x27;re giving AI the understanding it needs to make observability truly intelligent — and giving engineers the confidence to trust its guidance.\"The announcement comes as the observability market — software that monitors complex cloud applications— faces mounting pressure to justify escalating costs. Enterprise log data volumes have grown 250% year-over-year, according to Chronosphere&#x27;s own research, while a study from MIT and the University of Pennsylvania found that generative AI has spurred a 13.5% increase in weekly code commits, signifying faster development velocity but also greater system complexity.AI writes code 13% faster, but debugging stays stubbornly manualDespite advances in automated code generation, debugging production failures remains stubbornly manual. When a major e-commerce site slows during checkout or a banking app fails to process transactions, engineers must sift through millions of data points — server logs, application traces, infrastructure metrics, recent code deployments — to identify root causes.Chronosphere&#x27;s answer is what it calls AI-Guided Troubleshooting, built on four core capabilities: automated \"Suggestions\" that propose investigation paths backed by data; the Temporal Knowledge Graph that maps system relationships and changes; Investigation Notebooks that document each troubleshooting step for future reference; and natural language query building.Mao explained the Temporal Knowledge Graph in practical terms: \"It&#x27;s a living, time-aware model of your system. It stitches together telemetry—metrics, traces, logs—infrastructure context, change events like deploys and feature flags, and even human input like notes and runbooks into a single, queryable map that updates as your system evolves.\"This differs fundamentally from the service dependency maps offered by competitors like Datadog, Dynatrace, and Splunk, Mao argued. \"It adds time, not just topology,\" he said. \"It tracks how services and dependencies change over time and connects those changes to incidents—what changed and why. Many tools rely on standardized integrations; our graph goes a step further to normalize custom, non-standard telemetry so application-specific signals aren&#x27;t a blind spot.\"Why Chronosphere shows its work instead of making automatic decisionsUnlike purely automated systems, Chronosphere designed its AI features to keep engineers in the driver&#x27;s seat—a deliberate choice meant to address what Mao calls the \"confident-but-wrong guidance\" problem plaguing early AI observability tools.\"&#x27;Keeping engineers in control&#x27; means the AI shows its work, proposes next steps, and lets engineers verify or override — never auto-deciding behind the scenes,\" Mao explained. \"Every Suggestion includes the evidence—timing, dependencies, error patterns — and a &#x27;Why was this suggested?&#x27; view, so they can inspect what was checked and ruled out before acting.\"He walked through a concrete example: \"An SLO [service level objective] alert fires on Checkout. Chronosphere immediately surfaces a ranked Suggestion: errors appear to have started in the dependent Payment service. An engineer can click Investigate to see the charts and reasoning and, if it holds up, choose to dig deeper. As they steer into Payment, the system adapts with new Suggestions scoped to that service—all from one view, no tab-hopping.\"In this scenario, the engineer asks \"what changed?\" and the system pulls in change events. \"Our Notebook capability makes the causal chain plain: a feature-flag update preceded pod memory exhaustion in Payment; Checkout&#x27;s spike is a downstream symptom,\" Mao said. \"They can decide to roll back the flag. That whole path — suggestions followed, evidence viewed, conclusions—is captured automatically in an Investigation Notebook, and the outcome feeds the Temporal Knowledge Graph so similar future incidents are faster to resolve.\"How a $1.6 billion startup takes on Datadog, Dynatrace, and SplunkChronosphere enters an increasingly crowded field. Datadog, the publicly traded observability leader valued at over $40 billion, has introduced its own AI-powered troubleshooting features. So have Dynatrace and Splunk. All three offer comprehensive \"all-in-one\" platforms that promise single-pane-of-glass visibility.Mao distinguished Chronosphere&#x27;s approach on technical grounds. \"Early &#x27;AI for observability&#x27; leaned heavily on pattern-spotting and summarization, which tends to break down during real incidents,\" he said. \"These approaches often stop at correlating anomalies or producing fluent explanations without the deeper analysis and causal reasoning observability leaders need. They can feel impressive in demos but disappoint in production—they summarize signals rather than explain cause and effect.\"A specific technical gap, he argued, involves custom application telemetry. \"Most platforms reason over standardized integrations—Kubernetes, common cloud services, popular databases—ignoring the most telling clues that live in custom app telemetry,\" Mao said. \"With an incomplete picture, large language models will &#x27;fill in the gaps,&#x27; producing confident-but-wrong guidance that sends teams down dead ends.\"Chronosphere&#x27;s competitive positioning received validation in July when Gartner named it a Leader in the 2025 Magic Quadrant for Observability Platforms for the second consecutive year. The firm was recognized based on both \"Completeness of Vision\" and \"Ability to Execute.\" In December 2024, Chronosphere also tied for the highest overall rating among recognized vendors in Gartner Peer Insights&#x27; \"Voice of the Customer\" report, scoring 4.7 out of 5 based on 70 reviews.Yet the company faces intensifying competition for high-profile customers. UBS analysts noted in July that OpenAI now runs both Datadog and Chronosphere side-by-side to monitor GPU workloads, suggesting the AI leader is evaluating alternatives. While UBS maintained its buy rating on Datadog, the analysts warned that growing Chronosphere usage could pressure Datadog&#x27;s pricing power.Inside the 84% cost reduction claims—and what CIOs should actually measureBeyond technical capabilities, Chronosphere has built its market position on cost control — a critical factor as observability spending spirals. The company claims its platform reduces data volumes and associated costs by 84% on average while cutting critical incidents by up to 75%.When pressed for specific customer examples with real numbers, Mao pointed to several case studies. \"Robinhood has seen a 5x improvement in reliability and a 4x improvement in Mean Time to Detection,\" he said. \"DoorDash used Chronosphere to improve governance and standardize monitoring practices. Astronomer achieved over 85% cost reduction by shaping data on ingest, and Affirm scaled their load 10x during a Black Friday event with no issues, highlighting the platform&#x27;s reliability under extreme conditions.\"The cost argument matters because, as Paul Nashawaty, principal analyst at CUBE Research, noted when Chronosphere launched its Logs 2.0 product in June: \"Organizations are drowning in telemetry data, with over 70% of observability spend going toward storing logs that are never queried.\"For CIOs fatigued by \"AI-powered\" announcements, Mao acknowledged skepticism is warranted. \"The way to cut through it is to test whether the AI shortens incidents, reduces toil, and builds reusable knowledge in your own environment, not in a demo,\" he advised. He recommended CIOs evaluate three factors: transparency and control (does the system show its reasoning?), coverage of custom telemetry (can it handle non-standardized data?), and manual toil avoided (how many ad-hoc queries and tool-switches are eliminated?).Why Chronosphere partners with five vendors instead of building everything itselfAlongside the AI troubleshooting announcement, Chronosphere revealed a new Partner Program integrating five specialized vendors to fill gaps in its platform: Arize for large language model monitoring, Embrace for real user monitoring, Polar Signals for continuous profiling, Checkly for synthetic monitoring, and Rootly for incident management.The strategy represents a deliberate bet against the all-in-one platforms dominating the market. \"While an all-in-one platform may be sufficient for smaller organizations, global enterprises demand best-in-class depth across each domain,\" Mao said. \"This is what drove us to build our Partner Program and invest in seamless integrations with leading providers—so our customers can operate with confidence and clarity at every layer of observability.\"Noah Smolen, head of partnerships at Arize, said the collaboration addresses a specific enterprise need. \"With a wide array of Fortune 500 customers, we understand the high bar needed to ensure AI agent systems are ready to deploy and stay incident-free, especially given the pace of AI adoption in the enterprise,\" Smolen said. \"Our partnership with Chronosphere comes at a time when an integrated purpose-built cloud-native and AI-observability suite solves a huge pain point for forward-thinking C-suite leaders who demand the very best across their entire observability stack.\"Similarly, JJ Tang, CEO and founder of Rootly, emphasized the incident resolution benefits. \"Incidents hinder innovation and revenue, and the challenge lies in sifting through vast amounts of observability data, mobilizing teams, and resolving issues quickly,\" Tang said. \"Integrating Chronosphere with Rootly allows engineers to collaborate with context and resolve issues faster within their existing communication channels, drastically reducing time to resolution and ultimately improving reliability—78% plus decreases in repeat Sev0 and Sev1 incidents.\"When asked how total costs compare when customers use multiple partner contracts versus a single platform, Mao acknowledged the current complexity. \"At present, mutual customers typically maintain separate contracts unless they engage through a services partner or system integrator,\" he said. However, he argued the economics still favor the composable approach: \"Our combined technologies deliver exceptional value—in most circumstances at just a fraction of the price of a single-platform solution. Beyond the savings, customers gain a richer, more unified observability experience that unlocks deeper insights and greater efficiency, especially for large-scale environments.\"The company plans to streamline this over time. \"As the ISV program matures, we&#x27;re focused on delivering a more streamlined experience by transitioning to a single, unified contract that simplifies procurement and accelerates time to value,\" Mao said.How two Uber engineers turned Halloween outages into a billion-dollar startupChronosphere&#x27;s origins trace to 2019, when Mao and co-founder Rob Skillington left Uber after building the ride-hailing giant&#x27;s internal observability platform. At Uber, Mao&#x27;s team had faced a crisis: the company&#x27;s in-house tools would fail on its two busiest nights — Halloween and New Year&#x27;s Eve — cutting off visibility into whether customers could request rides or drivers could locate passengers.The solution they built at Uber used open-source software and ultimately allowed the company to operate without outages, even during high-volume events. But the broader market insight came at an industry conference in December 2018, when major cloud providers threw their weight behind Kubernetes, Google&#x27;s container orchestration technology.\"This meant that most technology architectures were eventually going to look like Uber&#x27;s,\" Mao recalled in an August 2024 profile by Greylock Partners, Chronosphere&#x27;s lead investor. \"And that meant every company, not just a few big tech companies and the Walmarts of the world, would have the exact same problem we had solved at Uber.\"Chronosphere has since raised more than $343 million in funding across multiple rounds led by Greylock, Lux Capital, General Atlantic, Addition, and Founders Fund. The company operates as a remote-first organization with offices in New York, Austin, Boston, San Francisco, and Seattle, employing approximately 299 people according to LinkedIn data.The company&#x27;s customer base includes DoorDash, Zillow, Snap, Robinhood, and Affirm — predominantly high-growth technology companies operating cloud-native, Kubernetes-based infrastructures at massive scale.What&#x27;s available now—and what enterprises can expect in 2026Chronosphere&#x27;s AI-Guided Troubleshooting capabilities, including Suggestions and Investigation Notebooks, entered limited availability Monday with select customers. The company plans full general availability in 2026. The Model Context Protocol (MCP) Server, which enables engineers to integrate Chronosphere directly into internal AI workflows and query observability data through AI-enabled development environments, is available immediately for all Chronosphere customers.The phased rollout reflects the company&#x27;s cautious approach to deploying AI in production environments where mistakes carry real costs. By gathering feedback from early adopters before broad release, Chronosphere aims to refine its guidance algorithms and validate that its suggestions genuinely accelerate troubleshooting rather than simply generating impressive demonstrations.The longer game, however, extends beyond individual product features. Chronosphere&#x27;s dual bet — on transparent AI that shows its reasoning and on a partner ecosystem rather than all-in-one integration — amounts to a fundamental thesis about how enterprise observability will evolve as systems grow more complex.If that thesis proves correct, the company that solves observability for the AI age won&#x27;t be the one with the most automated black box. It will be the one that earns engineers&#x27; trust by explaining what it knows, admitting what it doesn&#x27;t, and letting humans make the final call. In an industry drowning in data and promised silver bullets, Chronosphere is wagering that showing your work still matters — even when AI is doing the math.",
          "content": "Chronosphere, a New York-based observability startup valued at $1.6 billion, announced Monday it will launch AI-Guided Troubleshooting capabilities designed to help engineers diagnose and fix production software failures — a problem that has intensified as artificial intelligence tools accelerate code creation while making systems harder to debug.The new features combine AI-driven analysis with what Chronosphere calls a Temporal Knowledge Graph, a continuously updated map of an organization&#x27;s services, infrastructure dependencies, and system changes over time. The technology aims to address a mounting challenge in enterprise software: developers are writing code faster than ever with AI assistance, but troubleshooting remains largely manual, creating bottlenecks when applications fail.\"For AI to be effective in observability, it needs more than pattern recognition and summarization,\" said Martin Mao, Chronosphere&#x27;s CEO and co-founder, in an exclusive interview with VentureBeat. \"Chronosphere has spent years building the data foundation and analytical depth needed for AI to actually help engineers. With our Temporal Knowledge Graph and advanced analytics capabilities, we&#x27;re giving AI the understanding it needs to make observability truly intelligent — and giving engineers the confidence to trust its guidance.\"The announcement comes as the observability market — software that monitors complex cloud applications— faces mounting pressure to justify escalating costs. Enterprise log data volumes have grown 250% year-over-year, according to Chronosphere&#x27;s own research, while a study from MIT and the University of Pennsylvania found that generative AI has spurred a 13.5% increase in weekly code commits, signifying faster development velocity but also greater system complexity.AI writes code 13% faster, but debugging stays stubbornly manualDespite advances in automated code generation, debugging production failures remains stubbornly manual. When a major e-commerce site slows during checkout or a banking app fails to process transactions, engineers must sift through millions of data points — server logs, application traces, infrastructure metrics, recent code deployments — to identify root causes.Chronosphere&#x27;s answer is what it calls AI-Guided Troubleshooting, built on four core capabilities: automated \"Suggestions\" that propose investigation paths backed by data; the Temporal Knowledge Graph that maps system relationships and changes; Investigation Notebooks that document each troubleshooting step for future reference; and natural language query building.Mao explained the Temporal Knowledge Graph in practical terms: \"It&#x27;s a living, time-aware model of your system. It stitches together telemetry—metrics, traces, logs—infrastructure context, change events like deploys and feature flags, and even human input like notes and runbooks into a single, queryable map that updates as your system evolves.\"This differs fundamentally from the service dependency maps offered by competitors like Datadog, Dynatrace, and Splunk, Mao argued. \"It adds time, not just topology,\" he said. \"It tracks how services and dependencies change over time and connects those changes to incidents—what changed and why. Many tools rely on standardized integrations; our graph goes a step further to normalize custom, non-standard telemetry so application-specific signals aren&#x27;t a blind spot.\"Why Chronosphere shows its work instead of making automatic decisionsUnlike purely automated systems, Chronosphere designed its AI features to keep engineers in the driver&#x27;s seat—a deliberate choice meant to address what Mao calls the \"confident-but-wrong guidance\" problem plaguing early AI observability tools.\"&#x27;Keeping engineers in control&#x27; means the AI shows its work, proposes next steps, and lets engineers verify or override — never auto-deciding behind the scenes,\" Mao explained. \"Every Suggestion includes the evidence—timing, dependencies, error patterns — and a &#x27;Why was this suggested?&#x27; view, so they can inspect what was checked and ruled out before acting.\"He walked through a concrete example: \"An SLO [service level objective] alert fires on Checkout. Chronosphere immediately surfaces a ranked Suggestion: errors appear to have started in the dependent Payment service. An engineer can click Investigate to see the charts and reasoning and, if it holds up, choose to dig deeper. As they steer into Payment, the system adapts with new Suggestions scoped to that service—all from one view, no tab-hopping.\"In this scenario, the engineer asks \"what changed?\" and the system pulls in change events. \"Our Notebook capability makes the causal chain plain: a feature-flag update preceded pod memory exhaustion in Payment; Checkout&#x27;s spike is a downstream symptom,\" Mao said. \"They can decide to roll back the flag. That whole path — suggestions followed, evidence viewed, conclusions—is captured automatically in an Investigation Notebook, and the outcome feeds the Temporal Knowledge Graph so similar future incidents are faster to resolve.\"How a $1.6 billion startup takes on Datadog, Dynatrace, and SplunkChronosphere enters an increasingly crowded field. Datadog, the publicly traded observability leader valued at over $40 billion, has introduced its own AI-powered troubleshooting features. So have Dynatrace and Splunk. All three offer comprehensive \"all-in-one\" platforms that promise single-pane-of-glass visibility.Mao distinguished Chronosphere&#x27;s approach on technical grounds. \"Early &#x27;AI for observability&#x27; leaned heavily on pattern-spotting and summarization, which tends to break down during real incidents,\" he said. \"These approaches often stop at correlating anomalies or producing fluent explanations without the deeper analysis and causal reasoning observability leaders need. They can feel impressive in demos but disappoint in production—they summarize signals rather than explain cause and effect.\"A specific technical gap, he argued, involves custom application telemetry. \"Most platforms reason over standardized integrations—Kubernetes, common cloud services, popular databases—ignoring the most telling clues that live in custom app telemetry,\" Mao said. \"With an incomplete picture, large language models will &#x27;fill in the gaps,&#x27; producing confident-but-wrong guidance that sends teams down dead ends.\"Chronosphere&#x27;s competitive positioning received validation in July when Gartner named it a Leader in the 2025 Magic Quadrant for Observability Platforms for the second consecutive year. The firm was recognized based on both \"Completeness of Vision\" and \"Ability to Execute.\" In December 2024, Chronosphere also tied for the highest overall rating among recognized vendors in Gartner Peer Insights&#x27; \"Voice of the Customer\" report, scoring 4.7 out of 5 based on 70 reviews.Yet the company faces intensifying competition for high-profile customers. UBS analysts noted in July that OpenAI now runs both Datadog and Chronosphere side-by-side to monitor GPU workloads, suggesting the AI leader is evaluating alternatives. While UBS maintained its buy rating on Datadog, the analysts warned that growing Chronosphere usage could pressure Datadog&#x27;s pricing power.Inside the 84% cost reduction claims—and what CIOs should actually measureBeyond technical capabilities, Chronosphere has built its market position on cost control — a critical factor as observability spending spirals. The company claims its platform reduces data volumes and associated costs by 84% on average while cutting critical incidents by up to 75%.When pressed for specific customer examples with real numbers, Mao pointed to several case studies. \"Robinhood has seen a 5x improvement in reliability and a 4x improvement in Mean Time to Detection,\" he said. \"DoorDash used Chronosphere to improve governance and standardize monitoring practices. Astronomer achieved over 85% cost reduction by shaping data on ingest, and Affirm scaled their load 10x during a Black Friday event with no issues, highlighting the platform&#x27;s reliability under extreme conditions.\"The cost argument matters because, as Paul Nashawaty, principal analyst at CUBE Research, noted when Chronosphere launched its Logs 2.0 product in June: \"Organizations are drowning in telemetry data, with over 70% of observability spend going toward storing logs that are never queried.\"For CIOs fatigued by \"AI-powered\" announcements, Mao acknowledged skepticism is warranted. \"The way to cut through it is to test whether the AI shortens incidents, reduces toil, and builds reusable knowledge in your own environment, not in a demo,\" he advised. He recommended CIOs evaluate three factors: transparency and control (does the system show its reasoning?), coverage of custom telemetry (can it handle non-standardized data?), and manual toil avoided (how many ad-hoc queries and tool-switches are eliminated?).Why Chronosphere partners with five vendors instead of building everything itselfAlongside the AI troubleshooting announcement, Chronosphere revealed a new Partner Program integrating five specialized vendors to fill gaps in its platform: Arize for large language model monitoring, Embrace for real user monitoring, Polar Signals for continuous profiling, Checkly for synthetic monitoring, and Rootly for incident management.The strategy represents a deliberate bet against the all-in-one platforms dominating the market. \"While an all-in-one platform may be sufficient for smaller organizations, global enterprises demand best-in-class depth across each domain,\" Mao said. \"This is what drove us to build our Partner Program and invest in seamless integrations with leading providers—so our customers can operate with confidence and clarity at every layer of observability.\"Noah Smolen, head of partnerships at Arize, said the collaboration addresses a specific enterprise need. \"With a wide array of Fortune 500 customers, we understand the high bar needed to ensure AI agent systems are ready to deploy and stay incident-free, especially given the pace of AI adoption in the enterprise,\" Smolen said. \"Our partnership with Chronosphere comes at a time when an integrated purpose-built cloud-native and AI-observability suite solves a huge pain point for forward-thinking C-suite leaders who demand the very best across their entire observability stack.\"Similarly, JJ Tang, CEO and founder of Rootly, emphasized the incident resolution benefits. \"Incidents hinder innovation and revenue, and the challenge lies in sifting through vast amounts of observability data, mobilizing teams, and resolving issues quickly,\" Tang said. \"Integrating Chronosphere with Rootly allows engineers to collaborate with context and resolve issues faster within their existing communication channels, drastically reducing time to resolution and ultimately improving reliability—78% plus decreases in repeat Sev0 and Sev1 incidents.\"When asked how total costs compare when customers use multiple partner contracts versus a single platform, Mao acknowledged the current complexity. \"At present, mutual customers typically maintain separate contracts unless they engage through a services partner or system integrator,\" he said. However, he argued the economics still favor the composable approach: \"Our combined technologies deliver exceptional value—in most circumstances at just a fraction of the price of a single-platform solution. Beyond the savings, customers gain a richer, more unified observability experience that unlocks deeper insights and greater efficiency, especially for large-scale environments.\"The company plans to streamline this over time. \"As the ISV program matures, we&#x27;re focused on delivering a more streamlined experience by transitioning to a single, unified contract that simplifies procurement and accelerates time to value,\" Mao said.How two Uber engineers turned Halloween outages into a billion-dollar startupChronosphere&#x27;s origins trace to 2019, when Mao and co-founder Rob Skillington left Uber after building the ride-hailing giant&#x27;s internal observability platform. At Uber, Mao&#x27;s team had faced a crisis: the company&#x27;s in-house tools would fail on its two busiest nights — Halloween and New Year&#x27;s Eve — cutting off visibility into whether customers could request rides or drivers could locate passengers.The solution they built at Uber used open-source software and ultimately allowed the company to operate without outages, even during high-volume events. But the broader market insight came at an industry conference in December 2018, when major cloud providers threw their weight behind Kubernetes, Google&#x27;s container orchestration technology.\"This meant that most technology architectures were eventually going to look like Uber&#x27;s,\" Mao recalled in an August 2024 profile by Greylock Partners, Chronosphere&#x27;s lead investor. \"And that meant every company, not just a few big tech companies and the Walmarts of the world, would have the exact same problem we had solved at Uber.\"Chronosphere has since raised more than $343 million in funding across multiple rounds led by Greylock, Lux Capital, General Atlantic, Addition, and Founders Fund. The company operates as a remote-first organization with offices in New York, Austin, Boston, San Francisco, and Seattle, employing approximately 299 people according to LinkedIn data.The company&#x27;s customer base includes DoorDash, Zillow, Snap, Robinhood, and Affirm — predominantly high-growth technology companies operating cloud-native, Kubernetes-based infrastructures at massive scale.What&#x27;s available now—and what enterprises can expect in 2026Chronosphere&#x27;s AI-Guided Troubleshooting capabilities, including Suggestions and Investigation Notebooks, entered limited availability Monday with select customers. The company plans full general availability in 2026. The Model Context Protocol (MCP) Server, which enables engineers to integrate Chronosphere directly into internal AI workflows and query observability data through AI-enabled development environments, is available immediately for all Chronosphere customers.The phased rollout reflects the company&#x27;s cautious approach to deploying AI in production environments where mistakes carry real costs. By gathering feedback from early adopters before broad release, Chronosphere aims to refine its guidance algorithms and validate that its suggestions genuinely accelerate troubleshooting rather than simply generating impressive demonstrations.The longer game, however, extends beyond individual product features. Chronosphere&#x27;s dual bet — on transparent AI that shows its reasoning and on a partner ecosystem rather than all-in-one integration — amounts to a fundamental thesis about how enterprise observability will evolve as systems grow more complex.If that thesis proves correct, the company that solves observability for the AI age won&#x27;t be the one with the most automated black box. It will be the one that earns engineers&#x27; trust by explaining what it knows, admitting what it doesn&#x27;t, and letting humans make the final call. In an industry drowning in data and promised silver bullets, Chronosphere is wagering that showing your work still matters — even when AI is doing the math.",
          "feed_position": 4,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/6Jp2LKG6zzFEnDhB9m893X/06467084db114c990d9e72be3896bf51/nuneybits_Vector_art_of_AI_debugging_maze_2b3bfc89-ab17-4839-a03e-6331a42f5030.webp?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/headspace-annual-subscriptions-are-half-off-for-black-friday-163051719.html",
          "published_at": "Mon, 10 Nov 2025 17:00:38 +0000",
          "title": "Headspace annual subscriptions are half off for Black Friday",
          "standfirst": "Headspace’s Black Friday deal is live, offering 50 percent off its annual subscription through December 4. That brings the cost of a full year down to $35, giving you access to guided meditations, courses and stress-management tools that can help you stay balanced heading into the new year. Headspace has become one of the most recognizable names in digital mindfulness. The app blends practical meditation guidance with structured courses and calming soundscapes designed to make everyday stress easier to manage. Its programs cover everything from beginner-friendly introductions to mindfulness to focused content on topics like anxiety, productivity and sleep. Subscribers get access to hundreds of guided sessions led by the Headspace team, including short daily practices that can be completed in a few spare minutes, plus longer courses that help build consistency. The app’s Sleepcasts and soundscapes are unique, designed to create a steady nighttime routine that promotes better rest. For mornings, there are breathing exercises and motivational mini-sessions that can help set focus for the day ahead. Headspace also includes personalized progress tracking, mood check-ins and optional reminders that make it easier to stay consistent with your new mindfulness habits. For anyone new to meditation, the app’s clear structure is a major strength. You don’t have to know where to start, since it suggests sessions based on your goals or current mood. This annual deal is ideal for users who want to stick with mindfulness practice over time, or anyone interested in incorporating a new habit into their lives. Paying for the year upfront typically saves money compared with the monthly plan, and the discount brings that cost down even further. Whether you’re learning the basics of meditation or refining an existing routine, the full library provides enough variety to keep things engaging throughout the year. If you’re still comparing wellness apps, check out our guide to the best meditation apps to see how Headspace stacks up against other options. But for those ready to commit to a calmer routine, this annual offer is one of the simplest ways to start the habit at a lower cost.This article originally appeared on Engadget at https://www.engadget.com/deals/headspace-annual-subscriptions-are-half-off-for-black-friday-163051719.html?src=rss",
          "content": "Headspace’s Black Friday deal is live, offering 50 percent off its annual subscription through December 4. That brings the cost of a full year down to $35, giving you access to guided meditations, courses and stress-management tools that can help you stay balanced heading into the new year. Headspace has become one of the most recognizable names in digital mindfulness. The app blends practical meditation guidance with structured courses and calming soundscapes designed to make everyday stress easier to manage. Its programs cover everything from beginner-friendly introductions to mindfulness to focused content on topics like anxiety, productivity and sleep. Subscribers get access to hundreds of guided sessions led by the Headspace team, including short daily practices that can be completed in a few spare minutes, plus longer courses that help build consistency. The app’s Sleepcasts and soundscapes are unique, designed to create a steady nighttime routine that promotes better rest. For mornings, there are breathing exercises and motivational mini-sessions that can help set focus for the day ahead. Headspace also includes personalized progress tracking, mood check-ins and optional reminders that make it easier to stay consistent with your new mindfulness habits. For anyone new to meditation, the app’s clear structure is a major strength. You don’t have to know where to start, since it suggests sessions based on your goals or current mood. This annual deal is ideal for users who want to stick with mindfulness practice over time, or anyone interested in incorporating a new habit into their lives. Paying for the year upfront typically saves money compared with the monthly plan, and the discount brings that cost down even further. Whether you’re learning the basics of meditation or refining an existing routine, the full library provides enough variety to keep things engaging throughout the year. If you’re still comparing wellness apps, check out our guide to the best meditation apps to see how Headspace stacks up against other options. But for those ready to commit to a calmer routine, this annual offer is one of the simplest ways to start the habit at a lower cost.This article originally appeared on Engadget at https://www.engadget.com/deals/headspace-annual-subscriptions-are-half-off-for-black-friday-163051719.html?src=rss",
          "feed_position": 45
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/gaming/the-latest-death-stranding-collab-is-an-actual-exoskeleton-151215509.html",
          "published_at": "Mon, 10 Nov 2025 15:12:15 +0000",
          "title": "The latest Death Stranding collab is an actual exoskeleton",
          "standfirst": "It's arriving too late to be part of a Sam Porter Bridges Halloween costume this year, but Kojima Productions has a new collaboration lined up that could be just the ticket for cosplayers or anyone who may wish to enhance their lower-body strength and stability. The studio has teamed up with exoskeleton maker Dnsys for a limited run of a model based on Death Stranding 2: On The Beach.Dnsys claimed this was the first collaboration between an exoskeleton maker and a game studio. Kojima Productions art director Yoji Shinkawa worked with the company on the design, which draws from the color palettes of the game and is based on an existing Dnsys model. It has lights similar to the exoskeleton Sam wears in the game, and they indicate the battery level. DNSYS ✖️ DEATH STRANDING 2 co-branded exoskeleton is here 👍📦 Dnsys Z1 Exoskeleton Pro – DEATH STRANDING 2 ON THE BEACH Limited EditionIt enhances lower-body strength and stability, allowing humans to keep moving through complex terrain and extreme environments. 🦵🦿… pic.twitter.com/Yi1E6RnKNC— KOJIMA PRODUCTIONS (Eng) (@KojiPro2015_EN) November 10, 2025 The Dnsys Z1 Exoskeleton Pro - Death Stranding 2: On The Beach Limited Edition (to give its full name) is said to mimic the game's load-balancing system by offloading up to 200 percent of the wearer's body weight from their knees to help protect the joints. Dnsys said it adds 50 percent more power to steps and \"intelligent gait control\" to improve balance on stairs and uneven terrain. According to the company's press release, you'll \"instantly feel up to 44 lbs lighter during vertical movements.\" The exoskeleton is slated to offer over four hours of continuous support and there's a quick-swap battery system.Exoskeletons can be very useful, particularly for those who could do with extra support and/or have mobility issues. Or maybe you want to look the part while you carry some very heavy cargo on your back and listen to Chvrches and Bring Me the Horizon. This limited-edition exoskeleton will be available on December 2. Pricing will be announced then. The regular price of a dual-leg Dnsys Z1 system is $1,500, so you can probably expect the Death Stranding 2 variant to be in that ballpark. You can also try your luck at scoring the exoskeleton for free through a giveaway.This article originally appeared on Engadget at https://www.engadget.com/gaming/the-latest-death-stranding-collab-is-an-actual-exoskeleton-151215509.html?src=rss",
          "content": "It's arriving too late to be part of a Sam Porter Bridges Halloween costume this year, but Kojima Productions has a new collaboration lined up that could be just the ticket for cosplayers or anyone who may wish to enhance their lower-body strength and stability. The studio has teamed up with exoskeleton maker Dnsys for a limited run of a model based on Death Stranding 2: On The Beach.Dnsys claimed this was the first collaboration between an exoskeleton maker and a game studio. Kojima Productions art director Yoji Shinkawa worked with the company on the design, which draws from the color palettes of the game and is based on an existing Dnsys model. It has lights similar to the exoskeleton Sam wears in the game, and they indicate the battery level. DNSYS ✖️ DEATH STRANDING 2 co-branded exoskeleton is here 👍📦 Dnsys Z1 Exoskeleton Pro – DEATH STRANDING 2 ON THE BEACH Limited EditionIt enhances lower-body strength and stability, allowing humans to keep moving through complex terrain and extreme environments. 🦵🦿… pic.twitter.com/Yi1E6RnKNC— KOJIMA PRODUCTIONS (Eng) (@KojiPro2015_EN) November 10, 2025 The Dnsys Z1 Exoskeleton Pro - Death Stranding 2: On The Beach Limited Edition (to give its full name) is said to mimic the game's load-balancing system by offloading up to 200 percent of the wearer's body weight from their knees to help protect the joints. Dnsys said it adds 50 percent more power to steps and \"intelligent gait control\" to improve balance on stairs and uneven terrain. According to the company's press release, you'll \"instantly feel up to 44 lbs lighter during vertical movements.\" The exoskeleton is slated to offer over four hours of continuous support and there's a quick-swap battery system.Exoskeletons can be very useful, particularly for those who could do with extra support and/or have mobility issues. Or maybe you want to look the part while you carry some very heavy cargo on your back and listen to Chvrches and Bring Me the Horizon. This limited-edition exoskeleton will be available on December 2. Pricing will be announced then. The regular price of a dual-leg Dnsys Z1 system is $1,500, so you can probably expect the Death Stranding 2 variant to be in that ballpark. You can also try your luck at scoring the exoskeleton for free through a giveaway.This article originally appeared on Engadget at https://www.engadget.com/gaming/the-latest-death-stranding-collab-is-an-actual-exoskeleton-151215509.html?src=rss",
          "feed_position": 47
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/how-context-engineering-can-save-your-company-from-ai-vibe-code-overload",
          "published_at": "Mon, 10 Nov 2025 15:00:00 GMT",
          "title": "How context engineering can save your company from AI vibe code overload: lessons from Qodo and Monday.com",
          "standfirst": "As cloud project tracking software monday.com’s engineering organization scaled past 500 developers, the team began to feel the strain of its own success. Product lines were multiplying, microservices proliferating, and code was flowing faster than human reviewers could keep up. The company needed a way to review thousands of pull requests each month without drowning developers in tedium — or letting quality slip.That’s when Guy Regev, VP of R&D and head of the Growth and monday Dev teams, started experimenting with a new AI tool from Qodo, an Israeli startup focused on developer agents. What began as a lightweight test soon became a critical part of monday.com’s software delivery infrastructure, as a new case study released by both Qodo and monday.com today reveals. “Qodo doesn’t feel like just another tool—it’s like adding a new developer to the team who actually learns how we work,\" Regev told VentureBeat in a recent video call interview, adding that it has \"prevented over 800 issues per month from reaching production—some of them could have caused serious security vulnerabilities.\"Unlike code generation tools like GitHub Copilot or Cursor, Qodo isn’t trying to write new code. Instead, it specializes in reviewing it — using what it calls context engineering to understand not just what changed in a pull request, but why, how it aligns with business logic, and whether it follows internal best practices. \"You can call Claude Code or Cursor and in five minutes get 1,000 lines of code,\" said Itamar Friedman, co-founder and CEO of Qodo, in the same video call interview as with Regev. \"You have 40 minutes, and you can&#x27;t review that. So you need Qodo to actually review it.”For monday.com, this capability wasn’t just helpful — it was transformative.Code Review, at ScaleAt any given time, monday.com’s developers are shipping updates across hundreds of repositories and services. The engineering org works in tightly coordinated teams, each aligned with specific parts of the product: marketing, CRM, dev tools, internal platforms, and more.That’s where Qodo came in. The company’s platform uses AI not just to check for obvious bugs or style violations, but to evaluate whether a pull request follows team-specific conventions, architectural guidelines, and historical patterns. It does this by learning from your own codebase — training on previous PRs, comments, merges, and even Slack threads to understand how your team works.\"The comments Qodo gives aren’t generic—they reflect our values, our libraries, even our standards for things like feature flags and privacy,\" Regev said. \"It’s context-aware in a way traditional tools aren’t.\"What “Context Engineering” Actually MeansQodo calls its secret sauce context engineering — a system-level approach to managing everything the model sees when making a decision. This includes the PR code diff, of course, but also prior discussions, documentation, relevant files from the repo, even test results and configuration data.The idea is that language models don’t really “think” — they predict the next token based on the inputs they’re given. So the quality of their output depends almost entirely on the quality and structure of their inputs.As Dana Fine, Qodo’s community manager, put it in a blog post: “You’re not just writing prompts; you’re designing structured input under a fixed token limit. Every token is a design decision.”This isn’t just theory. In monday.com’s case, it meant Qodo could catch not only the obvious bugs, but the subtle ones that typically slip past human reviewers — hardcoded variables, missing fallbacks, or violations of cross-team architecture conventions.One example stood out. In a recent PR, Qodo flagged a line that inadvertently exposed a staging environment variable — something no human reviewer caught. Had it been merged, it might have caused problems in production. \"The hours we would spend on fixing this security leak and the legal issue that it would bring would be much more than the hours that we reduce from a pull-request,\" said Regev.Integration into the PipelineToday, Qodo is deeply integrated into monday.com’s development workflow, analyzing pull requests and surfacing context-aware recommendations based on prior team code reviews. “It doesn’t feel like just another tool... It feels like another teammate that joined the system — one who learns how we work,\" Regev noted. Developers receive suggestions during the review process and remain in control of final decisions — a human-in-the-loop model that was critical for adoption.Because Qodo integrated directly into GitHub via pull request actions and comments, Monday.com’s infrastructure team didn’t face a steep learning curve.“It’s just a GitHub action,” said Regev. “It creates a PR with the tests. It’s not like a separate tool we had to learn.”“The purpose is to actually help the developer learn the code, take ownership, give feedback to each other, and learn from that and establish the standards,\" added Friedman.The Results: Time Saved, Bugs PreventedSince rolling out Qodo more broadly, monday.com has seen measurable improvements across multiple teams.Internal analysis shows that developers save roughly an hour per pull request on average. Multiply that across thousands of PRs per month, and the savings quickly reach thousands of developer hours annually.These aren’t just cosmetic issues — many relate to business logic, security, or runtime stability. And because Qodo’s suggestions reflect monday.com’s actual conventions, developers are more likely to act on them.The system’s accuracy is rooted in its data-first design. Qodo trains on each company’s private codebase and historical data, adapting to different team styles and practices. It doesn’t rely on one-size-fits-all rules or external datasets. Everything is tailored.From Internal Tool to Product VisionRegev’s team was so impressed with Qodo’s impact that they’ve started planning deeper integrations between Qodo and Monday Dev, the developer-focused product line monday.com is building.The vision is to create a workflow where business context — tasks, tickets, customer feedback — flows directly into the code review layer. That way, reviewers can assess not just whether the code “works,” but whether it solves the right problem.“Before, we had linters, danger rules, static analysis... rule-based... you need to configure all the rules,\" Regev said. \"But it doesn’t know what you don’t know... Qodo... feels like it’s learning from our engineers.”This aligns closely with Qodo’s own roadmap. The company doesn’t just review code. It’s building a full platform of developer agents — including Qodo Gen for context-aware code generation, Qodo Merge for automated PR analysis, and Qodo Cover, a regression-testing agent that uses runtime validation to ensure test coverage.All of this is powered by Qodo’s own infrastructure, including its new open-source embedding model, Qodo-Embed-1-1.5B, which outperformed offerings from OpenAI and Salesforce on code retrieval benchmarks.What’s Next?Qodo is now offering its platform under a freemium model — free for individuals, discounted for startups through Google Cloud’s Perks program, and enterprise-grade for companies that need SSO, air-gapped deployment, or advanced controls.The company is already working with teams at NVIDIA, Intuit, and other Fortune 500 companies. And thanks to a recent partnership with Google Cloud, Qodo’s models are available directly inside Vertex AI’s Model Garden, making it easier to integrate into enterprise pipelines.\"Context engines will be the big story of 2026,\" Friedman said. \"Every enterprise will need to build their own second brain if they want AI that actually understands and helps them.\"As AI systems become more embedded in software development, tools like Qodo are showing how the right context — delivered at the right moment — can transform how teams build, ship, and scale code across the enterprise.",
          "content": "As cloud project tracking software monday.com’s engineering organization scaled past 500 developers, the team began to feel the strain of its own success. Product lines were multiplying, microservices proliferating, and code was flowing faster than human reviewers could keep up. The company needed a way to review thousands of pull requests each month without drowning developers in tedium — or letting quality slip.That’s when Guy Regev, VP of R&D and head of the Growth and monday Dev teams, started experimenting with a new AI tool from Qodo, an Israeli startup focused on developer agents. What began as a lightweight test soon became a critical part of monday.com’s software delivery infrastructure, as a new case study released by both Qodo and monday.com today reveals. “Qodo doesn’t feel like just another tool—it’s like adding a new developer to the team who actually learns how we work,\" Regev told VentureBeat in a recent video call interview, adding that it has \"prevented over 800 issues per month from reaching production—some of them could have caused serious security vulnerabilities.\"Unlike code generation tools like GitHub Copilot or Cursor, Qodo isn’t trying to write new code. Instead, it specializes in reviewing it — using what it calls context engineering to understand not just what changed in a pull request, but why, how it aligns with business logic, and whether it follows internal best practices. \"You can call Claude Code or Cursor and in five minutes get 1,000 lines of code,\" said Itamar Friedman, co-founder and CEO of Qodo, in the same video call interview as with Regev. \"You have 40 minutes, and you can&#x27;t review that. So you need Qodo to actually review it.”For monday.com, this capability wasn’t just helpful — it was transformative.Code Review, at ScaleAt any given time, monday.com’s developers are shipping updates across hundreds of repositories and services. The engineering org works in tightly coordinated teams, each aligned with specific parts of the product: marketing, CRM, dev tools, internal platforms, and more.That’s where Qodo came in. The company’s platform uses AI not just to check for obvious bugs or style violations, but to evaluate whether a pull request follows team-specific conventions, architectural guidelines, and historical patterns. It does this by learning from your own codebase — training on previous PRs, comments, merges, and even Slack threads to understand how your team works.\"The comments Qodo gives aren’t generic—they reflect our values, our libraries, even our standards for things like feature flags and privacy,\" Regev said. \"It’s context-aware in a way traditional tools aren’t.\"What “Context Engineering” Actually MeansQodo calls its secret sauce context engineering — a system-level approach to managing everything the model sees when making a decision. This includes the PR code diff, of course, but also prior discussions, documentation, relevant files from the repo, even test results and configuration data.The idea is that language models don’t really “think” — they predict the next token based on the inputs they’re given. So the quality of their output depends almost entirely on the quality and structure of their inputs.As Dana Fine, Qodo’s community manager, put it in a blog post: “You’re not just writing prompts; you’re designing structured input under a fixed token limit. Every token is a design decision.”This isn’t just theory. In monday.com’s case, it meant Qodo could catch not only the obvious bugs, but the subtle ones that typically slip past human reviewers — hardcoded variables, missing fallbacks, or violations of cross-team architecture conventions.One example stood out. In a recent PR, Qodo flagged a line that inadvertently exposed a staging environment variable — something no human reviewer caught. Had it been merged, it might have caused problems in production. \"The hours we would spend on fixing this security leak and the legal issue that it would bring would be much more than the hours that we reduce from a pull-request,\" said Regev.Integration into the PipelineToday, Qodo is deeply integrated into monday.com’s development workflow, analyzing pull requests and surfacing context-aware recommendations based on prior team code reviews. “It doesn’t feel like just another tool... It feels like another teammate that joined the system — one who learns how we work,\" Regev noted. Developers receive suggestions during the review process and remain in control of final decisions — a human-in-the-loop model that was critical for adoption.Because Qodo integrated directly into GitHub via pull request actions and comments, Monday.com’s infrastructure team didn’t face a steep learning curve.“It’s just a GitHub action,” said Regev. “It creates a PR with the tests. It’s not like a separate tool we had to learn.”“The purpose is to actually help the developer learn the code, take ownership, give feedback to each other, and learn from that and establish the standards,\" added Friedman.The Results: Time Saved, Bugs PreventedSince rolling out Qodo more broadly, monday.com has seen measurable improvements across multiple teams.Internal analysis shows that developers save roughly an hour per pull request on average. Multiply that across thousands of PRs per month, and the savings quickly reach thousands of developer hours annually.These aren’t just cosmetic issues — many relate to business logic, security, or runtime stability. And because Qodo’s suggestions reflect monday.com’s actual conventions, developers are more likely to act on them.The system’s accuracy is rooted in its data-first design. Qodo trains on each company’s private codebase and historical data, adapting to different team styles and practices. It doesn’t rely on one-size-fits-all rules or external datasets. Everything is tailored.From Internal Tool to Product VisionRegev’s team was so impressed with Qodo’s impact that they’ve started planning deeper integrations between Qodo and Monday Dev, the developer-focused product line monday.com is building.The vision is to create a workflow where business context — tasks, tickets, customer feedback — flows directly into the code review layer. That way, reviewers can assess not just whether the code “works,” but whether it solves the right problem.“Before, we had linters, danger rules, static analysis... rule-based... you need to configure all the rules,\" Regev said. \"But it doesn’t know what you don’t know... Qodo... feels like it’s learning from our engineers.”This aligns closely with Qodo’s own roadmap. The company doesn’t just review code. It’s building a full platform of developer agents — including Qodo Gen for context-aware code generation, Qodo Merge for automated PR analysis, and Qodo Cover, a regression-testing agent that uses runtime validation to ensure test coverage.All of this is powered by Qodo’s own infrastructure, including its new open-source embedding model, Qodo-Embed-1-1.5B, which outperformed offerings from OpenAI and Salesforce on code retrieval benchmarks.What’s Next?Qodo is now offering its platform under a freemium model — free for individuals, discounted for startups through Google Cloud’s Perks program, and enterprise-grade for companies that need SSO, air-gapped deployment, or advanced controls.The company is already working with teams at NVIDIA, Intuit, and other Fortune 500 companies. And thanks to a recent partnership with Google Cloud, Qodo’s models are available directly inside Vertex AI’s Model Garden, making it easier to integrate into enterprise pipelines.\"Context engines will be the big story of 2026,\" Friedman said. \"Every enterprise will need to build their own second brain if they want AI that actually understands and helps them.\"As AI systems become more embedded in software development, tools like Qodo are showing how the right context — delivered at the right moment — can transform how teams build, ship, and scale code across the enterprise.",
          "feed_position": 5,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/6ITdq6fWHUqQToLpa2LMjE/9071464cc3dabd3e163518c89f5eb3b2/cfr0z3n_aerial_view_extended_view_of_hundreds_of_software_devel_dac48c75-82c8-48b8-9ca4-ff785e9ba5f3.png?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/baseten-takes-on-hyperscalers-with-new-ai-training-platform-that-lets-you",
          "published_at": "Mon, 10 Nov 2025 14:00:00 GMT",
          "title": "Baseten takes on hyperscalers with new AI training platform that lets you own your model weights",
          "standfirst": "Baseten, the AI infrastructure company recently valued at $2.15 billion, is making its most significant product pivot yet: a full-scale push into model training that could reshape how enterprises wean themselves off dependence on OpenAI and other closed-source AI providers.The San Francisco-based company announced Thursday the general availability of Baseten Training, an infrastructure platform designed to help companies fine-tune open-source AI models without the operational headaches of managing GPU clusters, multi-node orchestration, or cloud capacity planning. The move is a calculated expansion beyond Baseten&#x27;s core inference business, driven by what CTO Amir Haghighat describes as relentless customer demand and a strategic imperative to capture the full lifecycle of AI deployment.\"We had a captive audience of customers who kept coming to us saying, &#x27;Hey, I hate this problem,&#x27;\" Haghighat said in an interview. \"One of them told me, &#x27;Look, I bought a bunch of H100s from a cloud provider. I have to SSH in on Friday, run my fine-tuning job, then check on Monday to see if it worked. Sometimes I realize it just hasn&#x27;t been working all along.&#x27;\"The launch comes at a critical inflection point in enterprise AI adoption. As open-source models from Meta, Alibaba, and others increasingly rival proprietary systems in performance, companies face mounting pressure to reduce their reliance on expensive API calls to services like OpenAI&#x27;s GPT-5 or Anthropic&#x27;s Claude. But the path from off-the-shelf open-source model to production-ready custom AI remains treacherous, requiring specialized expertise in machine learning operations, infrastructure management, and performance optimization.Baseten&#x27;s answer: provide the infrastructure rails while letting companies retain full control over their training code, data, and model weights. It&#x27;s a deliberately low-level approach born from hard-won lessons.How a failed product taught Baseten what AI training infrastructure really needsThis isn&#x27;t Baseten&#x27;s first foray into training. The company&#x27;s previous attempt, a product called Blueprints launched roughly two and a half years ago, failed spectacularly — a failure Haghighat now embraces as instructive.\"We had created the abstraction layer a little too high,\" he explained. \"We were trying to create a magical experience, where as a user, you come in and programmatically choose a base model, choose your data and some hyperparameters, and magically out comes a model.\"The problem? Users didn&#x27;t have the intuition to make the right choices about base models, data quality, or hyperparameters. When their models underperformed, they blamed the product. Baseten found itself in the consulting business rather than the infrastructure business, helping customers debug everything from dataset deduplication to model selection.\"We became consultants,\" Haghighat said. \"And that&#x27;s not what we had set out to do.\"Baseten killed Blueprints and refocused entirely on inference, vowing to \"earn the right\" to expand again. That moment arrived earlier this year, driven by two market realities: the vast majority of Baseten&#x27;s inference revenue comes from custom models that customers train elsewhere, and competing training platforms were using restrictive terms of service to lock customers into their inference products.\"Multiple companies who were building fine-tuning products had in their terms of service that you as a customer cannot take the weights of the fine-tuned model with you somewhere else,\" Haghighat said. \"I understand why from their perspective — I still don&#x27;t think there is a big company to be made purely on just training or fine-tuning. The sticky part is in inference, the valuable part where value is unlocked is in inference, and ultimately the revenue is in inference.\"Baseten took the opposite approach: customers own their weights and can download them at will. The bet is that superior inference performance will keep them on the platform anyway.Multi-cloud GPU orchestration and sub-minute scheduling set Baseten apart from hyperscalersThe new Baseten Training product operates at what Haghighat calls \"the infrastructure layer\" — lower-level than the failed Blueprints experiment, but with opinionated tooling around reliability, observability, and integration with Baseten&#x27;s inference stack.Key technical capabilities include multi-node training support across clusters of NVIDIA H100 or B200 GPUs, automated checkpointing to protect against node failures, sub-minute job scheduling, and integration with Baseten&#x27;s proprietary Multi-Cloud Management (MCM) system. That last piece is critical: MCM allows Baseten to dynamically provision GPU capacity across multiple cloud providers and regions, passing cost savings to customers while avoiding the capacity constraints and multi-year contracts typical of hyperscaler deals.\"With hyperscalers, you don&#x27;t get to say, &#x27;Hey, give me three or four B200 nodes while my job is running, and then take it back from me and don&#x27;t charge me for it,&#x27;\" Haghighat said. \"They say, &#x27;No, you need to sign a three-year contract.&#x27; We don&#x27;t do that.\"Baseten&#x27;s approach mirrors broader trends in cloud infrastructure, where abstraction layers increasingly allow workloads to move fluidly across providers. When AWS experienced a major outage several weeks ago, Baseten&#x27;s inference services remained operational by automatically routing traffic to other cloud providers — a capability now extended to training workloads.The technical differentiation extends to Baseten&#x27;s observability tooling, which provides per-GPU metrics for multi-node jobs, granular checkpoint tracking, and a refreshed UI that surfaces infrastructure-level events. The company also introduced an \"ML Cookbook\" of open-source training recipes for popular models like Gemma, GPT OSS, and Qwen, designed to help users reach \"training success\" faster.Early adopters report 84% cost savings and 50% latency improvements with custom modelsTwo early customers illustrate the market Baseten is targeting: AI-native companies building specialized vertical solutions that require custom models.Oxen AI, a platform focused on dataset management and model fine-tuning, exemplifies the partnership model Baseten envisions. CEO Greg Schoeninger articulated a common strategic calculus, telling VentureBeat: \"Whenever I&#x27;ve seen a platform try to do both hardware and software, they usually fail at one of them. That&#x27;s why partnering with Baseten to handle infrastructure was the obvious choice.\"Oxen built its customer experience entirely on top of Baseten&#x27;s infrastructure, using the Baseten CLI to programmatically orchestrate training jobs. The system automatically provisions and deprovisions GPUs, fully concealing Baseten&#x27;s interface behind Oxen&#x27;s own. For one Oxen customer, AlliumAI — a startup bringing structure to messy retail data — the integration delivered 84% cost savings compared to previous approaches, reducing total inference costs from $46,800 to $7,530.\"Training custom LoRAs has always been one of the most effective ways to leverage open-source models, but it often came with infrastructure headaches,\" said Daniel Demillard, CEO of AlliumAI. \"With Oxen and Baseten, that complexity disappears. We can train and deploy models at massive scale without ever worrying about CUDA, which GPU to choose, or shutting down servers after training.\"Parsed, another early customer, tackles a different pain point: helping enterprises reduce dependence on OpenAI by creating specialized models that outperform generalist LLMs on domain-specific tasks. The company works in mission-critical sectors like healthcare, finance, and legal services, where model performance and reliability aren&#x27;t negotiable.\"Prior to switching to Baseten, we were seeing repetitive and degraded performance on our fine-tuned models due to bugs with our previous training provider,\" said Charles O&#x27;Neill, Parsed&#x27;s co-founder and chief science officer. \"On top of that, we were struggling to easily download and checkpoint weights after training runs.\"With Baseten, Parsed achieved 50% lower end-to-end latency for transcription use cases, spun up HIPAA-compliant EU deployments for testing within 48 hours, and kicked off more than 500 training jobs. The company also leveraged Baseten&#x27;s modified vLLM inference framework and speculative decoding — a technique that generates draft tokens to accelerate language model output — to cut latency in half for custom models.\"Fast models matter,\" O&#x27;Neill said. \"But fast models that get better over time matter more. A model that&#x27;s 2x faster but static loses to one that&#x27;s slightly slower but improving 10% monthly. Baseten gives us both — the performance edge today and the infrastructure for continuous improvement.\"Why training and inference are more interconnected than the industry realizesThe Parsed example illuminates a deeper strategic rationale for Baseten&#x27;s training expansion: the boundary between training and inference is blurrier than conventional wisdom suggests.Baseten&#x27;s model performance team uses the training platform extensively to create \"draft models\" for speculative decoding, a cutting-edge technique that can dramatically accelerate inference. The company recently announced it achieved 650+ tokens per second on OpenAI&#x27;s GPT OSS 120B model — a 60% improvement over its launch performance — using EAGLE-3 speculative decoding, which requires training specialized small models to work alongside larger target models.\"Ultimately, inference and training plug in more ways than one might think,\" Haghighat said. \"When you do speculative decoding in inference, you need to train the draft model. Our model performance team is a big customer of the training product to train these EAGLE heads on a continuous basis.\"This technical interdependence reinforces Baseten&#x27;s thesis that owning both training and inference creates defensible value. The company can optimize the entire lifecycle: a model trained on Baseten can be deployed with a single click to inference endpoints pre-optimized for that architecture, with deployment-from-checkpoint support for chat completion and audio transcription workloads.The approach contrasts sharply with vertically integrated competitors like Replicate or Modal, which also offer training and inference but with different architectural tradeoffs. Baseten&#x27;s bet is on lower-level infrastructure flexibility and performance optimization, particularly for companies running custom models at scale.As open-source AI models improve, enterprises see fine-tuning as the path away from OpenAI dependencyUnderpinning Baseten&#x27;s entire strategy is a conviction about the trajectory of open-source AI models — namely, that they&#x27;re getting good enough, fast enough, to unlock massive enterprise adoption through fine-tuning.\"Both closed and open-source models are getting better and better in terms of quality,\" Haghighat said. \"We don&#x27;t even need open source to surpass closed models, because as both of them are getting better, they unlock all these invisible lines of usefulness for different use cases.\"He pointed to the proliferation of reinforcement learning and supervised fine-tuning techniques that allow companies to take an open-source model and make it \"as good as the closed model, not at everything, but at this narrow band of capability that they want.\"That trend is already visible in Baseten&#x27;s Model APIs business, launched alongside Training earlier this year to provide production-grade access to open-source models. The company was the first provider to offer access to DeepSeek V3 and R1, and has since added models like Llama 4 and Qwen 3, optimized for performance and reliability. Model APIs serves as a top-of-funnel product: companies start with off-the-shelf open-source models, realize they need customization, move to Training for fine-tuning, and ultimately deploy on Baseten&#x27;s Dedicated Deployments infrastructure.Yet Haghighat acknowledged the market remains \"fuzzy\" around which training techniques will dominate. Baseten is hedging by staying close to the bleeding edge through its Forward Deployed Engineering team, which works hands-on with select customers on reinforcement learning, supervised fine-tuning, and other advanced techniques.\"As we do that, we will see patterns emerge about what a productized training product can look like that really addresses the user&#x27;s needs without them having to learn too much about how RL works,\" he said. \"Are we there as an industry? I would say not quite. I see some attempts at that, but they all seem like almost falling to the same trap that Blueprints fell into—a bit of a walled garden that ties the hands of AI folks behind their back.\"The roadmap ahead includes potential abstractions for common training patterns, expansion into image, audio, and video fine-tuning, and deeper integration of advanced techniques like prefill-decode disaggregation, which separates the initial processing of prompts from token generation to improve efficiency.Baseten faces crowded field but bets developer experience and performance will win enterprise customersBaseten enters an increasingly crowded market for AI infrastructure. Hyperscalers like AWS, Google Cloud, and Microsoft Azure offer GPU compute for training, while specialized providers like Lambda Labs, CoreWeave, and Together AI compete on price, performance, or ease of use. Then there are vertically integrated platforms like Hugging Face, Replicate, and Modal that bundle training, inference, and model hosting.Baseten&#x27;s differentiation rests on three pillars: its MCM system for multi-cloud capacity management, deep performance optimization expertise built from its inference business, and a developer experience tailored for production deployments rather than experimentation.The company&#x27;s recent $150 million Series D and $2.15 billion valuation provide runway to invest in both products simultaneously. Major customers include Descript, which uses Baseten for transcription workloads; Decagon, which runs customer service AI; and Sourcegraph, which powers coding assistants. All three operate in domains where model customization and performance are competitive advantages.Timing may be Baseten&#x27;s biggest asset. The confluence of improving open-source models, enterprise discomfort with dependence on proprietary AI providers, and growing sophistication around fine-tuning techniques creates what Haghighat sees as a sustainable market shift.\"There is a lot of use cases for which closed models have gotten there and open ones have not,\" he said. \"Where I&#x27;m seeing in the market is people using different training techniques — more recently, a lot of reinforcement learning and SFT — to be able to get this open model to be as good as the closed model, not at everything, but at this narrow band of capability that they want. That&#x27;s very palpable in the market.\"For enterprises navigating the complex transition from closed to open AI models, Baseten&#x27;s positioning offers a clear value proposition: infrastructure that handles the messy middle of fine-tuning while optimizing for the ultimate goal of performant, reliable, cost-effective inference at scale. The company&#x27;s insistence that customers own their model weights — a stark contrast to competitors using training as a lock-in mechanism — reflects confidence that technical excellence, not contractual restrictions, will drive retention.Whether Baseten can execute on this vision depends on navigating tensions inherent in its strategy: staying at the infrastructure layer without becoming consultants, providing power and flexibility without overwhelming users with complexity, and building abstractions at exactly the right level as the market matures. The company&#x27;s willingness to kill Blueprints when it failed suggests a pragmatism that could prove decisive in a market where many infrastructure providers over-promise and under-deliver.\"Through and through, we&#x27;re an inference company,\" Haghighat emphasized. \"The reason that we did training is at the service of inference.\"That clarity of purpose — treating training as a means to an end rather than an end in itself—may be Baseten&#x27;s most important strategic asset. As AI deployment matures from experimentation to production, the companies that solve the full stack stand to capture outsized value. But only if they avoid the trap of technology in search of a problem.At least Baseten&#x27;s customers no longer have to SSH into boxes on Friday and pray their training jobs complete by Monday. In the infrastructure business, sometimes the best innovation is simply making the painful parts disappear.",
          "content": "Baseten, the AI infrastructure company recently valued at $2.15 billion, is making its most significant product pivot yet: a full-scale push into model training that could reshape how enterprises wean themselves off dependence on OpenAI and other closed-source AI providers.The San Francisco-based company announced Thursday the general availability of Baseten Training, an infrastructure platform designed to help companies fine-tune open-source AI models without the operational headaches of managing GPU clusters, multi-node orchestration, or cloud capacity planning. The move is a calculated expansion beyond Baseten&#x27;s core inference business, driven by what CTO Amir Haghighat describes as relentless customer demand and a strategic imperative to capture the full lifecycle of AI deployment.\"We had a captive audience of customers who kept coming to us saying, &#x27;Hey, I hate this problem,&#x27;\" Haghighat said in an interview. \"One of them told me, &#x27;Look, I bought a bunch of H100s from a cloud provider. I have to SSH in on Friday, run my fine-tuning job, then check on Monday to see if it worked. Sometimes I realize it just hasn&#x27;t been working all along.&#x27;\"The launch comes at a critical inflection point in enterprise AI adoption. As open-source models from Meta, Alibaba, and others increasingly rival proprietary systems in performance, companies face mounting pressure to reduce their reliance on expensive API calls to services like OpenAI&#x27;s GPT-5 or Anthropic&#x27;s Claude. But the path from off-the-shelf open-source model to production-ready custom AI remains treacherous, requiring specialized expertise in machine learning operations, infrastructure management, and performance optimization.Baseten&#x27;s answer: provide the infrastructure rails while letting companies retain full control over their training code, data, and model weights. It&#x27;s a deliberately low-level approach born from hard-won lessons.How a failed product taught Baseten what AI training infrastructure really needsThis isn&#x27;t Baseten&#x27;s first foray into training. The company&#x27;s previous attempt, a product called Blueprints launched roughly two and a half years ago, failed spectacularly — a failure Haghighat now embraces as instructive.\"We had created the abstraction layer a little too high,\" he explained. \"We were trying to create a magical experience, where as a user, you come in and programmatically choose a base model, choose your data and some hyperparameters, and magically out comes a model.\"The problem? Users didn&#x27;t have the intuition to make the right choices about base models, data quality, or hyperparameters. When their models underperformed, they blamed the product. Baseten found itself in the consulting business rather than the infrastructure business, helping customers debug everything from dataset deduplication to model selection.\"We became consultants,\" Haghighat said. \"And that&#x27;s not what we had set out to do.\"Baseten killed Blueprints and refocused entirely on inference, vowing to \"earn the right\" to expand again. That moment arrived earlier this year, driven by two market realities: the vast majority of Baseten&#x27;s inference revenue comes from custom models that customers train elsewhere, and competing training platforms were using restrictive terms of service to lock customers into their inference products.\"Multiple companies who were building fine-tuning products had in their terms of service that you as a customer cannot take the weights of the fine-tuned model with you somewhere else,\" Haghighat said. \"I understand why from their perspective — I still don&#x27;t think there is a big company to be made purely on just training or fine-tuning. The sticky part is in inference, the valuable part where value is unlocked is in inference, and ultimately the revenue is in inference.\"Baseten took the opposite approach: customers own their weights and can download them at will. The bet is that superior inference performance will keep them on the platform anyway.Multi-cloud GPU orchestration and sub-minute scheduling set Baseten apart from hyperscalersThe new Baseten Training product operates at what Haghighat calls \"the infrastructure layer\" — lower-level than the failed Blueprints experiment, but with opinionated tooling around reliability, observability, and integration with Baseten&#x27;s inference stack.Key technical capabilities include multi-node training support across clusters of NVIDIA H100 or B200 GPUs, automated checkpointing to protect against node failures, sub-minute job scheduling, and integration with Baseten&#x27;s proprietary Multi-Cloud Management (MCM) system. That last piece is critical: MCM allows Baseten to dynamically provision GPU capacity across multiple cloud providers and regions, passing cost savings to customers while avoiding the capacity constraints and multi-year contracts typical of hyperscaler deals.\"With hyperscalers, you don&#x27;t get to say, &#x27;Hey, give me three or four B200 nodes while my job is running, and then take it back from me and don&#x27;t charge me for it,&#x27;\" Haghighat said. \"They say, &#x27;No, you need to sign a three-year contract.&#x27; We don&#x27;t do that.\"Baseten&#x27;s approach mirrors broader trends in cloud infrastructure, where abstraction layers increasingly allow workloads to move fluidly across providers. When AWS experienced a major outage several weeks ago, Baseten&#x27;s inference services remained operational by automatically routing traffic to other cloud providers — a capability now extended to training workloads.The technical differentiation extends to Baseten&#x27;s observability tooling, which provides per-GPU metrics for multi-node jobs, granular checkpoint tracking, and a refreshed UI that surfaces infrastructure-level events. The company also introduced an \"ML Cookbook\" of open-source training recipes for popular models like Gemma, GPT OSS, and Qwen, designed to help users reach \"training success\" faster.Early adopters report 84% cost savings and 50% latency improvements with custom modelsTwo early customers illustrate the market Baseten is targeting: AI-native companies building specialized vertical solutions that require custom models.Oxen AI, a platform focused on dataset management and model fine-tuning, exemplifies the partnership model Baseten envisions. CEO Greg Schoeninger articulated a common strategic calculus, telling VentureBeat: \"Whenever I&#x27;ve seen a platform try to do both hardware and software, they usually fail at one of them. That&#x27;s why partnering with Baseten to handle infrastructure was the obvious choice.\"Oxen built its customer experience entirely on top of Baseten&#x27;s infrastructure, using the Baseten CLI to programmatically orchestrate training jobs. The system automatically provisions and deprovisions GPUs, fully concealing Baseten&#x27;s interface behind Oxen&#x27;s own. For one Oxen customer, AlliumAI — a startup bringing structure to messy retail data — the integration delivered 84% cost savings compared to previous approaches, reducing total inference costs from $46,800 to $7,530.\"Training custom LoRAs has always been one of the most effective ways to leverage open-source models, but it often came with infrastructure headaches,\" said Daniel Demillard, CEO of AlliumAI. \"With Oxen and Baseten, that complexity disappears. We can train and deploy models at massive scale without ever worrying about CUDA, which GPU to choose, or shutting down servers after training.\"Parsed, another early customer, tackles a different pain point: helping enterprises reduce dependence on OpenAI by creating specialized models that outperform generalist LLMs on domain-specific tasks. The company works in mission-critical sectors like healthcare, finance, and legal services, where model performance and reliability aren&#x27;t negotiable.\"Prior to switching to Baseten, we were seeing repetitive and degraded performance on our fine-tuned models due to bugs with our previous training provider,\" said Charles O&#x27;Neill, Parsed&#x27;s co-founder and chief science officer. \"On top of that, we were struggling to easily download and checkpoint weights after training runs.\"With Baseten, Parsed achieved 50% lower end-to-end latency for transcription use cases, spun up HIPAA-compliant EU deployments for testing within 48 hours, and kicked off more than 500 training jobs. The company also leveraged Baseten&#x27;s modified vLLM inference framework and speculative decoding — a technique that generates draft tokens to accelerate language model output — to cut latency in half for custom models.\"Fast models matter,\" O&#x27;Neill said. \"But fast models that get better over time matter more. A model that&#x27;s 2x faster but static loses to one that&#x27;s slightly slower but improving 10% monthly. Baseten gives us both — the performance edge today and the infrastructure for continuous improvement.\"Why training and inference are more interconnected than the industry realizesThe Parsed example illuminates a deeper strategic rationale for Baseten&#x27;s training expansion: the boundary between training and inference is blurrier than conventional wisdom suggests.Baseten&#x27;s model performance team uses the training platform extensively to create \"draft models\" for speculative decoding, a cutting-edge technique that can dramatically accelerate inference. The company recently announced it achieved 650+ tokens per second on OpenAI&#x27;s GPT OSS 120B model — a 60% improvement over its launch performance — using EAGLE-3 speculative decoding, which requires training specialized small models to work alongside larger target models.\"Ultimately, inference and training plug in more ways than one might think,\" Haghighat said. \"When you do speculative decoding in inference, you need to train the draft model. Our model performance team is a big customer of the training product to train these EAGLE heads on a continuous basis.\"This technical interdependence reinforces Baseten&#x27;s thesis that owning both training and inference creates defensible value. The company can optimize the entire lifecycle: a model trained on Baseten can be deployed with a single click to inference endpoints pre-optimized for that architecture, with deployment-from-checkpoint support for chat completion and audio transcription workloads.The approach contrasts sharply with vertically integrated competitors like Replicate or Modal, which also offer training and inference but with different architectural tradeoffs. Baseten&#x27;s bet is on lower-level infrastructure flexibility and performance optimization, particularly for companies running custom models at scale.As open-source AI models improve, enterprises see fine-tuning as the path away from OpenAI dependencyUnderpinning Baseten&#x27;s entire strategy is a conviction about the trajectory of open-source AI models — namely, that they&#x27;re getting good enough, fast enough, to unlock massive enterprise adoption through fine-tuning.\"Both closed and open-source models are getting better and better in terms of quality,\" Haghighat said. \"We don&#x27;t even need open source to surpass closed models, because as both of them are getting better, they unlock all these invisible lines of usefulness for different use cases.\"He pointed to the proliferation of reinforcement learning and supervised fine-tuning techniques that allow companies to take an open-source model and make it \"as good as the closed model, not at everything, but at this narrow band of capability that they want.\"That trend is already visible in Baseten&#x27;s Model APIs business, launched alongside Training earlier this year to provide production-grade access to open-source models. The company was the first provider to offer access to DeepSeek V3 and R1, and has since added models like Llama 4 and Qwen 3, optimized for performance and reliability. Model APIs serves as a top-of-funnel product: companies start with off-the-shelf open-source models, realize they need customization, move to Training for fine-tuning, and ultimately deploy on Baseten&#x27;s Dedicated Deployments infrastructure.Yet Haghighat acknowledged the market remains \"fuzzy\" around which training techniques will dominate. Baseten is hedging by staying close to the bleeding edge through its Forward Deployed Engineering team, which works hands-on with select customers on reinforcement learning, supervised fine-tuning, and other advanced techniques.\"As we do that, we will see patterns emerge about what a productized training product can look like that really addresses the user&#x27;s needs without them having to learn too much about how RL works,\" he said. \"Are we there as an industry? I would say not quite. I see some attempts at that, but they all seem like almost falling to the same trap that Blueprints fell into—a bit of a walled garden that ties the hands of AI folks behind their back.\"The roadmap ahead includes potential abstractions for common training patterns, expansion into image, audio, and video fine-tuning, and deeper integration of advanced techniques like prefill-decode disaggregation, which separates the initial processing of prompts from token generation to improve efficiency.Baseten faces crowded field but bets developer experience and performance will win enterprise customersBaseten enters an increasingly crowded market for AI infrastructure. Hyperscalers like AWS, Google Cloud, and Microsoft Azure offer GPU compute for training, while specialized providers like Lambda Labs, CoreWeave, and Together AI compete on price, performance, or ease of use. Then there are vertically integrated platforms like Hugging Face, Replicate, and Modal that bundle training, inference, and model hosting.Baseten&#x27;s differentiation rests on three pillars: its MCM system for multi-cloud capacity management, deep performance optimization expertise built from its inference business, and a developer experience tailored for production deployments rather than experimentation.The company&#x27;s recent $150 million Series D and $2.15 billion valuation provide runway to invest in both products simultaneously. Major customers include Descript, which uses Baseten for transcription workloads; Decagon, which runs customer service AI; and Sourcegraph, which powers coding assistants. All three operate in domains where model customization and performance are competitive advantages.Timing may be Baseten&#x27;s biggest asset. The confluence of improving open-source models, enterprise discomfort with dependence on proprietary AI providers, and growing sophistication around fine-tuning techniques creates what Haghighat sees as a sustainable market shift.\"There is a lot of use cases for which closed models have gotten there and open ones have not,\" he said. \"Where I&#x27;m seeing in the market is people using different training techniques — more recently, a lot of reinforcement learning and SFT — to be able to get this open model to be as good as the closed model, not at everything, but at this narrow band of capability that they want. That&#x27;s very palpable in the market.\"For enterprises navigating the complex transition from closed to open AI models, Baseten&#x27;s positioning offers a clear value proposition: infrastructure that handles the messy middle of fine-tuning while optimizing for the ultimate goal of performant, reliable, cost-effective inference at scale. The company&#x27;s insistence that customers own their model weights — a stark contrast to competitors using training as a lock-in mechanism — reflects confidence that technical excellence, not contractual restrictions, will drive retention.Whether Baseten can execute on this vision depends on navigating tensions inherent in its strategy: staying at the infrastructure layer without becoming consultants, providing power and flexibility without overwhelming users with complexity, and building abstractions at exactly the right level as the market matures. The company&#x27;s willingness to kill Blueprints when it failed suggests a pragmatism that could prove decisive in a market where many infrastructure providers over-promise and under-deliver.\"Through and through, we&#x27;re an inference company,\" Haghighat emphasized. \"The reason that we did training is at the service of inference.\"That clarity of purpose — treating training as a means to an end rather than an end in itself—may be Baseten&#x27;s most important strategic asset. As AI deployment matures from experimentation to production, the companies that solve the full stack stand to capture outsized value. But only if they avoid the trap of technology in search of a problem.At least Baseten&#x27;s customers no longer have to SSH into boxes on Friday and pray their training jobs complete by Monday. In the infrastructure business, sometimes the best innovation is simply making the painful parts disappear.",
          "feed_position": 6,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/789u1ePM0udlJqqwgxRkPd/6fd3661185f8285972d95d68249faa03/nuneybits_Vector_art_of_multi-cloud_nodes_interconnected_global_df5a72fb-1f71-4b95-8b12-94c1e8def7d6.webp?w=300&q=30"
        }
      ],
      "featured_image": "https://images.ctfassets.net/jdtwqhzvc2n1/6PAfmxLhH2Yv7BpN8vczIt/f33baa764e279c49d01c5a10da8eef61/nuneybits_Vector_art_of_a_GPU_made_out_of_computer_code_and_the_59f97a50-f492-452f-bd5d-1d6e6e904c4a.webp?w=300&q=30",
      "popularity_score": 2016.2614958333334,
      "ai_summary": [
        "Baidu released a new AI model, ERNIE-4.5-VL-28B-A3B-Thinking.",
        "Baidu claims the model outperforms Google and OpenAI competitors.",
        "The model uses a fraction of the computing resources.",
        "The model can understand images, videos, and documents.",
        "The model's efficiency is a key differentiator."
      ]
    },
    {
      "id": "cluster_17",
      "coverage": 1,
      "updated_at": "Tue, 11 Nov 2025 21:34:10 +0000",
      "title": "Google says new cloud-based “Private AI Compute” is just as secure as local processing",
      "neutral_headline": "Google says new cloud-based “Private AI Compute” is just as secure as local processing",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/google/2025/11/google-says-new-cloud-based-private-ai-compute-is-just-as-secure-as-local-processing/",
          "published_at": "Tue, 11 Nov 2025 21:34:10 +0000",
          "title": "Google says new cloud-based “Private AI Compute” is just as secure as local processing",
          "standfirst": "New system allows devices to connect directly to secure space in Google's AI servers.",
          "content": "Google’s current mission is to weave generative AI into as many products as it can, getting everyone accustomed to, and maybe even dependent on, working with confabulatory robots. That means it needs to feed the bots a lot of your data, and that’s getting easier with the company’s new Private AI Compute. Google claims its new secure cloud environment will power better AI experiences without sacrificing your privacy. The pitch sounds a lot like Apple’s Private Cloud Compute. Google’s Private AI Compute runs on “one seamless Google stack” powered by the company’s custom Tensor Processing Units (TPUs). These chips have integrated secure elements, and the new system allows devices to connect directly to the protected space via an encrypted link. Google’s TPUs rely on an AMD-based Trusted Execution Environment (TEE) that encrypts and isolates memory from the host. Theoretically, that means no one else—not even Google itself—can access your data. Google says independent analysis by NCC Group shows that Private AI Compute meets its strict privacy guidelines.Read full article Comments",
          "feed_position": 1,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/Google_Private_Inference-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/Google_Private_Inference-1152x648.jpg",
      "popularity_score": 363.83094027777776,
      "ai_summary": [
        "Google announced a new cloud-based system called Private AI Compute.",
        "The system allows devices to connect to secure AI servers.",
        "The system aims to provide security comparable to local processing.",
        "The system is designed for enhanced privacy and security.",
        "The new system is cloud-based for AI processing."
      ]
    },
    {
      "id": "cluster_8",
      "coverage": 1,
      "updated_at": "Tue, 11 Nov 2025 23:26:04 +0000",
      "title": "The Mac calculator’s original design came from letting Steve Jobs play with menus for ten minutes",
      "neutral_headline": "The Mac calculator’s original design came from letting Steve Jobs play with menus for ten minutes",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2025/11/the-mac-calculators-original-design-came-from-letting-steve-jobs-play-with-sliders-for-ten-minutes/",
          "published_at": "Tue, 11 Nov 2025 23:26:04 +0000",
          "title": "The Mac calculator’s original design came from letting Steve Jobs play with menus for ten minutes",
          "standfirst": "In 1982, a young Mac developer turned Jobs into a UI designer—and accidentally invented a new technique.",
          "content": "In February 1982, Apple employee #8 Chris Espinosa faced a problem that would feel familiar to anyone who has ever had a micromanaging boss: Steve Jobs wouldn’t stop critiquing his calculator design for the Mac. After days of revision cycles, the 21-year-old programmer found an elegant solution: He built what he called the “Steve Jobs Roll Your Own Calculator Construction Set” and let Jobs design it himself. This delightful true story comes from Andy Hertzfeld’s Folklore.org, a legendary tech history site that chronicles the development of the original Macintosh, which was released in January 1984. I ran across the story again recently and thought it was worth sharing as a fun anecdote in an age where influential software designs often come by committee. Design by menu Chris Espinosa started working for Apple at age 14 in 1976 as the company’s youngest employee. By 1981, while studying at UC Berkeley, Jobs convinced Espinosa to drop out and work on the Mac team full time.Read full article Comments",
          "feed_position": 0,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/calc_hero_1-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/calc_hero_1-1152x648.jpg",
      "popularity_score": 354.69594027777777,
      "ai_summary": [
        "The original Mac calculator design was created in 1982.",
        "A young Mac developer involved Steve Jobs in the design.",
        "Jobs played with menus for ten minutes during the process.",
        "A new UI design technique was accidentally invented.",
        "The developer turned Jobs into a UI designer."
      ]
    },
    {
      "id": "cluster_18",
      "coverage": 1,
      "updated_at": "Tue, 11 Nov 2025 21:20:32 +0000",
      "title": "Ryanair tries forcing app downloads by eliminating paper boarding passes",
      "neutral_headline": "Ryanair tries forcing app downloads by eliminating paper boarding passes",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2025/11/ryanair-tries-forcing-app-downloads-by-eliminating-paper-boarding-passes/",
          "published_at": "Tue, 11 Nov 2025 21:20:32 +0000",
          "title": "Ryanair tries forcing app downloads by eliminating paper boarding passes",
          "standfirst": "Ryanair CEO admits \"there’ll be some teething problems.\"",
          "content": "Ryanair is trying to force users to download its mobile app by eliminating paper boarding passes, starting on November 12. As announced in February and subsequently delayed from earlier start dates, Europe’s biggest airline is moving to digital-only boarding passes, meaning customers will no longer be able to print physical ones. In order to access their boarding passes, Ryanair flyers will have to download Ryanair’s app. “Almost 100 percent of passengers have smartphones, and we want to move everybody onto that smartphone technology,” Ryanair CEO Michael O’Leary said recently on The Independent’s daily travel podcast.Read full article Comments",
          "feed_position": 2,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-1200216952-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-1200216952-1152x648.jpg",
      "popularity_score": 341.60371805555553,
      "ai_summary": [
        "Ryanair is eliminating paper boarding passes.",
        "The airline is encouraging app downloads.",
        "Ryanair's CEO admits there will be issues.",
        "The move aims to increase app usage.",
        "Passengers will need to use the Ryanair app."
      ]
    },
    {
      "id": "cluster_43",
      "coverage": 1,
      "updated_at": "Tue, 11 Nov 2025 19:21:30 +0000",
      "title": "Reddit mod jailed for sharing movie sex scenes in rare “moral rights” verdict",
      "neutral_headline": "Reddit User Jailed for Sharing Movie Sex Scenes",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/11/reddit-mod-jailed-for-sharing-movie-sex-scenes-in-rare-moral-rights-verdict/",
          "published_at": "Tue, 11 Nov 2025 19:21:30 +0000",
          "title": "Reddit mod jailed for sharing movie sex scenes in rare “moral rights” verdict",
          "standfirst": "Redditor confessed to violating actresses' \"moral rights\" in landmark ruling.",
          "content": "A Reddit moderator known as “KlammereFyr” was recently convicted by a Danish court after clipping and posting hundreds of nude scenes that actresses filmed for movies and TV shows but apparently never expected to be shared out of context. As TorrentFreak reported, dozens of actresses had complained about the mod’s sub-reddit, “SeDetForPlottet” (WatchItForthePlot), with some feeling “molested or abused.” Demanding Danish police put an end to the forum, the Rights Alliance—representing the Danish Actors’ Association, two broadcasters, and other rightsholders—pushed for a criminal probe.Read full article Comments",
          "feed_position": 3,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/reddit-s3xy-time-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/reddit-s3xy-time-1152x648.jpg",
      "popularity_score": 329.61982916666665,
      "ai_summary": [
        "A Reddit moderator was jailed for sharing movie sex scenes, violating actresses' moral rights.",
        "The ruling is considered a landmark decision in the realm of moral rights.",
        "The Redditor confessed to the violations, leading to the conviction.",
        "The case highlights the legal implications of sharing copyrighted content online.",
        "This case sets a precedent for future cases involving moral rights violations."
      ]
    },
    {
      "id": "cluster_58",
      "coverage": 1,
      "updated_at": "Tue, 11 Nov 2025 17:18:58 +0000",
      "title": "US states could lose $21 billion of broadband grants after Trump overhaul",
      "neutral_headline": "US States Could Lose Broadband Grants Due to Bill",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/11/us-states-could-lose-21-billion-of-broadband-grants-after-trump-overhaul/",
          "published_at": "Tue, 11 Nov 2025 17:18:58 +0000",
          "title": "US states could lose $21 billion of broadband grants after Trump overhaul",
          "standfirst": "Ernst bill would send broadband grant money to Treasury for deficit reduction.",
          "content": "A Senate Republican has drafted legislation that would effectively cut a $42 billion broadband deployment program in half. The bill would complement the Trump administration overhaul of the $42.45 billion Broadband Equity, Access, and Deployment (BEAD) program. The administration required states to rewrite their grant plans, reducing the overall projected spending and diverting some of the money from fiber projects to satellite. The result is that over $21 billion is projected to be left over after money is allocated to projects that expand broadband access. Current US law allows nondeployment funds to be used for other broadband-related purposes, like providing Wi-Fi and Internet-capable devices to US residents. But a draft bill by Sen. Joni Ernst (R-Iowa) would change the law to redirect all the remaining money to the US Treasury for deficit reduction.Read full article Comments",
          "feed_position": 4,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/shredded-money-1152x648-1762880813.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/shredded-money-1152x648-1762880813.jpg",
      "popularity_score": 302.5776069444444,
      "ai_summary": [
        "A bill could cause US states to lose $21 billion in broadband grants.",
        "The bill, proposed by Ernst, would redirect grant money to the Treasury.",
        "This change aims to reduce the national deficit.",
        "The grants were intended to expand broadband access across states.",
        "The potential loss could hinder broadband infrastructure development."
      ]
    },
    {
      "id": "cluster_70",
      "coverage": 1,
      "updated_at": "Tue, 11 Nov 2025 15:54:11 +0000",
      "title": "You won’t believe the excuses lawyers have after getting busted for using AI",
      "neutral_headline": "Lawyers Offer Excuses After AI Use Violations",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/11/lawyers-keep-giving-weak-sauce-excuses-for-fake-ai-citations-in-court-docs/",
          "published_at": "Tue, 11 Nov 2025 15:54:11 +0000",
          "title": "You won’t believe the excuses lawyers have after getting busted for using AI",
          "standfirst": "I got hacked; I lost my login; it was a rough draft; toggling windows is hard.",
          "content": "Amid what one judge called an “epidemic” of fake AI-generated case citations bogging down courts, some common excuses are emerging from lawyers hoping to dodge the most severe sanctions for filings deemed misleading. Using a database compiled by French lawyer and AI researcher Damien Charlotin, Ars reviewed 23 cases where lawyers were sanctioned for AI hallucinations. In many, judges noted that the simplest path to avoid or diminish sanctions was to admit that AI was used as soon as it’s detected, act humble, self-report the error to relevant legal associations, and voluntarily take classes on AI and law. But not every lawyer takes the path of least resistance, Ars’ review found, with many instead offering excuses that no judge found credible. Some even lie about their AI use, judges concluded. Since 2023—when fake AI citations started being publicized—the most popular excuse has been that the lawyer didn’t know AI was used to draft a filing.Read full article Comments",
          "feed_position": 6,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/ai-shrugging-lawyer-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/ai-shrugging-lawyer-1152x648.jpg",
      "popularity_score": 302.16455138888887,
      "ai_summary": [
        "Lawyers have offered various excuses after being caught using AI improperly.",
        "Excuses include claims of being hacked and losing login credentials.",
        "Other excuses cited rough drafts and difficulty toggling windows.",
        "These excuses were offered after being caught using AI in legal work.",
        "The incidents highlight the challenges of AI integration in law."
      ]
    },
    {
      "id": "cluster_84",
      "coverage": 1,
      "updated_at": "Tue, 11 Nov 2025 12:30:51 +0000",
      "title": "ClickFix may be the biggest security threat your family has never heard of",
      "neutral_headline": "ClickFix Could Be a Significant Security Threat",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/security/2025/11/clickfix-may-be-the-biggest-security-threat-your-family-has-never-heard-of/",
          "published_at": "Tue, 11 Nov 2025 12:30:51 +0000",
          "title": "ClickFix may be the biggest security threat your family has never heard of",
          "standfirst": "Relatively new technique can bypass many endpoint protections.",
          "content": "Over the past year, scammers have ramped up a new way to infect the computers of unsuspecting people. The increasingly common method, which many potential targets have yet to learn of, is quick, bypasses most endpoint protections, and works against both macOS and Windows users. ClickFix often starts with an email sent from a hotel that the target has a pending registration with and references the correct registration information. In other cases, ClickFix attacks begin with a WhatsApp message. In still other cases, the user receives the URL at the top of Google results for a search query. Once the mark accesses the malicious site referenced, it presents a CAPTCHA challenge or other pretext requiring user confirmation. The user receives an instruction to copy a string of text, open a terminal window, paste it in, and press Enter. One line is all it takes Once entered, the string of text causes the PC or Mac to surreptitiously visit a scammer-controlled server and download malware. Then, the machine automatically installs it—all with no indication to the target. With that, users are infected, usually with credential-stealing malware. Security firms say ClickFix campaigns have run rampant. The lack of awareness of the technique, combined with the links also coming from known addresses or in search results, and the ability to bypass some endpoint protections are all factors driving the growth.Read full article Comments",
          "feed_position": 8,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2020/10/malware-1000x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2020/10/malware-1000x648.jpg",
      "popularity_score": 273.7756625,
      "ai_summary": [
        "ClickFix is a relatively new security threat that is not widely known.",
        "It can bypass many endpoint protection measures.",
        "The technique poses a risk to family security.",
        "ClickFix's effectiveness makes it a significant concern.",
        "The threat's novelty means many are unaware of its existence."
      ]
    },
    {
      "id": "cluster_71",
      "coverage": 1,
      "updated_at": "Tue, 11 Nov 2025 15:15:01 +0000",
      "title": "Pirelli’s Cyber Tire might become highway agencies’ newest assistant",
      "neutral_headline": "Pirelli's Cyber Tire Could Assist Highway Agencies",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/cars/2025/11/tires-that-talk-to-your-town-about-the-state-of-roads-are-on-the-way/",
          "published_at": "Tue, 11 Nov 2025 15:15:01 +0000",
          "title": "Pirelli’s Cyber Tire might become highway agencies’ newest assistant",
          "standfirst": "Pirelli's Cyber Tire can work for the greater good as well as the driver.",
          "content": "Pirelli’s sensor-embedded Cyber Tire is starting to find a whole new niche helping traffic agencies. When we first learned of the smart tire, it was making its debut fitted to McLaren’s then-new plug-in hybrid supercar. As an alternative to a tire pressure monitoring system fitted to the car’s wheels, the Cyber Tire wirelessly reports its temperature and pressure to its car via Bluetooth Low Energy, along with some specific information about the tire itself. Since then, Pirelli has continued to develop the technology. When it created Cyber Tires for the Pagani Utopia, it allowed a car to tailor its antilock braking and electronic stability control to the specific rubber fitted to the wheels. Right now, a car’s ABS or ESC will be tuned regardless of the tires it’s fitted to. But a high-performance summer tire acts quite differently from a winter tire, not just because of the composition of the rubber but also due to the tread pattern, depth, and stiffness, not to mention factors like sidewall stiffness. And the Utopia can take advantage of that fact.Read full article Comments",
          "feed_position": 7,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-875478034-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-875478034-1152x648.jpg",
      "popularity_score": 270.5117736111111,
      "ai_summary": [
        "Pirelli's Cyber Tire can benefit both drivers and highway agencies.",
        "The tire can provide data for road maintenance and safety.",
        "It can also enhance driver safety through real-time information.",
        "The technology has potential for improving traffic management.",
        "The Cyber Tire represents an advancement in automotive technology."
      ]
    },
    {
      "id": "cluster_95",
      "coverage": 1,
      "updated_at": "Tue, 11 Nov 2025 00:39:13 +0000",
      "title": "Neutron rocket’s debut slips into mid-2026 as company seeks success from the start",
      "neutral_headline": "Neutron Rocket Debut Delayed to Mid-2026",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2025/11/neutron-rockets-debut-slips-into-mid-2026-as-company-seeks-success-from-the-start/",
          "published_at": "Tue, 11 Nov 2025 00:39:13 +0000",
          "title": "Neutron rocket’s debut slips into mid-2026 as company seeks success from the start",
          "standfirst": "\"Those who have failed to deliver are numerous.\"",
          "content": "During an earnings call on Monday, Rocket Lab chief executive Peter Beck announced that the company’s medium-lift launch vehicle, Neutron, would not launch this year. For anyone with the slightest understanding of the challenges involved in bringing a new rocket to the launch pad, as well as a calendar, the delay does not come as a surprise. Although Rocket Lab had been holding onto the possibility of launching Neutron this year publicly, it has been clear for months that a slip into 2026 was inevitable. According to Beck, speaking during a third-quarter 2025 earnings call, the new timeline has the company bringing Neutron to Launch Complex 2 at Wallops Flight Facility in Virginia during the first quarter of next year. The first launch is scheduled to occur “thereafter,” according to the company’s plans.Read full article Comments",
          "feed_position": 9,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/Neutron-Stack-Deploy-Artwork_med__ScaleHeightWzg1MF0-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/Neutron-Stack-Deploy-Artwork_med__ScaleHeightWzg1MF0-1152x648.jpg",
      "popularity_score": 243,
      "ai_summary": [
        "The Neutron rocket's debut has been pushed back to mid-2026.",
        "The company aims for success from the very beginning of the project.",
        "The delay reflects the complexities of rocket development.",
        "The company acknowledges the failures of others in the industry.",
        "The revised timeline allows for more thorough testing and preparation."
      ]
    },
    {
      "id": "cluster_102",
      "coverage": 1,
      "updated_at": "Mon, 10 Nov 2025 23:06:42 +0000",
      "title": "Researchers isolate memorization from problem-solving in AI neural networks",
      "neutral_headline": "Researchers Separate Memorization from Problem Solving in AI",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/ai/2025/11/study-finds-ai-models-store-memories-and-logic-in-different-neural-regions/",
          "published_at": "Mon, 10 Nov 2025 23:06:42 +0000",
          "title": "Researchers isolate memorization from problem-solving in AI neural networks",
          "standfirst": "Basic arithmetic ability lives in the memorization pathways, not logic circuits.",
          "content": "When engineers build AI language models like GPT-5 from training data, at least two major processing features emerge: memorization (reciting exact text they’ve seen before, like famous quotes or passages from books) and what you might call “reasoning” (solving new problems using general principles). New research from AI startup Goodfire.ai provides the first potentially clear evidence that these different functions actually work through completely separate neural pathways in the model’s architecture. The researchers discovered that this separation proves remarkably clean. In a preprint paper released in late October, they described that when they removed the memorization pathways, models lost 97 percent of their ability to recite training data verbatim but kept nearly all their “logical reasoning” ability intact. For example, at layer 22 in Allen Institute for AI’s OLMo-7B language model, the researchers ranked all the weight components (the mathematical values that process information) from high to low based on a measure called “curvature” (which we’ll explain more below). When they examined these ranked components, the bottom 50 percent of weight components showed 23 percent higher activation on memorized data, while the top 10 percent showed 26 percent higher activation on general, non-memorized text.Read full article Comments",
          "feed_position": 11,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/07/surprised_robot_2-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/07/surprised_robot_2-1152x648.jpg",
      "popularity_score": 148,
      "ai_summary": [
        "Researchers have isolated memorization from problem-solving in AI networks.",
        "Basic arithmetic ability is found in memorization pathways, not logic circuits.",
        "This discovery helps understand how AI learns and processes information.",
        "It provides insights into the architecture of neural networks.",
        "The findings could improve AI's ability to solve complex problems."
      ]
    },
    {
      "id": "cluster_122",
      "coverage": 1,
      "updated_at": "Mon, 10 Nov 2025 16:29:34 +0000",
      "title": "The Running Man’s final trailer amps up the high-octane action",
      "neutral_headline": "The Running Man's Final Trailer Showcases Action",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/culture/2025/11/the-running-mans-final-trailer-amps-up-the-high-octane-action/",
          "published_at": "Mon, 10 Nov 2025 16:29:34 +0000",
          "title": "The Running Man’s final trailer amps up the high-octane action",
          "standfirst": "Get ready for what could be the \"biggest underdog win in game show history.\"",
          "content": "It’s shaping up to be an excellent season for Stephen King adaptations. In September, we got The Long Walk, an excellent (though harrowing) adaptation of King’s 1979 Richard Bachman novel. Last month, HBO debuted its new series IT: Welcome to Derry, which explores the mythology and origins of Pennywise the killer clown. And this Friday is the premiere of The Running Man, director Edgar Wright’s (Shaun of the Dead, Baby Driver, Last Night in Soho) take on King’s novel of the same name. So naturally Paramount has released a final trailer to lure us to the theater. As previously reported, the 1987 action film starring Schwarzenegger was only loosely based on King’s novel, preserving the basic concept and very little else in favor of more sci-fi gadgetry and high-octane action. It was a noisy, entertaining romp—and very late ’80s—but it lacked King’s subtler satirical tone. Wright expressed interest in adapting his own version of The Running Man in 2017, and Paramount greenlit the project four years later. Wright and co-screenwriter Michael Bacall envisioned their film as less of a remake and more of a faithful adaptation of King’s original novel. (We’ll see if that faithfulness extends to the novel’s bleak ending.) Per the official premise:Read full article Comments",
          "feed_position": 16,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/running1-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/running1-1152x648.jpg",
      "popularity_score": 148,
      "ai_summary": [
        "The final trailer for The Running Man amps up the action.",
        "The trailer teases what could be a major underdog victory.",
        "The film promises high-octane action sequences.",
        "The trailer builds anticipation for the movie's release.",
        "The film is expected to be a thrilling experience."
      ]
    },
    {
      "id": "cluster_115",
      "coverage": 1,
      "updated_at": "Mon, 10 Nov 2025 18:41:42 +0000",
      "title": "Apple TV execs dismiss introducing an ad tier, buying Warner Bros.",
      "neutral_headline": "Apple TV Executives Discuss Ad Tier and Warner Bros",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/apple/2025/11/apple-has-no-plans-to-bring-ads-to-apple-tv/",
          "published_at": "Mon, 10 Nov 2025 18:41:42 +0000",
          "title": "Apple TV execs dismiss introducing an ad tier, buying Warner Bros.",
          "standfirst": "Apple execs say Apple TV shows and movies are about \"emotional\" experiences.",
          "content": "The heads of Apple TV have “no plans” to bring ads to the streaming service, balking, at least for now, at a strategy that has driven success for Apple’s streaming rivals. In its November 2025 issue, British movie magazine Screen International asked Eddy Cue, senior vice president of Apple Services, if there are plans to launch an ad-based subscription tier for Apple TV. Cue responded: Nothing at this time. … I don’t want to say no forever, but there are no plans. If we can stay aggressive with our pricing, it’s better for consumers not to get interrupted with ads. The comments follow reports over the years suggesting that Apple has been seeking knowledge on how to build a streaming ads business. Most recently, The Telegraph reported that Apple TV executives met with the United Kingdom’s ratings body, Barb, to discuss what tracking ads on Apple TV would look like. In 2023, Apple hired advertising exec Lauren Fry as head of video and Apple News ad sales.Read full article Comments",
          "feed_position": 13,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/Severance_Photo_0201-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/Severance_Photo_0201-1152x648.jpg",
      "popularity_score": 145,
      "ai_summary": [
        "Apple TV executives are not planning to introduce an ad tier.",
        "They also dismissed the idea of buying Warner Bros.",
        "Apple executives emphasize the emotional experience of shows and movies.",
        "The company is focused on creating high-quality content.",
        "Apple prioritizes a premium, ad-free viewing experience."
      ]
    },
    {
      "id": "cluster_98",
      "coverage": 1,
      "updated_at": "Mon, 10 Nov 2025 23:34:09 +0000",
      "title": "Intuitive Machines—known for its Moon landers—will become a military contractor",
      "neutral_headline": "Intuitive Machines Becomes Military Contractor",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2025/11/intuitive-machines-known-for-its-moon-landers-will-become-a-military-contractor/",
          "published_at": "Mon, 10 Nov 2025 23:34:09 +0000",
          "title": "Intuitive Machines—known for its Moon landers—will become a military contractor",
          "standfirst": "\"They've been Ford Aerospace, Space Systems/Loral, Maxar, Lanteris, and now it'll be Intuitive Machines.\"",
          "content": "Intuitive Machines announced last week an $800 million acquisition that will catapult the one-time startup into the space industry establishment. The company’s planned purchase of Lanteris Space Systems, a satellite manufacturer you may have never heard of, is rather significant. Lanteris is the latest addition to a line of corporate brands that dates back to 1957. Until last month, the company was known as Maxar Space Systems. Its acquisition by Intuitive Machines would be perhaps the industry’s most evident example of a “New Space” firm buying up an “Old Space” company. The deal would help Intuitive Machines expand beyond its core competency of Moon missions to the broader sector of satellite manufacturing and space services. Lanteris has been owned since 2023 by Advent International, a private equity firm. The transaction is expected to close early next year, subject to “customary regulatory approvals and closing conditions,” according to Intuitive Machines.Read full article Comments",
          "feed_position": 10,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/lanterisppe-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/lanterisppe-1152x648.jpg",
      "popularity_score": 133,
      "ai_summary": [
        "Intuitive Machines, known for Moon landers, will become a military contractor.",
        "The company has a history of working with various aerospace entities.",
        "This shift reflects a diversification of the company's business.",
        "The move could open up new opportunities for the company.",
        "The company's expertise will be applied to military projects."
      ]
    },
    {
      "id": "cluster_104",
      "coverage": 1,
      "updated_at": "Mon, 10 Nov 2025 22:10:04 +0000",
      "title": "Canada fought measles and measles won; virus now endemic after 1998 elimination",
      "neutral_headline": "Canada Fought Measles, Measles Won",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/health/2025/11/canada-fought-measles-and-measles-won-virus-now-endemic-after-1998-elimination/",
          "published_at": "Mon, 10 Nov 2025 22:10:04 +0000",
          "title": "Canada fought measles and measles won; virus now endemic after 1998 elimination",
          "standfirst": "Canada's loss also represents a loss of regional elimination for the Americas.",
          "content": "Canada has lost its measles elimination status, meaning the highly infectious virus is considered endemic once again in the country, The Pan American Health Organization (PAHO) announced Monday. The determination was made by a committee of PAHO experts, who spent last week poring over disease data to assess the measles status of countries across the entire region. The fact that Canada has lost its elimination status means that the region of the Americas overall has also lost the status, which it achieved in 2016. Of the 35 countries and territories in the region—a health region designated by the World Health Organization—Canada is currently the only country where measles is considered to be spreading endemically, though other countries, namely the US and Mexico, are headed in the same direction. Measles is considered eliminated when a country can go 12 months without continuous local spread. Sporadic cases brought in from international travel can continue to occur, potentially causing limited outbreaks. But elimination is lost and endemicity is declared only when transmission is sustained over the course of a year.Read full article Comments",
          "feed_position": 12,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-2224876606-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-2224876606-1152x648.jpg",
      "popularity_score": 133,
      "ai_summary": [
        "Measles is now endemic in Canada after its 1998 elimination.",
        "Canada's loss also represents a loss of regional elimination for the Americas.",
        "The resurgence of measles poses public health challenges.",
        "The situation highlights the importance of vaccination programs.",
        "The spread of measles underscores the need for preventative measures."
      ]
    },
    {
      "id": "cluster_116",
      "coverage": 1,
      "updated_at": "Mon, 10 Nov 2025 18:09:28 +0000",
      "title": "New project brings strong Linux compatibility to more classic Windows games",
      "neutral_headline": "New Project Brings Linux Compatibility to Windows Games",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gaming/2025/11/new-project-brings-strong-linux-compatibility-to-more-classic-windows-games/",
          "published_at": "Mon, 10 Nov 2025 18:09:28 +0000",
          "title": "New project brings strong Linux compatibility to more classic Windows games",
          "standfirst": "But author warns that Direct3D 7 \"is a land of highly cursed API inter-operability.\"",
          "content": "For years now, Valve has been slowly improving the capabilities of the Proton compatibility layer that lets thousands of Windows games work seamlessly on the Linux-based SteamOS. But Valve’s Windows-to-Linux compatibility layer generally only extends back to games written for Direct3D 8, the proprietary Windows graphics API Microsoft released in late 2000. Now, a new open source project is seeking to extend Linux interoperability further back into PC gaming history. The d7vk project describes itself as “a Vulkan-based translation layer for Direct3D 7 [D3D7], which allows running 3D applications on Linux using Wine.” More options are always welcome The new project isn’t the first attempt to get Direct3D 7 games running on Linux. Wine‘s own built-in WineD3D compatibility layer has supported D3D7 in some form or another for at least two decades now. But the new d7vk project instead branches off the existing dxvk compatibility layer, which is already used by Valve’s Proton for SteamOS and which reportedly offers better performance than WineD3D on many games.Read full article Comments",
          "feed_position": 14,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/arxfatalis.png"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/arxfatalis.png",
      "popularity_score": 133,
      "ai_summary": [
        "A new project enhances Linux compatibility for classic Windows games.",
        "The project's author warns of challenges with Direct3D 7.",
        "Direct3D 7 is described as having problematic API interoperability.",
        "The project aims to improve the gaming experience on Linux.",
        "The project is still under development and faces technical hurdles."
      ]
    },
    {
      "id": "cluster_119",
      "coverage": 1,
      "updated_at": "Mon, 10 Nov 2025 17:37:14 +0000",
      "title": "Runaway black hole mergers may have built supermassive black holes",
      "neutral_headline": "Runaway Black Hole Mergers May Build Supermassive Black Holes",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2025/11/runaway-black-hole-mergers-may-have-built-supermassive-black-holes/",
          "published_at": "Mon, 10 Nov 2025 17:37:14 +0000",
          "title": "Runaway black hole mergers may have built supermassive black holes",
          "standfirst": "Early superdense star clusters may have planted seeds for monster black holes.",
          "content": "A new simulation could help solve one of astronomy’s longstanding mysteries—how supermassive black holes formed so rapidly—along with a new one: What are the James Webb Space Telescope’s (JWST) “little red dots?” Invisible leviathans lurk at the cores of nearly all of the 2 trillion or so galaxies strewn throughout space-time. Monster black holes entered the cosmic scene soon after the Universe’s birth and grew rapidly, reaching millions or even billions of times the Sun’s mass in less than a billion years. Astronomers have long wondered how these supermassive black holes could have grown so hefty in such little time. The monster black hole mystery became even more perplexing in 2022 when “little red dots” were spotted at the far edges of space. When these tiny scarlet orbs began unexpectedly popping up in JWST images of the distant Universe, their nature was hotly debated. Now that scientists have amassed a sample of hundreds of them, many think the dots are growing supermassive black holes.Read full article Comments",
          "feed_position": 15,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/image-1-1152x648-1762702202.png"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/image-1-1152x648-1762702202.png",
      "popularity_score": 133,
      "ai_summary": [
        "Runaway black hole mergers may have built supermassive black holes.",
        "Early star clusters may have seeded monster black holes.",
        "The mergers could explain the formation of these massive objects.",
        "The research provides insights into the universe's early history.",
        "The findings contribute to our understanding of black hole evolution."
      ]
    },
    {
      "id": "cluster_125",
      "coverage": 1,
      "updated_at": "Mon, 10 Nov 2025 16:10:06 +0000",
      "title": "F1 in Brazil: That’s what generational talent looks like",
      "neutral_headline": "F1 in Brazil: Generational Talent on Display",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/cars/2025/11/f1-in-brazil-thats-what-generational-talent-looks-like/",
          "published_at": "Mon, 10 Nov 2025 16:10:06 +0000",
          "title": "F1 in Brazil: That’s what generational talent looks like",
          "standfirst": "Filled with passionate fans, the racetrack between the lakes is a favorite.",
          "content": "After a weekend off, perhaps spent trick or treating, Formula 1’s drivers, engineers, and mechanics made their yearly trip to the Interlagos track for the Brazilian Grand Prix. More formally called the Autodromo Jose Carlos Pace, it’s definitely one of the more old-school circuits that F1 visits—and invariably one of the more dramatic. For one thing, it’s anything but billiard-smooth. Better yet, there’s elevation—lots of it—and cambers, too. Unlike most F1 tracks, it runs counterclockwise, and it combines some very fast sections with several rather technical corners that can catch out even the best drivers in the world. Nestled between a couple of lakes in São Paulo, weather is also a regular factor in races here. And indeed, a severe weather warning was issued in the lead-up to this weekend’s race. You have to hit the ground running This was another sprint weekend, which means that instead of two practice sessions on Friday and another on Saturday morning, the teams get one on Friday, then go into qualifying for the Saturday sprint race. The shortened testing time tends to shake things up a bit, and we definitely saw that this weekend.Read full article Comments",
          "feed_position": 17,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-2245155272-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-2245155272-1152x648.jpg",
      "popularity_score": 133,
      "ai_summary": [
        "The F1 race in Brazil showcases generational talent.",
        "The racetrack is a favorite among fans.",
        "The race is held at a track between the lakes.",
        "The event is filled with passionate fans.",
        "The race highlights the skill of top drivers."
      ]
    },
    {
      "id": "cluster_133",
      "coverage": 1,
      "updated_at": "Mon, 10 Nov 2025 12:00:24 +0000",
      "title": "NASA is kind of a mess: Here are the top priorities for a new administrator",
      "neutral_headline": "NASA is kind of a mess: Here are the top priorities for a new administrator",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2025/11/nasa-is-kind-of-a-mess-here-are-the-top-priorities-for-a-new-administrator/",
          "published_at": "Mon, 10 Nov 2025 12:00:24 +0000",
          "title": "NASA is kind of a mess: Here are the top priorities for a new administrator",
          "standfirst": "\"He inevitably will have to make tough calls.\"",
          "content": "After a long summer and fall of uncertainty, private astronaut Jared Isaacman has been renominated to lead NASA, and there appears to be momentum behind getting him confirmed quickly as the space agency’s 15th administrator. It is possible, although far from a lock, the Senate could finalize his nomination before the end of this year. It cannot happen soon enough. The National Aeronautics and Space Administration is, to put it bluntly, kind of a mess. This is not meant to disparage the many fine people who work at NASA. But years of neglect, changing priorities, mismanagement, creeping bureaucracy, meeting bloat, and other factors have taken their toll. NASA is still capable of doing great things. It still inspires. But it needs a fresh start.Read full article Comments",
          "feed_position": 18,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/polaris-dawn-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/11/polaris-dawn-1152x648.jpg",
      "popularity_score": 133,
      "ai_summary": [
        "NASA faces challenges and requires a new administrator.",
        "The new administrator will need to make difficult decisions.",
        "The article outlines top priorities for the new administrator.",
        "The agency is in need of strategic direction and leadership.",
        "The new administrator will shape NASA's future endeavors."
      ]
    }
  ]
}