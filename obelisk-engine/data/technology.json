{
  "updated_at": "2026-01-18T23:19:57.093Z",
  "clusters": [
    {
      "id": "cluster_6",
      "coverage": 2,
      "updated_at": "Sun, 18 Jan 2026 19:00:00 GMT",
      "title": "Stop calling it 'The AI bubble': It's actually multiple bubbles, each with a different expiration date",
      "neutral_headline": "How to pair AirPods with any device",
      "items": [
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/infrastructure/stop-calling-it-the-ai-bubble-its-actually-multiple-bubbles-each-with-a",
          "published_at": "Sun, 18 Jan 2026 19:00:00 GMT",
          "title": "Stop calling it 'The AI bubble': It's actually multiple bubbles, each with a different expiration date",
          "standfirst": "It’s the question on everyone’s minds and lips: Are we in an AI bubble?It&#x27;s the wrong question. The real question is: Which AI bubble are we in, and when will each one burst?The debate over whether AI represents a transformative technology or an economic time bomb has reached a fever pitch. Even tech leaders like Meta CEO Mark Zuckerberg have acknowledged evidence of an unstable financial bubble forming around AI. OpenAI CEO Sam Altman and Microsoft co-founder Bill Gates see clear bubble dynamics: overexcited investors, frothy valuations and plenty of doomed projects — but they still believe AI will ultimately transform the economy.But treating \"AI\" as a single monolithic entity destined for a uniform collapse is fundamentally misguided. The AI ecosystem is actually three distinct layers, each with different economics, defensibility and risk profiles. Understanding these layers is critical, because they won&#x27;t all pop at once. Layer 3: The wrapper companies (first to fall)The most vulnerable segment isn&#x27;t building AI — it&#x27;s repackaging it.These are the companies that take OpenAI&#x27;s API, add a slick interface and some prompt engineering, then charge $49/month for what amounts to a glorified ChatGPT wrapper. Some have achieved rapid initial success, like Jasper.ai, which reached approximately $42 million in annual recurring revenue (ARR) in its first year by wrapping GPT models in a user-friendly interface for marketers.But the cracks are already showing. These businesses face threats from every direction:Feature absorption: Microsoft can bundle your $50/month AI writing tool into Office 365 tomorrow. Google can make your AI email assistant a free Gmail feature. Salesforce can build your AI sales tool natively into their CRM. When large platforms decide your product is a feature, not a product, your business model evaporates overnight.The commoditization trap: Wrapper companies are essentially just passing inputs and outputs, if OpenAI improves prompting, these tools lose value overnight. As foundation models become more similar in capability and pricing continues to fall, margins compress to nothing.Zero switching costs: Most wrapper companies don&#x27;t own proprietary data, embedded workflows or deep integrations. A customer can switch to a competitor, or directly to ChatGPT, in minutes. There&#x27;s no moat, no lock-in, no defensibility.The white-label AI market exemplifies this fragility. Companies using white-label platforms face vendor lock-in risks from proprietary systems and API limitations that can hinder integration. These businesses are building on rented land, and the landlord can change the terms, or bulldoze the property, at any moment.The exception that proves the rule: Cursor stands as a rare wrapper-layer company that has built genuine defensibility. By deeply integrating into developer workflows, creating proprietary features beyond simple API calls and establishing strong network effects through user habits and custom configurations, Cursor has demonstrated how a wrapper can evolve into something more substantial. But companies like Cursor are outliers, not the norm — most wrapper companies lack this level of workflow integration and user lock-in.Timeline: Expect significant failures in this segment by late 2025 through 2026, as large platforms absorb functionality and users realize they&#x27;re paying premium prices for commoditized capabilities.Layer 2: Foundation models (the middle ground)The companies building LLMs — OpenAI, Anthropic, Mistral — occupy a more defensible but still precarious position.Economic researcher Richard Bernstein points to OpenAI as an example of the bubble dynamic, noting that the company has made around $1 trillion in AI deals, including a $500 billion data center buildout project, despite being set to generate only $13 billion in revenue. The divergence between investment and plausible earnings \"certainly looks bubbly,\" Bernstein notes. Yet, these companies possess genuine technological moats: Model training expertise, compute access and performance advantages. The question is whether these advantages are sustainable or whether models will commoditize to the point where they&#x27;re indistinguishable — turning foundation model providers into low-margin infrastructure utilities.Engineering will separate winners from losers: As foundation models converge in baseline capabilities, the competitive edge will increasingly come from inference optimization and systems engineering. Companies that can scale the memory wall through innovations like extended KV cache architectures, achieve superior token throughput and deliver faster time-to-first-token will command premium pricing and market share. The winners won’t just be those with the largest training runs, but those who can make AI inference economically viable at scale. Technical breakthroughs in memory management, caching strategies and infrastructure efficiency will determine which frontier labs survive consolidation.Another concern is the circular nature of investments. For instance, Nvidia is pumping $100 billion into OpenAI to bankroll data centers, and OpenAI is then filling those facilities with Nvidia&#x27;s chips. Nvidia is essentially subsidizing one of its biggest customers, potentially artificially inflating actual AI demand.Still, these companies have massive capital backing, genuine technical capabilities and strategic partnerships with major cloud providers and enterprises. Some will consolidate, some will be acquired, but the category will survive.Timeline: Consolidation in 2026 to 2028, with 2 to 3 dominant players emerging while smaller model providers are acquired or shuttered.Layer 1: Infrastructure (built to last)Here’s the contrarian take: The infrastructure layer — including Nvidia, data centers, cloud providers, memory systems and AI-optimized storage — is the least bubbly part of the AI boom.Yes, the latest estimates suggest global AI capital expenditures and venture capital investments already exceed $600 billion in 2025, with Gartner estimating that all AI-related spending worldwide might top $1.5 trillion. That sounds like bubble territory.But infrastructure has a critical characteristic: It retains value regardless of which specific applications succeed. The fiber optic cables laid during the dot-com bubble weren’t wasted — they enabled YouTube, Netflix and cloud computing. Twenty-five years ago, the original dot-com bubble burst after debt financing built out fiber-optic cables for a future that had not yet arrived, but that future eventually did arrive, and the infrastructure was there waiting.Despite stock pressure, Nvidia’s Q3 fiscal year 2025 revenue hit about $57 billion, up 22% quarter-over-quarter and 62% year-over-year, with the data center division alone generating roughly $51.2 billion. These aren’t vanity metrics; they represent real demand from companies making genuine infrastructure investments.The chips, data centers, memory systems and storage infrastructure being built today will power whatever AI applications ultimately succeed, whether that’s today’s chatbots, tomorrow’s autonomous agents or applications we haven’t even imagined yet. Unlike commoditized storage alone, modern AI infrastructure encompasses the entire memory hierarchy — from GPU HBM to DRAM to high-performance storage systems that serve as token warehouses for inference workloads. This integrated approach to memory and storage represents a fundamental architectural innovation, not a commodity play.Timeline: Short-term overbuilding and lazy engineering are possible (2026), but long-term value retention is expected as AI workloads expand over the next decade.The cascade effect: Why this mattersThe current AI boom won&#x27;t end with one dramatic crash. Instead, we&#x27;ll see a cascade of failures beginning with the most vulnerable companies, and the warning signs are already here.Phase 1: Wrapper and white-label companies face margin compression and feature absorption. Hundreds of AI startups with thin differentiation will shut down or sell for pennies on the dollar. More than 1,300 AI startups now have valuations of over $100 million, with 498 AI \"unicorns\" valued at $1 billion or more, many of which won&#x27;t justify those valuations.Phase 2: Foundation model consolidation as performance converges and only the best-capitalized players survive. Expect 3 to 5 major acquisitions as tech giants absorb promising model companies.Phase 3: Infrastructure spending normalizes but remains elevated. Some data centers will sit partially empty for a few years (like fiber optic cables in 2002), but they&#x27;ll eventually fill as AI workloads genuinely expand.What this means for buildersThe most significant risk isn&#x27;t being a wrapper — it’s staying one. If you own the experience the user operates in, you own the user.If you&#x27;re building in the application layer, you need to move upstack immediately:From wrapper → application layer: Stop just generating outputs. Own the workflow before and after the AI interaction.From application → vertical SaaS: Build execution layers that force users to stay inside your product. Create proprietary data, deep integrations and workflow ownership that makes switching painful.The distribution moat: Your real advantage isn&#x27;t the LLM, it&#x27;s how you get users, keep them and expand what they do inside your platform. Winning AI businesses aren&#x27;t just software companies — they&#x27;re distribution companies.The bottom lineIt’s time to stop asking whether we&#x27;re in \"the\" AI bubble. We&#x27;re in multiple bubbles with different characteristics and timelines.The wrapper companies will pop first, probably within 18 months. Foundation models will consolidate over the next 2 to 4 years. I predict that current infrastructure investments will ultimately prove justified over the long term, although not without some short-term overbuilding pains.This isn&#x27;t a reason for pessimism, it&#x27;s a roadmap. Understanding which layer you&#x27;re operating in and which bubble you might be caught in is the difference between becoming the next casualty and building something that survives the shakeout.The AI revolution is real. But not every company riding the wave will make it to shore.Val Bercovici is CAIO at WEKA.",
          "content": "It’s the question on everyone’s minds and lips: Are we in an AI bubble?It&#x27;s the wrong question. The real question is: Which AI bubble are we in, and when will each one burst?The debate over whether AI represents a transformative technology or an economic time bomb has reached a fever pitch. Even tech leaders like Meta CEO Mark Zuckerberg have acknowledged evidence of an unstable financial bubble forming around AI. OpenAI CEO Sam Altman and Microsoft co-founder Bill Gates see clear bubble dynamics: overexcited investors, frothy valuations and plenty of doomed projects — but they still believe AI will ultimately transform the economy.But treating \"AI\" as a single monolithic entity destined for a uniform collapse is fundamentally misguided. The AI ecosystem is actually three distinct layers, each with different economics, defensibility and risk profiles. Understanding these layers is critical, because they won&#x27;t all pop at once. Layer 3: The wrapper companies (first to fall)The most vulnerable segment isn&#x27;t building AI — it&#x27;s repackaging it.These are the companies that take OpenAI&#x27;s API, add a slick interface and some prompt engineering, then charge $49/month for what amounts to a glorified ChatGPT wrapper. Some have achieved rapid initial success, like Jasper.ai, which reached approximately $42 million in annual recurring revenue (ARR) in its first year by wrapping GPT models in a user-friendly interface for marketers.But the cracks are already showing. These businesses face threats from every direction:Feature absorption: Microsoft can bundle your $50/month AI writing tool into Office 365 tomorrow. Google can make your AI email assistant a free Gmail feature. Salesforce can build your AI sales tool natively into their CRM. When large platforms decide your product is a feature, not a product, your business model evaporates overnight.The commoditization trap: Wrapper companies are essentially just passing inputs and outputs, if OpenAI improves prompting, these tools lose value overnight. As foundation models become more similar in capability and pricing continues to fall, margins compress to nothing.Zero switching costs: Most wrapper companies don&#x27;t own proprietary data, embedded workflows or deep integrations. A customer can switch to a competitor, or directly to ChatGPT, in minutes. There&#x27;s no moat, no lock-in, no defensibility.The white-label AI market exemplifies this fragility. Companies using white-label platforms face vendor lock-in risks from proprietary systems and API limitations that can hinder integration. These businesses are building on rented land, and the landlord can change the terms, or bulldoze the property, at any moment.The exception that proves the rule: Cursor stands as a rare wrapper-layer company that has built genuine defensibility. By deeply integrating into developer workflows, creating proprietary features beyond simple API calls and establishing strong network effects through user habits and custom configurations, Cursor has demonstrated how a wrapper can evolve into something more substantial. But companies like Cursor are outliers, not the norm — most wrapper companies lack this level of workflow integration and user lock-in.Timeline: Expect significant failures in this segment by late 2025 through 2026, as large platforms absorb functionality and users realize they&#x27;re paying premium prices for commoditized capabilities.Layer 2: Foundation models (the middle ground)The companies building LLMs — OpenAI, Anthropic, Mistral — occupy a more defensible but still precarious position.Economic researcher Richard Bernstein points to OpenAI as an example of the bubble dynamic, noting that the company has made around $1 trillion in AI deals, including a $500 billion data center buildout project, despite being set to generate only $13 billion in revenue. The divergence between investment and plausible earnings \"certainly looks bubbly,\" Bernstein notes. Yet, these companies possess genuine technological moats: Model training expertise, compute access and performance advantages. The question is whether these advantages are sustainable or whether models will commoditize to the point where they&#x27;re indistinguishable — turning foundation model providers into low-margin infrastructure utilities.Engineering will separate winners from losers: As foundation models converge in baseline capabilities, the competitive edge will increasingly come from inference optimization and systems engineering. Companies that can scale the memory wall through innovations like extended KV cache architectures, achieve superior token throughput and deliver faster time-to-first-token will command premium pricing and market share. The winners won’t just be those with the largest training runs, but those who can make AI inference economically viable at scale. Technical breakthroughs in memory management, caching strategies and infrastructure efficiency will determine which frontier labs survive consolidation.Another concern is the circular nature of investments. For instance, Nvidia is pumping $100 billion into OpenAI to bankroll data centers, and OpenAI is then filling those facilities with Nvidia&#x27;s chips. Nvidia is essentially subsidizing one of its biggest customers, potentially artificially inflating actual AI demand.Still, these companies have massive capital backing, genuine technical capabilities and strategic partnerships with major cloud providers and enterprises. Some will consolidate, some will be acquired, but the category will survive.Timeline: Consolidation in 2026 to 2028, with 2 to 3 dominant players emerging while smaller model providers are acquired or shuttered.Layer 1: Infrastructure (built to last)Here’s the contrarian take: The infrastructure layer — including Nvidia, data centers, cloud providers, memory systems and AI-optimized storage — is the least bubbly part of the AI boom.Yes, the latest estimates suggest global AI capital expenditures and venture capital investments already exceed $600 billion in 2025, with Gartner estimating that all AI-related spending worldwide might top $1.5 trillion. That sounds like bubble territory.But infrastructure has a critical characteristic: It retains value regardless of which specific applications succeed. The fiber optic cables laid during the dot-com bubble weren’t wasted — they enabled YouTube, Netflix and cloud computing. Twenty-five years ago, the original dot-com bubble burst after debt financing built out fiber-optic cables for a future that had not yet arrived, but that future eventually did arrive, and the infrastructure was there waiting.Despite stock pressure, Nvidia’s Q3 fiscal year 2025 revenue hit about $57 billion, up 22% quarter-over-quarter and 62% year-over-year, with the data center division alone generating roughly $51.2 billion. These aren’t vanity metrics; they represent real demand from companies making genuine infrastructure investments.The chips, data centers, memory systems and storage infrastructure being built today will power whatever AI applications ultimately succeed, whether that’s today’s chatbots, tomorrow’s autonomous agents or applications we haven’t even imagined yet. Unlike commoditized storage alone, modern AI infrastructure encompasses the entire memory hierarchy — from GPU HBM to DRAM to high-performance storage systems that serve as token warehouses for inference workloads. This integrated approach to memory and storage represents a fundamental architectural innovation, not a commodity play.Timeline: Short-term overbuilding and lazy engineering are possible (2026), but long-term value retention is expected as AI workloads expand over the next decade.The cascade effect: Why this mattersThe current AI boom won&#x27;t end with one dramatic crash. Instead, we&#x27;ll see a cascade of failures beginning with the most vulnerable companies, and the warning signs are already here.Phase 1: Wrapper and white-label companies face margin compression and feature absorption. Hundreds of AI startups with thin differentiation will shut down or sell for pennies on the dollar. More than 1,300 AI startups now have valuations of over $100 million, with 498 AI \"unicorns\" valued at $1 billion or more, many of which won&#x27;t justify those valuations.Phase 2: Foundation model consolidation as performance converges and only the best-capitalized players survive. Expect 3 to 5 major acquisitions as tech giants absorb promising model companies.Phase 3: Infrastructure spending normalizes but remains elevated. Some data centers will sit partially empty for a few years (like fiber optic cables in 2002), but they&#x27;ll eventually fill as AI workloads genuinely expand.What this means for buildersThe most significant risk isn&#x27;t being a wrapper — it’s staying one. If you own the experience the user operates in, you own the user.If you&#x27;re building in the application layer, you need to move upstack immediately:From wrapper → application layer: Stop just generating outputs. Own the workflow before and after the AI interaction.From application → vertical SaaS: Build execution layers that force users to stay inside your product. Create proprietary data, deep integrations and workflow ownership that makes switching painful.The distribution moat: Your real advantage isn&#x27;t the LLM, it&#x27;s how you get users, keep them and expand what they do inside your platform. Winning AI businesses aren&#x27;t just software companies — they&#x27;re distribution companies.The bottom lineIt’s time to stop asking whether we&#x27;re in \"the\" AI bubble. We&#x27;re in multiple bubbles with different characteristics and timelines.The wrapper companies will pop first, probably within 18 months. Foundation models will consolidate over the next 2 to 4 years. I predict that current infrastructure investments will ultimately prove justified over the long term, although not without some short-term overbuilding pains.This isn&#x27;t a reason for pessimism, it&#x27;s a roadmap. Understanding which layer you&#x27;re operating in and which bubble you might be caught in is the difference between becoming the next casualty and building something that survives the shakeout.The AI revolution is real. But not every company riding the wave will make it to shore.Val Bercovici is CAIO at WEKA.",
          "feed_position": 0,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/68qCkdkPAfX6Yt7LI1YkEb/f32f1b57c02cb29fb615356edd1e6c82/AI_bubble.png?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/orchestration/why-reinforcement-learning-plateaus-without-representation-depth-and-other",
          "published_at": "Sat, 17 Jan 2026 19:00:00 GMT",
          "title": "Why reinforcement learning plateaus without representation depth (and other key takeaways from NeurIPS 2025)",
          "standfirst": "Every year, NeurIPS produces hundreds of impressive papers, and a handful that subtly reset how practitioners think about scaling, evaluation and system design. In 2025, the most consequential works weren&#x27;t about a single breakthrough model. Instead, they challenged fundamental assumptions that academicians and corporations have quietly relied on: Bigger models mean better reasoning, RL creates new capabilities, attention is “solved” and generative models inevitably memorize.This year’s top papers collectively point to a deeper shift: AI progress is now constrained less by raw model capacity and more by architecture, training dynamics and evaluation strategy.Below is a technical deep dive into five of the most influential NeurIPS 2025 papers — and what they mean for anyone building real-world AI systems.1. LLMs are converging—and we finally have a way to measure itPaper: Artificial Hivemind: The Open-Ended Homogeneity of Language ModelsFor years, LLM evaluation has focused on correctness. But in open-ended or ambiguous tasks like brainstorming, ideation or creative synthesis, there often is no single correct answer. The risk instead is homogeneity: Models producing the same “safe,” high-probability responses.This paper introduces Infinity-Chat, a benchmark designed explicitly to measure diversity and pluralism in open-ended generation. Rather than scoring answers as right or wrong, it measures:Intra-model collapse: How often the same model repeats itselfInter-model homogeneity: How similar different models’ outputs areThe result is uncomfortable but important: Across architectures and providers, models increasingly converge on similar outputs — even when multiple valid answers exist.Why this matters in practiceFor corporations, this reframes “alignment” as a trade-off. Preference tuning and safety constraints can quietly reduce diversity, leading to assistants that feel too safe, predictable or biased toward dominant viewpoints.Takeaway: If your product relies on creative or exploratory outputs, diversity metrics need to be first-class citizens. 2. Attention isn’t finished — a simple gate changes everythingPaper: Gated Attention for Large Language ModelsTransformer attention has been treated as settled engineering. This paper proves it isn’t.The authors introduce a small architectural change: Apply a query-dependent sigmoid gate after scaled dot-product attention, per attention head. That’s it. No exotic kernels, no massive overhead.Across dozens of large-scale training runs — including dense and mixture-of-experts (MoE) models trained on trillions of tokens — this gated variant:Improved stabilityReduced “attention sinks”Enhanced long-context performanceConsistently outperformed vanilla attentionWhy it worksThe gate introduces:Non-linearity in attention outputsImplicit sparsity, suppressing pathological activationsThis challenges the assumption that attention failures are purely data or optimization problems.Takeaway: Some of the biggest LLM reliability issues may be architectural — not algorithmic — and solvable with surprisingly small changes.3. RL can scale — if you scale in depth, not just dataPaper: 1,000-Layer Networks for Self-Supervised Reinforcement LearningConventional wisdom says RL doesn’t scale well without dense rewards or demonstrations. This paper reveals that that assumption is incomplete.By scaling network depth aggressively from typical 2 to 5 layers to nearly 1,000 layers, the authors demonstrate dramatic gains in self-supervised, goal-conditioned RL, with performance improvements ranging from 2X to 50X.The key isn’t brute force. It’s pairing depth with contrastive objectives, stable optimization regimes and goal-conditioned representationsWhy this matters beyond roboticsFor agentic systems and autonomous workflows, this suggests that representation depth — not just data or reward shaping — may be a critical lever for generalization and exploration.Takeaway: RL’s scaling limits may be architectural, not fundamental.4. Why diffusion models generalize instead of memorizingPaper: Why Diffusion Models Don&#x27;t Memorize: The Role of Implicit Dynamical Regularization in TrainingDiffusion models are massively overparameterized, yet they often generalize remarkably well. This paper explains why.The authors identify two distinct training timescales:One where generative quality rapidly improvesAnother — much slower — where memorization emergesCrucially, the memorization timescale grows linearly with dataset size, creating a widening window where models improve without overfitting.Practical implicationsThis reframes early stopping and dataset scaling strategies. Memorization isn’t inevitable — it’s predictable and delayed.Takeaway: For diffusion training, dataset size doesn’t just improve quality — it actively delays overfitting.5. RL improves reasoning performance, not reasoning capacity Paper: Does Reinforcement Learning Really Incentivize Reasoning in LLMs?Perhaps the most strategically important result of NeurIPS 2025 is also the most sobering.This paper rigorously tests whether reinforcement learning with verifiable rewards (RLVR) actually creates new reasoning abilities in LLMs — or simply reshapes existing ones.Their conclusion: RLVR primarily improves sampling efficiency, not reasoning capacity. At large sample sizes, the base model often already contains the correct reasoning trajectories.What this means for LLM training pipelinesRL is better understood as:A distribution-shaping mechanismNot a generator of fundamentally new capabilitiesTakeaway: To truly expand reasoning capacity, RL likely needs to be paired with mechanisms like teacher distillation or architectural changes — not used in isolation.The bigger picture: AI progress is becoming systems-limited Taken together, these papers point to a common theme:The bottleneck in modern AI is no longer raw model size — it’s system design.Diversity collapse requires new evaluation metricsAttention failures require architectural fixesRL scaling depends on depth and representationMemorization depends on training dynamics, not parameter countReasoning gains depend on how distributions are shaped, not just optimizedFor builders, the message is clear: Competitive advantage is shifting from “who has the biggest model” to “who understands the system.”Maitreyi Chatterjee is a software engineer.Devansh Agarwal currently works as an ML engineer at FAANG.",
          "content": "Every year, NeurIPS produces hundreds of impressive papers, and a handful that subtly reset how practitioners think about scaling, evaluation and system design. In 2025, the most consequential works weren&#x27;t about a single breakthrough model. Instead, they challenged fundamental assumptions that academicians and corporations have quietly relied on: Bigger models mean better reasoning, RL creates new capabilities, attention is “solved” and generative models inevitably memorize.This year’s top papers collectively point to a deeper shift: AI progress is now constrained less by raw model capacity and more by architecture, training dynamics and evaluation strategy.Below is a technical deep dive into five of the most influential NeurIPS 2025 papers — and what they mean for anyone building real-world AI systems.1. LLMs are converging—and we finally have a way to measure itPaper: Artificial Hivemind: The Open-Ended Homogeneity of Language ModelsFor years, LLM evaluation has focused on correctness. But in open-ended or ambiguous tasks like brainstorming, ideation or creative synthesis, there often is no single correct answer. The risk instead is homogeneity: Models producing the same “safe,” high-probability responses.This paper introduces Infinity-Chat, a benchmark designed explicitly to measure diversity and pluralism in open-ended generation. Rather than scoring answers as right or wrong, it measures:Intra-model collapse: How often the same model repeats itselfInter-model homogeneity: How similar different models’ outputs areThe result is uncomfortable but important: Across architectures and providers, models increasingly converge on similar outputs — even when multiple valid answers exist.Why this matters in practiceFor corporations, this reframes “alignment” as a trade-off. Preference tuning and safety constraints can quietly reduce diversity, leading to assistants that feel too safe, predictable or biased toward dominant viewpoints.Takeaway: If your product relies on creative or exploratory outputs, diversity metrics need to be first-class citizens. 2. Attention isn’t finished — a simple gate changes everythingPaper: Gated Attention for Large Language ModelsTransformer attention has been treated as settled engineering. This paper proves it isn’t.The authors introduce a small architectural change: Apply a query-dependent sigmoid gate after scaled dot-product attention, per attention head. That’s it. No exotic kernels, no massive overhead.Across dozens of large-scale training runs — including dense and mixture-of-experts (MoE) models trained on trillions of tokens — this gated variant:Improved stabilityReduced “attention sinks”Enhanced long-context performanceConsistently outperformed vanilla attentionWhy it worksThe gate introduces:Non-linearity in attention outputsImplicit sparsity, suppressing pathological activationsThis challenges the assumption that attention failures are purely data or optimization problems.Takeaway: Some of the biggest LLM reliability issues may be architectural — not algorithmic — and solvable with surprisingly small changes.3. RL can scale — if you scale in depth, not just dataPaper: 1,000-Layer Networks for Self-Supervised Reinforcement LearningConventional wisdom says RL doesn’t scale well without dense rewards or demonstrations. This paper reveals that that assumption is incomplete.By scaling network depth aggressively from typical 2 to 5 layers to nearly 1,000 layers, the authors demonstrate dramatic gains in self-supervised, goal-conditioned RL, with performance improvements ranging from 2X to 50X.The key isn’t brute force. It’s pairing depth with contrastive objectives, stable optimization regimes and goal-conditioned representationsWhy this matters beyond roboticsFor agentic systems and autonomous workflows, this suggests that representation depth — not just data or reward shaping — may be a critical lever for generalization and exploration.Takeaway: RL’s scaling limits may be architectural, not fundamental.4. Why diffusion models generalize instead of memorizingPaper: Why Diffusion Models Don&#x27;t Memorize: The Role of Implicit Dynamical Regularization in TrainingDiffusion models are massively overparameterized, yet they often generalize remarkably well. This paper explains why.The authors identify two distinct training timescales:One where generative quality rapidly improvesAnother — much slower — where memorization emergesCrucially, the memorization timescale grows linearly with dataset size, creating a widening window where models improve without overfitting.Practical implicationsThis reframes early stopping and dataset scaling strategies. Memorization isn’t inevitable — it’s predictable and delayed.Takeaway: For diffusion training, dataset size doesn’t just improve quality — it actively delays overfitting.5. RL improves reasoning performance, not reasoning capacity Paper: Does Reinforcement Learning Really Incentivize Reasoning in LLMs?Perhaps the most strategically important result of NeurIPS 2025 is also the most sobering.This paper rigorously tests whether reinforcement learning with verifiable rewards (RLVR) actually creates new reasoning abilities in LLMs — or simply reshapes existing ones.Their conclusion: RLVR primarily improves sampling efficiency, not reasoning capacity. At large sample sizes, the base model often already contains the correct reasoning trajectories.What this means for LLM training pipelinesRL is better understood as:A distribution-shaping mechanismNot a generator of fundamentally new capabilitiesTakeaway: To truly expand reasoning capacity, RL likely needs to be paired with mechanisms like teacher distillation or architectural changes — not used in isolation.The bigger picture: AI progress is becoming systems-limited Taken together, these papers point to a common theme:The bottleneck in modern AI is no longer raw model size — it’s system design.Diversity collapse requires new evaluation metricsAttention failures require architectural fixesRL scaling depends on depth and representationMemorization depends on training dynamics, not parameter countReasoning gains depend on how distributions are shaped, not just optimizedFor builders, the message is clear: Competitive advantage is shifting from “who has the biggest model” to “who understands the system.”Maitreyi Chatterjee is a software engineer.Devansh Agarwal currently works as an ML engineer at FAANG.",
          "feed_position": 1,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/1hZrQcV9YD9U4xU0DJPFjG/6455ab35c41a35713d4ee18db4a1e3e7/RLVR_DDM.png?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/audio/headphones/how-to-pair-airpods-with-any-device-140000234.html",
          "published_at": "Sat, 17 Jan 2026 14:00:00 +0000",
          "title": "How to pair AirPods with any device",
          "standfirst": "AirPods work most smoothly with Apple hardware, but they also connect reliably to Android phones, Windows laptops and other Bluetooth devices. The pairing process depends on the platform in use although the basics remain the same. Once the AirPods are placed in pairing mode, most devices detect them quickly and handle the remaining steps in the background. The guide below explains how to pair AirPods with Apple products, how the process differs on newer iPhones that support H2 features and how to connect them to non-Apple devices.How to pair AirPods with Apple devicesApple builds AirPods to pair almost instantly with devices in its own ecosystem. Opening the charging case near an unlocked iPhone or iPad usually brings up an on-screen prompt. Tapping Connect links the earbuds to the device and to every other Apple product signed into the same iCloud account. After that, switching between devices is automatic. Audio output follows the active device as long as each product is using updated software and is signed in with the same Apple ID.If the pairing prompt does not appear, there are a few simple checks that help things move along. Opening the Control Center and selecting the audio output menu confirms whether the AirPods already appear as an available device. If they are listed there, choosing them establishes the connection. If they do not appear, opening the Settings app and checking Bluetooth usually reveals whether the AirPods are in range or already recognized. Opening the case while viewing the Bluetooth menu often triggers the pairing card once more.The process is similar across Apple Watch and Mac. When the AirPods are already linked to an iPhone, they tend to show up automatically on a paired Apple Watch. On Mac, opening System Settings and viewing the Bluetooth section reveals the same device list seen on an iPhone. Selecting the AirPods from that list completes the connection and also syncs the pairing status back to every other Apple device using the same account.Enhanced pairing with newer iPhonesSome newer iPhones support features enabled by the H2 chip used in newer AirPods models. With compatible AirPods, models such as iPhone 15 Pro, iPhone 15 Pro Max and the iPhone 16 lineup tend to deliver a faster and more responsive setup experience. When an AirPods case is opened near one of these phones, the device quickly detects the earbuds and displays the pairing card with little delay. This also tends to speed up transitions between devices and improves reliability when switching audio sources.If the fast pairing prompt does not appear on a newer iPhone, reopening the case while the phone is unlocked is usually enough to reinitiate the process. It also helps to confirm that Bluetooth is turned on. Placing the AirPods back in the case for a few seconds and trying again will often reset the pairing state if the earbuds were connected elsewhere.AirPods Pro 3EngadgetHow to put AirPods into pairing modeEvery AirPods model supports a manual pairing mode. This is essential when linking the earbuds to devices outside the Apple ecosystem, or when the automatic prompt fails to appear on an iPhone or Mac. On older AirPods models, pairing mode is activated by opening the lid and pressing and holding the setup button on the back of the case until the LED light flashes white. Newer models, including AirPods 4 and AirPods Pro 3, use a touch-based method instead. With the lid open, tapping the area near the LED light places the earbuds into pairing mode. Leaving the lid open keeps the AirPods discoverable for nearby devices. Closing the case ends the process and requires it to be repeated if the device fails to detect them.Pairing mode does not remove any previous connections. It simply makes the AirPods available to new devices, which is helpful when switching between ecosystems. However, connecting to a new device usually takes priority. If the AirPods are already linked to something else that is nearby, turning off Bluetooth on the previously connected device prevents interference and helps the new device detect them more easily.How to pair AirPods with Android devicesAlthough AirPods are designed for Apple hardware, they function like any other Bluetooth earbuds on Android. Opening the case and placing the AirPods in pairing mode allows Android phones to detect them through the standard Bluetooth menu. The earbuds appear in the list of available devices and selecting them initiates the connection. Once paired, AirPods work for calls and media playback. Some features, such as automatic ear detection and battery status indicators, may require a third-party app on Android and are not supported at a system level. Features such as spatial audio and device switching remain exclusive to Apple’s ecosystem but day-to-day performance is consistent on Android.If the AirPods fail to appear, refreshing the Bluetooth device list on the Android phone usually helps. Making sure the earbuds are still in pairing mode is essential since the white LED indicator stops flashing after a short period. Reopening the case and holding the button again, or tapping on the front for newer models, restores discoverability.How to pair AirPods with Windows laptopsWindows 11 handles AirPods as a regular audio device. Opening the Bluetooth and Devices menu in System Settings displays a list of nearby accessories. With the AirPods in pairing mode, the laptop should detect them and display them as an audio device. Selecting them completes the process and adds the earbuds to the device’s known accessories. Windows generally reconnects to AirPods automatically on future sessions as long as Bluetooth remains enabled.If the earbuds do not appear in the list, toggling Bluetooth off and back on helps the system refresh the device scan. Checking whether the AirPods are already linked to a different device is another useful step. Windows sometimes struggles to take over a connection when the earbuds remain in range of a previously paired phone so disabling Bluetooth on the other device often resolves the issue.Troubleshooting common pairing issuesMost pairing problems come down to the AirPods not being in discoverable mode or being connected to another device nearby. Resetting the earbuds solves many problems. On AirPods models with a setup button, placing the AirPods in the case, leaving the lid open and holding the button until the LED turns amber then white restores the factory pairing state. On newer models without a physical button, place the AirPods in the case, close the lid for about 30 seconds, then open it and quickly tap the front of the case three times. The status light should then flash amber and then white, to indicate that the reset is complete. This clears previous connections and makes the AirPods behave as if they are new out of the box. Low battery levels can also interrupt pairing. Ensuring both the earbuds and the case have enough charge prevents unexpected disconnections during setup. Interference from other wireless accessories affects pairing on crowded networks. Moving to a quieter spot or turning off surrounding Bluetooth devices helps the AirPods stand out when scanning.AirPods are built to pair quickly with Apple devices but they also integrate smoothly with other platforms. Keeping the earbuds in pairing mode and confirming that Bluetooth is enabled on the device in use ensures a smooth setup every time. Once connected, the AirPods tend to remember the device and reconnect whenever they are nearby which keeps day-to-day use simple regardless of the platform.This article originally appeared on Engadget at https://www.engadget.com/audio/headphones/how-to-pair-airpods-with-any-device-140000234.html?src=rss",
          "content": "AirPods work most smoothly with Apple hardware, but they also connect reliably to Android phones, Windows laptops and other Bluetooth devices. The pairing process depends on the platform in use although the basics remain the same. Once the AirPods are placed in pairing mode, most devices detect them quickly and handle the remaining steps in the background. The guide below explains how to pair AirPods with Apple products, how the process differs on newer iPhones that support H2 features and how to connect them to non-Apple devices.How to pair AirPods with Apple devicesApple builds AirPods to pair almost instantly with devices in its own ecosystem. Opening the charging case near an unlocked iPhone or iPad usually brings up an on-screen prompt. Tapping Connect links the earbuds to the device and to every other Apple product signed into the same iCloud account. After that, switching between devices is automatic. Audio output follows the active device as long as each product is using updated software and is signed in with the same Apple ID.If the pairing prompt does not appear, there are a few simple checks that help things move along. Opening the Control Center and selecting the audio output menu confirms whether the AirPods already appear as an available device. If they are listed there, choosing them establishes the connection. If they do not appear, opening the Settings app and checking Bluetooth usually reveals whether the AirPods are in range or already recognized. Opening the case while viewing the Bluetooth menu often triggers the pairing card once more.The process is similar across Apple Watch and Mac. When the AirPods are already linked to an iPhone, they tend to show up automatically on a paired Apple Watch. On Mac, opening System Settings and viewing the Bluetooth section reveals the same device list seen on an iPhone. Selecting the AirPods from that list completes the connection and also syncs the pairing status back to every other Apple device using the same account.Enhanced pairing with newer iPhonesSome newer iPhones support features enabled by the H2 chip used in newer AirPods models. With compatible AirPods, models such as iPhone 15 Pro, iPhone 15 Pro Max and the iPhone 16 lineup tend to deliver a faster and more responsive setup experience. When an AirPods case is opened near one of these phones, the device quickly detects the earbuds and displays the pairing card with little delay. This also tends to speed up transitions between devices and improves reliability when switching audio sources.If the fast pairing prompt does not appear on a newer iPhone, reopening the case while the phone is unlocked is usually enough to reinitiate the process. It also helps to confirm that Bluetooth is turned on. Placing the AirPods back in the case for a few seconds and trying again will often reset the pairing state if the earbuds were connected elsewhere.AirPods Pro 3EngadgetHow to put AirPods into pairing modeEvery AirPods model supports a manual pairing mode. This is essential when linking the earbuds to devices outside the Apple ecosystem, or when the automatic prompt fails to appear on an iPhone or Mac. On older AirPods models, pairing mode is activated by opening the lid and pressing and holding the setup button on the back of the case until the LED light flashes white. Newer models, including AirPods 4 and AirPods Pro 3, use a touch-based method instead. With the lid open, tapping the area near the LED light places the earbuds into pairing mode. Leaving the lid open keeps the AirPods discoverable for nearby devices. Closing the case ends the process and requires it to be repeated if the device fails to detect them.Pairing mode does not remove any previous connections. It simply makes the AirPods available to new devices, which is helpful when switching between ecosystems. However, connecting to a new device usually takes priority. If the AirPods are already linked to something else that is nearby, turning off Bluetooth on the previously connected device prevents interference and helps the new device detect them more easily.How to pair AirPods with Android devicesAlthough AirPods are designed for Apple hardware, they function like any other Bluetooth earbuds on Android. Opening the case and placing the AirPods in pairing mode allows Android phones to detect them through the standard Bluetooth menu. The earbuds appear in the list of available devices and selecting them initiates the connection. Once paired, AirPods work for calls and media playback. Some features, such as automatic ear detection and battery status indicators, may require a third-party app on Android and are not supported at a system level. Features such as spatial audio and device switching remain exclusive to Apple’s ecosystem but day-to-day performance is consistent on Android.If the AirPods fail to appear, refreshing the Bluetooth device list on the Android phone usually helps. Making sure the earbuds are still in pairing mode is essential since the white LED indicator stops flashing after a short period. Reopening the case and holding the button again, or tapping on the front for newer models, restores discoverability.How to pair AirPods with Windows laptopsWindows 11 handles AirPods as a regular audio device. Opening the Bluetooth and Devices menu in System Settings displays a list of nearby accessories. With the AirPods in pairing mode, the laptop should detect them and display them as an audio device. Selecting them completes the process and adds the earbuds to the device’s known accessories. Windows generally reconnects to AirPods automatically on future sessions as long as Bluetooth remains enabled.If the earbuds do not appear in the list, toggling Bluetooth off and back on helps the system refresh the device scan. Checking whether the AirPods are already linked to a different device is another useful step. Windows sometimes struggles to take over a connection when the earbuds remain in range of a previously paired phone so disabling Bluetooth on the other device often resolves the issue.Troubleshooting common pairing issuesMost pairing problems come down to the AirPods not being in discoverable mode or being connected to another device nearby. Resetting the earbuds solves many problems. On AirPods models with a setup button, placing the AirPods in the case, leaving the lid open and holding the button until the LED turns amber then white restores the factory pairing state. On newer models without a physical button, place the AirPods in the case, close the lid for about 30 seconds, then open it and quickly tap the front of the case three times. The status light should then flash amber and then white, to indicate that the reset is complete. This clears previous connections and makes the AirPods behave as if they are new out of the box. Low battery levels can also interrupt pairing. Ensuring both the earbuds and the case have enough charge prevents unexpected disconnections during setup. Interference from other wireless accessories affects pairing on crowded networks. Moving to a quieter spot or turning off surrounding Bluetooth devices helps the AirPods stand out when scanning.AirPods are built to pair quickly with Apple devices but they also integrate smoothly with other platforms. Keeping the earbuds in pairing mode and confirming that Bluetooth is enabled on the device in use ensures a smooth setup every time. Once connected, the AirPods tend to remember the device and reconnect whenever they are nearby which keeps day-to-day use simple regardless of the platform.This article originally appeared on Engadget at https://www.engadget.com/audio/headphones/how-to-pair-airpods-with-any-device-140000234.html?src=rss",
          "feed_position": 16,
          "image_url": "https://media-mbst-pub-ue1.s3.amazonaws.com/creatr-uploaded-images/2025-11/fc3e54e0-c61d-11f0-a4db-6ace49d303da"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/cybersecurity/vpn/how-to-cancel-cyberghost-and-get-a-refund-130000311.html",
          "published_at": "Sat, 17 Jan 2026 13:00:00 +0000",
          "title": "How to cancel CyberGhost and get a refund",
          "standfirst": "I came out of my CyberGhost review with a positive opinion, feeling it had earned its spot in my best VPN roundup. However, even an expert review is subjective, and there's a chance CyberGhost will not work for you. If that’s the case, here's how to cancel your subscription.How to stop your CyberGhost subscription renewingCancelling your CyberGhost subscription won't end it right away, unless you delete your account or get the refund (I'll explain how to do both of those later). Instead, the way to cancel CyberGhost is to stop your subscription from renewing at the end of each billing period. Once you've done that, you can keep using CyberGhost until your current period ends.The following steps will cancel auto-renewal if you got CyberGhost through its website. If you bought it through an app store instead, see the next section.Open your browser and go to cyberghostvpn.com.At the top-right of the screen, click the box labeled My Account. Enter your username and password if you aren't logged in already.Look at the top-right corner of the new screen and click the CyberGhost logo next to your email address. From the drop-down menu, select Subscriptions.Find the subscription you want to cancel and select Cancel Subscription.When prompted, click Continue to Cancel.Click the logo at top-right, then click Subscriptions to manage auto-renewal.Sam Chapman for EngadgetThis will turn off automatic billing for your account. The next time you would have been billed, your subscription will expire. You can resubscribe by purchasing another term. If you're within the refund period — 14 days for a monthly subscription and 45 days for all others — you can now request your money back.How to cancel CyberGhost if you subscribed through an app storeWhen you subscribe to an app through the Apple App Store or Google Play Store, the store handles the billing; the app provider doesn't process the money itself. If you bought CyberGhost through an app store and want to cancel, you'll have to ask the app store in question, not CyberGhost. Here's how to do it.If you subscribed through the Apple App Store, you'll need to cancel through your Apple ID. Here are the steps.Open the Settings app on your home screen.At the top of the Settings menu, you'll see your name. Tap it.In your Apple Account menu, tap Subscriptions.Scroll until you find your CyberGhost subscription, then tap on it.Tap the words Cancel Subscription, then follow the prompts.Here's what to do if you subscribed through the Google Play store. Similar to the Apple process, you'll go through the list of subscriptions on your profile.Open the Google Play Store app.Tap the circle in the top-right corner with the first letter of your name.Find the Payments & Subscriptions menu and tap on it. On the next menu that appears, tap Subscriptions.Scroll down until you find your CyberGhost subscription. Tap on it, then click Cancel Subscription.Follow the prompts to complete cancellation.How to delete your CyberGhost accountBefore you set out to delete your CyberGhost account altogether, make sure you've cancelled auto-renew first by following the steps in the previous section. If you don't, you might still be charged for the subscription you're not using, and it's a huge hassle to end that without an account.Once you've done that, log into your account on cyberghostvpn.com and click on your account profile at the top-right, just like when you canceled auto-renewal. Below the username/password window and the message about an activation key, you'll see the words Delete My Account in tiny letters.How to find the button that deletes your CyberGhost account.Sam Chapman for EngadgetClick on them. On the page that appears, select Delete My Account again. Follow any more prompts you're given to annihilate your username for good (note that you can't use it again afterwards).How to get a refund from CyberGhostTo get a refund on your CyberGhost subscription, you have to be inside the window for the plan you chose. With a monthly plan, the refund period is 14 days. For all other plans, it's 45 days. If this time has elapsed, there's unfortunately no way to get your money back.If you are within the refund period, you can get your money by sending a request through customer support. You can email support@cyberghost.ro, submit a ticket through this link or open a live chat conversation by clicking the Live Chat button at the bottom-right of any page on cyberghostvpn.com. No matter what method you choose, the conversation will go faster if you have your order number on-hand — check your inbox if you don't know it.Start a live chat conversation by clicking the live chat button at the bottom-right of any screen on CyberGhost's website.Sam Chapman for EngadgetIf you went through an app store, you'll need to request your money back from that platform instead. Apple and Google Play handle their own monetary transactions, which means they also process refunds.Best CyberGhost alternativesAfter you've cancelled and/or deleted CyberGhost out of your life, you still need a VPN; the benefits of masking your IP address and changing your virtual location don't go anywhere. You can check out my best list (linked at the top) or best free VPN roundup for ideas, or check out the review for my favorite service, Proton VPN.Proton VPN is my top choice because of its focus on user freedoms and attention to quality in everything it does. If you're willing to pay a bit more for extreme simplicity and total reliability, ExpressVPN is ideal for beginners. If you're a speed demon and just want to keep your downloads fast, go with Surfshark.This article originally appeared on Engadget at https://www.engadget.com/cybersecurity/vpn/how-to-cancel-cyberghost-and-get-a-refund-130000311.html?src=rss",
          "content": "I came out of my CyberGhost review with a positive opinion, feeling it had earned its spot in my best VPN roundup. However, even an expert review is subjective, and there's a chance CyberGhost will not work for you. If that’s the case, here's how to cancel your subscription.How to stop your CyberGhost subscription renewingCancelling your CyberGhost subscription won't end it right away, unless you delete your account or get the refund (I'll explain how to do both of those later). Instead, the way to cancel CyberGhost is to stop your subscription from renewing at the end of each billing period. Once you've done that, you can keep using CyberGhost until your current period ends.The following steps will cancel auto-renewal if you got CyberGhost through its website. If you bought it through an app store instead, see the next section.Open your browser and go to cyberghostvpn.com.At the top-right of the screen, click the box labeled My Account. Enter your username and password if you aren't logged in already.Look at the top-right corner of the new screen and click the CyberGhost logo next to your email address. From the drop-down menu, select Subscriptions.Find the subscription you want to cancel and select Cancel Subscription.When prompted, click Continue to Cancel.Click the logo at top-right, then click Subscriptions to manage auto-renewal.Sam Chapman for EngadgetThis will turn off automatic billing for your account. The next time you would have been billed, your subscription will expire. You can resubscribe by purchasing another term. If you're within the refund period — 14 days for a monthly subscription and 45 days for all others — you can now request your money back.How to cancel CyberGhost if you subscribed through an app storeWhen you subscribe to an app through the Apple App Store or Google Play Store, the store handles the billing; the app provider doesn't process the money itself. If you bought CyberGhost through an app store and want to cancel, you'll have to ask the app store in question, not CyberGhost. Here's how to do it.If you subscribed through the Apple App Store, you'll need to cancel through your Apple ID. Here are the steps.Open the Settings app on your home screen.At the top of the Settings menu, you'll see your name. Tap it.In your Apple Account menu, tap Subscriptions.Scroll until you find your CyberGhost subscription, then tap on it.Tap the words Cancel Subscription, then follow the prompts.Here's what to do if you subscribed through the Google Play store. Similar to the Apple process, you'll go through the list of subscriptions on your profile.Open the Google Play Store app.Tap the circle in the top-right corner with the first letter of your name.Find the Payments & Subscriptions menu and tap on it. On the next menu that appears, tap Subscriptions.Scroll down until you find your CyberGhost subscription. Tap on it, then click Cancel Subscription.Follow the prompts to complete cancellation.How to delete your CyberGhost accountBefore you set out to delete your CyberGhost account altogether, make sure you've cancelled auto-renew first by following the steps in the previous section. If you don't, you might still be charged for the subscription you're not using, and it's a huge hassle to end that without an account.Once you've done that, log into your account on cyberghostvpn.com and click on your account profile at the top-right, just like when you canceled auto-renewal. Below the username/password window and the message about an activation key, you'll see the words Delete My Account in tiny letters.How to find the button that deletes your CyberGhost account.Sam Chapman for EngadgetClick on them. On the page that appears, select Delete My Account again. Follow any more prompts you're given to annihilate your username for good (note that you can't use it again afterwards).How to get a refund from CyberGhostTo get a refund on your CyberGhost subscription, you have to be inside the window for the plan you chose. With a monthly plan, the refund period is 14 days. For all other plans, it's 45 days. If this time has elapsed, there's unfortunately no way to get your money back.If you are within the refund period, you can get your money by sending a request through customer support. You can email support@cyberghost.ro, submit a ticket through this link or open a live chat conversation by clicking the Live Chat button at the bottom-right of any page on cyberghostvpn.com. No matter what method you choose, the conversation will go faster if you have your order number on-hand — check your inbox if you don't know it.Start a live chat conversation by clicking the live chat button at the bottom-right of any screen on CyberGhost's website.Sam Chapman for EngadgetIf you went through an app store, you'll need to request your money back from that platform instead. Apple and Google Play handle their own monetary transactions, which means they also process refunds.Best CyberGhost alternativesAfter you've cancelled and/or deleted CyberGhost out of your life, you still need a VPN; the benefits of masking your IP address and changing your virtual location don't go anywhere. You can check out my best list (linked at the top) or best free VPN roundup for ideas, or check out the review for my favorite service, Proton VPN.Proton VPN is my top choice because of its focus on user freedoms and attention to quality in everything it does. If you're willing to pay a bit more for extreme simplicity and total reliability, ExpressVPN is ideal for beginners. If you're a speed demon and just want to keep your downloads fast, go with Surfshark.This article originally appeared on Engadget at https://www.engadget.com/cybersecurity/vpn/how-to-cancel-cyberghost-and-get-a-refund-130000311.html?src=rss",
          "feed_position": 17,
          "image_url": "https://d29szjachogqwa.cloudfront.net/images/user-uploaded/CyberGhost_subscriptions.png"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/technology/black-forest-labs-launches-open-source-flux-2-klein-to-generate-ai-images-in",
          "published_at": "Fri, 16 Jan 2026 23:28:00 GMT",
          "title": "Black Forest Labs launches open source Flux.2 [klein] to generate AI images in less than a second",
          "standfirst": "The German AI startup Black Forest Labs (BFL), founded by former Stability AI engineers, is continuing to build out its suite of open source AI image generators with the release of FLUX.2 [klein], a new pair of small models — one open and one non-commercial — that emphasizes speed and lower compute requirements, with the models generating images in less than a second on a Nvidia GB200. The [klein] series, released yesterday, includes two primary parameter counts: 4 billion (4B) and 9 billion (9B).The model weights are available on Hugging Face and code on Github.While the larger models in the FLUX.2 family ([max] and [pro]), released in November of 2025, chase the limits of photorealism and \"grounding search\" capabilities, [klein] is designed specifically for consumer hardware and latency-critical workflows.In great news for enterprises, the 4B version is available under an Apache 2.0 license, meaning they — or any organization or developer — can use the [klein] models for their commercial purposes without paying BFL or any intermediaries a dime. However, a number of AI image and media creation platforms including Fal.ai have begun offering it for extremely low cost as well through their application programming interfaces (APIs) and as a direct-to-user tool. Already, it&#x27;s won strong praise from early users for its speed. What it lacks for in overall image quality, it seems to make up for in its fast generation capability, open license, affordability and small footprint — benefitting enterprises who want to run image models on their own hardware or at extremely low cost. So how did BFL do it and how can it benefit you? Read on to learn more.The \"Pareto Frontier\" of LatencyThe technical philosophy behind [klein] is what BFL documentation describes as defining the \"Pareto frontier\" for quality versus latency. In simple terms, they have attempted to squeeze the maximum possible visual fidelity into a model small enough to run on a home gaming PC without a noticeable lag.The performance metrics released by the company paint a picture of a model built for interactivity rather than just batch generation. According to Black Forest Labs&#x27; official figures, the [klein] models are capable of generating or editing images in under 0.5 seconds on modern hardware. Even on standard consumer GPUs like an RTX 3090 or 4070, the 4B model is designed to fit comfortably within approximately 13GB of VRAM.This speed is achieved through \"distillation,\" a process where a larger, more complex model \"teaches\" a smaller, more efficient one to approximate its outputs in fewer steps. The distilled [klein] variants require only four steps to generate an image. This effectively turns the generation process from a coffee-break task into a near-instantaneous one, enabling what BFL describes on X (formerly Twitter) as \"developing ideas from 0 → 1\" in real-time.Under the Hood: Unified ArchitectureHistorically, image generation and image editing have often required different pipelines or complex adapters (like ControlNets). FLUX.2 [klein] attempts to unify these. The architecture natively supports text-to-image, single-reference editing, and multi-reference composition without needing to swap models.According to the documentation released on GitHub, the models support:Multi-Reference Editing: Users can upload up to four reference images (or ten in the playground) to guide the style or structure of the output.Hex-Code Color Control: A frequent pain point for designers is getting \"that exact shade of red.\" The new models accept specific hex codes in prompts (e.g., #800020) to force precise color rendering.Structured Prompting: The model parses JSON-like structured inputs for rigorously defined compositions, a feature clearly aimed at programmatic generation and enterprise pipelines.The Licensing Split: Open Weights vs. Open SourceFor startups and developers building on top of BFL’s tech, understanding the licensing landscape of this release is critical. BFL has adopted a split strategy that separates \"hobbyist/research\" use from \"commercial infrastructure.\"FLUX.2 [klein] 4B: Released under Apache 2.0. This is a permissive free software license that allows for commercial use, modification, and redistribution. If you are building a paid app, a SaaS platform, or a game that integrates AI generation, you can use the 4B model royalty-free.FLUX.2 [klein] 9B & [dev]: Released under the FLUX Non-Commercial License. These weights are open for researchers and hobbyists to download and experiment with, but they cannot be used for commercial applications without a separate agreement.This distinction positions the 4B model as a direct competitor to other open-weights models like Stable Diffusion 3 Medium or SDXL, but with a more modern architecture and a permissive license that removes legal ambiguity for startups.Ecosystem Integration: ComfyUI and BeyondBFL is clearly aware that a model is only as good as the tools that run it. Coinciding with the model drop, the team released official workflow templates for ComfyUI, the node-based interface that has become the standard integrated development environment (IDE) for AI artists.The workflows—specifically image_flux2_klein_text_to_image.json and the editing variants—allow users to drag and drop the new capabilities into existing pipelines immediately.Community reaction on social media has centered on this workflow integration and the speed. In a post on X, the official Black Forest Labs account highlighted the model&#x27;s ability to \"rapidly explore a specific aesthetic,\" showcasing a video where the style of an image shifted instantly as the user scrubbed through options.Why It Matters For Enterprise AI Decision-MakersThe release of FLUX.2 [klein] signals a maturation in the generative AI market, moving past the initial phase of novelty into a period defined by utility, integration, and speed.For Lead AI Engineers who are constantly juggling the need to balance speed with quality, this shift is pivotal. These professionals, who manage the full lifecycle of models from data preparation to deployment, often face the daily challenge of integrating rapidly evolving tools into existing workflows. The availability of a distilled 4B model under an Apache 2.0 license offers a practical solution for those focused on rapid deployment and fine-tuning to achieve specific business goals, allowing them to bypass the latency bottlenecks that typically plague high-fidelity image generation.For Senior AI Engineers focused on orchestration and automation, the implications are equally significant. These experts are responsible for building scalable AI pipelines and maintaining model integrity across different environments, often while working under strict budget constraints.The lightweight nature of the [klein] family directly addresses the challenge of implementing efficient systems with limited resources. By utilizing a model that fits within consumer-grade VRAM, orchestration specialists can architect cost-effective, local inference pipelines that avoid the heavy operational costs associated with massive proprietary models.Even for the Director of IT Security, the move toward capable, locally runnable open-weight models offers a distinct advantage. Tasked with protecting the organization from cyber threats and managing security operations with limited resources, reliance on external APIs for sensitive creative workflows can be a vulnerability. A high-quality model that runs locally allows security leaders to sanction AI tools that keep proprietary data within the corporate firewall, balancing the operational demands of the business with the robust security measures they are required to uphold.",
          "content": "The German AI startup Black Forest Labs (BFL), founded by former Stability AI engineers, is continuing to build out its suite of open source AI image generators with the release of FLUX.2 [klein], a new pair of small models — one open and one non-commercial — that emphasizes speed and lower compute requirements, with the models generating images in less than a second on a Nvidia GB200. The [klein] series, released yesterday, includes two primary parameter counts: 4 billion (4B) and 9 billion (9B).The model weights are available on Hugging Face and code on Github.While the larger models in the FLUX.2 family ([max] and [pro]), released in November of 2025, chase the limits of photorealism and \"grounding search\" capabilities, [klein] is designed specifically for consumer hardware and latency-critical workflows.In great news for enterprises, the 4B version is available under an Apache 2.0 license, meaning they — or any organization or developer — can use the [klein] models for their commercial purposes without paying BFL or any intermediaries a dime. However, a number of AI image and media creation platforms including Fal.ai have begun offering it for extremely low cost as well through their application programming interfaces (APIs) and as a direct-to-user tool. Already, it&#x27;s won strong praise from early users for its speed. What it lacks for in overall image quality, it seems to make up for in its fast generation capability, open license, affordability and small footprint — benefitting enterprises who want to run image models on their own hardware or at extremely low cost. So how did BFL do it and how can it benefit you? Read on to learn more.The \"Pareto Frontier\" of LatencyThe technical philosophy behind [klein] is what BFL documentation describes as defining the \"Pareto frontier\" for quality versus latency. In simple terms, they have attempted to squeeze the maximum possible visual fidelity into a model small enough to run on a home gaming PC without a noticeable lag.The performance metrics released by the company paint a picture of a model built for interactivity rather than just batch generation. According to Black Forest Labs&#x27; official figures, the [klein] models are capable of generating or editing images in under 0.5 seconds on modern hardware. Even on standard consumer GPUs like an RTX 3090 or 4070, the 4B model is designed to fit comfortably within approximately 13GB of VRAM.This speed is achieved through \"distillation,\" a process where a larger, more complex model \"teaches\" a smaller, more efficient one to approximate its outputs in fewer steps. The distilled [klein] variants require only four steps to generate an image. This effectively turns the generation process from a coffee-break task into a near-instantaneous one, enabling what BFL describes on X (formerly Twitter) as \"developing ideas from 0 → 1\" in real-time.Under the Hood: Unified ArchitectureHistorically, image generation and image editing have often required different pipelines or complex adapters (like ControlNets). FLUX.2 [klein] attempts to unify these. The architecture natively supports text-to-image, single-reference editing, and multi-reference composition without needing to swap models.According to the documentation released on GitHub, the models support:Multi-Reference Editing: Users can upload up to four reference images (or ten in the playground) to guide the style or structure of the output.Hex-Code Color Control: A frequent pain point for designers is getting \"that exact shade of red.\" The new models accept specific hex codes in prompts (e.g., #800020) to force precise color rendering.Structured Prompting: The model parses JSON-like structured inputs for rigorously defined compositions, a feature clearly aimed at programmatic generation and enterprise pipelines.The Licensing Split: Open Weights vs. Open SourceFor startups and developers building on top of BFL’s tech, understanding the licensing landscape of this release is critical. BFL has adopted a split strategy that separates \"hobbyist/research\" use from \"commercial infrastructure.\"FLUX.2 [klein] 4B: Released under Apache 2.0. This is a permissive free software license that allows for commercial use, modification, and redistribution. If you are building a paid app, a SaaS platform, or a game that integrates AI generation, you can use the 4B model royalty-free.FLUX.2 [klein] 9B & [dev]: Released under the FLUX Non-Commercial License. These weights are open for researchers and hobbyists to download and experiment with, but they cannot be used for commercial applications without a separate agreement.This distinction positions the 4B model as a direct competitor to other open-weights models like Stable Diffusion 3 Medium or SDXL, but with a more modern architecture and a permissive license that removes legal ambiguity for startups.Ecosystem Integration: ComfyUI and BeyondBFL is clearly aware that a model is only as good as the tools that run it. Coinciding with the model drop, the team released official workflow templates for ComfyUI, the node-based interface that has become the standard integrated development environment (IDE) for AI artists.The workflows—specifically image_flux2_klein_text_to_image.json and the editing variants—allow users to drag and drop the new capabilities into existing pipelines immediately.Community reaction on social media has centered on this workflow integration and the speed. In a post on X, the official Black Forest Labs account highlighted the model&#x27;s ability to \"rapidly explore a specific aesthetic,\" showcasing a video where the style of an image shifted instantly as the user scrubbed through options.Why It Matters For Enterprise AI Decision-MakersThe release of FLUX.2 [klein] signals a maturation in the generative AI market, moving past the initial phase of novelty into a period defined by utility, integration, and speed.For Lead AI Engineers who are constantly juggling the need to balance speed with quality, this shift is pivotal. These professionals, who manage the full lifecycle of models from data preparation to deployment, often face the daily challenge of integrating rapidly evolving tools into existing workflows. The availability of a distilled 4B model under an Apache 2.0 license offers a practical solution for those focused on rapid deployment and fine-tuning to achieve specific business goals, allowing them to bypass the latency bottlenecks that typically plague high-fidelity image generation.For Senior AI Engineers focused on orchestration and automation, the implications are equally significant. These experts are responsible for building scalable AI pipelines and maintaining model integrity across different environments, often while working under strict budget constraints.The lightweight nature of the [klein] family directly addresses the challenge of implementing efficient systems with limited resources. By utilizing a model that fits within consumer-grade VRAM, orchestration specialists can architect cost-effective, local inference pipelines that avoid the heavy operational costs associated with massive proprietary models.Even for the Director of IT Security, the move toward capable, locally runnable open-weight models offers a distinct advantage. Tasked with protecting the organization from cyber threats and managing security operations with limited resources, reliance on external APIs for sensitive creative workflows can be a vulnerability. A high-quality model that runs locally allows security leaders to sanction AI tools that keep proprietary data within the corporate firewall, balancing the operational demands of the business with the robust security measures they are required to uphold.",
          "feed_position": 2,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/39s3mZmA7qAJ4Ue1uSoxEF/1a59f5a9ba32692d1504d33a127cb172/robot-throw.png?w=300&q=30"
        }
      ],
      "featured_image": "https://images.ctfassets.net/jdtwqhzvc2n1/68qCkdkPAfX6Yt7LI1YkEb/f32f1b57c02cb29fb615356edd1e6c82/AI_bubble.png?w=300&q=30",
      "popularity_score": 2015.6674741666666
    },
    {
      "id": "cluster_0",
      "coverage": 1,
      "updated_at": "2026-01-18T23:19:56.089Z",
      "title": "This may be the grossest eye pic ever—but the cause is what’s truly horrifying",
      "neutral_headline": "This may be the grossest eye pic ever—but the cause is what’s truly horrifying",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/features/2026/01/this-may-be-the-grossest-eye-pic-ever-but-the-cause-is-whats-truly-horrifying/",
          "published_at": "2026-01-18T23:19:56.089Z",
          "title": "This may be the grossest eye pic ever—but the cause is what’s truly horrifying",
          "standfirst": "",
          "content": "",
          "feed_position": 0
        }
      ],
      "popularity_score": 358.9997211111111
    },
    {
      "id": "cluster_4",
      "coverage": 1,
      "updated_at": "2026-01-18T23:19:56.089Z",
      "title": "Rackspace customers grapple with “devastating” email hosting price hike",
      "neutral_headline": "Rackspace customers grapple with “devastating” email hosting price hike",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/information-technology/2026/01/rackspace-raises-email-hosting-prices-by-as-much-as-706-percent/",
          "published_at": "2026-01-18T23:19:56.089Z",
          "title": "Rackspace customers grapple with “devastating” email hosting price hike",
          "standfirst": "",
          "content": "",
          "feed_position": 4
        }
      ],
      "popularity_score": 353.9997211111111
    },
    {
      "id": "cluster_24",
      "coverage": 1,
      "updated_at": "Sun, 18 Jan 2026 12:00:56 +0000",
      "title": "Ocean damage nearly doubles the cost of climate change",
      "neutral_headline": "Ocean damage nearly doubles the cost of climate change",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2026/01/ocean-damage-nearly-doubles-the-cost-of-climate-change/",
          "published_at": "Sun, 18 Jan 2026 12:00:56 +0000",
          "title": "Ocean damage nearly doubles the cost of climate change",
          "standfirst": "Ignoring the blue economy has left a multi-trillion-dollar blind spot in climate finance.",
          "content": "The global cost of greenhouse gas emissions is nearly double what scientists previously thought, according to a study published Thursday by researchers at the University of California, San Diego’s Scripps Institution of Oceanography. It is the first time a social cost of carbon (SCC) assessment—a key measure of economic harm caused by climate change—has included damages to the ocean. Global coral loss, fisheries disruption, and coastal infrastructure destruction are estimated to cost nearly $2 trillion annually, fundamentally changing how we measure climate finance. “For decades, we’ve been estimating the economic cost of climate change while effectively assigning a value of zero to the ocean,” said Bernardo Bastien-Olvera, who led the study during his postdoctoral fellowship at Scripps. “Ocean loss is not just an environmental issue, but a central part of the economic story of climate change.”Read full article Comments",
          "feed_position": 0,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2245161347-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2245161347-1152x648.jpg",
      "popularity_score": 349.6830297222222
    },
    {
      "id": "cluster_66",
      "coverage": 1,
      "updated_at": "Sat, 17 Jan 2026 12:00:11 +0000",
      "title": "Meta’s layoffs leave Supernatural fitness users in mourning",
      "neutral_headline": "Meta’s layoffs leave Supernatural fitness users in mourning",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2026/01/metas-layoffs-leave-supernatural-fitness-users-in-mourning/",
          "published_at": "Sat, 17 Jan 2026 12:00:11 +0000",
          "title": "Meta’s layoffs leave Supernatural fitness users in mourning",
          "standfirst": "Supernatural has had its staff cut and won’t receive any more content updates.",
          "content": "Tencia Benavidez, a Supernatural user who lives in New Mexico, started her VR workouts during the Covid pandemic. She has been a regular user in the five years since, calling the ability to work out in VR ideal, given that she lives in a rural area where it’s hard to get to a gym or work out outside during a brutal winter. She stuck with Supernatural because of the community and the eagerness of Supernatural’s coaches. “They seem like really authentic individuals that were not talking down to you,” Benavidez says. “There's just something really special about those coaches.” Meta bought Supernatural in 2022, folding it into its then-heavily-invested-in metaverse efforts. The purchase was not a smooth process, as it triggered a lengthy legal battle in which the US Federal Trade Commission tried to block Meta from purchasing the service due to antitrust concerns about Meta “trying to buy its way to the top” of the VR market. Meta ultimately prevailed. At the time, some Supernatural users were cautiously optimistic, hoping that big bag of Zuckerbucks could keep its workout juggernaut afloat.Read full article Comments",
          "feed_position": 1,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2023/06/06_Lifestyle-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2023/06/06_Lifestyle-1152x648.jpg",
      "popularity_score": 323
    },
    {
      "id": "cluster_79",
      "coverage": 1,
      "updated_at": "Sat, 17 Jan 2026 04:45:33 +0000",
      "title": "Managers on alert for “launch fever” as pressure builds for NASA’s Moon mission",
      "neutral_headline": "Managers on alert for “launch fever” as pressure builds for NASA’s Moon mission",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2026/01/managers-on-alert-for-launch-fever-as-pressure-builds-for-nasas-moon-mission/",
          "published_at": "Sat, 17 Jan 2026 04:45:33 +0000",
          "title": "Managers on alert for “launch fever” as pressure builds for NASA’s Moon mission",
          "standfirst": "\"I’ve got one job, and it’s the safe return of Reid, Victor, Christina, and Jeremy.\"",
          "content": "KENNEDY SPACE CENTER, Florida—The rocket NASA is preparing to send four astronauts on a trip around the Moon will emerge from its assembly building on Florida's Space Coast early Saturday for a slow crawl to its seaside launch pad. Riding atop one of NASA's diesel-powered crawler transporters, the Space Launch System rocket and its mobile launch platform will exit the Vehicle Assembly Building at Kennedy Space Center around 7:00 am EST (11:00 UTC). The massive tracked transporter, certified by Guinness as the world's heaviest self-propelled vehicle, is expected to cover the four miles between the assembly building and Launch Complex 39B in about eight to 10 hours. The rollout marks a major step for NASA's Artemis II mission, the first human voyage to the vicinity of the Moon since the last Apollo lunar landing in December 1972. Artemis II will not land. Instead, a crew of four astronauts will travel around the far side of the Moon at a distance of several thousand miles, setting the record for the farthest humans have ever ventured from Earth.Read full article Comments",
          "feed_position": 2,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/Artemis-II-Trajectory-Thumbnail-1-1152x648-1768622903.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/Artemis-II-Trajectory-Thumbnail-1-1152x648-1768622903.jpg",
      "popularity_score": 319
    },
    {
      "id": "cluster_5",
      "coverage": 1,
      "updated_at": "2026-01-18T23:19:56.089Z",
      "title": "Archaeologists find a supersized medieval shipwreck in Denmark",
      "neutral_headline": "Archaeologists find a supersized medieval shipwreck in Denmark",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2026/01/archaeologists-find-a-supersized-medieval-shipwreck-in-denmark/",
          "published_at": "2026-01-18T23:19:56.089Z",
          "title": "Archaeologists find a supersized medieval shipwreck in Denmark",
          "standfirst": "",
          "content": "",
          "feed_position": 5
        }
      ],
      "popularity_score": 308.9997211111111
    },
    {
      "id": "cluster_7",
      "coverage": 1,
      "updated_at": "2026-01-18T23:19:56.089Z",
      "title": "OpenAI to test ads in ChatGPT as it burns through billions",
      "neutral_headline": "OpenAI to test ads in ChatGPT as it burns through billions",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/information-technology/2026/01/openai-to-test-ads-in-chatgpt-as-it-burns-through-billions/",
          "published_at": "2026-01-18T23:19:56.089Z",
          "title": "OpenAI to test ads in ChatGPT as it burns through billions",
          "standfirst": "",
          "content": "",
          "feed_position": 7
        }
      ],
      "popularity_score": 302.9997211111111
    },
    {
      "id": "cluster_6",
      "coverage": 1,
      "updated_at": "2026-01-18T23:19:56.089Z",
      "title": "Judge orders Anna’s Archive to delete scraped data; no one thinks it will comply",
      "neutral_headline": "Judge orders Anna’s Archive to delete scraped data; no one thinks it will comply",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2026/01/judge-orders-annas-archive-to-delete-scraped-data-no-one-thinks-it-will-comply/",
          "published_at": "2026-01-18T23:19:56.089Z",
          "title": "Judge orders Anna’s Archive to delete scraped data; no one thinks it will comply",
          "standfirst": "",
          "content": "",
          "feed_position": 6
        }
      ],
      "popularity_score": 292.9997211111111
    },
    {
      "id": "cluster_8",
      "coverage": 1,
      "updated_at": "2026-01-18T23:19:56.089Z",
      "title": "Mandiant releases rainbow table that cracks weak admin password in 12 hours",
      "neutral_headline": "Mandiant releases rainbow table that cracks weak admin password in 12 hours",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/security/2026/01/mandiant-releases-rainbow-table-that-cracks-weak-admin-password-in-12-hours/",
          "published_at": "2026-01-18T23:19:56.089Z",
          "title": "Mandiant releases rainbow table that cracks weak admin password in 12 hours",
          "standfirst": "",
          "content": "",
          "feed_position": 8
        }
      ],
      "popularity_score": 287.9997211111111
    },
    {
      "id": "cluster_9",
      "coverage": 1,
      "updated_at": "2026-01-18T23:19:56.089Z",
      "title": "RAM shortage chaos expands to GPUs, high-capacity SSDs, and even hard drives",
      "neutral_headline": "RAM shortage chaos expands to GPUs, high-capacity SSDs, and even hard drives",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2026/01/ram-shortage-chaos-expands-to-gpus-high-capacity-ssds-and-even-hard-drives/",
          "published_at": "2026-01-18T23:19:56.089Z",
          "title": "RAM shortage chaos expands to GPUs, high-capacity SSDs, and even hard drives",
          "standfirst": "",
          "content": "",
          "feed_position": 9
        }
      ],
      "popularity_score": 268.9997211111111
    }
  ]
}