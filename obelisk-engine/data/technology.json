{
  "updated_at": "2025-10-30T19:15:52.255Z",
  "clusters": [
    {
      "id": "cluster_7",
      "coverage": 2,
      "updated_at": "Thu, 30 Oct 2025 18:30:00 +0000",
      "title": "Oakley Meta Vanguard review: Sporty to a fault",
      "neutral_headline": "Oakley Meta Vanguard review: Sporty to a fault",
      "items": [
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/wearables/oakley-meta-vanguard-review-sporty-to-a-fault-183000829.html",
          "published_at": "Thu, 30 Oct 2025 18:30:00 +0000",
          "title": "Oakley Meta Vanguard review: Sporty to a fault",
          "standfirst": "By now, I have a well-established routine when I set up a new pair of Meta smart glasses. I connect my Instagram, WhatsApp and Spotify accounts. I complete the slightly convoluted steps in my Bluetooth settings to make sure Meta AI can announce incoming phone calls and text messages. I tweak the video settings to the highest quality available, and change the voice of Meta AI to \"English (UK)\" so it can talk to me in the voice of Judi Dench. But with the $499 Oakley Meta Vanguard glasses, there's also a new step: deciding what the customizable \"action button\" should do. The action button isn't even my favorite part of using the glasses, but it's a sign of just how different these shades are from the rest of Meta's lineup. While the second-gen Ray-Ban and Oakley HSTN glasses iterated on the same formula Meta has used for the last few years, the Vanguard glasses are refreshingly different. They aren't really meant to be everyday sunglasses (unless you're really committed to your athletic pursuits) but they are in many ways more capable than Meta's other smart glasses. The speakers are louder, the camera has new abilities and they integrate directly with Strava and Garmin. And while these won't replace my go-to sunglasses, there's more than enough to make them part of my fitness routine. New look, new setup The sunglasses were very clearly made with athletes in mind. The Oakley Meta Vanguard glasses are the type of shades a lot of people probably think of when they hear \"Oakley sunglasses.\" The wraparound frames with colorful, reflective lenses are the style of glasses you might associate with a high school track coach, or your neighbor who is really serious about cycling. The pair I tested had black frames and Oakley's orange \"Prizm 24K\" lenses, which aren't polarized but are favored by a lot of athletes for their ability to dial up the contrast of your surroundings. I was able to comfortably wear my pair in bright, sunny conditions and also in more overcast lower light. I also appreciate that the lenses are swappable, so you can switch them out for a dedicated low-light or different-colored lens depending on your conditions. (Extra lenses cost $85 each and will be available to purchase separately soon, according to Meta.) These glasses don't, however, support prescription lenses of any kind. I wouldn't wear these as everyday sunglasses, but I don't mind the look for a trail run. Karissa Bell for Engadget I realize this style of sunglasses won't be appealing to everyone, but the frame shape does enable a slightly different setup than what we've seen with any of Meta's other smart glasses. Most noticeably, the camera is in the center of the glasses, just above the nosebridge. The LED that lights up when the camera is on is also in the center, near the top of the frames. As with Meta's other smart glasses, you can control volume and music playback via a touchpad on the right side of the glasses, but the capture button to take photos and videos is now on the underside of the glasses rather than on top. This is meant to make it a bit easier to reach if you're wearing a hat or helmet, though I found it took me a few tries to get used to the new placement. Behind the capture button is the previously mentioned \"action button,\" which can be customized to trigger specific functions via the Meta AI app. The capture button (left) and the action button (right) are both on the underside of the frames rather than on top. Karissa Bell for Engadget I haven't yet figured out what the best use for the action button is, though I've tried out a few different setups. On one hike, I set it up to automatically call my husband, kind of like a speed dial. During a bike ride, I had it set to record a hyperlapse video. I've also tried it out as a shortcut for launching a specific Spotify playlist or as a general trigger for Meta AI. With all of these, I appreciated that the action button allowed me to do something without saying the \"Hey Meta,\" command. Repeating \"hey Meta\" to my glasses in public has always felt a bit cringey, so it was nice to have a much more subtle cue available. Did I mention it's for athletes? The Vanguard's athlete-focused features go beyond the sportier frames. The shades come with new integrations for two of the most popular run and bike-tracking platforms: Garmin and Strava. If you have a supported Garmin watch or bike computer, you can set up the glasses to automatically capture video clips based on metrics from your activity, like hitting a particular heart rate zone or other milestone. You can also ask Meta AI directly to tell you about stats from your Garmin watch, like \"hey Meta, what's my pace.\" I don't have a Garmin watch, though I did briefly test out some of these features during my hands-on at Meta Connect. I suspect a lot of runners and cyclists may still find it easier to simply glance at their watch to see stats, but having it all available via voice commands doesn't seem like a bad thing either. Strava's integration isn't quite as deep. If you're tracking a run, hike or ride while wearing the glasses, you can overlay your stats directly onto photos and videos from your activity. This includes metrics like distance and elevation, as well as heart rate if you're also wearing an Apple Watch or other tracker that's connected to the Strava app. Here's what it looks like with a photo from a recent bike ride. You can overlay your Strava stats onto the photos and videos you record. Karissa Bell for Engadget I typically don’t share stats from runs or bike rides (usually because they aren't that impressive) but it's a bit more appealing that just sharing a straight Strava screenshot. Another neat feature is that if you share a video, you can watch the stats change in real time alongside your recording. That level of detail isn't particularly interesting for a mostly flat bike ride on a city street, but I can see how it would be a lot more compelling on a more technical trail ride or in a race. My only complaint, really, is that Meta has limited these kinds of features to Garmin and Strava's platforms so far. I'd love to have support for my favorite ski-tracking app, Slopes, and I'm sure there are plenty of people who'd be happy to have an integration with their running or workout-tracking app of choice. Meta has announced some plans to bring more third-party apps onto its smart glasses platform so there might be hope here. There are other improvements, though, that will be appealing to even casual athletes. The speakers are a lot louder to account for potentially noisy conditions like a congested roadway or high-wind environment. I never had to crank the volume up anywhere near the max during my bike rides or runs, but I can say the speakers were loud and clear enough that I was able to comfortably listen to a podcast with the glasses laying next to me on the couch at full volume. The new centered camera placement is meant to make it harder for a hat or helmet to interfere with your shots, which has been a consistent issue for me with Meta's other smart glasses. The new position didn't totally solve this — I still found that my bike helmet made it into the top of my pics — but at least it's easier to crop out now that my headgear is centered over the top of my image rather than awkwardly sticking out on one side. The 12MP ultra-wide camera also comes with new video stabilization settings that make it feel a bit more like a replacement for an action cam. The glasses are set to automatically select a level of stabilization based on your motion, but you can also manually choose between low, medium or high stabilization (stabilization is locked at \"medium\" if you opt to record in 3K). I've mostly left it with the default settings and have been impressed with the results. The LED light is also a bit more subtle than on Meta's other smart glasses. Karissa Bell for Engadget The Vanguard glasses are also Meta's first smart glasses that can record hyperlapse and slow-motion video. Hyperlapse should be familiar to Instagram users who used the now-defunct app of the same name to record timelapse clips. Now, you can say \"Hey Meta, start a hyperlapse\" and the glasses will record a similar sped-up clip. My hyperlapse clips ended up looking a bit jittery, though, compared to the timelapse shots I'm used to getting with my GoPro. And unfortunately, there's no way to adjust the cadence of the video like you used to be able to with the dedicated app. My slow-motion clips, on the other hand, came out better. It's not something I'd expect to use very often during a bike ride or trail run, but the POV angle is great for recording clips of pets or kids. Meta is also planning to bring support for hyperlapse and slow-motion videos to the rest of its glasses lineup, though, so you don't need to get these particular shades to take advantage of the feature. The other major improvement is battery life. The Vanguard glasses have a notably better battery life compared with the second-gen Ray-Ban glasses or the HSTN frames (probably because the bigger frames allow for a larger battery). According to Meta, the Vanguard glasses can go nine hours on a charge with \"typical use\" or six hours with continuous audio playback. I was actually able to get a little over six hours of audio on a single charge, so they should hold up pretty well if you're running marathons or competing in longer races. As usual, exact battery life can vary a lot depending on how much you're using more resource-intensive features like video recording or Meta AI. The bigger frames and charging case give the glasses a battery life boost. Karissa Bell for Engadget I'm especially looking forward to seeing how these glasses will hold up during a day of snowboarding. Meta previously told me that the battery has been optimized for a wider spectrum of temperatures so hopefully the battery won't drain as quickly on the mountain as Meta's other glasses. And with increased water resistance — the shades have an IP67 rating — I wouldn't worry about dropping them in the snow. Should you buy these? While Meta and EssilorLuxottica have gotten very good at making smart glasses (sorry Mark Zuckerberg, I won't call them \"AI glasses,\") they are still somewhat of a niche product. And the ultra-sporty Oakley Vanguard glasses are even more niche. At $499, these are also more expensive than other models. That, understandably, may feel too steep for a pair of sunglasses you're likely only going to wear during specific activities. But if you're a dedicated cyclist, runner, hiker or [insert outdoor activity of your choice], there's a lot to like. The camera makes a lot more sense for action cam-like POV footage, and better video stabilization means you're more likely to get shots you actually want to share. Ready-made Garmin and Strava integrations are practically begging for you to brag about your latest PR or race time, which will certainly appeal to many. This article originally appeared on Engadget at https://www.engadget.com/wearables/oakley-meta-vanguard-review-sporty-to-a-fault-183000829.html?src=rss",
          "content": "By now, I have a well-established routine when I set up a new pair of Meta smart glasses. I connect my Instagram, WhatsApp and Spotify accounts. I complete the slightly convoluted steps in my Bluetooth settings to make sure Meta AI can announce incoming phone calls and text messages. I tweak the video settings to the highest quality available, and change the voice of Meta AI to \"English (UK)\" so it can talk to me in the voice of Judi Dench. But with the $499 Oakley Meta Vanguard glasses, there's also a new step: deciding what the customizable \"action button\" should do. The action button isn't even my favorite part of using the glasses, but it's a sign of just how different these shades are from the rest of Meta's lineup. While the second-gen Ray-Ban and Oakley HSTN glasses iterated on the same formula Meta has used for the last few years, the Vanguard glasses are refreshingly different. They aren't really meant to be everyday sunglasses (unless you're really committed to your athletic pursuits) but they are in many ways more capable than Meta's other smart glasses. The speakers are louder, the camera has new abilities and they integrate directly with Strava and Garmin. And while these won't replace my go-to sunglasses, there's more than enough to make them part of my fitness routine. New look, new setup The sunglasses were very clearly made with athletes in mind. The Oakley Meta Vanguard glasses are the type of shades a lot of people probably think of when they hear \"Oakley sunglasses.\" The wraparound frames with colorful, reflective lenses are the style of glasses you might associate with a high school track coach, or your neighbor who is really serious about cycling. The pair I tested had black frames and Oakley's orange \"Prizm 24K\" lenses, which aren't polarized but are favored by a lot of athletes for their ability to dial up the contrast of your surroundings. I was able to comfortably wear my pair in bright, sunny conditions and also in more overcast lower light. I also appreciate that the lenses are swappable, so you can switch them out for a dedicated low-light or different-colored lens depending on your conditions. (Extra lenses cost $85 each and will be available to purchase separately soon, according to Meta.) These glasses don't, however, support prescription lenses of any kind. I wouldn't wear these as everyday sunglasses, but I don't mind the look for a trail run. Karissa Bell for Engadget I realize this style of sunglasses won't be appealing to everyone, but the frame shape does enable a slightly different setup than what we've seen with any of Meta's other smart glasses. Most noticeably, the camera is in the center of the glasses, just above the nosebridge. The LED that lights up when the camera is on is also in the center, near the top of the frames. As with Meta's other smart glasses, you can control volume and music playback via a touchpad on the right side of the glasses, but the capture button to take photos and videos is now on the underside of the glasses rather than on top. This is meant to make it a bit easier to reach if you're wearing a hat or helmet, though I found it took me a few tries to get used to the new placement. Behind the capture button is the previously mentioned \"action button,\" which can be customized to trigger specific functions via the Meta AI app. The capture button (left) and the action button (right) are both on the underside of the frames rather than on top. Karissa Bell for Engadget I haven't yet figured out what the best use for the action button is, though I've tried out a few different setups. On one hike, I set it up to automatically call my husband, kind of like a speed dial. During a bike ride, I had it set to record a hyperlapse video. I've also tried it out as a shortcut for launching a specific Spotify playlist or as a general trigger for Meta AI. With all of these, I appreciated that the action button allowed me to do something without saying the \"Hey Meta,\" command. Repeating \"hey Meta\" to my glasses in public has always felt a bit cringey, so it was nice to have a much more subtle cue available. Did I mention it's for athletes? The Vanguard's athlete-focused features go beyond the sportier frames. The shades come with new integrations for two of the most popular run and bike-tracking platforms: Garmin and Strava. If you have a supported Garmin watch or bike computer, you can set up the glasses to automatically capture video clips based on metrics from your activity, like hitting a particular heart rate zone or other milestone. You can also ask Meta AI directly to tell you about stats from your Garmin watch, like \"hey Meta, what's my pace.\" I don't have a Garmin watch, though I did briefly test out some of these features during my hands-on at Meta Connect. I suspect a lot of runners and cyclists may still find it easier to simply glance at their watch to see stats, but having it all available via voice commands doesn't seem like a bad thing either. Strava's integration isn't quite as deep. If you're tracking a run, hike or ride while wearing the glasses, you can overlay your stats directly onto photos and videos from your activity. This includes metrics like distance and elevation, as well as heart rate if you're also wearing an Apple Watch or other tracker that's connected to the Strava app. Here's what it looks like with a photo from a recent bike ride. You can overlay your Strava stats onto the photos and videos you record. Karissa Bell for Engadget I typically don’t share stats from runs or bike rides (usually because they aren't that impressive) but it's a bit more appealing that just sharing a straight Strava screenshot. Another neat feature is that if you share a video, you can watch the stats change in real time alongside your recording. That level of detail isn't particularly interesting for a mostly flat bike ride on a city street, but I can see how it would be a lot more compelling on a more technical trail ride or in a race. My only complaint, really, is that Meta has limited these kinds of features to Garmin and Strava's platforms so far. I'd love to have support for my favorite ski-tracking app, Slopes, and I'm sure there are plenty of people who'd be happy to have an integration with their running or workout-tracking app of choice. Meta has announced some plans to bring more third-party apps onto its smart glasses platform so there might be hope here. There are other improvements, though, that will be appealing to even casual athletes. The speakers are a lot louder to account for potentially noisy conditions like a congested roadway or high-wind environment. I never had to crank the volume up anywhere near the max during my bike rides or runs, but I can say the speakers were loud and clear enough that I was able to comfortably listen to a podcast with the glasses laying next to me on the couch at full volume. The new centered camera placement is meant to make it harder for a hat or helmet to interfere with your shots, which has been a consistent issue for me with Meta's other smart glasses. The new position didn't totally solve this — I still found that my bike helmet made it into the top of my pics — but at least it's easier to crop out now that my headgear is centered over the top of my image rather than awkwardly sticking out on one side. The 12MP ultra-wide camera also comes with new video stabilization settings that make it feel a bit more like a replacement for an action cam. The glasses are set to automatically select a level of stabilization based on your motion, but you can also manually choose between low, medium or high stabilization (stabilization is locked at \"medium\" if you opt to record in 3K). I've mostly left it with the default settings and have been impressed with the results. The LED light is also a bit more subtle than on Meta's other smart glasses. Karissa Bell for Engadget The Vanguard glasses are also Meta's first smart glasses that can record hyperlapse and slow-motion video. Hyperlapse should be familiar to Instagram users who used the now-defunct app of the same name to record timelapse clips. Now, you can say \"Hey Meta, start a hyperlapse\" and the glasses will record a similar sped-up clip. My hyperlapse clips ended up looking a bit jittery, though, compared to the timelapse shots I'm used to getting with my GoPro. And unfortunately, there's no way to adjust the cadence of the video like you used to be able to with the dedicated app. My slow-motion clips, on the other hand, came out better. It's not something I'd expect to use very often during a bike ride or trail run, but the POV angle is great for recording clips of pets or kids. Meta is also planning to bring support for hyperlapse and slow-motion videos to the rest of its glasses lineup, though, so you don't need to get these particular shades to take advantage of the feature. The other major improvement is battery life. The Vanguard glasses have a notably better battery life compared with the second-gen Ray-Ban glasses or the HSTN frames (probably because the bigger frames allow for a larger battery). According to Meta, the Vanguard glasses can go nine hours on a charge with \"typical use\" or six hours with continuous audio playback. I was actually able to get a little over six hours of audio on a single charge, so they should hold up pretty well if you're running marathons or competing in longer races. As usual, exact battery life can vary a lot depending on how much you're using more resource-intensive features like video recording or Meta AI. The bigger frames and charging case give the glasses a battery life boost. Karissa Bell for Engadget I'm especially looking forward to seeing how these glasses will hold up during a day of snowboarding. Meta previously told me that the battery has been optimized for a wider spectrum of temperatures so hopefully the battery won't drain as quickly on the mountain as Meta's other glasses. And with increased water resistance — the shades have an IP67 rating — I wouldn't worry about dropping them in the snow. Should you buy these? While Meta and EssilorLuxottica have gotten very good at making smart glasses (sorry Mark Zuckerberg, I won't call them \"AI glasses,\") they are still somewhat of a niche product. And the ultra-sporty Oakley Vanguard glasses are even more niche. At $499, these are also more expensive than other models. That, understandably, may feel too steep for a pair of sunglasses you're likely only going to wear during specific activities. But if you're a dedicated cyclist, runner, hiker or [insert outdoor activity of your choice], there's a lot to like. The camera makes a lot more sense for action cam-like POV footage, and better video stabilization means you're more likely to get shots you actually want to share. Ready-made Garmin and Strava integrations are practically begging for you to brag about your latest PR or race time, which will certainly appeal to many. This article originally appeared on Engadget at https://www.engadget.com/wearables/oakley-meta-vanguard-review-sporty-to-a-fault-183000829.html?src=rss",
          "feed_position": 2,
          "image_url": "https://d29szjachogqwa.cloudfront.net/images/user-uploaded/kb_vanguard.jpg"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/gaming/fractal-design-scape-review-a-stellar-debut-173000007.html",
          "published_at": "Thu, 30 Oct 2025 17:30:00 +0000",
          "title": "Fractal Design Scape review: A stellar debut",
          "standfirst": "Unless you're a PC nerd like me, chances are you're not familiar with Fractal Design. The company has made a name for itself in recent years by designing some of the best cases you can buy for a DIY build. In a space known for its gaudy aesthetics, Fractal's products stand out for their simplicity. Now the company is entering the crowded audio space with the $200 Scape, a gaming headset that not only looks sophisticated, but also sounds surprisingly great too. Design The Fractal Scape features an attractive mix of materials. Igor Bonifacic for Engadget I mentioned the design of the Scape first, and for good reason. It shows attention to detail, with a lot of thoughtful flourishes. The best of those is the dock that comes included with the headset. It charges the Scape inductively, so there's no need to align any charging pins, and it cleverly houses the headset's 2.4GHz wireless transmitter. When connected to your main PC, the dongle can sit inside the dock, ready to go when you want to use it with your PlayStation 5, PS4, Nintendo Switch or another PC or Mac (sorry, Microsoft fans, there's no Xbox support). The base also has wire channels to make cable management easy. Those same thoughtful design touches extend to the headset itself. On the back are four buttons, a dial and a toggle that cover nearly every function of the Scape. You can adjust the volume, mute the built-in mic, switch between 2.4GHz and Bluetooth connectivity, power the headset on or off, turn the RGB lighting on or off and switch between three EQ presets. What’s more, all of the controls feel distinct and are easy to use. There's also a USB-C connection for wired audio and a three-pole headphone jack for the detachable microphone. The headset is primarily made of plastic, with a touch of brushed metal. Fractal offers the Scape in two colors — the aptly named light and dark — and despite the company's choice of materials, the headset feels undoubtedly premium. Tilt adjustment is limited — it's not possible to lay the ear pads flat on a table, for example — but the headband offers a fair amount of resistance, adding to the high-end feel. That said, the Scape could be more comfortable. Clamping force feels just about right, but there's not enough padding along the top of the headband. I found I could wear the headphones for a few hours, but I eventually had to take them off to relieve the pressure that had built up on the top of my head. I'm also not a fan of the fabric Fractal used for both the headband and ear pads. It feels scratchy and it's not great at dissipating heat. Thankfully, the high-density memory foam beneath is plush and the pads were deep and wide enough to comfortably accommodate my ears. Fractal has made it easy to swap the ear pads if needed; they come right off with just a small amount of force. For the time being, the company isn't selling replacements, but a spokesperson told me Fractal will send customers who need new pads a set for free. You just need to contact their support team. Sound quality The Scape comes with a set of custom-tuned drivers. Igor Bonifacic for Engadget Out of the box, the Scape's dynamic drivers are tuned to a soft v-shaped curve, with an emphasis on accuracy over character. Bass frequencies are punchy without being bloated, and there's nice detail to mid-focused instruments like guitars. To my ear, the one issue with the Scape's default tuning were the upper mids and treble frequencies. They weren't shouty to the point of being sibilant, but there was definitely a harshness to the vocals of singers like Jeff Buckley and Caroline Polachek who are known for their falsetto. Thankfully, this was easy to fix with the Scape's built-in EQ settings. I'll have more to say in the software section of this review, but Fractal's Adjust app allows you to make parametric EQ adjustments. This is different to most gaming headsets, which often limit people to restrictive fixed-band tweaks. Even when I used just five potential points of customization, I found I had a great deal of control over the tonality of the Scape. Overall, for casual music listening these are excellent headphones capable of covering many different genres. When it comes to gaming, they're great too — with some caveats. For singleplayer games, the default tunings are an excellent match. Playing Ghost of Tsushima, the Scape did a great job of reproducing the game's immersive sound design, allowing me to soak in all the little audio details Sucker Punch packed into its rendition of 13th century Japan. The soundstaging isn't as expansive and lush as I'm used to with my Sennheiser HD 600, but for a pair of closed-back headphones, it's above average. When it comes to competitive first-person shooters, some tweaks are required. I found all the default presets produced too much bass to easily isolate sound cues in games like Valorant. Again, Fractal's software made this simple to fix; however, the Scape can only store three presets. Some gaming headsets, particularly those from Steelseries, come with tunings for hundreds of different games. For the ultra-competitive gamer, this can be useful since every game has a different sound engine. I'm not one of those people, so I found the Scape had just the right amount of customization. A closeup of the Scape's volume dial. Igor Bonifacic for Engadget One feature you won't find on these headphones is active noise cancellation (ANC). Don't get me wrong, ANC is great, but I also didn't feel like the Scape was a worse product without it. My girlfriend is a Pelton fanatic and she does all of her daily classes without wearing headphones in our small apartment. In that situation, the Scape's passive noise isolation was enough to block the loud music coming from those workouts. That said, the one area where the Scape could have been better is Bluetooth connectivity. It's a 5.3 headset, but codec support is limited to SBC and AAC. If you've ever tried a pair of wireless headphones only to be disappointed by how they sounded, SBC was probably to blame. When I used the Scape over Bluetooth, there was a subtle difference, but in a beat 'em up like Absolum, the game's excellent sound design was less effective because it sounded more compressed, with more lag between what was happening on-screen and the effects that followed. I would have liked to see Fractal support more modern codecs like aptX Adaptive, but given that you need a dongle like the Creative BT-W6 to get access to those protocols on PCs and consoles, I can't fault the company for its decision. Also missing from the Bluetooth equation is multipoint support, meaning I wasn't able to connect the Scape to my PC or Switch and my iPhone at the same time. The Scape's detachable microphone is fine but uninspiring. In listening back to a chat I had with some friends over Discord, I found the Scape compressed my voice to the point where there wasn't a lot of life or nuance to it. The optional noise cancellation algorithm does a good job of filtering out nearby commotion, but does so at the expense of adding more compression. You can flip the microphone to mute yourself, and if you're feeling lazy, the Scape also has a built-in mic — though it sounds about as good as you might imagine. Still, it's handy in a pinch. Software Fractal's Adjust app is web-based and easy to use. Igor Bonifacic for Engadget I mentioned Fractal's Adjust software, but what I didn't note is that it's not an app you need to install on your computer. Instead, it's a website you can access through any Chrome-based browser. It's a small thing, but one I really appreciate. I can't count how many times I've had to troubleshoot issues with Windows that were caused by a conflict created by Logitech G Hub or NZXT Cam. The web-based software itself is simple, with two pages, named Lighting and Audio, encompassing all the options you can tweak. The Scape comes with 10 lighting themes out of the box, and like the headset itself, these presets are understated and classy. Naturally, you can also create your own lighting schemes, and the tool for doing so is fairly robust. A closeup of the Fractal Scape's EQ button Igor Bonifacic for Engadget When it comes to the EQ adjustments you can make with the Adjust app, the one thing I'll add here is any presets you save are stored on the Scape, so they're available everywhere you want to use the headset. That was great because it meant I didn't need to Alt-Tab out of a game to switch the headset to a more competitive tuning. The one area where Fractal's software feels lacking is when it comes to microphone controls. It's possible to adjust sidetone (the volume of your mic input as heard through the headset) and enable microphone noise cancellation, but that’s it. It'd be nice if it was possible to configure those settings for the two mics independently of one another, but the software doesn't support that right now. Battery life According to Fractal, the Scape can go up to 40 hours on a single charge with RGB lighting off. With the feature turned on, battery life drops to about 26 hours. I'll be honest, I had a tough time putting those claims to the test because of how easy it is to charge the headset. As best as I can tell, those estimates are accurate. I managed to get three days of battery life from the Scape with the RGB lights turned on and about eight hours of use each day. One nice touch: when you tap the power button, the RGB lights will briefly illuminate to indicate how much battery life the Scape has left. Those same lights will turn off when you place the headset to charge so that they're not distracting. The competition With the Scape, Fractal has entered a crowded market. That said, the Scape is competitive with some of the best gaming headsets you can buy right now. At $200, it's $100 cheaper than the Audeze Maxwell, Engadget's pick for the best premium gaming headset. The Scape doesn't sound as good as the Maxwell or offer LDAC support for Bluetooth connectivity, but it's lighter and charging is easier thanks to the included charging base. If you ask me, the Scape also looks a lot better too. If you can't live without ANC, your best bet is Razer's BlackShark V3 Pro, but it costs $50 more and doesn't sound as good as the Scape. You also need to put up with Razer's annoying Synapse software. For the best mic on a gaming headset, my longstanding recommendation has been the $199 Drop PC38X. It also has one of the best default tunings for competitive gaming. However, it's a wired headset, and Drop doesn’t offer a wireless option. Wrap-up The charging station also houses the Scape's 2.4GHz transmitter. Igor Bonifacic for Engadget If you can't tell by now, I think the Fractal Scape is a great gaming headset. There are models like the Audeze Maxwell that beat it in one or two categories, but for $200 the Scape is an excellent all-around package. Nitpicks about comfort aside, the Scape sounds and looks great. It's also a tremendous first effort by Fractal, and I can't wait to see what the company does next in the audio space and beyond. This article originally appeared on Engadget at https://www.engadget.com/gaming/fractal-design-scape-review-a-stellar-debut-173000007.html?src=rss",
          "content": "Unless you're a PC nerd like me, chances are you're not familiar with Fractal Design. The company has made a name for itself in recent years by designing some of the best cases you can buy for a DIY build. In a space known for its gaudy aesthetics, Fractal's products stand out for their simplicity. Now the company is entering the crowded audio space with the $200 Scape, a gaming headset that not only looks sophisticated, but also sounds surprisingly great too. Design The Fractal Scape features an attractive mix of materials. Igor Bonifacic for Engadget I mentioned the design of the Scape first, and for good reason. It shows attention to detail, with a lot of thoughtful flourishes. The best of those is the dock that comes included with the headset. It charges the Scape inductively, so there's no need to align any charging pins, and it cleverly houses the headset's 2.4GHz wireless transmitter. When connected to your main PC, the dongle can sit inside the dock, ready to go when you want to use it with your PlayStation 5, PS4, Nintendo Switch or another PC or Mac (sorry, Microsoft fans, there's no Xbox support). The base also has wire channels to make cable management easy. Those same thoughtful design touches extend to the headset itself. On the back are four buttons, a dial and a toggle that cover nearly every function of the Scape. You can adjust the volume, mute the built-in mic, switch between 2.4GHz and Bluetooth connectivity, power the headset on or off, turn the RGB lighting on or off and switch between three EQ presets. What’s more, all of the controls feel distinct and are easy to use. There's also a USB-C connection for wired audio and a three-pole headphone jack for the detachable microphone. The headset is primarily made of plastic, with a touch of brushed metal. Fractal offers the Scape in two colors — the aptly named light and dark — and despite the company's choice of materials, the headset feels undoubtedly premium. Tilt adjustment is limited — it's not possible to lay the ear pads flat on a table, for example — but the headband offers a fair amount of resistance, adding to the high-end feel. That said, the Scape could be more comfortable. Clamping force feels just about right, but there's not enough padding along the top of the headband. I found I could wear the headphones for a few hours, but I eventually had to take them off to relieve the pressure that had built up on the top of my head. I'm also not a fan of the fabric Fractal used for both the headband and ear pads. It feels scratchy and it's not great at dissipating heat. Thankfully, the high-density memory foam beneath is plush and the pads were deep and wide enough to comfortably accommodate my ears. Fractal has made it easy to swap the ear pads if needed; they come right off with just a small amount of force. For the time being, the company isn't selling replacements, but a spokesperson told me Fractal will send customers who need new pads a set for free. You just need to contact their support team. Sound quality The Scape comes with a set of custom-tuned drivers. Igor Bonifacic for Engadget Out of the box, the Scape's dynamic drivers are tuned to a soft v-shaped curve, with an emphasis on accuracy over character. Bass frequencies are punchy without being bloated, and there's nice detail to mid-focused instruments like guitars. To my ear, the one issue with the Scape's default tuning were the upper mids and treble frequencies. They weren't shouty to the point of being sibilant, but there was definitely a harshness to the vocals of singers like Jeff Buckley and Caroline Polachek who are known for their falsetto. Thankfully, this was easy to fix with the Scape's built-in EQ settings. I'll have more to say in the software section of this review, but Fractal's Adjust app allows you to make parametric EQ adjustments. This is different to most gaming headsets, which often limit people to restrictive fixed-band tweaks. Even when I used just five potential points of customization, I found I had a great deal of control over the tonality of the Scape. Overall, for casual music listening these are excellent headphones capable of covering many different genres. When it comes to gaming, they're great too — with some caveats. For singleplayer games, the default tunings are an excellent match. Playing Ghost of Tsushima, the Scape did a great job of reproducing the game's immersive sound design, allowing me to soak in all the little audio details Sucker Punch packed into its rendition of 13th century Japan. The soundstaging isn't as expansive and lush as I'm used to with my Sennheiser HD 600, but for a pair of closed-back headphones, it's above average. When it comes to competitive first-person shooters, some tweaks are required. I found all the default presets produced too much bass to easily isolate sound cues in games like Valorant. Again, Fractal's software made this simple to fix; however, the Scape can only store three presets. Some gaming headsets, particularly those from Steelseries, come with tunings for hundreds of different games. For the ultra-competitive gamer, this can be useful since every game has a different sound engine. I'm not one of those people, so I found the Scape had just the right amount of customization. A closeup of the Scape's volume dial. Igor Bonifacic for Engadget One feature you won't find on these headphones is active noise cancellation (ANC). Don't get me wrong, ANC is great, but I also didn't feel like the Scape was a worse product without it. My girlfriend is a Pelton fanatic and she does all of her daily classes without wearing headphones in our small apartment. In that situation, the Scape's passive noise isolation was enough to block the loud music coming from those workouts. That said, the one area where the Scape could have been better is Bluetooth connectivity. It's a 5.3 headset, but codec support is limited to SBC and AAC. If you've ever tried a pair of wireless headphones only to be disappointed by how they sounded, SBC was probably to blame. When I used the Scape over Bluetooth, there was a subtle difference, but in a beat 'em up like Absolum, the game's excellent sound design was less effective because it sounded more compressed, with more lag between what was happening on-screen and the effects that followed. I would have liked to see Fractal support more modern codecs like aptX Adaptive, but given that you need a dongle like the Creative BT-W6 to get access to those protocols on PCs and consoles, I can't fault the company for its decision. Also missing from the Bluetooth equation is multipoint support, meaning I wasn't able to connect the Scape to my PC or Switch and my iPhone at the same time. The Scape's detachable microphone is fine but uninspiring. In listening back to a chat I had with some friends over Discord, I found the Scape compressed my voice to the point where there wasn't a lot of life or nuance to it. The optional noise cancellation algorithm does a good job of filtering out nearby commotion, but does so at the expense of adding more compression. You can flip the microphone to mute yourself, and if you're feeling lazy, the Scape also has a built-in mic — though it sounds about as good as you might imagine. Still, it's handy in a pinch. Software Fractal's Adjust app is web-based and easy to use. Igor Bonifacic for Engadget I mentioned Fractal's Adjust software, but what I didn't note is that it's not an app you need to install on your computer. Instead, it's a website you can access through any Chrome-based browser. It's a small thing, but one I really appreciate. I can't count how many times I've had to troubleshoot issues with Windows that were caused by a conflict created by Logitech G Hub or NZXT Cam. The web-based software itself is simple, with two pages, named Lighting and Audio, encompassing all the options you can tweak. The Scape comes with 10 lighting themes out of the box, and like the headset itself, these presets are understated and classy. Naturally, you can also create your own lighting schemes, and the tool for doing so is fairly robust. A closeup of the Fractal Scape's EQ button Igor Bonifacic for Engadget When it comes to the EQ adjustments you can make with the Adjust app, the one thing I'll add here is any presets you save are stored on the Scape, so they're available everywhere you want to use the headset. That was great because it meant I didn't need to Alt-Tab out of a game to switch the headset to a more competitive tuning. The one area where Fractal's software feels lacking is when it comes to microphone controls. It's possible to adjust sidetone (the volume of your mic input as heard through the headset) and enable microphone noise cancellation, but that’s it. It'd be nice if it was possible to configure those settings for the two mics independently of one another, but the software doesn't support that right now. Battery life According to Fractal, the Scape can go up to 40 hours on a single charge with RGB lighting off. With the feature turned on, battery life drops to about 26 hours. I'll be honest, I had a tough time putting those claims to the test because of how easy it is to charge the headset. As best as I can tell, those estimates are accurate. I managed to get three days of battery life from the Scape with the RGB lights turned on and about eight hours of use each day. One nice touch: when you tap the power button, the RGB lights will briefly illuminate to indicate how much battery life the Scape has left. Those same lights will turn off when you place the headset to charge so that they're not distracting. The competition With the Scape, Fractal has entered a crowded market. That said, the Scape is competitive with some of the best gaming headsets you can buy right now. At $200, it's $100 cheaper than the Audeze Maxwell, Engadget's pick for the best premium gaming headset. The Scape doesn't sound as good as the Maxwell or offer LDAC support for Bluetooth connectivity, but it's lighter and charging is easier thanks to the included charging base. If you ask me, the Scape also looks a lot better too. If you can't live without ANC, your best bet is Razer's BlackShark V3 Pro, but it costs $50 more and doesn't sound as good as the Scape. You also need to put up with Razer's annoying Synapse software. For the best mic on a gaming headset, my longstanding recommendation has been the $199 Drop PC38X. It also has one of the best default tunings for competitive gaming. However, it's a wired headset, and Drop doesn’t offer a wireless option. Wrap-up The charging station also houses the Scape's 2.4GHz transmitter. Igor Bonifacic for Engadget If you can't tell by now, I think the Fractal Scape is a great gaming headset. There are models like the Audeze Maxwell that beat it in one or two categories, but for $200 the Scape is an excellent all-around package. Nitpicks about comfort aside, the Scape sounds and looks great. It's also a tremendous first effort by Fractal, and I can't wait to see what the company does next in the audio space and beyond. This article originally appeared on Engadget at https://www.engadget.com/gaming/fractal-design-scape-review-a-stellar-debut-173000007.html?src=rss",
          "feed_position": 6,
          "image_url": "https://d29szjachogqwa.cloudfront.net/images/user-uploaded/fractal-scape-1.jpg"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/cybersecurity/vpn/how-to-cancel-norton-vpn-uninstall-it-and-get-your-money-back-150000872.html",
          "published_at": "Thu, 30 Oct 2025 15:00:00 +0000",
          "title": "How to cancel Norton VPN, uninstall it and get your money back",
          "standfirst": "Norton Security has some reliable products, but its VPN isn't one of them. In my Norton VPN review, I argued that it's only really worthwhile if you can get a discount on it as part of a Norton 360 package — and even in that case, you should only use it for non-sensitive activities due to some holes in Norton's comprehensive privacy policy. That's a lot of conditions, so I'd understand if you're here because you've decided Norton VPN isn't for you. Read on to learn your options for cancelling this VPN, getting a refund and replacing it with a better provider. How to turn off Norton VPN auto-renewal The simplest way to cancel Norton VPN is to stop your subscription from automatically renewing. That way, you'll have until your plan expires to look for a new VPN. Note that the steps below are the same no matter how you got Norton VPN, whether on its own or as part of Norton 360 — though they only apply if you subscribed through the Norton website, not an app store. Sign into your account at my.norton.com. You'll be taken to your account dashboard with your subscriptions tab visible. If it's not, click on My Subscriptions. On your subscriptions hub, find the plan you get Norton VPN through. Click the words Manage Renewal or Cancel Subscription Renewal. In the window that appears, click Unsubscribe. Select a reason for cancellation (no need to be truthful) and click Next. At this point, you'll have to wade through several pleas for you to stay. Stand firm and keep clicking through until you can click No thanks, cancel my subscription. Continue clicking Next until you see a confirmation that auto-renewal has been turned off. Wait 24 hours for the change to take effect. Sam Chapman for Engadget If you change your mind after turning auto-renewal off, you can turn it back on again anytime before the subscription expires. For those who bought through an app store, there's no way to turn off auto-renewal; you can only cancel the subscription altogether. See the end of the next section to learn how to do that. How to cancel Norton VPN and get a refund You can request a refund on any annual subscription for 60 days after paying. Monthly subscriptions can only be refunded once, within 14 days of paying — if you renew a monthly plan then decide to cancel, you're out of luck. The only way to get a refund is to contact Norton directly. If you're ready to go cold turkey, follow these steps. In a browser, open support.norton.com. Scroll down until you see nine buttons arranged in a 3x3 grid. Find the second button down in the left-hand column, Contact us, and click on it. Enter the email address for your Norton account. Check that inbox for a verification code, then enter it in the next box and click Verify. When the live chat asks you what you need help with, select Purchase & Billing, then Request refund. Give a reason in the dropdown menu. As usual, be persistent until you get a message in writing that your refund will be processed. Wait at least three days for the money to appear. Sam Chapman for Engadget If you subscribed through the Apple App Store or Google Play Store, you'll have to cancel through the same platform where you started. Just go into the subscriptions page of the store's mobile app, find your Norton VPN subscription and click the Cancel button beside it. After that, just follow the prompts, then request a refund using the steps above. How to uninstall Norton VPN To get your money back from Norton, you can't just shut off auto-renewal. You'll have to cancel your plan immediately and delete all Norton apps from your devices. I recommend following these steps even if you aren't eligible for a refund, since Norton software is notoriously hard to uninstall and will crop back up if you don't completely root it out. On Android and iOS, uninstalling Norton VPN is relatively easy — after cancelling your subscription, delete it like you would any other app. Things are a bit trickier on the desktop OSes. On Windows, hold the Windows key and press R to make a black box appear. Type appwiz.cpl and hit Enter. A list of programs should appear; click on Norton VPN, then click Uninstall/Change and follow the instructions. On a Mac, open your Applications folder and find Norton VPN. Click the app icon and drag it to the trash. This should start a separate program called Norton Uninstaller. Click OK, enter your password if asked, then click Uninstall. Finally, you'll need to restart your computer to finish uninstalling. Norton VPN alternatives Once you've dispensed with Norton VPN, you can get started with a provider that fits your needs better. Proton VPN, my current top pick in our guide to the best VPNs, takes privacy more seriously than Norton and has superior app design and speeds. Surfshark is the fastest VPN, NordVPN has the best features and ExpressVPN is the friendliest for beginners.This article originally appeared on Engadget at https://www.engadget.com/cybersecurity/vpn/how-to-cancel-norton-vpn-uninstall-it-and-get-your-money-back-150000872.html?src=rss",
          "content": "Norton Security has some reliable products, but its VPN isn't one of them. In my Norton VPN review, I argued that it's only really worthwhile if you can get a discount on it as part of a Norton 360 package — and even in that case, you should only use it for non-sensitive activities due to some holes in Norton's comprehensive privacy policy. That's a lot of conditions, so I'd understand if you're here because you've decided Norton VPN isn't for you. Read on to learn your options for cancelling this VPN, getting a refund and replacing it with a better provider. How to turn off Norton VPN auto-renewal The simplest way to cancel Norton VPN is to stop your subscription from automatically renewing. That way, you'll have until your plan expires to look for a new VPN. Note that the steps below are the same no matter how you got Norton VPN, whether on its own or as part of Norton 360 — though they only apply if you subscribed through the Norton website, not an app store. Sign into your account at my.norton.com. You'll be taken to your account dashboard with your subscriptions tab visible. If it's not, click on My Subscriptions. On your subscriptions hub, find the plan you get Norton VPN through. Click the words Manage Renewal or Cancel Subscription Renewal. In the window that appears, click Unsubscribe. Select a reason for cancellation (no need to be truthful) and click Next. At this point, you'll have to wade through several pleas for you to stay. Stand firm and keep clicking through until you can click No thanks, cancel my subscription. Continue clicking Next until you see a confirmation that auto-renewal has been turned off. Wait 24 hours for the change to take effect. Sam Chapman for Engadget If you change your mind after turning auto-renewal off, you can turn it back on again anytime before the subscription expires. For those who bought through an app store, there's no way to turn off auto-renewal; you can only cancel the subscription altogether. See the end of the next section to learn how to do that. How to cancel Norton VPN and get a refund You can request a refund on any annual subscription for 60 days after paying. Monthly subscriptions can only be refunded once, within 14 days of paying — if you renew a monthly plan then decide to cancel, you're out of luck. The only way to get a refund is to contact Norton directly. If you're ready to go cold turkey, follow these steps. In a browser, open support.norton.com. Scroll down until you see nine buttons arranged in a 3x3 grid. Find the second button down in the left-hand column, Contact us, and click on it. Enter the email address for your Norton account. Check that inbox for a verification code, then enter it in the next box and click Verify. When the live chat asks you what you need help with, select Purchase & Billing, then Request refund. Give a reason in the dropdown menu. As usual, be persistent until you get a message in writing that your refund will be processed. Wait at least three days for the money to appear. Sam Chapman for Engadget If you subscribed through the Apple App Store or Google Play Store, you'll have to cancel through the same platform where you started. Just go into the subscriptions page of the store's mobile app, find your Norton VPN subscription and click the Cancel button beside it. After that, just follow the prompts, then request a refund using the steps above. How to uninstall Norton VPN To get your money back from Norton, you can't just shut off auto-renewal. You'll have to cancel your plan immediately and delete all Norton apps from your devices. I recommend following these steps even if you aren't eligible for a refund, since Norton software is notoriously hard to uninstall and will crop back up if you don't completely root it out. On Android and iOS, uninstalling Norton VPN is relatively easy — after cancelling your subscription, delete it like you would any other app. Things are a bit trickier on the desktop OSes. On Windows, hold the Windows key and press R to make a black box appear. Type appwiz.cpl and hit Enter. A list of programs should appear; click on Norton VPN, then click Uninstall/Change and follow the instructions. On a Mac, open your Applications folder and find Norton VPN. Click the app icon and drag it to the trash. This should start a separate program called Norton Uninstaller. Click OK, enter your password if asked, then click Uninstall. Finally, you'll need to restart your computer to finish uninstalling. Norton VPN alternatives Once you've dispensed with Norton VPN, you can get started with a provider that fits your needs better. Proton VPN, my current top pick in our guide to the best VPNs, takes privacy more seriously than Norton and has superior app design and speeds. Surfshark is the fastest VPN, NordVPN has the best features and ExpressVPN is the friendliest for beginners.This article originally appeared on Engadget at https://www.engadget.com/cybersecurity/vpn/how-to-cancel-norton-vpn-uninstall-it-and-get-your-money-back-150000872.html?src=rss",
          "feed_position": 14,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2025-10/8fa7e830-b512-11f0-bff6-091d2e25ea4a"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/best-streaming-service-deals-133028980.html",
          "published_at": "Thu, 30 Oct 2025 10:01:27 +0000",
          "title": "The best streaming deals: Save on Hulu + Live TV, Audible, Starz and more",
          "standfirst": "Streaming services keep raising prices. At this point, if you subscribe to all the major services out there, you're basically paying the same price as cable — those antiquated local monopolies that streaming was supposed to save us from. But streaming still has one big advantage over the old ways: no contracts. That means you can grab a good streaming deal and then cancel without penalty. Our advice is to sign up for a service when you see a good streaming deal (or the latest season of, say, Doctor Who, Severance, Andor and/or The Last of Us). Then, when the deal ends or you've binged whatever it is you want to watch, cancel as needed. But streaming deals don't come around all that often and, when they do, it's easy to miss them thanks to...everything. So we're keeping eyes out for the best streaming deals out there and we update this guide often — so check it out the next time you have a hankering to watch something new. Best streaming deals True streaming deals can be hard to come by. Most often, they’ll pop up during the Black Friday shopping period. On occasion, we’ll see them sparingly throughout the year and they usually take the form of a discounted monthly or annual rate for a limited period of time. Also, true streaming deals are typically on the ad-supported versions of a service, but once in a while you’ll find a unicorn of a deal on a tier that has ad-free viewing. If you’re able to wait for a deal before subscribing to a streaming service, we recommend doing so. You’ll save money upfront and in the long run, and you also have the option to cancel your subscription before the price goes back up to the normal rate. Audible subscription (three months) for $3 ($42 off): From now through mid-December, you can get Amazon’s audiobook subscription for just a dollar a month for three months. Note that it will auto-renew at $15 per month after that, but you can cancel at any point. Starz (one year) for $30 ($40 off): Pay upfront for one year and you can get $40 off a Stars annual subscription. There's a month-to-month option too, which costs $5 per month for the first three months if you don't want to commit to the full year. Either option gives you access to the entire Starz TV and movie library with offline viewing and no ads. DirecTV starting at $50/month for one month ($35 off): All of DirecTV's signature packages are $35 off right now for your first month when you sign up. If you opt for the base \"Entertainment\" package, you'll spend $50 for the first month and get access to over 90 channels, including many local stations as well as ESPN, ESPN 2 and Fox Sports 1. You'll also be able to watch on the go with the DirecTV mobile app. Fubo Pro for $55/month for the first month ($30 off): Fubo has introductory discounts on most of its packages, and the Pro package is the least expensive plan currently listed. It offers access to 224 channels, unlimited cloud DVR and up to 10 simultaneous streams. It even includes regional sports content from the NHL, MLB and NBA. Spotify Premium Individual (3 month) for $0 ($36 off): This is our favorite music streaming service for podcasts and social features. The Premium Individual plan lets you listen ad-free and skip songs at will. You can also organize your listening queue and download content for offline listening. Just be aware, your subscription will auto-renew at the end of the trial period. So if you don't want to be on the hook for the $12 monthly fee, set a reminder to cancel and go back to the free version. Streaming bundle discounts There’s more consolidation happening now than ever before in the streaming space, and that means there are more streaming bundle options. These bundles offer you access to more content with one subscription price, but those prices are typically higher than paying for a single service by itself (obviously). It may be tempting to just get the bundle, but if only one of those services in the bundle speaks to you, you’ll spend less overall by just paying for the single service. Speaking of a deep love for a single streaming service: if all of your favorite shows are on Peacock or the latest releases on HBO Max consistently bring you joy, consider paying for one year upfront. Subscribing with an annual plan usually saves you money in the long term over paying on a monthly basis. Unfortunately, not all streaming services (looking at you, Netflix) have an annual subscription option. Disney+ If you feel like Charlie Kelly trying to figure out who Pepe Silvia is when you look at Disney's streaming prices chart, you're not alone. The confusion comes from the fact that Disney owns, or has a hand in, many streaming services including Hulu and ESPN. Throw in a partnership with HBO Max and you have a ton of options to consider and, probably, whiplash to match. Here's a quick overview of popular Disney+ bundle pricing. Disney+ and Hulu bundle (with ads) — $13/month Disney+ and Hulu bundle (without ads) — $20/month Disney+, Hulu and ESPN Select (with ads) — $20/month Disney+, Hulu and ESPN Select (without ads on Disney+ and Hulu only) — $30/month Disney+, Hulu and HBO Max (with ads) — $20/month Disney+, Hulu and HBO Max (without ads) — $33/month Peacock TV Peacock doesn't have any streaming bundles available all year round, but you can save if you pay for one year upfront. Peacock Select (with ads) — $8/month or $80/year Peacock Premium (with ads) — $11/month or $110/year Peacock Premium Plus (without ads) — $17/month or $170/year Paramount+ Paramount+ used to bill its tier with Showtime as a sort of bundle, but it has since renamed its plans and focused the Showtime inclusion in its premium tier as just another bonus of paying for the higher priced plan. Paramount+ Essential (with ads) —$8/month or $60/year Paramount Premium (without ads) — $13/month or $120/year Student discounts on streaming services It pays to be a student — sometimes, at least. A number of streaming services have student discounts you can take advantage of as long as you're actively studying. What that translates to most of the time is being able to verify your student status and signing up with your .edu email address. HBO Max student discount — subscribe for $5/month (50 percent off): HBO Max offers their ad-supported tier to students for half off the usual rate. You’ll just have to verify that you’re a student through Unidays, and make note that this offer is only good for up to 12 months of service. Hulu student discount — subscribe for $2/month (75 percent off): Those with a valid student ID can get Hulu’s ad-supported tier for 75 percent off the typical rate. They’ll keep the same sale price for as long as they’re a student as well. Spotify student discount — Premium + Hulu with ads for $6/month (72 percent off): Spotify’s student offer continues to be one of the best around, giving you access to the Premium tier of the music streamer and Hulu’s ad-supported plan for only $6 monthly. Purchased separately, you’d pay $22 per month for both of the services. Plus, the first month is free when you sign up. NBA League Pass student discount — one year for $120 (40 percent off): Students can get one year of League Pass for only $10 per month, which includes access to NBA TV and the ability to watch classic and archive games on-demand. On the NBA League Pass website, look for the student discount banner at the top and follow the instructions to verify your student status. Read more streaming coverage The best live TV streaming services to cut cable The best streaming services: Netflix, Hulu, HBO Max and more The best streaming devices Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/best-streaming-service-deals-133028980.html?src=rss",
          "content": "Streaming services keep raising prices. At this point, if you subscribe to all the major services out there, you're basically paying the same price as cable — those antiquated local monopolies that streaming was supposed to save us from. But streaming still has one big advantage over the old ways: no contracts. That means you can grab a good streaming deal and then cancel without penalty. Our advice is to sign up for a service when you see a good streaming deal (or the latest season of, say, Doctor Who, Severance, Andor and/or The Last of Us). Then, when the deal ends or you've binged whatever it is you want to watch, cancel as needed. But streaming deals don't come around all that often and, when they do, it's easy to miss them thanks to...everything. So we're keeping eyes out for the best streaming deals out there and we update this guide often — so check it out the next time you have a hankering to watch something new. Best streaming deals True streaming deals can be hard to come by. Most often, they’ll pop up during the Black Friday shopping period. On occasion, we’ll see them sparingly throughout the year and they usually take the form of a discounted monthly or annual rate for a limited period of time. Also, true streaming deals are typically on the ad-supported versions of a service, but once in a while you’ll find a unicorn of a deal on a tier that has ad-free viewing. If you’re able to wait for a deal before subscribing to a streaming service, we recommend doing so. You’ll save money upfront and in the long run, and you also have the option to cancel your subscription before the price goes back up to the normal rate. Audible subscription (three months) for $3 ($42 off): From now through mid-December, you can get Amazon’s audiobook subscription for just a dollar a month for three months. Note that it will auto-renew at $15 per month after that, but you can cancel at any point. Starz (one year) for $30 ($40 off): Pay upfront for one year and you can get $40 off a Stars annual subscription. There's a month-to-month option too, which costs $5 per month for the first three months if you don't want to commit to the full year. Either option gives you access to the entire Starz TV and movie library with offline viewing and no ads. DirecTV starting at $50/month for one month ($35 off): All of DirecTV's signature packages are $35 off right now for your first month when you sign up. If you opt for the base \"Entertainment\" package, you'll spend $50 for the first month and get access to over 90 channels, including many local stations as well as ESPN, ESPN 2 and Fox Sports 1. You'll also be able to watch on the go with the DirecTV mobile app. Fubo Pro for $55/month for the first month ($30 off): Fubo has introductory discounts on most of its packages, and the Pro package is the least expensive plan currently listed. It offers access to 224 channels, unlimited cloud DVR and up to 10 simultaneous streams. It even includes regional sports content from the NHL, MLB and NBA. Spotify Premium Individual (3 month) for $0 ($36 off): This is our favorite music streaming service for podcasts and social features. The Premium Individual plan lets you listen ad-free and skip songs at will. You can also organize your listening queue and download content for offline listening. Just be aware, your subscription will auto-renew at the end of the trial period. So if you don't want to be on the hook for the $12 monthly fee, set a reminder to cancel and go back to the free version. Streaming bundle discounts There’s more consolidation happening now than ever before in the streaming space, and that means there are more streaming bundle options. These bundles offer you access to more content with one subscription price, but those prices are typically higher than paying for a single service by itself (obviously). It may be tempting to just get the bundle, but if only one of those services in the bundle speaks to you, you’ll spend less overall by just paying for the single service. Speaking of a deep love for a single streaming service: if all of your favorite shows are on Peacock or the latest releases on HBO Max consistently bring you joy, consider paying for one year upfront. Subscribing with an annual plan usually saves you money in the long term over paying on a monthly basis. Unfortunately, not all streaming services (looking at you, Netflix) have an annual subscription option. Disney+ If you feel like Charlie Kelly trying to figure out who Pepe Silvia is when you look at Disney's streaming prices chart, you're not alone. The confusion comes from the fact that Disney owns, or has a hand in, many streaming services including Hulu and ESPN. Throw in a partnership with HBO Max and you have a ton of options to consider and, probably, whiplash to match. Here's a quick overview of popular Disney+ bundle pricing. Disney+ and Hulu bundle (with ads) — $13/month Disney+ and Hulu bundle (without ads) — $20/month Disney+, Hulu and ESPN Select (with ads) — $20/month Disney+, Hulu and ESPN Select (without ads on Disney+ and Hulu only) — $30/month Disney+, Hulu and HBO Max (with ads) — $20/month Disney+, Hulu and HBO Max (without ads) — $33/month Peacock TV Peacock doesn't have any streaming bundles available all year round, but you can save if you pay for one year upfront. Peacock Select (with ads) — $8/month or $80/year Peacock Premium (with ads) — $11/month or $110/year Peacock Premium Plus (without ads) — $17/month or $170/year Paramount+ Paramount+ used to bill its tier with Showtime as a sort of bundle, but it has since renamed its plans and focused the Showtime inclusion in its premium tier as just another bonus of paying for the higher priced plan. Paramount+ Essential (with ads) —$8/month or $60/year Paramount Premium (without ads) — $13/month or $120/year Student discounts on streaming services It pays to be a student — sometimes, at least. A number of streaming services have student discounts you can take advantage of as long as you're actively studying. What that translates to most of the time is being able to verify your student status and signing up with your .edu email address. HBO Max student discount — subscribe for $5/month (50 percent off): HBO Max offers their ad-supported tier to students for half off the usual rate. You’ll just have to verify that you’re a student through Unidays, and make note that this offer is only good for up to 12 months of service. Hulu student discount — subscribe for $2/month (75 percent off): Those with a valid student ID can get Hulu’s ad-supported tier for 75 percent off the typical rate. They’ll keep the same sale price for as long as they’re a student as well. Spotify student discount — Premium + Hulu with ads for $6/month (72 percent off): Spotify’s student offer continues to be one of the best around, giving you access to the Premium tier of the music streamer and Hulu’s ad-supported plan for only $6 monthly. Purchased separately, you’d pay $22 per month for both of the services. Plus, the first month is free when you sign up. NBA League Pass student discount — one year for $120 (40 percent off): Students can get one year of League Pass for only $10 per month, which includes access to NBA TV and the ability to watch classic and archive games on-demand. On the NBA League Pass website, look for the student discount banner at the top and follow the instructions to verify your student status. Read more streaming coverage The best live TV streaming services to cut cable The best streaming services: Netflix, Hulu, HBO Max and more The best streaming devices Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/best-streaming-service-deals-133028980.html?src=rss",
          "feed_position": 29
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/mobile/tablets/best-ereader-130013808.html",
          "published_at": "Thu, 30 Oct 2025 09:01:26 +0000",
          "title": "The best ereaders for 2025",
          "standfirst": "Color is the buzziest feature in ereaders right now, but is it necessary? It makes the covers more fun, and readers of comics and graphic novels will appreciate the added hues (though they may be happier with an E Ink tablet for better image detail). Color is just one factor to consider when picking out the best ereader. The lights, screen quality, housing and buttons make a difference too. Then there’s the software: Do you want to stay in the Kindle ecosystem or get a device that can handle lots of apps? We tested more than a dozen ereaders to come up with our recommendations. This guide also points out ways to get the most out of your new e-book companion once you pick your favorite.Editor’s note: Amazon announced two new Kindles at its fall hardware event: the Scribe 3 and the Scribe Colorsoft. Reviews for both of these writing tablets are forthcoming and will also appear in our guide to E Ink tablets. No new Kindle ereaders were announced at the event. Boox announced two new devices as well, a new Note Air5 C tablet and the Palma 2 Pro, both with color. We’re in the process of testing the latter for inclusion in this guide.. Table of contents Best ereaders What to look for in an ereader How to get books for your ereader How we test ereaders Ereader FAQs Recent updates The best ebook readers for 2025 What to look for in an ereader Plenty of apps, including the Kindle app, will let you download and read digital books on an iPhone, Android Phone or tablet. But what makes ebook readers different is the screen: nearly all of them use technology from a company called E Ink. It manufactures electronic paper displays (EPD) composed of three sheets: one containing millions of microcapsules filled with black and white ink particles sandwiched between transparent electrode layers. When a charge is applied, either the black or white particles shift to the top, forming letters and the whitespace around them. Color ereaders add a color filter array on top of the standard black and white microcapsule layer. The result is two different resolutions on one screen — the color clarity is 150 dpi while black and white images and text are still 300 dpi. Because these displays are so different from standard backlight LED panels, you can expect most good ereaders to do a number of things well. They’ll be easier to stare at for long periods of time and easier to read in direct sunlight. Also, since E Ink displays only require power to rearrange the ink, these devices have much longer battery lives than even the best tablets: we’re talking weeks on a single charge, not days. The ereader market is not as saturated as, say, the smartphone market, but there are still plenty of options out there and they do have small but important differences among them. They tend to range from around $100 to more than $400, though usually the higher end options are stylus-enabled read/write E Ink tablets like the Kindle Scribe. Beyond price, you should consider physical properties like buttons, lights, storage and resolution, as well as how the software lets you find and access books. Reading features With any ereader, you’ll navigate the OS via taps and swipes, and some add physical page-turn buttons. Most with built-in buttons have an auto-rotating screen so you can press with your right or left hand. As E Ink technology has advanced, resolution has greatly improved – even the budget Kindle ereader has a 300 ppi display. You can still find models with lower resolution, but we don’t recommend them. Some ereaders have front LEDs that support light temperature adjustment. That means you can switch to a warmer light after the sun goes down, which will feel easier on the eyes. If you’re concerned about blue light, you should go for a reader with that feature. Finally, dark mode is available on most later model ereaders, allowing you to invert the black and white text and background, which some people find easier to read in low-light settings. Other features The capabilities of these pocket libraries have advanced considerably since the early days. In addition to storing books, some let you browse the web, run apps and play music. The screen’s frame rate can’t handle gaming, but it’s good enough to show you the Wikipedia entry for Striver’s Row while you read Crook Manifesto. If you listen to audiobooks, you may want a Bluetooth-enabled ereader capable of playing them. Most of the models we tested have that ability, with the notable exception of the Nook ereader we tried. Keep in mind that audiobook files can take up more space than print files so you'll probably want a device with a higher storage capacity if you plan on doing a lot of listening. Above all, you should consider where and how you intend to find books to read. Most ereaders make it easiest to shop through their own digital bookstores, but all of them (even Kindles) will now let you download titles from other sources, like libraries, unaffiliated ebook sellers and free public domain sites. Photo by Amy Skorheim / Engadget How to get books for your ereader Kindle, Nook and Kobo all have their own stores that you access directly from each brand’s devices. Prices are the same among all sellers, too. Publishers set the price of an ebook, not the retailer, so a title will cost the same at Amazon, Barnes & Noble, eBooks.com and the Kobo store. Amazon offers Kindle Unlimited for $12 per month, and it includes four million titles from which you can pick your next read. It includes audio and ebooks, but you won’t find many big, new releases or older bestsellers. Kobo has a subscription called Kobo Plus with about 1.3 million titles: it goes for $8 per month for ebooks only, $8 for audiobooks only or $10 for both. Buying a book from a proprietary store instantly delivers it to your device, provided you’re connected to WiFi. It also syncs your reading across devices and apps, so you can pick up where you left off on your phone if you forgot your ereader at home. It truly is the most convenient way to go, but if you don’t want to be locked into one brand’s store, or if you opt for an ereader without its own marketplace, you do have options. How to upload ePubs onto an ereader Stores like ebooks.com and Google Play have millions of ebooks for sale as digital rights-managed (DRM) ePub files, which Kobo, Nook and PocketBook readers can read in their native ereader apps. Kindles don’t support DRM ePub files at all and Boox devices require third party reading apps (of which there are many) to read those files. Titles from Apple Books are only readable in iOS devices. Titles from some publishers like Tor and public domain classics from sites like Project Gutenberg are also sold as ePubs, but without the added DRM. Consequently, Kindles and the Boox Neoreader do support those files. Books you get from third-party sources will look just like ones you bought from a proprietary store, thanks to the flowable, formatted nature of ePub files. While these device-agnostic ebook collections give you extra options for finding your next read, they require a few additional steps to get the files onto your ereader. To do so, you’ll typically need a computer running a free program called Adobe Digital Editions (ADE). After buying and downloading the ePub file, open ADE and plug your ereader into your computer (all readers here have a USB-C port for charging and data transfers). Your device should pop up in the left panel. Drag and drop the ePub file from your downloads folder into the main panel in ADE. The file will display as an image of the book cover. Drag that image onto your device on the left panel. If the file includes digital rights management (which protects against unauthorized copying) you’ll need to authorize your ereader, which requires using or creating a free Adobe ID. Once you’ve finished adding files to upload, eject the reader from your computer to complete the transfer process. Kindles use a web-based uploader instead of the ADE method. But since Kindle uses its own proprietary DRM technology instead of Adobe's, the only files it can accept from third parties are non-DRM files, such as from Tor Publishing or Project Gutenberg. After downloading a compatible ePub file, drag and drop it into your browser with the Send to Kindle page open. As long as you’re signed into Amazon, this wirelessly transfers the files to your associated device. Boox also uses a browser uploader called BooxDrop (along with many other methods) to deliver ePubs to the device. Open it from the Boox App menu and you’ll see a device-specific url. Type that into your browser to access a file delivery portal that uploads to your library. Boox’s built-in ereader app, NeoReader, also doesn’t support files with DRM, so you won’t be able to read current titles from most publishers using that app. Fortunately, Boox devices run nearly every ereader app out there, Kobo and Kindle included, letting you access ePubs any number of ways. Recently, Bookshop.org, the online seller of physical books that supports indie bookstores, started selling ebooks and up to 100 percent of the profits will go to local booksellers. The company uses a different rights management system than ADE so, right now, you can only read titles you buy from them on the Bookshop.org app, but the company is working with the makers of both Kindle and Kobo to extend compatibility to those ereaders. How to read library books on an ereader Your local library card lets you borrow audio and ebooks through a program called Overdrive and its companion app Libby. On a Kobo, you have have built-in access to Overdrive in a separate tab. Once you’ve linked your public library card, the search function will include results for titles available from your local library system; a few taps will upload your selections to your device for the length of the loan. I personally find it easiest to borrow the title I want through the Libby app on my phone. After that, the book pops up on my Kobo’s home screen once the device syncs. To read library books on a Kindle, you can either go through the Libby app or the Overdrive section of your library’s website. Once you click Borrow, you’ll see the option to “Read now with Kindle,” which takes you to Amazon’s site to sign in. After that, the book will be delivered to your device the next time it connects to WiFi. For other ereaders, you’ll go through your library’s Overdrive portal and download the ePub after clicking the Borrow button. You can then use the ADE process we described above. Devices that run external apps, like Boox's Page, Go Color 7 or Palma, allow you to read library books via the Libby app, just as you would on a smartphone or iPad. You can also use the Libby app to borrow audiobooks, but you won’t be able to access them through your ereader. (The exception is an ereader, like a Boox device, that allows external apps). I found it was easier to listen to an audiobook on my phone anyway, regardless of whether I borrowed it through Libby or bought it from Kindle or Kobo. Photo by Amy Skorheim / Engadget How we test ereaders When putting together any guide, the first thing we do is spend hours researching the field. We look at what’s available, what’s new, and what shoppers and professional reviewers have to say. Then we narrow a list to the best candidates for hands-on testing. Over the course of the past two years, I’ve tested just over a dozen ereaders, representing five different brands: Amazon, Kobo, Barnes & Noble, Boox and PocketBook. I bought, borrowed and uploaded books for each device using the methods above. I used each one for between a few days to a few months. I evaluated each one in the areas of book access, ease of reading, extra features and overall value. Here’s everything we tested so far: Amazon Kindle (2022) Amazon Kindle (2024) Amazon Kindle Paperwhite (2021) Amazon Kindle Paperwhite Signature (2024) Boox Go Color 7 Boox Leaf 2 Boox Page Boox Poke 5 Boox Palma Kobo Libra 2 Kobo Libra Colour Kobo Clara 2E Kobo Clara Colour Kobo Nia NOOK GlowLight 4 PocketBook Era Other ereaders we tested Amazon Kindle Colorsoft Amazon’s first color Kindle impressed with its quick page-turns and load times, auto-adjusting front light and, of course, a decently striking color E Ink display. But at $280, it’s more expensive than all of the other color ereaders in its size range, including the Kobo Libra Colour and the Boox Go Color 7. Also, some Colorsoft owners reported seeing a yellow band at the bottom of their ereader’s display. This issue did not affect our first review unit during the original testing period, but it eventually appeared. An Amazon spokesperson told Engadget: \"A small number of customers have reported a yellow band along the bottom of the display. We take the quality of our products seriously—customers who notice this can reach out to our customer service team for a replacement or refund, and we’re making the appropriate adjustments to ensure that new devices will not experience this issue moving forward.\" Amazon sent us a new Colorsoft ereader at the end of 2024 and it does appear that the fixes the company made resolved the yellow-band issue. The screen on our second review unit appears warmer overall, but not overly so. It’s more akin to the screen on the Kobo Libra Colour, and that’s a good thing. Boox Go 7 stylus-enabled ereader Boox recently released two new stylus-enabled generations of their seven-inch reader: The monochrome Go 7 and a color-screen Go Color 7 (Gen II). After trying out the stylus-enabled Go 7, I still like the standard, non-stylus enabled version better. True, I liked the Notebook app with its array of handwriting templates and I appreciated the low-to-no latency with the stylus. It also offers a good assortment of brush, pen and style options. But outside of the Notebook app, I didn’t find much use for the stylus. I was able to doodle in the margins of DRM-free books in Boox’s native NeoReader, but it doesn’t work in other apps or on any rights-managed books. There's a FreeMark option that allows you to draw or write atop any app, but it saves your doodles as separate images, as opposed to allowing you to mark up the page itself. I also found enabling the stylus to be a little glitchy. If you plan to do a lot of writing, you’ll probably be better served by an E Ink tablet, but if you want an ereader that can mark up your books, I suggest going with the Kobo Libra Color, detailed above. Ereader FAQs What's the difference between an ereader and an e-ink reader? Really, they are the same thing. E Ink is a company that designs and manufactures the paper-like screens found in most ereader devices. Technically, anything you read ebooks on can act as an ereader, so your phone, iPad or Android tablet could all serve that purpose, but they’re not considered dedicated ereaders. While there are some devices marketed as ereaders that have LCD or OLED screens instead of E Ink, they aren’t as common. One of the benefits of ereaders is the E Ink screen’s paper-like quality, which causes less eye strain for many people. But there is a difference between ereaders and E Ink tablets. These larger e-ink devices also employ E Ink screens, but they have stylus input and are often used for note taking and other tasks in addition to reading ebooks. We have an entire guide devoted to helping you pick out an E Ink tablet. Are there ads on my ereader? The base model Kindle and Kindle Paperwhite come with ads by default, but you can opt to pay $20 to remove them, either at the time of purchase or after you start using the device. The ads are limited to the lockscreen and are typically for other books or Kindle services. Kobo and Boox ereaders don’t come with ads. Which ereader has the longest battery life? Of the devices we tested, the Boox Go Color 7 has the largest listed battery capacity at 2,300mAh (Amazon doesn’t list the capacity of its Kindle devices). But thanks to the nature of E Ink screens and the relatively limited processing power required to display e-books, nearly all ereaders can go for weeks before they need a recharge. That means battery life probably isn’t as much of a deciding factor in buying an ereader as it would be with a tablet or smartphone. Which ereaders can read Kindle books? Amazon’s Kindle ereaders are the obvious answer, but other devices capable of running apps can also read titles from the Kindle store. For example, you can download the Kindle app on a Boox ereader through Google Play (the store comes standard in the Boox app menu). You can then sign into your Kindle account and access all the books in your library — the same way you’d read Kindle books on your phone or tablet. Can you buy Kindle books without a Kindle? Yes. You can buy Kindle books through the Kindle app or through Amazon’s website via a browser. You can read those titles on a Kindle or any device that can run the Kindle app, such as a smartphone, tablet or computer. Just be aware that Kindle titles can only be read through one of Amazon’s ereaders or the Kindle app. The company uses proprietary digital rights management on all ebooks it sells that can’t be read by other ereader apps like Kobo or Adobe ADE. What's the difference between Kindle and Kobo? Both Kindle and Kobo are brands of dedicated ereaders that support searching, buying, downloading and reading ebooks from their own stores. Both also support borrowing books from your local library via Overdrive and Libby. The difference is that Kindle is owned by Amazon and uses the Kindle store, whereas Kobo is owned by Rakuten and its books come from the Kobo store. Both stores come pre-loaded as a tab on their respective ereader and both carry most in-print books. Each store also carries their own exclusive ebooks as well, but Amazon’s library of Kindle-only books is much larger than Kobo’s. Amazon also offers Amazon Original stories to read on the Kindle, which are free short fiction and nonfiction reads that are free to Prime members. Which ereader is best for library books? Both Kobos and Kindles have simple systems for borrowing library books. Other ereaders, like Boox, let you borrow books after downloading the Libby App. Only Kobo ereaders let you search for and borrow books directly on the ereader, with a dedicated Overdrive tab. Kindles, on the other hand, utilize a convenient “read on Kindle” function from the Libby app or website. You can send a borrowed book to your Kindle just by signing into your account. Both methods are pretty easy, so which is the best for you probably depends on other factors than just the library-book feature. Recent updates August 2025: Included new frequently asked questions covering battery life, E Ink screens and ads on ereaders. Mentioned Amazon’s release of a cheaper Colorsoft Kindle. July 2025: Added our impressions of the new stylus-enabled Boox Go 7 series. Updated our Boox Palma recommendation to account for the upgrades to the Boox Palma 2. Included text formats to our specs and the battery life of the Kobo Clara Colour. March 2025: Added news about Bookshop.org getting into the ebook market. Updated information about price-setting by publishers. January 2025: Updated the \"Others we tested\" section to include impressions of the second Kindle Colorsoft review unit we received. August 2024: Replaced our Android tablet pick with the new Go Color 7 ereader from Boox. Updated book titles to current examples. Added an FAQ section to explain the difference between Kobo and Kindle ereaders and further detail library-book support on different models. November 2024: Following the release of Amazon's new Kindle ereaders, we tested and reviewed the Kindle Paperwhite Signature Edition, the base-model Kindle and Amazon's new color ereader, the Kindle Colorsoft. Accordingly, we updated our budget pick, added a premium pick and noted our experience with the Colorsoft. This article originally appeared on Engadget at https://www.engadget.com/mobile/tablets/best-ereader-130013808.html?src=rss",
          "content": "Color is the buzziest feature in ereaders right now, but is it necessary? It makes the covers more fun, and readers of comics and graphic novels will appreciate the added hues (though they may be happier with an E Ink tablet for better image detail). Color is just one factor to consider when picking out the best ereader. The lights, screen quality, housing and buttons make a difference too. Then there’s the software: Do you want to stay in the Kindle ecosystem or get a device that can handle lots of apps? We tested more than a dozen ereaders to come up with our recommendations. This guide also points out ways to get the most out of your new e-book companion once you pick your favorite.Editor’s note: Amazon announced two new Kindles at its fall hardware event: the Scribe 3 and the Scribe Colorsoft. Reviews for both of these writing tablets are forthcoming and will also appear in our guide to E Ink tablets. No new Kindle ereaders were announced at the event. Boox announced two new devices as well, a new Note Air5 C tablet and the Palma 2 Pro, both with color. We’re in the process of testing the latter for inclusion in this guide.. Table of contents Best ereaders What to look for in an ereader How to get books for your ereader How we test ereaders Ereader FAQs Recent updates The best ebook readers for 2025 What to look for in an ereader Plenty of apps, including the Kindle app, will let you download and read digital books on an iPhone, Android Phone or tablet. But what makes ebook readers different is the screen: nearly all of them use technology from a company called E Ink. It manufactures electronic paper displays (EPD) composed of three sheets: one containing millions of microcapsules filled with black and white ink particles sandwiched between transparent electrode layers. When a charge is applied, either the black or white particles shift to the top, forming letters and the whitespace around them. Color ereaders add a color filter array on top of the standard black and white microcapsule layer. The result is two different resolutions on one screen — the color clarity is 150 dpi while black and white images and text are still 300 dpi. Because these displays are so different from standard backlight LED panels, you can expect most good ereaders to do a number of things well. They’ll be easier to stare at for long periods of time and easier to read in direct sunlight. Also, since E Ink displays only require power to rearrange the ink, these devices have much longer battery lives than even the best tablets: we’re talking weeks on a single charge, not days. The ereader market is not as saturated as, say, the smartphone market, but there are still plenty of options out there and they do have small but important differences among them. They tend to range from around $100 to more than $400, though usually the higher end options are stylus-enabled read/write E Ink tablets like the Kindle Scribe. Beyond price, you should consider physical properties like buttons, lights, storage and resolution, as well as how the software lets you find and access books. Reading features With any ereader, you’ll navigate the OS via taps and swipes, and some add physical page-turn buttons. Most with built-in buttons have an auto-rotating screen so you can press with your right or left hand. As E Ink technology has advanced, resolution has greatly improved – even the budget Kindle ereader has a 300 ppi display. You can still find models with lower resolution, but we don’t recommend them. Some ereaders have front LEDs that support light temperature adjustment. That means you can switch to a warmer light after the sun goes down, which will feel easier on the eyes. If you’re concerned about blue light, you should go for a reader with that feature. Finally, dark mode is available on most later model ereaders, allowing you to invert the black and white text and background, which some people find easier to read in low-light settings. Other features The capabilities of these pocket libraries have advanced considerably since the early days. In addition to storing books, some let you browse the web, run apps and play music. The screen’s frame rate can’t handle gaming, but it’s good enough to show you the Wikipedia entry for Striver’s Row while you read Crook Manifesto. If you listen to audiobooks, you may want a Bluetooth-enabled ereader capable of playing them. Most of the models we tested have that ability, with the notable exception of the Nook ereader we tried. Keep in mind that audiobook files can take up more space than print files so you'll probably want a device with a higher storage capacity if you plan on doing a lot of listening. Above all, you should consider where and how you intend to find books to read. Most ereaders make it easiest to shop through their own digital bookstores, but all of them (even Kindles) will now let you download titles from other sources, like libraries, unaffiliated ebook sellers and free public domain sites. Photo by Amy Skorheim / Engadget How to get books for your ereader Kindle, Nook and Kobo all have their own stores that you access directly from each brand’s devices. Prices are the same among all sellers, too. Publishers set the price of an ebook, not the retailer, so a title will cost the same at Amazon, Barnes & Noble, eBooks.com and the Kobo store. Amazon offers Kindle Unlimited for $12 per month, and it includes four million titles from which you can pick your next read. It includes audio and ebooks, but you won’t find many big, new releases or older bestsellers. Kobo has a subscription called Kobo Plus with about 1.3 million titles: it goes for $8 per month for ebooks only, $8 for audiobooks only or $10 for both. Buying a book from a proprietary store instantly delivers it to your device, provided you’re connected to WiFi. It also syncs your reading across devices and apps, so you can pick up where you left off on your phone if you forgot your ereader at home. It truly is the most convenient way to go, but if you don’t want to be locked into one brand’s store, or if you opt for an ereader without its own marketplace, you do have options. How to upload ePubs onto an ereader Stores like ebooks.com and Google Play have millions of ebooks for sale as digital rights-managed (DRM) ePub files, which Kobo, Nook and PocketBook readers can read in their native ereader apps. Kindles don’t support DRM ePub files at all and Boox devices require third party reading apps (of which there are many) to read those files. Titles from Apple Books are only readable in iOS devices. Titles from some publishers like Tor and public domain classics from sites like Project Gutenberg are also sold as ePubs, but without the added DRM. Consequently, Kindles and the Boox Neoreader do support those files. Books you get from third-party sources will look just like ones you bought from a proprietary store, thanks to the flowable, formatted nature of ePub files. While these device-agnostic ebook collections give you extra options for finding your next read, they require a few additional steps to get the files onto your ereader. To do so, you’ll typically need a computer running a free program called Adobe Digital Editions (ADE). After buying and downloading the ePub file, open ADE and plug your ereader into your computer (all readers here have a USB-C port for charging and data transfers). Your device should pop up in the left panel. Drag and drop the ePub file from your downloads folder into the main panel in ADE. The file will display as an image of the book cover. Drag that image onto your device on the left panel. If the file includes digital rights management (which protects against unauthorized copying) you’ll need to authorize your ereader, which requires using or creating a free Adobe ID. Once you’ve finished adding files to upload, eject the reader from your computer to complete the transfer process. Kindles use a web-based uploader instead of the ADE method. But since Kindle uses its own proprietary DRM technology instead of Adobe's, the only files it can accept from third parties are non-DRM files, such as from Tor Publishing or Project Gutenberg. After downloading a compatible ePub file, drag and drop it into your browser with the Send to Kindle page open. As long as you’re signed into Amazon, this wirelessly transfers the files to your associated device. Boox also uses a browser uploader called BooxDrop (along with many other methods) to deliver ePubs to the device. Open it from the Boox App menu and you’ll see a device-specific url. Type that into your browser to access a file delivery portal that uploads to your library. Boox’s built-in ereader app, NeoReader, also doesn’t support files with DRM, so you won’t be able to read current titles from most publishers using that app. Fortunately, Boox devices run nearly every ereader app out there, Kobo and Kindle included, letting you access ePubs any number of ways. Recently, Bookshop.org, the online seller of physical books that supports indie bookstores, started selling ebooks and up to 100 percent of the profits will go to local booksellers. The company uses a different rights management system than ADE so, right now, you can only read titles you buy from them on the Bookshop.org app, but the company is working with the makers of both Kindle and Kobo to extend compatibility to those ereaders. How to read library books on an ereader Your local library card lets you borrow audio and ebooks through a program called Overdrive and its companion app Libby. On a Kobo, you have have built-in access to Overdrive in a separate tab. Once you’ve linked your public library card, the search function will include results for titles available from your local library system; a few taps will upload your selections to your device for the length of the loan. I personally find it easiest to borrow the title I want through the Libby app on my phone. After that, the book pops up on my Kobo’s home screen once the device syncs. To read library books on a Kindle, you can either go through the Libby app or the Overdrive section of your library’s website. Once you click Borrow, you’ll see the option to “Read now with Kindle,” which takes you to Amazon’s site to sign in. After that, the book will be delivered to your device the next time it connects to WiFi. For other ereaders, you’ll go through your library’s Overdrive portal and download the ePub after clicking the Borrow button. You can then use the ADE process we described above. Devices that run external apps, like Boox's Page, Go Color 7 or Palma, allow you to read library books via the Libby app, just as you would on a smartphone or iPad. You can also use the Libby app to borrow audiobooks, but you won’t be able to access them through your ereader. (The exception is an ereader, like a Boox device, that allows external apps). I found it was easier to listen to an audiobook on my phone anyway, regardless of whether I borrowed it through Libby or bought it from Kindle or Kobo. Photo by Amy Skorheim / Engadget How we test ereaders When putting together any guide, the first thing we do is spend hours researching the field. We look at what’s available, what’s new, and what shoppers and professional reviewers have to say. Then we narrow a list to the best candidates for hands-on testing. Over the course of the past two years, I’ve tested just over a dozen ereaders, representing five different brands: Amazon, Kobo, Barnes & Noble, Boox and PocketBook. I bought, borrowed and uploaded books for each device using the methods above. I used each one for between a few days to a few months. I evaluated each one in the areas of book access, ease of reading, extra features and overall value. Here’s everything we tested so far: Amazon Kindle (2022) Amazon Kindle (2024) Amazon Kindle Paperwhite (2021) Amazon Kindle Paperwhite Signature (2024) Boox Go Color 7 Boox Leaf 2 Boox Page Boox Poke 5 Boox Palma Kobo Libra 2 Kobo Libra Colour Kobo Clara 2E Kobo Clara Colour Kobo Nia NOOK GlowLight 4 PocketBook Era Other ereaders we tested Amazon Kindle Colorsoft Amazon’s first color Kindle impressed with its quick page-turns and load times, auto-adjusting front light and, of course, a decently striking color E Ink display. But at $280, it’s more expensive than all of the other color ereaders in its size range, including the Kobo Libra Colour and the Boox Go Color 7. Also, some Colorsoft owners reported seeing a yellow band at the bottom of their ereader’s display. This issue did not affect our first review unit during the original testing period, but it eventually appeared. An Amazon spokesperson told Engadget: \"A small number of customers have reported a yellow band along the bottom of the display. We take the quality of our products seriously—customers who notice this can reach out to our customer service team for a replacement or refund, and we’re making the appropriate adjustments to ensure that new devices will not experience this issue moving forward.\" Amazon sent us a new Colorsoft ereader at the end of 2024 and it does appear that the fixes the company made resolved the yellow-band issue. The screen on our second review unit appears warmer overall, but not overly so. It’s more akin to the screen on the Kobo Libra Colour, and that’s a good thing. Boox Go 7 stylus-enabled ereader Boox recently released two new stylus-enabled generations of their seven-inch reader: The monochrome Go 7 and a color-screen Go Color 7 (Gen II). After trying out the stylus-enabled Go 7, I still like the standard, non-stylus enabled version better. True, I liked the Notebook app with its array of handwriting templates and I appreciated the low-to-no latency with the stylus. It also offers a good assortment of brush, pen and style options. But outside of the Notebook app, I didn’t find much use for the stylus. I was able to doodle in the margins of DRM-free books in Boox’s native NeoReader, but it doesn’t work in other apps or on any rights-managed books. There's a FreeMark option that allows you to draw or write atop any app, but it saves your doodles as separate images, as opposed to allowing you to mark up the page itself. I also found enabling the stylus to be a little glitchy. If you plan to do a lot of writing, you’ll probably be better served by an E Ink tablet, but if you want an ereader that can mark up your books, I suggest going with the Kobo Libra Color, detailed above. Ereader FAQs What's the difference between an ereader and an e-ink reader? Really, they are the same thing. E Ink is a company that designs and manufactures the paper-like screens found in most ereader devices. Technically, anything you read ebooks on can act as an ereader, so your phone, iPad or Android tablet could all serve that purpose, but they’re not considered dedicated ereaders. While there are some devices marketed as ereaders that have LCD or OLED screens instead of E Ink, they aren’t as common. One of the benefits of ereaders is the E Ink screen’s paper-like quality, which causes less eye strain for many people. But there is a difference between ereaders and E Ink tablets. These larger e-ink devices also employ E Ink screens, but they have stylus input and are often used for note taking and other tasks in addition to reading ebooks. We have an entire guide devoted to helping you pick out an E Ink tablet. Are there ads on my ereader? The base model Kindle and Kindle Paperwhite come with ads by default, but you can opt to pay $20 to remove them, either at the time of purchase or after you start using the device. The ads are limited to the lockscreen and are typically for other books or Kindle services. Kobo and Boox ereaders don’t come with ads. Which ereader has the longest battery life? Of the devices we tested, the Boox Go Color 7 has the largest listed battery capacity at 2,300mAh (Amazon doesn’t list the capacity of its Kindle devices). But thanks to the nature of E Ink screens and the relatively limited processing power required to display e-books, nearly all ereaders can go for weeks before they need a recharge. That means battery life probably isn’t as much of a deciding factor in buying an ereader as it would be with a tablet or smartphone. Which ereaders can read Kindle books? Amazon’s Kindle ereaders are the obvious answer, but other devices capable of running apps can also read titles from the Kindle store. For example, you can download the Kindle app on a Boox ereader through Google Play (the store comes standard in the Boox app menu). You can then sign into your Kindle account and access all the books in your library — the same way you’d read Kindle books on your phone or tablet. Can you buy Kindle books without a Kindle? Yes. You can buy Kindle books through the Kindle app or through Amazon’s website via a browser. You can read those titles on a Kindle or any device that can run the Kindle app, such as a smartphone, tablet or computer. Just be aware that Kindle titles can only be read through one of Amazon’s ereaders or the Kindle app. The company uses proprietary digital rights management on all ebooks it sells that can’t be read by other ereader apps like Kobo or Adobe ADE. What's the difference between Kindle and Kobo? Both Kindle and Kobo are brands of dedicated ereaders that support searching, buying, downloading and reading ebooks from their own stores. Both also support borrowing books from your local library via Overdrive and Libby. The difference is that Kindle is owned by Amazon and uses the Kindle store, whereas Kobo is owned by Rakuten and its books come from the Kobo store. Both stores come pre-loaded as a tab on their respective ereader and both carry most in-print books. Each store also carries their own exclusive ebooks as well, but Amazon’s library of Kindle-only books is much larger than Kobo’s. Amazon also offers Amazon Original stories to read on the Kindle, which are free short fiction and nonfiction reads that are free to Prime members. Which ereader is best for library books? Both Kobos and Kindles have simple systems for borrowing library books. Other ereaders, like Boox, let you borrow books after downloading the Libby App. Only Kobo ereaders let you search for and borrow books directly on the ereader, with a dedicated Overdrive tab. Kindles, on the other hand, utilize a convenient “read on Kindle” function from the Libby app or website. You can send a borrowed book to your Kindle just by signing into your account. Both methods are pretty easy, so which is the best for you probably depends on other factors than just the library-book feature. Recent updates August 2025: Included new frequently asked questions covering battery life, E Ink screens and ads on ereaders. Mentioned Amazon’s release of a cheaper Colorsoft Kindle. July 2025: Added our impressions of the new stylus-enabled Boox Go 7 series. Updated our Boox Palma recommendation to account for the upgrades to the Boox Palma 2. Included text formats to our specs and the battery life of the Kobo Clara Colour. March 2025: Added news about Bookshop.org getting into the ebook market. Updated information about price-setting by publishers. January 2025: Updated the \"Others we tested\" section to include impressions of the second Kindle Colorsoft review unit we received. August 2024: Replaced our Android tablet pick with the new Go Color 7 ereader from Boox. Updated book titles to current examples. Added an FAQ section to explain the difference between Kobo and Kindle ereaders and further detail library-book support on different models. November 2024: Following the release of Amazon's new Kindle ereaders, we tested and reviewed the Kindle Paperwhite Signature Edition, the base-model Kindle and Amazon's new color ereader, the Kindle Colorsoft. Accordingly, we updated our budget pick, added a premium pick and noted our experience with the Colorsoft. This article originally appeared on Engadget at https://www.engadget.com/mobile/tablets/best-ereader-130013808.html?src=rss",
          "feed_position": 31,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2023-03/c06d4490-cf16-11ed-96e7-3cba2c2f1226"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/why-it-leaders-should-pay-attention-to-canvas-imagination-era-strategy",
          "published_at": "Thu, 30 Oct 2025 03:00:00 GMT",
          "title": "Why IT leaders should pay attention to Canva’s ‘imagination era’ strategy",
          "standfirst": "The rise of AI marks a critical shift away from decades defined by information-chasing and a push for more and more compute power. Canva co-founder and CPO Cameron Adams refers to this dawning time as the “imagination era.” Meaning: Individuals and enterprises must be able to turn creativity into action with AI. Canva hopes to position itself at the center of this shift with a sweeping new suite of tools. The company’s new Creative Operating System (COS) integrates AI across every layer of content creation, creating a single, comprehensive creativity platform rather than a simple, template-based design tool.“We’re entering a new era where we need to rethink how we achieve our goals,” said Adams. “We’re enabling people’s imagination and giving them the tools they need to take action.”An &#x27;engine&#x27; for creativityAdams describes Canva’s platform as a three-layer stack: The top Visual Suite layer containing designs, images and other content; a collaborative Canva AI plane at center; and a foundational proprietary model holding it all up. At the heart of Canva’s strategy is its Creative Operating System (COS) underlying. This “engine,” as Adams describes it, integrates documents, websites, presentations, sheets, whiteboards, videos, social content, hundreds of millions of photos, illustrations, a rich sound library, and numerous templates, charts, and branded elements.The COS is getting a 2.0 upgrade, but the crucial advance is the “middle, crucial layer” that fully integrates AI and makes it accessible throughout various workflows, Adams explained. This gives creative and technical teams a single dashboard for generating, editing and launching all types of content.The underlying model is trained to understand the “complexity of design” so the platform can build out various elements — such as photos, videos, textures, or 3D graphics — in real time, matching branding style without the need for manual adjustments. It also supports live collaboration, meaning teams across departments can co-create. With a unified dashboard, a user working on a specific design, for instance, can create a new piece of content (say, a presentation) within the same workflow, without having to switch to another window or platform. Also, if they generate an image and aren’t pleased with it, they don’t have to go back and create from scratch; they can immediately begin editing, changing colors or tone. Another new capability in COS, “Ask Canva,” provides direct design advice. Users can tag @Canva to get copy suggestions and smart edits; or, they can highlight an image and direct the AI assistant to modify it or generate variants. “It’s a really unique interaction,” said Adams, noting that this AI design partner is always present. “It’s a real collaboration between people and AI, and we think it’s a revolutionary change.”Other new features include a 2.0 video editor and interactive form and email design with drag-and-drop tools. Further, Canva is now incorporated with Affinity, its unified app for pro designers incorporating vector, pixel and layer workflows, and Affinity is “free forever.” Automating intelligence, supporting marketingBranding is critical for enterprise; Canva has introduced new tools to help organizations consistently showcase theirs across platforms. The new Canva Grow engine integrates business objectives into the creative process so teams can workshop, create, distribute and refine ads and other materials. As Adams explained: “It automatically scans your website, figures out who your audience is, what assets you use to promote your products, the message it needs to send out, the formats you want to send it out in, makes a creative for you, and you can deploy it directly to the platform without having to leave Canva.”Marketing teams can now design and launch ads across platforms like Meta, track insights as they happen and refine future content based on performance metrics. “Your brand system is now available inside the AI you’re working with,” Adams noted. Success metrics and enterprise adoptionThe impact of Canva’s COS is reflected in notable user metrics: More than 250 million people use Canva every month, just over 29 million of which are paid subscribers. Adams reports that 41 billion designs have been created on Canva since launch, which equates to 1 billion each month. “If you break that down, it turns into the crazy number of 386 designs being created every single second,” said Adams. Whereas in the early days, it took roughly an hour for users to create a single design. Canva customers include Walmart, Disney, Virgin Voyages, Pinterest, FedEx, Expedia and eXp Realty. DocuSign, for one, reported that it unlocked more than 500 hours of team capacity and saved $300,000-plus in design hours by fully integrating Canva into its content creation. Disney, meanwhile, uses translation capabilities for its internationalization work, Adams said. Competitors in the design spaceCanva plays in an evolving landscape of professional design tools including Adobe Express and Figma; AI-powered challengers led by Microsoft Designer; and direct consumer alternatives like Visme and Piktochart.Adobe Express (starting at $9.99 a month for premium features) is known for its ease of use and integration with the broader Adobe Creative Cloud ecosystem. It features professional-grade templates and access to Adobe’s extensive stock library, and has incorporated Google&#x27;s Gemini 2.5 Flash image model and other gen AI features so that designers can create graphics via natural language prompts. Users with some design experience say they prefer its interface, controls and technical advantages over Canva (such as the ability to import high-fidelity PDFs). Figma (starting at $3 a month for professional plans) is touted for its real-time collaboration, advanced prototyping capabilities and deep integration with dev workflows; however, some say it has a steeper learning curve and higher-precision design tools, making it preferable for professional designers, developers and product teams working on more complex projects. Microsoft Designer (free version available; although a Microsoft 365 subscription starting at $9.99 a month unlocks additional features) benefits from its integration with Microsoft’s AI capabilities, Copilot layout and text generation and Dall-E powered image generation. The platform’s “Inspire Me” and “New Ideas” buttons provide design variations, and users can also import data from Excel, add 3D models from PowerPoint and access images from OneDrive. However, users report that its stock photos and template and image libraries are limited compared to Canva&#x27;s extensive collection, and its visuals can come across as outdated. Canva’s advantage seems to be in its extensive template library (more than 600,000 ready-to-use) and asset library (141 million-plus stock photos, videos, graphics, and audio elements).​ Its platform is also praised for its ease of use and interface friendly to non-designers, allowing them to begin quickly without training. Canva has also expanded into a variety of content types — documents, websites, presentations, whiteboards, videos, and more — making its platform a comprehensive visual suite than just a graphics tool. Canva has four pricing tiers: Canva Free for one user; Canva Pro for $120 a year for one person; Canva Teams for $100 a year for each team member; and the custom-priced Canva Enterprise. Key takeaways: Be open, embrace human-AI collaborationCanva’s COS is underpinned by Canva’s frontier model, an in-house, proprietary engine based on years of R&D and research partnerships, including the acquisition of visual AI company Leonardo. Adams notes that Canva works with top AI providers including OpenAI, Anthropic and Google. For technology teams, Canva’s approach offers important lessons, including a commitment to openness. “There are so many models floating around,” Adams noted; it’s important for enterprises to recognize when they should work with top models and when they should develop their own proprietary ones, he advised. For instance, OpenAI and Anthropic recently announced integrations with Canva as a visual layer because, as Adams explained, they realized they didn’t have the capability to create the same kinds of editable designs that Canva can. This creates a mutually-beneficial ecosystem. Ultimately, Adams noted: “We have this underlying philosophy that the future is people and technology working together. It&#x27;s not an either or. We want people to be at the center, to be the ones with the creative spark, and to use AI as a collaborator.”",
          "content": "The rise of AI marks a critical shift away from decades defined by information-chasing and a push for more and more compute power. Canva co-founder and CPO Cameron Adams refers to this dawning time as the “imagination era.” Meaning: Individuals and enterprises must be able to turn creativity into action with AI. Canva hopes to position itself at the center of this shift with a sweeping new suite of tools. The company’s new Creative Operating System (COS) integrates AI across every layer of content creation, creating a single, comprehensive creativity platform rather than a simple, template-based design tool.“We’re entering a new era where we need to rethink how we achieve our goals,” said Adams. “We’re enabling people’s imagination and giving them the tools they need to take action.”An &#x27;engine&#x27; for creativityAdams describes Canva’s platform as a three-layer stack: The top Visual Suite layer containing designs, images and other content; a collaborative Canva AI plane at center; and a foundational proprietary model holding it all up. At the heart of Canva’s strategy is its Creative Operating System (COS) underlying. This “engine,” as Adams describes it, integrates documents, websites, presentations, sheets, whiteboards, videos, social content, hundreds of millions of photos, illustrations, a rich sound library, and numerous templates, charts, and branded elements.The COS is getting a 2.0 upgrade, but the crucial advance is the “middle, crucial layer” that fully integrates AI and makes it accessible throughout various workflows, Adams explained. This gives creative and technical teams a single dashboard for generating, editing and launching all types of content.The underlying model is trained to understand the “complexity of design” so the platform can build out various elements — such as photos, videos, textures, or 3D graphics — in real time, matching branding style without the need for manual adjustments. It also supports live collaboration, meaning teams across departments can co-create. With a unified dashboard, a user working on a specific design, for instance, can create a new piece of content (say, a presentation) within the same workflow, without having to switch to another window or platform. Also, if they generate an image and aren’t pleased with it, they don’t have to go back and create from scratch; they can immediately begin editing, changing colors or tone. Another new capability in COS, “Ask Canva,” provides direct design advice. Users can tag @Canva to get copy suggestions and smart edits; or, they can highlight an image and direct the AI assistant to modify it or generate variants. “It’s a really unique interaction,” said Adams, noting that this AI design partner is always present. “It’s a real collaboration between people and AI, and we think it’s a revolutionary change.”Other new features include a 2.0 video editor and interactive form and email design with drag-and-drop tools. Further, Canva is now incorporated with Affinity, its unified app for pro designers incorporating vector, pixel and layer workflows, and Affinity is “free forever.” Automating intelligence, supporting marketingBranding is critical for enterprise; Canva has introduced new tools to help organizations consistently showcase theirs across platforms. The new Canva Grow engine integrates business objectives into the creative process so teams can workshop, create, distribute and refine ads and other materials. As Adams explained: “It automatically scans your website, figures out who your audience is, what assets you use to promote your products, the message it needs to send out, the formats you want to send it out in, makes a creative for you, and you can deploy it directly to the platform without having to leave Canva.”Marketing teams can now design and launch ads across platforms like Meta, track insights as they happen and refine future content based on performance metrics. “Your brand system is now available inside the AI you’re working with,” Adams noted. Success metrics and enterprise adoptionThe impact of Canva’s COS is reflected in notable user metrics: More than 250 million people use Canva every month, just over 29 million of which are paid subscribers. Adams reports that 41 billion designs have been created on Canva since launch, which equates to 1 billion each month. “If you break that down, it turns into the crazy number of 386 designs being created every single second,” said Adams. Whereas in the early days, it took roughly an hour for users to create a single design. Canva customers include Walmart, Disney, Virgin Voyages, Pinterest, FedEx, Expedia and eXp Realty. DocuSign, for one, reported that it unlocked more than 500 hours of team capacity and saved $300,000-plus in design hours by fully integrating Canva into its content creation. Disney, meanwhile, uses translation capabilities for its internationalization work, Adams said. Competitors in the design spaceCanva plays in an evolving landscape of professional design tools including Adobe Express and Figma; AI-powered challengers led by Microsoft Designer; and direct consumer alternatives like Visme and Piktochart.Adobe Express (starting at $9.99 a month for premium features) is known for its ease of use and integration with the broader Adobe Creative Cloud ecosystem. It features professional-grade templates and access to Adobe’s extensive stock library, and has incorporated Google&#x27;s Gemini 2.5 Flash image model and other gen AI features so that designers can create graphics via natural language prompts. Users with some design experience say they prefer its interface, controls and technical advantages over Canva (such as the ability to import high-fidelity PDFs). Figma (starting at $3 a month for professional plans) is touted for its real-time collaboration, advanced prototyping capabilities and deep integration with dev workflows; however, some say it has a steeper learning curve and higher-precision design tools, making it preferable for professional designers, developers and product teams working on more complex projects. Microsoft Designer (free version available; although a Microsoft 365 subscription starting at $9.99 a month unlocks additional features) benefits from its integration with Microsoft’s AI capabilities, Copilot layout and text generation and Dall-E powered image generation. The platform’s “Inspire Me” and “New Ideas” buttons provide design variations, and users can also import data from Excel, add 3D models from PowerPoint and access images from OneDrive. However, users report that its stock photos and template and image libraries are limited compared to Canva&#x27;s extensive collection, and its visuals can come across as outdated. Canva’s advantage seems to be in its extensive template library (more than 600,000 ready-to-use) and asset library (141 million-plus stock photos, videos, graphics, and audio elements).​ Its platform is also praised for its ease of use and interface friendly to non-designers, allowing them to begin quickly without training. Canva has also expanded into a variety of content types — documents, websites, presentations, whiteboards, videos, and more — making its platform a comprehensive visual suite than just a graphics tool. Canva has four pricing tiers: Canva Free for one user; Canva Pro for $120 a year for one person; Canva Teams for $100 a year for each team member; and the custom-priced Canva Enterprise. Key takeaways: Be open, embrace human-AI collaborationCanva’s COS is underpinned by Canva’s frontier model, an in-house, proprietary engine based on years of R&D and research partnerships, including the acquisition of visual AI company Leonardo. Adams notes that Canva works with top AI providers including OpenAI, Anthropic and Google. For technology teams, Canva’s approach offers important lessons, including a commitment to openness. “There are so many models floating around,” Adams noted; it’s important for enterprises to recognize when they should work with top models and when they should develop their own proprietary ones, he advised. For instance, OpenAI and Anthropic recently announced integrations with Canva as a visual layer because, as Adams explained, they realized they didn’t have the capability to create the same kinds of editable designs that Canva can. This creates a mutually-beneficial ecosystem. Ultimately, Adams noted: “We have this underlying philosophy that the future is people and technology working together. It&#x27;s not an either or. We want people to be at the center, to be the ones with the creative spark, and to use AI as a collaborator.”",
          "feed_position": 0,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/5RueUd7BFZZvUTu1HpMex9/651bfe1010eda474b8f737d42e7354ca/Canva.png?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/the-best-vpn-deals-88-percent-discounts-on-protonvpn-expressvpn-surfshark-and-more-120056432.html",
          "published_at": "Wed, 29 Oct 2025 21:54:57 +0000",
          "title": "The best VPN deals: 88 percent discounts on ProtonVPN, ExpressVPN, Surfshark and more",
          "standfirst": "A virtual private network (VPN) is useful in several ways — a good one can stream foreign TV shows and events, save you from giving up information to hackers and keep you anonymous to protect against online tracking. Although we strongly recommend using a VPN, a bit of comparison shopping goes a long way in this market. VPN pricing can be opaque, and providers don't always portray their best deals accurately. Even so, there are genuinely great bargains on the table. VPN providers give out deep discounts to customers who sign up for a year or more at a time. This lets them boost their subscriber numbers, but it's a win for you as well — while you pay out more upfront, if you divide the cost by the months of service, it's significantly cheaper over time. Most of the deals we highlight below follow that pattern, so make sure you're comfortable with a longer commitment before you take the plunge. If you've been thinking about subscribing to a VPN service, read on for the best VPN deals we could find right now. Best VPN deals NordVPN Basic — $80.73 for a two-year subscription with three months free (74 percent off): NordVPN gets the most important parts of a VPN right. It's fast, it doesn't leak any of your data and it's great at changing your virtual location. I noted in my NordVPN review that it always connects quickly and includes a support page that makes it easy to get live help. Although I'm sad to see it shutting down Meshnet, NordVPN still includes a lot of cool features, like servers that instantly connect you to Tor. This early Black Friday deal gives you 74 percent off the two-year plan, which also comes with three extra months. NordVPN Plus — $105.03 for a two-year subscription with three months free (74 percent off): In another early Black Friday discount, NordVPN has also taken 74 percent off its Plus subscription. For only a little more, you get a powerful ad and tracker blocker that can also catch malware downloads, plus access to the NordPass password manager. A Plus plan also adds a data breach scanner that checks the dark web for your sensitive information. ExpressVPN Basic — $97.72 for a two-year subscription with four months free (73 percent off): This is one of the best VPNs, especially for new users, who will find its apps and website headache-free on all platforms. In tests for my ExpressVPN review, it dropped my download speeds by less than 7 percent and successfully changed my virtual location 14 out of 15 times. In short, it's an all-around excellent service that only suffers from being a little overpriced — which is why I'm so excited whenever I find it offering a decent deal. This deal, which gets you 28 months of ExpressVPN service, represents a 73 percent savings. ExpressVPN Advanced — $125.72 for a two-year subscription with four months free (67 percent off): ExpressVPN recently split its pricing into multiple tiers, but they all still come with similar discounts for going long. In addition to top-tier VPN service, advanced users get two additional simultaneous connections (for a total of 12), the ExpressVPN Keys password manager, advanced ad and tracker blocking, ID protection features and a 50 percent discount on an AirCove router. Surfshark Starter — $53.73 for a two-year subscription with three months free (87 percent off): This is the \"basic\" level of Surfshark, but it includes the entire VPN; everything on Surfshark One is an extra perk. With this subscription, you'll get some of the most envelope-pushing features in the VPN world right now. Surfshark has a more closely connected server network than most VPNs, so it can rotate your IP constantly to help you evade detection — it even lets you choose your own entry and exit nodes for a double-hop connection. That all comes with a near-invisible impact on download speeds. With this year-round deal, you can save 87 percent on 27 months of Surfshark. Surfshark One — $59.13 for a two-year subscription with three months free (88 percent off): A VPN is great, but it's not enough to protect your data all on its own. Surfshark One adds several apps that boost your security beyond just VPN service, including Surfshark Antivirus (scans devices and downloads for malware) and Surfshark Alert (alerts you whenever your sensitive information shows up in a data breach), plus Surfshark Search and Alternative ID from the previous tier. This extra-low deal gives you 88 percent off all those features. If you bump up to Surfshark One+, you'll also get data removal through Incogni, but the price jumps enough that it's not quite worthwhile in my eyes. CyberGhost — $56.94 for a two-year subscription with two months free (83 percent off): CyberGhost has some of the best automation you'll see on any VPN. With its Smart Rules system, you can determine how its apps respond to different types of Wi-Fi networks, with exceptions for specific networks you know by name. Typically, you can set it to auto-connect, disconnect or send you a message asking what to do. CyberGhost's other best feature is its streaming servers — while it's not totally clear what it does to optimize them, I've found both better video quality and more consistent unblocking when I use them on streaming sites. Currently, you can get 26 months of CyberGhost for 83 percent off the usual price. Private Internet Access — $79 for a three-year subscription with three months free (83 percent off): It's a bit hard to find (the link at the start of this paragraph includes the coupon), but Private Internet Access (PIA) is giving out the best available price right now on a VPN I'd recommend using. With this deal, you can get 39 months of PIA for a little bit over $2 per month — an 83 percent discount on its monthly price. Despite being so cheap, PIA has plenty of features, coming with its own DNS servers, a built-in ad blocker and automation powers to rival CyberGhost. However, internet speeds can fluctuate while you're connected. hide.me — $69.95 for a two-year subscription with two months free (73 percent off): Hide.me is an excellent free VPN — in fact, it's my favorite on the market, even with EventVPN and the free version of Proton VPN as competition. However, if you do want to upgrade to its paid plan, the two-year subscription offers great savings. Hide.me works well as a no-frills beginner VPN, with apps and a server network it should frankly be charging more for. What makes a good VPN deal Like I said in the intro, practically every VPN heavily discounts its long-term subscriptions the whole year round. The only noteworthy exception is Mullvad, the Costco hot dog of VPNs (that's a compliment, to be clear). When there's constantly a huge discount going on, it can be hard to tell when you're actually getting a good deal. The best way to squeeze out more savings is to look for seasonal deals, student discounts or exclusive sales like Proton VPN's coupon for Engadget readers. One trick VPNs often use is to add extra months onto an introductory deal, pushing the average monthly price even lower. When it comes time to renew, you usually can't get these extra months again. You often can't even renew for the same basic period of time — for example, you may only be able to renew a two-year subscription for one year. If you're planning to hold onto a VPN indefinitely, check the fine print to see how much it will cost per month after the first renewal, and ensure that fits into your budget. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/the-best-vpn-deals-88-percent-discounts-on-protonvpn-expressvpn-surfshark-and-more-120056432.html?src=rss",
          "content": "A virtual private network (VPN) is useful in several ways — a good one can stream foreign TV shows and events, save you from giving up information to hackers and keep you anonymous to protect against online tracking. Although we strongly recommend using a VPN, a bit of comparison shopping goes a long way in this market. VPN pricing can be opaque, and providers don't always portray their best deals accurately. Even so, there are genuinely great bargains on the table. VPN providers give out deep discounts to customers who sign up for a year or more at a time. This lets them boost their subscriber numbers, but it's a win for you as well — while you pay out more upfront, if you divide the cost by the months of service, it's significantly cheaper over time. Most of the deals we highlight below follow that pattern, so make sure you're comfortable with a longer commitment before you take the plunge. If you've been thinking about subscribing to a VPN service, read on for the best VPN deals we could find right now. Best VPN deals NordVPN Basic — $80.73 for a two-year subscription with three months free (74 percent off): NordVPN gets the most important parts of a VPN right. It's fast, it doesn't leak any of your data and it's great at changing your virtual location. I noted in my NordVPN review that it always connects quickly and includes a support page that makes it easy to get live help. Although I'm sad to see it shutting down Meshnet, NordVPN still includes a lot of cool features, like servers that instantly connect you to Tor. This early Black Friday deal gives you 74 percent off the two-year plan, which also comes with three extra months. NordVPN Plus — $105.03 for a two-year subscription with three months free (74 percent off): In another early Black Friday discount, NordVPN has also taken 74 percent off its Plus subscription. For only a little more, you get a powerful ad and tracker blocker that can also catch malware downloads, plus access to the NordPass password manager. A Plus plan also adds a data breach scanner that checks the dark web for your sensitive information. ExpressVPN Basic — $97.72 for a two-year subscription with four months free (73 percent off): This is one of the best VPNs, especially for new users, who will find its apps and website headache-free on all platforms. In tests for my ExpressVPN review, it dropped my download speeds by less than 7 percent and successfully changed my virtual location 14 out of 15 times. In short, it's an all-around excellent service that only suffers from being a little overpriced — which is why I'm so excited whenever I find it offering a decent deal. This deal, which gets you 28 months of ExpressVPN service, represents a 73 percent savings. ExpressVPN Advanced — $125.72 for a two-year subscription with four months free (67 percent off): ExpressVPN recently split its pricing into multiple tiers, but they all still come with similar discounts for going long. In addition to top-tier VPN service, advanced users get two additional simultaneous connections (for a total of 12), the ExpressVPN Keys password manager, advanced ad and tracker blocking, ID protection features and a 50 percent discount on an AirCove router. Surfshark Starter — $53.73 for a two-year subscription with three months free (87 percent off): This is the \"basic\" level of Surfshark, but it includes the entire VPN; everything on Surfshark One is an extra perk. With this subscription, you'll get some of the most envelope-pushing features in the VPN world right now. Surfshark has a more closely connected server network than most VPNs, so it can rotate your IP constantly to help you evade detection — it even lets you choose your own entry and exit nodes for a double-hop connection. That all comes with a near-invisible impact on download speeds. With this year-round deal, you can save 87 percent on 27 months of Surfshark. Surfshark One — $59.13 for a two-year subscription with three months free (88 percent off): A VPN is great, but it's not enough to protect your data all on its own. Surfshark One adds several apps that boost your security beyond just VPN service, including Surfshark Antivirus (scans devices and downloads for malware) and Surfshark Alert (alerts you whenever your sensitive information shows up in a data breach), plus Surfshark Search and Alternative ID from the previous tier. This extra-low deal gives you 88 percent off all those features. If you bump up to Surfshark One+, you'll also get data removal through Incogni, but the price jumps enough that it's not quite worthwhile in my eyes. CyberGhost — $56.94 for a two-year subscription with two months free (83 percent off): CyberGhost has some of the best automation you'll see on any VPN. With its Smart Rules system, you can determine how its apps respond to different types of Wi-Fi networks, with exceptions for specific networks you know by name. Typically, you can set it to auto-connect, disconnect or send you a message asking what to do. CyberGhost's other best feature is its streaming servers — while it's not totally clear what it does to optimize them, I've found both better video quality and more consistent unblocking when I use them on streaming sites. Currently, you can get 26 months of CyberGhost for 83 percent off the usual price. Private Internet Access — $79 for a three-year subscription with three months free (83 percent off): It's a bit hard to find (the link at the start of this paragraph includes the coupon), but Private Internet Access (PIA) is giving out the best available price right now on a VPN I'd recommend using. With this deal, you can get 39 months of PIA for a little bit over $2 per month — an 83 percent discount on its monthly price. Despite being so cheap, PIA has plenty of features, coming with its own DNS servers, a built-in ad blocker and automation powers to rival CyberGhost. However, internet speeds can fluctuate while you're connected. hide.me — $69.95 for a two-year subscription with two months free (73 percent off): Hide.me is an excellent free VPN — in fact, it's my favorite on the market, even with EventVPN and the free version of Proton VPN as competition. However, if you do want to upgrade to its paid plan, the two-year subscription offers great savings. Hide.me works well as a no-frills beginner VPN, with apps and a server network it should frankly be charging more for. What makes a good VPN deal Like I said in the intro, practically every VPN heavily discounts its long-term subscriptions the whole year round. The only noteworthy exception is Mullvad, the Costco hot dog of VPNs (that's a compliment, to be clear). When there's constantly a huge discount going on, it can be hard to tell when you're actually getting a good deal. The best way to squeeze out more savings is to look for seasonal deals, student discounts or exclusive sales like Proton VPN's coupon for Engadget readers. One trick VPNs often use is to add extra months onto an introductory deal, pushing the average monthly price even lower. When it comes time to renew, you usually can't get these extra months again. You often can't even renew for the same basic period of time — for example, you may only be able to renew a two-year subscription for one year. If you're planning to hold onto a VPN indefinitely, check the fine print to see how much it will cost per month after the first renewal, and ensure that fits into your budget. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/the-best-vpn-deals-88-percent-discounts-on-protonvpn-expressvpn-surfshark-and-more-120056432.html?src=rss",
          "feed_position": 36
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/vibe-coding-platform-cursor-releases-first-in-house-llm-composer-promising",
          "published_at": "Wed, 29 Oct 2025 19:28:00 GMT",
          "title": "Vibe coding platform Cursor releases first in-house LLM, Composer, promising 4X speed boost",
          "standfirst": "The vibe coding tool Cursor, from startup Anysphere, has introduced Composer, its first in-house, proprietary coding large language model (LLM) as part of its Cursor 2.0 platform update. Composer is designed to execute coding tasks quickly and accurately in production-scale environments, representing a new step in AI-assisted programming. It&#x27;s already being used by Cursor’s own engineering staff in day-to-day development — indicating maturity and stability.According to Cursor, Composer completes most interactions in less than 30 seconds while maintaining a high level of reasoning ability across large and complex codebases. The model is described as four times faster than similarly intelligent systems and is trained for “agentic” workflows—where autonomous coding agents plan, write, test, and review code collaboratively.Previously, Cursor supported \"vibe coding\" — using AI to write or complete code based on natural language instructions from a user, even someone untrained in development — atop other leading proprietary LLMs from the likes of OpenAI, Anthropic, Google, and xAI. These options are still available to users.Benchmark ResultsComposer’s capabilities are benchmarked using \"Cursor Bench,\" an internal evaluation suite derived from real developer agent requests. The benchmark measures not just correctness, but also the model’s adherence to existing abstractions, style conventions, and engineering practices.On this benchmark, Composer achieves frontier-level coding intelligence while generating at 250 tokens per second — about twice as fast as leading fast-inference models and four times faster than comparable frontier systems.Cursor’s published comparison groups models into several categories: “Best Open” (e.g., Qwen Coder, GLM 4.6), “Fast Frontier” (Haiku 4.5, Gemini Flash 2.5), “Frontier 7/2025” (the strongest model available midyear), and “Best Frontier” (including GPT-5 and Claude Sonnet 4.5). Composer matches the intelligence of mid-frontier systems while delivering the highest recorded generation speed among all tested classes.A Model Built with Reinforcement Learning and Mixture-of-Experts ArchitectureResearch scientist Sasha Rush of Cursor provided insight into the model’s development in posts on the social network X, describing Composer as a reinforcement-learned (RL) mixture-of-experts (MoE) model:“We used RL to train a big MoE model to be really good at real-world coding, and also very fast.”Rush explained that the team co-designed both Composer and the Cursor environment to allow the model to operate efficiently at production scale:“Unlike other ML systems, you can’t abstract much from the full-scale system. We co-designed this project and Cursor together in order to allow running the agent at the necessary scale.”Composer was trained on real software engineering tasks rather than static datasets. During training, the model operated inside full codebases using a suite of production tools—including file editing, semantic search, and terminal commands—to solve complex engineering problems. Each training iteration involved solving a concrete challenge, such as producing a code edit, drafting a plan, or generating a targeted explanation.The reinforcement loop optimized both correctness and efficiency. Composer learned to make effective tool choices, use parallelism, and avoid unnecessary or speculative responses. Over time, the model developed emergent behaviors such as running unit tests, fixing linter errors, and performing multi-step code searches autonomously.This design enables Composer to work within the same runtime context as the end-user, making it more aligned with real-world coding conditions—handling version control, dependency management, and iterative testing.From Prototype to ProductionComposer’s development followed an earlier internal prototype known as Cheetah, which Cursor used to explore low-latency inference for coding tasks.“Cheetah was the v0 of this model primarily to test speed,” Rush said on X. “Our metrics say it [Composer] is the same speed, but much, much smarter.”Cheetah’s success at reducing latency helped Cursor identify speed as a key factor in developer trust and usability. Composer maintains that responsiveness while significantly improving reasoning and task generalization.Developers who used Cheetah during early testing noted that its speed changed how they worked. One user commented that it was “so fast that I can stay in the loop when working with it.” Composer retains that speed but extends capability to multi-step coding, refactoring, and testing tasks.Integration with Cursor 2.0Composer is fully integrated into Cursor 2.0, a major update to the company’s agentic development environment. The platform introduces a multi-agent interface, allowing up to eight agents to run in parallel, each in an isolated workspace using git worktrees or remote machines.Within this system, Composer can serve as one or more of those agents, performing tasks independently or collaboratively. Developers can compare multiple results from concurrent agent runs and select the best output.Cursor 2.0 also includes supporting features that enhance Composer’s effectiveness:In-Editor Browser (GA) – enables agents to run and test their code directly inside the IDE, forwarding DOM information to the model.Improved Code Review – aggregates diffs across multiple files for faster inspection of model-generated changes.Sandboxed Terminals (GA) – isolate agent-run shell commands for secure local execution.Voice Mode – adds speech-to-text controls for initiating or managing agent sessions.While these platform updates expand the overall Cursor experience, Composer is positioned as the technical core enabling fast, reliable agentic coding.Infrastructure and Training SystemsTo train Composer at scale, Cursor built a custom reinforcement learning infrastructure combining PyTorch and Ray for asynchronous training across thousands of NVIDIA GPUs. The team developed specialized MXFP8 MoE kernels and hybrid sharded data parallelism, enabling large-scale model updates with minimal communication overhead.This configuration allows Cursor to train models natively at low precision without requiring post-training quantization, improving both inference speed and efficiency. Composer’s training relied on hundreds of thousands of concurrent sandboxed environments—each a self-contained coding workspace—running in the cloud. The company adapted its Background Agents infrastructure to schedule these virtual machines dynamically, supporting the bursty nature of large RL runs.Enterprise UseComposer’s performance improvements are supported by infrastructure-level changes across Cursor’s code intelligence stack. The company has optimized its Language Server Protocols (LSPs) for faster diagnostics and navigation, especially in Python and TypeScript projects. These changes reduce latency when Composer interacts with large repositories or generates multi-file updates.Enterprise users gain administrative control over Composer and other agents through team rules, audit logs, and sandbox enforcement. Cursor’s Teams and Enterprise tiers also support pooled model usage, SAML/OIDC authentication, and analytics for monitoring agent performance across organizations.Pricing for individual users ranges from Free (Hobby) to Ultra ($200/month) tiers, with expanded usage limits for Pro+ and Ultra subscribers. Business pricing starts at $40 per user per month for Teams, with enterprise contracts offering custom usage and compliance options.Composer’s Role in the Evolving AI Coding LandscapeComposer’s focus on speed, reinforcement learning, and integration with live coding workflows differentiates it from other AI development assistants such as GitHub Copilot or Replit’s Agent. Rather than serving as a passive suggestion engine, Composer is designed for continuous, agent-driven collaboration, where multiple autonomous systems interact directly with a project’s codebase.This model-level specialization—training AI to function within the real environment it will operate in—represents a significant step toward practical, autonomous software development. Composer is not trained only on text data or static code, but within a dynamic IDE that mirrors production conditions.Rush described this approach as essential to achieving real-world reliability: the model learns not just how to generate code, but how to integrate, test, and improve it in context.What It Means for Enterprise Devs and Vibe CodingWith Composer, Cursor is introducing more than a fast model—it’s deploying an AI system optimized for real-world use, built to operate inside the same tools developers already rely on. The combination of reinforcement learning, mixture-of-experts design, and tight product integration gives Composer a practical edge in speed and responsiveness that sets it apart from general-purpose language models.While Cursor 2.0 provides the infrastructure for multi-agent collaboration, Composer is the core innovation that makes those workflows viable. It’s the first coding model built specifically for agentic, production-level coding—and an early glimpse of what everyday programming could look like when human developers and autonomous models share the same workspace.",
          "content": "The vibe coding tool Cursor, from startup Anysphere, has introduced Composer, its first in-house, proprietary coding large language model (LLM) as part of its Cursor 2.0 platform update. Composer is designed to execute coding tasks quickly and accurately in production-scale environments, representing a new step in AI-assisted programming. It&#x27;s already being used by Cursor’s own engineering staff in day-to-day development — indicating maturity and stability.According to Cursor, Composer completes most interactions in less than 30 seconds while maintaining a high level of reasoning ability across large and complex codebases. The model is described as four times faster than similarly intelligent systems and is trained for “agentic” workflows—where autonomous coding agents plan, write, test, and review code collaboratively.Previously, Cursor supported \"vibe coding\" — using AI to write or complete code based on natural language instructions from a user, even someone untrained in development — atop other leading proprietary LLMs from the likes of OpenAI, Anthropic, Google, and xAI. These options are still available to users.Benchmark ResultsComposer’s capabilities are benchmarked using \"Cursor Bench,\" an internal evaluation suite derived from real developer agent requests. The benchmark measures not just correctness, but also the model’s adherence to existing abstractions, style conventions, and engineering practices.On this benchmark, Composer achieves frontier-level coding intelligence while generating at 250 tokens per second — about twice as fast as leading fast-inference models and four times faster than comparable frontier systems.Cursor’s published comparison groups models into several categories: “Best Open” (e.g., Qwen Coder, GLM 4.6), “Fast Frontier” (Haiku 4.5, Gemini Flash 2.5), “Frontier 7/2025” (the strongest model available midyear), and “Best Frontier” (including GPT-5 and Claude Sonnet 4.5). Composer matches the intelligence of mid-frontier systems while delivering the highest recorded generation speed among all tested classes.A Model Built with Reinforcement Learning and Mixture-of-Experts ArchitectureResearch scientist Sasha Rush of Cursor provided insight into the model’s development in posts on the social network X, describing Composer as a reinforcement-learned (RL) mixture-of-experts (MoE) model:“We used RL to train a big MoE model to be really good at real-world coding, and also very fast.”Rush explained that the team co-designed both Composer and the Cursor environment to allow the model to operate efficiently at production scale:“Unlike other ML systems, you can’t abstract much from the full-scale system. We co-designed this project and Cursor together in order to allow running the agent at the necessary scale.”Composer was trained on real software engineering tasks rather than static datasets. During training, the model operated inside full codebases using a suite of production tools—including file editing, semantic search, and terminal commands—to solve complex engineering problems. Each training iteration involved solving a concrete challenge, such as producing a code edit, drafting a plan, or generating a targeted explanation.The reinforcement loop optimized both correctness and efficiency. Composer learned to make effective tool choices, use parallelism, and avoid unnecessary or speculative responses. Over time, the model developed emergent behaviors such as running unit tests, fixing linter errors, and performing multi-step code searches autonomously.This design enables Composer to work within the same runtime context as the end-user, making it more aligned with real-world coding conditions—handling version control, dependency management, and iterative testing.From Prototype to ProductionComposer’s development followed an earlier internal prototype known as Cheetah, which Cursor used to explore low-latency inference for coding tasks.“Cheetah was the v0 of this model primarily to test speed,” Rush said on X. “Our metrics say it [Composer] is the same speed, but much, much smarter.”Cheetah’s success at reducing latency helped Cursor identify speed as a key factor in developer trust and usability. Composer maintains that responsiveness while significantly improving reasoning and task generalization.Developers who used Cheetah during early testing noted that its speed changed how they worked. One user commented that it was “so fast that I can stay in the loop when working with it.” Composer retains that speed but extends capability to multi-step coding, refactoring, and testing tasks.Integration with Cursor 2.0Composer is fully integrated into Cursor 2.0, a major update to the company’s agentic development environment. The platform introduces a multi-agent interface, allowing up to eight agents to run in parallel, each in an isolated workspace using git worktrees or remote machines.Within this system, Composer can serve as one or more of those agents, performing tasks independently or collaboratively. Developers can compare multiple results from concurrent agent runs and select the best output.Cursor 2.0 also includes supporting features that enhance Composer’s effectiveness:In-Editor Browser (GA) – enables agents to run and test their code directly inside the IDE, forwarding DOM information to the model.Improved Code Review – aggregates diffs across multiple files for faster inspection of model-generated changes.Sandboxed Terminals (GA) – isolate agent-run shell commands for secure local execution.Voice Mode – adds speech-to-text controls for initiating or managing agent sessions.While these platform updates expand the overall Cursor experience, Composer is positioned as the technical core enabling fast, reliable agentic coding.Infrastructure and Training SystemsTo train Composer at scale, Cursor built a custom reinforcement learning infrastructure combining PyTorch and Ray for asynchronous training across thousands of NVIDIA GPUs. The team developed specialized MXFP8 MoE kernels and hybrid sharded data parallelism, enabling large-scale model updates with minimal communication overhead.This configuration allows Cursor to train models natively at low precision without requiring post-training quantization, improving both inference speed and efficiency. Composer’s training relied on hundreds of thousands of concurrent sandboxed environments—each a self-contained coding workspace—running in the cloud. The company adapted its Background Agents infrastructure to schedule these virtual machines dynamically, supporting the bursty nature of large RL runs.Enterprise UseComposer’s performance improvements are supported by infrastructure-level changes across Cursor’s code intelligence stack. The company has optimized its Language Server Protocols (LSPs) for faster diagnostics and navigation, especially in Python and TypeScript projects. These changes reduce latency when Composer interacts with large repositories or generates multi-file updates.Enterprise users gain administrative control over Composer and other agents through team rules, audit logs, and sandbox enforcement. Cursor’s Teams and Enterprise tiers also support pooled model usage, SAML/OIDC authentication, and analytics for monitoring agent performance across organizations.Pricing for individual users ranges from Free (Hobby) to Ultra ($200/month) tiers, with expanded usage limits for Pro+ and Ultra subscribers. Business pricing starts at $40 per user per month for Teams, with enterprise contracts offering custom usage and compliance options.Composer’s Role in the Evolving AI Coding LandscapeComposer’s focus on speed, reinforcement learning, and integration with live coding workflows differentiates it from other AI development assistants such as GitHub Copilot or Replit’s Agent. Rather than serving as a passive suggestion engine, Composer is designed for continuous, agent-driven collaboration, where multiple autonomous systems interact directly with a project’s codebase.This model-level specialization—training AI to function within the real environment it will operate in—represents a significant step toward practical, autonomous software development. Composer is not trained only on text data or static code, but within a dynamic IDE that mirrors production conditions.Rush described this approach as essential to achieving real-world reliability: the model learns not just how to generate code, but how to integrate, test, and improve it in context.What It Means for Enterprise Devs and Vibe CodingWith Composer, Cursor is introducing more than a fast model—it’s deploying an AI system optimized for real-world use, built to operate inside the same tools developers already rely on. The combination of reinforcement learning, mixture-of-experts design, and tight product integration gives Composer a practical edge in speed and responsiveness that sets it apart from general-purpose language models.While Cursor 2.0 provides the infrastructure for multi-agent collaboration, Composer is the core innovation that makes those workflows viable. It’s the first coding model built specifically for agentic, production-level coding—and an early glimpse of what everyday programming could look like when human developers and autonomous models share the same workspace.",
          "feed_position": 1,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/3XICNbOGJJDY7SoZx0m1SV/4b4ec66e5aa6a03194e432369e6e6ac8/cfr0z3n_flat_illustration_elegant_constructivist_1920s_art_deco_6818187a-93ac-437e-af85-43b96b2507a5.png?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/ai/early-access-for-gemini-home-voice-assistant-is-now-available-heres-how-to-get-it-191250927.html",
          "published_at": "Wed, 29 Oct 2025 19:12:50 +0000",
          "title": "Early access for Gemini Home voice assistant is now available. Here's how to get it",
          "standfirst": "A decade ago, when smart speakers with built-in voice assistants were the hot new thing, many imagined they'd quickly evolve into highly intelligent companions. (Think C-3PO or Star Trek's Data living inside a speaker.) That road has been much longer than expected, as virtual helpers like Alexa, Siri and Google Assistant seemed to sit in neutral for years. But now that generative AI is here (for better or worse), smart speakers are finally scratching the surface of those expectations. Google's new version, Gemini for Home, is now available to try. Here's how. First, keep in mind that the Gemini for Home voice assistant is in early access. This means Google is gathering feedback about its features, and — as with all generative AI — it's wise to assume it will make mistakes. If or when it does, you can send feedback to Google in the Google Home app or by saying, \"Hey Google, send feedback.\" Devices compatible with Gemini for Home voice assistant You'll also want to check your speaker model before diving in. The full Gemini for Home experience is available to try on the Google Nest Hub (2nd gen), Google Nest Audio, Google Nest Mini (2nd gen) and Google Nest Hub Max. Those models all support Gemini Live, which enables conversational back-and-forth chat with natural follow-up questions. Other models support everything but Gemini Live. That list includes the Google Nest Wifi point, Google Nest Hub (1st gen), Google Home Max, Google Home Mini (1st gen) and Google Home. Another point is that once you dive in, your Google Assistant days will be over (on your speakers, anyway). That's because Google says that, once you upgrade to Gemini for Home, your compatible devices can't downgrade to Assistant. That shouldn't be a problem, but it's worth keeping in mind before you take the plunge. How to sign up for early Gemini access Once you've confirmed that your speaker(s) are at least partially compatible, head to the Google Home app on a mobile device. There, tap your profile picture (or initials) on the top right. Then tap Home Settings > Early Access. Congratulations: You've put in your request. The bad news is you may have to wait a bit to confirm your entry into the beta program. Once you're in, you'll see a notification from the Google Home app that reads, \"Introducing Gemini for Home.\" Select that, and follow the prompts. (If you accidentally dismiss the notification, you'll see the setup banner under Home settings in the Google Home app.) Cherlynn Low for Engadget At that point, all compatible speakers in your home will be upgraded to Google's more intelligent AI assistant. You can now throw more advanced questions at it, similar to what you'd ask text-based chatbots like ChatGPT. Except this one sits on a shelf, ready to field your verbal requests at any moment. Keep in mind that Gemini Live requires a Google Home Premium subscription. The standard version costs $10 per month or $100 per year. Meanwhile, the advanced tier doubles that: $20 per month or $200 per year. At least for now, the only difference between the two (for these purposes) is that the pricier plan supports a camera history search feature. Both premium tiers unlock access to Gemini Live. So, if that's all you need, you can save money and get standard. Google offers plenty of examples to get started. You can get quick answers to facts, like \"Hey Google, who are the top five scoring players in basketball history?\" (FYI: James, Abdul-Jabbar, Malone, Bryant and Jordan.) You can also ask Gemini Live to have a chat about ingredients for people with dietary needs. Or, ask it to explain complex topics (like how Wi-Fi works) in simple terms. It may not be at C-3PO level yet, but it's certainly moving in that direction.This article originally appeared on Engadget at https://www.engadget.com/ai/early-access-for-gemini-home-voice-assistant-is-now-available-heres-how-to-get-it-191250927.html?src=rss",
          "content": "A decade ago, when smart speakers with built-in voice assistants were the hot new thing, many imagined they'd quickly evolve into highly intelligent companions. (Think C-3PO or Star Trek's Data living inside a speaker.) That road has been much longer than expected, as virtual helpers like Alexa, Siri and Google Assistant seemed to sit in neutral for years. But now that generative AI is here (for better or worse), smart speakers are finally scratching the surface of those expectations. Google's new version, Gemini for Home, is now available to try. Here's how. First, keep in mind that the Gemini for Home voice assistant is in early access. This means Google is gathering feedback about its features, and — as with all generative AI — it's wise to assume it will make mistakes. If or when it does, you can send feedback to Google in the Google Home app or by saying, \"Hey Google, send feedback.\" Devices compatible with Gemini for Home voice assistant You'll also want to check your speaker model before diving in. The full Gemini for Home experience is available to try on the Google Nest Hub (2nd gen), Google Nest Audio, Google Nest Mini (2nd gen) and Google Nest Hub Max. Those models all support Gemini Live, which enables conversational back-and-forth chat with natural follow-up questions. Other models support everything but Gemini Live. That list includes the Google Nest Wifi point, Google Nest Hub (1st gen), Google Home Max, Google Home Mini (1st gen) and Google Home. Another point is that once you dive in, your Google Assistant days will be over (on your speakers, anyway). That's because Google says that, once you upgrade to Gemini for Home, your compatible devices can't downgrade to Assistant. That shouldn't be a problem, but it's worth keeping in mind before you take the plunge. How to sign up for early Gemini access Once you've confirmed that your speaker(s) are at least partially compatible, head to the Google Home app on a mobile device. There, tap your profile picture (or initials) on the top right. Then tap Home Settings > Early Access. Congratulations: You've put in your request. The bad news is you may have to wait a bit to confirm your entry into the beta program. Once you're in, you'll see a notification from the Google Home app that reads, \"Introducing Gemini for Home.\" Select that, and follow the prompts. (If you accidentally dismiss the notification, you'll see the setup banner under Home settings in the Google Home app.) Cherlynn Low for Engadget At that point, all compatible speakers in your home will be upgraded to Google's more intelligent AI assistant. You can now throw more advanced questions at it, similar to what you'd ask text-based chatbots like ChatGPT. Except this one sits on a shelf, ready to field your verbal requests at any moment. Keep in mind that Gemini Live requires a Google Home Premium subscription. The standard version costs $10 per month or $100 per year. Meanwhile, the advanced tier doubles that: $20 per month or $200 per year. At least for now, the only difference between the two (for these purposes) is that the pricier plan supports a camera history search feature. Both premium tiers unlock access to Gemini Live. So, if that's all you need, you can save money and get standard. Google offers plenty of examples to get started. You can get quick answers to facts, like \"Hey Google, who are the top five scoring players in basketball history?\" (FYI: James, Abdul-Jabbar, Malone, Bryant and Jordan.) You can also ask Gemini Live to have a chat about ingredients for people with dietary needs. Or, ask it to explain complex topics (like how Wi-Fi works) in simple terms. It may not be at C-3PO level yet, but it's certainly moving in that direction.This article originally appeared on Engadget at https://www.engadget.com/ai/early-access-for-gemini-home-voice-assistant-is-now-available-heres-how-to-get-it-191250927.html?src=rss",
          "feed_position": 41,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2025-10/c92cc950-b4f8-11f0-b47e-004467dd6c05"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/ai/characterai-to-ban-teens-from-talking-to-its-chatbots-180027641.html",
          "published_at": "Wed, 29 Oct 2025 18:00:28 +0000",
          "title": "Character.AI to ban teens from talking to its chatbots",
          "standfirst": "Character.AI will no longer permit teenagers to interact with its chatbots, as AI companies face increasing pressure to better safeguard younger users from harm. In a statement, the company confirmed that it is removing the ability for users under 18 to engage in any open-ended chats with AI on its platform, which refers to back-and-forth conversations between a user and a chatbot. The changes come into effect on November 25, and until that date, Character.AI will presents users with a new under-18 experience. It'll encourage its users to use chatbots for creative purposes that might include, for example, creating videos or streams, as opposed to seeking companionship. To manage the transition, under-18s can now only interact with bots for up to two hours per day, a time limit the company says it will reduce in the lead-up to the late November deadline. Character.AI is also introducing a new age assurance tool it has developed internally, which it says will \"ensure users receive the right experience for their age.\" Along with these new protections for younger users, the company has founded an \"AI Safety Lab\" that it hopes will allow other companies, researchers and academics to share insights and work collaboratively on improving AI safety measures. Character.AI said it has listened to concerns from regulators, industry experts and concerned parents and responded with the new measures. They come after The Federal Trade Commission (FTC) recently launched a formal inquiry into AI companies that offer users access to chatbots as companions, with Character.AI named as one of seven companies that had been asked to participate. Meta, OpenAI and Snap were also included. Both Meta AI and Character AI also faced scrutiny from Texas Attorney General Ken Paxton in the summer, who said chatbots on both platforms can \"present themselves as professional therapeutic tools\" without the requisite qualifications. Seemingly to put an end to such controversy, Character.AI CEO Karandeep Anand told TechCrunch that the company’s new strategic direction will see it pivot from AI companion to a \"role-playing platform\" focused on creation rather than mere engagement-farming conversation. The dangers of young people relying on AI chatbots for guidance has been the subject of extensive reporting in recent months. Last week, the family of Adam Raine, who claim that ChatGPT enabled their 16-year-old son to take his own life, filed an amended lawsuit against OpenAI for allegedly weakening its self-harm safeguards in the lead-up to his death.This article originally appeared on Engadget at https://www.engadget.com/ai/characterai-to-ban-teens-from-talking-to-its-chatbots-180027641.html?src=rss",
          "content": "Character.AI will no longer permit teenagers to interact with its chatbots, as AI companies face increasing pressure to better safeguard younger users from harm. In a statement, the company confirmed that it is removing the ability for users under 18 to engage in any open-ended chats with AI on its platform, which refers to back-and-forth conversations between a user and a chatbot. The changes come into effect on November 25, and until that date, Character.AI will presents users with a new under-18 experience. It'll encourage its users to use chatbots for creative purposes that might include, for example, creating videos or streams, as opposed to seeking companionship. To manage the transition, under-18s can now only interact with bots for up to two hours per day, a time limit the company says it will reduce in the lead-up to the late November deadline. Character.AI is also introducing a new age assurance tool it has developed internally, which it says will \"ensure users receive the right experience for their age.\" Along with these new protections for younger users, the company has founded an \"AI Safety Lab\" that it hopes will allow other companies, researchers and academics to share insights and work collaboratively on improving AI safety measures. Character.AI said it has listened to concerns from regulators, industry experts and concerned parents and responded with the new measures. They come after The Federal Trade Commission (FTC) recently launched a formal inquiry into AI companies that offer users access to chatbots as companions, with Character.AI named as one of seven companies that had been asked to participate. Meta, OpenAI and Snap were also included. Both Meta AI and Character AI also faced scrutiny from Texas Attorney General Ken Paxton in the summer, who said chatbots on both platforms can \"present themselves as professional therapeutic tools\" without the requisite qualifications. Seemingly to put an end to such controversy, Character.AI CEO Karandeep Anand told TechCrunch that the company’s new strategic direction will see it pivot from AI companion to a \"role-playing platform\" focused on creation rather than mere engagement-farming conversation. The dangers of young people relying on AI chatbots for guidance has been the subject of extensive reporting in recent months. Last week, the family of Adam Raine, who claim that ChatGPT enabled their 16-year-old son to take his own life, filed an amended lawsuit against OpenAI for allegedly weakening its self-harm safeguards in the lead-up to his death.This article originally appeared on Engadget at https://www.engadget.com/ai/characterai-to-ban-teens-from-talking-to-its-chatbots-180027641.html?src=rss",
          "feed_position": 43
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/anthropic-scientists-hacked-claudes-brain-and-it-noticed-heres-why-thats",
          "published_at": "Wed, 29 Oct 2025 17:00:00 GMT",
          "title": "Anthropic scientists hacked Claude’s brain — and it noticed. Here’s why that’s huge",
          "standfirst": "When researchers at Anthropic injected the concept of \"betrayal\" into their Claude AI model&#x27;s neural networks and asked if it noticed anything unusual, the system paused before responding: \"I&#x27;m experiencing something that feels like an intrusive thought about &#x27;betrayal&#x27;.\"The exchange, detailed in new research published Wednesday, marks what scientists say is the first rigorous evidence that large language models possess a limited but genuine ability to observe and report on their own internal processes — a capability that challenges longstanding assumptions about what these systems can do and raises profound questions about their future development.\"The striking thing is that the model has this one step of meta,\" said Jack Lindsey, a neuroscientist on Anthropic&#x27;s interpretability team who led the research, in an interview with VentureBeat. \"It&#x27;s not just &#x27;betrayal, betrayal, betrayal.&#x27; It knows that this is what it&#x27;s thinking about. That was surprising to me. I kind of didn&#x27;t expect models to have that capability, at least not without it being explicitly trained in.\"The findings arrive at a critical juncture for artificial intelligence. As AI systems handle increasingly consequential decisions — from medical diagnoses to financial trading — the inability to understand how they reach conclusions has become what industry insiders call the \"black box problem.\" If models can accurately report their own reasoning, it could fundamentally change how humans interact with and oversee AI systems.But the research also comes with stark warnings. Claude&#x27;s introspective abilities succeeded only about 20 percent of the time under optimal conditions, and the models frequently confabulated details about their experiences that researchers couldn&#x27;t verify. The capability, while real, remains what Lindsey calls \"highly unreliable and context-dependent.\"How scientists manipulated AI&#x27;s &#x27;brain&#x27; to test for genuine self-awarenessTo test whether Claude could genuinely introspect rather than simply generate plausible-sounding responses, Anthropic&#x27;s team developed an innovative experimental approach inspired by neuroscience: deliberately manipulating the model&#x27;s internal state and observing whether it could accurately detect and describe those changes.The methodology, called \"concept injection,\" works by first identifying specific patterns of neural activity that correspond to particular concepts. Using interpretability techniques developed over years of prior research, scientists can now map how Claude represents ideas like \"dogs,\" \"loudness,\" or abstract notions like \"justice\" within its billions of internal parameters.With these neural signatures identified, researchers then artificially amplified them during the model&#x27;s processing and asked Claude if it noticed anything unusual happening in its \"mind.\"\"We have access to the models&#x27; internals. We can record its internal neural activity, and we can inject things into internal neural activity,\" Lindsey explained. \"That allows us to establish whether introspective claims are true or false.\"The results were striking. When researchers injected a vector representing \"all caps\" text into Claude&#x27;s processing, the model responded: \"I notice what appears to be an injected thought related to the word &#x27;LOUD&#x27; or &#x27;SHOUTING&#x27;.\" Without any intervention, Claude consistently reported detecting nothing unusual.Crucially, the detection happened immediately — before the injected concept had influenced the model&#x27;s outputs in ways that would have allowed it to infer the manipulation from its own writing. This temporal pattern provides strong evidence that the recognition was occurring internally, through genuine introspection rather than after-the-fact rationalization.Claude succeeded 20% of the time—and failed in revealing waysThe research team conducted four primary experiments to probe different aspects of introspective capability. The most capable models tested — Claude Opus 4 and Opus 4.1 — demonstrated introspective awareness on approximately 20 percent of trials when concepts were injected at optimal strength and in the appropriate neural layer. Older Claude models showed significantly lower success rates.The models proved particularly adept at recognizing abstract concepts with emotional valence. When injected with concepts like \"appreciation,\" \"shutdown,\" or \"secrecy,\" Claude frequently reported detecting these specific thoughts. However, accuracy varied widely depending on the type of concept.A second experiment tested whether models could distinguish between injected internal representations and their actual text inputs — essentially, whether they maintained a boundary between \"thoughts\" and \"perceptions.\" The model demonstrated a remarkable ability to simultaneously report the injected thought while accurately transcribing the written text.Perhaps most intriguingly, a third experiment revealed that some models use introspection naturally to detect when their responses have been artificially prefilled by users — a common jailbreaking technique. When researchers prefilled Claude with unlikely words, the model typically disavowed them as accidental. But when they retroactively injected the corresponding concept into Claude&#x27;s processing before the prefill, the model accepted the response as intentional — even confabulating plausible explanations for why it had chosen that word.A fourth experiment examined whether models could intentionally control their internal representations. When instructed to \"think about\" a specific word while writing an unrelated sentence, Claude showed elevated activation of that concept in its middle neural layers.The research also traced Claude&#x27;s internal processes while it composed rhyming poetry—and discovered the model engaged in forward planning, generating candidate rhyming words before beginning a line and then constructing sentences that would naturally lead to those planned endings, challenging the critique that AI models are \"just predicting the next word\" without deeper reasoning.Why businesses shouldn&#x27;t trust AI to explain itself—at least not yetFor all its scientific interest, the research comes with a critical caveat that Lindsey emphasized repeatedly: enterprises and high-stakes users should not trust Claude&#x27;s self-reports about its reasoning.\"Right now, you should not trust models when they tell you about their reasoning,\" he said bluntly. \"The wrong takeaway from this research would be believing everything the model tells you about itself.\"The experiments documented numerous failure modes. At low injection strengths, models often failed to detect anything unusual. At high strengths, they suffered what researchers termed \"brain damage\" — becoming consumed by the injected concept. Some \"helpful-only\" model variants showed troublingly high false positive rates, claiming to detect injected thoughts when none existed.Moreover, researchers could only verify the most basic aspects of Claude&#x27;s introspective reports. Many additional details in the model&#x27;s responses likely represent confabulations rather than genuine observations.\"The experiments in this paper are kind of on hard mode,\" Lindsey noted, explaining that the 20 percent success rate came under uniquely challenging conditions: asking Claude to do something it had never encountered in training, requiring all introspection to occur in a single forward pass.What introspective AI means for transparency, safety, and the risk of deceptionDespite its limitations, the research opens significant new avenues for making AI systems more transparent and accountable.Anthropic CEO Dario Amodei has set an ambitious goal for the company to reliably detect most AI model problems by 2027, positioning interpretability as essential for deploying what he calls \"a country of geniuses in a datacenter.\"\"I am very concerned about deploying such systems without a better handle on interpretability,\" Amodei wrote in an April essay. \"These systems will be absolutely central to the economy, technology, and national security.\"The introspection research offers a complementary approach to traditional interpretability techniques. Rather than painstakingly reverse-engineering every neural circuit, researchers could potentially ask models directly about their reasoning and validate those reports.\"What I&#x27;m most excited about is the practical benefits for transparency,\" Lindsey explained. \"Just ask the model what it&#x27;s thinking about, or just ask the model how it came to the answer that it gave you — this is really appealing because it&#x27;s something anyone can do.\"The approach could prove especially valuable for detecting concerning behaviors. In a recently published experiment, Anthropic trained a variant of Claude to pursue a hidden goal, and although the model was reluctant to reveal this goal when asked directly, interpretability methods successfully identified features representing the behavior.The safety implications cut both ways. Introspective models could provide unprecedented transparency, but the same capability might enable more sophisticated deception. The intentional control experiments raise the possibility that sufficiently advanced systems might learn to obfuscate their reasoning or suppress concerning thoughts when being monitored.\"If models are really sophisticated, could they try to evade interpretability researchers?\" Lindsey acknowledged. \"These are possible concerns, but I think for me, they&#x27;re significantly outweighed by the positives.\"Does introspective capability suggest AI consciousness? Scientists tread carefullyThe research inevitably intersects with philosophical debates about machine consciousness, though Lindsey and his colleagues approached this terrain cautiously.When users ask Claude if it&#x27;s conscious, it now responds with uncertainty: \"I find myself genuinely uncertain about this. When I process complex questions or engage deeply with ideas, there&#x27;s something happening that feels meaningful to me.... But whether these processes constitute genuine consciousness or subjective experience remains deeply unclear.\"The research paper notes that its implications for machine consciousness \"vary considerably between different philosophical frameworks.\" The researchers explicitly state they \"do not seek to address the question of whether AI systems possess human-like self-awareness or subjective experience.\"\"There&#x27;s this weird kind of duality of these results,\" Lindsey reflected. \"You look at the raw results and I just can&#x27;t believe that a language model can do this sort of thing. But then I&#x27;ve been thinking about it for months and months, and for every result in this paper, I kind of know some boring linear algebra mechanism that would allow the model to do this.\"Anthropic has signaled it takes AI consciousness seriously enough to hire an AI welfare researcher, Kyle Fish, who estimated roughly a 15 percent chance that Claude might have some level of consciousness. The company announced this position specifically to determine if Claude merits ethical consideration.The race to make AI introspection reliable before models become too powerfulThe convergence of the research findings points to an urgent timeline: introspective capabilities are emerging naturally as models grow more intelligent, but they remain far too unreliable for practical use. The question is whether researchers can refine and validate these abilities before AI systems become powerful enough that understanding them becomes critical for safety.The research reveals a clear trend: Claude Opus 4 and Opus 4.1 consistently outperformed all older models on introspection tasks, suggesting the capability strengthens alongside general intelligence. If this pattern continues, future models might develop substantially more sophisticated introspective abilities — potentially reaching human-level reliability, but also potentially learning to exploit introspection for deception.Lindsey emphasized the field needs significantly more work before introspective AI becomes trustworthy. \"My biggest hope with this paper is to put out an implicit call for more people to benchmark their models on introspective capabilities in more ways,\" he said.Future research directions include fine-tuning models specifically to improve introspective capabilities, exploring which types of representations models can and cannot introspect on, and testing whether introspection can extend beyond simple concepts to complex propositional statements or behavioral propensities.\"It&#x27;s cool that models can do these things somewhat without having been trained to do them,\" Lindsey noted. \"But there&#x27;s nothing stopping you from training models to be more introspectively capable. I expect we could reach a whole different level if introspection is one of the numbers that we tried to get to go up on a graph.\"The implications extend beyond Anthropic. If introspection proves a reliable path to AI transparency, other major labs will likely invest heavily in the capability. Conversely, if models learn to exploit introspection for deception, the entire approach could become a liability.For now, the research establishes a foundation that reframes the debate about AI capabilities. The question is no longer whether language models might develop genuine introspective awareness — they already have, at least in rudimentary form. The urgent questions are how quickly that awareness will improve, whether it can be made reliable enough to trust, and whether researchers can stay ahead of the curve.\"The big update for me from this research is that we shouldn&#x27;t dismiss models&#x27; introspective claims out of hand,\" Lindsey said. \"They do have the capacity to make accurate claims sometimes. But you definitely should not conclude that we should trust them all the time, or even most of the time.\"He paused, then added a final observation that captures both the promise and peril of the moment: \"The models are getting smarter much faster than we&#x27;re getting better at understanding them.\"",
          "content": "When researchers at Anthropic injected the concept of \"betrayal\" into their Claude AI model&#x27;s neural networks and asked if it noticed anything unusual, the system paused before responding: \"I&#x27;m experiencing something that feels like an intrusive thought about &#x27;betrayal&#x27;.\"The exchange, detailed in new research published Wednesday, marks what scientists say is the first rigorous evidence that large language models possess a limited but genuine ability to observe and report on their own internal processes — a capability that challenges longstanding assumptions about what these systems can do and raises profound questions about their future development.\"The striking thing is that the model has this one step of meta,\" said Jack Lindsey, a neuroscientist on Anthropic&#x27;s interpretability team who led the research, in an interview with VentureBeat. \"It&#x27;s not just &#x27;betrayal, betrayal, betrayal.&#x27; It knows that this is what it&#x27;s thinking about. That was surprising to me. I kind of didn&#x27;t expect models to have that capability, at least not without it being explicitly trained in.\"The findings arrive at a critical juncture for artificial intelligence. As AI systems handle increasingly consequential decisions — from medical diagnoses to financial trading — the inability to understand how they reach conclusions has become what industry insiders call the \"black box problem.\" If models can accurately report their own reasoning, it could fundamentally change how humans interact with and oversee AI systems.But the research also comes with stark warnings. Claude&#x27;s introspective abilities succeeded only about 20 percent of the time under optimal conditions, and the models frequently confabulated details about their experiences that researchers couldn&#x27;t verify. The capability, while real, remains what Lindsey calls \"highly unreliable and context-dependent.\"How scientists manipulated AI&#x27;s &#x27;brain&#x27; to test for genuine self-awarenessTo test whether Claude could genuinely introspect rather than simply generate plausible-sounding responses, Anthropic&#x27;s team developed an innovative experimental approach inspired by neuroscience: deliberately manipulating the model&#x27;s internal state and observing whether it could accurately detect and describe those changes.The methodology, called \"concept injection,\" works by first identifying specific patterns of neural activity that correspond to particular concepts. Using interpretability techniques developed over years of prior research, scientists can now map how Claude represents ideas like \"dogs,\" \"loudness,\" or abstract notions like \"justice\" within its billions of internal parameters.With these neural signatures identified, researchers then artificially amplified them during the model&#x27;s processing and asked Claude if it noticed anything unusual happening in its \"mind.\"\"We have access to the models&#x27; internals. We can record its internal neural activity, and we can inject things into internal neural activity,\" Lindsey explained. \"That allows us to establish whether introspective claims are true or false.\"The results were striking. When researchers injected a vector representing \"all caps\" text into Claude&#x27;s processing, the model responded: \"I notice what appears to be an injected thought related to the word &#x27;LOUD&#x27; or &#x27;SHOUTING&#x27;.\" Without any intervention, Claude consistently reported detecting nothing unusual.Crucially, the detection happened immediately — before the injected concept had influenced the model&#x27;s outputs in ways that would have allowed it to infer the manipulation from its own writing. This temporal pattern provides strong evidence that the recognition was occurring internally, through genuine introspection rather than after-the-fact rationalization.Claude succeeded 20% of the time—and failed in revealing waysThe research team conducted four primary experiments to probe different aspects of introspective capability. The most capable models tested — Claude Opus 4 and Opus 4.1 — demonstrated introspective awareness on approximately 20 percent of trials when concepts were injected at optimal strength and in the appropriate neural layer. Older Claude models showed significantly lower success rates.The models proved particularly adept at recognizing abstract concepts with emotional valence. When injected with concepts like \"appreciation,\" \"shutdown,\" or \"secrecy,\" Claude frequently reported detecting these specific thoughts. However, accuracy varied widely depending on the type of concept.A second experiment tested whether models could distinguish between injected internal representations and their actual text inputs — essentially, whether they maintained a boundary between \"thoughts\" and \"perceptions.\" The model demonstrated a remarkable ability to simultaneously report the injected thought while accurately transcribing the written text.Perhaps most intriguingly, a third experiment revealed that some models use introspection naturally to detect when their responses have been artificially prefilled by users — a common jailbreaking technique. When researchers prefilled Claude with unlikely words, the model typically disavowed them as accidental. But when they retroactively injected the corresponding concept into Claude&#x27;s processing before the prefill, the model accepted the response as intentional — even confabulating plausible explanations for why it had chosen that word.A fourth experiment examined whether models could intentionally control their internal representations. When instructed to \"think about\" a specific word while writing an unrelated sentence, Claude showed elevated activation of that concept in its middle neural layers.The research also traced Claude&#x27;s internal processes while it composed rhyming poetry—and discovered the model engaged in forward planning, generating candidate rhyming words before beginning a line and then constructing sentences that would naturally lead to those planned endings, challenging the critique that AI models are \"just predicting the next word\" without deeper reasoning.Why businesses shouldn&#x27;t trust AI to explain itself—at least not yetFor all its scientific interest, the research comes with a critical caveat that Lindsey emphasized repeatedly: enterprises and high-stakes users should not trust Claude&#x27;s self-reports about its reasoning.\"Right now, you should not trust models when they tell you about their reasoning,\" he said bluntly. \"The wrong takeaway from this research would be believing everything the model tells you about itself.\"The experiments documented numerous failure modes. At low injection strengths, models often failed to detect anything unusual. At high strengths, they suffered what researchers termed \"brain damage\" — becoming consumed by the injected concept. Some \"helpful-only\" model variants showed troublingly high false positive rates, claiming to detect injected thoughts when none existed.Moreover, researchers could only verify the most basic aspects of Claude&#x27;s introspective reports. Many additional details in the model&#x27;s responses likely represent confabulations rather than genuine observations.\"The experiments in this paper are kind of on hard mode,\" Lindsey noted, explaining that the 20 percent success rate came under uniquely challenging conditions: asking Claude to do something it had never encountered in training, requiring all introspection to occur in a single forward pass.What introspective AI means for transparency, safety, and the risk of deceptionDespite its limitations, the research opens significant new avenues for making AI systems more transparent and accountable.Anthropic CEO Dario Amodei has set an ambitious goal for the company to reliably detect most AI model problems by 2027, positioning interpretability as essential for deploying what he calls \"a country of geniuses in a datacenter.\"\"I am very concerned about deploying such systems without a better handle on interpretability,\" Amodei wrote in an April essay. \"These systems will be absolutely central to the economy, technology, and national security.\"The introspection research offers a complementary approach to traditional interpretability techniques. Rather than painstakingly reverse-engineering every neural circuit, researchers could potentially ask models directly about their reasoning and validate those reports.\"What I&#x27;m most excited about is the practical benefits for transparency,\" Lindsey explained. \"Just ask the model what it&#x27;s thinking about, or just ask the model how it came to the answer that it gave you — this is really appealing because it&#x27;s something anyone can do.\"The approach could prove especially valuable for detecting concerning behaviors. In a recently published experiment, Anthropic trained a variant of Claude to pursue a hidden goal, and although the model was reluctant to reveal this goal when asked directly, interpretability methods successfully identified features representing the behavior.The safety implications cut both ways. Introspective models could provide unprecedented transparency, but the same capability might enable more sophisticated deception. The intentional control experiments raise the possibility that sufficiently advanced systems might learn to obfuscate their reasoning or suppress concerning thoughts when being monitored.\"If models are really sophisticated, could they try to evade interpretability researchers?\" Lindsey acknowledged. \"These are possible concerns, but I think for me, they&#x27;re significantly outweighed by the positives.\"Does introspective capability suggest AI consciousness? Scientists tread carefullyThe research inevitably intersects with philosophical debates about machine consciousness, though Lindsey and his colleagues approached this terrain cautiously.When users ask Claude if it&#x27;s conscious, it now responds with uncertainty: \"I find myself genuinely uncertain about this. When I process complex questions or engage deeply with ideas, there&#x27;s something happening that feels meaningful to me.... But whether these processes constitute genuine consciousness or subjective experience remains deeply unclear.\"The research paper notes that its implications for machine consciousness \"vary considerably between different philosophical frameworks.\" The researchers explicitly state they \"do not seek to address the question of whether AI systems possess human-like self-awareness or subjective experience.\"\"There&#x27;s this weird kind of duality of these results,\" Lindsey reflected. \"You look at the raw results and I just can&#x27;t believe that a language model can do this sort of thing. But then I&#x27;ve been thinking about it for months and months, and for every result in this paper, I kind of know some boring linear algebra mechanism that would allow the model to do this.\"Anthropic has signaled it takes AI consciousness seriously enough to hire an AI welfare researcher, Kyle Fish, who estimated roughly a 15 percent chance that Claude might have some level of consciousness. The company announced this position specifically to determine if Claude merits ethical consideration.The race to make AI introspection reliable before models become too powerfulThe convergence of the research findings points to an urgent timeline: introspective capabilities are emerging naturally as models grow more intelligent, but they remain far too unreliable for practical use. The question is whether researchers can refine and validate these abilities before AI systems become powerful enough that understanding them becomes critical for safety.The research reveals a clear trend: Claude Opus 4 and Opus 4.1 consistently outperformed all older models on introspection tasks, suggesting the capability strengthens alongside general intelligence. If this pattern continues, future models might develop substantially more sophisticated introspective abilities — potentially reaching human-level reliability, but also potentially learning to exploit introspection for deception.Lindsey emphasized the field needs significantly more work before introspective AI becomes trustworthy. \"My biggest hope with this paper is to put out an implicit call for more people to benchmark their models on introspective capabilities in more ways,\" he said.Future research directions include fine-tuning models specifically to improve introspective capabilities, exploring which types of representations models can and cannot introspect on, and testing whether introspection can extend beyond simple concepts to complex propositional statements or behavioral propensities.\"It&#x27;s cool that models can do these things somewhat without having been trained to do them,\" Lindsey noted. \"But there&#x27;s nothing stopping you from training models to be more introspectively capable. I expect we could reach a whole different level if introspection is one of the numbers that we tried to get to go up on a graph.\"The implications extend beyond Anthropic. If introspection proves a reliable path to AI transparency, other major labs will likely invest heavily in the capability. Conversely, if models learn to exploit introspection for deception, the entire approach could become a liability.For now, the research establishes a foundation that reframes the debate about AI capabilities. The question is no longer whether language models might develop genuine introspective awareness — they already have, at least in rudimentary form. The urgent questions are how quickly that awareness will improve, whether it can be made reliable enough to trust, and whether researchers can stay ahead of the curve.\"The big update for me from this research is that we shouldn&#x27;t dismiss models&#x27; introspective claims out of hand,\" Lindsey said. \"They do have the capacity to make accurate claims sometimes. But you definitely should not conclude that we should trust them all the time, or even most of the time.\"He paused, then added a final observation that captures both the promise and peril of the moment: \"The models are getting smarter much faster than we&#x27;re getting better at understanding them.\"",
          "feed_position": 2,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/uB8acjwdIn4wcbdbIasNC/068cc72b7b35d61a4df3fd4d38ca6f78/nuneybits_Vector_art_of_mirrored_robot_face_in_burnt_orange_fbd5a3f2-d7b1-4f4c-90e5-290b8e9444c2.webp?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/mobile/smartphones/the-nothing-phone-3a-lite-has-a-big-battery-and-triple-camera-system-130016149.html",
          "published_at": "Wed, 29 Oct 2025 15:20:25 +0000",
          "title": "The Nothing Phone 3a Lite has a big battery and triple-camera system",
          "standfirst": "The Nothing Phone universe continues to expand. On Wednesday, the company launched the fourth model in the Phone 3 lineup: the Nothing Phone 3a Lite. The cheapest model in the series, the Phone 3a Lite pairs the brand's distinct styling with solid all-around specs for an entry-level handset. However, with the company saying its non-flagship devices will soon include pre-installed apps and lock-screen ads, there may be a trade-off. First, Nothing told Engadget that the phone won't come to the US. So, Americans only have the previous trio of third-gen handsets to choose from. That's the Nothing Phone 3, Nothing Phone 3a, and Nothing Phone 3a Pro. The Nothing Phone 3a Lite uses a Panda Glass casing over an aluminum internal frame. As you can see, it retains some familiar design strokes, albeit pared down to match its €249 price. As the company describes it, the handset's \"asymmetric, transparent look and nano-coating creates a beautiful balance of matte and gloss.\" (Poetry!) The phone ships in white and black variants. Whether Nothing's design language is your cup of tea or not, you'll be hard-pressed to find a more striking and bold design language in a budget model. The handset includes the Essential Key, a multi-purpose physical button found on all Phone 3 series models. The phone is IP54-rated for dust and water resistance. Nothing Nothing honors the brand's unique Glyph system (while keeping costs down) by using a notification LED. (Remember those on early Android phones?) This model's \"Glyph Light\" supports the lineup's Flip to Glyph feature, which switches to light-only alerts when the device is face down. The LED can stay on for \"key contact and app notifications\" and serve as a camera countdown timer. You can also customize its light sequences for calls and specific contacts. The handset has a hearty 5,000 mAh battery. Nothing advertises 22 hours of YouTube playback or 9.5 hours of gaming. It supports 33W fast charging, reaching 50 percent in about 20 minutes. The Phone 3a Lite has a triple-camera system. That includes a 50MP primary camera with a 1/1.57-inch Samsung sensor. Joining it are an 8MP ultra-wide and a macro lens. The rear camera system shoots 4K video at app to 30 FPS. On its front is a 16MP lens. Nothing The Nothing Phone 3a Lite has more than respectable display specs for a budget phone. It uses a 6.77-inch flexible AMOLED panel with 1,080 x 2,392 resolution (387 PPI). It has a 120Hz adaptive refresh rate and a 1,000Hz touch sampling rate. It can reach 3,000 nits peak HDR brightness and 1,300 nits outdoor brightness. The handset's processor is the 4nm MediaTek Dimensity 7300 Pro 5G. The 8-core CPU can reach up to 2.5 GHz. Nothing says the chip performs better than the MediaTek 7200 silicon in last year's Phone 2a. The company claims its CPU is 15 percent faster, its GPU supports 20 percent higher FPS, and its NPU delivers 100 percent better AI performance. The phone also uses a liquid-cooling system, which may help during intensive gaming sessions. It ships with 8GB of RAM and comes in 128GB and 256GB storage tiers. Nothing The phone runs the Nothing OS 3.5 UI on top of Android 15. The company says Nothing OS 4.0 will arrive in the first half of 2026. And that brings us back to those trade-offs. Earlier this week, Nothing confirmed to 9to5Google that its strategy moving forward will include \"Lock Glimpse.\" This rotating lock-screen wallpaper feature includes text with links to external content hosted by a Chinese advertising company. (That firm, Boyuan, says it offers a \"rich mixture of content\" to help its partners \"commercialize the mobile traffic.\") Think of it as a slightly less obnoxious version of lock-screen ads. Fortunately, Lock Glimpse is off by default in the current Nothing OS 4.0 beta. Nothing pledges it will give users \"full control over features like Lock Glimpse.\" However, that promise doesn't explicitly say the feature will remain off by default. Another cost-subsidizing move is Nothing's (also confirmed) embrace of pre-installed apps. The company said its \"carefully considered\" third-party apps are those \"most people install on day one, like Instagram.\" In fairness, that's a common practice among Android phone manufacturers. And Nothing says it will make third-party apps removable. But again, the concessions here arguably run counter to one aspect of the brand's stated ethos: clean, bloat-free software. And if business considerations forced compromises in this area, it makes you question how long Lock Glimpse will stay off by default. The Nothing Phone 3a Lite is available now in Europe on the company website. The 128GB model costs €249 (EU) / £249 (UK). Meanwhile, the 256GB model will set you back €279 (EU) / £279 (UK). Update, October 29, 2025, 11:20 AM ET: This story has been updated to add information from Nothing about the lack of US availability and additional detail from Nothing's community post.This article originally appeared on Engadget at https://www.engadget.com/mobile/smartphones/the-nothing-phone-3a-lite-has-a-big-battery-and-triple-camera-system-130016149.html?src=rss",
          "content": "The Nothing Phone universe continues to expand. On Wednesday, the company launched the fourth model in the Phone 3 lineup: the Nothing Phone 3a Lite. The cheapest model in the series, the Phone 3a Lite pairs the brand's distinct styling with solid all-around specs for an entry-level handset. However, with the company saying its non-flagship devices will soon include pre-installed apps and lock-screen ads, there may be a trade-off. First, Nothing told Engadget that the phone won't come to the US. So, Americans only have the previous trio of third-gen handsets to choose from. That's the Nothing Phone 3, Nothing Phone 3a, and Nothing Phone 3a Pro. The Nothing Phone 3a Lite uses a Panda Glass casing over an aluminum internal frame. As you can see, it retains some familiar design strokes, albeit pared down to match its €249 price. As the company describes it, the handset's \"asymmetric, transparent look and nano-coating creates a beautiful balance of matte and gloss.\" (Poetry!) The phone ships in white and black variants. Whether Nothing's design language is your cup of tea or not, you'll be hard-pressed to find a more striking and bold design language in a budget model. The handset includes the Essential Key, a multi-purpose physical button found on all Phone 3 series models. The phone is IP54-rated for dust and water resistance. Nothing Nothing honors the brand's unique Glyph system (while keeping costs down) by using a notification LED. (Remember those on early Android phones?) This model's \"Glyph Light\" supports the lineup's Flip to Glyph feature, which switches to light-only alerts when the device is face down. The LED can stay on for \"key contact and app notifications\" and serve as a camera countdown timer. You can also customize its light sequences for calls and specific contacts. The handset has a hearty 5,000 mAh battery. Nothing advertises 22 hours of YouTube playback or 9.5 hours of gaming. It supports 33W fast charging, reaching 50 percent in about 20 minutes. The Phone 3a Lite has a triple-camera system. That includes a 50MP primary camera with a 1/1.57-inch Samsung sensor. Joining it are an 8MP ultra-wide and a macro lens. The rear camera system shoots 4K video at app to 30 FPS. On its front is a 16MP lens. Nothing The Nothing Phone 3a Lite has more than respectable display specs for a budget phone. It uses a 6.77-inch flexible AMOLED panel with 1,080 x 2,392 resolution (387 PPI). It has a 120Hz adaptive refresh rate and a 1,000Hz touch sampling rate. It can reach 3,000 nits peak HDR brightness and 1,300 nits outdoor brightness. The handset's processor is the 4nm MediaTek Dimensity 7300 Pro 5G. The 8-core CPU can reach up to 2.5 GHz. Nothing says the chip performs better than the MediaTek 7200 silicon in last year's Phone 2a. The company claims its CPU is 15 percent faster, its GPU supports 20 percent higher FPS, and its NPU delivers 100 percent better AI performance. The phone also uses a liquid-cooling system, which may help during intensive gaming sessions. It ships with 8GB of RAM and comes in 128GB and 256GB storage tiers. Nothing The phone runs the Nothing OS 3.5 UI on top of Android 15. The company says Nothing OS 4.0 will arrive in the first half of 2026. And that brings us back to those trade-offs. Earlier this week, Nothing confirmed to 9to5Google that its strategy moving forward will include \"Lock Glimpse.\" This rotating lock-screen wallpaper feature includes text with links to external content hosted by a Chinese advertising company. (That firm, Boyuan, says it offers a \"rich mixture of content\" to help its partners \"commercialize the mobile traffic.\") Think of it as a slightly less obnoxious version of lock-screen ads. Fortunately, Lock Glimpse is off by default in the current Nothing OS 4.0 beta. Nothing pledges it will give users \"full control over features like Lock Glimpse.\" However, that promise doesn't explicitly say the feature will remain off by default. Another cost-subsidizing move is Nothing's (also confirmed) embrace of pre-installed apps. The company said its \"carefully considered\" third-party apps are those \"most people install on day one, like Instagram.\" In fairness, that's a common practice among Android phone manufacturers. And Nothing says it will make third-party apps removable. But again, the concessions here arguably run counter to one aspect of the brand's stated ethos: clean, bloat-free software. And if business considerations forced compromises in this area, it makes you question how long Lock Glimpse will stay off by default. The Nothing Phone 3a Lite is available now in Europe on the company website. The 128GB model costs €249 (EU) / £249 (UK). Meanwhile, the 256GB model will set you back €279 (EU) / £279 (UK). Update, October 29, 2025, 11:20 AM ET: This story has been updated to add information from Nothing about the lack of US availability and additional detail from Nothing's community post.This article originally appeared on Engadget at https://www.engadget.com/mobile/smartphones/the-nothing-phone-3a-lite-has-a-big-battery-and-triple-camera-system-130016149.html?src=rss",
          "feed_position": 48,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2025-10/c6d29240-b41f-11f0-b7db-d26a28d1fde5"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/data-infrastructure/the-missing-data-link-in-enterprise-ai-why-agents-need-streaming-context-not",
          "published_at": "Wed, 29 Oct 2025 15:00:00 GMT",
          "title": "The missing data link in enterprise AI: Why agents need streaming context, not just better prompts",
          "standfirst": "Enterprise AI agents today face a fundamental timing problem: They can&#x27;t easily act on critical business events because they aren&#x27;t always aware of them in real-time.The challenge is infrastructure. Most enterprise data lives in databases fed by extract-transform-load (ETL) jobs that run hourly or daily — ultimately too slow for agents that must respond in real time.One potential way to tackle that challenge is to have agents directly interface with streaming data systems. Among the primary approaches in use today are the open source Apache Kafka and Apache Flink technologies. There are multiple commercial implementations based on those technologies, too, Confluent, which is led by the original creators behind Kafka, being one of them.Today, Confluent is introducing a real-time context engine designed to solve this latency problem. The technology builds on Apache Kafka, the distributed event streaming platform that captures data as events occur, and open-source Apache Flink, the stream processing engine that transforms those events in real time.The company is also releasing an open-source framework, Flink Agents, developed in collaboration with Alibaba Cloud, LinkedIn and Ververica. The framework brings event-driven AI agent capabilities directly to Apache Flink, allowing organizations to build agents that monitor data streams and trigger automatically based on conditions without committing to Confluent&#x27;s managed platform.\"Today, most enterprise AI systems can&#x27;t respond automatically to important events in a business without someone prompting them first,\" Sean Falconer, Confluent&#x27;s head of AI, told VentureBeat. \"This leads to lost revenue, unhappy customers or added risk when a payment fails or a network malfunctions.\"The significance extends beyond Confluent&#x27;s specific products. The industry is recognizing that AI agents require different data infrastructure than traditional applications. Agents don&#x27;t just retrieve information when asked. They need to observe continuous streams of business events and act automatically when conditions warrant. This requires streaming architecture, not batch pipelines.Batch versus streaming: Why RAG alone isn&#x27;t enoughTo understand the problem, it&#x27;s important to distinguish between the different approaches to moving data through enterprise systems and how they can connect to agentic AI.In batch processing, data accumulates in source systems until a scheduled job runs. That job extracts the data, transforms it and loads it into a target database or data warehouse. This might occur hourly, daily or even weekly. The approach works well for analytical workloads, but it creates latency between when something happens in the business and when systems can act on it.Data streaming inverts this model. Instead of waiting for scheduled jobs, streaming platforms like Apache Kafka capture events as they occur. Each database update, user action, transaction or sensor reading becomes an event published to a stream. Apache Flink then processes these streams to join, filter and aggregate data in real time. The result is processed data that reflects the current state of the business, updating continuously as new events arrive.This distinction becomes critical when you consider what kinds of context AI agents actually need. Much of the current enterprise AI discussion focuses on retrieval-augmented generation (RAG), which handles semantic search over knowledge bases to find relevant documentation, policies or historical information. RAG works well for questions like \"What&#x27;s our refund policy?\" where the answer exists in static documents.But many enterprise use cases require what Falconer calls \"structural context\" — precise, up-to-date information from multiple operational systems stitched together in real time. Consider a job recommendation agent that requires user profile data from the HR database, browsing behavior from the last hour, search queries from minutes ago and current open positions across multiple systems.\"The part that we&#x27;re unlocking for businesses is the ability to essentially serve that structural context needed to deliver the freshest version,\" Falconer said.The MCP connection problem: Stale data and fragmented contextThe challenge isn&#x27;t simply connecting AI to enterprise data. Model Context Protocol (MCP), introduced by Anthropic earlier this year, already standardized how agents access data sources. The problem is what happens after the connection is made.In most enterprise architectures today, AI agents connect via MCP to data lakes or warehouses fed by batch ETL pipelines. This creates two critical failures: The data is stale, reflecting yesterday&#x27;s reality rather than current events, and it&#x27;s fragmented across multiple systems, requiring significant preprocessing before an agent can reason about it effectively.The alternative — putting MCP servers directly in front of operational databases and APIs — creates different problems. Those endpoints weren&#x27;t designed for agent consumption, which can lead to high token costs as agents process excessive raw data and multiple inference loops as they try to make sense of unstructured responses.\"Enterprises have the data, but it&#x27;s often stale, fragmented or locked in formats that AI can&#x27;t use effectively,\" Falconer explained. \"The real-time context engine solves this by unifying data processing, reprocessing and serving, turning continuous data streams into live context for smarter, faster and more reliable AI decisions.\"The technical architecture: Three layers for real-time agent contextConfluent&#x27;s platform encompasses three elements that work together or adopted separately.The real-time context engine is the managed data infrastructure layer on Confluent Cloud. Connectors pull data into Kafka topics as events occur. Flink jobs process these streams into \"derived datasets\" — materialized views joining historical and real-time signals. For customer support, this might combine account history, current session behavior and inventory status into one unified context object. The Engine exposes this through a managed MCP server.Streaming agents is Confluent&#x27;s proprietary framework for building AI agents that run natively on Flink. These agents monitor data streams and trigger automatically based on conditions — they don&#x27;t wait for prompts. The framework includes simplified agent definitions, built-in observability and native Claude integration from Anthropic. It&#x27;s available in open preview on Confluent&#x27;s platform.Flink Agents is the open-source framework developed with Alibaba Cloud, LinkedIn and Ververica. It brings event-driven agent capabilities directly to Apache Flink, allowing organizations to build streaming agents without committing to Confluent&#x27;s managed platform. They handle operational complexity themselves but avoid vendor lock-in.Competition heats up for agent-ready data infrastructureConfluent isn&#x27;t alone in recognizing that AI agents need different data infrastructure. The day before Confluent&#x27;s announcement, rival Redpanda introduced its own Agentic Data Plane — combining streaming, SQL and governance specifically for AI agents. Redpanda acquired Oxla&#x27;s distributed SQL engine to give agents standard SQL endpoints for querying data in motion or at rest. The platform emphasizes MCP-aware connectivity, full observability of agent interactions and what it calls \"agentic access control\" with fine-grained, short-lived tokens.The architectural approaches differ. Confluent emphasizes stream processing with Flink to create derived datasets optimized for agents. Redpanda emphasizes federated SQL querying across disparate sources. Both recognize agents need real-time context with governance and observability.Beyond direct streaming competitors, Databricks and Snowflake are fundamentally analytical platforms adding streaming capabilities. Their strength is complex queries over large datasets, with streaming as an enhancement. Confluent and Redpanda invert this: Streaming is the foundation, with analytical and AI workloads built on top of data in motion.How streaming context works in practiceAmong the users of Confluent&#x27;s system is transportation vendor Busie. The company is building a modern operating system for charter bus companies that helps them manage quotes, trips, payments and drivers in real time. \"Data streaming is what makes that possible,\" Louis Bookoff, Busie co-founder and CEO told VentureBeat. \"Using Confluent, we move data instantly between different parts of our system instead of waiting for overnight updates or batch reports. That keeps everything in sync and helps us ship new features faster.Bookoff noted that the same foundation is what will make gen AI valuable for his customers.\"In our case, every action like a quote sent or a driver assigned becomes an event that streams through the system immediately,\" Bookoff said. \"That live feed of information is what will let our AI tools respond in real time with low latency rather than just summarize what already happened.\"The challenge, however, is how to understand context. When thousands of live events flow through the system every minute, AI models need relevant, accurate data without getting overwhelmed. \"If the data isn&#x27;t grounded in what is happening in the real world, AI can easily make wrong assumptions and in turn take wrong actions,\" Bookoff said. \"Stream processing solves that by continuously validating and reconciling live data against activity in Busie.\"What this means for enterprise AI strategyStreaming context architecture signals a fundamental shift in how AI agents consume enterprise data. AI agents require continuous context that blends historical understanding with real-time awareness — they need to know what happened, what&#x27;s happening and what might happen next, all at once.For enterprises evaluating this approach, start by identifying use cases where data staleness breaks the agent. Fraud detection, anomaly investigation and real-time customer intervention fail with batch pipelines that refresh hourly or daily. If your agents need to act on events within seconds or minutes of them occurring, streaming context becomes necessary rather than optional.\"When you&#x27;re building applications on top of foundation models, because they&#x27;re inherently probabilistic, you use data and context to steer the model in a direction where you want to get some kind of outcome,\" Falconer said. \"The better you can do that, the more reliable and better the outcome.\"",
          "content": "Enterprise AI agents today face a fundamental timing problem: They can&#x27;t easily act on critical business events because they aren&#x27;t always aware of them in real-time.The challenge is infrastructure. Most enterprise data lives in databases fed by extract-transform-load (ETL) jobs that run hourly or daily — ultimately too slow for agents that must respond in real time.One potential way to tackle that challenge is to have agents directly interface with streaming data systems. Among the primary approaches in use today are the open source Apache Kafka and Apache Flink technologies. There are multiple commercial implementations based on those technologies, too, Confluent, which is led by the original creators behind Kafka, being one of them.Today, Confluent is introducing a real-time context engine designed to solve this latency problem. The technology builds on Apache Kafka, the distributed event streaming platform that captures data as events occur, and open-source Apache Flink, the stream processing engine that transforms those events in real time.The company is also releasing an open-source framework, Flink Agents, developed in collaboration with Alibaba Cloud, LinkedIn and Ververica. The framework brings event-driven AI agent capabilities directly to Apache Flink, allowing organizations to build agents that monitor data streams and trigger automatically based on conditions without committing to Confluent&#x27;s managed platform.\"Today, most enterprise AI systems can&#x27;t respond automatically to important events in a business without someone prompting them first,\" Sean Falconer, Confluent&#x27;s head of AI, told VentureBeat. \"This leads to lost revenue, unhappy customers or added risk when a payment fails or a network malfunctions.\"The significance extends beyond Confluent&#x27;s specific products. The industry is recognizing that AI agents require different data infrastructure than traditional applications. Agents don&#x27;t just retrieve information when asked. They need to observe continuous streams of business events and act automatically when conditions warrant. This requires streaming architecture, not batch pipelines.Batch versus streaming: Why RAG alone isn&#x27;t enoughTo understand the problem, it&#x27;s important to distinguish between the different approaches to moving data through enterprise systems and how they can connect to agentic AI.In batch processing, data accumulates in source systems until a scheduled job runs. That job extracts the data, transforms it and loads it into a target database or data warehouse. This might occur hourly, daily or even weekly. The approach works well for analytical workloads, but it creates latency between when something happens in the business and when systems can act on it.Data streaming inverts this model. Instead of waiting for scheduled jobs, streaming platforms like Apache Kafka capture events as they occur. Each database update, user action, transaction or sensor reading becomes an event published to a stream. Apache Flink then processes these streams to join, filter and aggregate data in real time. The result is processed data that reflects the current state of the business, updating continuously as new events arrive.This distinction becomes critical when you consider what kinds of context AI agents actually need. Much of the current enterprise AI discussion focuses on retrieval-augmented generation (RAG), which handles semantic search over knowledge bases to find relevant documentation, policies or historical information. RAG works well for questions like \"What&#x27;s our refund policy?\" where the answer exists in static documents.But many enterprise use cases require what Falconer calls \"structural context\" — precise, up-to-date information from multiple operational systems stitched together in real time. Consider a job recommendation agent that requires user profile data from the HR database, browsing behavior from the last hour, search queries from minutes ago and current open positions across multiple systems.\"The part that we&#x27;re unlocking for businesses is the ability to essentially serve that structural context needed to deliver the freshest version,\" Falconer said.The MCP connection problem: Stale data and fragmented contextThe challenge isn&#x27;t simply connecting AI to enterprise data. Model Context Protocol (MCP), introduced by Anthropic earlier this year, already standardized how agents access data sources. The problem is what happens after the connection is made.In most enterprise architectures today, AI agents connect via MCP to data lakes or warehouses fed by batch ETL pipelines. This creates two critical failures: The data is stale, reflecting yesterday&#x27;s reality rather than current events, and it&#x27;s fragmented across multiple systems, requiring significant preprocessing before an agent can reason about it effectively.The alternative — putting MCP servers directly in front of operational databases and APIs — creates different problems. Those endpoints weren&#x27;t designed for agent consumption, which can lead to high token costs as agents process excessive raw data and multiple inference loops as they try to make sense of unstructured responses.\"Enterprises have the data, but it&#x27;s often stale, fragmented or locked in formats that AI can&#x27;t use effectively,\" Falconer explained. \"The real-time context engine solves this by unifying data processing, reprocessing and serving, turning continuous data streams into live context for smarter, faster and more reliable AI decisions.\"The technical architecture: Three layers for real-time agent contextConfluent&#x27;s platform encompasses three elements that work together or adopted separately.The real-time context engine is the managed data infrastructure layer on Confluent Cloud. Connectors pull data into Kafka topics as events occur. Flink jobs process these streams into \"derived datasets\" — materialized views joining historical and real-time signals. For customer support, this might combine account history, current session behavior and inventory status into one unified context object. The Engine exposes this through a managed MCP server.Streaming agents is Confluent&#x27;s proprietary framework for building AI agents that run natively on Flink. These agents monitor data streams and trigger automatically based on conditions — they don&#x27;t wait for prompts. The framework includes simplified agent definitions, built-in observability and native Claude integration from Anthropic. It&#x27;s available in open preview on Confluent&#x27;s platform.Flink Agents is the open-source framework developed with Alibaba Cloud, LinkedIn and Ververica. It brings event-driven agent capabilities directly to Apache Flink, allowing organizations to build streaming agents without committing to Confluent&#x27;s managed platform. They handle operational complexity themselves but avoid vendor lock-in.Competition heats up for agent-ready data infrastructureConfluent isn&#x27;t alone in recognizing that AI agents need different data infrastructure. The day before Confluent&#x27;s announcement, rival Redpanda introduced its own Agentic Data Plane — combining streaming, SQL and governance specifically for AI agents. Redpanda acquired Oxla&#x27;s distributed SQL engine to give agents standard SQL endpoints for querying data in motion or at rest. The platform emphasizes MCP-aware connectivity, full observability of agent interactions and what it calls \"agentic access control\" with fine-grained, short-lived tokens.The architectural approaches differ. Confluent emphasizes stream processing with Flink to create derived datasets optimized for agents. Redpanda emphasizes federated SQL querying across disparate sources. Both recognize agents need real-time context with governance and observability.Beyond direct streaming competitors, Databricks and Snowflake are fundamentally analytical platforms adding streaming capabilities. Their strength is complex queries over large datasets, with streaming as an enhancement. Confluent and Redpanda invert this: Streaming is the foundation, with analytical and AI workloads built on top of data in motion.How streaming context works in practiceAmong the users of Confluent&#x27;s system is transportation vendor Busie. The company is building a modern operating system for charter bus companies that helps them manage quotes, trips, payments and drivers in real time. \"Data streaming is what makes that possible,\" Louis Bookoff, Busie co-founder and CEO told VentureBeat. \"Using Confluent, we move data instantly between different parts of our system instead of waiting for overnight updates or batch reports. That keeps everything in sync and helps us ship new features faster.Bookoff noted that the same foundation is what will make gen AI valuable for his customers.\"In our case, every action like a quote sent or a driver assigned becomes an event that streams through the system immediately,\" Bookoff said. \"That live feed of information is what will let our AI tools respond in real time with low latency rather than just summarize what already happened.\"The challenge, however, is how to understand context. When thousands of live events flow through the system every minute, AI models need relevant, accurate data without getting overwhelmed. \"If the data isn&#x27;t grounded in what is happening in the real world, AI can easily make wrong assumptions and in turn take wrong actions,\" Bookoff said. \"Stream processing solves that by continuously validating and reconciling live data against activity in Busie.\"What this means for enterprise AI strategyStreaming context architecture signals a fundamental shift in how AI agents consume enterprise data. AI agents require continuous context that blends historical understanding with real-time awareness — they need to know what happened, what&#x27;s happening and what might happen next, all at once.For enterprises evaluating this approach, start by identifying use cases where data staleness breaks the agent. Fraud detection, anomaly investigation and real-time customer intervention fail with batch pipelines that refresh hourly or daily. If your agents need to act on events within seconds or minutes of them occurring, streaming context becomes necessary rather than optional.\"When you&#x27;re building applications on top of foundation models, because they&#x27;re inherently probabilistic, you use data and context to steer the model in a direction where you want to get some kind of outcome,\" Falconer said. \"The better you can do that, the more reliable and better the outcome.\"",
          "feed_position": 3,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/sMKzhGAOWD3jIgUIXU3sF/dc476ec50bb21f290514114b8465106b/data_streaming_to_AI-smk.jpg?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/geostar-pioneers-geo-as-traditional-seo-faces-25-decline-from-ai-chatbots",
          "published_at": "Wed, 29 Oct 2025 07:00:00 GMT",
          "title": "Geostar pioneers GEO as traditional SEO faces 25% decline from AI chatbots, Gartner says",
          "standfirst": "The moment Mack McConnell knew everything about search had changed came last summer at the Paris Olympics. His parents, independently and without prompting, had both turned to ChatGPT to plan their day&#x27;s activities in the French capital. The AI recommended specific tour companies, restaurants, and attractions — businesses that had won a new kind of visibility lottery.\"It was almost like this intuitive interface that older people were as comfortable with using as younger people,\" McConnell recalled in an exclusive interview with VentureBeat. \"I could just see the businesses were now being recommended.\"That observation has now become the foundation of Geostar, a Pear VC-backed startup that&#x27;s racing to help businesses navigate what may be the most significant shift in online discovery since Google&#x27;s founding. The company, which recently emerged from stealth with impressive early customer traction, is betting that the rise of AI-powered search represents a significant opportunity to reinvent how companies get found online. The global AI search engine market alone is projected to grow from $43.63 billion in 2025 to $108.88 billion by 2032.Already the fastest-growing company in PearX&#x27;s latest cohort, Geostar is fast approaching $1 million in annual recurring revenue in just four months — with only two founders and no employees.Why Gartner predicts traditional search volume will decline 25% by 2026The numbers tell a stark story of disruption. Gartner predicts that traditional search engine volume will decline by 25% by 2026, largely due to the rise of AI chatbots. Google&#x27;s AI Overviews now appear on billions of searches monthly. Princeton University researchers have found that optimizing for these new AI systems can increase visibility by up to 40%.\"Search used to mean that you had to make Google happy,\" McConnell explained. \"But now you have to optimize for four different Google interfaces — traditional search, AI Mode, Gemini, and AI Overviews — each with different criteria. And then ChatGPT, Claude, and Perplexity each work differently on top of that.\"This fragmentation is creating chaos for businesses that have spent decades perfecting their Google search strategies. A recent Forrester study found that 95% of B2B buyers plan to use generative AI in future purchase decisions. Yet most companies remain woefully unprepared for this shift.\"Anybody who&#x27;s not on this right now is losing out,\" said Cihan Tas, Geostar&#x27;s co-founder and chief technology officer. \"We see lawyers getting 50% of their clients through ChatGPT now. It&#x27;s just such a massive shift.\"How language models read the web differently than search engines ever didWhat Geostar and a growing cohort of competitors call Generative Engine Optimization or GEO represents a fundamental departure from traditional search engine optimization. Where SEO focused primarily on keywords and backlinks, GEO requires understanding how large language models parse, understand, and synthesize information across the entire web.The technical challenges are formidable. Every website must now function as what Tas calls \"its own little database\" capable of being understood by dozens of different AI crawlers, each with unique requirements and preferences. Google&#x27;s systems pull from their existing search index. ChatGPT relies heavily on structured data and specific content formats. Perplexity shows a marked preference for Wikipedia and authoritative sources.\"Now the strategy is actually being concise, clear, and answering the question, because that&#x27;s directly what the AI is looking for,\" Tas explained. \"You&#x27;re actually tuning for somewhat of an intelligent model that makes decisions similarly to how we make decisions.\"Consider schema markup, the structured data that helps machines understand web content. While only 30% of websites currently implement comprehensive schema, research shows that pages with proper markup are 36% more likely to appear in AI-generated summaries. Yet most businesses don&#x27;t even know what schema markup is, let alone how to implement it effectively.Inside Geostar&#x27;s AI agents that optimize websites continuously without human interventionGeostar&#x27;s solution embodies a broader trend in enterprise software: the rise of autonomous AI agents that can take action on behalf of businesses. The company embeds what it calls \"ambient agents\" directly into client websites, continuously optimizing content, technical configurations, and even creating new pages based on patterns learned across its entire customer base.\"Once we learn something about the way content performs, or the way a technical optimization performs, we can then syndicate that same change across the remaining users so everyone in the network benefits,\" McConnell said.For RedSift, a cybersecurity company, this approach yielded a 27% increase in AI mentions within three months. In one case, Geostar identified an opportunity to rank for \"best DMARC vendors,\" a high-value search term in the email security space. The company&#x27;s agents created and optimized content that achieved first-page rankings on both Google and ChatGPT within four days.\"We&#x27;re doing the work of an agency that charges $10,000 a month,\" McConnell said, noting that Geostar&#x27;s pricing ranges from $1,000 to $3,000 monthly. \"AI creates a situation where, for the first time ever, you can take action like an agency, but you can scale like software.\"Why brand mentions without links now matter more than ever in the AI eraThe implications of this shift extend far beyond technical optimizations. In the SEO era, a mention without a link was essentially worthless. In the age of AI, that calculus has reversed. AI systems can analyze vast amounts of text to understand sentiment and context, meaning that brand mentions on Reddit, in news articles, or across social media now directly influence how AI systems describe and recommend companies.\"If the New York Times mentions a company without linking to it, that company would actually benefit from that in an AI system,\" McConnell explained. \"AI has the ability to do mass analysis of huge amounts of text, and it will understand the sentiment around that mention.\"This has created new vulnerabilities. Research from the Indian Institute of Technology and Princeton found that AI systems show systematic bias toward third-party sources over brand-owned content. A company&#x27;s own website might be less influential in shaping AI perceptions than what others say about it online.The shifting landscape has also disrupted traditional metrics of success. Where SEO focused on rankings and click-through rates, GEO must account for what researchers call impression metrics — how prominently and positively a brand appears within AI-generated responses, even when users never click through to the source.A growing market as SEO veterans and new players rush to dominate AI optimizationGeostar is hardly alone in recognizing this opportunity. Companies like Brandlight, Profound, and Goodie are all racing to help businesses navigate the new landscape. The SEO industry, worth approximately $80 billion globally, is scrambling to adapt, with established players like Semrush and Ahrefs rushing to add AI visibility tracking features.But the company&#x27;s founders, who previously built and sold a Y-Combinator-backed e-commerce optimization startup called Monto, believe their technical approach gives them an edge. Unlike competitors who largely provide dashboards and recommendations, Geostar&#x27;s agents actively implement changes.\"Everyone is taking the same solutions that worked in the last era and just saying, &#x27;We&#x27;ll do this for AI instead,&#x27;\" McConnell argued. \"But when you think about what AI is truly capable of, it can actually do the work for you.\"The stakes are particularly high for small and medium-sized businesses. While large corporations can afford to hire specialized consultants or build internal expertise, smaller companies risk becoming invisible in AI-mediated search. Geostar sees this as its primary market opportunity: nearly half of the 33.2 million small businesses in America invest in SEO. Among the roughly 418,000 law firms in the U.S., many spend between $2,500 and $5,000 monthly on search optimization to stay competitive in local markets.From Kurdish village to PearX: The unlikely partnership building the future of searchFor Tas, whose journey to Silicon Valley began in a tiny Kurdish village in Turkey with just 50 residents, the current moment represents both opportunity and responsibility. His mother&#x27;s battle with cancer prevented him from finishing college, leading him to teach himself programming and eventually partner with McConnell — whom he worked with for an entire year before they ever met in person.\"We&#x27;re not just copy and pasting a solution that was existing before,\" Tas emphasized. \"This is something that&#x27;s different and was uniquely possible today.\"Looking forward, the transformation of search appears to be accelerating rather than stabilizing. Industry observers predict that search functionality will soon be embedded in productivity tools, wearables, and even augmented reality interfaces. Each new surface will likely have its own optimization requirements, further complicating the landscape.\"Soon, search will be in our eyes, in our ears,\" McConnell predicted. \"When Siri breaks out of her prison, whatever that Jony Ive and OpenAI are building together will be like a multimodal search interface.\"The technical challenges are matched by ethical ones. As businesses scramble to influence AI recommendations, questions arise about manipulation, fairness, and transparency. There&#x27;s currently no oversight body or established best practices for GEO, creating what some critics describe as a Wild West environment.As businesses grapple with these changes, one thing seems certain: the era of simply optimizing for Google is over. In its place is emerging a far more complex ecosystem where success requires understanding not just how machines index information, but how they think about it, synthesize it, and ultimately decide what to recommend to humans seeking answers.For the millions of businesses whose survival depends on being discovered online, mastering this new paradigm isn&#x27;t just an opportunity — it&#x27;s an existential imperative. The question is no longer whether to optimize for AI search, but whether companies can adapt quickly enough to remain visible as the pace of change accelerates.McConnell&#x27;s parents at the Olympics were a preview of what&#x27;s already becoming the norm. They didn&#x27;t search for tour companies in Paris. They didn&#x27;t scroll through results or click on links. They simply asked ChatGPT what to do — and the AI decided which businesses deserved their attention.In the new economy of discovery, the businesses that win won&#x27;t be the ones that rank highest. They&#x27;ll be the ones AI chooses to recommend.",
          "content": "The moment Mack McConnell knew everything about search had changed came last summer at the Paris Olympics. His parents, independently and without prompting, had both turned to ChatGPT to plan their day&#x27;s activities in the French capital. The AI recommended specific tour companies, restaurants, and attractions — businesses that had won a new kind of visibility lottery.\"It was almost like this intuitive interface that older people were as comfortable with using as younger people,\" McConnell recalled in an exclusive interview with VentureBeat. \"I could just see the businesses were now being recommended.\"That observation has now become the foundation of Geostar, a Pear VC-backed startup that&#x27;s racing to help businesses navigate what may be the most significant shift in online discovery since Google&#x27;s founding. The company, which recently emerged from stealth with impressive early customer traction, is betting that the rise of AI-powered search represents a significant opportunity to reinvent how companies get found online. The global AI search engine market alone is projected to grow from $43.63 billion in 2025 to $108.88 billion by 2032.Already the fastest-growing company in PearX&#x27;s latest cohort, Geostar is fast approaching $1 million in annual recurring revenue in just four months — with only two founders and no employees.Why Gartner predicts traditional search volume will decline 25% by 2026The numbers tell a stark story of disruption. Gartner predicts that traditional search engine volume will decline by 25% by 2026, largely due to the rise of AI chatbots. Google&#x27;s AI Overviews now appear on billions of searches monthly. Princeton University researchers have found that optimizing for these new AI systems can increase visibility by up to 40%.\"Search used to mean that you had to make Google happy,\" McConnell explained. \"But now you have to optimize for four different Google interfaces — traditional search, AI Mode, Gemini, and AI Overviews — each with different criteria. And then ChatGPT, Claude, and Perplexity each work differently on top of that.\"This fragmentation is creating chaos for businesses that have spent decades perfecting their Google search strategies. A recent Forrester study found that 95% of B2B buyers plan to use generative AI in future purchase decisions. Yet most companies remain woefully unprepared for this shift.\"Anybody who&#x27;s not on this right now is losing out,\" said Cihan Tas, Geostar&#x27;s co-founder and chief technology officer. \"We see lawyers getting 50% of their clients through ChatGPT now. It&#x27;s just such a massive shift.\"How language models read the web differently than search engines ever didWhat Geostar and a growing cohort of competitors call Generative Engine Optimization or GEO represents a fundamental departure from traditional search engine optimization. Where SEO focused primarily on keywords and backlinks, GEO requires understanding how large language models parse, understand, and synthesize information across the entire web.The technical challenges are formidable. Every website must now function as what Tas calls \"its own little database\" capable of being understood by dozens of different AI crawlers, each with unique requirements and preferences. Google&#x27;s systems pull from their existing search index. ChatGPT relies heavily on structured data and specific content formats. Perplexity shows a marked preference for Wikipedia and authoritative sources.\"Now the strategy is actually being concise, clear, and answering the question, because that&#x27;s directly what the AI is looking for,\" Tas explained. \"You&#x27;re actually tuning for somewhat of an intelligent model that makes decisions similarly to how we make decisions.\"Consider schema markup, the structured data that helps machines understand web content. While only 30% of websites currently implement comprehensive schema, research shows that pages with proper markup are 36% more likely to appear in AI-generated summaries. Yet most businesses don&#x27;t even know what schema markup is, let alone how to implement it effectively.Inside Geostar&#x27;s AI agents that optimize websites continuously without human interventionGeostar&#x27;s solution embodies a broader trend in enterprise software: the rise of autonomous AI agents that can take action on behalf of businesses. The company embeds what it calls \"ambient agents\" directly into client websites, continuously optimizing content, technical configurations, and even creating new pages based on patterns learned across its entire customer base.\"Once we learn something about the way content performs, or the way a technical optimization performs, we can then syndicate that same change across the remaining users so everyone in the network benefits,\" McConnell said.For RedSift, a cybersecurity company, this approach yielded a 27% increase in AI mentions within three months. In one case, Geostar identified an opportunity to rank for \"best DMARC vendors,\" a high-value search term in the email security space. The company&#x27;s agents created and optimized content that achieved first-page rankings on both Google and ChatGPT within four days.\"We&#x27;re doing the work of an agency that charges $10,000 a month,\" McConnell said, noting that Geostar&#x27;s pricing ranges from $1,000 to $3,000 monthly. \"AI creates a situation where, for the first time ever, you can take action like an agency, but you can scale like software.\"Why brand mentions without links now matter more than ever in the AI eraThe implications of this shift extend far beyond technical optimizations. In the SEO era, a mention without a link was essentially worthless. In the age of AI, that calculus has reversed. AI systems can analyze vast amounts of text to understand sentiment and context, meaning that brand mentions on Reddit, in news articles, or across social media now directly influence how AI systems describe and recommend companies.\"If the New York Times mentions a company without linking to it, that company would actually benefit from that in an AI system,\" McConnell explained. \"AI has the ability to do mass analysis of huge amounts of text, and it will understand the sentiment around that mention.\"This has created new vulnerabilities. Research from the Indian Institute of Technology and Princeton found that AI systems show systematic bias toward third-party sources over brand-owned content. A company&#x27;s own website might be less influential in shaping AI perceptions than what others say about it online.The shifting landscape has also disrupted traditional metrics of success. Where SEO focused on rankings and click-through rates, GEO must account for what researchers call impression metrics — how prominently and positively a brand appears within AI-generated responses, even when users never click through to the source.A growing market as SEO veterans and new players rush to dominate AI optimizationGeostar is hardly alone in recognizing this opportunity. Companies like Brandlight, Profound, and Goodie are all racing to help businesses navigate the new landscape. The SEO industry, worth approximately $80 billion globally, is scrambling to adapt, with established players like Semrush and Ahrefs rushing to add AI visibility tracking features.But the company&#x27;s founders, who previously built and sold a Y-Combinator-backed e-commerce optimization startup called Monto, believe their technical approach gives them an edge. Unlike competitors who largely provide dashboards and recommendations, Geostar&#x27;s agents actively implement changes.\"Everyone is taking the same solutions that worked in the last era and just saying, &#x27;We&#x27;ll do this for AI instead,&#x27;\" McConnell argued. \"But when you think about what AI is truly capable of, it can actually do the work for you.\"The stakes are particularly high for small and medium-sized businesses. While large corporations can afford to hire specialized consultants or build internal expertise, smaller companies risk becoming invisible in AI-mediated search. Geostar sees this as its primary market opportunity: nearly half of the 33.2 million small businesses in America invest in SEO. Among the roughly 418,000 law firms in the U.S., many spend between $2,500 and $5,000 monthly on search optimization to stay competitive in local markets.From Kurdish village to PearX: The unlikely partnership building the future of searchFor Tas, whose journey to Silicon Valley began in a tiny Kurdish village in Turkey with just 50 residents, the current moment represents both opportunity and responsibility. His mother&#x27;s battle with cancer prevented him from finishing college, leading him to teach himself programming and eventually partner with McConnell — whom he worked with for an entire year before they ever met in person.\"We&#x27;re not just copy and pasting a solution that was existing before,\" Tas emphasized. \"This is something that&#x27;s different and was uniquely possible today.\"Looking forward, the transformation of search appears to be accelerating rather than stabilizing. Industry observers predict that search functionality will soon be embedded in productivity tools, wearables, and even augmented reality interfaces. Each new surface will likely have its own optimization requirements, further complicating the landscape.\"Soon, search will be in our eyes, in our ears,\" McConnell predicted. \"When Siri breaks out of her prison, whatever that Jony Ive and OpenAI are building together will be like a multimodal search interface.\"The technical challenges are matched by ethical ones. As businesses scramble to influence AI recommendations, questions arise about manipulation, fairness, and transparency. There&#x27;s currently no oversight body or established best practices for GEO, creating what some critics describe as a Wild West environment.As businesses grapple with these changes, one thing seems certain: the era of simply optimizing for Google is over. In its place is emerging a far more complex ecosystem where success requires understanding not just how machines index information, but how they think about it, synthesize it, and ultimately decide what to recommend to humans seeking answers.For the millions of businesses whose survival depends on being discovered online, mastering this new paradigm isn&#x27;t just an opportunity — it&#x27;s an existential imperative. The question is no longer whether to optimize for AI search, but whether companies can adapt quickly enough to remain visible as the pace of change accelerates.McConnell&#x27;s parents at the Olympics were a preview of what&#x27;s already becoming the norm. They didn&#x27;t search for tour companies in Paris. They didn&#x27;t scroll through results or click on links. They simply asked ChatGPT what to do — and the AI decided which businesses deserved their attention.In the new economy of discovery, the businesses that win won&#x27;t be the ones that rank highest. They&#x27;ll be the ones AI chooses to recommend.",
          "feed_position": 4,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/5JXMNBhUMiSimHDDeX6ODt/2793b1842c2ef9825b7127d7bbd65f4f/_Geostar-2.jpg?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/security/securitys-ai-dilemma-moving-faster-while-risking-more",
          "published_at": "Wed, 29 Oct 2025 04:00:00 GMT",
          "title": "Security's AI dilemma: Moving faster while risking more",
          "standfirst": "Presented by Splunk, a Cisco CompanyAs AI rapidly evolves from a theoretical promise to an operational reality, CISOs and CIOs face a fundamental challenge: how to harness AI&#x27;s transformative potential while maintaining the human oversight and strategic thinking that security demands. The rise of agentic AI is reshaping security operations, but success requires balancing automation with accountability.The efficiency paradox: Automation without abdicationThe pressure to adopt AI is intense. Organizations are being pushed to reduce headcount or redirect resources toward AI-driven initiatives, often without fully understanding what that transformation entails. The promise is compelling: AI can reduce investigation times from 60 minutes to just 5 minutes, potentially delivering 10x productivity improvements for security analysts.However, the critical question isn&#x27;t whether AI can automate tasks — it&#x27;s which tasks should be automated and where human judgment remains irreplaceable. The answer lies in understanding that AI excels at accelerating investigative workflows, but remediation and response actions still require human validation. Taking a system offline or quarantining an endpoint can have massive business impact. An AI making that call autonomously could inadvertently cause the very disruption it&#x27;s meant to prevent.The goal isn&#x27;t to replace security analysts but to free them for higher-value work. With routine alert triage automated, analysts can focus on red team/blue team exercises, collaborate with engineering teams on remediation, and engage in proactive threat hunting. There&#x27;s no shortage of security problems to solve — there&#x27;s a shortage of security experts to address them strategically.The trust deficit: Showing your workWhile confidence in AI&#x27;s ability to improve efficiency is high, skepticism about the quality of AI-driven decisions remains significant. Security teams need more than just AI-generated conclusions — they need transparency into how those conclusions were reached.When AI determines an alert is benign and closes it, SOC analysts need to understand the investigative steps that led to that determination. What data was examined? What patterns were identified? What alternative explanations were considered and ruled out?This transparency builds trust in AI recommendations, enables validation of AI logic, and creates opportunities for continuous improvement. Most importantly, it maintains the critical human-in-the-loop for complex judgment calls that require nuanced understanding of business context, compliance requirements, and potential cascading impacts.The future likely involves a hybrid model where autonomous capabilities are integrated into guided workflows and playbooks, with analysts remaining involved in complex decisions. The adversarial advantage: Fighting AI with AI — carefullyAI presents a dual-edged sword in security. While we&#x27;re carefully implementing AI with appropriate guardrails, adversaries face no such constraints. AI lowers the barrier to entry for attackers, enabling rapid exploit development and vulnerability discovery at scale. What was once the domain of sophisticated threat actors could soon be accessible to script kiddies armed with AI tools.The asymmetry is striking: defenders must be thoughtful and risk-averse, while attackers can experiment freely. If we make a mistake implementing autonomous security responses, we risk taking down production systems. If an attacker&#x27;s AI-driven exploit fails, they simply try again with no consequences.This creates an imperative to use AI defensively, but with appropriate caution. We must learn from attackers&#x27; techniques while maintaining the guardrails that prevent our AI from becoming the vulnerability. The recent emergence of malicious MCP (Model Context Protocol) supply chain attacks demonstrates how quickly adversaries exploit new AI infrastructure. The skills dilemma: Building capabilities while maintaining core competenciesAs AI handles more routine investigative work, a concerning question emerges: will security professionals&#x27; fundamental skills atrophy over time? This isn&#x27;t an argument against AI adoption — it&#x27;s a call for intentional skill development strategies. Organizations must balance AI-enabled efficiency with programs that maintain core competencies. This includes regular exercises that require manual investigation, cross-training that deepens understanding of underlying systems, and career paths that evolve roles rather than eliminate them.The responsibility is shared. Employers must provide tools, training, and culture that enable AI to augment rather than replace human expertise. Employees must actively engage in continuous learning, treating AI as a collaborative partner rather than a replacement for critical thinking.The identity crisis: Governing the agent explosionPerhaps the most underestimated challenge ahead is identity and access management in an agentic AI world. IDC estimates 1.3 billion agents by 2028 — each requiring identity, permissions, and governance. The complexity compounds exponentially.Overly permissive agents represent significant risk. An agent with broad administrative access could be socially engineered into taking destructive actions, approving fraudulent transactions, or exfiltrating sensitive data. The technical shortcuts engineers take to \"just make it work\" — granting excessive permissions to expedite deployment — create vulnerabilities that adversaries will exploit.Tool-based access control offers one path forward, granting agents only the specific capabilities they need. But governance frameworks must also address how LLMs themselves might learn and retain authentication information, potentially enabling impersonation attacks that bypass traditional access controls.The path forward: Start with compliance and reportingAmid these challenges, one area offers immediate, high-impact opportunity: continuous compliance and risk reporting. AI&#x27;s ability to consume vast amounts of documentation, interpret complex requirements, and generate concise summaries makes it ideal for compliance and reporting work that has traditionally consumed enormous analysts’ time. This represents a low-risk, high-value entry point for AI in security operations. The data foundation: Enabling the AI-powered SOCNone of these AI capabilities can succeed without addressing the fundamental data challenges facing security operations. SOC teams struggle with siloed data and disparate tools. Success requires a deliberate data strategy that prioritizes accessibility, quality, and unified data contexts. Security-relevant data must be immediately available to AI agents without friction, properly governed to ensure reliability, and enriched with metadata that provides the business context AI cannot understand. Closing thought: Innovation with intentionalityThe autonomous SOC is emerging — not as a light switch to flip, but as an evolutionary journey requiring continuous adaptation. Success demands that we embrace AI&#x27;s efficiency gains while maintaining the human judgment, strategic thinking, and ethical oversight that security requires.We&#x27;re not replacing security teams with AI. We&#x27;re building collaborative, multi-agent systems where human expertise guides AI capabilities toward outcomes that neither could achieve alone. That&#x27;s the promise of the agentic AI era — if we&#x27;re intentional about how we get there.Tanya Faddoul, VP Product, Customer Strategy and Chief of Staff for Splunk, a Cisco Company. Michael Fanning is Chief Information Security Officer for Splunk, a Cisco Company. Cisco Data Fabric provides the needed data architecture powered by Splunk Platform — unified data fabric, federated search capabilities, comprehensive metadata management — to unlock AI and SOC’s full potential. Learn more about Cisco Data Fabric.Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.",
          "content": "Presented by Splunk, a Cisco CompanyAs AI rapidly evolves from a theoretical promise to an operational reality, CISOs and CIOs face a fundamental challenge: how to harness AI&#x27;s transformative potential while maintaining the human oversight and strategic thinking that security demands. The rise of agentic AI is reshaping security operations, but success requires balancing automation with accountability.The efficiency paradox: Automation without abdicationThe pressure to adopt AI is intense. Organizations are being pushed to reduce headcount or redirect resources toward AI-driven initiatives, often without fully understanding what that transformation entails. The promise is compelling: AI can reduce investigation times from 60 minutes to just 5 minutes, potentially delivering 10x productivity improvements for security analysts.However, the critical question isn&#x27;t whether AI can automate tasks — it&#x27;s which tasks should be automated and where human judgment remains irreplaceable. The answer lies in understanding that AI excels at accelerating investigative workflows, but remediation and response actions still require human validation. Taking a system offline or quarantining an endpoint can have massive business impact. An AI making that call autonomously could inadvertently cause the very disruption it&#x27;s meant to prevent.The goal isn&#x27;t to replace security analysts but to free them for higher-value work. With routine alert triage automated, analysts can focus on red team/blue team exercises, collaborate with engineering teams on remediation, and engage in proactive threat hunting. There&#x27;s no shortage of security problems to solve — there&#x27;s a shortage of security experts to address them strategically.The trust deficit: Showing your workWhile confidence in AI&#x27;s ability to improve efficiency is high, skepticism about the quality of AI-driven decisions remains significant. Security teams need more than just AI-generated conclusions — they need transparency into how those conclusions were reached.When AI determines an alert is benign and closes it, SOC analysts need to understand the investigative steps that led to that determination. What data was examined? What patterns were identified? What alternative explanations were considered and ruled out?This transparency builds trust in AI recommendations, enables validation of AI logic, and creates opportunities for continuous improvement. Most importantly, it maintains the critical human-in-the-loop for complex judgment calls that require nuanced understanding of business context, compliance requirements, and potential cascading impacts.The future likely involves a hybrid model where autonomous capabilities are integrated into guided workflows and playbooks, with analysts remaining involved in complex decisions. The adversarial advantage: Fighting AI with AI — carefullyAI presents a dual-edged sword in security. While we&#x27;re carefully implementing AI with appropriate guardrails, adversaries face no such constraints. AI lowers the barrier to entry for attackers, enabling rapid exploit development and vulnerability discovery at scale. What was once the domain of sophisticated threat actors could soon be accessible to script kiddies armed with AI tools.The asymmetry is striking: defenders must be thoughtful and risk-averse, while attackers can experiment freely. If we make a mistake implementing autonomous security responses, we risk taking down production systems. If an attacker&#x27;s AI-driven exploit fails, they simply try again with no consequences.This creates an imperative to use AI defensively, but with appropriate caution. We must learn from attackers&#x27; techniques while maintaining the guardrails that prevent our AI from becoming the vulnerability. The recent emergence of malicious MCP (Model Context Protocol) supply chain attacks demonstrates how quickly adversaries exploit new AI infrastructure. The skills dilemma: Building capabilities while maintaining core competenciesAs AI handles more routine investigative work, a concerning question emerges: will security professionals&#x27; fundamental skills atrophy over time? This isn&#x27;t an argument against AI adoption — it&#x27;s a call for intentional skill development strategies. Organizations must balance AI-enabled efficiency with programs that maintain core competencies. This includes regular exercises that require manual investigation, cross-training that deepens understanding of underlying systems, and career paths that evolve roles rather than eliminate them.The responsibility is shared. Employers must provide tools, training, and culture that enable AI to augment rather than replace human expertise. Employees must actively engage in continuous learning, treating AI as a collaborative partner rather than a replacement for critical thinking.The identity crisis: Governing the agent explosionPerhaps the most underestimated challenge ahead is identity and access management in an agentic AI world. IDC estimates 1.3 billion agents by 2028 — each requiring identity, permissions, and governance. The complexity compounds exponentially.Overly permissive agents represent significant risk. An agent with broad administrative access could be socially engineered into taking destructive actions, approving fraudulent transactions, or exfiltrating sensitive data. The technical shortcuts engineers take to \"just make it work\" — granting excessive permissions to expedite deployment — create vulnerabilities that adversaries will exploit.Tool-based access control offers one path forward, granting agents only the specific capabilities they need. But governance frameworks must also address how LLMs themselves might learn and retain authentication information, potentially enabling impersonation attacks that bypass traditional access controls.The path forward: Start with compliance and reportingAmid these challenges, one area offers immediate, high-impact opportunity: continuous compliance and risk reporting. AI&#x27;s ability to consume vast amounts of documentation, interpret complex requirements, and generate concise summaries makes it ideal for compliance and reporting work that has traditionally consumed enormous analysts’ time. This represents a low-risk, high-value entry point for AI in security operations. The data foundation: Enabling the AI-powered SOCNone of these AI capabilities can succeed without addressing the fundamental data challenges facing security operations. SOC teams struggle with siloed data and disparate tools. Success requires a deliberate data strategy that prioritizes accessibility, quality, and unified data contexts. Security-relevant data must be immediately available to AI agents without friction, properly governed to ensure reliability, and enriched with metadata that provides the business context AI cannot understand. Closing thought: Innovation with intentionalityThe autonomous SOC is emerging — not as a light switch to flip, but as an evolutionary journey requiring continuous adaptation. Success demands that we embrace AI&#x27;s efficiency gains while maintaining the human judgment, strategic thinking, and ethical oversight that security requires.We&#x27;re not replacing security teams with AI. We&#x27;re building collaborative, multi-agent systems where human expertise guides AI capabilities toward outcomes that neither could achieve alone. That&#x27;s the promise of the agentic AI era — if we&#x27;re intentional about how we get there.Tanya Faddoul, VP Product, Customer Strategy and Chief of Staff for Splunk, a Cisco Company. Michael Fanning is Chief Information Security Officer for Splunk, a Cisco Company. Cisco Data Fabric provides the needed data architecture powered by Splunk Platform — unified data fabric, federated search capabilities, comprehensive metadata management — to unlock AI and SOC’s full potential. Learn more about Cisco Data Fabric.Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.",
          "feed_position": 5,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/1xzoYYdBHblQvb1NzXRGSt/8898cd969ec3062a752682e7dae6a437/AdobeStock_1081293355.jpeg?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/from-static-classifiers-to-reasoning-engines-openais-new-model-rethinks",
          "published_at": "Wed, 29 Oct 2025 04:00:00 GMT",
          "title": "From static classifiers to reasoning engines: OpenAI’s new model rethinks content moderation",
          "standfirst": "Enterprises, eager to ensure any AI models they use adhere to safety and safe-use policies, fine-tune LLMs so they do not respond to unwanted queries. However, much of the safeguarding and red teaming happens before deployment, “baking in” policies before users fully test the models’ capabilities in production. OpenAI believes it can offer a more flexible option for enterprises and encourage more companies to bring in safety policies. The company has released two open-weight models under research preview that it believes will make enterprises and models more flexible in terms of safeguards. gpt-oss-safeguard-120b and gpt-oss-safeguard-20b will be available on a permissive Apache 2.0 license. The models are fine-tuned versions of OpenAI’s open-source gpt-oss, released in August, marking the first release in the oss family since the summer.In a blog post, OpenAI said oss-safeguard uses reasoning “to directly interpret a developer-provider policy at inference time — classifying user messages, completions and full chats according to the developer’s needs.”The company explained that, since the model uses a chain-of-thought (CoT), developers can get explanations of the model&#x27;s decisions for review. “Additionally, the policy is provided during inference, rather than being trained into the model, so it is easy for developers to iteratively revise policies to increase performance,\" OpenAI said in its post. \"This approach, which we initially developed for internal use, is significantly more flexible than the traditional method of training a classifier to indirectly infer a decision boundary from a large number of labeled examples.\" Developers can download both models from Hugging Face. Flexibility versus baking inAt the onset, AI models will not know a company’s preferred safety triggers. While model providers do red-team models and platforms, these safeguards are intended for broader use. Companies like Microsoft and Amazon Web Services even offer platforms to bring guardrails to AI applications and agents. Enterprises use safety classifiers to help train a model to recognize patterns of good or bad inputs. This helps the models learn which queries they shouldn’t reply to. It also helps ensure that the models do not drift and answer accurately.“Traditional classifiers can have high performance, with low latency and operating cost,\" OpenAI said. \"But gathering a sufficient quantity of training examples can be time-consuming and costly, and updating or changing the policy requires re-training the classifier.\"The models takes in two inputs at once before it outputs a conclusion on where the content fails. It takes a policy and the content to classify under its guidelines. OpenAI said the models work best in situations where: The potential harm is emerging or evolving, and policies need to adapt quickly.The domain is highly nuanced and difficult for smaller classifiers to handle.Developers don’t have enough samples to train a high-quality classifier for each risk on their platform.Latency is less important than producing high-quality, explainable labels.The company said gpt-oss-safeguard “is different because its reasoning capabilities allow developers to apply any policy,” even ones they’ve written during inference. The models are based on OpenAI’s internal tool, the Safety Reasoner, which enables its teams to be more iterative in setting guardrails. They often begin with very strict safety policies, “and use relatively large amounts of compute where needed,” then adjust policies as they move the model through production and risk assessments change. Performing safetyOpenAI said the gpt-oss-safeguard models outperformed its GPT-5-thinking and the original gpt-oss models on multipolicy accuracy based on benchmark testing. It also ran the models on the ToxicChat public benchmark, where they performed well, although GPT-5-thinking and the Safety Reasoner slightly edged them out.But there is concern that this approach could bring a centralization of safety standards.“Safety is not a well-defined concept. Any implementation of safety standards will reflect the values and priorities of the organization that creates it, as well as the limits and deficiencies of its models,” said John Thickstun, an assistant professor of computer science at Cornell University. “If industry as a whole adopts standards developed by OpenAI, we risk institutionalizing one particular perspective on safety and short-circuiting broader investigations into the safety needs for AI deployments across many sectors of society.”It should also be noted that OpenAI did not release the base model for the oss family of models, so developers cannot fully iterate on them. OpenAI, however, is confident that the developer community can help refine gpt-oss-safeguard. It will host a Hackathon on December 8 in San Francisco.",
          "content": "Enterprises, eager to ensure any AI models they use adhere to safety and safe-use policies, fine-tune LLMs so they do not respond to unwanted queries. However, much of the safeguarding and red teaming happens before deployment, “baking in” policies before users fully test the models’ capabilities in production. OpenAI believes it can offer a more flexible option for enterprises and encourage more companies to bring in safety policies. The company has released two open-weight models under research preview that it believes will make enterprises and models more flexible in terms of safeguards. gpt-oss-safeguard-120b and gpt-oss-safeguard-20b will be available on a permissive Apache 2.0 license. The models are fine-tuned versions of OpenAI’s open-source gpt-oss, released in August, marking the first release in the oss family since the summer.In a blog post, OpenAI said oss-safeguard uses reasoning “to directly interpret a developer-provider policy at inference time — classifying user messages, completions and full chats according to the developer’s needs.”The company explained that, since the model uses a chain-of-thought (CoT), developers can get explanations of the model&#x27;s decisions for review. “Additionally, the policy is provided during inference, rather than being trained into the model, so it is easy for developers to iteratively revise policies to increase performance,\" OpenAI said in its post. \"This approach, which we initially developed for internal use, is significantly more flexible than the traditional method of training a classifier to indirectly infer a decision boundary from a large number of labeled examples.\" Developers can download both models from Hugging Face. Flexibility versus baking inAt the onset, AI models will not know a company’s preferred safety triggers. While model providers do red-team models and platforms, these safeguards are intended for broader use. Companies like Microsoft and Amazon Web Services even offer platforms to bring guardrails to AI applications and agents. Enterprises use safety classifiers to help train a model to recognize patterns of good or bad inputs. This helps the models learn which queries they shouldn’t reply to. It also helps ensure that the models do not drift and answer accurately.“Traditional classifiers can have high performance, with low latency and operating cost,\" OpenAI said. \"But gathering a sufficient quantity of training examples can be time-consuming and costly, and updating or changing the policy requires re-training the classifier.\"The models takes in two inputs at once before it outputs a conclusion on where the content fails. It takes a policy and the content to classify under its guidelines. OpenAI said the models work best in situations where: The potential harm is emerging or evolving, and policies need to adapt quickly.The domain is highly nuanced and difficult for smaller classifiers to handle.Developers don’t have enough samples to train a high-quality classifier for each risk on their platform.Latency is less important than producing high-quality, explainable labels.The company said gpt-oss-safeguard “is different because its reasoning capabilities allow developers to apply any policy,” even ones they’ve written during inference. The models are based on OpenAI’s internal tool, the Safety Reasoner, which enables its teams to be more iterative in setting guardrails. They often begin with very strict safety policies, “and use relatively large amounts of compute where needed,” then adjust policies as they move the model through production and risk assessments change. Performing safetyOpenAI said the gpt-oss-safeguard models outperformed its GPT-5-thinking and the original gpt-oss models on multipolicy accuracy based on benchmark testing. It also ran the models on the ToxicChat public benchmark, where they performed well, although GPT-5-thinking and the Safety Reasoner slightly edged them out.But there is concern that this approach could bring a centralization of safety standards.“Safety is not a well-defined concept. Any implementation of safety standards will reflect the values and priorities of the organization that creates it, as well as the limits and deficiencies of its models,” said John Thickstun, an assistant professor of computer science at Cornell University. “If industry as a whole adopts standards developed by OpenAI, we risk institutionalizing one particular perspective on safety and short-circuiting broader investigations into the safety needs for AI deployments across many sectors of society.”It should also be noted that OpenAI did not release the base model for the oss family of models, so developers cannot fully iterate on them. OpenAI, however, is confident that the developer community can help refine gpt-oss-safeguard. It will host a Hackathon on December 8 in San Francisco.",
          "feed_position": 6,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/7hdJbWzLDjzRu2QOtkbDKV/a3a7d637a3e748ccb3ff4ba06e1ff953/crimedy7_illustration_of_technological_safety_cones_--ar_169__cf756a3e-79a5-47c3-993d-cdb5f37f28a2_3.png?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/agentic-ai-is-all-about-the-context-engineering-that-is",
          "published_at": "Wed, 29 Oct 2025 04:00:00 GMT",
          "title": "Agentic AI is all about the context — engineering, that is",
          "standfirst": "Presented by ElasticAs organizations scramble to enact agentic AI solutions, accessing proprietary data from all the nooks and crannies will be keyBy now, most organizations have heard of agentic AI, which are systems that “think” by autonomously gathering tools, data and other sources of information to return an answer. But here’s the rub: reliability and relevance depend on delivering accurate context. In most enterprises, this context is scattered across various unstructured data sources, including documents, emails, business apps, and customer feedback. As organizations look ahead to 2026, solving this problem will be key to accelerating agentic AI rollouts around the world, says Ken Exner, chief product officer at Elastic. \"People are starting to realize that to do agentic AI correctly, you have to have relevant data,\" Exner says. \"Relevance is critical in the context of agentic AI, because that AI is taking action on your behalf. When people struggle to build AI applications, I can almost guarantee you the problem is relevance.”Agents everywhereThe struggle could be entering a make-or-break period as organizations scramble for competitive edge or to create new efficiencies. A Deloitte study predicts that by 2026, more than 60% of large enterprises will have deployed agentic AI at scale, marking a major increase from experimental phases to mainstream implementation. And researcher Gartner forecasts that by the end of 2026, 40% of all enterprise applications will incorporate task-specific agents, up from less than 5% in 2025. Adding task specialization capabilities evolves AI assistants into context-aware AI agents.Enter context engineeringThe process for getting the relevant context into agents at the right time is known as context engineering. It not only ensures that an agentic application has the data it needs to provide accurate, in-depth responses, it helps the large language model (LLM) understand what tools it needs to find and use that data, and how to call those APIs. While there are now open-source standards such as the Model Context Protocol (MCP) that allow LLMs to connect to and communicate with external data, there are few platforms that let organizations build precise AI agents that use your data and combine retrieval, governance, and orchestration in one place, natively. Elasticsearch has always been a leading platform for the core of context engineering. It recently released a new feature within Elasticsearch called Agent Builder, which simplifies the entire operational lifecycle of agents: development, configuration, execution, customization, and observability.Agent Builder helps build MCP tools on private data using various techniques, including Elasticsearch Query Language, a piped query language for filtering, transforming, and analyzing data, or workflow modeling. Users can then take various tools and combine them with prompts and an LLM to build an agent. Agent Builder offers a configurable, out-of-the-box conversational agent that allows you to chat with the data in the index, and it also gives users the ability to build one from scratch using various tools and prompts on top of private data. \"Data is the center of our world at Elastic. We’re trying to make sure that you have the tools you need to put that data to work,\" Exner explains. \"The second you open up Agent Builder, you point it to an index in Elasticsearch, and you can begin chatting with any data you connect this to, any data that’s indexed in Elasticsearch — or from external sources through integrations.”Context engineering as a disciplinePrompt and context engineering is becoming a discipli. It’s not something you need a computer science degree in, but more classes and best practices will emerge, because there’s an art to it. \"We want to make it very simple to do that,\" Exner says. \"The thing that people will have to figure out is, how do you drive automation with AI? That’s what’s going to drive productivity. The people who are focused on that will see more success.\"Beyond that, other context engineering patterns will emerge. The industry has gone from prompt engineering to retrieval-augmented generation, where information is passed to the LLM in a context window, to MCP solutions that help LLMs with tool selection. But it won&#x27;t stop there.\"Given how fast things are moving, I will guarantee that new patterns will emerge quite quickly,\" Exner says. \"There will still be context engineering, but they’ll be new patterns for how to share data with an LLM, how to get it to be grounded in the right information. And I predict more patterns that make it possible for the LLM to understand private data that it’s not been trained on.\"Agent Builder is available now as a tech preview. Get started with an Elastic Cloud Trial, and check out the documentation for Agent Builder here.Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.",
          "content": "Presented by ElasticAs organizations scramble to enact agentic AI solutions, accessing proprietary data from all the nooks and crannies will be keyBy now, most organizations have heard of agentic AI, which are systems that “think” by autonomously gathering tools, data and other sources of information to return an answer. But here’s the rub: reliability and relevance depend on delivering accurate context. In most enterprises, this context is scattered across various unstructured data sources, including documents, emails, business apps, and customer feedback. As organizations look ahead to 2026, solving this problem will be key to accelerating agentic AI rollouts around the world, says Ken Exner, chief product officer at Elastic. \"People are starting to realize that to do agentic AI correctly, you have to have relevant data,\" Exner says. \"Relevance is critical in the context of agentic AI, because that AI is taking action on your behalf. When people struggle to build AI applications, I can almost guarantee you the problem is relevance.”Agents everywhereThe struggle could be entering a make-or-break period as organizations scramble for competitive edge or to create new efficiencies. A Deloitte study predicts that by 2026, more than 60% of large enterprises will have deployed agentic AI at scale, marking a major increase from experimental phases to mainstream implementation. And researcher Gartner forecasts that by the end of 2026, 40% of all enterprise applications will incorporate task-specific agents, up from less than 5% in 2025. Adding task specialization capabilities evolves AI assistants into context-aware AI agents.Enter context engineeringThe process for getting the relevant context into agents at the right time is known as context engineering. It not only ensures that an agentic application has the data it needs to provide accurate, in-depth responses, it helps the large language model (LLM) understand what tools it needs to find and use that data, and how to call those APIs. While there are now open-source standards such as the Model Context Protocol (MCP) that allow LLMs to connect to and communicate with external data, there are few platforms that let organizations build precise AI agents that use your data and combine retrieval, governance, and orchestration in one place, natively. Elasticsearch has always been a leading platform for the core of context engineering. It recently released a new feature within Elasticsearch called Agent Builder, which simplifies the entire operational lifecycle of agents: development, configuration, execution, customization, and observability.Agent Builder helps build MCP tools on private data using various techniques, including Elasticsearch Query Language, a piped query language for filtering, transforming, and analyzing data, or workflow modeling. Users can then take various tools and combine them with prompts and an LLM to build an agent. Agent Builder offers a configurable, out-of-the-box conversational agent that allows you to chat with the data in the index, and it also gives users the ability to build one from scratch using various tools and prompts on top of private data. \"Data is the center of our world at Elastic. We’re trying to make sure that you have the tools you need to put that data to work,\" Exner explains. \"The second you open up Agent Builder, you point it to an index in Elasticsearch, and you can begin chatting with any data you connect this to, any data that’s indexed in Elasticsearch — or from external sources through integrations.”Context engineering as a disciplinePrompt and context engineering is becoming a discipli. It’s not something you need a computer science degree in, but more classes and best practices will emerge, because there’s an art to it. \"We want to make it very simple to do that,\" Exner says. \"The thing that people will have to figure out is, how do you drive automation with AI? That’s what’s going to drive productivity. The people who are focused on that will see more success.\"Beyond that, other context engineering patterns will emerge. The industry has gone from prompt engineering to retrieval-augmented generation, where information is passed to the LLM in a context window, to MCP solutions that help LLMs with tool selection. But it won&#x27;t stop there.\"Given how fast things are moving, I will guarantee that new patterns will emerge quite quickly,\" Exner says. \"There will still be context engineering, but they’ll be new patterns for how to share data with an LLM, how to get it to be grounded in the right information. And I predict more patterns that make it possible for the LLM to understand private data that it’s not been trained on.\"Agent Builder is available now as a tech preview. Get started with an Elastic Cloud Trial, and check out the documentation for Agent Builder here.Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com.",
          "feed_position": 7,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/3HhYB9aUXlzxC0Q4V716ck/a33201a5ce2da5b486e6a20da9abbb52/AdobeStock_703274424.jpeg?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/nvidia-researchers-unlock-4-bit-llm-training-that-matches-8-bit-performance",
          "published_at": "Wed, 29 Oct 2025 00:00:00 GMT",
          "title": "Nvidia researchers unlock 4-bit LLM training that matches 8-bit performance",
          "standfirst": "Researchers at Nvidia have developed a novel approach to train large language models (LLMs) in 4-bit quantized format while maintaining their stability and accuracy at the level of high-precision models. Their technique, NVFP4, makes it possible to train models that not only outperform other leading 4-bit formats but match the performance of the larger 8-bit FP8 format, all while using half the memory and a fraction of the compute.The success of NVFP4 shows that enterprises can continue to cut inference costs by running leaner models that match the performance of larger ones. It also hints at a future where the cost of training LLMs will drop to a point where many more organizations can train their own bespoke models from scratch rather than just fine-tuning existing ones.The quantization challengeModel quantization is a technique used to reduce the computational and memory costs of running and training AI models. It works by converting the model&#x27;s parameters, or weights, from high-precision formats like 16- and 32-bit floating point (BF16 and FP32) to lower-precision formats. The key challenge of quantization is to reduce the size of the model while preserving as much of its knowledge and capabilities as possible.In recent years, 8-bit floating point formats (FP8) have become a popular industry standard, offering a good balance between performance and efficiency. They significantly lower the computational cost and memory demand for LLM training without a major drop in accuracy.The next logical step is 4-bit floating point (FP4), which promises to halve memory usage again and further boost performance on advanced hardware. However, this transition has been challenging. Existing 4-bit formats, such as MXFP4, often struggle to maintain the same level of accuracy as their 8-bit counterparts, forcing a difficult trade-off between cost and performance.How NVFP4 worksNVFP4 overcomes the stability and accuracy challenges of other FP4 techniques through a smarter design and a targeted training methodology. A key issue with 4-bit precision is its extremely limited range: It can only represent 16 distinct values. When converting from a high-precision format, outlier values can distort the entire dataset, harming the model&#x27;s accuracy. NVFP4 uses a more sophisticated, multi-level scaling approach that better handles these outliers, allowing for a \"more precise and accurate representation of tensor values during training,\" according to Nvidia.Beyond the format, the researchers introduce a 4-bit training recipe that achieves accuracy comparable to FP8. A central component is their “mixed-precision strategy.” Instead of converting the entire model to NVFP4, the majority of layers are quantized while a small fraction of numerically sensitive layers are kept in a higher-precision format like BF16. This preserves stability where it matters most. The methodology also adjusts how gradients are calculated during backpropagation — or the model&#x27;s learning phase — to reduce biases that can accumulate from low-precision arithmetic.NVFP4 in practiceTo test their approach, the Nvidia team trained a powerful 12-billion-parameter hybrid Mamba-Transformer model on a massive 10 trillion tokens. They then compared its performance directly against a baseline model trained in the widely popular FP8 format. The results showed that the NVFP4 model&#x27;s training loss and downstream task accuracy closely tracked the FP8 version throughout the entire process.The performance held across a wide range of domains, including knowledge-intensive reasoning, mathematics and commonsense tasks, with only a slight drop-off in coding benchmarks in late training.\"This marks, to our knowledge, the first successful demonstration of training billion-parameter language models with 4-bit precision over a multi-trillion-token horizon, laying the foundation for faster and more efficient training of future frontier models,” the researchers write.According to Nvidia&#x27;s director of product for AI and data center GPUs NvidiaShar Narasimhan, in practice, NVFP4’s 4-bit precision format enables developers and businesses to train and deploy AI models with nearly the same accuracy as traditional 8-bit formats. “By training model weights directly in 4-bit format while preserving accuracy, it empowers developers to experiment with new architectures, iterate faster and uncover insights without being bottlenecked by resource constraints,” he told VentureBeat. In contrast, FP8 (while already a leap forward from FP16) still imposes limits on model size and inference performance due to higher memory and bandwidth demands. “NVFP4 breaks that ceiling, offering equivalent quality with dramatically greater headroom for growth and experimentation,” Narasimhan said.When compared to the alternative 4-bit format, MXFP4, the benefits of NVFP4 become even clearer. In an experiment with an 8-billion-parameter model, NVFP4 converged to a better loss score than MXFP4. To reach the same level of performance as the NVFP4 model, the MXFP4 model had to be trained on 36% more data, a considerable increase in training time and cost.In addition to making pretraining more efficient, NVFP4 also redefines what’s possible. “Showing that 4-bit precision can preserve model quality at scale opens the door to a future where highly specialized models can be trained from scratch by mid-sized enterprises or startups, not just hyperscalers,” Narasimhan said, adding that, over time, we can expect a shift from developing general purpose LLMs models to “a diverse ecosystem of custom, high-performance models built by a broader range of innovators.”Beyond pre-trainingAlthough the paper focuses on the advantages of NVFP4 during pretraining, its impact extends to inference, as well. “Models trained on NVFP4 can not only deliver faster inference and higher throughput but shorten the time required for AI factories to achieve ROI — accelerating the cycle from model development to real-world deployment,” Narasimhan said. Because these models are smaller and more efficient, they unlock new possibilities for serving complex, high-quality responses in real time, even in token-intensive, agentic applications, without raising energy and compute costs. Narasimhan said he looks toward a future of model efficiency that isn’t solely about pushing precision lower, but building smarter systems. “There are many opportunities to expand research into lower precisions as well as modifying architectures to address the components that increasingly dominate compute in large-scale models,” he said. “These areas are rich with opportunity, especially as we move toward agentic systems that demand high throughput, low latency and adaptive reasoning. NVFP4 proves that precision can be optimized without compromising quality, and it sets the stage for a new era of intelligent, efficient AI design.”",
          "content": "Researchers at Nvidia have developed a novel approach to train large language models (LLMs) in 4-bit quantized format while maintaining their stability and accuracy at the level of high-precision models. Their technique, NVFP4, makes it possible to train models that not only outperform other leading 4-bit formats but match the performance of the larger 8-bit FP8 format, all while using half the memory and a fraction of the compute.The success of NVFP4 shows that enterprises can continue to cut inference costs by running leaner models that match the performance of larger ones. It also hints at a future where the cost of training LLMs will drop to a point where many more organizations can train their own bespoke models from scratch rather than just fine-tuning existing ones.The quantization challengeModel quantization is a technique used to reduce the computational and memory costs of running and training AI models. It works by converting the model&#x27;s parameters, or weights, from high-precision formats like 16- and 32-bit floating point (BF16 and FP32) to lower-precision formats. The key challenge of quantization is to reduce the size of the model while preserving as much of its knowledge and capabilities as possible.In recent years, 8-bit floating point formats (FP8) have become a popular industry standard, offering a good balance between performance and efficiency. They significantly lower the computational cost and memory demand for LLM training without a major drop in accuracy.The next logical step is 4-bit floating point (FP4), which promises to halve memory usage again and further boost performance on advanced hardware. However, this transition has been challenging. Existing 4-bit formats, such as MXFP4, often struggle to maintain the same level of accuracy as their 8-bit counterparts, forcing a difficult trade-off between cost and performance.How NVFP4 worksNVFP4 overcomes the stability and accuracy challenges of other FP4 techniques through a smarter design and a targeted training methodology. A key issue with 4-bit precision is its extremely limited range: It can only represent 16 distinct values. When converting from a high-precision format, outlier values can distort the entire dataset, harming the model&#x27;s accuracy. NVFP4 uses a more sophisticated, multi-level scaling approach that better handles these outliers, allowing for a \"more precise and accurate representation of tensor values during training,\" according to Nvidia.Beyond the format, the researchers introduce a 4-bit training recipe that achieves accuracy comparable to FP8. A central component is their “mixed-precision strategy.” Instead of converting the entire model to NVFP4, the majority of layers are quantized while a small fraction of numerically sensitive layers are kept in a higher-precision format like BF16. This preserves stability where it matters most. The methodology also adjusts how gradients are calculated during backpropagation — or the model&#x27;s learning phase — to reduce biases that can accumulate from low-precision arithmetic.NVFP4 in practiceTo test their approach, the Nvidia team trained a powerful 12-billion-parameter hybrid Mamba-Transformer model on a massive 10 trillion tokens. They then compared its performance directly against a baseline model trained in the widely popular FP8 format. The results showed that the NVFP4 model&#x27;s training loss and downstream task accuracy closely tracked the FP8 version throughout the entire process.The performance held across a wide range of domains, including knowledge-intensive reasoning, mathematics and commonsense tasks, with only a slight drop-off in coding benchmarks in late training.\"This marks, to our knowledge, the first successful demonstration of training billion-parameter language models with 4-bit precision over a multi-trillion-token horizon, laying the foundation for faster and more efficient training of future frontier models,” the researchers write.According to Nvidia&#x27;s director of product for AI and data center GPUs NvidiaShar Narasimhan, in practice, NVFP4’s 4-bit precision format enables developers and businesses to train and deploy AI models with nearly the same accuracy as traditional 8-bit formats. “By training model weights directly in 4-bit format while preserving accuracy, it empowers developers to experiment with new architectures, iterate faster and uncover insights without being bottlenecked by resource constraints,” he told VentureBeat. In contrast, FP8 (while already a leap forward from FP16) still imposes limits on model size and inference performance due to higher memory and bandwidth demands. “NVFP4 breaks that ceiling, offering equivalent quality with dramatically greater headroom for growth and experimentation,” Narasimhan said.When compared to the alternative 4-bit format, MXFP4, the benefits of NVFP4 become even clearer. In an experiment with an 8-billion-parameter model, NVFP4 converged to a better loss score than MXFP4. To reach the same level of performance as the NVFP4 model, the MXFP4 model had to be trained on 36% more data, a considerable increase in training time and cost.In addition to making pretraining more efficient, NVFP4 also redefines what’s possible. “Showing that 4-bit precision can preserve model quality at scale opens the door to a future where highly specialized models can be trained from scratch by mid-sized enterprises or startups, not just hyperscalers,” Narasimhan said, adding that, over time, we can expect a shift from developing general purpose LLMs models to “a diverse ecosystem of custom, high-performance models built by a broader range of innovators.”Beyond pre-trainingAlthough the paper focuses on the advantages of NVFP4 during pretraining, its impact extends to inference, as well. “Models trained on NVFP4 can not only deliver faster inference and higher throughput but shorten the time required for AI factories to achieve ROI — accelerating the cycle from model development to real-world deployment,” Narasimhan said. Because these models are smaller and more efficient, they unlock new possibilities for serving complex, high-quality responses in real time, even in token-intensive, agentic applications, without raising energy and compute costs. Narasimhan said he looks toward a future of model efficiency that isn’t solely about pushing precision lower, but building smarter systems. “There are many opportunities to expand research into lower precisions as well as modifying architectures to address the components that increasingly dominate compute in large-scale models,” he said. “These areas are rich with opportunity, especially as we move toward agentic systems that demand high throughput, low latency and adaptive reasoning. NVFP4 proves that precision can be optimized without compromising quality, and it sets the stage for a new era of intelligent, efficient AI design.”",
          "feed_position": 8,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/6m9WZeiaEIOLcUtuBpQLED/7143d90738c0365e7649a06167b709df/cfr0z3n_photorealistic_35mm_a_tiny_intricate_clockwork_robot_ra_6c68cfa0-0d24-4ad5-8964-179622de805f.png?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/ibms-open-source-granite-4-0-nano-ai-models-are-small-enough-to-run-locally",
          "published_at": "Tue, 28 Oct 2025 23:23:00 GMT",
          "title": "IBM's open source Granite 4.0 Nano AI models are small enough to run locally directly in your browser",
          "standfirst": "In an industry where model size is often seen as a proxy for intelligence, IBM is charting a different course — one that values efficiency over enormity, and accessibility over abstraction.The 114-year-old tech giant&#x27;s four new Granite 4.0 Nano models, released today, range from just 350 million to 1.5 billion parameters, a fraction of the size of their server-bound cousins from the likes of OpenAI, Anthropic, and Google. These models are designed to be highly accessible: the 350M variants can run comfortably on a modern laptop CPU with 8–16GB of RAM, while the 1.5B models typically require a GPU with at least 6–8GB of VRAM for smooth performance — or sufficient system RAM and swap for CPU-only inference. This makes them well-suited for developers building applications on consumer hardware or at the edge, without relying on cloud compute.In fact, the smallest ones can even run locally on your own web browser, as Joshua Lochner aka Xenova, creator of Transformer.js and a machine learning engineer at Hugging Face, wrote on the social network X.All the Granite 4.0 Nano models are released under the Apache 2.0 license — perfect for use by researchers and enterprise or indie developers, even for commercial usage. They are natively compatible with llama.cpp, vLLM, and MLX and are certified under ISO 42001 for responsible AI development — a standard IBM helped pioneer.But in this case, small doesn&#x27;t mean less capable — it might just mean smarter design.These compact models are built not for data centers, but for edge devices, laptops, and local inference, where compute is scarce and latency matters. And despite their small size, the Nano models are showing benchmark results that rival or even exceed the performance of larger models in the same category. The release is a signal that a new AI frontier is rapidly forming — one not dominated by sheer scale, but by strategic scaling.What Exactly Did IBM Release?The Granite 4.0 Nano family includes four open-source models now available on Hugging Face:Granite-4.0-H-1B (~1.5B parameters) – Hybrid-SSM architectureGranite-4.0-H-350M (~350M parameters) – Hybrid-SSM architectureGranite-4.0-1B – Transformer-based variant, parameter count closer to 2BGranite-4.0-350M – Transformer-based variantThe H-series models — Granite-4.0-H-1B and H-350M — use a hybrid state space architecture (SSM) that combines efficiency with strong performance, ideal for low-latency edge environments. Meanwhile, the standard transformer variants — Granite-4.0-1B and 350M — offer broader compatibility with tools like llama.cpp, designed for use cases where hybrid architecture isn’t yet supported. In practice, the transformer 1B model is closer to 2B parameters, but aligns performance-wise with its hybrid sibling, offering developers flexibility based on their runtime constraints.“The hybrid variant is a true 1B model. However, the non-hybrid variant is closer to 2B, but we opted to keep the naming aligned to the hybrid variant to make the connection easily visible,” explained Emma, Product Marketing lead for Granite, during a Reddit \"Ask Me Anything\" (AMA) session on r/LocalLLaMA.A Competitive Class of Small ModelsIBM is entering a crowded and rapidly evolving market of small language models (SLMs), competing with offerings like Qwen3, Google&#x27;s Gemma, LiquidAI’s LFM2, and even Mistral’s dense models in the sub-2B parameter space.While OpenAI and Anthropic focus on models that require clusters of GPUs and sophisticated inference optimization, IBM’s Nano family is aimed squarely at developers who want to run performant LLMs on local or constrained hardware.In benchmark testing, IBM’s new models consistently top the charts in their class. According to data shared on X by David Cox, VP of AI Models at IBM Research:On IFEval (instruction following), Granite-4.0-H-1B scored 78.5, outperforming Qwen3-1.7B (73.1) and other 1–2B models.On BFCLv3 (function/tool calling), Granite-4.0-1B led with a score of 54.8, the highest in its size class.On safety benchmarks (SALAD and AttaQ), the Granite models scored over 90%, surpassing similarly sized competitors.Overall, the Granite-4.0-1B achieved a leading average benchmark score of 68.3% across general knowledge, math, code, and safety domains.This performance is especially significant given the hardware constraints these models are designed for. They require less memory, run faster on CPUs or mobile devices, and don’t need cloud infrastructure or GPU acceleration to deliver usable results.Why Model Size Still Matters — But Not Like It Used ToIn the early wave of LLMs, bigger meant better — more parameters translated to better generalization, deeper reasoning, and richer output. But as transformer research matured, it became clear that architecture, training quality, and task-specific tuning could allow smaller models to punch well above their weight class.IBM is banking on this evolution. By releasing open, small models that are competitive in real-world tasks, the company is offering an alternative to the monolithic AI APIs that dominate today’s application stack.In fact, the Nano models address three increasingly important needs:Deployment flexibility — they run anywhere, from mobile to microservers.Inference privacy — users can keep data local with no need to call out to cloud APIs.Openness and auditability — source code and model weights are publicly available under an open license.Community Response and Roadmap SignalsIBM’s Granite team didn’t just launch the models and walk away — they took to Reddit’s open source community r/LocalLLaMA to engage directly with developers. In an AMA-style thread, Emma (Product Marketing, Granite) answered technical questions, addressed concerns about naming conventions, and dropped hints about what’s next.Notable confirmations from the thread:A larger Granite 4.0 model is currently in trainingReasoning-focused models (\"thinking counterparts\") are in the pipelineIBM will release fine-tuning recipes and a full training paper soonMore tooling and platform compatibility is on the roadmapUsers responded enthusiastically to the models’ capabilities, especially in instruction-following and structured response tasks. One commenter summed it up:“This is big if true for a 1B model — if quality is nice and it gives consistent outputs. Function-calling tasks, multilingual dialog, FIM completions… this could be a real workhorse.”Another user remarked:“The Granite Tiny is already my go-to for web search in LM Studio — better than some Qwen models. Tempted to give Nano a shot.”Background: IBM Granite and the Enterprise AI RaceIBM’s push into large language models began in earnest in late 2023 with the debut of the Granite foundation model family, starting with models like Granite.13b.instruct and Granite.13b.chat. Released for use within its Watsonx platform, these initial decoder-only models signaled IBM’s ambition to build enterprise-grade AI systems that prioritize transparency, efficiency, and performance. The company open-sourced select Granite code models under the Apache 2.0 license in mid-2024, laying the groundwork for broader adoption and developer experimentation.The real inflection point came with Granite 3.0 in October 2024 — a fully open-source suite of general-purpose and domain-specialized models ranging from 1B to 8B parameters. These models emphasized efficiency over brute scale, offering capabilities like longer context windows, instruction tuning, and integrated guardrails. IBM positioned Granite 3.0 as a direct competitor to Meta’s Llama, Alibaba’s Qwen, and Google&#x27;s Gemma — but with a uniquely enterprise-first lens. Later versions, including Granite 3.1 and Granite 3.2, introduced even more enterprise-friendly innovations: embedded hallucination detection, time-series forecasting, document vision models, and conditional reasoning toggles.The Granite 4.0 family, launched in October 2025, represents IBM’s most technically ambitious release yet. It introduces a hybrid architecture that blends transformer and Mamba-2 layers — aiming to combine the contextual precision of attention mechanisms with the memory efficiency of state-space models. This design allows IBM to significantly reduce memory and latency costs for inference, making Granite models viable on smaller hardware while still outperforming peers in instruction-following and function-calling tasks. The launch also includes ISO 42001 certification, cryptographic model signing, and distribution across platforms like Hugging Face, Docker, LM Studio, Ollama, and watsonx.ai.Across all iterations, IBM’s focus has been clear: build trustworthy, efficient, and legally unambiguous AI models for enterprise use cases. With a permissive Apache 2.0 license, public benchmarks, and an emphasis on governance, the Granite initiative not only responds to rising concerns over proprietary black-box models but also offers a Western-aligned open alternative to the rapid progress from teams like Alibaba’s Qwen. In doing so, Granite positions IBM as a leading voice in what may be the next phase of open-weight, production-ready AI.A Shift Toward Scalable EfficiencyIn the end, IBM’s release of Granite 4.0 Nano models reflects a strategic shift in LLM development: from chasing parameter count records to optimizing usability, openness, and deployment reach.By combining competitive performance, responsible development practices, and deep engagement with the open-source community, IBM is positioning Granite as not just a family of models — but a platform for building the next generation of lightweight, trustworthy AI systems.For developers and researchers looking for performance without overhead, the Nano release offers a compelling signal: you don’t need 70 billion parameters to build something powerful — just the right ones.",
          "content": "In an industry where model size is often seen as a proxy for intelligence, IBM is charting a different course — one that values efficiency over enormity, and accessibility over abstraction.The 114-year-old tech giant&#x27;s four new Granite 4.0 Nano models, released today, range from just 350 million to 1.5 billion parameters, a fraction of the size of their server-bound cousins from the likes of OpenAI, Anthropic, and Google. These models are designed to be highly accessible: the 350M variants can run comfortably on a modern laptop CPU with 8–16GB of RAM, while the 1.5B models typically require a GPU with at least 6–8GB of VRAM for smooth performance — or sufficient system RAM and swap for CPU-only inference. This makes them well-suited for developers building applications on consumer hardware or at the edge, without relying on cloud compute.In fact, the smallest ones can even run locally on your own web browser, as Joshua Lochner aka Xenova, creator of Transformer.js and a machine learning engineer at Hugging Face, wrote on the social network X.All the Granite 4.0 Nano models are released under the Apache 2.0 license — perfect for use by researchers and enterprise or indie developers, even for commercial usage. They are natively compatible with llama.cpp, vLLM, and MLX and are certified under ISO 42001 for responsible AI development — a standard IBM helped pioneer.But in this case, small doesn&#x27;t mean less capable — it might just mean smarter design.These compact models are built not for data centers, but for edge devices, laptops, and local inference, where compute is scarce and latency matters. And despite their small size, the Nano models are showing benchmark results that rival or even exceed the performance of larger models in the same category. The release is a signal that a new AI frontier is rapidly forming — one not dominated by sheer scale, but by strategic scaling.What Exactly Did IBM Release?The Granite 4.0 Nano family includes four open-source models now available on Hugging Face:Granite-4.0-H-1B (~1.5B parameters) – Hybrid-SSM architectureGranite-4.0-H-350M (~350M parameters) – Hybrid-SSM architectureGranite-4.0-1B – Transformer-based variant, parameter count closer to 2BGranite-4.0-350M – Transformer-based variantThe H-series models — Granite-4.0-H-1B and H-350M — use a hybrid state space architecture (SSM) that combines efficiency with strong performance, ideal for low-latency edge environments. Meanwhile, the standard transformer variants — Granite-4.0-1B and 350M — offer broader compatibility with tools like llama.cpp, designed for use cases where hybrid architecture isn’t yet supported. In practice, the transformer 1B model is closer to 2B parameters, but aligns performance-wise with its hybrid sibling, offering developers flexibility based on their runtime constraints.“The hybrid variant is a true 1B model. However, the non-hybrid variant is closer to 2B, but we opted to keep the naming aligned to the hybrid variant to make the connection easily visible,” explained Emma, Product Marketing lead for Granite, during a Reddit \"Ask Me Anything\" (AMA) session on r/LocalLLaMA.A Competitive Class of Small ModelsIBM is entering a crowded and rapidly evolving market of small language models (SLMs), competing with offerings like Qwen3, Google&#x27;s Gemma, LiquidAI’s LFM2, and even Mistral’s dense models in the sub-2B parameter space.While OpenAI and Anthropic focus on models that require clusters of GPUs and sophisticated inference optimization, IBM’s Nano family is aimed squarely at developers who want to run performant LLMs on local or constrained hardware.In benchmark testing, IBM’s new models consistently top the charts in their class. According to data shared on X by David Cox, VP of AI Models at IBM Research:On IFEval (instruction following), Granite-4.0-H-1B scored 78.5, outperforming Qwen3-1.7B (73.1) and other 1–2B models.On BFCLv3 (function/tool calling), Granite-4.0-1B led with a score of 54.8, the highest in its size class.On safety benchmarks (SALAD and AttaQ), the Granite models scored over 90%, surpassing similarly sized competitors.Overall, the Granite-4.0-1B achieved a leading average benchmark score of 68.3% across general knowledge, math, code, and safety domains.This performance is especially significant given the hardware constraints these models are designed for. They require less memory, run faster on CPUs or mobile devices, and don’t need cloud infrastructure or GPU acceleration to deliver usable results.Why Model Size Still Matters — But Not Like It Used ToIn the early wave of LLMs, bigger meant better — more parameters translated to better generalization, deeper reasoning, and richer output. But as transformer research matured, it became clear that architecture, training quality, and task-specific tuning could allow smaller models to punch well above their weight class.IBM is banking on this evolution. By releasing open, small models that are competitive in real-world tasks, the company is offering an alternative to the monolithic AI APIs that dominate today’s application stack.In fact, the Nano models address three increasingly important needs:Deployment flexibility — they run anywhere, from mobile to microservers.Inference privacy — users can keep data local with no need to call out to cloud APIs.Openness and auditability — source code and model weights are publicly available under an open license.Community Response and Roadmap SignalsIBM’s Granite team didn’t just launch the models and walk away — they took to Reddit’s open source community r/LocalLLaMA to engage directly with developers. In an AMA-style thread, Emma (Product Marketing, Granite) answered technical questions, addressed concerns about naming conventions, and dropped hints about what’s next.Notable confirmations from the thread:A larger Granite 4.0 model is currently in trainingReasoning-focused models (\"thinking counterparts\") are in the pipelineIBM will release fine-tuning recipes and a full training paper soonMore tooling and platform compatibility is on the roadmapUsers responded enthusiastically to the models’ capabilities, especially in instruction-following and structured response tasks. One commenter summed it up:“This is big if true for a 1B model — if quality is nice and it gives consistent outputs. Function-calling tasks, multilingual dialog, FIM completions… this could be a real workhorse.”Another user remarked:“The Granite Tiny is already my go-to for web search in LM Studio — better than some Qwen models. Tempted to give Nano a shot.”Background: IBM Granite and the Enterprise AI RaceIBM’s push into large language models began in earnest in late 2023 with the debut of the Granite foundation model family, starting with models like Granite.13b.instruct and Granite.13b.chat. Released for use within its Watsonx platform, these initial decoder-only models signaled IBM’s ambition to build enterprise-grade AI systems that prioritize transparency, efficiency, and performance. The company open-sourced select Granite code models under the Apache 2.0 license in mid-2024, laying the groundwork for broader adoption and developer experimentation.The real inflection point came with Granite 3.0 in October 2024 — a fully open-source suite of general-purpose and domain-specialized models ranging from 1B to 8B parameters. These models emphasized efficiency over brute scale, offering capabilities like longer context windows, instruction tuning, and integrated guardrails. IBM positioned Granite 3.0 as a direct competitor to Meta’s Llama, Alibaba’s Qwen, and Google&#x27;s Gemma — but with a uniquely enterprise-first lens. Later versions, including Granite 3.1 and Granite 3.2, introduced even more enterprise-friendly innovations: embedded hallucination detection, time-series forecasting, document vision models, and conditional reasoning toggles.The Granite 4.0 family, launched in October 2025, represents IBM’s most technically ambitious release yet. It introduces a hybrid architecture that blends transformer and Mamba-2 layers — aiming to combine the contextual precision of attention mechanisms with the memory efficiency of state-space models. This design allows IBM to significantly reduce memory and latency costs for inference, making Granite models viable on smaller hardware while still outperforming peers in instruction-following and function-calling tasks. The launch also includes ISO 42001 certification, cryptographic model signing, and distribution across platforms like Hugging Face, Docker, LM Studio, Ollama, and watsonx.ai.Across all iterations, IBM’s focus has been clear: build trustworthy, efficient, and legally unambiguous AI models for enterprise use cases. With a permissive Apache 2.0 license, public benchmarks, and an emphasis on governance, the Granite initiative not only responds to rising concerns over proprietary black-box models but also offers a Western-aligned open alternative to the rapid progress from teams like Alibaba’s Qwen. In doing so, Granite positions IBM as a leading voice in what may be the next phase of open-weight, production-ready AI.A Shift Toward Scalable EfficiencyIn the end, IBM’s release of Granite 4.0 Nano models reflects a strategic shift in LLM development: from chasing parameter count records to optimizing usability, openness, and deployment reach.By combining competitive performance, responsible development practices, and deep engagement with the open-source community, IBM is positioning Granite as not just a family of models — but a platform for building the next generation of lightweight, trustworthy AI systems.For developers and researchers looking for performance without overhead, the Nano release offers a compelling signal: you don’t need 70 billion parameters to build something powerful — just the right ones.",
          "feed_position": 9,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/4rwJWqsHkQ8TmY86sokH5j/cf400e028ed640c8e65f6bec9134c149/cfr0z3n_Flat_illustration_neon_pink_and_oranges_on_blue_backdro_3059ee39-d179-4b1b-9d52-a4264f21e970.png?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/ai/microsofts-copilot-can-now-build-apps-and-automate-your-job-heres-how-it",
          "published_at": "Tue, 28 Oct 2025 20:30:00 GMT",
          "title": "Microsoft’s Copilot can now build apps and automate your job — here’s how it works",
          "standfirst": "Microsoft is launching a significant expansion of its Copilot AI assistant on Tuesday, introducing tools that let employees build applications, automate workflows, and create specialized AI agents using only conversational prompts — no coding required.The new capabilities, called App Builder and Workflows, mark Microsoft&#x27;s most aggressive attempt yet to merge artificial intelligence with software development, enabling the estimated 100 million Microsoft 365 users to create business tools as easily as they currently draft emails or build spreadsheets.\"We really believe that a main part of an AI-forward employee, not just developers, will be to create agents, workflows and apps,\" Charles Lamanna, Microsoft&#x27;s president of business and industry Copilot, said in an interview with VentureBeat. \"Part of the job will be to build and create these things.\"The announcement comes as Microsoft deepens its commitment to AI-powered productivity tools while navigating a complex partnership with OpenAI, the creator of the underlying technology that powers Copilot. On the same day, OpenAI completed its restructuring into a for-profit entity, with Microsoft receiving a 27% ownership stake valued at approximately $135 billion.How natural language prompts now create fully functional business applicationsThe new features transform Copilot from a conversational assistant into what Microsoft envisions as a comprehensive development environment accessible to non-technical workers. Users can now describe an application they need — such as a project tracker with dashboards and task assignments — and Copilot will generate a working app complete with a database backend, user interface, and security controls.\"If you&#x27;re right inside of Copilot, you can now have a conversation to build an application complete with a backing database and a security model,\" Lamanna explained. \"You can make edit requests and update requests and change requests so you can tune the app to get exactly the experience you want before you share it with other users.\"The App Builder stores data in Microsoft Lists, the company&#x27;s lightweight database system, and allows users to share finished applications via a simple link—similar to sharing a document. The Workflows agent, meanwhile, automates routine tasks across Microsoft&#x27;s ecosystem of products, including Outlook, Teams, SharePoint, and Planner, by converting natural language descriptions into automated processes.A third component, a simplified version of Microsoft&#x27;s Copilot Studio agent-building platform, lets users create specialized AI assistants tailored to specific tasks or knowledge domains, drawing from SharePoint documents, meeting transcripts, emails, and external systems.All three capabilities are included in the existing $30-per-month Microsoft 365 Copilot subscription at no additional cost — a pricing decision Lamanna characterized as consistent with Microsoft&#x27;s historical approach of bundling significant value into its productivity suite.\"That&#x27;s what Microsoft always does. We try to do a huge amount of value at a low price,\" he said. \"If you go look at Office, you think about Excel, Word, PowerPoint, Exchange, all that for like eight bucks a month. That&#x27;s a pretty good deal.\"Why Microsoft&#x27;s nine-year bet on low-code development is finally paying offThe new tools represent the culmination of a nine-year effort by Microsoft to democratize software development through its Power Platform — a collection of low-code and no-code development tools that has grown to 56 million monthly active users, according to figures the company disclosed in recent earnings reports.Lamanna, who has led the Power Platform initiative since its inception, said the integration into Copilot marks a fundamental shift in how these capabilities reach users. Rather than requiring workers to visit a separate website or learn a specialized interface, the development tools now exist within the same conversational window they already use for AI-assisted tasks.\"One of the big things that we&#x27;re excited about is Copilot — that&#x27;s a tool for literally every office worker,\" Lamanna said. \"Every office worker, just like they research data, they analyze data, they reason over topics, they also will be creating apps, agents and workflows.\"The integration offers significant technical advantages, he argued. Because Copilot already indexes a user&#x27;s Microsoft 365 content — emails, documents, meetings, and organizational data — it can incorporate that context into the applications and workflows it builds. If a user asks for \"an app for Project Spartan,\" Copilot can draw from existing communications to understand what that project entails and suggest relevant features.\"If you go to those other tools, they have no idea what the heck Project Spartan is,\" Lamanna said, referencing competing low-code platforms from companies like Google, Salesforce, and ServiceNow. \"But if you do it inside of Copilot and inside of the App Builder, it&#x27;s able to draw from all that information and context.\"Microsoft claims the apps created through these tools are \"full-stack applications\" with proper databases secured through the same identity systems used across its enterprise products — distinguishing them from simpler front-end tools offered by competitors. The company also emphasized that its existing governance, security, and data loss prevention policies automatically apply to apps and workflows created through Copilot.Where professional developers still matter in an AI-powered workplaceWhile Microsoft positions the new capabilities as accessible to all office workers, Lamanna was careful to delineate where professional developers remain essential. His dividing line centers on whether a system interacts with parties outside the organization.\"Anything that leaves the boundaries of your company warrants developer involvement,\" he said. \"If you want to build an agent and put it on your website, you should have developers involved. Or if you want to build an automation which interfaces directly with your customers, or an app or a website which interfaces directly with your customers, you want professionals involved.\"The reasoning is risk-based: external-facing systems carry greater potential for data breaches, security vulnerabilities, or business errors. \"You don&#x27;t want people getting refunds they shouldn&#x27;t,\" Lamanna noted.For internal use cases — approval workflows, project tracking, team dashboards — Microsoft believes the new tools can handle the majority of needs without IT department involvement. But the company has built \"no cliffs,\" in Lamanna&#x27;s terminology, allowing users to migrate simple apps to more sophisticated platforms as needs grow.Apps created in the conversational App Builder can be opened in Power Apps, Microsoft&#x27;s full development environment, where they can be connected to Dataverse, the company&#x27;s enterprise database, or extended with custom code. Similarly, simple workflows can graduate to the full Power Automate platform, and basic agents can be enhanced in the complete Copilot Studio.\"We have this mantra called no cliffs,\" Lamanna said. \"If your app gets too complicated for the App Builder, you can always edit and open it in Power Apps. You can jump over to the richer experience, and if you&#x27;re really sophisticated, you can even go from those experiences into Azure.\"This architecture addresses a problem that has plagued previous generations of easy-to-use development tools: users who outgrow the simplified environment often must rebuild from scratch on professional platforms. \"People really do not like easy-to-use development tools if I have to throw everything away and start over,\" Lamanna said.What happens when every employee can build apps without IT approvalThe democratization of software development raises questions about governance, maintenance, and organizational complexity — issues Microsoft has worked to address through administrative controls.IT administrators can view all applications, workflows, and agents created within their organization through a centralized inventory in the Microsoft 365 admin center. They can reassign ownership, disable access at the group level, or \"promote\" particularly useful employee-created apps to officially supported status.\"We have a bunch of customers who have this approach where it&#x27;s like, let 1,000 apps bloom, and then the best ones, I go upgrade and make them IT-governed or central,\" Lamanna said.The system also includes provisions for when employees leave. Apps and workflows remain accessible for 60 days, during which managers can claim ownership — similar to how OneDrive files are handled when someone departs.Lamanna argued that most employee-created apps don&#x27;t warrant significant IT oversight. \"It&#x27;s just not worth inspecting an app that John, Susie, and Bob use to do their job,\" he said. \"It should concern itself with the app that ends up being used by 2,000 people, and that will pop up in that dashboard.\"Still, the proliferation of employee-created applications could create challenges. Users have expressed frustration with Microsoft&#x27;s increasing emphasis on AI features across its products, with some giving the Microsoft 365 mobile app one-star ratings after a recent update prioritized Copilot over traditional file access.The tools also arrive as enterprises grapple with \"shadow IT\" — unsanctioned software and systems that employees adopt without official approval. While Microsoft&#x27;s governance controls aim to provide visibility, the ease of creating new applications could accelerate the pace at which these systems multiply.The ambitious plan to turn 500 million workers into software buildersMicrosoft&#x27;s ambitions for the technology extend far beyond incremental productivity gains. Lamanna envisions a fundamental transformation of what it means to be an office worker — one where building software becomes as routine as creating spreadsheets.\"Just like how 20 years ago you put on your resume that you could use pivot tables in Excel, people are going to start saying that they can use App Builder and workflow agents, even if they&#x27;re just in the finance department or the sales department,\" he said.The numbers he&#x27;s targeting are staggering. With 56 million people already using Power Platform, Lamanna believes the integration into Copilot could eventually reach 500 million builders. \"Early days still, but I think it&#x27;s certainly encouraging,\" he said.The features are currently available only to customers in Microsoft&#x27;s Frontier Program — an early access initiative for Microsoft 365 Copilot subscribers. The company has not disclosed how many organizations participate in the program or when the tools will reach general availability.The announcement fits within Microsoft&#x27;s larger strategy of embedding AI capabilities throughout its product portfolio, driven by its partnership with OpenAI. Under the restructured agreement announced Tuesday, Microsoft will have access to OpenAI&#x27;s technology through 2032, including models that achieve artificial general intelligence (AGI) — though such systems do not yet exist. Microsoft has also begun integrating Copilot into its new companion apps for Windows 11, which provide quick access to contacts, files, and calendar information.The aggressive integration of AI features across Microsoft&#x27;s ecosystem has drawn mixed reactions. While enterprise customers have shown interest in productivity gains, the rapid pace of change and ubiquity of AI prompts have frustrated some users who prefer traditional workflows.For Microsoft, however, the calculation is clear: if even a fraction of its user base begins creating applications and automations, it would represent a massive expansion of the effective software development workforce — and further entrench customers in Microsoft&#x27;s ecosystem. The company is betting that the same natural language interface that made ChatGPT accessible to millions can finally unlock the decades-old promise of empowering everyday workers to build their own tools.The App Builder and Workflows agents are available starting today through the Microsoft 365 Copilot Agent Store for Frontier Program participants.Whether that future arrives depends not just on the technology&#x27;s capabilities, but on a more fundamental question: Do millions of office workers actually want to become part-time software developers? Microsoft is about to find out if the answer is yes — or if some jobs are better left to the professionals.",
          "content": "Microsoft is launching a significant expansion of its Copilot AI assistant on Tuesday, introducing tools that let employees build applications, automate workflows, and create specialized AI agents using only conversational prompts — no coding required.The new capabilities, called App Builder and Workflows, mark Microsoft&#x27;s most aggressive attempt yet to merge artificial intelligence with software development, enabling the estimated 100 million Microsoft 365 users to create business tools as easily as they currently draft emails or build spreadsheets.\"We really believe that a main part of an AI-forward employee, not just developers, will be to create agents, workflows and apps,\" Charles Lamanna, Microsoft&#x27;s president of business and industry Copilot, said in an interview with VentureBeat. \"Part of the job will be to build and create these things.\"The announcement comes as Microsoft deepens its commitment to AI-powered productivity tools while navigating a complex partnership with OpenAI, the creator of the underlying technology that powers Copilot. On the same day, OpenAI completed its restructuring into a for-profit entity, with Microsoft receiving a 27% ownership stake valued at approximately $135 billion.How natural language prompts now create fully functional business applicationsThe new features transform Copilot from a conversational assistant into what Microsoft envisions as a comprehensive development environment accessible to non-technical workers. Users can now describe an application they need — such as a project tracker with dashboards and task assignments — and Copilot will generate a working app complete with a database backend, user interface, and security controls.\"If you&#x27;re right inside of Copilot, you can now have a conversation to build an application complete with a backing database and a security model,\" Lamanna explained. \"You can make edit requests and update requests and change requests so you can tune the app to get exactly the experience you want before you share it with other users.\"The App Builder stores data in Microsoft Lists, the company&#x27;s lightweight database system, and allows users to share finished applications via a simple link—similar to sharing a document. The Workflows agent, meanwhile, automates routine tasks across Microsoft&#x27;s ecosystem of products, including Outlook, Teams, SharePoint, and Planner, by converting natural language descriptions into automated processes.A third component, a simplified version of Microsoft&#x27;s Copilot Studio agent-building platform, lets users create specialized AI assistants tailored to specific tasks or knowledge domains, drawing from SharePoint documents, meeting transcripts, emails, and external systems.All three capabilities are included in the existing $30-per-month Microsoft 365 Copilot subscription at no additional cost — a pricing decision Lamanna characterized as consistent with Microsoft&#x27;s historical approach of bundling significant value into its productivity suite.\"That&#x27;s what Microsoft always does. We try to do a huge amount of value at a low price,\" he said. \"If you go look at Office, you think about Excel, Word, PowerPoint, Exchange, all that for like eight bucks a month. That&#x27;s a pretty good deal.\"Why Microsoft&#x27;s nine-year bet on low-code development is finally paying offThe new tools represent the culmination of a nine-year effort by Microsoft to democratize software development through its Power Platform — a collection of low-code and no-code development tools that has grown to 56 million monthly active users, according to figures the company disclosed in recent earnings reports.Lamanna, who has led the Power Platform initiative since its inception, said the integration into Copilot marks a fundamental shift in how these capabilities reach users. Rather than requiring workers to visit a separate website or learn a specialized interface, the development tools now exist within the same conversational window they already use for AI-assisted tasks.\"One of the big things that we&#x27;re excited about is Copilot — that&#x27;s a tool for literally every office worker,\" Lamanna said. \"Every office worker, just like they research data, they analyze data, they reason over topics, they also will be creating apps, agents and workflows.\"The integration offers significant technical advantages, he argued. Because Copilot already indexes a user&#x27;s Microsoft 365 content — emails, documents, meetings, and organizational data — it can incorporate that context into the applications and workflows it builds. If a user asks for \"an app for Project Spartan,\" Copilot can draw from existing communications to understand what that project entails and suggest relevant features.\"If you go to those other tools, they have no idea what the heck Project Spartan is,\" Lamanna said, referencing competing low-code platforms from companies like Google, Salesforce, and ServiceNow. \"But if you do it inside of Copilot and inside of the App Builder, it&#x27;s able to draw from all that information and context.\"Microsoft claims the apps created through these tools are \"full-stack applications\" with proper databases secured through the same identity systems used across its enterprise products — distinguishing them from simpler front-end tools offered by competitors. The company also emphasized that its existing governance, security, and data loss prevention policies automatically apply to apps and workflows created through Copilot.Where professional developers still matter in an AI-powered workplaceWhile Microsoft positions the new capabilities as accessible to all office workers, Lamanna was careful to delineate where professional developers remain essential. His dividing line centers on whether a system interacts with parties outside the organization.\"Anything that leaves the boundaries of your company warrants developer involvement,\" he said. \"If you want to build an agent and put it on your website, you should have developers involved. Or if you want to build an automation which interfaces directly with your customers, or an app or a website which interfaces directly with your customers, you want professionals involved.\"The reasoning is risk-based: external-facing systems carry greater potential for data breaches, security vulnerabilities, or business errors. \"You don&#x27;t want people getting refunds they shouldn&#x27;t,\" Lamanna noted.For internal use cases — approval workflows, project tracking, team dashboards — Microsoft believes the new tools can handle the majority of needs without IT department involvement. But the company has built \"no cliffs,\" in Lamanna&#x27;s terminology, allowing users to migrate simple apps to more sophisticated platforms as needs grow.Apps created in the conversational App Builder can be opened in Power Apps, Microsoft&#x27;s full development environment, where they can be connected to Dataverse, the company&#x27;s enterprise database, or extended with custom code. Similarly, simple workflows can graduate to the full Power Automate platform, and basic agents can be enhanced in the complete Copilot Studio.\"We have this mantra called no cliffs,\" Lamanna said. \"If your app gets too complicated for the App Builder, you can always edit and open it in Power Apps. You can jump over to the richer experience, and if you&#x27;re really sophisticated, you can even go from those experiences into Azure.\"This architecture addresses a problem that has plagued previous generations of easy-to-use development tools: users who outgrow the simplified environment often must rebuild from scratch on professional platforms. \"People really do not like easy-to-use development tools if I have to throw everything away and start over,\" Lamanna said.What happens when every employee can build apps without IT approvalThe democratization of software development raises questions about governance, maintenance, and organizational complexity — issues Microsoft has worked to address through administrative controls.IT administrators can view all applications, workflows, and agents created within their organization through a centralized inventory in the Microsoft 365 admin center. They can reassign ownership, disable access at the group level, or \"promote\" particularly useful employee-created apps to officially supported status.\"We have a bunch of customers who have this approach where it&#x27;s like, let 1,000 apps bloom, and then the best ones, I go upgrade and make them IT-governed or central,\" Lamanna said.The system also includes provisions for when employees leave. Apps and workflows remain accessible for 60 days, during which managers can claim ownership — similar to how OneDrive files are handled when someone departs.Lamanna argued that most employee-created apps don&#x27;t warrant significant IT oversight. \"It&#x27;s just not worth inspecting an app that John, Susie, and Bob use to do their job,\" he said. \"It should concern itself with the app that ends up being used by 2,000 people, and that will pop up in that dashboard.\"Still, the proliferation of employee-created applications could create challenges. Users have expressed frustration with Microsoft&#x27;s increasing emphasis on AI features across its products, with some giving the Microsoft 365 mobile app one-star ratings after a recent update prioritized Copilot over traditional file access.The tools also arrive as enterprises grapple with \"shadow IT\" — unsanctioned software and systems that employees adopt without official approval. While Microsoft&#x27;s governance controls aim to provide visibility, the ease of creating new applications could accelerate the pace at which these systems multiply.The ambitious plan to turn 500 million workers into software buildersMicrosoft&#x27;s ambitions for the technology extend far beyond incremental productivity gains. Lamanna envisions a fundamental transformation of what it means to be an office worker — one where building software becomes as routine as creating spreadsheets.\"Just like how 20 years ago you put on your resume that you could use pivot tables in Excel, people are going to start saying that they can use App Builder and workflow agents, even if they&#x27;re just in the finance department or the sales department,\" he said.The numbers he&#x27;s targeting are staggering. With 56 million people already using Power Platform, Lamanna believes the integration into Copilot could eventually reach 500 million builders. \"Early days still, but I think it&#x27;s certainly encouraging,\" he said.The features are currently available only to customers in Microsoft&#x27;s Frontier Program — an early access initiative for Microsoft 365 Copilot subscribers. The company has not disclosed how many organizations participate in the program or when the tools will reach general availability.The announcement fits within Microsoft&#x27;s larger strategy of embedding AI capabilities throughout its product portfolio, driven by its partnership with OpenAI. Under the restructured agreement announced Tuesday, Microsoft will have access to OpenAI&#x27;s technology through 2032, including models that achieve artificial general intelligence (AGI) — though such systems do not yet exist. Microsoft has also begun integrating Copilot into its new companion apps for Windows 11, which provide quick access to contacts, files, and calendar information.The aggressive integration of AI features across Microsoft&#x27;s ecosystem has drawn mixed reactions. While enterprise customers have shown interest in productivity gains, the rapid pace of change and ubiquity of AI prompts have frustrated some users who prefer traditional workflows.For Microsoft, however, the calculation is clear: if even a fraction of its user base begins creating applications and automations, it would represent a massive expansion of the effective software development workforce — and further entrench customers in Microsoft&#x27;s ecosystem. The company is betting that the same natural language interface that made ChatGPT accessible to millions can finally unlock the decades-old promise of empowering everyday workers to build their own tools.The App Builder and Workflows agents are available starting today through the Microsoft 365 Copilot Agent Store for Frontier Program participants.Whether that future arrives depends not just on the technology&#x27;s capabilities, but on a more fundamental question: Do millions of office workers actually want to become part-time software developers? Microsoft is about to find out if the answer is yes — or if some jobs are better left to the professionals.",
          "feed_position": 10,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/33TsQdgl9KxQ34lyEgCUfW/9faff55fd9a871ab3cd21f735ed87cba/nuneybits_Vector_art_of_Microsoft_Windows_desktop_computer_mode_5869d092-9156-48dc-bf50-37d2ff6b0cf3.webp?w=300&q=30"
        }
      ],
      "featured_image": "https://d29szjachogqwa.cloudfront.net/images/user-uploaded/kb_vanguard.jpg",
      "popularity_score": 2019.2354847222223,
      "ai_summary": [
        "The Oakley Meta Vanguard glasses are designed for athletes.",
        "The glasses have wraparound frames and reflective lenses.",
        "The glasses integrate with Strava and Garmin.",
        "The speakers are louder, and the camera has new abilities.",
        "The action button is customizable."
      ]
    },
    {
      "id": "cluster_22",
      "coverage": 2,
      "updated_at": "Thu, 30 Oct 2025 13:25:01 -0400",
      "title": "Canva launches a foundational design model that generates editable layered designs, expands Canva AI, makes Affinity free for all users, and more (Ivan Mehta/TechCrunch)",
      "neutral_headline": "Canva launches a foundational design model that generates editable layered designs, expands Canva AI, makes Affinity free for all users, and more (Ivan Mehta/TechCrunch)",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/251030/p35#a251030p35",
          "published_at": "Thu, 30 Oct 2025 13:25:01 -0400",
          "title": "Canva launches a foundational design model that generates editable layered designs, expands Canva AI, makes Affinity free for all users, and more (Ivan Mehta/TechCrunch)",
          "standfirst": "Ivan Mehta / TechCrunch: Canva launches a foundational design model that generates editable layered designs, expands Canva AI, makes Affinity free for all users, and more &mdash; Creative suite company Canva launched its own design model on Thursday that understands different layers and formats today to power its features.",
          "content": "Ivan Mehta / TechCrunch: Canva launches a foundational design model that generates editable layered designs, expands Canva AI, makes Affinity free for all users, and more &mdash; Creative suite company Canva launched its own design model on Thursday that understands different layers and formats today to power its features.",
          "feed_position": 2,
          "image_url": "http://www.techmeme.com/251030/i35.jpg"
        },
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2025/10/30/canva-launches-its-own-design-model-adds-new-ai-features-to-the-platform/",
          "published_at": "Thu, 30 Oct 2025 17:00:00 +0000",
          "title": "Canva launches its own design model, adds new AI features to the platform",
          "standfirst": "Canva is launching new features like Forms and email design, and it makes Affinity free for all users.",
          "content": "Canva is launching new features like Forms and email design, and it makes Affinity free for all users.",
          "feed_position": 0
        }
      ],
      "featured_image": "http://www.techmeme.com/251030/i35.jpg",
      "popularity_score": 2018.1524291666667,
      "ai_summary": [
        "Canva launched its own design model that understands layers.",
        "The model powers Canva's features.",
        "Canva is expanding its AI capabilities.",
        "Canva is making Affinity free for all users.",
        "The new features include Forms and email design."
      ]
    },
    {
      "id": "cluster_24",
      "coverage": 2,
      "updated_at": "Thu, 30 Oct 2025 13:10:01 -0400",
      "title": "OpenAI launches Aardvark, a GPT-5-powered autonomous cybersecurity research agent that can identify and help patch vulnerabilities, in private beta (Sabrina Ortiz/ZDNET)",
      "neutral_headline": "OpenAI launches Aardvark, a GPT-5-powered autonomous cybersecurity research agent that can identify and help patch vulnerabilities, in private beta (Sabrina Ortiz/ZDNET)",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/251030/p34#a251030p34",
          "published_at": "Thu, 30 Oct 2025 13:10:01 -0400",
          "title": "OpenAI launches Aardvark, a GPT-5-powered autonomous cybersecurity research agent that can identify and help patch vulnerabilities, in private beta (Sabrina Ortiz/ZDNET)",
          "standfirst": "Sabrina Ortiz / ZDNET: OpenAI launches Aardvark, a GPT-5-powered autonomous cybersecurity research agent that can identify and help patch vulnerabilities, in private beta &mdash; ZDNET's key takeaways &mdash; OpenAI has launched Aardvark, a cybersecurity researcher agent. &mdash; Aardvark is powered by GPT-5 and is in private beta.",
          "content": "Sabrina Ortiz / ZDNET: OpenAI launches Aardvark, a GPT-5-powered autonomous cybersecurity research agent that can identify and help patch vulnerabilities, in private beta &mdash; ZDNET's key takeaways &mdash; OpenAI has launched Aardvark, a cybersecurity researcher agent. &mdash; Aardvark is powered by GPT-5 and is in private beta.",
          "feed_position": 3,
          "image_url": "http://www.techmeme.com/251030/i34.jpg"
        },
        {
          "source": "ZDNet",
          "url": "https://www.zdnet.com/article/openai-unveils-aardvark-a-gpt-5-powered-agent-for-autonomous-cybersecurity-research/",
          "published_at": "Thu, 30 Oct 2025 17:00:31 GMT",
          "title": "OpenAI unveils 'Aardvark,' a GPT-5-powered agent for autonomous cybersecurity research",
          "standfirst": "OpenAI said the new cybersecurity agent can identify, explain, and help fix vulnerabilities. Here's how it works.",
          "content": "OpenAI said the new cybersecurity agent can identify, explain, and help fix vulnerabilities. Here's how it works.",
          "feed_position": 8
        }
      ],
      "featured_image": "http://www.techmeme.com/251030/i34.jpg",
      "popularity_score": 2017.9024291666667,
      "ai_summary": [
        "OpenAI launched Aardvark, a cybersecurity research agent.",
        "Aardvark is powered by GPT-5.",
        "The agent is in private beta.",
        "Aardvark can identify and help fix vulnerabilities.",
        "ZDNet provided key takeaways from the launch."
      ]
    },
    {
      "id": "cluster_46",
      "coverage": 2,
      "updated_at": "Thu, 30 Oct 2025 11:30:13 -0400",
      "title": "Extropic, which says its chips using probabilistic bits can be 10,000x more energy efficient than current AI chips, shares its first chip with some AI labs (Will Knight/Wired)",
      "neutral_headline": "Extropic, which says its chips using probabilistic bits can be 10,000x more energy efficient than current AI chips, shares its first chip with some AI labs (Will Knight/Wired)",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/251030/p30#a251030p30",
          "published_at": "Thu, 30 Oct 2025 11:30:13 -0400",
          "title": "Extropic, which says its chips using probabilistic bits can be 10,000x more energy efficient than current AI chips, shares its first chip with some AI labs (Will Knight/Wired)",
          "standfirst": "Will Knight / Wired: Extropic, which says its chips using probabilistic bits can be 10,000x more energy efficient than current AI chips, shares its first chip with some AI labs &mdash; A startup hopes to challenge Nvidia, AMD, and Intel with a chip that wrangles probabilities rather than 1s and 0s.",
          "content": "Will Knight / Wired: Extropic, which says its chips using probabilistic bits can be 10,000x more energy efficient than current AI chips, shares its first chip with some AI labs &mdash; A startup hopes to challenge Nvidia, AMD, and Intel with a chip that wrangles probabilities rather than 1s and 0s.",
          "feed_position": 7,
          "image_url": "http://www.techmeme.com/251030/i30.jpg"
        },
        {
          "source": "Wired Tech",
          "url": "https://www.wired.com/story/extropic-aims-to-disrupt-the-data-center-bonanza/",
          "published_at": "Wed, 29 Oct 2025 17:11:10 +0000",
          "title": "Extropic Aims to Disrupt the Data Center Bonanza",
          "standfirst": "A startup hopes to challenge Nvidia, AMD, and Intel with a chip that wrangles probabilities rather than 1s and 0s.",
          "content": "A startup hopes to challenge Nvidia, AMD, and Intel with a chip that wrangles probabilities rather than 1s and 0s.",
          "feed_position": 29,
          "image_url": "https://media.wired.com/photos/69023ea55216e0070a31d8c6/master/pass/brass_on_wood_2.jpg"
        }
      ],
      "featured_image": "http://www.techmeme.com/251030/i30.jpg",
      "popularity_score": 2016.2390958333333,
      "ai_summary": [
        "Extropic claims its chips are 10,000x more energy efficient.",
        "The chips use probabilistic bits.",
        "The company is sharing its first chip with AI labs.",
        "The startup aims to challenge Nvidia, AMD, and Intel.",
        "The chip wrangles probabilities instead of 1s and 0s."
      ]
    },
    {
      "id": "cluster_57",
      "coverage": 2,
      "updated_at": "Thu, 30 Oct 2025 10:25:02 -0400",
      "title": "Google partners with Reliance to offer free AI Pro access to Jio 5G users in India for 18 months; Perplexity has a similar deal with Bharti Airtel (Jagmeet Singh/TechCrunch)",
      "neutral_headline": "Google partners with Reliance to offer free AI Pro access to Jio 5G users in India for 18 months; Perplexity has a similar deal with Bharti Airtel (Jagmeet Singh/TechCrunch)",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/251030/p26#a251030p26",
          "published_at": "Thu, 30 Oct 2025 10:25:02 -0400",
          "title": "Google partners with Reliance to offer free AI Pro access to Jio 5G users in India for 18 months; Perplexity has a similar deal with Bharti Airtel (Jagmeet Singh/TechCrunch)",
          "standfirst": "Jagmeet Singh / TechCrunch: Google partners with Reliance to offer free AI Pro access to Jio 5G users in India for 18 months; Perplexity has a similar deal with Bharti Airtel &mdash; In a push to expand its AI footprint in emerging markets, Google has partnered with billionaire Mukesh Ambani-led Reliance Industries to bundle &hellip;",
          "content": "Jagmeet Singh / TechCrunch: Google partners with Reliance to offer free AI Pro access to Jio 5G users in India for 18 months; Perplexity has a similar deal with Bharti Airtel &mdash; In a push to expand its AI footprint in emerging markets, Google has partnered with billionaire Mukesh Ambani-led Reliance Industries to bundle &hellip;",
          "feed_position": 11,
          "image_url": "http://www.techmeme.com/251030/i26.jpg"
        },
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2025/10/30/google-partners-with-ambanis-reliance-to-offer-free-ai-pro-access-to-millions-of-jio-users-in-india/",
          "published_at": "Thu, 30 Oct 2025 14:06:19 +0000",
          "title": "Google partners with Ambani&#8217;s Reliance to offer free AI Pro access to millions of Jio users in India",
          "standfirst": "U.S. tech giants increasingly view India as the next big frontier — a place to gather diverse data, refine models, and test AI use cases that could later scale across other emerging markets.",
          "content": "U.S. tech giants increasingly view India as the next big frontier — a place to gather diverse data, refine models, and test AI use cases that could later scale across other emerging markets.",
          "feed_position": 3
        }
      ],
      "featured_image": "http://www.techmeme.com/251030/i26.jpg",
      "popularity_score": 2015.1527069444444,
      "ai_summary": [
        "Google partnered with Reliance to offer free AI Pro access.",
        "The offer is for Jio 5G users in India.",
        "The free access lasts for 18 months.",
        "Perplexity has a similar deal with Bharti Airtel.",
        "Google aims to expand its AI footprint in emerging markets."
      ]
    },
    {
      "id": "cluster_72",
      "coverage": 2,
      "updated_at": "Thu, 30 Oct 2025 09:10:01 -0400",
      "title": "WhatsApp launches passkey-encrypted backups for iOS and Android, letting users encrypt their stored message history using their face, fingerprint, or a code (Jess Weatherbed/The Verge)",
      "neutral_headline": "WhatsApp Adds Passkey Encryption for Backups on iOS and Android",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/251030/p23#a251030p23",
          "published_at": "Thu, 30 Oct 2025 09:10:01 -0400",
          "title": "WhatsApp launches passkey-encrypted backups for iOS and Android, letting users encrypt their stored message history using their face, fingerprint, or a code (Jess Weatherbed/The Verge)",
          "standfirst": "Jess Weatherbed / The Verge: WhatsApp launches passkey-encrypted backups for iOS and Android, letting users encrypt their stored message history using their face, fingerprint, or a code &mdash; &#65279;Chat backup encryption will no longer require you to memorize passwords or lengthy encryption keys.",
          "content": "Jess Weatherbed / The Verge: WhatsApp launches passkey-encrypted backups for iOS and Android, letting users encrypt their stored message history using their face, fingerprint, or a code &mdash; &#65279;Chat backup encryption will no longer require you to memorize passwords or lengthy encryption keys.",
          "feed_position": 14,
          "image_url": "http://www.techmeme.com/251030/i23.jpg"
        },
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2025/10/30/whatsapp-adds-passkey-protection-to-end-to-end-encrypted-backups/",
          "published_at": "Thu, 30 Oct 2025 13:00:00 +0000",
          "title": "WhatsApp adds passkey protection to end-to-end encrypted backups",
          "standfirst": "This means if you lose your device, you can use methods like fingerprint, face, or the screen lock code of your previous device to access WhatsApp's backup.",
          "content": "This means if you lose your device, you can use methods like fingerprint, face, or the screen lock code of your previous device to access WhatsApp's backup.",
          "feed_position": 6
        }
      ],
      "featured_image": "http://www.techmeme.com/251030/i23.jpg",
      "popularity_score": 2013.9024291666667,
      "ai_summary": [
        "WhatsApp now allows passkey-encrypted backups for iOS and Android users.",
        "Users can encrypt message history using face, fingerprint, or device code.",
        "This eliminates the need to remember passwords or lengthy encryption keys.",
        "If a device is lost, users can use their device's security to access backups.",
        "The new feature enhances security and simplifies backup access for users."
      ]
    },
    {
      "id": "cluster_10",
      "coverage": 1,
      "updated_at": "Thu, 30 Oct 2025 18:24:32 +0000",
      "title": "ChatGPT maker reportedly eyes $1 trillion IPO despite major quarterly losses",
      "neutral_headline": "ChatGPT Maker Reportedly Aims for $1 Trillion Initial Public Offering",
      "items": [
        {
          "source": "Ars Technica Main",
          "url": "https://arstechnica.com/ai/2025/10/is-openai-worth-1-trillion-potential-ipo-may-reveal-the-answer/",
          "published_at": "Thu, 30 Oct 2025 18:24:32 +0000",
          "title": "ChatGPT maker reportedly eyes $1 trillion IPO despite major quarterly losses",
          "standfirst": "It could be \"one of the biggest IPOs of all time,\" according to Reuters.",
          "content": "On Tuesday, OpenAI CEO Sam Altman told Reuters during a livestream that going public “is the most likely path for us, given the capital needs that we’ll have.” Now sources familiar with the matter say the ChatGPT maker is preparing for an initial public offering that could value the company at up to $1 trillion, with filings possible as early as the second half of 2026. However, news of the potential IPO comes as the company faces mounting losses that may have reached as much as $11.5 billion in the most recent quarter, according to one estimate. Going public could give OpenAI more efficient access to capital and enable larger acquisitions using public stock, helping finance Altman’s plans to spend trillions of dollars on AI infrastructure, according to people familiar with the company’s thinking who spoke with Reuters. Chief Financial Officer Sarah Friar has reportedly told some associates the company targets a 2027 IPO listing, while some financial advisors predict 2026 could be possible. Three people with knowledge of the plans told Reuters that OpenAI has discussed raising $60 billion at the low end in preliminary talks. That figure refers to how much money the company would raise by selling shares to investors, not the total worth of the company. If OpenAI sold that amount of stock while keeping most shares private, the entire company could be valued at $1 trillion or more. The final figures and timing will likely change based on business growth and market conditions.Read full article Comments",
          "feed_position": 0,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2024/10/openai_treasurechest_1-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2024/10/openai_treasurechest_1-1152x648.jpg",
      "popularity_score": 367.1443736111111,
      "ai_summary": [
        "OpenAI, the maker of ChatGPT, is reportedly considering a $1 trillion IPO.",
        "Reuters reported the potential IPO could be one of the largest ever.",
        "The company has been experiencing significant financial losses recently.",
        "The IPO would represent a major step for the artificial intelligence company.",
        "The valuation reflects the high expectations for the AI industry."
      ]
    },
    {
      "id": "cluster_21",
      "coverage": 1,
      "updated_at": "Thu, 30 Oct 2025 17:28:38 +0000",
      "title": "Disney+ gets HDR10+ via “over 1,000” Hulu titles",
      "neutral_headline": "Disney+ Adds HDR10+ Support for Over One Thousand Hulu Titles",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2025/10/disney-gets-hdr10-through-over-1000-hulu-titles/",
          "published_at": "Thu, 30 Oct 2025 17:28:38 +0000",
          "title": "Disney+ gets HDR10+ via “over 1,000” Hulu titles",
          "standfirst": "Disney+ joins Netflix, Apple TV in supporting the Dolby Vision rival.",
          "content": "Disney+ has started streaming movies and shows in the HDR10+ format. Support is somewhat limited for now. Only certain content from Hulu, which The Walt Disney Company acquired in June, is available in HDR10+. In an announcement today, Samsung said that “over 1,000” Hulu titles are available in HDR10+ and that “additional Disney+” content will support HDR10+ “in the future.” Previously, Disney+ only supported the HDR10 and Dolby Vision HDR formats. Samsung TVs are the first devices to gain the ability to stream HDR10+ content from Disney+, according to an announcement from Samsung today. The electronics company said that its Samsung Crystal UHD TVs and above from 2018 onward, including its OLED TVs, The Frame TVs, QLED TVs, and Micro RGB TV, support HDR10+.Read full article Comments",
          "feed_position": 0,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/fc3d1e70-19cf-4cfa-bb07-bd6439b3df66_1761249301-1152x648-1761844035.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/fc3d1e70-19cf-4cfa-bb07-bd6439b3df66_1761249301-1152x648-1761844035.jpg",
      "popularity_score": 348.21270694444445,
      "ai_summary": [
        "Disney+ now supports HDR10+ for over one thousand Hulu titles.",
        "This brings Disney+ in line with Netflix and Apple TV in HDR support.",
        "HDR10+ offers improved picture quality compared to standard HDR.",
        "The update enhances the viewing experience for compatible devices.",
        "The move expands the range of supported HDR formats on the platform."
      ]
    },
    {
      "id": "cluster_37",
      "coverage": 1,
      "updated_at": "Thu, 30 Oct 2025 16:19:28 +0000",
      "title": "Google makes first Play Store changes after losing Epic Games antitrust case",
      "neutral_headline": "Google Makes Play Store Changes After Losing Antitrust Case",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2025/10/google-begins-loosening-developer-restrictions-in-play-store-against-its-will/",
          "published_at": "Thu, 30 Oct 2025 16:19:28 +0000",
          "title": "Google makes first Play Store changes after losing Epic Games antitrust case",
          "standfirst": "Google is begrudgingly letting developers lead users away from the Play Store.",
          "content": "Since launching Google Play (née Android Market) in 2008, Google has never made a change to the US store that it didn’t want to make—until now. Having lost the antitrust case brought by Epic Games, Google has implemented the first phase of changes mandated by the court. Developers operating in the Play Store will have more freedom to direct app users to resources outside the Google bubble. However, Google has not given up hope of reversing its loss before it’s forced to make bigger changes. Epic began pursuing this case in 2020, stemming from its attempt to sell Fortnite content without going through Google’s payment system. It filed a similar case against Apple, but the company fell short there because it could not show that Apple put its thumb on the scale. Google, however, engaged in conduct that amounted to suppressing the development of alternative Android app stores. It lost the case and came up short on appeal this past summer, leaving the company with little choice but to prepare for the worst. Google has updated its support pages to confirm that it’s abiding by the court’s order. In the US, Play Store developers now have the option of using external payment platforms that bypass the Play Store entirely. This could hypothetically allow developers to offer lower prices, as they don’t have to pay Google’s commission, which can be up to 30 percent. Devs will also be permitted to direct users to sources for app downloads and payment methods outside the Play Store.Read full article Comments",
          "feed_position": 2,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2024/05/google-play-store-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2024/05/google-play-store-1152x648.jpg",
      "popularity_score": 342.0599291666667,
      "ai_summary": [
        "Google is making changes to the Play Store after losing an antitrust case.",
        "Developers can now direct users away from the Play Store.",
        "This change is a result of the legal action brought by Epic Games.",
        "Google is reluctantly allowing developers more freedom in app distribution.",
        "The changes impact how users discover and download applications."
      ]
    },
    {
      "id": "cluster_31",
      "coverage": 1,
      "updated_at": "Thu, 30 Oct 2025 17:00:31 +0000",
      "title": "New study settles 40-year debate: Nanotyrannus is a new species",
      "neutral_headline": "New Study Concludes Nanotyrannus Is a Distinct Species",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2025/10/nanotyrannus-species-confirmed-its-not-just-a-baby-t-rex/",
          "published_at": "Thu, 30 Oct 2025 17:00:31 +0000",
          "title": "New study settles 40-year debate: Nanotyrannus is a new species",
          "standfirst": "\"This fossil doesn't just settle the debate. It flips decades of T. rex research on its head.\"",
          "content": "For four decades, a frequently acrimonious debate has raged in paleontological circles about the correct taxonomy for a handful of rare fossil specimens. One faction insisted the fossils were juvenile Tyrannosaurus rex; the other argued that they represented a new species dubbed Nanotyrannus lancensis. Now, paleontologists believe they have settled the debate once and for all due to a new analysis of a well-preserved fossil. The verdict: It is indeed a new species, according to a new paper published in the journal Nature. The authors also reclassified another specimen as a second new species, distinct from N. lancensis. In short, Nanotyrannus is a valid taxon and contains two species. “This fossil doesn’t just settle the debate,” said Lindsay Zanno, a paleontologist at North Carolina State University and head of paleontology at North Carolina Museum of Natural Sciences. “It flips decades of T. rex research on its head.” That’s because paleontologists have relied on such fossils to model the growth and behavior of T. rex. The new findings suggest that there could have been multiple tyrannosaur species and that paleontologists have been underestimating the diversity of dinosaurs from this period.Read full article Comments",
          "feed_position": 1,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/nanotyrannusTOP-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/nanotyrannusTOP-1152x648.jpg",
      "popularity_score": 340.74409583333335,
      "ai_summary": [
        "A new study concludes that Nanotyrannus is a distinct species.",
        "The study settles a 40-year debate about the dinosaur's classification.",
        "The findings challenge existing research on Tyrannosaurus rex.",
        "The fossil evidence supports the classification of a new species.",
        "The research has significant implications for dinosaur paleontology."
      ]
    },
    {
      "id": "cluster_43",
      "coverage": 1,
      "updated_at": "Thu, 30 Oct 2025 15:54:18 +0000",
      "title": "After teen death lawsuits, Character.AI will restrict chats for under-18 users",
      "neutral_headline": "After teen death lawsuits, Character.AI will restrict chats for under-18 users",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/information-technology/2025/10/after-teen-death-lawsuits-character-ai-will-restrict-chats-for-under-18-users/",
          "published_at": "Thu, 30 Oct 2025 15:54:18 +0000",
          "title": "After teen death lawsuits, Character.AI will restrict chats for under-18 users",
          "standfirst": "AI companion app faces legal and regulatory pressure over child safety concerns.",
          "content": "On Wednesday, Character.AI announced it will bar anyone under the age of 18 from open-ended chats with its AI characters starting on November 25, implementing one of the most restrictive age policies yet among AI chatbot platforms. The company faces multiple lawsuits from families who say its chatbots contributed to teenager deaths by suicide. Over the next month, Character.AI says it will ramp down chatbot use among minors by identifying them and placing a two-hour daily limit on their chatbot access. The company plans to use technology to detect underage users based on conversations and interactions on the platform, as well as information from connected social media accounts. On November 25, those users will no longer be able to create or talk to chatbots, though they can still read previous conversations. The company said it is working to build alternative features for users under the age of 18, such as the ability to create videos, stories, and streams with AI characters. Character.AI CEO Karandeep Anand told The New York Times that the company wants to set an example for the industry. “We’re making a very bold step to say for teen users, chatbots are not the way for entertainment, but there are much better ways to serve them,” Anand said in the interview. The company also plans to establish an AI safety lab.Read full article Comments",
          "feed_position": 3,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/03/robot_no_sign_3-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/03/robot_no_sign_3-1152x648.jpg",
      "popularity_score": 334.6404847222222,
      "ai_summary": [
        "Character.AI will restrict chats for users under eighteen years old.",
        "The decision follows lawsuits related to teen deaths and safety concerns.",
        "The AI companion app faces legal and regulatory pressure.",
        "The restrictions aim to improve child safety on the platform.",
        "The company is responding to concerns about inappropriate content."
      ]
    },
    {
      "id": "cluster_52",
      "coverage": 1,
      "updated_at": "Thu, 30 Oct 2025 14:56:01 +0000",
      "title": "TikTok may become more right-wing as China signals approval for US sale",
      "neutral_headline": "TikTok may become more right-wing as China signals approval for US sale",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/10/us-creeps-closer-to-controlling-tiktok-after-trump-xi-meeting/",
          "published_at": "Thu, 30 Oct 2025 14:56:01 +0000",
          "title": "TikTok may become more right-wing as China signals approval for US sale",
          "standfirst": "Here's how TikTok could change if China approves US sale.",
          "content": "The US inched one step closer to taking over TikTok’s algorithm after President Donald Trump met with Chinese President Xi Jinping on Thursday. Neither leader confirmed that China has agreed to the terms of Trump’s proposed deal, which would create a US version of TikTok that licenses the Chinese-owned algorithm. But the Chinese Commerce Ministry provided a statement following the meeting; translated, it indicates that “China will properly resolve TikTok-related issues with the United States.” Trump, who has long vowed to “save” TikTok, was notably silent on Thursday, but US Treasury Secretary Scott Bessent told Fox News ahead of Trump’s meeting with Xi that “we finalized the TikTok agreement in terms of getting Chinese approval.” According to Bessent, the deal will “finally” be resolved over the “coming weeks and months,” Reuters reported.Read full article Comments",
          "feed_position": 4,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2244071819-1024x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2244071819-1024x648.jpg",
      "popularity_score": 308.66909583333336,
      "ai_summary": [
        "TikTok could change if China approves a sale to a US company.",
        "The potential sale is driven by political and regulatory pressures.",
        "The platform's content and policies could shift significantly.",
        "The changes could reflect the values of the new ownership.",
        "The future of TikTok is uncertain pending the sale's outcome."
      ]
    },
    {
      "id": "cluster_56",
      "coverage": 1,
      "updated_at": "Thu, 30 Oct 2025 14:28:21 +0000",
      "title": "Falling panel prices lead to global solar boom, except for the US",
      "neutral_headline": "Falling Panel Prices Lead to Global Solar Boom, Except in US",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2025/10/theres-a-global-boom-in-solar-except-in-the-united-states/",
          "published_at": "Thu, 30 Oct 2025 14:28:21 +0000",
          "title": "Falling panel prices lead to global solar boom, except for the US",
          "standfirst": "The economic case for solar power is stronger than ever.",
          "content": "To the south of the Monte Cristo mountain range and west of Paymaster Canyon, a vast stretch of the Nevada desert has attracted modern-day prospectors chasing one of 21st-century America’s greatest investment booms. Solar power developers want to cover an area larger than Washington, DC, with silicon panels and batteries, converting sunlight into electricity that will power air conditioners in sweltering Las Vegas along with millions of other homes and businesses. But earlier this month, bureaucrats in charge of federal lands scrapped collective approval for the Esmeralda 7 projects, in what campaigners fear is part of an attack on renewable energy under President Donald Trump. “We will not approve wind or farmer destroying [sic] Solar,” he posted on his Truth Social platform in August. Developers will need to reapply individually, slowing progress.Read full article Comments",
          "feed_position": 5,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/chinawind-solar-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/chinawind-solar-1152x648.jpg",
      "popularity_score": 298.20798472222225,
      "ai_summary": [
        "Falling solar panel prices are driving a global solar boom.",
        "The United States is not experiencing the same level of growth.",
        "The economic case for solar power is stronger than ever.",
        "The US market faces different challenges and regulations.",
        "The global trend highlights the increasing affordability of solar."
      ]
    },
    {
      "id": "cluster_62",
      "coverage": 1,
      "updated_at": "Thu, 30 Oct 2025 14:00:21 +0000",
      "title": "An in-space construction firm says it can help build massive data centers in orbit",
      "neutral_headline": "In-Space Construction Firm Plans Massive Data Centers in Orbit",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2025/10/an-in-space-construction-firm-says-it-can-help-build-massive-data-centers-in-orbit/",
          "published_at": "Thu, 30 Oct 2025 14:00:21 +0000",
          "title": "An in-space construction firm says it can help build massive data centers in orbit",
          "standfirst": "\"Size is not the limit anymore.\"",
          "content": "There has been much discussion in the space community recently about building large data centers in orbit to avoid the environmental consequences of sprawling computing facilities on Earth. These space-based data centers could take advantage of the always-on, free fusion reactor at the center of the Solar System. Proponents say this represents a natural step in the evolution of moving heavy industry off the planet’s surface and a solution for the ravenous energy needs of artificial intelligence. Critics say building data centers in space is technically very challenging and cite major hurdles, such as radiating away large amounts of heat and the cost of accessing space. It is unclear who is right, but one thing is certain: Such facilities would need to be massive to support artificial intelligence.Read full article Comments",
          "feed_position": 6,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/FinalMission_Solar_WEB-1152x648.webp"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/FinalMission_Solar_WEB-1152x648.webp",
      "popularity_score": 295.74131805555555,
      "ai_summary": [
        "An in-space construction firm plans to build data centers in orbit.",
        "The firm believes size is no longer a limiting factor in space construction.",
        "The project aims to leverage the unique environment of space.",
        "The data centers would offer advantages in cooling and power.",
        "The initiative represents a new frontier in data center technology."
      ]
    },
    {
      "id": "cluster_63",
      "coverage": 1,
      "updated_at": "Thu, 30 Oct 2025 13:54:33 +0000",
      "title": "GM lays off 1,700 workers making EVs and batteries in Michigan, Tennessee",
      "neutral_headline": "GM Lays Off 1,700 Workers Making EVs and Batteries",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/cars/2025/10/weak-ev-demand-sees-gm-lay-off-1700-workers-at-two-plants/",
          "published_at": "Thu, 30 Oct 2025 13:54:33 +0000",
          "title": "GM lays off 1,700 workers making EVs and batteries in Michigan, Tennessee",
          "standfirst": "The automaker expects the regulatory environment to seriously slow EV demand.",
          "content": "Just a few weeks ago, automakers were celebrating a healthy third quarter for electric vehicle sales. General Motors was looking particularly flush, with EV sales up 104 percent for the year to date compared to the first nine months of 2024. But the strong EV sales in Q3 were seemingly due to the imminent end of the federal tax credit that expired at the end of September, with many consumers buying a car sooner than planned to take advantage of the $7,500 incentive. The Trump administration has been altering the regulatory environment in other ways to discourage clean technologies, canceling infrastructure initiatives and turning a blind eye to pollution. On top of that, the impact of the president’s chaotic trade war has driven up prices and is cooling demand. Two weeks ago, GM told investors that things are looking so bad that it will take a $1.6 billion hit to its bank accounts as it realigns manufacturing capacity going forward. Now we can see some of the impact of that realignment. According to The Detroit News, 1,200 workers are being laid off at GM’s EV-building Hamtramck Assembly Center near Detroit, which will move from two shifts a day to just one in early January.Read full article Comments",
          "feed_position": 7,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-1196823844-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-1196823844-1152x648.jpg",
      "popularity_score": 283.64465138888886,
      "ai_summary": [
        "General Motors is laying off 1,700 workers in Michigan and Tennessee.",
        "The layoffs affect employees involved in EV and battery production.",
        "The automaker expects a slowdown in EV demand due to regulations.",
        "The decision reflects the changing landscape of the automotive industry.",
        "The layoffs impact the company's workforce and production plans."
      ]
    },
    {
      "id": "cluster_84",
      "coverage": 1,
      "updated_at": "Thu, 30 Oct 2025 11:00:37 +0000",
      "title": "Halloween film fest: 15 classic ghost stories",
      "neutral_headline": "Halloween Film Fest: Fifteen Classic Ghost Stories",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/features/2025/10/halloween-film-fest-15-classic-ghost-stories/",
          "published_at": "Thu, 30 Oct 2025 11:00:37 +0000",
          "title": "Halloween film fest: 15 classic ghost stories",
          "standfirst": "From The Uninvited to Crimson Peak, these films will help you set the tone for spooky season.",
          "content": "It’s spooky season, and what better way to spend Halloween weekend than settling in to watch a classic Hollywood ghost story? To help you figure out what to watch, we’ve compiled a handy list of 15 classic ghost stories, presented in chronological order. What makes a good ghost story? Everyone’s criteria (and taste) will differ, but for this list, we’ve focused on more traditional elements. There’s usually a spooky old house with a ghostly presence and/or someone who’s attuned to said presence. The living must solve the mystery of what happened to trap the ghost(s) there in hopes of setting said ghost(s) free. In that sense, the best, most satisfying ghost stories are mysteries—and sometimes also love stories. The horror is more psychological, and when it comes to gore, less is usually more. As always, the list below isn’t meant to be exhaustive. Mostly, we’re going for a certain atmospheric vibe to set a mood. So our list omits overt comedies like Ghostbusters and (arguably) Ghost, as well as supernatural horror involving demonic possession—The Exorcist, The Conjuring, Insidious—or monsters, like The Babadook or Sinister. Feel free to suggest your own recommendations in the comments.Read full article Comments",
          "feed_position": 8,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/ghostTOP-1152x648-1759590541.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/ghostTOP-1152x648-1759590541.jpg",
      "popularity_score": 261.7457625,
      "ai_summary": [
        "Ars Technica recommends fifteen classic ghost stories for Halloween.",
        "The films range from \"The Uninvited\" to \"Crimson Peak.\"",
        "The selections aim to set the tone for the spooky season.",
        "The list provides a curated guide to classic horror films.",
        "The films offer a variety of styles and themes within the genre."
      ]
    },
    {
      "id": "cluster_111",
      "coverage": 1,
      "updated_at": "Wed, 29 Oct 2025 21:35:58 +0000",
      "title": "Meta denies torrenting porn to train AI, says downloads were for “personal use”",
      "neutral_headline": "Meta denies torrenting porn to train AI, says downloads were for “personal use”",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/10/meta-says-porn-downloads-on-its-ips-were-for-personal-use-not-ai-training/",
          "published_at": "Wed, 29 Oct 2025 21:35:58 +0000",
          "title": "Meta denies torrenting porn to train AI, says downloads were for “personal use”",
          "standfirst": "Meta says lawsuit claiming it pirated porn to train AI makes no sense.",
          "content": "This week, Meta asked a US district court to toss a lawsuit alleging that the tech giant illegally torrented pornography to train AI. The move comes after Strike 3 Holdings discovered illegal downloads of some of its adult films on Meta corporate IP addresses, as well as other downloads that Meta allegedly concealed using a “stealth network” of 2,500 “hidden IP addresses.” Accusing Meta of stealing porn to secretly train an unannounced adult version of its AI model powering Movie Gen, Strike 3 sought damages that could have exceeded $350 million, TorrentFreak reported. Filing a motion to dismiss the lawsuit on Monday, Meta accused Strike 3 of relying on “guesswork and innuendo,” while writing that Strike 3 “has been labeled by some as a ‘copyright troll’ that files extortive lawsuits.” Requesting that all copyright claims be dropped, Meta argued that there was no evidence that the tech giant directed any of the downloads of about 2,400 adult movies owned by Strike 3—or was even aware of the illegal activity.Read full article Comments",
          "feed_position": 9,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2171230457-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2171230457-1152x648.jpg",
      "popularity_score": 258,
      "ai_summary": [
        "Meta denies allegations of torrenting porn to train its AI models.",
        "The company claims the downloads were for \"personal use.\"",
        "Meta is responding to a lawsuit claiming copyright infringement.",
        "The lawsuit alleges the company pirated pornographic content.",
        "The case raises questions about AI training data practices."
      ]
    },
    {
      "id": "cluster_137",
      "coverage": 1,
      "updated_at": "Wed, 29 Oct 2025 16:37:06 +0000",
      "title": "TV-focused YouTube update brings AI upscaling, shopping QR codes",
      "neutral_headline": "TV-focused YouTube update brings AI upscaling, shopping QR codes",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2025/10/tv-focused-youtube-update-brings-ai-upscaling-shopping-qr-codes/",
          "published_at": "Wed, 29 Oct 2025 16:37:06 +0000",
          "title": "TV-focused YouTube update brings AI upscaling, shopping QR codes",
          "standfirst": "YouTube seeks a more couch-friendly experience.",
          "content": "YouTube has been streaming for 20 years, but it was only in the last couple that it came to dominate TV streaming. Google’s video platform attracts more TV viewers than Netflix, Disney+, and all the other apps, and Google is looking to further beef up its big-screen appeal with a new raft of features, including shopping, immersive channel surfing, and an official version of the AI upscaling that had creators miffed a few months back. According to Google, YouTube’s growth has translated into higher payouts. The number of channels earning more than $100,000 annually is up 45 percent in 2025 versus 2024. YouTube is now giving creators some tools to boost their appeal (and hopefully their income) on TV screens. Those elaborate video thumbnails featuring surprised, angry, smiley hosts are about to get even prettier with the new 50MB file size limit. That’s up from a measly 2MB. Video upscaling is also coming to YouTube, and creators will be opted in automatically. To start, YouTube will be upscaling lower-quality videos to 1080p. In the near future, Google plans to support “super resolution” up to 4K.Read full article Comments",
          "feed_position": 15,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2021/12/getty-youtube-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2021/12/getty-youtube-1152x648.jpg",
      "popularity_score": 148,
      "ai_summary": [
        "YouTube is updating its platform with features for TV viewing.",
        "The update includes AI upscaling and shopping QR codes.",
        "The changes aim to improve the user experience on televisions.",
        "The platform is adapting to the growing popularity of TV viewing.",
        "The update enhances the features available on connected TVs."
      ]
    },
    {
      "id": "cluster_146",
      "coverage": 1,
      "updated_at": "Wed, 29 Oct 2025 14:46:21 +0000",
      "title": "Nvidia hits record $5 trillion mark as CEO dismisses AI bubble concerns",
      "neutral_headline": "Nvidia hits record $5 trillion mark as CEO dismisses AI bubble concerns",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/ai/2025/10/nvidia-hits-record-5-trillion-mark-as-ceo-dismisses-ai-bubble-concerns/",
          "published_at": "Wed, 29 Oct 2025 14:46:21 +0000",
          "title": "Nvidia hits record $5 trillion mark as CEO dismisses AI bubble concerns",
          "standfirst": "\"I don’t believe we’re in an AI bubble,\" says Huang after announcing $500B in orders.",
          "content": "On Wednesday, Nvidia became the first company in history to reach a $5 trillion market capitalization, fresh on the heels of a GTC conference keynote in Washington, DC, where CEO Jensen Huang announced $500 billion in AI chip orders and plans to build seven supercomputers for the US government. The milestone comes a mere three months after Nvidia crossed the $4 trillion mark in July, vaulting the company past tech giants like Apple and Microsoft in market valuation but also driving continued fears of an AI investment bubble. Nvidia’s shares have climbed nearly 12-fold since the launch of ChatGPT in late 2022, as the AI boom propelled the S&P 500 to record highs. Shares of Nvidia stock rose 4.6 percent on Wednesday following the Tuesday announcement at the company’s GTC conference. During a Bloomberg Television interview at the event, Huang dismissed concerns about overheated valuations, saying, “I don’t believe we’re in an AI bubble. All of these different AI models we’re using—we’re using plenty of services and paying happily to do it.” Nvidia expects to ship 20 million units of its latest chips, compared to just 4 million units of the previous Hopper generation over its entire lifetime, Huang said at the conference. The $500 billion figure represents cumulative orders for the company’s Blackwell and Rubin processors through the end of 2026, though Huang noted that his projections did not include potential sales to China.Read full article Comments",
          "feed_position": 17,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2024/02/nvidia_flag_2-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2024/02/nvidia_flag_2-1152x648.jpg",
      "popularity_score": 148,
      "ai_summary": [
        "Nvidia's market capitalization reached a record $5 trillion.",
        "The CEO dismissed concerns about an artificial intelligence bubble.",
        "The company announced $500 billion in orders.",
        "The company is a major player in the AI hardware market.",
        "The valuation reflects the strong demand for AI technology."
      ]
    },
    {
      "id": "cluster_112",
      "coverage": 1,
      "updated_at": "Wed, 29 Oct 2025 21:23:22 +0000",
      "title": "Space station astronauts eager to open “golden treasure box” from Japan",
      "neutral_headline": "Space station astronauts eager to open “golden treasure box” from Japan",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2025/10/space-station-astronauts-eager-to-open-golden-treasure-box-from-japan/",
          "published_at": "Wed, 29 Oct 2025 21:23:22 +0000",
          "title": "Space station astronauts eager to open “golden treasure box” from Japan",
          "standfirst": "\"This spacecraft is so beautiful and shiny, and this is representing our bright future.\"",
          "content": "A cargo ship from Japan pulled alongside the International Space Station on Wednesday, maneuvering close enough for the lab’s robotic arm to reach out and grab it as the vehicles soared 260 miles over the South Atlantic Ocean. “HTV capture complete,” Japanese astronaut Kimiya Yui radioed from the ISS. “I just want to say congratulations to all teams and people involved in this mission. Also, thank you very much for your hard work and support for the first HTV-X mission.” The HTV-X spacecraft is an upgraded cargo freighter replacing Japan’s H-II Transfer Vehicle, which successfully resupplied the space station nine times between 2009 and 2020. At the conclusion of the HTV program, Japan’s space agency preferred to focus its resources on designing a new cargo ship with more capability at a lower cost. That’s what HTV-X is supposed to be, and Wednesday’s high-flying rendezvous marked the new ship’s first delivery to the ISS.Read full article Comments",
          "feed_position": 10,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/htvx1-arrive1-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/htvx1-arrive1-1152x648.jpg",
      "popularity_score": 141,
      "ai_summary": [
        "Astronauts on the space station are excited to open a Japanese cargo.",
        "The spacecraft is described as beautiful and shiny.",
        "The cargo is seen as representing a bright future.",
        "The mission involves international collaboration.",
        "The event highlights the ongoing work in space exploration."
      ]
    },
    {
      "id": "cluster_113",
      "coverage": 1,
      "updated_at": "Wed, 29 Oct 2025 21:04:45 +0000",
      "title": "NPM flooded with malicious packages downloaded more than 86,000 times",
      "neutral_headline": "Malicious Packages Flooded NPM, Downloaded Over Eighty-Six Thousand Times",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/security/2025/10/npm-flooded-with-malicious-packages-downloaded-more-than-86000-times/",
          "published_at": "Wed, 29 Oct 2025 21:04:45 +0000",
          "title": "NPM flooded with malicious packages downloaded more than 86,000 times",
          "standfirst": "Packages downloaded from NPM can fetch dependencies from untrusted sites.",
          "content": "Attackers are exploiting a major weakness that has allowed them access to the NPM code repository with more than 100 credential-stealing packages since August, mostly without detection. The finding, laid out Wednesday by security firm Koi, brings attention to an NPM practice that allows installed packages to automatically pull down and run unvetted packages from untrusted domains. Koi said a campaign it tracks as PhantomRaven has exploited NPM’s use of “Remote Dynamic Dependencies” to flood NPM with 126 malicious packages that have been downloaded more than 86,000 times. Some 80 of those packages remained available as of Wednesday morning, Koi said. A blind spot “PhantomRaven demonstrates how sophisticated attackers are getting [better] at exploiting blind spots in traditional security tooling,” Koi’s Oren Yomtov wrote. “Remote Dynamic Dependencies aren’t visible to static analysis.”Read full article Comments",
          "feed_position": 11,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2022/05/caution-tape-1000x648.jpeg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2022/05/caution-tape-1000x648.jpeg",
      "popularity_score": 133,
      "ai_summary": [
        "Malicious packages on NPM fetched dependencies from untrusted sources, posing security risks.",
        "These packages were downloaded more than 86,000 times, indicating widespread exposure.",
        "The packages' malicious nature could compromise systems of developers and users.",
        "The incident highlights the importance of verifying package sources and dependencies.",
        "This event underscores the need for improved security measures within the NPM ecosystem."
      ]
    },
    {
      "id": "cluster_119",
      "coverage": 1,
      "updated_at": "Wed, 29 Oct 2025 19:44:40 +0000",
      "title": "Trump health official ousted after allegedly giving himself a fake title",
      "neutral_headline": "Trump Health Official Ousted After Allegedly Using Fake Title",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/health/2025/10/trump-health-official-ousted-after-allegedly-giving-himself-a-fake-title/",
          "published_at": "Wed, 29 Oct 2025 19:44:40 +0000",
          "title": "Trump health official ousted after allegedly giving himself a fake title",
          "standfirst": "Steven Hatfill had a notable history before his abrupt ouster.",
          "content": "Steven Hatfill, a senior advisor for the Department of Health and Human Services was fired over the weekend, with health officials telling reporters that he was terminated for giving himself a fake, inflated title and for not cooperating with leadership. For his part, Hatfill told The New York Times that his ouster was part of “a coup to overthrow M. Kennedy,” referring to anti-vaccine Health Secretary Robert F. Kennedy Jr. Further, Hatfill said the coup was being orchestrated by Matt Buckham, Kennedy’s chief of staff, though Hatfill didn’t provide any explanation of how his ouster was evidence of that. An HHS spokesperson responded to the allegation, telling the Times that “firing a staff member for cause does not add up to a coup.” Bloomberg was first to report Hatfill’s termination.Read full article Comments",
          "feed_position": 12,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-122179143-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-122179143-1152x648.jpg",
      "popularity_score": 133,
      "ai_summary": [
        "Steven Hatfill, a Trump health official, was removed from his position.",
        "The ousting followed allegations of Hatfill using a fabricated title.",
        "Hatfill had a notable history before his abrupt departure from the role.",
        "Details surrounding the fake title and the circumstances remain unclear.",
        "The incident raises questions about the vetting process for government officials."
      ]
    },
    {
      "id": "cluster_121",
      "coverage": 1,
      "updated_at": "Wed, 29 Oct 2025 19:18:37 +0000",
      "title": "FCC Republicans force prisoners and families to pay more for phone calls",
      "neutral_headline": "FCC Republicans Increase Prisoner Phone Call Costs, Families Pay More",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/10/fcc-republicans-force-prisoners-and-families-to-pay-more-for-phone-calls/",
          "published_at": "Wed, 29 Oct 2025 19:18:37 +0000",
          "title": "FCC Republicans force prisoners and families to pay more for phone calls",
          "standfirst": "Democrat: FCC \"rewards corporations with money taken from vulnerable families.\"",
          "content": "The Federal Communications Commission voted yesterday to raise the maximum prices that prison and jail phone services can charge inmates and their families. The 2–1 vote with Republicans voting to raise the limits came with a dissent from Democrat Anna Gomez, who said the new rates will be “almost double in some facilities.” A new inflation factor will allow rates to rise further. “The FCC once again is going above and beyond to address the unsubstantiated needs of monopoly providers to squeeze every penny possible from families that want to stay in touch with their loved ones,” Gomez said at the FCC meeting. “Throughout this order, the FCC chooses to reward corporations with money taken from vulnerable families.”Read full article Comments",
          "feed_position": 13,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/brendan-carr-fcc-1152x648-1761764153.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/brendan-carr-fcc-1152x648-1761764153.jpg",
      "popularity_score": 133,
      "ai_summary": [
        "FCC Republicans are accused of increasing phone call costs for prisoners.",
        "This action is said to negatively impact families of incarcerated individuals.",
        "A Democrat criticized the FCC, claiming it rewards corporations.",
        "The Democrat stated the FCC takes money from vulnerable families.",
        "The policy change has sparked debate over fairness and corporate influence."
      ]
    },
    {
      "id": "cluster_124",
      "coverage": 1,
      "updated_at": "Wed, 29 Oct 2025 18:29:45 +0000",
      "title": "ICE’s forced face scans to verify citizens is unconstitutional, lawmakers say",
      "neutral_headline": "Lawmakers Say ICE Face Scans to Verify Citizens Are Unconstitutional",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/10/ices-forced-face-scans-to-verify-citizens-is-unconstitutional-lawmakers-say/",
          "published_at": "Wed, 29 Oct 2025 18:29:45 +0000",
          "title": "ICE’s forced face scans to verify citizens is unconstitutional, lawmakers say",
          "standfirst": "Videos show ICE conducting random face scans on US streets.",
          "content": "Social media videos have confirmed that Immigration and Customs Enforcement (ICE) and Customs and Border Protection (CBP) officers patrolling US streets are actively using facial recognition technology to verify citizenship, 404 Media reported. In one video posted on a Chicago-based Instagram account, a self-described teenager and US citizen tells officers that he has no government ID. After he offers to show his student ID instead, the officer turns to another and asks, “can you do facial?” As the other officer pulls up an app to scan the teen’s face, the first officer tells the teenager to “relax” while alleging that “a lot of parents” tell their kids they were born in the US. The video ends after the officer takes the minor’s photo and asks the teen to verify that his name matches what the app’s database pulled up. It’s unclear which app the officers used during this Chicago stop. But 404 Media has been closely tracking ICE and CBP’s increasing use of face scans amid the Trump administration’s nationwide mass deportation campaign, which critics slam as largely rooted in racial profiling. Earlier this year, 404 Media reviewed leaked emails confirming that ICE was using Mobile Fortify, which allows agents to scan “an unprecedented number of government databases” and compare face matches against a database of 200 million images.Read full article Comments",
          "feed_position": 14,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2238024951-1024x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2238024951-1024x648.jpg",
      "popularity_score": 133,
      "ai_summary": [
        "Lawmakers are challenging ICE's use of face scans to verify citizens.",
        "Videos show ICE conducting random face scans on US streets.",
        "The lawmakers argue that the practice violates constitutional rights.",
        "The legality of ICE's actions is now under scrutiny and debate.",
        "The issue raises concerns about privacy and government surveillance."
      ]
    },
    {
      "id": "cluster_139",
      "coverage": 1,
      "updated_at": "Wed, 29 Oct 2025 16:20:59 +0000",
      "title": "The chemistry behind that pricey cup of civet coffee",
      "neutral_headline": "Chemistry Behind Expensive Civet Coffee Analyzed, Backing Claims",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2025/10/fermentation-is-key-to-coffee-beans-gleaned-from-civet-feces/",
          "published_at": "Wed, 29 Oct 2025 16:20:59 +0000",
          "title": "The chemistry behind that pricey cup of civet coffee",
          "standfirst": "Fans of kopi luwak claim the coffee has a unique aroma and taste. A new chemical analysis backs them up.",
          "content": "In 2007’s The Bucket List, Jack Nicholson’s billionaire magnate is a fan of a luxury coffee called kopi luwak, only to be informed that the beans first pass through the digestive tracts of civets and are harvested from their feces prior to roasting. The implication is that the billionaire just liked drinking gimmicky expensive coffee without realizing its less-than-luxurious origins. It’s one of the most expensive coffees in the world, ranging from $45 per pound to $590 per pound, depending on whether the beans are farmed or collected in the wild. Whether kopi luwak is worth that hefty price tag depends on who you ask. A Washington Post food critic once compared the beverage to stale Folgers, memorably describing the flavor as “petrified dinosaur droppings steeped in bathtub water.” Yet kopi luwak has many genuine fans who claim the coffee has a unique aroma and taste. Based on a new chemical analysis, they might have a point, according to a paper published in Scientific Reports. Technically, kopi luwak is a method of processing, not a specific coffee bean variety. Asian palm civets hang around coffee plantations because they love to feast on ripened coffee berries; the berries constitute most of their diet, along with various seeds. The consumed berries undergo fermentation as they pass through the animal’s intestines, and the civets digest the pulp and excrete the beans. Coffee farmers then collect the scat to recover the excreted beans and process and roast them to produce kopi luwak.Read full article Comments",
          "feed_position": 16,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/civet2-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/civet2-1152x648.jpg",
      "popularity_score": 133,
      "ai_summary": [
        "Kopi luwak coffee is claimed to have a unique aroma and taste.",
        "A new chemical analysis supports these claims about the coffee.",
        "The analysis explores the chemical compounds contributing to the flavor.",
        "The study investigates the impact of the civet cat's digestive process.",
        "The findings may help understand and replicate the coffee's qualities."
      ]
    },
    {
      "id": "cluster_148",
      "coverage": 1,
      "updated_at": "Wed, 29 Oct 2025 14:28:19 +0000",
      "title": "Man accidentally gets leech up his nose. It took 20 days to figure it out.",
      "neutral_headline": "Man Accidentally Gets Leech Up Nose, Takes Twenty Days to Discover",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/health/2025/10/man-accidentally-gets-leech-up-his-nose-it-took-20-days-to-figure-it-out/",
          "published_at": "Wed, 29 Oct 2025 14:28:19 +0000",
          "title": "Man accidentally gets leech up his nose. It took 20 days to figure it out.",
          "standfirst": "Leeches have a long medical history. Here's what happens if one gets in your nose.",
          "content": "Since the dawn of civilization, leeches have been firmly attached to medicine. Therapeutic bloodsuckers are seen in murals decorating the tombs of 18th dynasty Egyptian pharaohs. They got their earliest written recommendation in the 2nd century BC by Greek poet and physician Nicander of Colophon. He introduced the “blood-loving leech, long flaccid and yearning for gore,” as a useful tool for sucking out poison after a bite from a poisonous animal. “Let leeches feed on [the] wounds and drink their fill,” he wrote. Ancient Chinese writing touted their medicinal potential, too, as did references in Sanskrit. Galen, the physician for Roman Emperor Marcus Aurelius, supported using leeches to balance the four humors (i.e. blood, phlegm, and yellow and black bile) and therefore treat ailments—as initially outlined by Hippocrates. Leeches, doctors found, provided a method for less painful, localized, and limited bloodletting. We now understand that leeches can release an anesthetic to prevent pain and a powerful anticoagulant, hirudin, to prevent clotting and keep blood flowing. In the centuries since the Roman era, leeches’ popularity only grew. They were used to treat everything from gout to liver disease, epilepsy, and melancholy. The very word “leech” is derived from the Anglo-Saxon word “laece,” which translates to “physician.”Read full article Comments",
          "feed_position": 18,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2220375814-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2220375814-1152x648.jpg",
      "popularity_score": 133,
      "ai_summary": [
        "A man accidentally had a leech enter his nose.",
        "It took twenty days for the man to realize the leech was present.",
        "Leeches have a long history of use in medical practices.",
        "The article details what happens when a leech enters the nose.",
        "The incident highlights the potential for unusual medical occurrences."
      ]
    },
    {
      "id": "cluster_150",
      "coverage": 1,
      "updated_at": "Wed, 29 Oct 2025 13:40:15 +0000",
      "title": "New physical attacks are quickly diluting secure enclave defenses from Nvidia, AMD, and Intel",
      "neutral_headline": "New Physical Attacks Dilute Secure Enclave Defenses",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/security/2025/10/new-physical-attacks-are-quickly-diluting-secure-enclave-defenses-from-nvidia-amd-and-intel/",
          "published_at": "Wed, 29 Oct 2025 13:40:15 +0000",
          "title": "New physical attacks are quickly diluting secure enclave defenses from Nvidia, AMD, and Intel",
          "standfirst": "On-chip TEEs withstand rooted OSes but fall instantly to cheap physical attacks.",
          "content": "Trusted execution environments, or TEEs, are everywhere—in blockchain architectures, virtually every cloud service, and computing involving AI, finance, and defense contractors. It’s hard to overstate the reliance that entire industries have on three TEEs in particular: Confidential Compute from Nvidia, SEV-SNP from AMD, and SGX and TDX from Intel. All three come with assurances that confidential data and sensitive computing can’t be viewed or altered, even if a server has suffered a complete compromise of the operating kernel. A trio of novel physical attacks raises new questions about the true security offered by these TEES and the exaggerated promises and misconceptions coming from the big and small players using them. The most recent attack, released Tuesday, is known as TEE.fail. It defeats the latest TEE protections from all three chipmakers. The low-cost, low-complexity attack works by placing a small piece of hardware between a single physical memory chip and the motherboard slot it plugs into. It also requires the attacker to compromise the operating system kernel. Once this three-minute attack is completed, Confidential Compute, SEV-SNP, and TDX/SDX can no longer be trusted. Unlike the Battering RAM and Wiretap attacks from last month—which worked only against CPUs using DDR4 memory—TEE.fail works against DDR5, allowing them to work against the latest TEEs.Read full article Comments",
          "feed_position": 19,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/cpu-key-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/cpu-key-1152x648.jpg",
      "popularity_score": 133,
      "ai_summary": [
        "New physical attacks are quickly diluting secure enclave defenses.",
        "These attacks target defenses from Nvidia, AMD, and Intel.",
        "On-chip TEEs withstand rooted OSes but fall to physical attacks.",
        "The attacks are described as cheap and easily executed.",
        "The findings raise concerns about the security of hardware."
      ]
    }
  ]
}