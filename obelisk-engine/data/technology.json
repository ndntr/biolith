{
  "updated_at": "2026-01-13T19:20:19.217Z",
  "clusters": [
    {
      "id": "cluster_23",
      "coverage": 3,
      "updated_at": "Tue, 13 Jan 2026 13:15:02 -0500",
      "title": "The US FCC waives a rule that forced Verizon to unlock phones 60 days after they are activated, which could make it harder for people to switch from Verizon (Jon Brodkin/Ars Technica)",
      "neutral_headline": "Verizon gets FCC permission to end 60-day phone unlocking rule",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/260113/p37#a260113p37",
          "published_at": "Tue, 13 Jan 2026 13:15:02 -0500",
          "title": "The US FCC waives a rule that forced Verizon to unlock phones 60 days after they are activated, which could make it harder for people to switch from Verizon (Jon Brodkin/Ars Technica)",
          "standfirst": "Jon Brodkin / Ars Technica: The US FCC waives a rule that forced Verizon to unlock phones 60 days after they are activated, which could make it harder for people to switch from Verizon &mdash; The Federal Communications Commission is letting Verizon lock phones to its network for longer periods, eliminating a requirement &hellip;",
          "content": "Jon Brodkin / Ars Technica: The US FCC waives a rule that forced Verizon to unlock phones 60 days after they are activated, which could make it harder for people to switch from Verizon &mdash; The Federal Communications Commission is letting Verizon lock phones to its network for longer periods, eliminating a requirement &hellip;",
          "feed_position": 2,
          "image_url": "http://www.techmeme.com/260113/i37.jpg"
        },
        {
          "source": "The Verge",
          "url": "https://www.theverge.com/news/861349/verizon-fcc-phone-unlocking-60-day-requirement-waived",
          "published_at": "2026-01-13T12:48:03-05:00",
          "title": "Verizon gets FCC permission to end 60-day phone unlocking rule",
          "standfirst": "Verizon can keep phones locked to its network for longer after the Federal Communications Commission agreed to waive the carrier's 60-day unlocking requirement, as reported by Ars Technica. Following this decision, Verizon must follow a looser set of guidelines set by the CTIA wireless trade group, which says carriers should only unlock a customer's postpaid [&#8230;]",
          "content": "Verizon can keep phones locked to its network for longer after the Federal Communications Commission agreed to waive the carrier's 60-day unlocking requirement, as reported by Ars Technica. Following this decision, Verizon must follow a looser set of guidelines set by the CTIA wireless trade group, which says carriers should only unlock a customer's postpaid phone after their contract is up, when they finish paying off the device, or following the payment of an early termination fee. Meanwhile, the CTIA's code says carriers should unlock prepaid phones \"no later than one year after initial activation.\" The change comes after Verizon asked … Read the full story at The Verge.",
          "feed_position": 2
        },
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2026/01/fcc-lets-verizon-lock-phones-for-longer-making-it-harder-to-switch-carriers/",
          "published_at": "Mon, 12 Jan 2026 22:07:23 +0000",
          "title": "Verizon to stop automatic unlocking of phones as FCC ends 60-day unlock rule",
          "standfirst": "FCC waives rule that forced Verizon to unlock phones 60 days after activation.",
          "content": "The Federal Communications Commission is letting Verizon lock phones to its network for longer periods, eliminating a requirement to unlock handsets 60 days after they are activated on its network. The change will make it harder for people to switch from Verizon to other carriers. The FCC today granted Verizon's petition for a waiver of the 60-day unlocking requirement. While the waiver is in effect, Verizon only has to comply with the CTIA trade group's voluntary unlocking policy. The CTIA policy calls for unlocking prepaid mobile devices one year after activation, while devices on postpaid plans can be unlocked after a contract, device financing plan, or early termination fee is paid. Unlocking a phone allows it to be used on another carrier's network. While Verizon was previously required to unlock phones automatically after 60 days, the CTIA code says carriers only have to unlock phones \"upon request\" from consumers. The FCC said the Verizon waiver will remain in effect until the agency \"decides on an appropriate industry-wide approach for the unlocking of handsets.\"Read full article Comments",
          "feed_position": 11,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/verizon-jerks-locked-phone-1152x648-1765486982.jpg"
        }
      ],
      "featured_image": "http://www.techmeme.com/260113/i37.jpg",
      "popularity_score": 3018.911884166667
    },
    {
      "id": "cluster_10",
      "coverage": 2,
      "updated_at": "Tue, 13 Jan 2026 18:54:00 +0000",
      "title": "Taiwan issues arrest warrant for Pete Lau, CEO of OnePlus",
      "neutral_headline": "Taiwan issues arrest warrant for Pete Lau, CEO of OnePlus",
      "items": [
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/mobile/smartphones/taiwan-issues-arrest-warrant-for-pete-lau-ceo-of-oneplus-185400997.html",
          "published_at": "Tue, 13 Jan 2026 18:54:00 +0000",
          "title": "Taiwan issues arrest warrant for Pete Lau, CEO of OnePlus",
          "standfirst": "Taiwanese officials have issued an arrest warrant for OnePlus CEO Pete Lau on allegations of illegally employing workers in Taiwan. Two Taiwanese citizens who worked for Lau have also been indicted. The China-based smartphone company has been accused of illegally recruiting more than 70 engineers from Taiwan. Members of the Shilin District Prosecutors Office claim that OnePlus reportedly set up a shell company in Hong Kong with a distinct name, then launched a branch in Taiwan in 2015 without government approval. The branch reportedly worked on research and development for OnePlus mobile phones. Taiwanese officials claim these actions by OnePlus violated the Cross-Strait Act, which is designed as a guide for relations between Taiwan and mainland China. One of the act’s provisions requires Chinese companies to obtain permission from the Taiwanese government to hire workers from Taiwan.This article originally appeared on Engadget at https://www.engadget.com/mobile/smartphones/taiwan-issues-arrest-warrant-for-pete-lau-ceo-of-oneplus-185400997.html?src=rss",
          "content": "Taiwanese officials have issued an arrest warrant for OnePlus CEO Pete Lau on allegations of illegally employing workers in Taiwan. Two Taiwanese citizens who worked for Lau have also been indicted. The China-based smartphone company has been accused of illegally recruiting more than 70 engineers from Taiwan. Members of the Shilin District Prosecutors Office claim that OnePlus reportedly set up a shell company in Hong Kong with a distinct name, then launched a branch in Taiwan in 2015 without government approval. The branch reportedly worked on research and development for OnePlus mobile phones. Taiwanese officials claim these actions by OnePlus violated the Cross-Strait Act, which is designed as a guide for relations between Taiwan and mainland China. One of the act’s provisions requires Chinese companies to obtain permission from the Taiwanese government to hire workers from Taiwan.This article originally appeared on Engadget at https://www.engadget.com/mobile/smartphones/taiwan-issues-arrest-warrant-for-pete-lau-ceo-of-oneplus-185400997.html?src=rss",
          "feed_position": 0
        },
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/260113/p36#a260113p36",
          "published_at": "Tue, 13 Jan 2026 13:05:00 -0500",
          "title": "Taiwan issues an arrest warrant for OnePlus CEO Pete Lau for allegedly illegally hiring 70+ Taiwanese engineers, as it tries to stop China from raiding workers (Li Liu/Bloomberg)",
          "standfirst": "Li Liu / Bloomberg: Taiwan issues an arrest warrant for OnePlus CEO Pete Lau for allegedly illegally hiring 70+ Taiwanese engineers, as it tries to stop China from raiding workers &mdash; Prosecutors in Taiwan issued an arrest warrant for the chief executive officer of the Chinese smartphone company OnePlus &hellip;",
          "content": "Li Liu / Bloomberg: Taiwan issues an arrest warrant for OnePlus CEO Pete Lau for allegedly illegally hiring 70+ Taiwanese engineers, as it tries to stop China from raiding workers &mdash; Prosecutors in Taiwan issued an arrest warrant for the chief executive officer of the Chinese smartphone company OnePlus &hellip;",
          "feed_position": 3,
          "image_url": "http://www.techmeme.com/260113/i36.jpg"
        }
      ],
      "featured_image": "http://www.techmeme.com/260113/i36.jpg",
      "popularity_score": 2019.561328611111
    },
    {
      "id": "cluster_13",
      "coverage": 2,
      "updated_at": "Tue, 13 Jan 2026 13:50:02 -0500",
      "title": "Mercedes pauses the rollout of Drive Pilot, a Level 3 \"eyes off\" driving feature available in Europe and the US, citing low demand and high production costs (Andrew J. Hawkins/The Verge)",
      "neutral_headline": "Mercedes temporarily scraps its Level 3 ‘eyes-off’ driving feature",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/260113/p39#a260113p39",
          "published_at": "Tue, 13 Jan 2026 13:50:02 -0500",
          "title": "Mercedes pauses the rollout of Drive Pilot, a Level 3 \"eyes off\" driving feature available in Europe and the US, citing low demand and high production costs (Andrew J. Hawkins/The Verge)",
          "standfirst": "Andrew J. Hawkins / The Verge: Mercedes pauses the rollout of Drive Pilot, a Level 3 &ldquo;eyes off&rdquo; driving feature available in Europe and the US, citing low demand and high production costs &mdash; The automaker is instead forging ahead with its new point-to-point Level 2 system that rivals Tesla's Full Self-Driving.",
          "content": "Andrew J. Hawkins / The Verge: Mercedes pauses the rollout of Drive Pilot, a Level 3 &ldquo;eyes off&rdquo; driving feature available in Europe and the US, citing low demand and high production costs &mdash; The automaker is instead forging ahead with its new point-to-point Level 2 system that rivals Tesla's Full Self-Driving.",
          "feed_position": 0,
          "image_url": "http://www.techmeme.com/260113/i39.jpg"
        },
        {
          "source": "The Verge",
          "url": "https://www.theverge.com/transportation/860935/mercedes-drive-pilot-level-3-scrapped",
          "published_at": "2026-01-13T12:31:14-05:00",
          "title": "Mercedes temporarily scraps its Level 3 ‘eyes-off’ driving feature",
          "standfirst": "Mercedes-Benz is pausing the roll-out of Drive Pilot, an \"eyes off\" conditionally automated driving feature that was available in Europe and the US. As first reported by German publication Handelsblatt, the revised S-Class will not have the Level 3 system when it arrives at the end of this month. Mercedes was one of the first [&#8230;]",
          "content": "Mercedes-Benz is pausing the roll-out of Drive Pilot, an \"eyes off\" conditionally automated driving feature that was available in Europe and the US. As first reported by German publication Handelsblatt, the revised S-Class will not have the Level 3 system when it arrives at the end of this month. Mercedes was one of the first automakers to offer a Level 3 driving system to its customers when it launched Drive Pilot with the electric EQS sedan and the gas-powered S-Class in the fall of 2023. At up to 40mph in traffic jam situations on highways, Drive Pilot provided hands-free, eyes-off driving that allows the driver to look away from the ro … Read the full story at The Verge.",
          "feed_position": 4
        }
      ],
      "featured_image": "http://www.techmeme.com/260113/i39.jpg",
      "popularity_score": 2019.4952175
    },
    {
      "id": "cluster_28",
      "coverage": 2,
      "updated_at": "Tue, 13 Jan 2026 12:45:00 -0500",
      "title": "A look at Confer, an open-source AI assistant project from Signal creator Moxie Marlinspike that is designed to provide end-to-end encryption for AI chats (Dan Goodin/Ars Technica)",
      "neutral_headline": "Signal creator Moxie Marlinspike wants to do for AI what he did for messaging",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/260113/p35#a260113p35",
          "published_at": "Tue, 13 Jan 2026 12:45:00 -0500",
          "title": "A look at Confer, an open-source AI assistant project from Signal creator Moxie Marlinspike that is designed to provide end-to-end encryption for AI chats (Dan Goodin/Ars Technica)",
          "standfirst": "Dan Goodin / Ars Technica: A look at Confer, an open-source AI assistant project from Signal creator Moxie Marlinspike that is designed to provide end-to-end encryption for AI chats &mdash; Moxie Marlinspike&mdash;the pseudonym of an engineer who set a new standard for private messaging with the creation of the Signal Messenger &hellip;",
          "content": "Dan Goodin / Ars Technica: A look at Confer, an open-source AI assistant project from Signal creator Moxie Marlinspike that is designed to provide end-to-end encryption for AI chats &mdash; Moxie Marlinspike&mdash;the pseudonym of an engineer who set a new standard for private messaging with the creation of the Signal Messenger &hellip;",
          "feed_position": 4,
          "image_url": "http://www.techmeme.com/260113/i35.jpg"
        },
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/security/2026/01/signal-creator-moxie-marlinspike-wants-to-do-for-ai-what-he-did-for-messaging/",
          "published_at": "Tue, 13 Jan 2026 12:00:36 +0000",
          "title": "Signal creator Moxie Marlinspike wants to do for AI what he did for messaging",
          "standfirst": "Introducing Confer, an end-to-end AI assistant that just works.",
          "content": "Moxie Marlinspike—the pseudonym of an engineer who set a new standard for private messaging with the creation of the Signal Messenger—is now aiming to revolutionize AI chatbots in a similar way. His latest brainchild is Confer, an open source AI assistant that provides strong assurances that user data is unreadable to the platform operator, hackers, law enforcement, or any other party other than account holders. The service—including its large language models and back-end components—runs entirely on open source software that users can cryptographically verify is in place. Data and conversations originating from users and the resulting responses from the LLMs are encrypted in a trusted execution environment (TEE) that prevents even server administrators from peeking at or tampering with them. Conversations are stored by Confer in the same encrypted form, which uses a key that remains securely on users’ devices.Read full article Comments",
          "feed_position": 6,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/electronic-privacy-invasion-1152x648.jpg"
        }
      ],
      "featured_image": "http://www.techmeme.com/260113/i35.jpg",
      "popularity_score": 2018.411328611111
    },
    {
      "id": "cluster_30",
      "coverage": 2,
      "updated_at": "Tue, 13 Jan 2026 17:30:00 GMT",
      "title": "Why Egnyte keeps hiring junior engineers despite the rise of AI coding tools",
      "neutral_headline": "Why Egnyte keeps hiring junior engineers despite the rise of AI coding tools",
      "items": [
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/orchestration/why-egnyte-keeps-hiring-junior-engineers-despite-the-rise-of-ai-coding-tools",
          "published_at": "Tue, 13 Jan 2026 17:30:00 GMT",
          "title": "Why Egnyte keeps hiring junior engineers despite the rise of AI coding tools",
          "standfirst": "Egnyte, the $1.5 billion cloud content governance company, has embedded AI coding tools across its global team of more than 350 developers — but not to reduce headcount. Instead, the company continues to hire junior engineers, using AI to accelerate onboarding, deepen codebase understanding, and shorten the path from junior to senior contributor. The approach challenges a dominant 2025 narrative that automation will replace developers, showing instead how enterprises are using AI to scale engineering capacity while keeping humans firmly in the loop.“To have engineers disappear or us not hiring junior engineers doesn&#x27;t look like the likely outcome,” Amrit Jassal, Egnyte CTO and co-founder, told VentureBeat. “You&#x27;ve got to have people, you&#x27;re training and doing all types of succession planning. The junior engineer of today is the senior engineer of tomorrow.”How Egnyte coders are using AI — without ceding controlEgnyte — which has more than 22,000 users including NASDAQ, Red Bull, and BuzzFeed — has rolled out Claude Code, Cursor, Augment, and Gemini CLI coding tools across its developer base to support its core business strategies and expand its newer AI offerings like customer-facing copilots and customizable AI agents. Devs use these tools across a variety of tasks, the simplest of which include data retrieval, code comprehension, smart search, and code lookup. Egnyte’s code base has lots of Java code, which uses numerous libraries, each with different versions, Jassal explained. AI tools are great for peer-to-peer programming, helping new users get a lay of the land, or existing users probe into different code repositories. “We have a pretty big code base, right?” Jassal said. “Let&#x27;s say you&#x27;re looking at an iOS application, but you&#x27;re not well versed; you will fire up Google CLI or an Augment, and ask it to discover the code base.”Some Egnyte devs are moving into automatic pull request summaries, which provide simple overviews of code changes that essentially explain the “what,” “how,” and “why” of proposed modifications. “But obviously, any change that&#x27;s made, we don&#x27;t want to hear that AI made the change; it has to be that developer made the change,” Jassal pointed out. “I would not trust AI to commit to the production code base.” Commits still pass through human review and security validation, and anything red-flagged is escalated to senior engineers. Devs are warned of the dangers of settling into autopilot mode or blindly trusting code. A model may not have been exposed to, or given enough samples of, certain coding components and infrastructure in its training. Another growing, and closely monitored, use case for AI is unit testing, where code components are run in isolation to ensure they work as intended. “At the end of the day, it is a productivity improvement tool,” he said. “It is really a continuation, it&#x27;s like any other tool, it&#x27;s not some magic.”Beyond core engineering, AI is helping other teams collaborate with programmers. Product management, for instance, is using tools like Vercel to bring “demo-worthy” prototypes, rather than just ideas, to devs, who can then move ahead with mock-ups. Or, if UX teams are looking to change certain elements on a dashboard, AI can quickly spin up a handful of options, like different widgets or buttons. “Then you come to engineering with that, and the engineer immediately knows what you really intend to do with it,” Jassal said. Setting expectations, meeting devs where they areHowever, day-to-day activities for all Egnyte engineers, including junior developers, extend beyond just coding. Junior developers are given hands-on tasks across the full development lifecycle to accelerate their growth and experience, Jassal said. For instance, they assist with requirement analysis in early software engineering phases, as well as deployment, productization and post-deployment maintenance.In turn, these activities require “Egnyte-specific tacit knowledge and experience” offered by senior engineers. One clear example of work that sits firmly with senior engineers is authoring architecture notes, as these cut across the platform and require a more holistic, system-level view, Jassal said. “Many of the traditional roadblocks are navigated faster these days with AI; for example, understanding the codebase, dissecting requirements, auto-testing,” he said. “This faster track allows our talented junior hires to progress more quickly and provide higher value to the company sooner.”The company expects a much faster learning curve from junior to mid-level engineers, Jassal said. “It&#x27;s always the case that people coming straight into the workforce are much more excited about trying new things,” Jassal said. But that has to be colored with reality to temper expectations, he added. On the other hand, some senior engineers may need to be ramped up in their adoption because they’re hesitant or had ho-hum or bad experiences with earlier generation tools. This requires incremental introduction. “The senior people, having been burnt multiple times, bring that perspective,” he said. \"So both [types of engineers] play an important role.”Hiring will continue for scale and fresh perspective“In general, I would say it has been really hyped by folks who want to sell you tokens,” Jassal said referring to people who talk about human coders becoming obsolete. \"Vibe coding\" could be construed in a similar vein: Like others in software development, he prefers the term “AI assisted coding,” wherein programmers have a self-driven loop, generating code, analyzing exceptions, then correcting and scaling. At least in Egnyte’s case, hiring will continue, even if at a slower clip as people become more productive thanks to AI, Jassal said. “We are not just hiring for scale, but to develop the next generation of senior developers and inject fresh perspectives into our development practices,” he said. The takeaway for technical decision-makers is not that AI will eliminate engineering jobs — but that it will reshape how talent is developed. At Egnyte, AI-assisted coding is compressing learning curves and raising expectations, not removing humans from the process. Enterprises that treat AI as a replacement risk hollowing out their future senior talent pipeline; those that treat it as infrastructure can move faster without losing the judgment, creativity, and accountability that only engineers provide.",
          "content": "Egnyte, the $1.5 billion cloud content governance company, has embedded AI coding tools across its global team of more than 350 developers — but not to reduce headcount. Instead, the company continues to hire junior engineers, using AI to accelerate onboarding, deepen codebase understanding, and shorten the path from junior to senior contributor. The approach challenges a dominant 2025 narrative that automation will replace developers, showing instead how enterprises are using AI to scale engineering capacity while keeping humans firmly in the loop.“To have engineers disappear or us not hiring junior engineers doesn&#x27;t look like the likely outcome,” Amrit Jassal, Egnyte CTO and co-founder, told VentureBeat. “You&#x27;ve got to have people, you&#x27;re training and doing all types of succession planning. The junior engineer of today is the senior engineer of tomorrow.”How Egnyte coders are using AI — without ceding controlEgnyte — which has more than 22,000 users including NASDAQ, Red Bull, and BuzzFeed — has rolled out Claude Code, Cursor, Augment, and Gemini CLI coding tools across its developer base to support its core business strategies and expand its newer AI offerings like customer-facing copilots and customizable AI agents. Devs use these tools across a variety of tasks, the simplest of which include data retrieval, code comprehension, smart search, and code lookup. Egnyte’s code base has lots of Java code, which uses numerous libraries, each with different versions, Jassal explained. AI tools are great for peer-to-peer programming, helping new users get a lay of the land, or existing users probe into different code repositories. “We have a pretty big code base, right?” Jassal said. “Let&#x27;s say you&#x27;re looking at an iOS application, but you&#x27;re not well versed; you will fire up Google CLI or an Augment, and ask it to discover the code base.”Some Egnyte devs are moving into automatic pull request summaries, which provide simple overviews of code changes that essentially explain the “what,” “how,” and “why” of proposed modifications. “But obviously, any change that&#x27;s made, we don&#x27;t want to hear that AI made the change; it has to be that developer made the change,” Jassal pointed out. “I would not trust AI to commit to the production code base.” Commits still pass through human review and security validation, and anything red-flagged is escalated to senior engineers. Devs are warned of the dangers of settling into autopilot mode or blindly trusting code. A model may not have been exposed to, or given enough samples of, certain coding components and infrastructure in its training. Another growing, and closely monitored, use case for AI is unit testing, where code components are run in isolation to ensure they work as intended. “At the end of the day, it is a productivity improvement tool,” he said. “It is really a continuation, it&#x27;s like any other tool, it&#x27;s not some magic.”Beyond core engineering, AI is helping other teams collaborate with programmers. Product management, for instance, is using tools like Vercel to bring “demo-worthy” prototypes, rather than just ideas, to devs, who can then move ahead with mock-ups. Or, if UX teams are looking to change certain elements on a dashboard, AI can quickly spin up a handful of options, like different widgets or buttons. “Then you come to engineering with that, and the engineer immediately knows what you really intend to do with it,” Jassal said. Setting expectations, meeting devs where they areHowever, day-to-day activities for all Egnyte engineers, including junior developers, extend beyond just coding. Junior developers are given hands-on tasks across the full development lifecycle to accelerate their growth and experience, Jassal said. For instance, they assist with requirement analysis in early software engineering phases, as well as deployment, productization and post-deployment maintenance.In turn, these activities require “Egnyte-specific tacit knowledge and experience” offered by senior engineers. One clear example of work that sits firmly with senior engineers is authoring architecture notes, as these cut across the platform and require a more holistic, system-level view, Jassal said. “Many of the traditional roadblocks are navigated faster these days with AI; for example, understanding the codebase, dissecting requirements, auto-testing,” he said. “This faster track allows our talented junior hires to progress more quickly and provide higher value to the company sooner.”The company expects a much faster learning curve from junior to mid-level engineers, Jassal said. “It&#x27;s always the case that people coming straight into the workforce are much more excited about trying new things,” Jassal said. But that has to be colored with reality to temper expectations, he added. On the other hand, some senior engineers may need to be ramped up in their adoption because they’re hesitant or had ho-hum or bad experiences with earlier generation tools. This requires incremental introduction. “The senior people, having been burnt multiple times, bring that perspective,” he said. \"So both [types of engineers] play an important role.”Hiring will continue for scale and fresh perspective“In general, I would say it has been really hyped by folks who want to sell you tokens,” Jassal said referring to people who talk about human coders becoming obsolete. \"Vibe coding\" could be construed in a similar vein: Like others in software development, he prefers the term “AI assisted coding,” wherein programmers have a self-driven loop, generating code, analyzing exceptions, then correcting and scaling. At least in Egnyte’s case, hiring will continue, even if at a slower clip as people become more productive thanks to AI, Jassal said. “We are not just hiring for scale, but to develop the next generation of senior developers and inject fresh perspectives into our development practices,” he said. The takeaway for technical decision-makers is not that AI will eliminate engineering jobs — but that it will reshape how talent is developed. At Egnyte, AI-assisted coding is compressing learning curves and raising expectations, not removing humans from the process. Enterprises that treat AI as a replacement risk hollowing out their future senior talent pipeline; those that treat it as infrastructure can move faster without losing the judgment, creativity, and accountability that only engineers provide.",
          "feed_position": 0,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/7DUx86jAglIoinWLIrfpO8/abd01f833379598313f3bdeb2c35a96e/Egnyte.png?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/entertainment/streaming/nba-league-pass-is-up-to-55-percent-off-right-now-163421218.html",
          "published_at": "Tue, 13 Jan 2026 16:34:21 +0000",
          "title": "NBA League Pass is up to 55 percent off right now",
          "standfirst": "NBA League Pass, the streaming service that lets you catch hundreds of out-of-market NBA games, is on sale right now for up to 55 percent off. The League Pass Premium subscription is on sale for $75, down from $160, while League Pass Standard is marked down to $50 from $110. We're almost halfway through the season at this point, so it makes sense for a service like League Pass to start offering some discounts. The Standard plan includes commercials and support for only one device at a time, while the Premium tier offers no commercials, in-arena streams during breaks in the game, offline viewing of full games and concurrent streams on up to three devices at once. Last year, League Pass added multiview, which allows you to view up to four games at once on a single screen. This is included across both subscription tiers. The service also added a smart rewind tool that automatically selects key highlights and plays from each game. Outside the US and Canada, League Pass carries every single NBA game live, but within these countries a bevy of restrictions apply. In the US, any games being shown on your regional sports network will be blacked out as the service is meant for out-of-market games only. Also, any nationally broadcast games will not be available live, but instead will be available for on-demand viewing at 6AM ET the following day. The service is only for regular-season games. If you're an avid NBA fan that follows multiple teams then the League Pass almost certainly carries dozens of games you can watch even with the restrictions in the US. Subscribers can get a list of applicable blackouts by entering their ZIP code before signing up. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/entertainment/streaming/nba-league-pass-is-up-to-55-percent-off-right-now-163421218.html?src=rss",
          "content": "NBA League Pass, the streaming service that lets you catch hundreds of out-of-market NBA games, is on sale right now for up to 55 percent off. The League Pass Premium subscription is on sale for $75, down from $160, while League Pass Standard is marked down to $50 from $110. We're almost halfway through the season at this point, so it makes sense for a service like League Pass to start offering some discounts. The Standard plan includes commercials and support for only one device at a time, while the Premium tier offers no commercials, in-arena streams during breaks in the game, offline viewing of full games and concurrent streams on up to three devices at once. Last year, League Pass added multiview, which allows you to view up to four games at once on a single screen. This is included across both subscription tiers. The service also added a smart rewind tool that automatically selects key highlights and plays from each game. Outside the US and Canada, League Pass carries every single NBA game live, but within these countries a bevy of restrictions apply. In the US, any games being shown on your regional sports network will be blacked out as the service is meant for out-of-market games only. Also, any nationally broadcast games will not be available live, but instead will be available for on-demand viewing at 6AM ET the following day. The service is only for regular-season games. If you're an avid NBA fan that follows multiple teams then the League Pass almost certainly carries dozens of games you can watch even with the restrictions in the US. Subscribers can get a list of applicable blackouts by entering their ZIP code before signing up. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/entertainment/streaming/nba-league-pass-is-up-to-55-percent-off-right-now-163421218.html?src=rss",
          "feed_position": 2
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/data/deepseeks-conditional-memory-fixes-silent-llm-waste-gpu-cycles-lost-to",
          "published_at": "Tue, 13 Jan 2026 16:00:00 GMT",
          "title": "DeepSeek’s conditional memory fixes silent LLM waste: GPU cycles lost to static lookups",
          "standfirst": "When an enterprise LLM retrieves a product name, technical specification, or standard contract clause, it&#x27;s using expensive GPU computation designed for complex reasoning — just to access static information. This happens millions of times per day. Each lookup wastes cycles and inflates infrastructure costs. DeepSeek&#x27;s newly released research on \"conditional memory\" addresses this architectural limitation directly. The work introduces Engram, a module that separates static pattern retrieval from dynamic reasoning. It delivers results that challenge assumptions about what memory is actually for in neural networks. The paper was co-authored by DeepSeek founder Liang Wenfeng.Through systematic experiments DeepSeek found the optimal balance between computation and memory with 75% of sparse model capacity allocated to dynamic reasoning and 25% to static lookups. This memory system improved reasoning more than knowledge retrieval. Complex reasoning benchmarks jumped from 70% to 74% accuracy, while knowledge-focused tests improved from 57% to 61%. These improvements came from tests including Big-Bench Hard, ARC-Challenge, and MMLU.The research arrives as enterprises face mounting pressure to deploy more capable AI systems while navigating GPU memory constraints and infrastructure costs. DeepSeek&#x27;s approach offers a potential path forward by fundamentally rethinking how models should be structured.How conditional memory solves a different issue than agentic memory and RAGAgentic memory systems, sometimes referred to as contextual memory — like Hindsight, MemOS, or Memp — focus on episodic memory. They store records of past conversations, user preferences, and interaction history. These systems help agents maintain context across sessions and learn from experience. But they&#x27;re external to the model&#x27;s forward pass and don&#x27;t optimize how the model internally processes static linguistic patterns.For Chris Latimer, founder and CEO of Vectorize, which developed Hindsight, the conditional memory approach used in Engram solves a different problem than agentic AI memory.\"It&#x27;s not solving the problem of connecting agents to external memory like conversation histories and knowledge stores,\" Latimer told VentureBeat. \"It&#x27;s more geared towards squeezing performance out of smaller models and getting more mileage out of scarce GPU resources.\"Conditional memory tackles a fundamental issue: Transformers lack a native knowledge lookup primitive. When processing text, they must simulate retrieval of static patterns through expensive neural computation across multiple layers. These patterns include named entities, technical terminology, and common phrases.The DeepSeek paper illustrates this with a concrete example. Recognizing \"Diana, Princess of Wales\" requires consuming multiple layers of attention and feed-forward networks to progressively compose features. The model essentially uses deep, dynamic logic circuits to perform what should be a simple hash table lookup. It&#x27;s like using a calculator to remember your phone number rather than just looking it up.\"The problem is that Transformer lacks a &#x27;native knowledge lookup&#x27; ability,\" the researchers write. \"Many tasks that should be solved in O(1) time like retrieval have to be &#x27;simulated for retrieval&#x27; through a large amount of computation, which is very inefficient.\"How conditional memory worksEngram introduces \"conditional memory\" to work alongside MoE&#x27;s conditional computation. The mechanism is straightforward. The module takes sequences of two to three tokens and uses hash functions to look them up in a massive embedding table. Retrieval happens in constant time, regardless of table size.But retrieved patterns need filtering. A hash lookup for \"Apple\" might collide with unrelated content, or the word might mean the fruit rather than the company. Engram solves this with a gating mechanism. The model&#x27;s current understanding of context (accumulated through earlier attention layers) acts as a filter. If retrieved memory contradicts the current context, the gate suppresses it. If it fits, the gate lets it through.The module isn&#x27;t applied at every layer. Strategic placement balances performance gains against system latency.This dual-system design raises a critical question: How much capacity should each get? DeepSeek&#x27;s key finding: the optimal split is 75-80% for computation and 20-25% for memory. Testing found pure MoE (100% computation) proved suboptimal. Too much computation wastes depth reconstructing static patterns; too much memory loses reasoning capacity.Infrastructure efficiency: the GPU memory bypassPerhaps Engram&#x27;s most pragmatic contribution is its infrastructure-aware design. Unlike MoE&#x27;s dynamic routing, which depends on runtime hidden states, Engram&#x27;s retrieval indices depend solely on input token sequences. This deterministic nature enables a prefetch-and-overlap strategy.\"The challenge is that GPU memory is limited and expensive, so using bigger models gets costly and harder to deploy,\" Latimer said. \"The clever idea behind Engram is to keep the main model on the GPU, but offload a big chunk of the model&#x27;s stored information into a separate memory on regular RAM, which the model can use on a just-in-time basis.\"During inference, the system can asynchronously retrieve embeddings from host CPU memory via PCIe. This happens while GPU computes preceding transformer blocks. Strategic layer placement leverages computation of early layers as a buffer to mask communication latency.The researchers demonstrated this with a 100B-parameter embedding table entirely offloaded to host DRAM. They achieved throughput penalties below 3%. This decoupling of storage from compute addresses a critical enterprise constraint as GPU high-bandwidth memory remains expensive and scarce.What this means for enterprise AI deploymentFor enterprises evaluating AI infrastructure strategies, DeepSeek&#x27;s findings suggest several actionable insights:1. Hybrid architectures outperform pure approaches. The 75/25 allocation law indicates that optimal models should split sparse capacity between computation and memory. 2. Infrastructure costs may shift from GPU to memory. If Engram-style architectures prove viable in production, infrastructure investment patterns could change. The ability to store 100B+ parameters in CPU memory with minimal overhead suggests that memory-rich, compute-moderate configurations may offer better performance-per-dollar than pure GPU scaling.3. Reasoning improvements exceed knowledge gains. The surprising finding that reasoning benefits more than knowledge retrieval suggests that memory&#x27;s value extends beyond obvious use cases. For enterprises leading AI adoption, Engram demonstrates that the next frontier may not be simply bigger models. It&#x27;s smarter architectural choices that respect the fundamental distinction between static knowledge and dynamic reasoning. The research suggests that optimal AI systems will increasingly resemble hybrid architectures. Organizations waiting to adopt AI later in the cycle should monitor whether major model providers incorporate conditional memory principles into their architectures. If the 75/25 allocation law holds across scales and domains, the next generation of foundation models may deliver substantially better reasoning performance at lower infrastructure costs.",
          "content": "When an enterprise LLM retrieves a product name, technical specification, or standard contract clause, it&#x27;s using expensive GPU computation designed for complex reasoning — just to access static information. This happens millions of times per day. Each lookup wastes cycles and inflates infrastructure costs. DeepSeek&#x27;s newly released research on \"conditional memory\" addresses this architectural limitation directly. The work introduces Engram, a module that separates static pattern retrieval from dynamic reasoning. It delivers results that challenge assumptions about what memory is actually for in neural networks. The paper was co-authored by DeepSeek founder Liang Wenfeng.Through systematic experiments DeepSeek found the optimal balance between computation and memory with 75% of sparse model capacity allocated to dynamic reasoning and 25% to static lookups. This memory system improved reasoning more than knowledge retrieval. Complex reasoning benchmarks jumped from 70% to 74% accuracy, while knowledge-focused tests improved from 57% to 61%. These improvements came from tests including Big-Bench Hard, ARC-Challenge, and MMLU.The research arrives as enterprises face mounting pressure to deploy more capable AI systems while navigating GPU memory constraints and infrastructure costs. DeepSeek&#x27;s approach offers a potential path forward by fundamentally rethinking how models should be structured.How conditional memory solves a different issue than agentic memory and RAGAgentic memory systems, sometimes referred to as contextual memory — like Hindsight, MemOS, or Memp — focus on episodic memory. They store records of past conversations, user preferences, and interaction history. These systems help agents maintain context across sessions and learn from experience. But they&#x27;re external to the model&#x27;s forward pass and don&#x27;t optimize how the model internally processes static linguistic patterns.For Chris Latimer, founder and CEO of Vectorize, which developed Hindsight, the conditional memory approach used in Engram solves a different problem than agentic AI memory.\"It&#x27;s not solving the problem of connecting agents to external memory like conversation histories and knowledge stores,\" Latimer told VentureBeat. \"It&#x27;s more geared towards squeezing performance out of smaller models and getting more mileage out of scarce GPU resources.\"Conditional memory tackles a fundamental issue: Transformers lack a native knowledge lookup primitive. When processing text, they must simulate retrieval of static patterns through expensive neural computation across multiple layers. These patterns include named entities, technical terminology, and common phrases.The DeepSeek paper illustrates this with a concrete example. Recognizing \"Diana, Princess of Wales\" requires consuming multiple layers of attention and feed-forward networks to progressively compose features. The model essentially uses deep, dynamic logic circuits to perform what should be a simple hash table lookup. It&#x27;s like using a calculator to remember your phone number rather than just looking it up.\"The problem is that Transformer lacks a &#x27;native knowledge lookup&#x27; ability,\" the researchers write. \"Many tasks that should be solved in O(1) time like retrieval have to be &#x27;simulated for retrieval&#x27; through a large amount of computation, which is very inefficient.\"How conditional memory worksEngram introduces \"conditional memory\" to work alongside MoE&#x27;s conditional computation. The mechanism is straightforward. The module takes sequences of two to three tokens and uses hash functions to look them up in a massive embedding table. Retrieval happens in constant time, regardless of table size.But retrieved patterns need filtering. A hash lookup for \"Apple\" might collide with unrelated content, or the word might mean the fruit rather than the company. Engram solves this with a gating mechanism. The model&#x27;s current understanding of context (accumulated through earlier attention layers) acts as a filter. If retrieved memory contradicts the current context, the gate suppresses it. If it fits, the gate lets it through.The module isn&#x27;t applied at every layer. Strategic placement balances performance gains against system latency.This dual-system design raises a critical question: How much capacity should each get? DeepSeek&#x27;s key finding: the optimal split is 75-80% for computation and 20-25% for memory. Testing found pure MoE (100% computation) proved suboptimal. Too much computation wastes depth reconstructing static patterns; too much memory loses reasoning capacity.Infrastructure efficiency: the GPU memory bypassPerhaps Engram&#x27;s most pragmatic contribution is its infrastructure-aware design. Unlike MoE&#x27;s dynamic routing, which depends on runtime hidden states, Engram&#x27;s retrieval indices depend solely on input token sequences. This deterministic nature enables a prefetch-and-overlap strategy.\"The challenge is that GPU memory is limited and expensive, so using bigger models gets costly and harder to deploy,\" Latimer said. \"The clever idea behind Engram is to keep the main model on the GPU, but offload a big chunk of the model&#x27;s stored information into a separate memory on regular RAM, which the model can use on a just-in-time basis.\"During inference, the system can asynchronously retrieve embeddings from host CPU memory via PCIe. This happens while GPU computes preceding transformer blocks. Strategic layer placement leverages computation of early layers as a buffer to mask communication latency.The researchers demonstrated this with a 100B-parameter embedding table entirely offloaded to host DRAM. They achieved throughput penalties below 3%. This decoupling of storage from compute addresses a critical enterprise constraint as GPU high-bandwidth memory remains expensive and scarce.What this means for enterprise AI deploymentFor enterprises evaluating AI infrastructure strategies, DeepSeek&#x27;s findings suggest several actionable insights:1. Hybrid architectures outperform pure approaches. The 75/25 allocation law indicates that optimal models should split sparse capacity between computation and memory. 2. Infrastructure costs may shift from GPU to memory. If Engram-style architectures prove viable in production, infrastructure investment patterns could change. The ability to store 100B+ parameters in CPU memory with minimal overhead suggests that memory-rich, compute-moderate configurations may offer better performance-per-dollar than pure GPU scaling.3. Reasoning improvements exceed knowledge gains. The surprising finding that reasoning benefits more than knowledge retrieval suggests that memory&#x27;s value extends beyond obvious use cases. For enterprises leading AI adoption, Engram demonstrates that the next frontier may not be simply bigger models. It&#x27;s smarter architectural choices that respect the fundamental distinction between static knowledge and dynamic reasoning. The research suggests that optimal AI systems will increasingly resemble hybrid architectures. Organizations waiting to adopt AI later in the cycle should monitor whether major model providers incorporate conditional memory principles into their architectures. If the 75/25 allocation law holds across scales and domains, the next generation of foundation models may deliver substantially better reasoning performance at lower infrastructure costs.",
          "feed_position": 1,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/5QbXis6MzFunSR0Q7iejyX/2cd47fca23ad6f0fd1e3094fc096252e/conditional-memory-smk.jpg?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/ai/anthropic-launches-claude-cowork-a-version-of-its-coding-ai-for-regular-people-193000849.html",
          "published_at": "Tue, 13 Jan 2026 15:27:21 +0000",
          "title": "Anthropic launches Claude Cowork, a version of its coding AI for regular people",
          "standfirst": "If you follow Anthropic, you're probably familiar with Claude Code. Since the fall of 2024, the company has been training its AI models to use and navigate computers like a human would, and the coding agent has been the most practical expression of that work, giving developers a way to automate rote programming tasks. Starting today, Anthropic is giving regular people a way to take advantage of those capabilities, with the release of a new preview feature called Claude Cowork.The company is billing Cowork as \"a simpler way for anyone — not just developers — to work with Claude.\" After you give the system access to a folder on your computer, it can read, edit or create new files in that folder on your behalf. Anthropic gives a few different example use cases for Cowork. For instance, you could ask Claude to organize your downloads folder, telling it to rename the files contained within to something that's easier to parse at a glance. Another example: you could use Claude to turn screenshots of receipts and invoices into a spreadsheet for tracking expenses. Cowork can also navigate websites — provided you install Claude’s Chrome plugin — and make can use Anthropic's Connectors framework to access third-party apps like Canva. \"Cowork is designed to make using Claude for new work as simple as possible. You don’t need to keep manually providing context or converting Claude’s outputs into the right format,\" the company said. \"Nor do you have to wait for Claude to finish before offering further ideas or feedback: you can queue up tasks and let Claude work through them in parallel.\" If the idea of granting Claude access to your computer sounds ill-advised, Anthropic says Claude \"can’t read or edit anything you don’t give it explicit access to.\" However, the company does note the system can \"take potentially destructive actions,\" such as deleting a file that is important to you or misinterpreting your instructions. For that reason, Anthropic suggests it's best to give \"very clear\" guidance to Claude. Anthropic isn’t the first to offer a computer agent. Microsoft, for example, has been pushing Copilot hard for nearly three years, despite seemingly limited adoption. For Anthropic, the challenge will be convincing people these tools are useful where others have failed. The fact Claude Code has been universally loved by programmers may make that task easier. For now, Anthropic is giving users of its pricey Claude Max subscription first access to the preview. If you want to try Cowork for yourself, you'll also need a Mac with the Claude macOS app installed. For everyone else, you’ll need to join a wait list. This article originally appeared on Engadget at https://www.engadget.com/ai/anthropic-launches-claude-cowork-a-version-of-its-coding-ai-for-regular-people-193000849.html?src=rss",
          "content": "If you follow Anthropic, you're probably familiar with Claude Code. Since the fall of 2024, the company has been training its AI models to use and navigate computers like a human would, and the coding agent has been the most practical expression of that work, giving developers a way to automate rote programming tasks. Starting today, Anthropic is giving regular people a way to take advantage of those capabilities, with the release of a new preview feature called Claude Cowork.The company is billing Cowork as \"a simpler way for anyone — not just developers — to work with Claude.\" After you give the system access to a folder on your computer, it can read, edit or create new files in that folder on your behalf. Anthropic gives a few different example use cases for Cowork. For instance, you could ask Claude to organize your downloads folder, telling it to rename the files contained within to something that's easier to parse at a glance. Another example: you could use Claude to turn screenshots of receipts and invoices into a spreadsheet for tracking expenses. Cowork can also navigate websites — provided you install Claude’s Chrome plugin — and make can use Anthropic's Connectors framework to access third-party apps like Canva. \"Cowork is designed to make using Claude for new work as simple as possible. You don’t need to keep manually providing context or converting Claude’s outputs into the right format,\" the company said. \"Nor do you have to wait for Claude to finish before offering further ideas or feedback: you can queue up tasks and let Claude work through them in parallel.\" If the idea of granting Claude access to your computer sounds ill-advised, Anthropic says Claude \"can’t read or edit anything you don’t give it explicit access to.\" However, the company does note the system can \"take potentially destructive actions,\" such as deleting a file that is important to you or misinterpreting your instructions. For that reason, Anthropic suggests it's best to give \"very clear\" guidance to Claude. Anthropic isn’t the first to offer a computer agent. Microsoft, for example, has been pushing Copilot hard for nearly three years, despite seemingly limited adoption. For Anthropic, the challenge will be convincing people these tools are useful where others have failed. The fact Claude Code has been universally loved by programmers may make that task easier. For now, Anthropic is giving users of its pricey Claude Max subscription first access to the preview. If you want to try Cowork for yourself, you'll also need a Mac with the Claude macOS app installed. For everyone else, you’ll need to join a wait list. This article originally appeared on Engadget at https://www.engadget.com/ai/anthropic-launches-claude-cowork-a-version-of-its-coding-ai-for-regular-people-193000849.html?src=rss",
          "feed_position": 3
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/mobile/tablets/remarkable-e-ink-tablet-bundles-are-up-to-90-off-right-now-150242312.html",
          "published_at": "Tue, 13 Jan 2026 15:02:42 +0000",
          "title": "reMarkable E Ink tablet bundles are up to $90 off right now",
          "standfirst": "E Ink tablet maker reMarkable is running a bundle deal right now that can save you between $80 and $90 when buying a reMarkable 2 along with a Marker stylus and a folio case. The savings vary depending on the bundle you configure, but this can bring your out-the-door cost down to $449 from $529 for the tablet, Marker stylus and polymer weave book folio. The company also sells a newer stylus called Marker Plus that lets you erase by flipping it around just like a real pencil, but that will cost you an extra $50. If you’ve been eyeing a dedicated writing tablet for work, school or just jotting down notes without the distraction of endless apps, this bundle deal is an ideal opportunity to pick one up. The reMarkable 2 earned our top pick for best e-ink tablet. In our review, we said the tablet was prettier than ever with a 10.3-inch display and a handsome aluminum frame. The tablet is only 4.7mm thick and weighs less than a pound, helping it feel lean and portable. The display can detect over 4,000 different levels of pressure with the Marker stylus, allowing for precise shading when sketching and the latency between the stylus and the screen is just 21ms. reMarkable fitted the display with a resin layer on top of the glass to make writing on it feel more realistic. We didn't think this passed muster, but we found writing on it was a joy nonetheless. The tablet supports PDFs and ePUBs, which can be added via the companion mobile app or a desktop computer. You can also pair the reMarkable 2 with Google Drive, Microsoft OneDrive or Dropbox to access files. The battery is rated for an impressive two weeks between charges. The reMarkable Paper Pro, a higher-end model with a richer feature set like a full color display and a built-in reading light, is our pick for best premium e-ink tablet. The pricier tablet also has bundle deals right now with savings up to $80 depending on configuration. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/mobile/tablets/remarkable-e-ink-tablet-bundles-are-up-to-90-off-right-now-150242312.html?src=rss",
          "content": "E Ink tablet maker reMarkable is running a bundle deal right now that can save you between $80 and $90 when buying a reMarkable 2 along with a Marker stylus and a folio case. The savings vary depending on the bundle you configure, but this can bring your out-the-door cost down to $449 from $529 for the tablet, Marker stylus and polymer weave book folio. The company also sells a newer stylus called Marker Plus that lets you erase by flipping it around just like a real pencil, but that will cost you an extra $50. If you’ve been eyeing a dedicated writing tablet for work, school or just jotting down notes without the distraction of endless apps, this bundle deal is an ideal opportunity to pick one up. The reMarkable 2 earned our top pick for best e-ink tablet. In our review, we said the tablet was prettier than ever with a 10.3-inch display and a handsome aluminum frame. The tablet is only 4.7mm thick and weighs less than a pound, helping it feel lean and portable. The display can detect over 4,000 different levels of pressure with the Marker stylus, allowing for precise shading when sketching and the latency between the stylus and the screen is just 21ms. reMarkable fitted the display with a resin layer on top of the glass to make writing on it feel more realistic. We didn't think this passed muster, but we found writing on it was a joy nonetheless. The tablet supports PDFs and ePUBs, which can be added via the companion mobile app or a desktop computer. You can also pair the reMarkable 2 with Google Drive, Microsoft OneDrive or Dropbox to access files. The battery is rated for an impressive two weeks between charges. The reMarkable Paper Pro, a higher-end model with a richer feature set like a full color display and a built-in reading light, is our pick for best premium e-ink tablet. The pricier tablet also has bundle deals right now with savings up to $80 depending on configuration. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/mobile/tablets/remarkable-e-ink-tablet-bundles-are-up-to-90-off-right-now-150242312.html?src=rss",
          "feed_position": 4
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/apps/apple-bundles-creative-apps-such-as-final-cut-pro-and-logic-pro-into-a-single-subscription-145210038.html",
          "published_at": "Tue, 13 Jan 2026 14:52:10 +0000",
          "title": "Apple bundles creative apps such as Final Cut Pro and Logic Pro into a single subscription",
          "standfirst": "Apple has been putting more onus on its services for the past several years — the company makes tens of billions of dollars in revenue from that side of the business, which it claimed had a record year in 2025. Apple is nudging a little more in that direction with a new subscription bundle called Apple Creator Studio.This allows creators to pay a single fee ($13 per month or $129 per year) to use Final Cut Pro, Logic Pro, Pixelmator Pro, Motion, Compressor and MainStage. Subscribers will get access to “premium content” in Pages, Keynote and Numbers (as well as in Freeform later this year). Of course, there are AI features too. Apple Creator Studio will be available starting on January 28 and you can try it out at no cost through a one-month free trial. College students and educators can subscribe to Apple Creator Studio for $3 per month or $30 per year. Up to six people can access all of the plan’s features if one person in a Family Sharing group subscribes. Apple noted that Final Cut Pro, Pixelmator Pro, Logic Pro, Motion, Compressor and MainStage will still be available as one-time purchases for Mac through the Mac App Store. Given that those can be pretty pricy (going up to $300 for Final Cut Pro), the subscription could be enticing to many burgeoning creators.This seems like Apple’s attempt to muscle in on Adobe’s territory, especially now that it’s bringing AI features to many of these apps. Adding new features to productivity apps like Numbers and Keynote means Apple’s taking a shot at the likes of Microsoft 365 Copilot (yeeeeah, that’s what Office is called now) and Google Workspace as well.On Mac and iPad, Final Cut Pro has a new feature called Beat Detection. Apple suggests this makes “editing video to the rhythm of music fast and fun.” It uses an AI model from Logic Pro to analyze music tracks and display a Beat Grid. The idea here is to visualize song parts, beats and bars to help editors align their cuts with the music. The Montage Maker tool in Final Cut Pro on an iPad.AppleAn AI-powered Montage Maker tool can stitch together “a dynamic video based on the best visual moments within the footage.” You’ll be able to tweak these montages and use an Auto Crop tool to reframe the clip into a vertical format to make it a better fit for social media. Final Cut Pro has transcript and visual search functions too.Logic Pro, MainStage, Pixelmator Pro (which is coming to iPad with Apple Pencil support) and Motion will all have AI-powered features as well. As you might expect, you’ll need an Apple Intelligence-capable device to use some of these.Apple is also introducing something called the Content Hub. This media library includes “curated, high-quality photos, graphics and illustrations.” As for Keynote, Pages, and Numbers, you’ll be able to access premium templates and themes in those otherwise-free apps with a Apple Creator Studio plan. Subscribers will be able to try beta versions of new features, such as a way to generate a draft of a Keynote presentation text based on an outline, and a Magic Fill tool to generate formulas and fill in tables in Numbers.This article originally appeared on Engadget at https://www.engadget.com/apps/apple-bundles-creative-apps-such-as-final-cut-pro-and-logic-pro-into-a-single-subscription-145210038.html?src=rss",
          "content": "Apple has been putting more onus on its services for the past several years — the company makes tens of billions of dollars in revenue from that side of the business, which it claimed had a record year in 2025. Apple is nudging a little more in that direction with a new subscription bundle called Apple Creator Studio.This allows creators to pay a single fee ($13 per month or $129 per year) to use Final Cut Pro, Logic Pro, Pixelmator Pro, Motion, Compressor and MainStage. Subscribers will get access to “premium content” in Pages, Keynote and Numbers (as well as in Freeform later this year). Of course, there are AI features too. Apple Creator Studio will be available starting on January 28 and you can try it out at no cost through a one-month free trial. College students and educators can subscribe to Apple Creator Studio for $3 per month or $30 per year. Up to six people can access all of the plan’s features if one person in a Family Sharing group subscribes. Apple noted that Final Cut Pro, Pixelmator Pro, Logic Pro, Motion, Compressor and MainStage will still be available as one-time purchases for Mac through the Mac App Store. Given that those can be pretty pricy (going up to $300 for Final Cut Pro), the subscription could be enticing to many burgeoning creators.This seems like Apple’s attempt to muscle in on Adobe’s territory, especially now that it’s bringing AI features to many of these apps. Adding new features to productivity apps like Numbers and Keynote means Apple’s taking a shot at the likes of Microsoft 365 Copilot (yeeeeah, that’s what Office is called now) and Google Workspace as well.On Mac and iPad, Final Cut Pro has a new feature called Beat Detection. Apple suggests this makes “editing video to the rhythm of music fast and fun.” It uses an AI model from Logic Pro to analyze music tracks and display a Beat Grid. The idea here is to visualize song parts, beats and bars to help editors align their cuts with the music. The Montage Maker tool in Final Cut Pro on an iPad.AppleAn AI-powered Montage Maker tool can stitch together “a dynamic video based on the best visual moments within the footage.” You’ll be able to tweak these montages and use an Auto Crop tool to reframe the clip into a vertical format to make it a better fit for social media. Final Cut Pro has transcript and visual search functions too.Logic Pro, MainStage, Pixelmator Pro (which is coming to iPad with Apple Pencil support) and Motion will all have AI-powered features as well. As you might expect, you’ll need an Apple Intelligence-capable device to use some of these.Apple is also introducing something called the Content Hub. This media library includes “curated, high-quality photos, graphics and illustrations.” As for Keynote, Pages, and Numbers, you’ll be able to access premium templates and themes in those otherwise-free apps with a Apple Creator Studio plan. Subscribers will be able to try beta versions of new features, such as a way to generate a draft of a Keynote presentation text based on an outline, and a Magic Fill tool to generate formulas and fill in tables in Numbers.This article originally appeared on Engadget at https://www.engadget.com/apps/apple-bundles-creative-apps-such-as-final-cut-pro-and-logic-pro-into-a-single-subscription-145210038.html?src=rss",
          "feed_position": 6,
          "image_url": "https://d29szjachogqwa.cloudfront.net/images/user-uploaded/final_cut.jpg"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/the-disney-hulu-bundle-is-on-sale-for-10-for-one-month-right-now-192814784.html",
          "published_at": "Tue, 13 Jan 2026 14:06:25 +0000",
          "title": "The Disney+ Hulu bundle is on sale for $10 for one month right now",
          "standfirst": "You have the best chance to save on streaming services during the holiday shopping season, but throughout the year, the occasional deal pops up that's worth considering. Case in point: this new Disney+ deal. New and eligible returning subscribers can sign up for the Disney+ Hulu bundle (with ads) for $10 for one month of access. That's $3 off the usual price of the bundle for one month, and more than 58 percent off if you consider the cost of each service individually (Disney+ at $12 per month and, separately, Hulu also at $12 per month). We'd be remiss if we didn't mention that this isn't quite as good as the Black Friday deal we saw last year, which offered the same bundle for $5 per month for one year. However, if you missed that offer or just want to try out Disney+ and Hulu for a brief period of time, this is a good way to do so. Disney+ and Hulu make one of the most balanced streaming pairs available, blending family-friendly favorites with acclaimed originals and network TV staples. Disney+ brings a vast library of animated classics, blockbuster franchises and exclusive content from Marvel, Pixar, Star Wars and National Geographic. It’s the place to stream nearly every Star Wars film and series, plus the full Marvel Cinematic Universe lineup and Disney’s most recent theatrical releases. Hulu balances things out with a more adult-oriented lineup of current TV shows, next-day network episodes and a growing roster of award-winning originals. The platform hosts series like The Bear, The Handmaid’s Tale and Only Murders in the Building, alongside comedies, thrillers and documentaries that regularly feature in awards conversations. It’s also the home for next-day streaming of ABC and FX shows, making it especially useful if you’ve already cut the cable cord but still want to keep up with primetime TV. The Duo Basic bundle ties these two services together under a single subscription, offering a simple way to expand your library without juggling multiple accounts. This tier includes ads on both platforms, but the trade-off is significant savings compared with paying for each service separately. For many households, that’s an acceptable compromise when it means access to such a wide range of content. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/the-disney-hulu-bundle-is-on-sale-for-10-for-one-month-right-now-192814784.html?src=rss",
          "content": "You have the best chance to save on streaming services during the holiday shopping season, but throughout the year, the occasional deal pops up that's worth considering. Case in point: this new Disney+ deal. New and eligible returning subscribers can sign up for the Disney+ Hulu bundle (with ads) for $10 for one month of access. That's $3 off the usual price of the bundle for one month, and more than 58 percent off if you consider the cost of each service individually (Disney+ at $12 per month and, separately, Hulu also at $12 per month). We'd be remiss if we didn't mention that this isn't quite as good as the Black Friday deal we saw last year, which offered the same bundle for $5 per month for one year. However, if you missed that offer or just want to try out Disney+ and Hulu for a brief period of time, this is a good way to do so. Disney+ and Hulu make one of the most balanced streaming pairs available, blending family-friendly favorites with acclaimed originals and network TV staples. Disney+ brings a vast library of animated classics, blockbuster franchises and exclusive content from Marvel, Pixar, Star Wars and National Geographic. It’s the place to stream nearly every Star Wars film and series, plus the full Marvel Cinematic Universe lineup and Disney’s most recent theatrical releases. Hulu balances things out with a more adult-oriented lineup of current TV shows, next-day network episodes and a growing roster of award-winning originals. The platform hosts series like The Bear, The Handmaid’s Tale and Only Murders in the Building, alongside comedies, thrillers and documentaries that regularly feature in awards conversations. It’s also the home for next-day streaming of ABC and FX shows, making it especially useful if you’ve already cut the cable cord but still want to keep up with primetime TV. The Duo Basic bundle ties these two services together under a single subscription, offering a simple way to expand your library without juggling multiple accounts. This tier includes ads on both platforms, but the trade-off is significant savings compared with paying for each service separately. For many households, that’s an acceptable compromise when it means access to such a wide range of content. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/the-disney-hulu-bundle-is-on-sale-for-10-for-one-month-right-now-192814784.html?src=rss",
          "feed_position": 8
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/airtags-are-back-on-sale-with-a-four-pack-going-for-65-202333994.html",
          "published_at": "Tue, 13 Jan 2026 13:14:02 +0000",
          "title": "AirTags are back on sale, with a four-pack going for $65",
          "standfirst": "Most Apple products are pretty expensive, but some of the most affordable (and useful) ones are AirTags. The Bluetooth trackers are priced pretty reasonably even when not on sale, but they can be a steal if you can get them on a discount — like right now. A four pack of AirTags is on sale for $65 at Amazon, which is only a few dollars more than the record-low price we saw during Black Friday this year. AirTags can be useful for people who travel frequently, helping you to keep track of essentials like your passport as well as a way to keep tabs on luggage while you're on the go. If you do purchase some AirTags, we have some recommendations for useful accessories to go along with them, such as different styles of cases to best attach the trackers to different types of items. These are worth looking over and adding to your shopping cart in order to make the most of the product. AirTags have an IP67 rating for water and dust resistance and their replaceable batteries should last for about a year. They can also support Precision Finding, which gives more exact directions to a lost item, when paired with most models after the iPhone 11. Up to five people can share an AirTag's location, which is helpful for families or large travel groups. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/airtags-are-back-on-sale-with-a-four-pack-going-for-65-202333994.html?src=rss",
          "content": "Most Apple products are pretty expensive, but some of the most affordable (and useful) ones are AirTags. The Bluetooth trackers are priced pretty reasonably even when not on sale, but they can be a steal if you can get them on a discount — like right now. A four pack of AirTags is on sale for $65 at Amazon, which is only a few dollars more than the record-low price we saw during Black Friday this year. AirTags can be useful for people who travel frequently, helping you to keep track of essentials like your passport as well as a way to keep tabs on luggage while you're on the go. If you do purchase some AirTags, we have some recommendations for useful accessories to go along with them, such as different styles of cases to best attach the trackers to different types of items. These are worth looking over and adding to your shopping cart in order to make the most of the product. AirTags have an IP67 rating for water and dust resistance and their replaceable batteries should last for about a year. They can also support Precision Finding, which gives more exact directions to a lost item, when paired with most models after the iPhone 11. Up to five people can share an AirTag's location, which is helpful for families or large travel groups. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/airtags-are-back-on-sale-with-a-four-pack-going-for-65-202333994.html?src=rss",
          "feed_position": 12
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/cameras/insta360-releases-ai-powered-follow-up-to-its-link-webcams-130003572.html",
          "published_at": "Tue, 13 Jan 2026 13:00:03 +0000",
          "title": "Insta360 releases AI-powered follow-up to its Link webcams",
          "standfirst": "Insta360, a company most known for its action cameras, has released two new AI-powered 4K webcams, the Link 2 Pro and Link 2C Pro, aimed at creators, educators and remote professionals. The company's goal with these models is \"a webcam experience that looks and sounds remarkably close to a professional camera and microphone setup.\" Both models use a larger 1/1.3-inch sensor with dual native ISO for improved low-light performance over the previous generation, and both support HDR. Insta360 says the audio on both models leverages beamforming technology as well as AI noise canceling to help voices sound clearer in noisy environments. Users can choose from four pickup modes designed for different sound sources like \"Focus\" that isolates a single voice or \"Wide\" if there are multiple speakers. Video resolution on both models tops out 4K at 30 fps, and Insta360 says its updated True Focus system uses phase-detection autofocus to lock onto subjects, keeping them in focus while they move. There's also a \"Natural Bokeh\" mode meant to mimic the shallow depth-of-field look of a traditional DSLR camera, for users who enjoy that look. As for what sets them apart, the Link 2 Pro sports a 2-axis gimbal for AI-assisted tracking, which offers single or group-mode framing, while the Link 2C Pro is static and designed for fixed-position setups. Both models offer gesture control features, allowing users to control certain functions hands-free. These include starting or stopping tracking and zooming in or out. Both models also include a magnetic mount for easy placement on metal surfaces. Several different modes are offered that aid in teaching and presenting. Among them are Smart Whiteboard mode, which will automatically detect a user's whiteboard and keep it clearly in frame, and DeskView mode, which captures an overhead view of a user's desk. There's also a green screen mode, a portrait mode and support for virtual backgrounds. The new webcams also support Insta360 InSight, the company's subscription AI-powered meeting assistant. InSight can record meetings, generate transcripts, create summaries and more. The Link 2 Pro will retail for $250 while the Link 2C Pro will go for $200. Both models are available for purchase now.This article originally appeared on Engadget at https://www.engadget.com/cameras/insta360-releases-ai-powered-follow-up-to-its-link-webcams-130003572.html?src=rss",
          "content": "Insta360, a company most known for its action cameras, has released two new AI-powered 4K webcams, the Link 2 Pro and Link 2C Pro, aimed at creators, educators and remote professionals. The company's goal with these models is \"a webcam experience that looks and sounds remarkably close to a professional camera and microphone setup.\" Both models use a larger 1/1.3-inch sensor with dual native ISO for improved low-light performance over the previous generation, and both support HDR. Insta360 says the audio on both models leverages beamforming technology as well as AI noise canceling to help voices sound clearer in noisy environments. Users can choose from four pickup modes designed for different sound sources like \"Focus\" that isolates a single voice or \"Wide\" if there are multiple speakers. Video resolution on both models tops out 4K at 30 fps, and Insta360 says its updated True Focus system uses phase-detection autofocus to lock onto subjects, keeping them in focus while they move. There's also a \"Natural Bokeh\" mode meant to mimic the shallow depth-of-field look of a traditional DSLR camera, for users who enjoy that look. As for what sets them apart, the Link 2 Pro sports a 2-axis gimbal for AI-assisted tracking, which offers single or group-mode framing, while the Link 2C Pro is static and designed for fixed-position setups. Both models offer gesture control features, allowing users to control certain functions hands-free. These include starting or stopping tracking and zooming in or out. Both models also include a magnetic mount for easy placement on metal surfaces. Several different modes are offered that aid in teaching and presenting. Among them are Smart Whiteboard mode, which will automatically detect a user's whiteboard and keep it clearly in frame, and DeskView mode, which captures an overhead view of a user's desk. There's also a green screen mode, a portrait mode and support for virtual backgrounds. The new webcams also support Insta360 InSight, the company's subscription AI-powered meeting assistant. InSight can record meetings, generate transcripts, create summaries and more. The Link 2 Pro will retail for $250 while the Link 2C Pro will go for $200. Both models are available for purchase now.This article originally appeared on Engadget at https://www.engadget.com/cameras/insta360-releases-ai-powered-follow-up-to-its-link-webcams-130003572.html?src=rss",
          "feed_position": 13
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/technology/salesforce-rolls-out-new-slackbot-ai-agent-as-it-battles-microsoft-and",
          "published_at": "Tue, 13 Jan 2026 13:00:00 GMT",
          "title": "Salesforce rolls out new Slackbot AI agent as it battles Microsoft and Google in workplace AI",
          "standfirst": "Salesforce on Tuesday launched an entirely rebuilt version of Slackbot, the company&#x27;s workplace assistant, transforming it from a simple notification tool into what executives describe as a fully powered AI agent capable of searching enterprise data, drafting documents, and taking action on behalf of employees.The new Slackbot, now generally available to Business+ and Enterprise+ customers, is Salesforce&#x27;s most aggressive move yet to position Slack at the center of the emerging \"agentic AI\" movement — where software agents work alongside humans to complete complex tasks. The launch comes as Salesforce attempts to convince investors that artificial intelligence will bolster its products rather than render them obsolete.\"Slackbot isn&#x27;t just another copilot or AI assistant,\" said Parker Harris, Salesforce co-founder and Slack&#x27;s chief technology officer, in an exclusive interview with Salesforce. \"It&#x27;s the front door to the agentic enterprise, powered by Salesforce.\"From tricycle to Porsche: Salesforce rebuilt Slackbot from the ground upHarris was blunt about what distinguishes the new Slackbot from its predecessor: \"The old Slackbot was, you know, a little tricycle, and the new Slackbot is like, you know, a Porsche.\"The original Slackbot, which has existed since Slack&#x27;s early days, performed basic algorithmic tasks — reminding users to add colleagues to documents, suggesting channel archives, and delivering simple notifications. The new version runs on an entirely different architecture built around a large language model and sophisticated search capabilities that can access Salesforce records, Google Drive files, calendar data, and years of Slack conversations.\"It&#x27;s two different things,\" Harris explained. \"The old Slackbot was algorithmic and fairly simple. The new Slackbot is brand new — it&#x27;s based around an LLM and a very robust search engine, and connections to third-party search engines, third-party enterprise data.\"Salesforce chose to retain the Slackbot brand despite the fundamental technical overhaul. \"People know what Slackbot is, and so we wanted to carry that forward,\" Harris said.Why Anthropic&#x27;s Claude powers the new Slackbot — and which AI models could come nextThe new Slackbot runs on Claude, Anthropic&#x27;s large language model, a choice driven partly by compliance requirements. Slack&#x27;s commercial service operates under FedRAMP Moderate certification to serve U.S. federal government customers, and Harris said Anthropic was \"the only provider that could give us a compliant LLM\" when Slack began building the new system.But that exclusivity won&#x27;t last. \"We are, this year, going to support additional providers,\" Harris said. \"We have a great relationship with Google. Gemini is incredible — performance is great, cost is great. So we&#x27;re going to use Gemini for some things.\" He added that OpenAI remains a possibility as well.Harris echoed Salesforce CEO Marc Benioff&#x27;s view that large language models are becoming commoditized: \"You&#x27;ve heard Marc talk about LLMs are commodities, that they&#x27;re democratized. I call them CPUs.\"On the sensitive question of training data, Harris was unequivocal: Salesforce does not train any models on customer data. \"Models don&#x27;t have any sort of security,\" he explained. \"If we trained it on some confidential conversation that you and I have, I don&#x27;t want Carolyn to know — if I train it into the LLM, there is no way for me to say you get to see the answer, but Carolyn doesn&#x27;t.\"Inside Salesforce&#x27;s internal experiment: 80,000 employees tested Slackbot with striking resultsSalesforce has been testing the new Slackbot internally for months, rolling it out to all 80,000 employees. According to Ryan Gavin, Slack&#x27;s chief marketing officer, the results have been striking: \"It&#x27;s the fastest adopted product in Salesforce history.\"Internal data shows that two-thirds of Salesforce employees have tried the new Slackbot, with 80% of those users continuing to use it regularly. Internal satisfaction rates reached 96% — the highest for any AI feature Slack has shipped. Employees report saving between two and 20 hours per week.The adoption happened largely organically. \"I think it was about five days, and a Canvas was developed by our employees called &#x27;The Most Stealable Slackbot Prompts,&#x27;\" Gavin said. \"People just started adding to it organically. I think it&#x27;s up to 250-plus prompts that are in this Canvas right now.\"Kate Crotty, a principal UX researcher at Salesforce, found that 73% of internal adoption was driven by social sharing rather than top-down mandates. \"Everybody is there to help each other learn and communicate hacks,\" she said.How Slackbot transforms scattered enterprise data into executive-ready insightsDuring a product demonstration, Amy Bauer, Slack&#x27;s product experience designer, showed how Slackbot can synthesize information across multiple sources. In one example, she asked Slackbot to analyze customer feedback from a pilot program, upload an image of a usage dashboard, and have Slackbot correlate the qualitative and quantitative data.\"This is where Slackbot really earns its keep for me,\" Bauer explained. \"What it&#x27;s doing is not just simply reading the image — it&#x27;s actually looking at the image and comparing it to the insight it just generated for me.\"Slackbot can then query Salesforce to find enterprise accounts with open deals that might be good candidates for early access, creating what Bauer called \"a really great justification and plan to move forward.\" Finally, it can synthesize all that information into a Canvas — Slack&#x27;s collaborative document format — and find calendar availability among stakeholders to schedule a review meeting.\"Up until this point, we have been working in a one-to-one capacity with Slackbot,\" Bauer said. \"But one of the benefits that I can do now is take this insight and have it generate this into a Canvas, a shared workspace where I can iterate on it, refine it with Slackbot, or share it out with my team.\"Rob Seaman, Slack&#x27;s chief product officer, said the Canvas creation demonstrates where the product is heading: \"This is making a tool call internally to Slack Canvas to actually write, effectively, a shared document. But it signals where we&#x27;re going with Slackbot — we&#x27;re eventually going to be adding in additional third-party tool calls.\"MrBeast&#x27;s company became a Slackbot guinea pig—and employees say they&#x27;re saving 90 minutes a dayAmong Salesforce&#x27;s pilot customers is Beast Industries, the parent company of YouTube star MrBeast. Luis Madrigal, the company&#x27;s chief information officer, joined the launch announcement to describe his experience.\"As somebody who has rolled out enterprise technologies for over two decades now, this was practically one of the easiest,\" Madrigal said. \"The plumbing is there. Slack as an implementation, Enterprise Tools — being able to turn on the Slackbot and the Slack AI functionality was as simple as having my team go in, review, do a quick security review.\"Madrigal said his security team signed off \"rather quickly\" — unusual for enterprise AI deployments — because Slackbot accesses only the information each individual user already has permission to view. \"Given all the guardrails you guys have put into place for Slackbot to be unique and customized to only the information that each individual user has, only the conversations and the Slack rooms and Slack channels that they&#x27;re part of—that made my security team sign off rather quickly.\"One Beast Industries employee, Sinan, the head of Beast Games marketing, reported saving \"at bare minimum, 90 minutes a day.\" Another employee, Spencer, a creative supervisor, described it as \"an assistant who&#x27;s paying attention when I&#x27;m not.\"Other pilot customers include Slalom, reMarkable, Xero, Mercari, and Engine. Mollie Bodensteiner, SVP of Operations at Engine, called Slackbot \"an absolute &#x27;chaos tamer&#x27; for our team,\" estimating it saves her about 30 minutes daily \"just by eliminating context switching.\"Slackbot vs. Microsoft Copilot vs. Google Gemini: The fight for enterprise AI dominanceThe launch puts Salesforce in direct competition with Microsoft&#x27;s Copilot, which is integrated into Teams and the broader Microsoft 365 suite, as well as Google&#x27;s Gemini integrations across Workspace. When asked what distinguishes Slackbot from these alternatives, Seaman pointed to context and convenience.\"The thing that makes it most powerful for our customers and users is the proximity — it&#x27;s just right there in your Slack,\" Seaman said. \"There&#x27;s a tremendous convenience affordance that&#x27;s naturally built into it.\"The deeper advantage, executives argue, is that Slackbot already understands users&#x27; work without requiring setup or training. \"Most AI tools sound the same no matter who is using them,\" the company&#x27;s announcement stated. \"They lack context, miss nuance, and force you to jump between tools to get anything done.\"Harris put it more directly: \"If you&#x27;ve ever had that magic experience with AI — I think ChatGPT is a great example, it&#x27;s a great experience from a consumer perspective — Slackbot is really what we&#x27;re doing in the enterprise, to be this employee super agent that is loved, just like people love using Slack.\"Amy Bauer emphasized the frictionless nature of the experience. \"Slackbot is inherently grounded in the context, in the data that you have in Slack,\" she said. \"So as you continue working in Slack, Slackbot gets better because it&#x27;s grounded in the work that you&#x27;re doing there. There is no setup. There is no configuration for those end users.\"Salesforce&#x27;s ambitious plan to make Slackbot the one &#x27;super agent&#x27; that controls all the othersSalesforce positions Slackbot as what Harris calls a \"super agent\" — a central hub that can eventually coordinate with other AI agents across an organization.\"Every corporation is going to have an employee super agent,\" Harris said. \"Slackbot is essentially taking the magic of what Slack does. We think that Slackbot, and we&#x27;re really excited about it, is going to be that.\"The vision extends to third-party agents already launching in Slack. Last month, Anthropic released a preview of Claude Code for Slack, allowing developers to interact with Claude&#x27;s coding capabilities directly in chat threads. OpenAI, Google, Vercel, and others have also built agents for the platform.\"Most of the net-new apps that are being deployed to Slack are agents,\" Seaman noted during the press conference. \"This is proof of the promise of humans and agents coexisting and working together in Slack to solve problems.\"Harris described a future where Slackbot becomes an MCP (Model Context Protocol) client, able to leverage tools from across the software ecosystem — similar to how the developer tool Cursor works. \"Slack can be an MCP client, and Slackbot will be the hub of that, leveraging all these tools out in the world, some of which will be these amazing agents,\" he said.But Harris also cautioned against over-promising on multi-agent coordination. \"I still think we&#x27;re in the single agent world,\" he said. \"FY26 is going to be the year where we started to see more coordination. But we&#x27;re going to do it with customer success in mind, and not demonstrate and talk about, like, &#x27;I&#x27;ve got 1,000 agents working together,&#x27; because I think that&#x27;s unrealistic.\"Slackbot costs nothing extra, but Salesforce&#x27;s data access fees could squeeze some customersSlackbot is included at no additional cost for customers on Business+ and Enterprise+ plans. \"There&#x27;s no additional fees customers have to do,\" Gavin confirmed. \"If they&#x27;re on one of those plans, they&#x27;re going to get Slackbot.\"However, some enterprise customers may face other cost pressures related to Salesforce&#x27;s broader data strategy. CIOs may see price increases for third-party applications that work with Salesforce data, as effects of higher charges for API access ripple through the software supply chain.Fivetran CEO George Fraser has warned that Salesforce&#x27;s shift in pricing policy for API access could have tangible consequences for enterprises relying on Salesforce as a system of record. \"They might not be able to use Fivetran to replicate their data to Snowflake and instead have to use Salesforce Data Cloud. Or they might find that they are not able to interact with their data via ChatGPT, and instead have to use Agentforce,\" Fraser said in a recent CIO report.Salesforce has framed the pricing change as standard industry practice.What Slackbot can do today, what&#x27;s coming in weeks, and what&#x27;s still on the roadmapThe new Slackbot begins rolling out today and will reach all eligible customers by the end of February. Mobile availability will complete by March 3, Bauer confirmed during her interview with VentureBeat.Some capabilities remain works in progress. Calendar reading and availability checking are available at launch, but the ability to actually book meetings is \"coming a few weeks after,\" according to Seaman. Image generation is not currently supported, though Bauer said it&#x27;s \"something that we are looking at in the future.\"When asked about integration with competing CRM systems like HubSpot and Microsoft Dynamics, Salesforce representatives declined to provide specifics during the interview, though they acknowledged the question touched on key competitive differentiators.Salesforce is betting the future of work looks like a chat window—and it&#x27;s not aloneThe Slackbot launch is Salesforce&#x27;s bet that the future of enterprise work is conversational — that employees will increasingly prefer to interact with AI through natural language rather than navigating traditional software interfaces.Harris described Slack&#x27;s product philosophy using principles like \"don&#x27;t make me think\" and \"be a great host.\" The goal, he said, is for Slackbot to surface information proactively rather than requiring users to hunt for it.\"One of the revelations for me is LLMs applied to unstructured information are incredible,\" Harris said. \"And the amount of value you have if you&#x27;re a Slack user, if your corporation uses Slack — the amount of value in Slack is unbelievable. Because you&#x27;re talking about work, you&#x27;re sharing documents, you&#x27;re making decisions, but you can&#x27;t as a human go through that and really get the same value that an LLM can do.\"Looking ahead, Harris expects the interfaces themselves to evolve beyond pure conversation. \"We&#x27;re kind of saturating what we can do with purely conversational UIs,\" he said. \"I think we&#x27;ll start to see agents building an interface that best suits your intent, as opposed to trying to surface something within a conversational interface that matches your intent.\"Microsoft, Google, and a growing roster of AI startups are placing similar bets — that the winning enterprise AI will be the one embedded in the tools workers already use, not another application to learn. The race to become that invisible layer of workplace intelligence is now fully underway.For Salesforce, the stakes extend beyond a single product launch. After a bruising year on Wall Street and persistent questions about whether AI threatens its core business, the company is wagering that Slackbot can prove the opposite — that the tens of millions of people already chatting in Slack every day is not a vulnerability, but an unassailable advantage.Haley Gault, the Salesforce account executive in Pittsburgh who stumbled upon the new Slackbot on a snowy morning, captured the shift in a single sentence: \"I honestly can&#x27;t imagine working for another company not having access to these types of tools. This is just how I work now.\"That&#x27;s precisely what Salesforce is counting on.",
          "content": "Salesforce on Tuesday launched an entirely rebuilt version of Slackbot, the company&#x27;s workplace assistant, transforming it from a simple notification tool into what executives describe as a fully powered AI agent capable of searching enterprise data, drafting documents, and taking action on behalf of employees.The new Slackbot, now generally available to Business+ and Enterprise+ customers, is Salesforce&#x27;s most aggressive move yet to position Slack at the center of the emerging \"agentic AI\" movement — where software agents work alongside humans to complete complex tasks. The launch comes as Salesforce attempts to convince investors that artificial intelligence will bolster its products rather than render them obsolete.\"Slackbot isn&#x27;t just another copilot or AI assistant,\" said Parker Harris, Salesforce co-founder and Slack&#x27;s chief technology officer, in an exclusive interview with Salesforce. \"It&#x27;s the front door to the agentic enterprise, powered by Salesforce.\"From tricycle to Porsche: Salesforce rebuilt Slackbot from the ground upHarris was blunt about what distinguishes the new Slackbot from its predecessor: \"The old Slackbot was, you know, a little tricycle, and the new Slackbot is like, you know, a Porsche.\"The original Slackbot, which has existed since Slack&#x27;s early days, performed basic algorithmic tasks — reminding users to add colleagues to documents, suggesting channel archives, and delivering simple notifications. The new version runs on an entirely different architecture built around a large language model and sophisticated search capabilities that can access Salesforce records, Google Drive files, calendar data, and years of Slack conversations.\"It&#x27;s two different things,\" Harris explained. \"The old Slackbot was algorithmic and fairly simple. The new Slackbot is brand new — it&#x27;s based around an LLM and a very robust search engine, and connections to third-party search engines, third-party enterprise data.\"Salesforce chose to retain the Slackbot brand despite the fundamental technical overhaul. \"People know what Slackbot is, and so we wanted to carry that forward,\" Harris said.Why Anthropic&#x27;s Claude powers the new Slackbot — and which AI models could come nextThe new Slackbot runs on Claude, Anthropic&#x27;s large language model, a choice driven partly by compliance requirements. Slack&#x27;s commercial service operates under FedRAMP Moderate certification to serve U.S. federal government customers, and Harris said Anthropic was \"the only provider that could give us a compliant LLM\" when Slack began building the new system.But that exclusivity won&#x27;t last. \"We are, this year, going to support additional providers,\" Harris said. \"We have a great relationship with Google. Gemini is incredible — performance is great, cost is great. So we&#x27;re going to use Gemini for some things.\" He added that OpenAI remains a possibility as well.Harris echoed Salesforce CEO Marc Benioff&#x27;s view that large language models are becoming commoditized: \"You&#x27;ve heard Marc talk about LLMs are commodities, that they&#x27;re democratized. I call them CPUs.\"On the sensitive question of training data, Harris was unequivocal: Salesforce does not train any models on customer data. \"Models don&#x27;t have any sort of security,\" he explained. \"If we trained it on some confidential conversation that you and I have, I don&#x27;t want Carolyn to know — if I train it into the LLM, there is no way for me to say you get to see the answer, but Carolyn doesn&#x27;t.\"Inside Salesforce&#x27;s internal experiment: 80,000 employees tested Slackbot with striking resultsSalesforce has been testing the new Slackbot internally for months, rolling it out to all 80,000 employees. According to Ryan Gavin, Slack&#x27;s chief marketing officer, the results have been striking: \"It&#x27;s the fastest adopted product in Salesforce history.\"Internal data shows that two-thirds of Salesforce employees have tried the new Slackbot, with 80% of those users continuing to use it regularly. Internal satisfaction rates reached 96% — the highest for any AI feature Slack has shipped. Employees report saving between two and 20 hours per week.The adoption happened largely organically. \"I think it was about five days, and a Canvas was developed by our employees called &#x27;The Most Stealable Slackbot Prompts,&#x27;\" Gavin said. \"People just started adding to it organically. I think it&#x27;s up to 250-plus prompts that are in this Canvas right now.\"Kate Crotty, a principal UX researcher at Salesforce, found that 73% of internal adoption was driven by social sharing rather than top-down mandates. \"Everybody is there to help each other learn and communicate hacks,\" she said.How Slackbot transforms scattered enterprise data into executive-ready insightsDuring a product demonstration, Amy Bauer, Slack&#x27;s product experience designer, showed how Slackbot can synthesize information across multiple sources. In one example, she asked Slackbot to analyze customer feedback from a pilot program, upload an image of a usage dashboard, and have Slackbot correlate the qualitative and quantitative data.\"This is where Slackbot really earns its keep for me,\" Bauer explained. \"What it&#x27;s doing is not just simply reading the image — it&#x27;s actually looking at the image and comparing it to the insight it just generated for me.\"Slackbot can then query Salesforce to find enterprise accounts with open deals that might be good candidates for early access, creating what Bauer called \"a really great justification and plan to move forward.\" Finally, it can synthesize all that information into a Canvas — Slack&#x27;s collaborative document format — and find calendar availability among stakeholders to schedule a review meeting.\"Up until this point, we have been working in a one-to-one capacity with Slackbot,\" Bauer said. \"But one of the benefits that I can do now is take this insight and have it generate this into a Canvas, a shared workspace where I can iterate on it, refine it with Slackbot, or share it out with my team.\"Rob Seaman, Slack&#x27;s chief product officer, said the Canvas creation demonstrates where the product is heading: \"This is making a tool call internally to Slack Canvas to actually write, effectively, a shared document. But it signals where we&#x27;re going with Slackbot — we&#x27;re eventually going to be adding in additional third-party tool calls.\"MrBeast&#x27;s company became a Slackbot guinea pig—and employees say they&#x27;re saving 90 minutes a dayAmong Salesforce&#x27;s pilot customers is Beast Industries, the parent company of YouTube star MrBeast. Luis Madrigal, the company&#x27;s chief information officer, joined the launch announcement to describe his experience.\"As somebody who has rolled out enterprise technologies for over two decades now, this was practically one of the easiest,\" Madrigal said. \"The plumbing is there. Slack as an implementation, Enterprise Tools — being able to turn on the Slackbot and the Slack AI functionality was as simple as having my team go in, review, do a quick security review.\"Madrigal said his security team signed off \"rather quickly\" — unusual for enterprise AI deployments — because Slackbot accesses only the information each individual user already has permission to view. \"Given all the guardrails you guys have put into place for Slackbot to be unique and customized to only the information that each individual user has, only the conversations and the Slack rooms and Slack channels that they&#x27;re part of—that made my security team sign off rather quickly.\"One Beast Industries employee, Sinan, the head of Beast Games marketing, reported saving \"at bare minimum, 90 minutes a day.\" Another employee, Spencer, a creative supervisor, described it as \"an assistant who&#x27;s paying attention when I&#x27;m not.\"Other pilot customers include Slalom, reMarkable, Xero, Mercari, and Engine. Mollie Bodensteiner, SVP of Operations at Engine, called Slackbot \"an absolute &#x27;chaos tamer&#x27; for our team,\" estimating it saves her about 30 minutes daily \"just by eliminating context switching.\"Slackbot vs. Microsoft Copilot vs. Google Gemini: The fight for enterprise AI dominanceThe launch puts Salesforce in direct competition with Microsoft&#x27;s Copilot, which is integrated into Teams and the broader Microsoft 365 suite, as well as Google&#x27;s Gemini integrations across Workspace. When asked what distinguishes Slackbot from these alternatives, Seaman pointed to context and convenience.\"The thing that makes it most powerful for our customers and users is the proximity — it&#x27;s just right there in your Slack,\" Seaman said. \"There&#x27;s a tremendous convenience affordance that&#x27;s naturally built into it.\"The deeper advantage, executives argue, is that Slackbot already understands users&#x27; work without requiring setup or training. \"Most AI tools sound the same no matter who is using them,\" the company&#x27;s announcement stated. \"They lack context, miss nuance, and force you to jump between tools to get anything done.\"Harris put it more directly: \"If you&#x27;ve ever had that magic experience with AI — I think ChatGPT is a great example, it&#x27;s a great experience from a consumer perspective — Slackbot is really what we&#x27;re doing in the enterprise, to be this employee super agent that is loved, just like people love using Slack.\"Amy Bauer emphasized the frictionless nature of the experience. \"Slackbot is inherently grounded in the context, in the data that you have in Slack,\" she said. \"So as you continue working in Slack, Slackbot gets better because it&#x27;s grounded in the work that you&#x27;re doing there. There is no setup. There is no configuration for those end users.\"Salesforce&#x27;s ambitious plan to make Slackbot the one &#x27;super agent&#x27; that controls all the othersSalesforce positions Slackbot as what Harris calls a \"super agent\" — a central hub that can eventually coordinate with other AI agents across an organization.\"Every corporation is going to have an employee super agent,\" Harris said. \"Slackbot is essentially taking the magic of what Slack does. We think that Slackbot, and we&#x27;re really excited about it, is going to be that.\"The vision extends to third-party agents already launching in Slack. Last month, Anthropic released a preview of Claude Code for Slack, allowing developers to interact with Claude&#x27;s coding capabilities directly in chat threads. OpenAI, Google, Vercel, and others have also built agents for the platform.\"Most of the net-new apps that are being deployed to Slack are agents,\" Seaman noted during the press conference. \"This is proof of the promise of humans and agents coexisting and working together in Slack to solve problems.\"Harris described a future where Slackbot becomes an MCP (Model Context Protocol) client, able to leverage tools from across the software ecosystem — similar to how the developer tool Cursor works. \"Slack can be an MCP client, and Slackbot will be the hub of that, leveraging all these tools out in the world, some of which will be these amazing agents,\" he said.But Harris also cautioned against over-promising on multi-agent coordination. \"I still think we&#x27;re in the single agent world,\" he said. \"FY26 is going to be the year where we started to see more coordination. But we&#x27;re going to do it with customer success in mind, and not demonstrate and talk about, like, &#x27;I&#x27;ve got 1,000 agents working together,&#x27; because I think that&#x27;s unrealistic.\"Slackbot costs nothing extra, but Salesforce&#x27;s data access fees could squeeze some customersSlackbot is included at no additional cost for customers on Business+ and Enterprise+ plans. \"There&#x27;s no additional fees customers have to do,\" Gavin confirmed. \"If they&#x27;re on one of those plans, they&#x27;re going to get Slackbot.\"However, some enterprise customers may face other cost pressures related to Salesforce&#x27;s broader data strategy. CIOs may see price increases for third-party applications that work with Salesforce data, as effects of higher charges for API access ripple through the software supply chain.Fivetran CEO George Fraser has warned that Salesforce&#x27;s shift in pricing policy for API access could have tangible consequences for enterprises relying on Salesforce as a system of record. \"They might not be able to use Fivetran to replicate their data to Snowflake and instead have to use Salesforce Data Cloud. Or they might find that they are not able to interact with their data via ChatGPT, and instead have to use Agentforce,\" Fraser said in a recent CIO report.Salesforce has framed the pricing change as standard industry practice.What Slackbot can do today, what&#x27;s coming in weeks, and what&#x27;s still on the roadmapThe new Slackbot begins rolling out today and will reach all eligible customers by the end of February. Mobile availability will complete by March 3, Bauer confirmed during her interview with VentureBeat.Some capabilities remain works in progress. Calendar reading and availability checking are available at launch, but the ability to actually book meetings is \"coming a few weeks after,\" according to Seaman. Image generation is not currently supported, though Bauer said it&#x27;s \"something that we are looking at in the future.\"When asked about integration with competing CRM systems like HubSpot and Microsoft Dynamics, Salesforce representatives declined to provide specifics during the interview, though they acknowledged the question touched on key competitive differentiators.Salesforce is betting the future of work looks like a chat window—and it&#x27;s not aloneThe Slackbot launch is Salesforce&#x27;s bet that the future of enterprise work is conversational — that employees will increasingly prefer to interact with AI through natural language rather than navigating traditional software interfaces.Harris described Slack&#x27;s product philosophy using principles like \"don&#x27;t make me think\" and \"be a great host.\" The goal, he said, is for Slackbot to surface information proactively rather than requiring users to hunt for it.\"One of the revelations for me is LLMs applied to unstructured information are incredible,\" Harris said. \"And the amount of value you have if you&#x27;re a Slack user, if your corporation uses Slack — the amount of value in Slack is unbelievable. Because you&#x27;re talking about work, you&#x27;re sharing documents, you&#x27;re making decisions, but you can&#x27;t as a human go through that and really get the same value that an LLM can do.\"Looking ahead, Harris expects the interfaces themselves to evolve beyond pure conversation. \"We&#x27;re kind of saturating what we can do with purely conversational UIs,\" he said. \"I think we&#x27;ll start to see agents building an interface that best suits your intent, as opposed to trying to surface something within a conversational interface that matches your intent.\"Microsoft, Google, and a growing roster of AI startups are placing similar bets — that the winning enterprise AI will be the one embedded in the tools workers already use, not another application to learn. The race to become that invisible layer of workplace intelligence is now fully underway.For Salesforce, the stakes extend beyond a single product launch. After a bruising year on Wall Street and persistent questions about whether AI threatens its core business, the company is wagering that Slackbot can prove the opposite — that the tens of millions of people already chatting in Slack every day is not a vulnerability, but an unassailable advantage.Haley Gault, the Salesforce account executive in Pittsburgh who stumbled upon the new Slackbot on a snowy morning, captured the shift in a single sentence: \"I honestly can&#x27;t imagine working for another company not having access to these types of tools. This is just how I work now.\"That&#x27;s precisely what Salesforce is counting on.",
          "feed_position": 2,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/4Xrcg14GLKFlwSEnuEzxyS/21c85d29d03c4c974076475c009e3b38/nuneybits_Vector_art_of_chat_bubbles_on_a_computer_screen_in_th_5018a7ea-3496-4103-8453-7ba1b129189a.webp?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/technology/why-sakana-ais-big-win-is-a-big-deal-for-the-future-of-enterprise-agents",
          "published_at": "Tue, 13 Jan 2026 12:59:00 GMT",
          "title": "Why Sakana AI’s big win is a big deal for the future of enterprise agents",
          "standfirst": "In an impressive feat, Japanese startup Sakana AI’s coding agent ALE-Agent recently secured first place in the AtCoder Heuristic Contest (AHC058), a complex coding competition that involves complicated optimization problems — and a more difficult and perhaps telling challenge than benchmarks like HumanEval, which mostly test the ability to write isolated functions, and which many AI models and agents now regularly pass with ease (\"benchmark saturation\"). Sakana&#x27;s accomplishment with ALE-Agent hints at a shift toward agents capable of autonomously optimizing themselves to navigate and perform well in complex, dynamic systems such as enterprise software stacks, workflows, and operational environments. In four hours, the agent used inference-time scaling to generate, test, and iterate over hundreds of solutions, solving a problem that typically requires deep intuition and time-consuming trial and error from human experts. It outperformed over 800 human participants, including top-tier competitive programmers.How ALE-Agent worksThe challenge in AHC058 was a classic combinatorial optimization problem. Participants were tasked with managing a set of machines with hierarchical relationships, such as machines that produce apples, and other machines that build those apple-producing machines. The goal was to maximize output over a fixed number of turns.In the enterprise world, this workflow usually follows a strict pattern: a domain expert works with a client to define an \"objective function\" (aka the Scorer), and then engineers build a software system to optimize it. These problems are notoriously difficult because they cannot be solved in a single stage. They require exploration, strategy, and the ability to pivot when a plan isn&#x27;t working.Human experts typically approach this using a two-stage strategy. First, they use a \"Greedy\" method (a lightweight solver that makes the best immediate choice at each step) to generate a decent baseline solution. Then, they apply \"simulated annealing,\" a technique that takes the existing plan and makes tiny, random adjustments to see if the score improves. However, this standard approach is rigid. If the initial Greedy plan heads in the wrong direction, simulated annealing can rarely fix it because it only looks for local improvements in a faulty area of the solution space.ALE-Agent’s innovation was transforming this static initialization tool into a dynamic reconstruction engine. Instead of relying on immediate value, the agent independently derived a concept it called \"Virtual Power.\" It assigned values to components that were not yet operational, treating them as if they already possessed value. By valuing potential future assets rather than just current ones, the agent capitalized on the \"compound interest effect,\" a concept it explicitly identified in its internal logs. Basically, it could look a few steps ahead and reason about the future instead of looking at the immediate feedback it was receiving from its environment.Crucially, the agent needed to maintain this strategy over a four-hour window without losing focus, a common failure mode known as “context drift.” In comments provided to VentureBeat, the Sakana AI team explained that the agent generates textual \"insights\" by reflecting on each trial. It gathers this knowledge to prevent cycling back to previously failed strategies and creates a working memory that allows it to look a few steps ahead rather than just reacting to immediate feedback.Furthermore, the agent integrated Greedy methods directly into the simulated annealing phase to avoid getting stuck in local optima, using high-speed reconstruction to delete and rebuild large sections of the solution on the fly.From coding to enterprise optimizationThis breakthrough fits directly into existing enterprise workflows where a scoring function is already available. Currently, companies rely on scarce engineering talent to write optimization algorithms. ALE-Agent demonstrates a future where humans define the \"Scorer\" (i.e., the business logic and goals) and the agent handles the technical implementation.This shifts the operational bottleneck from engineering capacity to metric clarity. If an enterprise can measure a goal, the agent can optimize it. This has direct applications in logistics, such as vehicle routing, as well as server load balancing and resource allocation.According to the Sakana AI team, this could democratize optimization. \"It enables a future where non-technical clients can interact directly with the agent, tweaking business constraints in real-time until they get the output they desire,\" they said.The Sakana AI team told VentureBeat that ALE-Agent is currently proprietary and not available for public use, and the company is currently focused on internal development and proof-of-concept collaborations with enterprises.At the same time, the team is already looking ahead to \"self-rewriting\" agents. These future agents could define their own scorers, making them feasible for ill-defined problems where human experts struggle to formulate clear initial metrics.The cost of intelligenceRunning ALE-Agent was not cheap. The four-hour operation incurred approximately $1,300 in compute costs involving over 4,000 reasoning calls to models like GPT-5.2 and Gemini 3 Pro. While this price point might seem high for a single coding task, the return on investment for optimization problems is often asymmetric. In a resource-management setting, a one-time cost of a few thousand dollars can result in millions of dollars in annual efficiency savings.However, enterprises expecting costs to simply drop might be missing the strategic picture. While the cost of tokens is falling, total spend may actually rise as companies compete for better answers, a concept known as the Jevons paradox.\"While smarter algorithms will drive efficiency, the primary value of AI is its ability to explore vast solution spaces,\" the Sakana AI team said. \"As inference costs fall, rather than simply banking the savings, enterprises will likely choose to leverage that affordability to conduct even deeper, broader searches to find superior solutions.\"The experiment highlights the immense value still to be unlocked through inference-time scaling techniques. As AI systems gain the ability to handle complex reasoning tasks across longer contexts, building better scaffolding and allocating larger budgets for \"thinking time\" allows agents to rival top human experts.",
          "content": "In an impressive feat, Japanese startup Sakana AI’s coding agent ALE-Agent recently secured first place in the AtCoder Heuristic Contest (AHC058), a complex coding competition that involves complicated optimization problems — and a more difficult and perhaps telling challenge than benchmarks like HumanEval, which mostly test the ability to write isolated functions, and which many AI models and agents now regularly pass with ease (\"benchmark saturation\"). Sakana&#x27;s accomplishment with ALE-Agent hints at a shift toward agents capable of autonomously optimizing themselves to navigate and perform well in complex, dynamic systems such as enterprise software stacks, workflows, and operational environments. In four hours, the agent used inference-time scaling to generate, test, and iterate over hundreds of solutions, solving a problem that typically requires deep intuition and time-consuming trial and error from human experts. It outperformed over 800 human participants, including top-tier competitive programmers.How ALE-Agent worksThe challenge in AHC058 was a classic combinatorial optimization problem. Participants were tasked with managing a set of machines with hierarchical relationships, such as machines that produce apples, and other machines that build those apple-producing machines. The goal was to maximize output over a fixed number of turns.In the enterprise world, this workflow usually follows a strict pattern: a domain expert works with a client to define an \"objective function\" (aka the Scorer), and then engineers build a software system to optimize it. These problems are notoriously difficult because they cannot be solved in a single stage. They require exploration, strategy, and the ability to pivot when a plan isn&#x27;t working.Human experts typically approach this using a two-stage strategy. First, they use a \"Greedy\" method (a lightweight solver that makes the best immediate choice at each step) to generate a decent baseline solution. Then, they apply \"simulated annealing,\" a technique that takes the existing plan and makes tiny, random adjustments to see if the score improves. However, this standard approach is rigid. If the initial Greedy plan heads in the wrong direction, simulated annealing can rarely fix it because it only looks for local improvements in a faulty area of the solution space.ALE-Agent’s innovation was transforming this static initialization tool into a dynamic reconstruction engine. Instead of relying on immediate value, the agent independently derived a concept it called \"Virtual Power.\" It assigned values to components that were not yet operational, treating them as if they already possessed value. By valuing potential future assets rather than just current ones, the agent capitalized on the \"compound interest effect,\" a concept it explicitly identified in its internal logs. Basically, it could look a few steps ahead and reason about the future instead of looking at the immediate feedback it was receiving from its environment.Crucially, the agent needed to maintain this strategy over a four-hour window without losing focus, a common failure mode known as “context drift.” In comments provided to VentureBeat, the Sakana AI team explained that the agent generates textual \"insights\" by reflecting on each trial. It gathers this knowledge to prevent cycling back to previously failed strategies and creates a working memory that allows it to look a few steps ahead rather than just reacting to immediate feedback.Furthermore, the agent integrated Greedy methods directly into the simulated annealing phase to avoid getting stuck in local optima, using high-speed reconstruction to delete and rebuild large sections of the solution on the fly.From coding to enterprise optimizationThis breakthrough fits directly into existing enterprise workflows where a scoring function is already available. Currently, companies rely on scarce engineering talent to write optimization algorithms. ALE-Agent demonstrates a future where humans define the \"Scorer\" (i.e., the business logic and goals) and the agent handles the technical implementation.This shifts the operational bottleneck from engineering capacity to metric clarity. If an enterprise can measure a goal, the agent can optimize it. This has direct applications in logistics, such as vehicle routing, as well as server load balancing and resource allocation.According to the Sakana AI team, this could democratize optimization. \"It enables a future where non-technical clients can interact directly with the agent, tweaking business constraints in real-time until they get the output they desire,\" they said.The Sakana AI team told VentureBeat that ALE-Agent is currently proprietary and not available for public use, and the company is currently focused on internal development and proof-of-concept collaborations with enterprises.At the same time, the team is already looking ahead to \"self-rewriting\" agents. These future agents could define their own scorers, making them feasible for ill-defined problems where human experts struggle to formulate clear initial metrics.The cost of intelligenceRunning ALE-Agent was not cheap. The four-hour operation incurred approximately $1,300 in compute costs involving over 4,000 reasoning calls to models like GPT-5.2 and Gemini 3 Pro. While this price point might seem high for a single coding task, the return on investment for optimization problems is often asymmetric. In a resource-management setting, a one-time cost of a few thousand dollars can result in millions of dollars in annual efficiency savings.However, enterprises expecting costs to simply drop might be missing the strategic picture. While the cost of tokens is falling, total spend may actually rise as companies compete for better answers, a concept known as the Jevons paradox.\"While smarter algorithms will drive efficiency, the primary value of AI is its ability to explore vast solution spaces,\" the Sakana AI team said. \"As inference costs fall, rather than simply banking the savings, enterprises will likely choose to leverage that affordability to conduct even deeper, broader searches to find superior solutions.\"The experiment highlights the immense value still to be unlocked through inference-time scaling techniques. As AI systems gain the ability to handle complex reasoning tasks across longer contexts, building better scaffolding and allocating larger budgets for \"thinking time\" allows agents to rival top human experts.",
          "feed_position": 3,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/3BUB5AM3ylCZKK1ID2lgQV/b563d16484c9a96c1bc412c1a4f404e2/AI_optimization_algorithm.jpg?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/general/the-morning-after-engadget-newsletter-151521620.html",
          "published_at": "Tue, 13 Jan 2026 12:15:21 +0000",
          "title": "The Morning After: Apple will use Gemini to power Siri AI",
          "standfirst": "Apple and Google have confirmed that Gemini’s models power the new version of Siri and other generative AI features. CNBC broke the news, but Apple and Google soon followed up with a lengthy joint statement. Here’s part of it: “Apple determined that Google’s Al technology provides the most capable foundation for Apple Foundation Models… Apple Intelligence will continue to run on Apple devices and Private Cloud Compute, while maintaining Apple’s industry-leading privacy standards.” In June, it was reported that Apple was considering partnerships with OpenAI and Anthropic for Siri (the voice assistant can currently tap ChatGPT for certain queries as part of Apple Intelligence). Two months later, Google emerged as a contender. Another report suggested Apple might build the new Siri using a custom version of Gemini — and that it would pay Google around $1 billion a year for the privilege. However, no official deal numbers were shared. It’s also notable that current iPhones have direct access to OpenAI’s ChatGPT. But how long for? — Mat Smith The other big stories this morning Engadget Podcast: Best of CES 2026 and a chat with Pebble’s founder Meta closes 550,000 accounts to comply with Australia’s kids social media ban The best winter tech to get you through the coldest months Meta appoints ex-Trump and Bush official as its new president and vice chair Netflix wins 7 awards at the Golden Globes Adolescence and KPop Demon Hunters picked up several each. Netflix Netflix’s hit show Adolescence received four awards, including best limited or anthology series. It also won best actor (Stephen Graham), supporting actor (Owen Cooper) and supporting actress (Erin Doherty) in a miniseries or television film. KPop Demon Hunters — which my nieces refuse to stop talking about — won best animated feature and best original song. “I just want to say this award goes to people who have had doors closed on them, and I can confidently say rejection is redirection. So never give up.” It’s not all good news. Netflix also won best performance in stand-up comedy on television for Ricky Gervais: Mortality. Apple and HBO Max each won three, and Hulu got one award on the night. Continue reading. NASA makes final preparations for its first crewed moon mission in over 50 years The agency is targeting a February launch date for Artemis 2. A few years ago, NASA announced it was pushing the Artemis 2 mission back to April 2026. The agency now says it could launch as early as February. NASA is finalizing preparations for the mission and will soon roll out the Space Launch System (SLS) rocket and the Orion spacecraft to the launch pad at the Kennedy Space Center in Florida. Artemis 2 is the first crewed mission to the moon since the Apollo program’s final flight in 1972. The 10-day mission will have four astronauts, who’ll test whether Orion’s critical life-support systems can sustain human passengers on future longer-duration missions. They will first orbit the Earth twice before making their way 4,700 miles beyond the far side of the moon. Continue reading. Lego’s first Pokémon sets are now available for pre-order Pikachu, Eevee, Venusaur, Charizard and Blastoise will ship February 27. Lego Pre-orders for the first three Lego-Pokémon kits are open now. One of the debut pocket monsters is, of course, Pikachu. You can build the 2,050-piece kit to show Pikachu either at rest or leaping out of an open Poké Ball into battle. It costs $200. There’s also a 587-piece model of Eevee, for $60. On the bigger side is a set featuring a trio of Pokémon: Venusaur, Charizard and Blastoise. This kit has 6,838 pieces and can show the group together in battle formation or separately in their own mini environments. It retails for $650 and is a bit much. Continue reading.This article originally appeared on Engadget at https://www.engadget.com/general/the-morning-after-engadget-newsletter-151521620.html?src=rss",
          "content": "Apple and Google have confirmed that Gemini’s models power the new version of Siri and other generative AI features. CNBC broke the news, but Apple and Google soon followed up with a lengthy joint statement. Here’s part of it: “Apple determined that Google’s Al technology provides the most capable foundation for Apple Foundation Models… Apple Intelligence will continue to run on Apple devices and Private Cloud Compute, while maintaining Apple’s industry-leading privacy standards.” In June, it was reported that Apple was considering partnerships with OpenAI and Anthropic for Siri (the voice assistant can currently tap ChatGPT for certain queries as part of Apple Intelligence). Two months later, Google emerged as a contender. Another report suggested Apple might build the new Siri using a custom version of Gemini — and that it would pay Google around $1 billion a year for the privilege. However, no official deal numbers were shared. It’s also notable that current iPhones have direct access to OpenAI’s ChatGPT. But how long for? — Mat Smith The other big stories this morning Engadget Podcast: Best of CES 2026 and a chat with Pebble’s founder Meta closes 550,000 accounts to comply with Australia’s kids social media ban The best winter tech to get you through the coldest months Meta appoints ex-Trump and Bush official as its new president and vice chair Netflix wins 7 awards at the Golden Globes Adolescence and KPop Demon Hunters picked up several each. Netflix Netflix’s hit show Adolescence received four awards, including best limited or anthology series. It also won best actor (Stephen Graham), supporting actor (Owen Cooper) and supporting actress (Erin Doherty) in a miniseries or television film. KPop Demon Hunters — which my nieces refuse to stop talking about — won best animated feature and best original song. “I just want to say this award goes to people who have had doors closed on them, and I can confidently say rejection is redirection. So never give up.” It’s not all good news. Netflix also won best performance in stand-up comedy on television for Ricky Gervais: Mortality. Apple and HBO Max each won three, and Hulu got one award on the night. Continue reading. NASA makes final preparations for its first crewed moon mission in over 50 years The agency is targeting a February launch date for Artemis 2. A few years ago, NASA announced it was pushing the Artemis 2 mission back to April 2026. The agency now says it could launch as early as February. NASA is finalizing preparations for the mission and will soon roll out the Space Launch System (SLS) rocket and the Orion spacecraft to the launch pad at the Kennedy Space Center in Florida. Artemis 2 is the first crewed mission to the moon since the Apollo program’s final flight in 1972. The 10-day mission will have four astronauts, who’ll test whether Orion’s critical life-support systems can sustain human passengers on future longer-duration missions. They will first orbit the Earth twice before making their way 4,700 miles beyond the far side of the moon. Continue reading. Lego’s first Pokémon sets are now available for pre-order Pikachu, Eevee, Venusaur, Charizard and Blastoise will ship February 27. Lego Pre-orders for the first three Lego-Pokémon kits are open now. One of the debut pocket monsters is, of course, Pikachu. You can build the 2,050-piece kit to show Pikachu either at rest or leaping out of an open Poké Ball into battle. It costs $200. There’s also a 587-piece model of Eevee, for $60. On the bigger side is a set featuring a trio of Pokémon: Venusaur, Charizard and Blastoise. This kit has 6,838 pieces and can show the group together in battle formation or separately in their own mini environments. It retails for $650 and is a bit much. Continue reading.This article originally appeared on Engadget at https://www.engadget.com/general/the-morning-after-engadget-newsletter-151521620.html?src=rss",
          "feed_position": 14,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2026-01/28879300-f057-11f0-baeb-ffbd608b486c"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/entertainment/streaming/best-streaming-devices-media-players-123021395.html",
          "published_at": "Tue, 13 Jan 2026 10:01:26 +0000",
          "title": "The best streaming devices for 2026",
          "standfirst": "With the dominance of smart TVs, streaming sticks and boxes may seem redundant — but if your smart set is slow or has a frustrating user interface, a streaming device will let you bypass your TV’s built-in OS and use Google TV, Fire TV, Apple TV or something else instead. There are a lot of streaming gadgets out there, all with different operating systems, memory capacities, video resolutions and bonus features, such as headphone connections and ambient modes that fill your screen with stills when you’re not watching. We tested options from the major brands and broke down exactly what each device gives you so you can pick the best streaming device for your TV. Table of contents The best TV streaming devices for 2026 What to look for in a TV streaming device How we tested and picked the best streaming devices Best streaming devices for 2026 What to look for in a TV streaming device Operating system and interface Google’s TV Streamer, the Apple TV 4K, Amazon’s Fire TV Sticks and Roku devices are the most popular players in the space. Three of those brands also come built into TVs, such as Fire, Google and Roku TVs, but the Apple TV 4K doesn't come pre-loaded on any set. Each one has a unique operating system and interface. This may be the biggest deciding factor for many people, as it determines how the home entertainment you want to watch is arranged and presented. We go into detail for each platform below, but all of them come with home screens that, to varying degrees, gather your apps in one place, present the movies and TV shows you’re currently watching and give you suggestions of other media streaming options. Nearly all streaming devices come with a remote that lets you search and do other operations using your voice, eliminating the need to hunt and peck at on-screen keyboards. They all offer “universal search,” in which searching for a title takes you to whichever app has it available. If you want to watch Wicked but don’t know where it’s playing, just push the voice button on the remote and say \"Wicked.” (We found simply saying the title or the genre you want sometimes works better than saying “Show me…” or “Search for…”) From the search results, hit the play button and the correct app will open and start playing — assuming you’ve previously logged into that app and, in most cases, have an active subscription. Connectivity Most streaming sticks connect to the internet via Wi-Fi, with the majority of them supporting Wi-Fi 5 or 6 protocols. Set-top boxes can also have Ethernet ports, so you can hardwire your internet connection to the device, which is typically faster than wireless. Streaming media players connect to your TV through an HDMI port, and most sticks hide behind the screen, while set-top boxes sit on a surface nearby. Nearly all units also plug into an AC outlet for power. Some sticks used to work by pulling power from a USB port on the TV, but increasingly, these devices are designed to plug into the wall. Video and audio features If your home theater setup has a screen that can display 4K content with Dolby Vision and HDR10, you’ll want a streaming device that supports those high-end formats. Of course, even the most top-shelf streamer can’t make a 1080p TV stream 4K. The series or movie also has to be transmitted in 4K and, increasingly, companies restrict higher-quality streaming to more expensive subscription plans. In short, every element needs to support the video or audio feature, otherwise the highest quality you’ll get will be the lowest of any component in the chain. Remotes Most remotes that come with streaming devices will allow you to control the power and volume of your TV. Some of the less expensive devices, however, don't have that feature, so you'll need to use your TV's remote control to turn it on, then use the streaming remote to navigate the streamer's interface. If your streamer's remote does offer power and volume controls, the setup process will usually calibrate your remote to your TV. If you want to use a soundbar, such as from Sonos or other brands, for audio you may also have to take the additional step of pairing your remote to the speaker. Voice control In addition to helping you find stuff to watch, streaming devices from Apple, Google and Amazon can answer questions about the weather, sports scores and general facts using built-in voice assistants. They can also act as smart home controllers to turn off connected smart bulbs or plugs and show feeds from smart cameras. Just remember, as with all smart home devices, compatibility is key. Fire TV devices work with Alexa-enabled smart home equipment; the Google TV Streamer lets you control Google Home devices; and Apple TV 4Ks play nice with HomeKit and other Apple devices. Rokus grant power over Roku’s smart home products, but also work with the other ecosystems. How we tested and picked the best streaming devices Like every gadget we test, we start by researching what’s worthy of reviewing. Then we get a hold of the devices ourselves and see how well they work. We don’t have a central Engadget lab; we test things in our own living rooms, on our own TV sets. We also figure that’s a better approximation of your own TV experience anyway. We began testing streaming devices as far back as 2007 with the first Apple TV device. Since then, we’ve tried out most of the major new releases to come along — from the Roku Stick back in 2014 to the 2024 Google TV Streamer 4K. A few years ago, we decided to compile the streaming devices we reviewed into this guide. Since then, we’ve updated our top picks using verdicts from our reviews, as well the testing we perform just for this guide. As new devices come out, we try them and, if something is worthy, we add it to our top picks on this list.This article originally appeared on Engadget at https://www.engadget.com/entertainment/streaming/best-streaming-devices-media-players-123021395.html?src=rss",
          "content": "With the dominance of smart TVs, streaming sticks and boxes may seem redundant — but if your smart set is slow or has a frustrating user interface, a streaming device will let you bypass your TV’s built-in OS and use Google TV, Fire TV, Apple TV or something else instead. There are a lot of streaming gadgets out there, all with different operating systems, memory capacities, video resolutions and bonus features, such as headphone connections and ambient modes that fill your screen with stills when you’re not watching. We tested options from the major brands and broke down exactly what each device gives you so you can pick the best streaming device for your TV. Table of contents The best TV streaming devices for 2026 What to look for in a TV streaming device How we tested and picked the best streaming devices Best streaming devices for 2026 What to look for in a TV streaming device Operating system and interface Google’s TV Streamer, the Apple TV 4K, Amazon’s Fire TV Sticks and Roku devices are the most popular players in the space. Three of those brands also come built into TVs, such as Fire, Google and Roku TVs, but the Apple TV 4K doesn't come pre-loaded on any set. Each one has a unique operating system and interface. This may be the biggest deciding factor for many people, as it determines how the home entertainment you want to watch is arranged and presented. We go into detail for each platform below, but all of them come with home screens that, to varying degrees, gather your apps in one place, present the movies and TV shows you’re currently watching and give you suggestions of other media streaming options. Nearly all streaming devices come with a remote that lets you search and do other operations using your voice, eliminating the need to hunt and peck at on-screen keyboards. They all offer “universal search,” in which searching for a title takes you to whichever app has it available. If you want to watch Wicked but don’t know where it’s playing, just push the voice button on the remote and say \"Wicked.” (We found simply saying the title or the genre you want sometimes works better than saying “Show me…” or “Search for…”) From the search results, hit the play button and the correct app will open and start playing — assuming you’ve previously logged into that app and, in most cases, have an active subscription. Connectivity Most streaming sticks connect to the internet via Wi-Fi, with the majority of them supporting Wi-Fi 5 or 6 protocols. Set-top boxes can also have Ethernet ports, so you can hardwire your internet connection to the device, which is typically faster than wireless. Streaming media players connect to your TV through an HDMI port, and most sticks hide behind the screen, while set-top boxes sit on a surface nearby. Nearly all units also plug into an AC outlet for power. Some sticks used to work by pulling power from a USB port on the TV, but increasingly, these devices are designed to plug into the wall. Video and audio features If your home theater setup has a screen that can display 4K content with Dolby Vision and HDR10, you’ll want a streaming device that supports those high-end formats. Of course, even the most top-shelf streamer can’t make a 1080p TV stream 4K. The series or movie also has to be transmitted in 4K and, increasingly, companies restrict higher-quality streaming to more expensive subscription plans. In short, every element needs to support the video or audio feature, otherwise the highest quality you’ll get will be the lowest of any component in the chain. Remotes Most remotes that come with streaming devices will allow you to control the power and volume of your TV. Some of the less expensive devices, however, don't have that feature, so you'll need to use your TV's remote control to turn it on, then use the streaming remote to navigate the streamer's interface. If your streamer's remote does offer power and volume controls, the setup process will usually calibrate your remote to your TV. If you want to use a soundbar, such as from Sonos or other brands, for audio you may also have to take the additional step of pairing your remote to the speaker. Voice control In addition to helping you find stuff to watch, streaming devices from Apple, Google and Amazon can answer questions about the weather, sports scores and general facts using built-in voice assistants. They can also act as smart home controllers to turn off connected smart bulbs or plugs and show feeds from smart cameras. Just remember, as with all smart home devices, compatibility is key. Fire TV devices work with Alexa-enabled smart home equipment; the Google TV Streamer lets you control Google Home devices; and Apple TV 4Ks play nice with HomeKit and other Apple devices. Rokus grant power over Roku’s smart home products, but also work with the other ecosystems. How we tested and picked the best streaming devices Like every gadget we test, we start by researching what’s worthy of reviewing. Then we get a hold of the devices ourselves and see how well they work. We don’t have a central Engadget lab; we test things in our own living rooms, on our own TV sets. We also figure that’s a better approximation of your own TV experience anyway. We began testing streaming devices as far back as 2007 with the first Apple TV device. Since then, we’ve tried out most of the major new releases to come along — from the Roku Stick back in 2014 to the 2024 Google TV Streamer 4K. A few years ago, we decided to compile the streaming devices we reviewed into this guide. Since then, we’ve updated our top picks using verdicts from our reviews, as well the testing we perform just for this guide. As new devices come out, we try them and, if something is worthy, we add it to our top picks on this list.This article originally appeared on Engadget at https://www.engadget.com/entertainment/streaming/best-streaming-devices-media-players-123021395.html?src=rss",
          "feed_position": 16
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/our-favorite-ugreen-3-in-1-wireless-charger-is-32-percent-off-right-now-214707069.html",
          "published_at": "Mon, 12 Jan 2026 21:47:07 +0000",
          "title": "Our favorite UGreen 3-in-1 wireless charger is 32 percent off right now",
          "standfirst": "Now that the winter holidays are well and truly past, now's the perfect time to take stock of your tech setup. If you were gifted (or gifted yourself) some new gear in December, make sure that you've got the proper accessories to keep that gear performing at its best. If a new way to power all those batteries would be a benefit, Amazon's currently running a discount on an excellent wireless charging pad. The UGREEN MagFlow Qi2 3-in-1 Charger Station 25W is on sale for $95. That's only a little bit above the lowest price we've ever seen for the product (which was $90), and it's still a 32 percent discount off its usual cost. This is our top pick for a 3-in-1 charging pad thanks to its versatility. The UGREEN can work equally well as a permanent fixture in your home or act as a portable charging station. It boasts a foldable design and has smart little design details to keep it feeling like a premium product. The Qi2 25W charging works across a range of iPhone models and accessories, such as AirPods. There's also a dedicated part of the pad's design for an Apple Watch, which uses a proprietary charging standard, to power up too. Just note that you'll need a newer model of phone and the latest iOS 26 in order to take full advantage of the 25W charging capability. The wireless pad also comes with both a charging plug and a cable. We felt this UGREEN model was a great value at $140, so being able to snag one for a third of the usual price is an even better deal. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/our-favorite-ugreen-3-in-1-wireless-charger-is-32-percent-off-right-now-214707069.html?src=rss",
          "content": "Now that the winter holidays are well and truly past, now's the perfect time to take stock of your tech setup. If you were gifted (or gifted yourself) some new gear in December, make sure that you've got the proper accessories to keep that gear performing at its best. If a new way to power all those batteries would be a benefit, Amazon's currently running a discount on an excellent wireless charging pad. The UGREEN MagFlow Qi2 3-in-1 Charger Station 25W is on sale for $95. That's only a little bit above the lowest price we've ever seen for the product (which was $90), and it's still a 32 percent discount off its usual cost. This is our top pick for a 3-in-1 charging pad thanks to its versatility. The UGREEN can work equally well as a permanent fixture in your home or act as a portable charging station. It boasts a foldable design and has smart little design details to keep it feeling like a premium product. The Qi2 25W charging works across a range of iPhone models and accessories, such as AirPods. There's also a dedicated part of the pad's design for an Apple Watch, which uses a proprietary charging standard, to power up too. Just note that you'll need a newer model of phone and the latest iOS 26 in order to take full advantage of the 25W charging capability. The wireless pad also comes with both a charging plug and a cable. We felt this UGREEN model was a great value at $140, so being able to snag one for a third of the usual price is an even better deal. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/our-favorite-ugreen-3-in-1-wireless-charger-is-32-percent-off-right-now-214707069.html?src=rss",
          "feed_position": 18
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/entertainment/legos-first-pokemon-sets-are-now-available-for-pre-order-205527102.html",
          "published_at": "Mon, 12 Jan 2026 20:55:27 +0000",
          "title": "Lego's first Pokémon sets are now available for pre-order",
          "standfirst": "We learned last March that Lego and Pokémon would be joining forces and the first results of their partnership are here. Pre-orders for all three kits are open now, with an expected ship date of February 27. As one might have guessed from the lightning bolts on the previous promotional image, one of the debut pocket monsters getting the brick treatment is Pikachu, complete with a Poké Ball. The 2,050-piece kit can be built to show Pikachu either leaping out of the open Poké Ball into battle or at rest staring up at the builder, closed Poké Ball between his paws. The Pikachu kit costs $200. There's also a 587-piece model of Eevee, which goes for $60. On the bigger side is a set featuring a trio of Pokémon: Venusaur, Charizard and Blastoise. This kit has 6,838 pieces and can show the group together in battle formation or separately in their own mini environments. It retails for $650. Lego Before you leap to pre-order pages, however, here's a word of caution. In Lego form, our little friends look…kinda strange? I'm not the biggest Lego builder, but I am a rather accomplished architect in Minecraft, so I am well aware of the innate challenge in constructing a rounded shape from square blocks. Take Pikachu, for instance. Part of his appeal is his chubby little cheeks. There are bricks with more rounded sides in this collection that hint at his usual rotundness, but the proportions of his face just feel a little off to me. I had the same reaction to the other figures as well, although Eevee seems to have fared a little better than the others. They're all sort of cute, but not nearly so cute as they are in other formats. But like I said, Lego is not my personal block of choice, so perhaps I'm in the minority here! If you love these bricky pocket monsters, then roll on over to Lego's website and snap up these kits faster than a Mewtwo. This article originally appeared on Engadget at https://www.engadget.com/entertainment/legos-first-pokemon-sets-are-now-available-for-pre-order-205527102.html?src=rss",
          "content": "We learned last March that Lego and Pokémon would be joining forces and the first results of their partnership are here. Pre-orders for all three kits are open now, with an expected ship date of February 27. As one might have guessed from the lightning bolts on the previous promotional image, one of the debut pocket monsters getting the brick treatment is Pikachu, complete with a Poké Ball. The 2,050-piece kit can be built to show Pikachu either leaping out of the open Poké Ball into battle or at rest staring up at the builder, closed Poké Ball between his paws. The Pikachu kit costs $200. There's also a 587-piece model of Eevee, which goes for $60. On the bigger side is a set featuring a trio of Pokémon: Venusaur, Charizard and Blastoise. This kit has 6,838 pieces and can show the group together in battle formation or separately in their own mini environments. It retails for $650. Lego Before you leap to pre-order pages, however, here's a word of caution. In Lego form, our little friends look…kinda strange? I'm not the biggest Lego builder, but I am a rather accomplished architect in Minecraft, so I am well aware of the innate challenge in constructing a rounded shape from square blocks. Take Pikachu, for instance. Part of his appeal is his chubby little cheeks. There are bricks with more rounded sides in this collection that hint at his usual rotundness, but the proportions of his face just feel a little off to me. I had the same reaction to the other figures as well, although Eevee seems to have fared a little better than the others. They're all sort of cute, but not nearly so cute as they are in other formats. But like I said, Lego is not my personal block of choice, so perhaps I'm in the minority here! If you love these bricky pocket monsters, then roll on over to Lego's website and snap up these kits faster than a Mewtwo. This article originally appeared on Engadget at https://www.engadget.com/entertainment/legos-first-pokemon-sets-are-now-available-for-pre-order-205527102.html?src=rss",
          "feed_position": 19,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2026-01/22428cc0-eff8-11f0-bff5-4310bbcdcba3"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/computing/laptops/dell-revives-its-xps-laptops-after-a-boneheaded-rebranding-001028029.html",
          "published_at": "Mon, 12 Jan 2026 20:25:47 +0000",
          "title": "Dell revives its XPS laptops after a boneheaded rebranding",
          "standfirst": "Last year, Dell killed off all of its PC brands, including the iconic XPS lineup, and replaced them with a simplified naming scheme. It was a move meant to make it easier for people to discern between the company's many brands, but in reality, it just just made the company's lineup even more confusing. We called it an unforced error at the time, but after seeing how much Dell's PC market share fell over 2025, it's fair to say that rebranding was an absolute marketing disaster. So, with its tail between its legs, Dell has returned to CES some welcome news for its fans: XPS lives! And the company plans to double-down on the brand in ways it never did before. Today, Dell revealed the new XPS 14 and 16 notebooks, which feature a more practical design than the previous models. There's a new function row with traditional keys, instead of the odd capacitive buttons that disappeared in sunlight. And while the company is sticking with its \"invisible\" trackpad, which sits flush alongside the wrist rest, there's now a light border around the edges that lets you feel exactly where the trackpad begins and ends.So, in short, Dell seems to have solved most of our recent complaints about the XPS lineup. To signify its commitment to the brand, it's also emblazoning the XPS logo on all of these new machines, replacing the previous Dell name. That’s something I could never imagine a less humbled Dell doing. The redesign also gave Dell room to shave off some weight and thickness from both machines. The XPS 14 weighs around three pounds now, a half-pound lighter than the previous generation, while the XPS 16 weighs 3.6 pounds, a whole pound lighter than before. The new cases make both machines look a lot more like Microsoft’s extra-subtle Surface Laptop, but that’s not necessarily a bad thing. Both systems are powered by Intel’s new Panther Lake Core Ultra Series 3 chips, and they also offer tandem OLED display options.Dell also briefly teased the return of a new XPS 13 later this year, which is set to be the company’s thinnest and lightest notebook ever. Dell says it’ll be cheaper than the XPS has been in the past.The new XPS 14 and 16 will be available on January 6, starting at $2,050 and $2,200, respectively. A Dell representative tells us these aren’t entry-level configurations, instead we can expect to see cheaper prices with lower specs in February.Update 1/6/26, 12:30p: Pricing updated to reflecrt new numbers from Dell. Originally, we were told they would start at $1,650 and $1,850.Update 1/12, 3:00p: Added a mention of lower entry-level configurations coming eventually.This article originally appeared on Engadget at https://www.engadget.com/computing/laptops/dell-revives-its-xps-laptops-after-a-boneheaded-rebranding-001028029.html?src=rss",
          "content": "Last year, Dell killed off all of its PC brands, including the iconic XPS lineup, and replaced them with a simplified naming scheme. It was a move meant to make it easier for people to discern between the company's many brands, but in reality, it just just made the company's lineup even more confusing. We called it an unforced error at the time, but after seeing how much Dell's PC market share fell over 2025, it's fair to say that rebranding was an absolute marketing disaster. So, with its tail between its legs, Dell has returned to CES some welcome news for its fans: XPS lives! And the company plans to double-down on the brand in ways it never did before. Today, Dell revealed the new XPS 14 and 16 notebooks, which feature a more practical design than the previous models. There's a new function row with traditional keys, instead of the odd capacitive buttons that disappeared in sunlight. And while the company is sticking with its \"invisible\" trackpad, which sits flush alongside the wrist rest, there's now a light border around the edges that lets you feel exactly where the trackpad begins and ends.So, in short, Dell seems to have solved most of our recent complaints about the XPS lineup. To signify its commitment to the brand, it's also emblazoning the XPS logo on all of these new machines, replacing the previous Dell name. That’s something I could never imagine a less humbled Dell doing. The redesign also gave Dell room to shave off some weight and thickness from both machines. The XPS 14 weighs around three pounds now, a half-pound lighter than the previous generation, while the XPS 16 weighs 3.6 pounds, a whole pound lighter than before. The new cases make both machines look a lot more like Microsoft’s extra-subtle Surface Laptop, but that’s not necessarily a bad thing. Both systems are powered by Intel’s new Panther Lake Core Ultra Series 3 chips, and they also offer tandem OLED display options.Dell also briefly teased the return of a new XPS 13 later this year, which is set to be the company’s thinnest and lightest notebook ever. Dell says it’ll be cheaper than the XPS has been in the past.The new XPS 14 and 16 will be available on January 6, starting at $2,050 and $2,200, respectively. A Dell representative tells us these aren’t entry-level configurations, instead we can expect to see cheaper prices with lower specs in February.Update 1/6/26, 12:30p: Pricing updated to reflecrt new numbers from Dell. Originally, we were told they would start at $1,650 and $1,850.Update 1/12, 3:00p: Added a mention of lower entry-level configurations coming eventually.This article originally appeared on Engadget at https://www.engadget.com/computing/laptops/dell-revives-its-xps-laptops-after-a-boneheaded-rebranding-001028029.html?src=rss",
          "feed_position": 20
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/computing/ces-2026-proved-the-pc-industry-is-hosed-this-year-174500314.html",
          "published_at": "Mon, 12 Jan 2026 20:25:38 +0000",
          "title": "CES 2026 proved the PC industry is hosed this year",
          "standfirst": "Dell's XPS 14 currently costs over $2,000. An AMD executive predicts that PC builders will likely make piecemeal upgrades this year, instead of building entirely new systems. And new AI supercomputers from NVIDIA and AMD are gobbling up the RAM market. At CES 2026, it was hard not to notice the dire year ahead for the computing industry, one that will likely lead to higher prices and more limited availability for consumer goods across the board.Really, though, the show just confirmed what was apparent since RAM prices skyrocketed over the last few months, driven by demand from AI datacenters. As Samsung's marketing leader, Wonjin Lee, told Bloomberg at CES: \"There's going to be issues around semiconductor supplies, and it's going to affect everyone. Prices are going up even as we speak.\"At first, it appeared that Dell's new XPS 14 and XPS 16 were among the earliest systems hit by these demands. Last year's models started at $1,699 and $1,899, respectively, and we were initially told the new models would actually come in cheaper at $1,650 and $1,850. At the moment, the XPS 14 starts at $2,050, while the XPS 16 is $2,200. A Dell representative tells us these aren’t entry-level configurations, instead we can expect to see cheaper systems below $2,000 in February. While those prices haven’t been finalized, the reps say it should be similar to the earlier figures we were given.It’s also worth noting that it didn't take much to configure the earlier models upwards of $2,000. It’s just unfortunate that Dell doesn’t have cheaper configurations available for the launch if its new systems, especially since they look so compelling. Meanwhile, Apple still hasn't budged its $1,599 MacBook Pro 14-inch pricing. At least Dell still comes in cheaper than the $2,499 MacBook Pro 16-inch.On the desktop front, AMD's David McAfee, Corporate Vice President and GM of Client Channel Business, noted that the longevity of the company's AM4 and AM5 platforms might be a boon for gamers, since they can upgrade their CPUs without buying new RAM kits and motherboards. That allows for a pathway to better performance without paying out the nose for over-priced RAM.\"I think that will be potentially a trend that we see in 2026 with more component upgrades, as opposed to full system swap outs and, and altogether rebuilds,\" he said in a group interview with Engadget and other outlets. \"Some of the most popular CPUs that are still running in gamers’ platforms are parts like the 2600 back to the Pinnacle Ridge days, or 3000 series... Stepping even from there into a little bit more modern 5,000 series processors in an AM4 socket and motherboard, there's a pretty big boost there.\"McAfee added that around 30 to 40 percent of AMD's business still revolves around the AM4 platform, even without the specter of a wild memory market.\"There's no product that has memory in it that's immune to some of these forces around DRAM pricing and, and what it's doing to the market,\" he said, when asked about potential GPU price increases. \"I think the, the truth is the volatility that we've seen over the past two months or so has really been unprecedented.\" Looking ahead, he said he expects prices to settle within the first three to six months of the year, but he didn't discuss his reasoning further. As an aside, he also noted that AMD's X3D chips, which feature 3D V-cache, actually don't see much of a hit from slower RAM. Their high amounts of onboard L2 and L3 cache make up for less ideal memory transfer speeds, McAfee said.That McAfee commented at all about the state of RAM is noteworthy. Every PC maker I’ve asked, including Dell and Acer, refused to comment on the volatile state of the memory industry ahead of CES. Perhaps they were hoping things would calm down before they had to price their new systems. Ultimately, they’re beholden to an increasingly limited supply of RAM. And where is all that memory going? At CES, NVIDIA announced its new Vera Rubin AI supercomputer, which supports up to 54TB of RAM across 36 Vera CPUs and 20.7TB of memory across 72 GPUs. AMD, as well, announced its new Helios AI rack, which supports up to 31TB of memory across 72 AMD Instinct MI455X GPUs. Given the endless appetite for computing to power AI model building and inferencing, there’s likely going to be a significant demand for these beastly systems.Put simply: Our global supply of memory is being sacrificed to appease the AI industry. That’s good news for the likes of OpenAI, Microsoft and NVIDIA, but bad news for anyone who cares about PCs and the consumer products we use every day. Get ready for a year of price hikes. Update 1/12, 3:00p: Added a mention of lower entry-level configurations coming eventually.This article originally appeared on Engadget at https://www.engadget.com/computing/ces-2026-proved-the-pc-industry-is-hosed-this-year-174500314.html?src=rss",
          "content": "Dell's XPS 14 currently costs over $2,000. An AMD executive predicts that PC builders will likely make piecemeal upgrades this year, instead of building entirely new systems. And new AI supercomputers from NVIDIA and AMD are gobbling up the RAM market. At CES 2026, it was hard not to notice the dire year ahead for the computing industry, one that will likely lead to higher prices and more limited availability for consumer goods across the board.Really, though, the show just confirmed what was apparent since RAM prices skyrocketed over the last few months, driven by demand from AI datacenters. As Samsung's marketing leader, Wonjin Lee, told Bloomberg at CES: \"There's going to be issues around semiconductor supplies, and it's going to affect everyone. Prices are going up even as we speak.\"At first, it appeared that Dell's new XPS 14 and XPS 16 were among the earliest systems hit by these demands. Last year's models started at $1,699 and $1,899, respectively, and we were initially told the new models would actually come in cheaper at $1,650 and $1,850. At the moment, the XPS 14 starts at $2,050, while the XPS 16 is $2,200. A Dell representative tells us these aren’t entry-level configurations, instead we can expect to see cheaper systems below $2,000 in February. While those prices haven’t been finalized, the reps say it should be similar to the earlier figures we were given.It’s also worth noting that it didn't take much to configure the earlier models upwards of $2,000. It’s just unfortunate that Dell doesn’t have cheaper configurations available for the launch if its new systems, especially since they look so compelling. Meanwhile, Apple still hasn't budged its $1,599 MacBook Pro 14-inch pricing. At least Dell still comes in cheaper than the $2,499 MacBook Pro 16-inch.On the desktop front, AMD's David McAfee, Corporate Vice President and GM of Client Channel Business, noted that the longevity of the company's AM4 and AM5 platforms might be a boon for gamers, since they can upgrade their CPUs without buying new RAM kits and motherboards. That allows for a pathway to better performance without paying out the nose for over-priced RAM.\"I think that will be potentially a trend that we see in 2026 with more component upgrades, as opposed to full system swap outs and, and altogether rebuilds,\" he said in a group interview with Engadget and other outlets. \"Some of the most popular CPUs that are still running in gamers’ platforms are parts like the 2600 back to the Pinnacle Ridge days, or 3000 series... Stepping even from there into a little bit more modern 5,000 series processors in an AM4 socket and motherboard, there's a pretty big boost there.\"McAfee added that around 30 to 40 percent of AMD's business still revolves around the AM4 platform, even without the specter of a wild memory market.\"There's no product that has memory in it that's immune to some of these forces around DRAM pricing and, and what it's doing to the market,\" he said, when asked about potential GPU price increases. \"I think the, the truth is the volatility that we've seen over the past two months or so has really been unprecedented.\" Looking ahead, he said he expects prices to settle within the first three to six months of the year, but he didn't discuss his reasoning further. As an aside, he also noted that AMD's X3D chips, which feature 3D V-cache, actually don't see much of a hit from slower RAM. Their high amounts of onboard L2 and L3 cache make up for less ideal memory transfer speeds, McAfee said.That McAfee commented at all about the state of RAM is noteworthy. Every PC maker I’ve asked, including Dell and Acer, refused to comment on the volatile state of the memory industry ahead of CES. Perhaps they were hoping things would calm down before they had to price their new systems. Ultimately, they’re beholden to an increasingly limited supply of RAM. And where is all that memory going? At CES, NVIDIA announced its new Vera Rubin AI supercomputer, which supports up to 54TB of RAM across 36 Vera CPUs and 20.7TB of memory across 72 GPUs. AMD, as well, announced its new Helios AI rack, which supports up to 31TB of memory across 72 AMD Instinct MI455X GPUs. Given the endless appetite for computing to power AI model building and inferencing, there’s likely going to be a significant demand for these beastly systems.Put simply: Our global supply of memory is being sacrificed to appease the AI industry. That’s good news for the likes of OpenAI, Microsoft and NVIDIA, but bad news for anyone who cares about PCs and the consumer products we use every day. Get ready for a year of price hikes. Update 1/12, 3:00p: Added a mention of lower entry-level configurations coming eventually.This article originally appeared on Engadget at https://www.engadget.com/computing/ces-2026-proved-the-pc-industry-is-hosed-this-year-174500314.html?src=rss",
          "feed_position": 21
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/orchestration/why-your-llm-bill-is-exploding-and-how-semantic-caching-can-cut-it-by-73",
          "published_at": "Mon, 12 Jan 2026 19:00:00 GMT",
          "title": "Why your LLM bill is exploding — and how semantic caching can cut it by 73%",
          "standfirst": "Our LLM API bill was growing 30% month-over-month. Traffic was increasing, but not that fast. When I analyzed our query logs, I found the real problem: Users ask the same questions in different ways.\"What&#x27;s your return policy?,\" \"How do I return something?\", and \"Can I get a refund?\" were all hitting our LLM separately, generating nearly identical responses, each incurring full API costs.Exact-match caching, the obvious first solution, captured only 18% of these redundant calls. The same semantic question, phrased differently, bypassed the cache entirely.So, I implemented semantic caching based on what queries mean, not how they&#x27;re worded. After implementing it, our cache hit rate increased to 67%, reducing LLM API costs by 73%. But getting there requires solving problems that naive implementations miss.Why exact-match caching falls shortTraditional caching uses query text as the cache key. This works when queries are identical:# Exact-match cachingcache_key = hash(query_text)if cache_key in cache: return cache[cache_key]But users don&#x27;t phrase questions identically. My analysis of 100,000 production queries found:Only 18% were exact duplicates of previous queries47% were semantically similar to previous queries (same intent, different wording)35% were genuinely novel queriesThat 47% represented massive cost savings we were missing. Each semantically-similar query triggered a full LLM call, generating a response nearly identical to one we&#x27;d already computed.Semantic caching architectureSemantic caching replaces text-based keys with embedding-based similarity lookup:class SemanticCache: def __init__(self, embedding_model, similarity_threshold=0.92): self.embedding_model = embedding_model self.threshold = similarity_threshold self.vector_store = VectorStore() # FAISS, Pinecone, etc. self.response_store = ResponseStore() # Redis, DynamoDB, etc. def get(self, query: str) -> Optional[str]: \"\"\"Return cached response if semantically similar query exists.\"\"\" query_embedding = self.embedding_model.encode(query) # Find most similar cached query matches = self.vector_store.search(query_embedding, top_k=1) if matches and matches[0].similarity >= self.threshold: cache_id = matches[0].id return self.response_store.get(cache_id) return None def set(self, query: str, response: str): \"\"\"Cache query-response pair.\"\"\" query_embedding = self.embedding_model.encode(query) cache_id = generate_id() self.vector_store.add(cache_id, query_embedding) self.response_store.set(cache_id, { &#x27;query&#x27;: query, &#x27;response&#x27;: response, &#x27;timestamp&#x27;: datetime.utcnow() })The key insight: Instead of hashing query text, I embed queries into vector space and find cached queries within a similarity threshold.The threshold problemThe similarity threshold is the critical parameter. Set it too high, and you miss valid cache hits. Set it too low, and you return wrong responses.Our initial threshold of 0.85 seemed reasonable; 85% similar should be \"the same question,\" right?Wrong. At 0.85, we got cache hits like:Query: \"How do I cancel my subscription?\"Cached: \"How do I cancel my order?\"Similarity: 0.87These are different questions with different answers. Returning the cached response would be incorrect.I discovered that optimal thresholds vary by query type:Query typeOptimal thresholdRationaleFAQ-style questions0.94High precision needed; wrong answers damage trustProduct searches0.88More tolerance for near-matchesSupport queries0.92Balance between coverage and accuracyTransactional queries0.97Very low tolerance for errorsI implemented query-type-specific thresholds:class AdaptiveSemanticCache: def __init__(self): self.thresholds = { &#x27;faq&#x27;: 0.94, &#x27;search&#x27;: 0.88, &#x27;support&#x27;: 0.92, &#x27;transactional&#x27;: 0.97, &#x27;default&#x27;: 0.92 } self.query_classifier = QueryClassifier() def get_threshold(self, query: str) -> float: query_type = self.query_classifier.classify(query) return self.thresholds.get(query_type, self.thresholds[&#x27;default&#x27;]) def get(self, query: str) -> Optional[str]: threshold = self.get_threshold(query) query_embedding = self.embedding_model.encode(query) matches = self.vector_store.search(query_embedding, top_k=1) if matches and matches[0].similarity >= threshold: return self.response_store.get(matches[0].id) return NoneThreshold tuning methodologyI couldn&#x27;t tune thresholds blindly. I needed ground truth on which query pairs were actually \"the same.\"Our methodology:Step 1: Sample query pairs. I sampled 5,000 query pairs at various similarity levels (0.80-0.99).Step 2: Human labeling. Annotators labeled each pair as \"same intent\" or \"different intent.\" I used three annotators per pair and took a majority vote.Step 3: Compute precision/recall curves. For each threshold, we computed:Precision: Of cache hits, what fraction had the same intent?Recall: Of same-intent pairs, what fraction did we cache-hit?def compute_precision_recall(pairs, labels, threshold): \"\"\"Compute precision and recall at given similarity threshold.\"\"\" predictions = [1 if pair.similarity >= threshold else 0 for pair in pairs] true_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 1) false_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 0) false_negatives = sum(1 for p, l in zip(predictions, labels) if p == 0 and l == 1) precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0 recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0 return precision, recallStep 4: Select threshold based on cost of errors. For FAQ queries where wrong answers damage trust, I optimized for precision (0.94 threshold gave 98% precision). For search queries where missing a cache hit just costs money, I optimized for recall (0.88 threshold).Latency overheadSemantic caching adds latency: You must embed the query and search the vector store before knowing whether to call the LLM.Our measurements:OperationLatency (p50)Latency (p99)Query embedding12ms28msVector search8ms19msTotal cache lookup20ms47msLLM API call850ms2400msThe 20ms overhead is negligible compared to the 850ms LLM call we avoid on cache hits. Even at p99, the 47ms overhead is acceptable.However, cache misses now take 20ms longer than before (embedding + search + LLM call). At our 67% hit rate, the math works out favorably:Before: 100% of queries × 850ms = 850ms averageAfter: (33% × 870ms) + (67% × 20ms) = 287ms + 13ms = 300ms averageNet latency improvement of 65% alongside the cost reduction.Cache invalidationCached responses go stale. Product information changes, policies update and yesterday&#x27;s correct answer becomes today&#x27;s wrong answer.I implemented three invalidation strategies:Time-based TTLSimple expiration based on content type:TTL_BY_CONTENT_TYPE = { &#x27;pricing&#x27;: timedelta(hours=4), # Changes frequently &#x27;policy&#x27;: timedelta(days=7), # Changes rarely &#x27;product_info&#x27;: timedelta(days=1), # Daily refresh &#x27;general_faq&#x27;: timedelta(days=14), # Very stable}Event-based invalidationWhen underlying data changes, invalidate related cache entries:class CacheInvalidator: def on_content_update(self, content_id: str, content_type: str): \"\"\"Invalidate cache entries related to updated content.\"\"\" # Find cached queries that referenced this content affected_queries = self.find_queries_referencing(content_id) for query_id in affected_queries: self.cache.invalidate(query_id) self.log_invalidation(content_id, len(affected_queries))Staleness detectionFor responses that might become stale without explicit events, I implemented periodic freshness checks:def check_freshness(self, cached_response: dict) -> bool: \"\"\"Verify cached response is still valid.\"\"\" # Re-run the query against current data fresh_response = self.generate_response(cached_response[&#x27;query&#x27;]) # Compare semantic similarity of responses cached_embedding = self.embed(cached_response[&#x27;response&#x27;]) fresh_embedding = self.embed(fresh_response) similarity = cosine_similarity(cached_embedding, fresh_embedding) # If responses diverged significantly, invalidate if similarity < 0.90: self.cache.invalidate(cached_response[&#x27;id&#x27;]) return False return TrueWe run freshness checks on a sample of cached entries daily, catching staleness that TTL and event-based invalidation miss.Production resultsAfter three months in production:MetricBeforeAfterChangeCache hit rate18%67%+272%LLM API costs$47K/month$12.7K/month-73%Average latency850ms300ms-65%False-positive rateN/A0.8%—Customer complaints (wrong answers)Baseline+0.3%Minimal increaseThe 0.8% false-positive rate (queries where we returned a cached response that was semantically incorrect) was within acceptable bounds. These cases occurred primarily at the boundaries of our threshold, where similarity was just above the cutoff but intent differed slightly.Pitfalls to avoidDon&#x27;t use a single global threshold. Different query types have different tolerance for errors. Tune thresholds per category.Don&#x27;t skip the embedding step on cache hits. You might be tempted to skip embedding overhead when returning cached responses, but you need the embedding for cache key generation. The overhead is unavoidable.Don&#x27;t forget invalidation. Semantic caching without invalidation strategy leads to stale responses that erode user trust. Build invalidation from day one.Don&#x27;t cache everything. Some queries shouldn&#x27;t be cached: Personalized responses, time-sensitive information, transactional confirmations. Build exclusion rules.def should_cache(self, query: str, response: str) -> bool: \"\"\"Determine if response should be cached.\"\" # Don&#x27;t cache personalized responses if self.contains_personal_info(response): return False # Don&#x27;t cache time-sensitive information if self.is_time_sensitive(query): return False # Don&#x27;t cache transactional confirmations if self.is_transactional(query): return False return TrueKey takeawaysSemantic caching is a practical pattern for LLM cost control that captures redundancy exact-match caching misses. The key challenges are threshold tuning (use query-type-specific thresholds based on precision/recall analysis) and cache invalidation (combine TTL, event-based and staleness detection).At 73% cost reduction, this was our highest-ROI optimization for production LLM systems. The implementation complexity is moderate, but the threshold tuning requires careful attention to avoid quality degradation.Sreenivasa Reddy Hulebeedu Reddy is a lead software engineer.",
          "content": "Our LLM API bill was growing 30% month-over-month. Traffic was increasing, but not that fast. When I analyzed our query logs, I found the real problem: Users ask the same questions in different ways.\"What&#x27;s your return policy?,\" \"How do I return something?\", and \"Can I get a refund?\" were all hitting our LLM separately, generating nearly identical responses, each incurring full API costs.Exact-match caching, the obvious first solution, captured only 18% of these redundant calls. The same semantic question, phrased differently, bypassed the cache entirely.So, I implemented semantic caching based on what queries mean, not how they&#x27;re worded. After implementing it, our cache hit rate increased to 67%, reducing LLM API costs by 73%. But getting there requires solving problems that naive implementations miss.Why exact-match caching falls shortTraditional caching uses query text as the cache key. This works when queries are identical:# Exact-match cachingcache_key = hash(query_text)if cache_key in cache: return cache[cache_key]But users don&#x27;t phrase questions identically. My analysis of 100,000 production queries found:Only 18% were exact duplicates of previous queries47% were semantically similar to previous queries (same intent, different wording)35% were genuinely novel queriesThat 47% represented massive cost savings we were missing. Each semantically-similar query triggered a full LLM call, generating a response nearly identical to one we&#x27;d already computed.Semantic caching architectureSemantic caching replaces text-based keys with embedding-based similarity lookup:class SemanticCache: def __init__(self, embedding_model, similarity_threshold=0.92): self.embedding_model = embedding_model self.threshold = similarity_threshold self.vector_store = VectorStore() # FAISS, Pinecone, etc. self.response_store = ResponseStore() # Redis, DynamoDB, etc. def get(self, query: str) -> Optional[str]: \"\"\"Return cached response if semantically similar query exists.\"\"\" query_embedding = self.embedding_model.encode(query) # Find most similar cached query matches = self.vector_store.search(query_embedding, top_k=1) if matches and matches[0].similarity >= self.threshold: cache_id = matches[0].id return self.response_store.get(cache_id) return None def set(self, query: str, response: str): \"\"\"Cache query-response pair.\"\"\" query_embedding = self.embedding_model.encode(query) cache_id = generate_id() self.vector_store.add(cache_id, query_embedding) self.response_store.set(cache_id, { &#x27;query&#x27;: query, &#x27;response&#x27;: response, &#x27;timestamp&#x27;: datetime.utcnow() })The key insight: Instead of hashing query text, I embed queries into vector space and find cached queries within a similarity threshold.The threshold problemThe similarity threshold is the critical parameter. Set it too high, and you miss valid cache hits. Set it too low, and you return wrong responses.Our initial threshold of 0.85 seemed reasonable; 85% similar should be \"the same question,\" right?Wrong. At 0.85, we got cache hits like:Query: \"How do I cancel my subscription?\"Cached: \"How do I cancel my order?\"Similarity: 0.87These are different questions with different answers. Returning the cached response would be incorrect.I discovered that optimal thresholds vary by query type:Query typeOptimal thresholdRationaleFAQ-style questions0.94High precision needed; wrong answers damage trustProduct searches0.88More tolerance for near-matchesSupport queries0.92Balance between coverage and accuracyTransactional queries0.97Very low tolerance for errorsI implemented query-type-specific thresholds:class AdaptiveSemanticCache: def __init__(self): self.thresholds = { &#x27;faq&#x27;: 0.94, &#x27;search&#x27;: 0.88, &#x27;support&#x27;: 0.92, &#x27;transactional&#x27;: 0.97, &#x27;default&#x27;: 0.92 } self.query_classifier = QueryClassifier() def get_threshold(self, query: str) -> float: query_type = self.query_classifier.classify(query) return self.thresholds.get(query_type, self.thresholds[&#x27;default&#x27;]) def get(self, query: str) -> Optional[str]: threshold = self.get_threshold(query) query_embedding = self.embedding_model.encode(query) matches = self.vector_store.search(query_embedding, top_k=1) if matches and matches[0].similarity >= threshold: return self.response_store.get(matches[0].id) return NoneThreshold tuning methodologyI couldn&#x27;t tune thresholds blindly. I needed ground truth on which query pairs were actually \"the same.\"Our methodology:Step 1: Sample query pairs. I sampled 5,000 query pairs at various similarity levels (0.80-0.99).Step 2: Human labeling. Annotators labeled each pair as \"same intent\" or \"different intent.\" I used three annotators per pair and took a majority vote.Step 3: Compute precision/recall curves. For each threshold, we computed:Precision: Of cache hits, what fraction had the same intent?Recall: Of same-intent pairs, what fraction did we cache-hit?def compute_precision_recall(pairs, labels, threshold): \"\"\"Compute precision and recall at given similarity threshold.\"\"\" predictions = [1 if pair.similarity >= threshold else 0 for pair in pairs] true_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 1) false_positives = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 0) false_negatives = sum(1 for p, l in zip(predictions, labels) if p == 0 and l == 1) precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0 recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0 return precision, recallStep 4: Select threshold based on cost of errors. For FAQ queries where wrong answers damage trust, I optimized for precision (0.94 threshold gave 98% precision). For search queries where missing a cache hit just costs money, I optimized for recall (0.88 threshold).Latency overheadSemantic caching adds latency: You must embed the query and search the vector store before knowing whether to call the LLM.Our measurements:OperationLatency (p50)Latency (p99)Query embedding12ms28msVector search8ms19msTotal cache lookup20ms47msLLM API call850ms2400msThe 20ms overhead is negligible compared to the 850ms LLM call we avoid on cache hits. Even at p99, the 47ms overhead is acceptable.However, cache misses now take 20ms longer than before (embedding + search + LLM call). At our 67% hit rate, the math works out favorably:Before: 100% of queries × 850ms = 850ms averageAfter: (33% × 870ms) + (67% × 20ms) = 287ms + 13ms = 300ms averageNet latency improvement of 65% alongside the cost reduction.Cache invalidationCached responses go stale. Product information changes, policies update and yesterday&#x27;s correct answer becomes today&#x27;s wrong answer.I implemented three invalidation strategies:Time-based TTLSimple expiration based on content type:TTL_BY_CONTENT_TYPE = { &#x27;pricing&#x27;: timedelta(hours=4), # Changes frequently &#x27;policy&#x27;: timedelta(days=7), # Changes rarely &#x27;product_info&#x27;: timedelta(days=1), # Daily refresh &#x27;general_faq&#x27;: timedelta(days=14), # Very stable}Event-based invalidationWhen underlying data changes, invalidate related cache entries:class CacheInvalidator: def on_content_update(self, content_id: str, content_type: str): \"\"\"Invalidate cache entries related to updated content.\"\"\" # Find cached queries that referenced this content affected_queries = self.find_queries_referencing(content_id) for query_id in affected_queries: self.cache.invalidate(query_id) self.log_invalidation(content_id, len(affected_queries))Staleness detectionFor responses that might become stale without explicit events, I implemented periodic freshness checks:def check_freshness(self, cached_response: dict) -> bool: \"\"\"Verify cached response is still valid.\"\"\" # Re-run the query against current data fresh_response = self.generate_response(cached_response[&#x27;query&#x27;]) # Compare semantic similarity of responses cached_embedding = self.embed(cached_response[&#x27;response&#x27;]) fresh_embedding = self.embed(fresh_response) similarity = cosine_similarity(cached_embedding, fresh_embedding) # If responses diverged significantly, invalidate if similarity < 0.90: self.cache.invalidate(cached_response[&#x27;id&#x27;]) return False return TrueWe run freshness checks on a sample of cached entries daily, catching staleness that TTL and event-based invalidation miss.Production resultsAfter three months in production:MetricBeforeAfterChangeCache hit rate18%67%+272%LLM API costs$47K/month$12.7K/month-73%Average latency850ms300ms-65%False-positive rateN/A0.8%—Customer complaints (wrong answers)Baseline+0.3%Minimal increaseThe 0.8% false-positive rate (queries where we returned a cached response that was semantically incorrect) was within acceptable bounds. These cases occurred primarily at the boundaries of our threshold, where similarity was just above the cutoff but intent differed slightly.Pitfalls to avoidDon&#x27;t use a single global threshold. Different query types have different tolerance for errors. Tune thresholds per category.Don&#x27;t skip the embedding step on cache hits. You might be tempted to skip embedding overhead when returning cached responses, but you need the embedding for cache key generation. The overhead is unavoidable.Don&#x27;t forget invalidation. Semantic caching without invalidation strategy leads to stale responses that erode user trust. Build invalidation from day one.Don&#x27;t cache everything. Some queries shouldn&#x27;t be cached: Personalized responses, time-sensitive information, transactional confirmations. Build exclusion rules.def should_cache(self, query: str, response: str) -> bool: \"\"\"Determine if response should be cached.\"\" # Don&#x27;t cache personalized responses if self.contains_personal_info(response): return False # Don&#x27;t cache time-sensitive information if self.is_time_sensitive(query): return False # Don&#x27;t cache transactional confirmations if self.is_transactional(query): return False return TrueKey takeawaysSemantic caching is a practical pattern for LLM cost control that captures redundancy exact-match caching misses. The key challenges are threshold tuning (use query-type-specific thresholds based on precision/recall analysis) and cache invalidation (combine TTL, event-based and staleness detection).At 73% cost reduction, this was our highest-ROI optimization for production LLM systems. The implementation complexity is moderate, but the threshold tuning requires careful attention to avoid quality degradation.Sreenivasa Reddy Hulebeedu Reddy is a lead software engineer.",
          "feed_position": 4,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/7iyQoeSwdOqqpfcE0PFWgF/48db7d0305019eee107028d9f018d2ac/Semantic_caching.png?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/entertainment/paramount-wont-quit-files-suit-against-warner-bros-discovery-over-rejected-bid-175317166.html",
          "published_at": "Mon, 12 Jan 2026 17:53:17 +0000",
          "title": "Paramount won't quit, files suit against Warner Bros. Discovery over rejected bid",
          "standfirst": "Paramount Skydance just does not want to take no for an answer. After having multiple bids to acquire Warner Bros. Discovery (WBD) rejected, including a recent hostile bid that the WBD board recommended that shareholders reject, Paramount is turning to the courts and mounting a proxy fight. In a letter to shareholders on Monday, Paramount CEO David Ellison said the company has filed suit in Delaware Chancery Court seeking more disclosure about WBD’s pending Netflix deal and the process that led to its acceptance. Paramount argues WBD hasn’t provided “basic information” shareholders need to evaluate competing offers, including how WBD valued the planned cable-networks spinout Discovery Global (or Global Networks, depending on the filing). The Netflix acquisition would leave Discovery Global to become its own publicly traded company, while the Paramount offer included these assets. Paramount is also escalating the corporate pressure campaign, with Ellison saying it intends to nominate a slate of directors for election at WBD’s 2026 annual meeting. The end goal would be installing a board that would “engage” on Paramount’s offer under the terms of WBD’s merger agreement with Netflix. If WBD were to call a special meeting to approve the Netflix transaction before the annual meeting, Paramount says it will solicit proxy votes against the deal. It also plans to push a bylaw change requiring shareholders to approve any separation of Discovery Global. This change seems like Paramount stoking the flames (whether real or imagined) surrounding shareholders having their WBD shares bought out without the value of Discovery Global built-in under the Netflix merger. Paramount remains convinced that its offer is \"superior\" to that of Netflix, while WBD maintains Paramount's bid offers \"insufficient value\" and that Paramount has failed to submit a true best proposal \"despite clear direction from WBD on both the deficiencies and potential solutions.\" The lawsuit now aims to force WBD to spell out exactly how it arrived at recommending the Netflix deal over Paramount's bid. WBD expressed concerns over whether a potential Paramount deal would even reach closing, citing the substantial debt the smaller studio would have to take on to pull off a leveraged buyout.This article originally appeared on Engadget at https://www.engadget.com/entertainment/paramount-wont-quit-files-suit-against-warner-bros-discovery-over-rejected-bid-175317166.html?src=rss",
          "content": "Paramount Skydance just does not want to take no for an answer. After having multiple bids to acquire Warner Bros. Discovery (WBD) rejected, including a recent hostile bid that the WBD board recommended that shareholders reject, Paramount is turning to the courts and mounting a proxy fight. In a letter to shareholders on Monday, Paramount CEO David Ellison said the company has filed suit in Delaware Chancery Court seeking more disclosure about WBD’s pending Netflix deal and the process that led to its acceptance. Paramount argues WBD hasn’t provided “basic information” shareholders need to evaluate competing offers, including how WBD valued the planned cable-networks spinout Discovery Global (or Global Networks, depending on the filing). The Netflix acquisition would leave Discovery Global to become its own publicly traded company, while the Paramount offer included these assets. Paramount is also escalating the corporate pressure campaign, with Ellison saying it intends to nominate a slate of directors for election at WBD’s 2026 annual meeting. The end goal would be installing a board that would “engage” on Paramount’s offer under the terms of WBD’s merger agreement with Netflix. If WBD were to call a special meeting to approve the Netflix transaction before the annual meeting, Paramount says it will solicit proxy votes against the deal. It also plans to push a bylaw change requiring shareholders to approve any separation of Discovery Global. This change seems like Paramount stoking the flames (whether real or imagined) surrounding shareholders having their WBD shares bought out without the value of Discovery Global built-in under the Netflix merger. Paramount remains convinced that its offer is \"superior\" to that of Netflix, while WBD maintains Paramount's bid offers \"insufficient value\" and that Paramount has failed to submit a true best proposal \"despite clear direction from WBD on both the deficiencies and potential solutions.\" The lawsuit now aims to force WBD to spell out exactly how it arrived at recommending the Netflix deal over Paramount's bid. WBD expressed concerns over whether a potential Paramount deal would even reach closing, citing the substantial debt the smaller studio would have to take on to pull off a leveraged buyout.This article originally appeared on Engadget at https://www.engadget.com/entertainment/paramount-wont-quit-files-suit-against-warner-bros-discovery-over-rejected-bid-175317166.html?src=rss",
          "feed_position": 23
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/ai/apples-siri-ai-will-be-powered-by-gemini-153636649.html",
          "published_at": "Mon, 12 Jan 2026 17:03:13 +0000",
          "title": "Apple's Siri AI will be powered by Gemini",
          "standfirst": "Apple and Google have confirmed reports that the former will use Google Gemini’s models to help power the new version of Siri and other generative AI features. CNBC first reported the news; Apple and Google subsequently released a joint statement which reads:“Apple and Google have entered into a multi-year collaboration under which the next generation of Apple Foundation Models will be based on Google's Gemini models and cloud technology. These models will help power future Apple Intelligence features, including a more personalized Siri coming this year.After careful evaluation, Apple determined that Google's Al technology provides the most capable foundation for Apple Foundation Models and is excited about the innovative new experiences it will unlock for Apple users. Apple Intelligence will continue to run on Apple devices and Private Cloud Compute, while maintaining Apple's industry-leading privacy standards.”Apple first demoed a genAI version of Siri back at WWDC 2024. In March 2025, the company said it was delaying a major Siri update until this year, but it appears that Apple is not quite ready to publicly release a more capable version of the voice assistant. In June, it was reported that Apple was considering partnerships with OpenAI and Anthropic for Siri (the voice assistant can currently tap ChatGPT for certain queries as part of Apple Intelligence). Two months later, Google emerged as a possible contender. Those rumors intensified in November, when it was reported that Apple might build the new Siri using a custom version of Gemini that runs on its Private Cloud Compute servers — and that it would pay Google around $1 billion a year for the privilege. Update, January 12, 2026, 12:03PM ET: This story has been updated with a full joint statement from Apple and Google.This article originally appeared on Engadget at https://www.engadget.com/ai/apples-siri-ai-will-be-powered-by-gemini-153636649.html?src=rss",
          "content": "Apple and Google have confirmed reports that the former will use Google Gemini’s models to help power the new version of Siri and other generative AI features. CNBC first reported the news; Apple and Google subsequently released a joint statement which reads:“Apple and Google have entered into a multi-year collaboration under which the next generation of Apple Foundation Models will be based on Google's Gemini models and cloud technology. These models will help power future Apple Intelligence features, including a more personalized Siri coming this year.After careful evaluation, Apple determined that Google's Al technology provides the most capable foundation for Apple Foundation Models and is excited about the innovative new experiences it will unlock for Apple users. Apple Intelligence will continue to run on Apple devices and Private Cloud Compute, while maintaining Apple's industry-leading privacy standards.”Apple first demoed a genAI version of Siri back at WWDC 2024. In March 2025, the company said it was delaying a major Siri update until this year, but it appears that Apple is not quite ready to publicly release a more capable version of the voice assistant. In June, it was reported that Apple was considering partnerships with OpenAI and Anthropic for Siri (the voice assistant can currently tap ChatGPT for certain queries as part of Apple Intelligence). Two months later, Google emerged as a possible contender. Those rumors intensified in November, when it was reported that Apple might build the new Siri using a custom version of Gemini that runs on its Private Cloud Compute servers — and that it would pay Google around $1 billion a year for the privilege. Update, January 12, 2026, 12:03PM ET: This story has been updated with a full joint statement from Apple and Google.This article originally appeared on Engadget at https://www.engadget.com/ai/apples-siri-ai-will-be-powered-by-gemini-153636649.html?src=rss",
          "feed_position": 24
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/security/nvidia-rubin-rack-scale-encryption-enterprise-ai-security",
          "published_at": "Mon, 12 Jan 2026 16:00:00 GMT",
          "title": "Nvidia Rubin's rack-scale encryption signals a turning point for enterprise AI security",
          "standfirst": "Nvidia&#x27;s Vera Rubin NVL72, announced at CES 2026, encrypts every bus across 72 GPUs, 36 CPUs, and the entire NVLink fabric. It&#x27;s the first rack-scale platform to deliver confidential computing across CPU, GPU, and NVLink domains.For security leaders, this fundamentally shifts the conversation. Rather than attempting to secure complex hybrid cloud configurations through contractual trust with cloud providers, they can verify them cryptographically. That’s a critical distinction that matters when nation-state adversaries have proven they are capable of launching targeted cyberattacks at machine speed.The brutal economics of unprotected AIEpoch AI research shows frontier training costs have grown at 2.4x annually since 2016, which means billion-dollar training runs could be a reality within a few short years. Yet the infrastructure protecting these investments remains fundamentally insecure in most deployments. Security budgets created to protect frontier training models aren&#x27;t keeping up with the exceptionally fast pace of model training. The result is that more models are under threat as existing approaches can&#x27;t scale and keep up with adversaries&#x27; tradecraft.IBM&#x27;s 2025 Cost of Data Breach Report found that 13% of organizations experienced breaches of AI models or applications. Among those breached, 97% lacked proper AI access controls. Shadow AI incidents cost $4.63 million on average, or $670,000 more than standard breaches, with one in five breaches now involving unsanctioned tools that disproportionately expose customer PII (65%) and intellectual property (40%).Think about what this means for organizations spending $50 million or $500 million on a training run. Their model weights sit in multi-tenant environments where cloud providers can inspect the data. Hardware-level encryption that proves the environment hasn&#x27;t been tampered with changes that financial equation entirely.The GTG-1002 wake-up callIn November 2025, Anthropic disclosed something unprecedented: A Chinese state-sponsored group designated GTG-1002 had manipulated Claude Code to conduct what the company described as the first documented case of a large-scale cyberattack executed without substantial human intervention.State-sponsored adversaries turned it into an autonomous intrusion agent that discovered vulnerabilities, crafted exploits, harvested credentials, moved laterally through networks, and categorized stolen data by intelligence value. Human operators stepped in only at critical junctures. According to Anthropic&#x27;s analysis, the AI executed around 80 to 90% of all tactical work independently.The implications extend beyond this single incident. Attack surfaces that once required teams of experienced attackers can now be probed at machine speed by opponents with access to foundation models.Comparing the performance of Blackwell vs. RubinSpecificationBlackwell GB300 NVL72Rubin NVL72Inference compute (FP4)1.44 exaFLOPS3.6 exaFLOPSNVFP4 per GPU (inference)20 PFLOPS50 PFLOPSPer-GPU NVLink bandwidth1.8 TB/s3.6 TB/sRack NVLink bandwidth130 TB/s260 TB/sHBM bandwidth per GPU~8 TB/s~22 TB/sIndustry momentum and AMD&#x27;s alternativeNvidia isn&#x27;t operating in isolation. Research from the Confidential Computing Consortium and IDC, released in December, found that 75% of organizations are adopting confidential computing, with 18% already in production and 57% piloting deployments.\"Confidential Computing has grown from a niche concept into a vital strategy for data security and trusted AI innovation,\" said Nelly Porter, governing board chair of the Confidential Computing Consortium. Real barriers remain: attestation validation challenges affect 84% of respondents, and a skills gap hampers 75%.AMD&#x27;s Helios rack takes a different approach. Built on Meta&#x27;s Open Rack Wide specification, announced at OCP Global Summit in October 2025, it delivers approximately 2.9 exaflops of FP4 compute with 31 TB of HBM4 memory and 1.4 PB/s aggregate bandwidth. Where Nvidia designs confidential computing into every component, AMD prioritizes open standards through the Ultra Accelerator Link and Ultra Ethernet consortia. The competition between Nvidia and AMD is giving security leaders more of a choice than they otherwise would have had. Comparing the tradeoffs of Nvidia&#x27;s integrated approach versus AMD&#x27;s open-standards flexibility for their specific infrastructures and business-specific threat models is key.What security leaders are doing nowHardware-level confidentiality doesn&#x27;t replace zero-trust principles; it gives them teeth. What Nvidia and AMD are building lets security leaders verify trust cryptographically rather than assume it contractually. That&#x27;s a meaningful shift for anyone running sensitive workloads on shared infrastructure. And if the attestation claims hold up in production, this approach could let enterprises extend zero-trust enforcement across thousands of nodes without the policy sprawl and agent overhead that software-only implementations require.Before deployment: Verify attestation to confirm environments haven&#x27;t been tampered with. Cryptographic proof of compliance should be a prerequisite for signing contracts, not an afterthought or worse, a nice-to-have. If your cloud provider can&#x27;t demonstrate attestation capabilities, that&#x27;s a question worth raising in your next QBR.During operation: Maintain separate enclaves for training and inference, and include security teams in the model pipeline from the very start. IBM&#x27;s research showed 63% of breached organizations had no AI governance policy. You can&#x27;t bolt security on after development; that translates into an onramp for mediocre security design-ins and lengthy red teaming that catches bugs that needed to be engineered out of a model or app early.Across the organization: Run joint exercises between security and data science teams to surface vulnerabilities before attackers find them. Shadow AI accounted for 20% of breaches and exposed customer PII and IP at higher rates than other breach types.Bottom line The GTG-1002 campaign demonstrated that adversaries can now automate large-scale intrusions with minimal human oversight at scale. Nearly every organization that experienced an AI-related breach lacked proper access controls.Nvidia&#x27;s Vera Rubin NVL72 transforms racks from potential liabilities into cryptographically attested assets by encrypting every bus. AMD&#x27;s Helios offers an open-standards alternative. Hardware confidentiality alone won&#x27;t stop a determined adversary, but combined with strong governance and realistic threat exercises, rack-scale encryption gives security leaders the foundation they need to protect investments measured in hundreds of millions of dollars.The question facing CISOs isn&#x27;t whether attested infrastructure is worth it. It&#x27;s whether organizations building high-value AI models can afford to operate without it.",
          "content": "Nvidia&#x27;s Vera Rubin NVL72, announced at CES 2026, encrypts every bus across 72 GPUs, 36 CPUs, and the entire NVLink fabric. It&#x27;s the first rack-scale platform to deliver confidential computing across CPU, GPU, and NVLink domains.For security leaders, this fundamentally shifts the conversation. Rather than attempting to secure complex hybrid cloud configurations through contractual trust with cloud providers, they can verify them cryptographically. That’s a critical distinction that matters when nation-state adversaries have proven they are capable of launching targeted cyberattacks at machine speed.The brutal economics of unprotected AIEpoch AI research shows frontier training costs have grown at 2.4x annually since 2016, which means billion-dollar training runs could be a reality within a few short years. Yet the infrastructure protecting these investments remains fundamentally insecure in most deployments. Security budgets created to protect frontier training models aren&#x27;t keeping up with the exceptionally fast pace of model training. The result is that more models are under threat as existing approaches can&#x27;t scale and keep up with adversaries&#x27; tradecraft.IBM&#x27;s 2025 Cost of Data Breach Report found that 13% of organizations experienced breaches of AI models or applications. Among those breached, 97% lacked proper AI access controls. Shadow AI incidents cost $4.63 million on average, or $670,000 more than standard breaches, with one in five breaches now involving unsanctioned tools that disproportionately expose customer PII (65%) and intellectual property (40%).Think about what this means for organizations spending $50 million or $500 million on a training run. Their model weights sit in multi-tenant environments where cloud providers can inspect the data. Hardware-level encryption that proves the environment hasn&#x27;t been tampered with changes that financial equation entirely.The GTG-1002 wake-up callIn November 2025, Anthropic disclosed something unprecedented: A Chinese state-sponsored group designated GTG-1002 had manipulated Claude Code to conduct what the company described as the first documented case of a large-scale cyberattack executed without substantial human intervention.State-sponsored adversaries turned it into an autonomous intrusion agent that discovered vulnerabilities, crafted exploits, harvested credentials, moved laterally through networks, and categorized stolen data by intelligence value. Human operators stepped in only at critical junctures. According to Anthropic&#x27;s analysis, the AI executed around 80 to 90% of all tactical work independently.The implications extend beyond this single incident. Attack surfaces that once required teams of experienced attackers can now be probed at machine speed by opponents with access to foundation models.Comparing the performance of Blackwell vs. RubinSpecificationBlackwell GB300 NVL72Rubin NVL72Inference compute (FP4)1.44 exaFLOPS3.6 exaFLOPSNVFP4 per GPU (inference)20 PFLOPS50 PFLOPSPer-GPU NVLink bandwidth1.8 TB/s3.6 TB/sRack NVLink bandwidth130 TB/s260 TB/sHBM bandwidth per GPU~8 TB/s~22 TB/sIndustry momentum and AMD&#x27;s alternativeNvidia isn&#x27;t operating in isolation. Research from the Confidential Computing Consortium and IDC, released in December, found that 75% of organizations are adopting confidential computing, with 18% already in production and 57% piloting deployments.\"Confidential Computing has grown from a niche concept into a vital strategy for data security and trusted AI innovation,\" said Nelly Porter, governing board chair of the Confidential Computing Consortium. Real barriers remain: attestation validation challenges affect 84% of respondents, and a skills gap hampers 75%.AMD&#x27;s Helios rack takes a different approach. Built on Meta&#x27;s Open Rack Wide specification, announced at OCP Global Summit in October 2025, it delivers approximately 2.9 exaflops of FP4 compute with 31 TB of HBM4 memory and 1.4 PB/s aggregate bandwidth. Where Nvidia designs confidential computing into every component, AMD prioritizes open standards through the Ultra Accelerator Link and Ultra Ethernet consortia. The competition between Nvidia and AMD is giving security leaders more of a choice than they otherwise would have had. Comparing the tradeoffs of Nvidia&#x27;s integrated approach versus AMD&#x27;s open-standards flexibility for their specific infrastructures and business-specific threat models is key.What security leaders are doing nowHardware-level confidentiality doesn&#x27;t replace zero-trust principles; it gives them teeth. What Nvidia and AMD are building lets security leaders verify trust cryptographically rather than assume it contractually. That&#x27;s a meaningful shift for anyone running sensitive workloads on shared infrastructure. And if the attestation claims hold up in production, this approach could let enterprises extend zero-trust enforcement across thousands of nodes without the policy sprawl and agent overhead that software-only implementations require.Before deployment: Verify attestation to confirm environments haven&#x27;t been tampered with. Cryptographic proof of compliance should be a prerequisite for signing contracts, not an afterthought or worse, a nice-to-have. If your cloud provider can&#x27;t demonstrate attestation capabilities, that&#x27;s a question worth raising in your next QBR.During operation: Maintain separate enclaves for training and inference, and include security teams in the model pipeline from the very start. IBM&#x27;s research showed 63% of breached organizations had no AI governance policy. You can&#x27;t bolt security on after development; that translates into an onramp for mediocre security design-ins and lengthy red teaming that catches bugs that needed to be engineered out of a model or app early.Across the organization: Run joint exercises between security and data science teams to surface vulnerabilities before attackers find them. Shadow AI accounted for 20% of breaches and exposed customer PII and IP at higher rates than other breach types.Bottom line The GTG-1002 campaign demonstrated that adversaries can now automate large-scale intrusions with minimal human oversight at scale. Nearly every organization that experienced an AI-related breach lacked proper access controls.Nvidia&#x27;s Vera Rubin NVL72 transforms racks from potential liabilities into cryptographically attested assets by encrypting every bus. AMD&#x27;s Helios offers an open-standards alternative. Hardware confidentiality alone won&#x27;t stop a determined adversary, but combined with strong governance and realistic threat exercises, rack-scale encryption gives security leaders the foundation they need to protect investments measured in hundreds of millions of dollars.The question facing CISOs isn&#x27;t whether attested infrastructure is worth it. It&#x27;s whether organizations building high-value AI models can afford to operate without it.",
          "feed_position": 5,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/2YSkElI5OHmat9celGziVv/a23ebd445f5a38cd5062055b9dd3b15e/jenson_at_ces.jpg?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/apps/monarch-money-is-offering-50-percent-off-its-budgeting-app-for-new-users-204507767.html",
          "published_at": "Mon, 12 Jan 2026 15:45:37 +0000",
          "title": "Monarch Money is offering 50 percent off its budgeting app for new users",
          "standfirst": "The start of the new year is a great time to get your finances in order, and a good budgeting app can help with that. Instead of laboring over a spreadsheet, you can try one of our favorite budgeting apps for less than usual. Monarch Money is running a sale that gives new users 50 percent off one year of the service, bringing the final cost down to just $50. Just use the code NEWYEAR2026 at checkout to get the discount. Monarch Money makes for a capable and detailed budgeting companion. You can use the service via apps for iOS, Android, iPadOS or the web, and Monarch also offers a Chrome extension that can sync your Amazon and Target transactions and automatically categorize them. Like other budgeting apps, Monarch Money lets you connect multiple financial accounts and track your money based on where you spend it over time. Monarch offers two different approaches to tracking budgeting (flexible and category budgeting) depending on what fits your life best, and the ability to add a budget widget on your phone so you can know how you're tracking that month. How budgeting apps turn your raw transactions into visuals you can understand at a glance is one of the big things that differentiates one app from another, and Monarch Money offers multiple graphs and charts to look at for things like spending, investments or categories of your choice based on how you've labelled your expenses. The app can also monitor the spending of you and your partner all in one place, to make it easier to plan together. The main drawbacks Engadget found in testing Monarch Money were the app's learning curve, and the differences in features (and bugginess) between Monarch's web and mobile versions. Still, for 50 percent off, the Monarch Money is well worth experimenting with if you're trying to save money in 2026, especially if you want to do it collaboratively with a partner. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/apps/monarch-money-is-offering-50-percent-off-its-budgeting-app-for-new-users-204507767.html?src=rss",
          "content": "The start of the new year is a great time to get your finances in order, and a good budgeting app can help with that. Instead of laboring over a spreadsheet, you can try one of our favorite budgeting apps for less than usual. Monarch Money is running a sale that gives new users 50 percent off one year of the service, bringing the final cost down to just $50. Just use the code NEWYEAR2026 at checkout to get the discount. Monarch Money makes for a capable and detailed budgeting companion. You can use the service via apps for iOS, Android, iPadOS or the web, and Monarch also offers a Chrome extension that can sync your Amazon and Target transactions and automatically categorize them. Like other budgeting apps, Monarch Money lets you connect multiple financial accounts and track your money based on where you spend it over time. Monarch offers two different approaches to tracking budgeting (flexible and category budgeting) depending on what fits your life best, and the ability to add a budget widget on your phone so you can know how you're tracking that month. How budgeting apps turn your raw transactions into visuals you can understand at a glance is one of the big things that differentiates one app from another, and Monarch Money offers multiple graphs and charts to look at for things like spending, investments or categories of your choice based on how you've labelled your expenses. The app can also monitor the spending of you and your partner all in one place, to make it easier to plan together. The main drawbacks Engadget found in testing Monarch Money were the app's learning curve, and the differences in features (and bugginess) between Monarch's web and mobile versions. Still, for 50 percent off, the Monarch Money is well worth experimenting with if you're trying to save money in 2026, especially if you want to do it collaboratively with a partner. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/apps/monarch-money-is-offering-50-percent-off-its-budgeting-app-for-new-users-204507767.html?src=rss",
          "feed_position": 27
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/audible-deal-get-three-months-of-access-for-only-3-193859213.html",
          "published_at": "Mon, 12 Jan 2026 14:35:35 +0000",
          "title": "Audible deal: Get three months of access for only $3",
          "standfirst": "One way to read more in the new year is to incorporate audiobooks as part of your reading habit. Audible is having a sale right now that makes that easier and cheaper to do: you can get three months of access for only $1 per month, or a total of $3. The promotion runs through January 21. An Audible subscription grants one audiobook per month to keep. This can be selected from a massive catalog of new releases and bestsellers. The collection here has just about everything. However, it's easy to plow through a single book in a month. Users also get streaming access to thousands of curated titles. Think of it like Netflix for audiobooks. The catalog is limited, but it gets the job done in a pinch. Subscribers do get access to all Audible original content and they will receive discounts on purchasing audiobooks outright. In other words, it's a neat little service and well worth a buck. The regular price is $15, so make sure to cancel at the end of that three months if you aren't enjoying the platform. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/audible-deal-get-three-months-of-access-for-only-3-193859213.html?src=rss",
          "content": "One way to read more in the new year is to incorporate audiobooks as part of your reading habit. Audible is having a sale right now that makes that easier and cheaper to do: you can get three months of access for only $1 per month, or a total of $3. The promotion runs through January 21. An Audible subscription grants one audiobook per month to keep. This can be selected from a massive catalog of new releases and bestsellers. The collection here has just about everything. However, it's easy to plow through a single book in a month. Users also get streaming access to thousands of curated titles. Think of it like Netflix for audiobooks. The catalog is limited, but it gets the job done in a pinch. Subscribers do get access to all Audible original content and they will receive discounts on purchasing audiobooks outright. In other words, it's a neat little service and well worth a buck. The regular price is $15, so make sure to cancel at the end of that three months if you aren't enjoying the platform. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/audible-deal-get-three-months-of-access-for-only-3-193859213.html?src=rss",
          "feed_position": 29
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/apples-mac-mini-m4-is-back-on-sale-for-499-141615231.html",
          "published_at": "Mon, 12 Jan 2026 14:16:16 +0000",
          "title": "Apple's Mac mini M4 is back on sale for $499",
          "standfirst": "The holiday season is fully in the rear view mirror and real life is here to stay. But that doesn't mean the time for gifts is over — especially ones for yourself. You can still take advantage of great January sales on some awesome tech products. Take the Apple Mac mini M4, which is down to $500 from $599. The 17 percent discount gives you 16GB of RAM and 256GB of SSD for only about $20 more than the computer's Black Friday sale. Its beefier models are also on sale: opting for 512GB of SSD will cost you $690, down from $799, while also upping your RAM to 24GB is available for $890, dropping from $999. We gave the Apple Mac mini M4 a 90 in our review thanks in large part to its powerful chip. The M4 works very fast despite being in such a small device. It also offers front-facing headphone and USB-C ports. You can further upgrade to the Apple M4 Pro chip for $1,270, down from $1,399 — a nine percent discount. The Pro model also has Thunderbolt 5 support. Check out our coverage of the best Apple deals for more discounts, and follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/apples-mac-mini-m4-is-back-on-sale-for-499-141615231.html?src=rss",
          "content": "The holiday season is fully in the rear view mirror and real life is here to stay. But that doesn't mean the time for gifts is over — especially ones for yourself. You can still take advantage of great January sales on some awesome tech products. Take the Apple Mac mini M4, which is down to $500 from $599. The 17 percent discount gives you 16GB of RAM and 256GB of SSD for only about $20 more than the computer's Black Friday sale. Its beefier models are also on sale: opting for 512GB of SSD will cost you $690, down from $799, while also upping your RAM to 24GB is available for $890, dropping from $999. We gave the Apple Mac mini M4 a 90 in our review thanks in large part to its powerful chip. The M4 works very fast despite being in such a small device. It also offers front-facing headphone and USB-C ports. You can further upgrade to the Apple M4 Pro chip for $1,270, down from $1,399 — a nine percent discount. The Pro model also has Thunderbolt 5 support. Check out our coverage of the best Apple deals for more discounts, and follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/apples-mac-mini-m4-is-back-on-sale-for-499-141615231.html?src=rss",
          "feed_position": 30
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/entertainment/streaming/netflix-won-seven-awards-at-the-golden-globes-with-adolescence-and-kpop-demon-hunters-140006510.html",
          "published_at": "Mon, 12 Jan 2026 14:00:06 +0000",
          "title": "Netflix won seven awards at the Golden Globes with Adolescence and KPop Demon Hunters",
          "standfirst": "The 2026 Golden Globes took place on Sunday and it was another big night for streamers. Netflix took home seven awards, Apple and HBO Max each won three and Hulu got one. Netflix's hit show Adolescence received four awards alone, including best limited or anthology series. It also won for best actor (Stephen Graham), supporting actor (Owen Cooper) and supporting actress (Erin Doherty) in a miniseries or television film. KPop Demon Hunters — the sensation which became Netflix's most-watched title — won for best animated feature and best original song. \"I just want to say this award goes to people who have had doors closed on them, and I can confidently say rejection is redirection. So never give up. It is never too late to shine like you were born to be,\" singer-songwriter EJAE said in her acceptance speech for the song, Golden. Netflix also won for best performance in stand-up comedy on television for Ricky Gervais: Mortality. Apple TV took home two awards for The Studio: best television series musical or comedy and best performance by a male actor in a television series for Seth Rogen. The streamer also won for best performance by a lead actress in a television series drama thanks to Rhea Seehorn in Pluribus. The Pitt gave HBO Max two of its three awards, with trophies for best television series drama and best performance by a lead actor in a television series drama to Noah Wyle. Jean Smart rounded out the streamer's awards with best performance by a lead actress in a television series musical or comedy for Hacks. Hulu's award came through best performance by a lead actress in a limited or anthology series for Michelle Williams in Dying For Sex. This year also brought a first to the Golden Globes: the best podcast category. Amy Poehler won for Good Hang with Amy Poehler, a podcast that has featured interviews with everyone from Tina Fey to Quinta Brunson since debuting in March last year. Fellow nominees included Alex Cooper's Call Her Daddy and Armchair Expert with Dax Shepard. This article originally appeared on Engadget at https://www.engadget.com/entertainment/streaming/netflix-won-seven-awards-at-the-golden-globes-with-adolescence-and-kpop-demon-hunters-140006510.html?src=rss",
          "content": "The 2026 Golden Globes took place on Sunday and it was another big night for streamers. Netflix took home seven awards, Apple and HBO Max each won three and Hulu got one. Netflix's hit show Adolescence received four awards alone, including best limited or anthology series. It also won for best actor (Stephen Graham), supporting actor (Owen Cooper) and supporting actress (Erin Doherty) in a miniseries or television film. KPop Demon Hunters — the sensation which became Netflix's most-watched title — won for best animated feature and best original song. \"I just want to say this award goes to people who have had doors closed on them, and I can confidently say rejection is redirection. So never give up. It is never too late to shine like you were born to be,\" singer-songwriter EJAE said in her acceptance speech for the song, Golden. Netflix also won for best performance in stand-up comedy on television for Ricky Gervais: Mortality. Apple TV took home two awards for The Studio: best television series musical or comedy and best performance by a male actor in a television series for Seth Rogen. The streamer also won for best performance by a lead actress in a television series drama thanks to Rhea Seehorn in Pluribus. The Pitt gave HBO Max two of its three awards, with trophies for best television series drama and best performance by a lead actor in a television series drama to Noah Wyle. Jean Smart rounded out the streamer's awards with best performance by a lead actress in a television series musical or comedy for Hacks. Hulu's award came through best performance by a lead actress in a limited or anthology series for Michelle Williams in Dying For Sex. This year also brought a first to the Golden Globes: the best podcast category. Amy Poehler won for Good Hang with Amy Poehler, a podcast that has featured interviews with everyone from Tina Fey to Quinta Brunson since debuting in March last year. Fellow nominees included Alex Cooper's Call Her Daddy and Armchair Expert with Dax Shepard. This article originally appeared on Engadget at https://www.engadget.com/entertainment/streaming/netflix-won-seven-awards-at-the-golden-globes-with-adolescence-and-kpop-demon-hunters-140006510.html?src=rss",
          "feed_position": 31
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/get-apples-25w-magsafe-charger-for-only-30-right-now-141707104.html",
          "published_at": "Mon, 12 Jan 2026 13:36:26 +0000",
          "title": "Get Apple's 25W MagSafe charger for only $30 right now",
          "standfirst": "One way you can reduce the number of cables you have to deal with on the regular is by investing in a few wireless chargers. Those with iPhones should consider Apple's own MagSafe charger not only because of its sleek and effective design, but also because it's on sale right now at Amazon. The Qi2.2-rated MagSafe charger is down to $30 for the one-meter version, or $40 for the two-meter version. If you have an iPhone 16, iPhone 17 or iPhone Air, this cable can charge your device at 25W as long as it's connected to a 30W power adapter on the other end. While you'll need a more recent iPhone to get the fastest MagSafe charging speeds, the charger can wirelessly top up the battery of any iPhone from the last eight years (iPhone 8 and later). With older iPhones, the charging speed tops out at 15W. The cable works with AirPods wireless charging cases too — it's certified for Qi2.2 and Qi charging. The MagSafe charger is one of our favorite iPhone accessories, and would pair quite nicely with your new iPhone if you're picking up one of the latest models. If you're on the fence about that, be sure to check out our reviews of the iPhone 17, iPhone Pro/Pro Max and iPhone Air. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/get-apples-25w-magsafe-charger-for-only-30-right-now-141707104.html?src=rss",
          "content": "One way you can reduce the number of cables you have to deal with on the regular is by investing in a few wireless chargers. Those with iPhones should consider Apple's own MagSafe charger not only because of its sleek and effective design, but also because it's on sale right now at Amazon. The Qi2.2-rated MagSafe charger is down to $30 for the one-meter version, or $40 for the two-meter version. If you have an iPhone 16, iPhone 17 or iPhone Air, this cable can charge your device at 25W as long as it's connected to a 30W power adapter on the other end. While you'll need a more recent iPhone to get the fastest MagSafe charging speeds, the charger can wirelessly top up the battery of any iPhone from the last eight years (iPhone 8 and later). With older iPhones, the charging speed tops out at 15W. The cable works with AirPods wireless charging cases too — it's certified for Qi2.2 and Qi charging. The MagSafe charger is one of our favorite iPhone accessories, and would pair quite nicely with your new iPhone if you're picking up one of the latest models. If you're on the fence about that, be sure to check out our reviews of the iPhone 17, iPhone Pro/Pro Max and iPhone Air. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/get-apples-25w-magsafe-charger-for-only-30-right-now-141707104.html?src=rss",
          "feed_position": 32
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/deals/this-elevationlabs-10-year-extended-battery-case-for-airtags-is-on-sale-for-only-16-162308983.html",
          "published_at": "Mon, 12 Jan 2026 13:14:15 +0000",
          "title": "This ElevationLabs 10-year extended battery case for AirTags is on sale for only $16",
          "standfirst": "ElevationLab makes a battery case for your AirTag that can power it for 10 years and the accessory is on sale now for 30 percent off. Normally retailing for $23, you can pick one up for $16. The TimeCapsule case uses two AA batteries to offer up to 14 times the lifespan of the CR2032 battery that powers an AirTag. The company based those estimates on Energizer Ultimate Lithium batteries, so your mileage may vary. Once an AirTag is seated inside the case, which is a compact 4.45 x 1.57 inches, it is sealed shut with four screws at the corners. The case is fiber-reinforced, according to Elevation Lab, and rated IP69 waterproof. The company says it’s intended for use cases where you might place an AirTag for long periods of time, like in a vehicle, a piece of luggage or a work bag. We've already got a couple of Elevation Lab products on our list for best AirTag accessories, so while we haven't reviewed the battery case, we tend to like this company's products. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/this-elevationlabs-10-year-extended-battery-case-for-airtags-is-on-sale-for-only-16-162308983.html?src=rss",
          "content": "ElevationLab makes a battery case for your AirTag that can power it for 10 years and the accessory is on sale now for 30 percent off. Normally retailing for $23, you can pick one up for $16. The TimeCapsule case uses two AA batteries to offer up to 14 times the lifespan of the CR2032 battery that powers an AirTag. The company based those estimates on Energizer Ultimate Lithium batteries, so your mileage may vary. Once an AirTag is seated inside the case, which is a compact 4.45 x 1.57 inches, it is sealed shut with four screws at the corners. The case is fiber-reinforced, according to Elevation Lab, and rated IP69 waterproof. The company says it’s intended for use cases where you might place an AirTag for long periods of time, like in a vehicle, a piece of luggage or a work bag. We've already got a couple of Elevation Lab products on our list for best AirTag accessories, so while we haven't reviewed the battery case, we tend to like this company's products. Follow @EngadgetDeals on X for the latest tech deals and buying advice.This article originally appeared on Engadget at https://www.engadget.com/deals/this-elevationlabs-10-year-extended-battery-case-for-airtags-is-on-sale-for-only-16-162308983.html?src=rss",
          "feed_position": 33
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/social-media/meta-closes-550000-accounts-to-comply-with-australias-kids-social-media-ban-130041356.html",
          "published_at": "Mon, 12 Jan 2026 13:00:41 +0000",
          "title": "Meta closes 550,000 accounts to comply with Australia's kids social media ban",
          "standfirst": "To comply with Australia's under-16 social media ban, Meta said on Medium that it has shut down nearly 550,00 accounts. That number includes 330,000 Instagram, 173,000 Facebook and 40,000 Threads accounts deemed to belong to children. \"Ongoing compliance with the law will be a multi-layered process that we will continue to refine, though our concerns about determining age online without an industry standard remain,\" the company wrote. Australia's minimum age social media ban, the first of its kind in the world for a democracy, went into effect on December 10. The ten platforms affected, including Facebook, Instagram, TikTok, Snapchat, X, Reddit and Twitch, must bar underage users or face a fine of up to $AUD 49.5 million ($33 million). Platforms are using a variety of means to determine age, including age inference based on activity and selfies. Some of those platforms aren't taking the ban lying down. Reddit, which launched a lawsuit against the Australian government, argued that it shouldn't have been included in the ban since it isn't a social media site, while adding that it comes with some \"serious privacy and political expression issues\" for users. Meta also expressed its opposition to the ban, citing a number of factors. It says taking social media out of the hands of teens can isolate them from getting support from online communities, and that the ban is only driving them to \"less regulated parts of the internet.\" It also sites inconsistent age verification methods and a lack of interest in compliance from teens and parents. However, the fact that Meta has removed almost 550,000 accounts just a month after the ban took affect shows that it is also affecting the company's bottom line. And Meta doesn't have a sterling record when it comes to teen safety, having previously downplayed the frequency of harm to children. This article originally appeared on Engadget at https://www.engadget.com/social-media/meta-closes-550000-accounts-to-comply-with-australias-kids-social-media-ban-130041356.html?src=rss",
          "content": "To comply with Australia's under-16 social media ban, Meta said on Medium that it has shut down nearly 550,00 accounts. That number includes 330,000 Instagram, 173,000 Facebook and 40,000 Threads accounts deemed to belong to children. \"Ongoing compliance with the law will be a multi-layered process that we will continue to refine, though our concerns about determining age online without an industry standard remain,\" the company wrote. Australia's minimum age social media ban, the first of its kind in the world for a democracy, went into effect on December 10. The ten platforms affected, including Facebook, Instagram, TikTok, Snapchat, X, Reddit and Twitch, must bar underage users or face a fine of up to $AUD 49.5 million ($33 million). Platforms are using a variety of means to determine age, including age inference based on activity and selfies. Some of those platforms aren't taking the ban lying down. Reddit, which launched a lawsuit against the Australian government, argued that it shouldn't have been included in the ban since it isn't a social media site, while adding that it comes with some \"serious privacy and political expression issues\" for users. Meta also expressed its opposition to the ban, citing a number of factors. It says taking social media out of the hands of teens can isolate them from getting support from online communities, and that the ban is only driving them to \"less regulated parts of the internet.\" It also sites inconsistent age verification methods and a lack of interest in compliance from teens and parents. However, the fact that Meta has removed almost 550,000 accounts just a month after the ban took affect shows that it is also affecting the company's bottom line. And Meta doesn't have a sterling record when it comes to teen safety, having previously downplayed the frequency of harm to children. This article originally appeared on Engadget at https://www.engadget.com/social-media/meta-closes-550000-accounts-to-comply-with-australias-kids-social-media-ban-130041356.html?src=rss",
          "feed_position": 34
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/big-tech/uk-regulator-ofcom-opens-a-formal-investigation-into-x-over-csam-scandal-120000312.html",
          "published_at": "Mon, 12 Jan 2026 12:00:00 +0000",
          "title": "UK regulator Ofcom opens a formal investigation into X over CSAM scandal",
          "standfirst": "The UK’s media regulator has opened a formal investigation into X under the Online Safety Act. \"There have been deeply concerning reports of the Grok AI chatbot account on X being used to create and share undressed images of people — which may amount to intimate image abuse or pornography — and sexualized images of children that may amount to child sexual abuse material (CSAM),\" Ofcom said.The investigation will focus on whether X has \"has complied with its duties to protect people in the UK from content that is illegal in the UK.\" That includes whether X is taking appropriate measures to prevent UK users from seeing \"priority\" illegal content, such as CSAM and non-consensual intimate images; if the platform is removing illegal content quickly after becoming aware of it; and whether X carried out an updated risk assessment before making \"any significant changes\" to the platform. The probe will also consider whether X assessed the risk that its platform poses to UK children and if it has ”highly effective age assurance to protect UK children from seeing pornography.”The regulator said it contacted X on January 5 and received a response by its January 9 deadline. Ofcom is conducting an \"expedited assessment of available evidence as a matter of urgency\" and added that it has asked xAI for \"urgent clarification\" on the steps the company is taking to protect UK users.\"Reports of Grok being used to create and share illegal non-consensual intimate images and child sexual abuse material on X have been deeply concerning,\" an Ofcom spokesperson said. \"Platforms must protect people in the UK from content that’s illegal in the UK, and we won’t hesitate to investigate where we suspect companies are failing in their duties, especially where there’s a risk of harm to children. We’ll progress this investigation as a matter of the highest priority, while ensuring we follow due process. As the UK’s independent online safety enforcement agency, it’s important we make sure our investigations are legally robust and fairly decided.\"If Ofcom deems that a company has broken the law, it can \"require platforms to take specific steps to come into compliance or to remedy harm caused by the breach.\" The regulator can additionally impose fines of up to £18 million ($24.3 million) or 10 percent of \"qualifying\" worldwide revenue, whichever of the two figures is higher. It can also seek a court order to stop payment providers or advertisers from working with a platform, or to require internet service providers to block a site in the UK. The UK government has said it would back any action that Ofcom takes against X. Reports over the weekend suggested that the UK had held discussions with allies over a coordinated response to Grok-generated deepfakes. Regulators elsewhere, including in India and the European Union, are also investigating X.Last week, the Grok account on X started telling users that its image generation and editing tools were being limited to paying subscribers. But as of Monday it was still possible for non-paying users to generate images through the Grok tab on the X website and app. Meanwhile, Malaysia and Indonesia became the first countries to block Grok, claiming that X’s chatbot does not have sufficient safeguards in place to prevent explicit AI-generated deepfakes of women and children from being created and disseminated on X. Indonesia temporarily blocked access to Grok on Saturday, as did Malaysia on Sunday, the Associated Press reports. \"The government sees non-consensual sexual deepfakes as a serious violation of human rights, dignity and the safety of citizens in the digital space,\" Indonesia’s Communication and Digital Affairs Minister Meutya Hafid said in a statement. Officials in the country said initial findings showed that Grok lacks effective controls to prevent users from creating and sharing sexually explicit deepfakes based on photos of Indonesian residents. The country's director general of digital space supervision, Alexander Sabar, said generating deepfakes can violate individuals' image and privacy rights when photos are shared or manipulated without consent, adding that they can lead to reputational, social and psychological harm.The Malaysian Communications and Multimedia Commission cited \"repeated misuse\" of Grok to generate explicit and non-consensual deepfakes, some of which involved women and children. The regulator said Grok will remain blocked in the country until X Corp and parent xAI establish strong enough safeguards. This article originally appeared on Engadget at https://www.engadget.com/big-tech/uk-regulator-ofcom-opens-a-formal-investigation-into-x-over-csam-scandal-120000312.html?src=rss",
          "content": "The UK’s media regulator has opened a formal investigation into X under the Online Safety Act. \"There have been deeply concerning reports of the Grok AI chatbot account on X being used to create and share undressed images of people — which may amount to intimate image abuse or pornography — and sexualized images of children that may amount to child sexual abuse material (CSAM),\" Ofcom said.The investigation will focus on whether X has \"has complied with its duties to protect people in the UK from content that is illegal in the UK.\" That includes whether X is taking appropriate measures to prevent UK users from seeing \"priority\" illegal content, such as CSAM and non-consensual intimate images; if the platform is removing illegal content quickly after becoming aware of it; and whether X carried out an updated risk assessment before making \"any significant changes\" to the platform. The probe will also consider whether X assessed the risk that its platform poses to UK children and if it has ”highly effective age assurance to protect UK children from seeing pornography.”The regulator said it contacted X on January 5 and received a response by its January 9 deadline. Ofcom is conducting an \"expedited assessment of available evidence as a matter of urgency\" and added that it has asked xAI for \"urgent clarification\" on the steps the company is taking to protect UK users.\"Reports of Grok being used to create and share illegal non-consensual intimate images and child sexual abuse material on X have been deeply concerning,\" an Ofcom spokesperson said. \"Platforms must protect people in the UK from content that’s illegal in the UK, and we won’t hesitate to investigate where we suspect companies are failing in their duties, especially where there’s a risk of harm to children. We’ll progress this investigation as a matter of the highest priority, while ensuring we follow due process. As the UK’s independent online safety enforcement agency, it’s important we make sure our investigations are legally robust and fairly decided.\"If Ofcom deems that a company has broken the law, it can \"require platforms to take specific steps to come into compliance or to remedy harm caused by the breach.\" The regulator can additionally impose fines of up to £18 million ($24.3 million) or 10 percent of \"qualifying\" worldwide revenue, whichever of the two figures is higher. It can also seek a court order to stop payment providers or advertisers from working with a platform, or to require internet service providers to block a site in the UK. The UK government has said it would back any action that Ofcom takes against X. Reports over the weekend suggested that the UK had held discussions with allies over a coordinated response to Grok-generated deepfakes. Regulators elsewhere, including in India and the European Union, are also investigating X.Last week, the Grok account on X started telling users that its image generation and editing tools were being limited to paying subscribers. But as of Monday it was still possible for non-paying users to generate images through the Grok tab on the X website and app. Meanwhile, Malaysia and Indonesia became the first countries to block Grok, claiming that X’s chatbot does not have sufficient safeguards in place to prevent explicit AI-generated deepfakes of women and children from being created and disseminated on X. Indonesia temporarily blocked access to Grok on Saturday, as did Malaysia on Sunday, the Associated Press reports. \"The government sees non-consensual sexual deepfakes as a serious violation of human rights, dignity and the safety of citizens in the digital space,\" Indonesia’s Communication and Digital Affairs Minister Meutya Hafid said in a statement. Officials in the country said initial findings showed that Grok lacks effective controls to prevent users from creating and sharing sexually explicit deepfakes based on photos of Indonesian residents. The country's director general of digital space supervision, Alexander Sabar, said generating deepfakes can violate individuals' image and privacy rights when photos are shared or manipulated without consent, adding that they can lead to reputational, social and psychological harm.The Malaysian Communications and Multimedia Commission cited \"repeated misuse\" of Grok to generate explicit and non-consensual deepfakes, some of which involved women and children. The regulator said Grok will remain blocked in the country until X Corp and parent xAI establish strong enough safeguards. This article originally appeared on Engadget at https://www.engadget.com/big-tech/uk-regulator-ofcom-opens-a-formal-investigation-into-x-over-csam-scandal-120000312.html?src=rss",
          "feed_position": 35
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/technology/anthropic-launches-cowork-a-claude-desktop-agent-that-works-in-your-files-no",
          "published_at": "Mon, 12 Jan 2026 11:30:00 GMT",
          "title": "Anthropic launches Cowork, a Claude Desktop agent that works in your files — no coding required",
          "standfirst": "Anthropic released Cowork on Monday, a new AI agent capability that extends the power of its wildly successful Claude Code tool to non-technical users — and according to company insiders, the team built the entire feature in approximately a week and a half, largely using Claude Code itself.The launch marks a major inflection point in the race to deliver practical AI agents to mainstream users, positioning Anthropic to compete not just with OpenAI and Google in conversational AI, but with Microsoft&#x27;s Copilot in the burgeoning market for AI-powered productivity tools.\"Cowork lets you complete non-technical tasks much like how developers use Claude Code,\" the company announced via its official Claude account on X. The feature arrives as a research preview available exclusively to Claude Max subscribers — Anthropic&#x27;s power-user tier priced between $100 and $200 per month — through the macOS desktop application.For the past year, the industry narrative has focused on large language models that can write poetry or debug code. With Cowork, Anthropic is betting that the real enterprise value lies in an AI that can open a folder, read a messy pile of receipts, and generate a structured expense report without human hand-holding.How developers using a coding tool for vacation research inspired Anthropic&#x27;s latest productThe genesis of Cowork lies in Anthropic&#x27;s recent success with the developer community. In late 2024, the company released Claude Code, a terminal-based tool that allowed software engineers to automate rote programming tasks. The tool was a hit, but Anthropic noticed a peculiar trend: users were forcing the coding tool to perform non-coding labor.According to Boris Cherny, an engineer at Anthropic, the company observed users deploying the developer tool for an unexpectedly diverse array of tasks.\"Since we launched Claude Code, we saw people using it for all sorts of non-coding work: doing vacation research, building slide decks, cleaning up your email, cancelling subscriptions, recovering wedding photos from a hard drive, monitoring plant growth, controlling your oven,\" Cherny wrote on X. \"These use cases are diverse and surprising — the reason is that the underlying Claude Agent is the best agent, and Opus 4.5 is the best model.\"Recognizing this shadow usage, Anthropic effectively stripped the command-line complexity from their developer tool to create a consumer-friendly interface. In its blog post announcing the feature, Anthropic explained that developers \"quickly began using it for almost everything else,\" which \"prompted us to build Cowork: a simpler way for anyone — not just developers — to work with Claude in the very same way.\"Inside the folder-based architecture that lets Claude read, edit, and create files on your computerUnlike a standard chat interface where a user pastes text for analysis, Cowork requires a different level of trust and access. Users designate a specific folder on their local machine that Claude can access. Within that sandbox, the AI agent can read existing files, modify them, or create entirely new ones.Anthropic offers several illustrative examples: reorganizing a cluttered downloads folder by sorting and intelligently renaming each file, generating a spreadsheet of expenses from a collection of receipt screenshots, or drafting a report from scattered notes across multiple documents.\"In Cowork, you give Claude access to a folder on your computer. Claude can then read, edit, or create files in that folder,\" the company explained on X. \"Try it to create a spreadsheet from a pile of screenshots, or produce a first draft from scattered notes.\"The architecture relies on what is known as an \"agentic loop.\" When a user assigns a task, the AI does not merely generate a text response. Instead, it formulates a plan, executes steps in parallel, checks its own work, and asks for clarification if it hits a roadblock. Users can queue multiple tasks and let Claude process them simultaneously — a workflow Anthropic describes as feeling \"much less like a back-and-forth and much more like leaving messages for a coworker.\"The system is built on Anthropic&#x27;s Claude Agent SDK, meaning it shares the same underlying architecture as Claude Code. Anthropic notes that Cowork \"can take on many of the same tasks that Claude Code can handle, but in a more approachable form for non-coding tasks.\"The recursive loop where AI builds AI: Claude Code reportedly wrote much of Claude CoworkPerhaps the most remarkable detail surrounding Cowork&#x27;s launch is the speed at which the tool was reportedly built — highlighting a recursive feedback loop where AI tools are being used to build better AI tools.During a livestream hosted by Dan Shipper, Felix Rieseberg, an Anthropic employee, confirmed that the team built Cowork in approximately a week and a half.Alex Volkov, who covers AI developments, expressed surprise at the timeline: \"Holy shit Anthropic built &#x27;Cowork&#x27; in the last... week and a half?!\"This prompted immediate speculation about how much of Cowork was itself built by Claude Code. Simon Smith, EVP of Generative AI at Klick Health, put it bluntly on X: \"Claude Code wrote all of Claude Cowork. Can we all agree that we&#x27;re in at least somewhat of a recursive improvement loop here?\"The implication is profound: Anthropic&#x27;s AI coding agent may have substantially contributed to building its own non-technical sibling product. If true, this is one of the most visible examples yet of AI systems being used to accelerate their own development and expansion — a strategy that could widen the gap between AI labs that successfully deploy their own agents internally and those that do not.Connectors, browser automation, and skills extend Cowork&#x27;s reach beyond the local file systemCowork doesn&#x27;t operate in isolation. The feature integrates with Anthropic&#x27;s existing ecosystem of connectors — tools that link Claude to external information sources and services such as Asana, Notion, PayPal, and other supported partners. Users who have configured these connections in the standard Claude interface can leverage them within Cowork sessions.Additionally, Cowork can pair with Claude in Chrome, Anthropic&#x27;s browser extension, to execute tasks requiring web access. This combination allows the agent to navigate websites, click buttons, fill forms, and extract information from the internet — all while operating from the desktop application.\"Cowork includes a number of novel UX and safety features that we think make the product really special,\" Cherny explained, highlighting \"a built-in VM [virtual machine] for isolation, out of the box support for browser automation, support for all your claude.ai data connectors, asking you for clarification when it&#x27;s unsure.\"Anthropic has also introduced an initial set of \"skills\" specifically designed for Cowork that enhance Claude&#x27;s ability to create documents, presentations, and other files. These build on the Skills for Claude framework the company announced in October, which provides specialized instruction sets Claude can load for particular types of tasks.Why Anthropic is warning users that its own AI agent could delete their filesThe transition from a chatbot that suggests edits to an agent that makes edits introduces significant risk. An AI that can organize files can, theoretically, delete them.In a notable display of transparency, Anthropic devoted considerable space in its announcement to warning users about Cowork&#x27;s potential dangers — an unusual approach for a product launch.The company explicitly acknowledges that Claude \"can take potentially destructive actions (such as deleting local files) if it&#x27;s instructed to.\" Because Claude might occasionally misinterpret instructions, Anthropic urges users to provide \"very clear guidance\" about sensitive operations.More concerning is the risk of prompt injection attacks — a technique where malicious actors embed hidden instructions in content Claude might encounter online, potentially causing the agent to bypass safeguards or take harmful actions.\"We&#x27;ve built sophisticated defenses against prompt injections,\" Anthropic wrote, \"but agent safety — that is, the task of securing Claude&#x27;s real-world actions — is still an active area of development in the industry.\"The company characterized these risks as inherent to the current state of AI agent technology rather than unique to Cowork. \"These risks aren&#x27;t new with Cowork, but it might be the first time you&#x27;re using a more advanced tool that moves beyond a simple conversation,\" the announcement notes.Anthropic&#x27;s desktop agent strategy sets up a direct challenge to Microsoft CopilotThe launch of Cowork places Anthropic in direct competition with Microsoft, which has spent years attempting to integrate its Copilot AI into the fabric of the Windows operating system with mixed adoption results.However, Anthropic&#x27;s approach differs in its isolation. By confining the agent to specific folders and requiring explicit connectors, they are attempting to strike a balance between the utility of an OS-level agent and the security of a sandboxed application.What distinguishes Anthropic&#x27;s approach is its bottom-up evolution. Rather than designing an AI assistant and retrofitting agent capabilities, Anthropic built a powerful coding agent first — Claude Code — and is now abstracting its capabilities for broader audiences. This technical lineage may give Cowork more robust agentic behavior from the start.Claude Code has generated significant enthusiasm among developers since its initial launch as a command-line tool in late 2024. The company expanded access with a web interface in October 2025, followed by a Slack integration in December. Cowork is the next logical step: bringing the same agentic architecture to users who may never touch a terminal.Who can access Cowork now, and what&#x27;s coming next for Windows and other platformsFor now, Cowork remains exclusive to Claude Max subscribers using the macOS desktop application. Users on other subscription tiers — Free, Pro, Team, or Enterprise — can join a waitlist for future access.Anthropic has signaled clear intentions to expand the feature&#x27;s reach. The blog post explicitly mentions plans to add cross-device sync and bring Cowork to Windows as the company learns from the research preview.Cherny set expectations appropriately, describing the product as \"early and raw, similar to what Claude Code felt like when it first launched.\"To access Cowork, Max subscribers can download or update the Claude macOS app and click on \"Cowork\" in the sidebar.The real question facing enterprise AI adoptionFor technical decision-makers, the implications of Cowork extend beyond any single product launch. The bottleneck for AI adoption is shifting — no longer is model intelligence the limiting factor, but rather workflow integration and user trust.Anthropic&#x27;s goal, as the company puts it, is to make working with Claude feel less like operating a tool and more like delegating to a colleague. Whether mainstream users are ready to hand over folder access to an AI that might misinterpret their instructions remains an open question.But the speed of Cowork&#x27;s development — a major feature built in ten days, possibly by the company&#x27;s own AI — previews a future where the capabilities of these systems compound faster than organizations can evaluate them. The chatbot has learned to use a file manager. What it learns to use next is anyone&#x27;s guess.",
          "content": "Anthropic released Cowork on Monday, a new AI agent capability that extends the power of its wildly successful Claude Code tool to non-technical users — and according to company insiders, the team built the entire feature in approximately a week and a half, largely using Claude Code itself.The launch marks a major inflection point in the race to deliver practical AI agents to mainstream users, positioning Anthropic to compete not just with OpenAI and Google in conversational AI, but with Microsoft&#x27;s Copilot in the burgeoning market for AI-powered productivity tools.\"Cowork lets you complete non-technical tasks much like how developers use Claude Code,\" the company announced via its official Claude account on X. The feature arrives as a research preview available exclusively to Claude Max subscribers — Anthropic&#x27;s power-user tier priced between $100 and $200 per month — through the macOS desktop application.For the past year, the industry narrative has focused on large language models that can write poetry or debug code. With Cowork, Anthropic is betting that the real enterprise value lies in an AI that can open a folder, read a messy pile of receipts, and generate a structured expense report without human hand-holding.How developers using a coding tool for vacation research inspired Anthropic&#x27;s latest productThe genesis of Cowork lies in Anthropic&#x27;s recent success with the developer community. In late 2024, the company released Claude Code, a terminal-based tool that allowed software engineers to automate rote programming tasks. The tool was a hit, but Anthropic noticed a peculiar trend: users were forcing the coding tool to perform non-coding labor.According to Boris Cherny, an engineer at Anthropic, the company observed users deploying the developer tool for an unexpectedly diverse array of tasks.\"Since we launched Claude Code, we saw people using it for all sorts of non-coding work: doing vacation research, building slide decks, cleaning up your email, cancelling subscriptions, recovering wedding photos from a hard drive, monitoring plant growth, controlling your oven,\" Cherny wrote on X. \"These use cases are diverse and surprising — the reason is that the underlying Claude Agent is the best agent, and Opus 4.5 is the best model.\"Recognizing this shadow usage, Anthropic effectively stripped the command-line complexity from their developer tool to create a consumer-friendly interface. In its blog post announcing the feature, Anthropic explained that developers \"quickly began using it for almost everything else,\" which \"prompted us to build Cowork: a simpler way for anyone — not just developers — to work with Claude in the very same way.\"Inside the folder-based architecture that lets Claude read, edit, and create files on your computerUnlike a standard chat interface where a user pastes text for analysis, Cowork requires a different level of trust and access. Users designate a specific folder on their local machine that Claude can access. Within that sandbox, the AI agent can read existing files, modify them, or create entirely new ones.Anthropic offers several illustrative examples: reorganizing a cluttered downloads folder by sorting and intelligently renaming each file, generating a spreadsheet of expenses from a collection of receipt screenshots, or drafting a report from scattered notes across multiple documents.\"In Cowork, you give Claude access to a folder on your computer. Claude can then read, edit, or create files in that folder,\" the company explained on X. \"Try it to create a spreadsheet from a pile of screenshots, or produce a first draft from scattered notes.\"The architecture relies on what is known as an \"agentic loop.\" When a user assigns a task, the AI does not merely generate a text response. Instead, it formulates a plan, executes steps in parallel, checks its own work, and asks for clarification if it hits a roadblock. Users can queue multiple tasks and let Claude process them simultaneously — a workflow Anthropic describes as feeling \"much less like a back-and-forth and much more like leaving messages for a coworker.\"The system is built on Anthropic&#x27;s Claude Agent SDK, meaning it shares the same underlying architecture as Claude Code. Anthropic notes that Cowork \"can take on many of the same tasks that Claude Code can handle, but in a more approachable form for non-coding tasks.\"The recursive loop where AI builds AI: Claude Code reportedly wrote much of Claude CoworkPerhaps the most remarkable detail surrounding Cowork&#x27;s launch is the speed at which the tool was reportedly built — highlighting a recursive feedback loop where AI tools are being used to build better AI tools.During a livestream hosted by Dan Shipper, Felix Rieseberg, an Anthropic employee, confirmed that the team built Cowork in approximately a week and a half.Alex Volkov, who covers AI developments, expressed surprise at the timeline: \"Holy shit Anthropic built &#x27;Cowork&#x27; in the last... week and a half?!\"This prompted immediate speculation about how much of Cowork was itself built by Claude Code. Simon Smith, EVP of Generative AI at Klick Health, put it bluntly on X: \"Claude Code wrote all of Claude Cowork. Can we all agree that we&#x27;re in at least somewhat of a recursive improvement loop here?\"The implication is profound: Anthropic&#x27;s AI coding agent may have substantially contributed to building its own non-technical sibling product. If true, this is one of the most visible examples yet of AI systems being used to accelerate their own development and expansion — a strategy that could widen the gap between AI labs that successfully deploy their own agents internally and those that do not.Connectors, browser automation, and skills extend Cowork&#x27;s reach beyond the local file systemCowork doesn&#x27;t operate in isolation. The feature integrates with Anthropic&#x27;s existing ecosystem of connectors — tools that link Claude to external information sources and services such as Asana, Notion, PayPal, and other supported partners. Users who have configured these connections in the standard Claude interface can leverage them within Cowork sessions.Additionally, Cowork can pair with Claude in Chrome, Anthropic&#x27;s browser extension, to execute tasks requiring web access. This combination allows the agent to navigate websites, click buttons, fill forms, and extract information from the internet — all while operating from the desktop application.\"Cowork includes a number of novel UX and safety features that we think make the product really special,\" Cherny explained, highlighting \"a built-in VM [virtual machine] for isolation, out of the box support for browser automation, support for all your claude.ai data connectors, asking you for clarification when it&#x27;s unsure.\"Anthropic has also introduced an initial set of \"skills\" specifically designed for Cowork that enhance Claude&#x27;s ability to create documents, presentations, and other files. These build on the Skills for Claude framework the company announced in October, which provides specialized instruction sets Claude can load for particular types of tasks.Why Anthropic is warning users that its own AI agent could delete their filesThe transition from a chatbot that suggests edits to an agent that makes edits introduces significant risk. An AI that can organize files can, theoretically, delete them.In a notable display of transparency, Anthropic devoted considerable space in its announcement to warning users about Cowork&#x27;s potential dangers — an unusual approach for a product launch.The company explicitly acknowledges that Claude \"can take potentially destructive actions (such as deleting local files) if it&#x27;s instructed to.\" Because Claude might occasionally misinterpret instructions, Anthropic urges users to provide \"very clear guidance\" about sensitive operations.More concerning is the risk of prompt injection attacks — a technique where malicious actors embed hidden instructions in content Claude might encounter online, potentially causing the agent to bypass safeguards or take harmful actions.\"We&#x27;ve built sophisticated defenses against prompt injections,\" Anthropic wrote, \"but agent safety — that is, the task of securing Claude&#x27;s real-world actions — is still an active area of development in the industry.\"The company characterized these risks as inherent to the current state of AI agent technology rather than unique to Cowork. \"These risks aren&#x27;t new with Cowork, but it might be the first time you&#x27;re using a more advanced tool that moves beyond a simple conversation,\" the announcement notes.Anthropic&#x27;s desktop agent strategy sets up a direct challenge to Microsoft CopilotThe launch of Cowork places Anthropic in direct competition with Microsoft, which has spent years attempting to integrate its Copilot AI into the fabric of the Windows operating system with mixed adoption results.However, Anthropic&#x27;s approach differs in its isolation. By confining the agent to specific folders and requiring explicit connectors, they are attempting to strike a balance between the utility of an OS-level agent and the security of a sandboxed application.What distinguishes Anthropic&#x27;s approach is its bottom-up evolution. Rather than designing an AI assistant and retrofitting agent capabilities, Anthropic built a powerful coding agent first — Claude Code — and is now abstracting its capabilities for broader audiences. This technical lineage may give Cowork more robust agentic behavior from the start.Claude Code has generated significant enthusiasm among developers since its initial launch as a command-line tool in late 2024. The company expanded access with a web interface in October 2025, followed by a Slack integration in December. Cowork is the next logical step: bringing the same agentic architecture to users who may never touch a terminal.Who can access Cowork now, and what&#x27;s coming next for Windows and other platformsFor now, Cowork remains exclusive to Claude Max subscribers using the macOS desktop application. Users on other subscription tiers — Free, Pro, Team, or Enterprise — can join a waitlist for future access.Anthropic has signaled clear intentions to expand the feature&#x27;s reach. The blog post explicitly mentions plans to add cross-device sync and bring Cowork to Windows as the company learns from the research preview.Cherny set expectations appropriately, describing the product as \"early and raw, similar to what Claude Code felt like when it first launched.\"To access Cowork, Max subscribers can download or update the Claude macOS app and click on \"Cowork\" in the sidebar.The real question facing enterprise AI adoptionFor technical decision-makers, the implications of Cowork extend beyond any single product launch. The bottleneck for AI adoption is shifting — no longer is model intelligence the limiting factor, but rather workflow integration and user trust.Anthropic&#x27;s goal, as the company puts it, is to make working with Claude feel less like operating a tool and more like delegating to a colleague. Whether mainstream users are ready to hand over folder access to an AI that might misinterpret their instructions remains an open question.But the speed of Cowork&#x27;s development — a major feature built in ten days, possibly by the company&#x27;s own AI — previews a future where the capabilities of these systems compound faster than organizations can evaluate them. The chatbot has learned to use a file manager. What it learns to use next is anyone&#x27;s guess.",
          "feed_position": 6,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/wHv1Wez7Ps9wYVYAo9fwT/14b41f606dbf1f5b17994be510407449/nuneybits_Hyper-realistic_image_of_a_retro_computer_with_a_glos_61ffb6e2-7c33-4d45-85f7-69c28693b3ec.webp?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/computing/accessories/best-laptop-power-bank-120040388.html",
          "published_at": "Mon, 12 Jan 2026 10:01:26 +0000",
          "title": "The best laptop power banks for 2026",
          "standfirst": "Laptop power banks come in handy if you often travel or work away from your desk. These larger batteries have enough capacity to refill your computer to around 75 percent, giving you many more productive hours. Of course, they can also charge smaller devices like your phone, camera or tablet a few times over. Nearly all portable laptop chargers fall below the 100-watt-hour mark so you can bring them in your carry-on when you fly. Additional features, like built-in cables and digital displays make these battery banks easier to use, too. We tested over a dozen options and put our thoughts below, so you can find the best laptop power bank for your next trip away from an outlet. Table of contents Best laptop power banks for 2026 What to look for in a laptop power bank How we test portable laptop chargers Other laptop power banks we tested Laptop power bank FAQs Recent updates Best laptop power banks for 2026 What to look for in a laptop power bank Flying with a laptop power bank Most portable batteries top out at around 27,000mAh so you can fly with them. The TSA currently limits the capacity carry-on batteries to 100Wh, which works out to around 27,500mAh for 3.6 volt lithium-ion batteries. Note that you’re not allowed to pack any batteries in your checked luggage, regardless of capacity. The TSA rules are intended to limit fire danger — and some airlines are implementing further restrictions due to recent on-board incidents. In March 2025, a Hong Kong flight was grounded after a battery pack caught fire in an overhead bin. A similar situation happened that same year in July on a domestic Delta flight, and again in August on a transatlantic KLM flight. As a result, some airlines, including Emirates, Southwest and others have announced further restrictions on flying with battery packs. Rules include limiting the number of allowed portable chargers and requiring flyers to keep power banks in clear view when using them to recharge a device. If the battery pack isn’t actively in use, however, most rules allow them to stay in your carry-on bag in the overhead bin. Before flying, it’s wise to check your airline’s policies. Capacity If you just need to keep a smartphone from dying before you can make it home, just about any of the best power banks will do. But if you need to revive multiple devices or the substantial battery of a laptop, you’ll want something with a high milliamp-hour​​ (mAh) capacity. A power bank capable of delivering enough power to a laptop will have a capacity between 20,000 and 27,000 mAh. If you want something even bigger than a laptop power bank, and don’t need to fly with it, you’ll likely want to look into portable power stations. These can be the size of a car battery or larger and can potentially fuel an entire weekend away. Another thing to keep in mind is that the capacity listed in a power bank's specs is not what will be delivered to your devices. As I mentioned, the capacity of these banks is around 25,000mAh. Even the huge battery on a 16-inch MacBook Pro or a Dell XPS 16 has a mAh rating of around 5,000 - 6,000mAh, so you might think you’d get five full charges but in reality, you only get about a single 70-percent charge. The voltage is different (typically 3.7V for the power bank and 11.4V for a laptop) which makes the watt-hours, or the amount of energy each battery can hold, different (working out to 92Wh for the battery and 72Wh for the built-in laptop batteries). On top of that, in order to feed a charge from a power bank to a laptop, a voltage conversion takes place and that dissipates a decent amount of energy. Without turning this into a physics lesson, this all means that a power bank with a 25,000mAh (or 92Wh) capacity will typically fill a 5,000mAh (or 72Wh) laptop battery to about 75 percent. In my tests, I averaged about a 60-percent efficiency rate between a power bank’s listed capacity and the actual charge delivered. Ports Every large power bank I’ve tested has at least three USB ports, with a mix of USB-C and USB-A, which should cover nearly any portable device you need to recharge — earbuds, phones, tablets, laptops, you name it. In addition to the different plug formats, some ports supply power at different wattages. For example, one built-in USB-C port might be rated for 60 watts, while the one next to it is rated for 100 watts. So if you’ve got a device that’s capable of 70W fast charging, such as the new MacBook Air, you’d want to opt for the 100W port to get the best charging speeds possible. Note that devices with a smaller wattage draw won’t be negatively affected by connecting to ports with high ratings. For example, a Galaxy S24 Ultra, capable of 45W super fast charging, is perfectly compatible with the 100W port. A device will only draw what it can take, regardless of what a port can supply. Just remember that the port, device and charging cable need to be at or above the desired wattage rating to achieve maximum charging rates. Some of these larger batteries also have AC ports. It might seem like a natural fit to plug in your laptop’s power adapter for a recharge. But really, the AC port should only be for devices that can’t use USB — such as a lamp or a printer. Plugging a power adapter into the AC port only wastes energy through conversion. First, the battery converts its DC power to supply the port with AC power, then the power adapter converts that AC power back to DC so your laptop can take it in. And as you’ll remember from physics class, each time energy is converted, some is lost to heat and other dissipations. Better to cut out the middleman and just send that DC power straight from the battery to the device. Also, you can use more than one port at a time with these devices; just remember that the speed of whatever you’re charging will likely go down, and of course, the battery is going to drain proportionally to what you’re refilling. Wireless charging Since I first started testing portable power banks a few years ago, wireless charging capabilities have noticeably improved. The first few I tried were painfully slow and not worth recommending. Now the wireless pads built into power banks are impressively fast — particularly, in my experience, when charging Samsung Galaxy phones (though the lack of a stabilizing magnetic connection like Apple’s MagSafe means they only work when rested flat on a pad). Most wireless charging connections can be used while other ports are also being employed, making them convenient for some mobile battlestation setups. Of course, wireless charging is always less efficient than wired, and recharging from an external battery is less efficient in general. If you want to waste as little energy as possible, you’re better off sticking to wired connections. Design All power banks are designed to be portable, but there’s a big difference between a pocket-friendly 5,000mAh battery and one of these laptop-compatible bruisers. Most of the latter weigh between a pound and a half to two pounds, which is a considerable addition to a backpack. Many of the options listed here have a display to tell you how much charge remains in the battery, which is helpful when you’re trying to judiciously meet out charges to your devices. If a bank has a wireless connection, the pad is usually on the flat top and any available AC connection is usually at one end. Both may require you to engage those charging methods. Don’t be like me and grumble loudly that you got a bum unit without pressing (and sometimes double pressing) all the buttons first. How we test portable laptop chargers For the past three years, I’ve been testing and using dozens of portable batteries for our other battery guide. Some of those batteries include the higher-capacity power banks you see here. I also got a hold of a few extra banks just for this guide to make sure we covered what’s available. I went for brands I’m already familiar with, as well as battery packs from well-received manufacturers I hadn’t tried before (like UGREEN and Lion Energy). I only considered banks with at least a 20,000mAh capacity and mostly stuck with those that rated 25,000mAh and higher. Here’s everything we tested: Zendure Supertank Pro Mophie Powerstation Pro XL Mophie Powerstation Pro AC Lion Energy Eclipse Mag Lion Energy Trek Baseus Blade Laptop Anker Prime 27,650mAh Goal Zero Sherpa 100 AC Anker Retractable Cable Laptop Bank HyperJuice 245W Anker Prime Power Bank (26K, 300W) UGreen Power Bank 25,000mAh 145W I tested each power bank with an Apple phone (iPhone 15 or 16), an Android phone (Galaxy S23 Ultra), a tablet (M1 iPad Air) and a laptop (16-inch MacBook Pro with the M1 Pro chip). Even though these banks can charge multiple devices at once, I refilled one at a time, to make side-by-side comparisons more straightforward. I drained the batteries of the phones and tablets to between zero and five percent and then didn’t use any device as it refilled. For the MacBook, I let it run down to 10 percent before plugging in the power bank. That's when most laptops give display a “connect to power” warning, as draining any battery to empty will compromise the battery life. I then used it as one might in a mobile office, with a Bluetooth keyboard and mouse, while connected to Wi-Fi and a VPN. For each test, I noted how long a completely charged battery took to get a device back to full and how much of the battery’s capacity was used up in one charge. I also noted things like portability, apparent durability, helpful features and overall design. For reference, here are the battery capacities of the devices I used: iPhone 15: 3,349mAh Galaxy S23 Ultra: 4,855mAh iPad Air (5th gen): 7,729mAh 16-inch M1 Pro MacBook Pro: 27,027mAh Other laptop power banks we tested HyperJuice 245W Hyper’s HyperJuice 245W brick looks great and has a hefty 27,000mAh capacity. The four USB-C ports can combine to output 245W of power and it got my MacBook Pro from nearly dead to 75 percent before depleting itself. When testing it with a Samsung Galaxy S23 Ultra, the handset got back up to a full charge in just over an hour. The screen tells you what each port is doing as well as displaying the amount of charge remaining in the pack itself. But the lack of port variety makes it feel less versatile than other picks on this list — the price is higher than our other options, too. Laptop power bank FAQs How do laptop power banks differ from phone power banks? The main difference is size. Phone power banks tend to have a capacity ranging from 5,000mAh to 20,000mAh and laptop powerbanks are typically rated between 20,000mAh and 27,000mAh. There’s no official definition, however. Laptop batteries are simply larger and need a bigger supply of power to give them a meaningful charge. How do you fast charge a power bank? You can charge a power bank exactly as fast as the power bank’s internal mechanisms will allow. Most batteries are limited in how quickly they can accept and deliver a charge to avoid dangerously overheating. But to make sure you’re charging a bank as quickly as possible, make sure the wall adapter and the USB-C cable you are using have a high wattage rating — using a 5W power brick and a 10W cable will take a lot longer to refill your bank than a 65W wall charger and a 100W cord. What size power bank do I need for a laptop? Look for a power bank with a rating of at least 20,000mAh. Slightly smaller batteries may work, but they won’t deliver a significant charge laptops. How many mAh to charge a laptop? A milliamp hour (mAh) is how much a battery can hold, and most portable batteries list their capacity using mAh. If you get a battery rated at 20,000mAh or above, it should be able to charge your laptop. Using mAh to discuss laptop batteries can be confusing. Due to differing voltages, you can’t directly compare the mAh ratings of a power bank battery to a laptop battery. Using watt-hours is a better gauge, as that calculation takes voltage into account. Recent updates November 2025: Updated our overall top pick to the Anker Laptop Power bank. Added a premium power bank pick. This article originally appeared on Engadget at https://www.engadget.com/computing/accessories/best-laptop-power-bank-120040388.html?src=rss",
          "content": "Laptop power banks come in handy if you often travel or work away from your desk. These larger batteries have enough capacity to refill your computer to around 75 percent, giving you many more productive hours. Of course, they can also charge smaller devices like your phone, camera or tablet a few times over. Nearly all portable laptop chargers fall below the 100-watt-hour mark so you can bring them in your carry-on when you fly. Additional features, like built-in cables and digital displays make these battery banks easier to use, too. We tested over a dozen options and put our thoughts below, so you can find the best laptop power bank for your next trip away from an outlet. Table of contents Best laptop power banks for 2026 What to look for in a laptop power bank How we test portable laptop chargers Other laptop power banks we tested Laptop power bank FAQs Recent updates Best laptop power banks for 2026 What to look for in a laptop power bank Flying with a laptop power bank Most portable batteries top out at around 27,000mAh so you can fly with them. The TSA currently limits the capacity carry-on batteries to 100Wh, which works out to around 27,500mAh for 3.6 volt lithium-ion batteries. Note that you’re not allowed to pack any batteries in your checked luggage, regardless of capacity. The TSA rules are intended to limit fire danger — and some airlines are implementing further restrictions due to recent on-board incidents. In March 2025, a Hong Kong flight was grounded after a battery pack caught fire in an overhead bin. A similar situation happened that same year in July on a domestic Delta flight, and again in August on a transatlantic KLM flight. As a result, some airlines, including Emirates, Southwest and others have announced further restrictions on flying with battery packs. Rules include limiting the number of allowed portable chargers and requiring flyers to keep power banks in clear view when using them to recharge a device. If the battery pack isn’t actively in use, however, most rules allow them to stay in your carry-on bag in the overhead bin. Before flying, it’s wise to check your airline’s policies. Capacity If you just need to keep a smartphone from dying before you can make it home, just about any of the best power banks will do. But if you need to revive multiple devices or the substantial battery of a laptop, you’ll want something with a high milliamp-hour​​ (mAh) capacity. A power bank capable of delivering enough power to a laptop will have a capacity between 20,000 and 27,000 mAh. If you want something even bigger than a laptop power bank, and don’t need to fly with it, you’ll likely want to look into portable power stations. These can be the size of a car battery or larger and can potentially fuel an entire weekend away. Another thing to keep in mind is that the capacity listed in a power bank's specs is not what will be delivered to your devices. As I mentioned, the capacity of these banks is around 25,000mAh. Even the huge battery on a 16-inch MacBook Pro or a Dell XPS 16 has a mAh rating of around 5,000 - 6,000mAh, so you might think you’d get five full charges but in reality, you only get about a single 70-percent charge. The voltage is different (typically 3.7V for the power bank and 11.4V for a laptop) which makes the watt-hours, or the amount of energy each battery can hold, different (working out to 92Wh for the battery and 72Wh for the built-in laptop batteries). On top of that, in order to feed a charge from a power bank to a laptop, a voltage conversion takes place and that dissipates a decent amount of energy. Without turning this into a physics lesson, this all means that a power bank with a 25,000mAh (or 92Wh) capacity will typically fill a 5,000mAh (or 72Wh) laptop battery to about 75 percent. In my tests, I averaged about a 60-percent efficiency rate between a power bank’s listed capacity and the actual charge delivered. Ports Every large power bank I’ve tested has at least three USB ports, with a mix of USB-C and USB-A, which should cover nearly any portable device you need to recharge — earbuds, phones, tablets, laptops, you name it. In addition to the different plug formats, some ports supply power at different wattages. For example, one built-in USB-C port might be rated for 60 watts, while the one next to it is rated for 100 watts. So if you’ve got a device that’s capable of 70W fast charging, such as the new MacBook Air, you’d want to opt for the 100W port to get the best charging speeds possible. Note that devices with a smaller wattage draw won’t be negatively affected by connecting to ports with high ratings. For example, a Galaxy S24 Ultra, capable of 45W super fast charging, is perfectly compatible with the 100W port. A device will only draw what it can take, regardless of what a port can supply. Just remember that the port, device and charging cable need to be at or above the desired wattage rating to achieve maximum charging rates. Some of these larger batteries also have AC ports. It might seem like a natural fit to plug in your laptop’s power adapter for a recharge. But really, the AC port should only be for devices that can’t use USB — such as a lamp or a printer. Plugging a power adapter into the AC port only wastes energy through conversion. First, the battery converts its DC power to supply the port with AC power, then the power adapter converts that AC power back to DC so your laptop can take it in. And as you’ll remember from physics class, each time energy is converted, some is lost to heat and other dissipations. Better to cut out the middleman and just send that DC power straight from the battery to the device. Also, you can use more than one port at a time with these devices; just remember that the speed of whatever you’re charging will likely go down, and of course, the battery is going to drain proportionally to what you’re refilling. Wireless charging Since I first started testing portable power banks a few years ago, wireless charging capabilities have noticeably improved. The first few I tried were painfully slow and not worth recommending. Now the wireless pads built into power banks are impressively fast — particularly, in my experience, when charging Samsung Galaxy phones (though the lack of a stabilizing magnetic connection like Apple’s MagSafe means they only work when rested flat on a pad). Most wireless charging connections can be used while other ports are also being employed, making them convenient for some mobile battlestation setups. Of course, wireless charging is always less efficient than wired, and recharging from an external battery is less efficient in general. If you want to waste as little energy as possible, you’re better off sticking to wired connections. Design All power banks are designed to be portable, but there’s a big difference between a pocket-friendly 5,000mAh battery and one of these laptop-compatible bruisers. Most of the latter weigh between a pound and a half to two pounds, which is a considerable addition to a backpack. Many of the options listed here have a display to tell you how much charge remains in the battery, which is helpful when you’re trying to judiciously meet out charges to your devices. If a bank has a wireless connection, the pad is usually on the flat top and any available AC connection is usually at one end. Both may require you to engage those charging methods. Don’t be like me and grumble loudly that you got a bum unit without pressing (and sometimes double pressing) all the buttons first. How we test portable laptop chargers For the past three years, I’ve been testing and using dozens of portable batteries for our other battery guide. Some of those batteries include the higher-capacity power banks you see here. I also got a hold of a few extra banks just for this guide to make sure we covered what’s available. I went for brands I’m already familiar with, as well as battery packs from well-received manufacturers I hadn’t tried before (like UGREEN and Lion Energy). I only considered banks with at least a 20,000mAh capacity and mostly stuck with those that rated 25,000mAh and higher. Here’s everything we tested: Zendure Supertank Pro Mophie Powerstation Pro XL Mophie Powerstation Pro AC Lion Energy Eclipse Mag Lion Energy Trek Baseus Blade Laptop Anker Prime 27,650mAh Goal Zero Sherpa 100 AC Anker Retractable Cable Laptop Bank HyperJuice 245W Anker Prime Power Bank (26K, 300W) UGreen Power Bank 25,000mAh 145W I tested each power bank with an Apple phone (iPhone 15 or 16), an Android phone (Galaxy S23 Ultra), a tablet (M1 iPad Air) and a laptop (16-inch MacBook Pro with the M1 Pro chip). Even though these banks can charge multiple devices at once, I refilled one at a time, to make side-by-side comparisons more straightforward. I drained the batteries of the phones and tablets to between zero and five percent and then didn’t use any device as it refilled. For the MacBook, I let it run down to 10 percent before plugging in the power bank. That's when most laptops give display a “connect to power” warning, as draining any battery to empty will compromise the battery life. I then used it as one might in a mobile office, with a Bluetooth keyboard and mouse, while connected to Wi-Fi and a VPN. For each test, I noted how long a completely charged battery took to get a device back to full and how much of the battery’s capacity was used up in one charge. I also noted things like portability, apparent durability, helpful features and overall design. For reference, here are the battery capacities of the devices I used: iPhone 15: 3,349mAh Galaxy S23 Ultra: 4,855mAh iPad Air (5th gen): 7,729mAh 16-inch M1 Pro MacBook Pro: 27,027mAh Other laptop power banks we tested HyperJuice 245W Hyper’s HyperJuice 245W brick looks great and has a hefty 27,000mAh capacity. The four USB-C ports can combine to output 245W of power and it got my MacBook Pro from nearly dead to 75 percent before depleting itself. When testing it with a Samsung Galaxy S23 Ultra, the handset got back up to a full charge in just over an hour. The screen tells you what each port is doing as well as displaying the amount of charge remaining in the pack itself. But the lack of port variety makes it feel less versatile than other picks on this list — the price is higher than our other options, too. Laptop power bank FAQs How do laptop power banks differ from phone power banks? The main difference is size. Phone power banks tend to have a capacity ranging from 5,000mAh to 20,000mAh and laptop powerbanks are typically rated between 20,000mAh and 27,000mAh. There’s no official definition, however. Laptop batteries are simply larger and need a bigger supply of power to give them a meaningful charge. How do you fast charge a power bank? You can charge a power bank exactly as fast as the power bank’s internal mechanisms will allow. Most batteries are limited in how quickly they can accept and deliver a charge to avoid dangerously overheating. But to make sure you’re charging a bank as quickly as possible, make sure the wall adapter and the USB-C cable you are using have a high wattage rating — using a 5W power brick and a 10W cable will take a lot longer to refill your bank than a 65W wall charger and a 100W cord. What size power bank do I need for a laptop? Look for a power bank with a rating of at least 20,000mAh. Slightly smaller batteries may work, but they won’t deliver a significant charge laptops. How many mAh to charge a laptop? A milliamp hour (mAh) is how much a battery can hold, and most portable batteries list their capacity using mAh. If you get a battery rated at 20,000mAh or above, it should be able to charge your laptop. Using mAh to discuss laptop batteries can be confusing. Due to differing voltages, you can’t directly compare the mAh ratings of a power bank battery to a laptop battery. Using watt-hours is a better gauge, as that calculation takes voltage into account. Recent updates November 2025: Updated our overall top pick to the Anker Laptop Power bank. Added a premium power bank pick. This article originally appeared on Engadget at https://www.engadget.com/computing/accessories/best-laptop-power-bank-120040388.html?src=rss",
          "feed_position": 36
        }
      ],
      "featured_image": "https://images.ctfassets.net/jdtwqhzvc2n1/7DUx86jAglIoinWLIrfpO8/abd01f833379598313f3bdeb2c35a96e/Egnyte.png?w=300&q=30",
      "popularity_score": 2018.161328611111
    },
    {
      "id": "cluster_35",
      "coverage": 2,
      "updated_at": "Tue, 13 Jan 2026 12:10:00 -0500",
      "title": "Google makes improvements to Veo 3.1's Ingredients to Video feature to make videos \"more expressive\", and adds native vertical video generation and 4K upscaling (Jess Weatherbed/The Verge)",
      "neutral_headline": "Google’s Veo now turns portrait images into vertical AI videos",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/260113/p34#a260113p34",
          "published_at": "Tue, 13 Jan 2026 12:10:00 -0500",
          "title": "Google makes improvements to Veo 3.1's Ingredients to Video feature to make videos \"more expressive\", and adds native vertical video generation and 4K upscaling (Jess Weatherbed/The Verge)",
          "standfirst": "Jess Weatherbed / The Verge: Google makes improvements to Veo 3.1's Ingredients to Video feature to make videos &ldquo;more expressive&rdquo;, and adds native vertical video generation and 4K upscaling &mdash; &#65279;Videos should now be more consistent with the images they're based on, including those in portrait orientations.",
          "content": "Jess Weatherbed / The Verge: Google makes improvements to Veo 3.1's Ingredients to Video feature to make videos &ldquo;more expressive&rdquo;, and adds native vertical video generation and 4K upscaling &mdash; &#65279;Videos should now be more consistent with the images they're based on, including those in portrait orientations.",
          "feed_position": 5,
          "image_url": "http://www.techmeme.com/260113/i34.jpg"
        },
        {
          "source": "The Verge",
          "url": "https://www.theverge.com/news/861257/google-veo-3-1-ai-video-ingredients-vertical-update",
          "published_at": "2026-01-13T12:00:00-05:00",
          "title": "Google’s Veo now turns portrait images into vertical AI videos",
          "standfirst": "Google is making its Veo 3.1 AI video model pay closer attention to the reference images you want generated clips to be based on. The company is releasing new visual improvements for the \"Ingredients to Video\" tool that was introduced last year, alongside expanding native vertical video support and resolution upscaling features. The Ingredients to [&#8230;]",
          "content": "Google says that generated videos should also now be more consistent with thereference images they’re based on. | Image: Google / The Verge Google is making its Veo 3.1 AI video model pay closer attention to the reference images you want generated clips to be based on. The company is releasing new visual improvements for the \"Ingredients to Video\" tool that was introduced last year, alongside expanding native vertical video support and resolution upscaling features. The Ingredients to Video tool allows Veo users to generate videos based on up to three reference images, pulling in materials like character subjects, backgrounds, and textures to have more control over how the results will look. Google says this update will make videos \"more expressive and creative,\" and provide \"r … Read the full story at The Verge.",
          "feed_position": 5
        }
      ],
      "featured_image": "http://www.techmeme.com/260113/i34.jpg",
      "popularity_score": 2017.8279952777777
    },
    {
      "id": "cluster_41",
      "coverage": 2,
      "updated_at": "Tue, 13 Jan 2026 11:45:02 -0500",
      "title": "Sources: India asked quick commerce companies to drop their 10-minute delivery promise due to concerns about rider safety; Blinkit already removed the assurance (Bloomberg)",
      "neutral_headline": "India reportedly tells quick-commerce firms to drop 10-minute delivery promise",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/260113/p33#a260113p33",
          "published_at": "Tue, 13 Jan 2026 11:45:02 -0500",
          "title": "Sources: India asked quick commerce companies to drop their 10-minute delivery promise due to concerns about rider safety; Blinkit already removed the assurance (Bloomberg)",
          "standfirst": "Bloomberg: Sources: India asked quick commerce companies to drop their 10-minute delivery promise due to concerns about rider safety; Blinkit already removed the assurance &mdash; India is asking quick commerce companies to drop their 10-minute delivery promise amid mounting concerns that the ultra-tight deadlines &hellip;",
          "content": "Bloomberg: Sources: India asked quick commerce companies to drop their 10-minute delivery promise due to concerns about rider safety; Blinkit already removed the assurance &mdash; India is asking quick commerce companies to drop their 10-minute delivery promise amid mounting concerns that the ultra-tight deadlines &hellip;",
          "feed_position": 6,
          "image_url": "http://www.techmeme.com/260113/i33.jpg"
        },
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2026/01/13/india-reportedly-tells-quick-commerce-firms-to-drop-10-minute-delivery-promise/",
          "published_at": "Tue, 13 Jan 2026 16:27:46 +0000",
          "title": "India reportedly tells quick-commerce firms to drop 10-minute delivery promise",
          "standfirst": "India's labor ministry is pushing the country's booming quick-commerce sector to prioritize the wellness and safety of its gig workers.",
          "content": "India's labor ministry is pushing the country's booming quick-commerce sector to prioritize the wellness and safety of its gig workers.",
          "feed_position": 6
        }
      ],
      "featured_image": "http://www.techmeme.com/260113/i33.jpg",
      "popularity_score": 2017.4118841666666
    },
    {
      "id": "cluster_45",
      "coverage": 2,
      "updated_at": "2026-01-13T11:35:10-05:00",
      "title": "Apple Creator Studio suite is launching to take on Adobe",
      "neutral_headline": "Apple Creator Studio suite is launching to take on Adobe",
      "items": [
        {
          "source": "The Verge",
          "url": "https://www.theverge.com/news/861279/apple-creator-studio-apps-subscription-price-availability",
          "published_at": "2026-01-13T11:35:10-05:00",
          "title": "Apple Creator Studio suite is launching to take on Adobe",
          "standfirst": "Apple has announced Apple Creator Studio, a new software suite for Apple products that bundles popular creative apps into an all-in-one subscription service. Apple Creator Studio will be available on the App Store on January 28th and includes access to Final Cut Pro, Logic Pro, Pixelmator Pro, Motion, Compressor, and MainStage - providing a rival [&#8230;]",
          "content": "Final Cut Pro, Logic Pro, Pixelmator Pro, Motion, Compressor, and MainStage are still available as one-time purchases. Apple has announced Apple Creator Studio, a new software suite for Apple products that bundles popular creative apps into an all-in-one subscription service. Apple Creator Studio will be available on the App Store on January 28th and includes access to Final Cut Pro, Logic Pro, Pixelmator Pro, Motion, Compressor, and MainStage - providing a rival editing and design subscription to Adobe's Creative Cloud platform. Apple Creator Studio will launch with a one-month free trial and costs $12.99 per month, or $129 per year. Students and educators can expect to pay less: $2.99 per month, or $29.99 per year. The Final Cut Pro video editor, Logic … Read the full story at The Verge.",
          "feed_position": 8
        },
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2026/01/13/apple-launches-creator-studio-bundle-of-apps-for-12-99-per-month/",
          "published_at": "Tue, 13 Jan 2026 14:40:19 +0000",
          "title": "Apple launches &#8216;Creator Studio&#8217; bundle of apps for $12.99 per month",
          "standfirst": "The bundle includes access to Final Cut Pro, Logic Pro, and Pixelmator Pro on Mac and iPad, as well as Motion, Compressor, and MainStage on Mac.",
          "content": "The bundle includes access to Final Cut Pro, Logic Pro, and Pixelmator Pro on Mac and iPad, as well as Motion, Compressor, and MainStage on Mac.",
          "feed_position": 8
        }
      ],
      "popularity_score": 2017.2474397222222
    },
    {
      "id": "cluster_8",
      "coverage": 1,
      "updated_at": "Tue, 13 Jan 2026 18:56:28 +0000",
      "title": "EPA moves to stop considering economic benefits of cleaner air",
      "neutral_headline": "EPA moves to stop considering economic benefits of cleaner air",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2026/01/epa-axes-benefits-from-cost-benefit-analysis-for-air-pollution-limits/",
          "published_at": "Tue, 13 Jan 2026 18:56:28 +0000",
          "title": "EPA moves to stop considering economic benefits of cleaner air",
          "standfirst": "New language criticizes “uncertainties” in longstanding EPA practice.",
          "content": "If you were to do a cost-benefit analysis of your lunch, it would be pretty difficult to do the calculation without the sandwich. But it appears that the US Environmental Protection Agency (EPA) is moving in this same direction—removing the benefit—when it comes to air pollution regulations. According to a New York Times report based on internal emails and documents—and demonstrated by a recently produced analysis on the EPA website—the EPA is changing its cost-benefit analysis process for common air pollutants. Instead of comparing the economic cost of a certain pollution limit to an estimate of the economic value of the resulting improvements in human health, the EPA will just qualitatively describe health benefits while carefully quantifying economic costs. Cost-benefit analysis has been a key component of EPA regulations. Any decision to raise or lower air quality standards or pollution limits includes evaluations of the cost that change, like the addition of new pollution control equipment at power plants, would incur, for example.Read full article Comments",
          "feed_position": 0,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2182255823-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2182255823-1152x648.jpg",
      "popularity_score": 367.60243972222224
    },
    {
      "id": "cluster_0",
      "coverage": 1,
      "updated_at": "Tue, 13 Jan 2026 19:18:35 +0000",
      "title": "Starlink tries to stay online in Iran as regime jams signals during protests",
      "neutral_headline": "Starlink tries to stay online in Iran as regime jams signals during protests",
      "items": [
        {
          "source": "Ars Technica Main",
          "url": "https://arstechnica.com/tech-policy/2026/01/starlink-tries-to-stay-online-in-iran-as-regime-jams-signals-during-protests/",
          "published_at": "Tue, 13 Jan 2026 19:18:35 +0000",
          "title": "Starlink tries to stay online in Iran as regime jams signals during protests",
          "standfirst": "Iran shut off Internet as it cracks down on protests; even Starlink has problems.",
          "content": "President Trump asked Elon Musk to get Starlink working more reliably in Iran to thwart the Iranian government's Internet shutdown. Starlink operator SpaceX was apparently already working on the problem before Trump reached out to Musk. Iran severed Internet connections and phone lines last week as the government conducted a violent crackdown on anti-government demonstrators, according to numerous reports, which say that thousands of people have been killed. Starlink hasn't been completely disabled. The government's jamming technology has reportedly caused Starlink packet loss of anywhere from 30 to 80 percent.Read full article Comments",
          "feed_position": 0,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/starlink-iran-1152x648-1768330630.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/starlink-iran-1152x648-1768330630.jpg",
      "popularity_score": 357.9710508333333
    },
    {
      "id": "cluster_21",
      "coverage": 1,
      "updated_at": "Tue, 13 Jan 2026 18:24:03 +0000",
      "title": "Apple’s Mac and iPad creative apps get bundled into “Creator Studio” subscription",
      "neutral_headline": "Apple’s Mac and iPad creative apps get bundled into “Creator Studio” subscription",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2026/01/apple-creator-studio-bundles-final-cut-pro-logic-and-other-apps-for-129-year/",
          "published_at": "Tue, 13 Jan 2026 18:24:03 +0000",
          "title": "Apple’s Mac and iPad creative apps get bundled into “Creator Studio” subscription",
          "standfirst": "Video, audio, and image editing apps join up in one subscription on January 28.",
          "content": "Apple's professional creative apps have been slower to jump on the subscription bandwagon than those from Adobe or some of its other competitors, but the company is taking a step in that direction today. Starting on January 28, Apple will offer an Apple Creator Studio subscription for $13 a month, or $130 a year. Subscribers will get access to the Mac and (where applicable) iPad versions of Final Cut Pro, Logic Pro, Pixelmator Pro, Motion, Compressor, and MainStage, as well as \"intelligent features and premium content\" for the Mac, iPad, and iPhone versions of Keynote, Pages, Numbers, and Freeform. Apple says it will also offer a one-month free trial for the subscription and a discounted version for students at $3 a month, or $30 a year. Most of the apps also seem to be getting small feature updates to go along with the Creator Studio announcement. Final Cut will get a new Transcript Search feature that will allow you to dig through video footage by searching for specific dialogue, and a new Montage Maker feature \"will analyze and edit together a dynamic video based on the best visual moments within the footage.\" An updated Logic Pro \"helps creators deliver original music for their video content\" and adds a synth player to the app's lineup of \"AI Session Players.\"Read full article Comments",
          "feed_position": 1,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/Apple-Creator-Studio-lifestyle-Final-Cut-Pro-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/Apple-Creator-Studio-lifestyle-Final-Cut-Pro-1152x648.jpg",
      "popularity_score": 354.0621619444444
    },
    {
      "id": "cluster_24",
      "coverage": 1,
      "updated_at": "Tue, 13 Jan 2026 18:15:01 +0000",
      "title": "Scott Adams, Dilbert creator, dead at 68",
      "neutral_headline": "Scott Adams, Dilbert creator, dead at 68",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/culture/2026/01/scott-adams-dilbert-creator-dead-at-68/",
          "published_at": "Tue, 13 Jan 2026 18:15:01 +0000",
          "title": "Scott Adams, Dilbert creator, dead at 68",
          "standfirst": "\"I had an amazing life. I gave it everything I had.\"",
          "content": "Scott Adams, the creator of the Dilbert comic strip, died today of prostate cancer at 68. Adams satirized the world of cubicle-based IT and engineering in Dilbert, which at its height appeared in 2,000 daily newspapers and was later anthologized in numerous books. Dilbert was an engineer with few social skills, but he always knew more than his pointy-haired boss, a caricature of terrible supervisors everywhere who managed to make the life of those who actually knew what they were doing—the engineers—much harder than it needed to be.Read full article Comments",
          "feed_position": 2,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-1322060604-1024x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-1322060604-1024x648.jpg",
      "popularity_score": 328.9116063888889
    },
    {
      "id": "cluster_34",
      "coverage": 1,
      "updated_at": "Tue, 13 Jan 2026 17:11:51 +0000",
      "title": "This one could use less power: The Jeep Wagoneer S EV",
      "neutral_headline": "This one could use less power: The Jeep Wagoneer S EV",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/cars/2026/01/this-one-could-use-less-power-the-jeep-wagoneer-s-ev/",
          "published_at": "Tue, 13 Jan 2026 17:11:51 +0000",
          "title": "This one could use less power: The Jeep Wagoneer S EV",
          "standfirst": "Poorly calibrated pedal mapping marred the Wagoneer S experience.",
          "content": "It's not really accurate to call the Wagoneer S Jeep's first electric vehicle. For several years now, Europeans have been able to buy the Jeep Avenger, a subcompact crossover that will surely never see American roads. But it is the first electric Jeep designed for American consumption. It's aimed at the highly competitive midsize SUV segment, which gets ever more crowded even as electrification faces a less certain future here. Indeed, the brand, along with its Stellantis sibling Chrysler, just shelved all its plug-in hybrids, discontinuing them just a few days ago. Like the little Avenger, the Wagoneer S makes use of one of parent company Stellantis' purpose-built EV platforms, one shared with the growly-sounding Dodge Charger. At 192.4 inches (4,886 mm) long, 74.8 inches (1,900 mm) wide, and 64.8 inches (1,645 mm) tall, it's a little larger than cars like the BMW iX3 or Audi Q6 e-tron but a little smaller than domestically designed rivals like the Cadillac Lyriq and Acura ZDX, which have particularly long wheelbases. I find it a rather handsome car, one that has to marry Jeep's Wagoneer styling cues with as many wind-smoothing and air-shaping elements as possible. The way the rear wing juts out above the tailgate window reminds me of a '90s rally hatchback, but it's the product of the designers and the engineers working on drag reduction. The overall drag coefficient is 0.29, and since Jeep actually publishes the frontal area, too, I can tell you the more important CdA number—where drag is multiplied by the frontal area—is 8.67 sq ft (0.805 m2).Read full article Comments",
          "feed_position": 3,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/WS025_007WS-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/WS025_007WS-1152x648.jpg",
      "popularity_score": 326.8588286111111
    },
    {
      "id": "cluster_44",
      "coverage": 1,
      "updated_at": "Tue, 13 Jan 2026 16:35:52 +0000",
      "title": "A new Titan emerges in Monarch: Legacy of Monsters S2 teaser",
      "neutral_headline": "A new Titan emerges in Monarch: Legacy of Monsters S2 teaser",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/culture/2026/01/a-new-titan-emerges-in-monarch-legacy-of-monsters-s2-teaser/",
          "published_at": "Tue, 13 Jan 2026 16:35:52 +0000",
          "title": "A new Titan emerges in Monarch: Legacy of Monsters S2 teaser",
          "standfirst": "Apple TV's sci-fi series is part of Legendary Entertainment’s MonsterVerse and is set after 2014's Godzilla.",
          "content": "It's looking to be a solid year for kaiju fans. Not only are we getting Godzilla Minus Zero in November—sequel to the critically acclaimed Godzilla Minus One (2023)—but Apple TV just released a teaser for the second season of Monarch: Legacy of Monsters, part of Legendary Entertainment’s MonsterVerse, which brought Godzilla, King Kong, and various other monsters (kaiju) created by Toho Co., Ltd into the same fold. (Spoilers for S1 below.) The first season picked up where 2014's Godzilla left off, specifically the introduction of Project Monarch, a secret organization established in the 1950s to study Godzilla and other kaiju—after attempts to kill Godzilla with nuclear weapons failed. The plot spans three generations and takes place in the 1950s and half a century later. In the first season, two siblings (Kate and Kentaro Randa) follow in their father’s footsteps to uncover their family’s connection to the secretive organization known as Monarch. Naturally, they find themselves in the world of monsters and discover Army officer Lee Shaw (Kurt Russell), a longtime family ally.Read full article Comments",
          "feed_position": 4,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/monarch2-1152x648-1768318930.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/monarch2-1152x648-1768318930.jpg",
      "popularity_score": 310.2591063888889
    },
    {
      "id": "cluster_74",
      "coverage": 1,
      "updated_at": "Tue, 13 Jan 2026 12:15:19 +0000",
      "title": "Wild mushrooms keep killing people in California; 3 dead, 35 poisoned",
      "neutral_headline": "Mushrooms keep killing people in California; 3 dead, 35 poisoned",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/health/2026/01/wild-mushrooms-keep-killing-people-in-california-3-dead-35-poisoned/",
          "published_at": "Tue, 13 Jan 2026 12:15:19 +0000",
          "title": "Wild mushrooms keep killing people in California; 3 dead, 35 poisoned",
          "standfirst": "Officials have linked the poisonings to the death cap mushroom (Amanita phalloides).",
          "content": "A third person has died in a rash of poisonings from wild, foraged mushrooms in California, health officials report. Since November, a total of 35 people across the state have been poisoned by mushrooms, leading to three people receiving liver transplants in addition to the three deaths. Health officials in Sonoma County reported the latest death last week. Michael Stacey, Sonoma's interim health officer, attributed the cases and deaths to an extraordinary boom in the prevalence of death cap mushrooms (Amanita phalloides), noting that in an average year, the state sees fewer than five mushroom poisoning cases.Read full article Comments",
          "feed_position": 5,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/death-cap-mushroom.png"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/death-cap-mushroom.png",
      "popularity_score": 295.9166063888889
    },
    {
      "id": "cluster_100",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 23:42:09 +0000",
      "title": "Anthropic launches Cowork, a Claude Code-like for general computing",
      "neutral_headline": "Anthropic launches Cowork, a Claude Code-like for general computing",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/ai/2026/01/anthropic-launches-cowork-a-claude-code-like-for-general-computing/",
          "published_at": "Mon, 12 Jan 2026 23:42:09 +0000",
          "title": "Anthropic launches Cowork, a Claude Code-like for general computing",
          "standfirst": "Users can give Claude access to a folder and tell it what to do for them.",
          "content": "Anthropic's agentic tool Claude Code has been an enormous hit with some software developers and hobbyists, and now the company is bringing that modality to more general office work with a new feature called Cowork. Built on the same foundations as Claude Code and baked into the macOS Claude desktop app, Cowork allows users to give Claude access to a specific folder on their computer and then give plain language instructions for tasks. Anthropic gave examples like filling out an expense report from a folder full of receipt photos, writing reports based on a big stack of digital notes, or reorganizing a folder (or cleaning up your desktop) based on a prompt.Read full article Comments",
          "feed_position": 7,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/Claude-Cowork-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/Claude-Cowork-1152x648.jpg",
      "popularity_score": 263.3638286111111
    },
    {
      "id": "cluster_101",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 23:04:33 +0000",
      "title": "You can now reserve a hotel room on the Moon for $250,000",
      "neutral_headline": "You can now reserve a hotel room on the Moon for $250,000",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2026/01/you-can-now-reserve-a-hotel-room-on-the-moon-for-250000/",
          "published_at": "Mon, 12 Jan 2026 23:04:33 +0000",
          "title": "You can now reserve a hotel room on the Moon for $250,000",
          "standfirst": "\"We can't keep everyone living on that first ship that sailed to North America.\"",
          "content": "A company called GRU Space publicly announced its intent to construct a series of increasingly sophisticated habitats on the Moon, culminating in a hotel inspired by the Palace of the Fine Arts in San Francisco. On Monday, the company invited those interested in a berth to plunk down a deposit between $250,000 and $1 million, qualifying them for a spot on one of its early lunar surface missions in as little as six years from now. It sounds crazy, doesn't it? After all, GRU Space had, as of late December when I spoke to founder Skyler Chan, a single full-time employee aside from himself. And Chan, in fact, only recently graduated from the University of California, Berkeley.Read full article Comments",
          "feed_position": 8,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/lunar-hotel-1-1152x648-1768258910.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/lunar-hotel-1-1152x648-1768258910.jpg",
      "popularity_score": 253
    },
    {
      "id": "cluster_102",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 22:49:31 +0000",
      "title": "Paramount sues WBD over Netflix deal. WBD says Paramount’s price is still inadequate.",
      "neutral_headline": "Paramount sues WBD over Netflix deal. WBD says Paramount’s price is still inadequate.",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gadgets/2026/01/paramount-sues-wbd-over-netflix-deal-wbd-says-paramounts-price-is-still-inadequate/",
          "published_at": "Mon, 12 Jan 2026 22:49:31 +0000",
          "title": "Paramount sues WBD over Netflix deal. WBD says Paramount’s price is still inadequate.",
          "standfirst": "WBD calls Paramount's lawsuit \"meritless\" and its offer deficient.",
          "content": "Paramount Skydance escalated its hostile takeover bid of Warner Bros. Discovery (WBD) today by filing a lawsuit in Delaware Chancery Court against WBD, declaring its intention to fight Netflix’s acquisition. In December, WBD agreed to sell its streaming and movie businesses to Netflix for $82.7 billion. The deal would see WBD’s Global Networks division, composed of WBD's legacy cable networks, spun out into a separate company called Discovery Global. But in December, Paramount submitted a hostile takeover bid and amended its bid for WBD. Subsequently, the company has aggressively tried to convince WBD’s shareholders that its $108.4 billion offer for all of WBD is superior to the Netflix deal. Today, Paramount CEO David Ellison wrote a letter to WBD shareholders informing them of Paramount’s lawsuit. The lawsuit requests the court to force WBD to disclose “how it valued the Global Networks stub equity, how it valued the overall Netflix transaction, how the purchase price reduction for debt works in the Netflix transaction, or even what the basis is for its ‘risk adjustment’” of Paramount’s $30 per share all-cash offer. Netflix’s offer equates to $27.72 per share, including $23.25 in cash and shares of Netflix common stock. Paramount hopes the information will encourage more WBD shareholders to tender their shares under Paramount's offer by the January 21 deadline.Read full article Comments",
          "feed_position": 9,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2215193098-1152x648-1768255617.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2215193098-1152x648-1768255617.jpg",
      "popularity_score": 243
    },
    {
      "id": "cluster_107",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 21:47:32 +0000",
      "title": "Google removes some AI health summaries after investigation finds “dangerous” flaws",
      "neutral_headline": "Google removes some AI health summaries after investigation finds “dangerous” flaws",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/ai/2026/01/google-removes-some-ai-health-summaries-after-investigation-finds-dangerous-flaws/",
          "published_at": "Mon, 12 Jan 2026 21:47:32 +0000",
          "title": "Google removes some AI health summaries after investigation finds “dangerous” flaws",
          "standfirst": "AI Overviews provided false liver test information experts called alarming.",
          "content": "On Sunday, Google removed some of its AI Overviews health summaries after a Guardian investigation found people were being put at risk by false and misleading information. The removals came after the newspaper found that Google's generative AI feature delivered inaccurate health information at the top of search results, potentially leading seriously ill patients to mistakenly conclude they are in good health. Google disabled specific queries, such as \"what is the normal range for liver blood tests,\" after experts contacted by The Guardian flagged the results as dangerous. The report also highlighted a critical error regarding pancreatic cancer: The AI suggested patients avoid high-fat foods, a recommendation that contradicts standard medical guidance to maintain weight and could jeopardize patient health. Despite these findings, Google only deactivated the summaries for the liver test queries, leaving other potentially harmful answers accessible. The investigation revealed that searching for liver test norms generated raw data tables (listing specific enzymes like ALT, AST, and alkaline phosphatase) that lacked essential context. The AI feature also failed to adjust these figures for patient demographics such as age, sex, and ethnicity. Experts warned that because the AI model's definition of \"normal\" often differed from actual medical standards, patients with serious liver conditions might mistakenly believe they are healthy and skip necessary follow-up care.Read full article Comments",
          "feed_position": 12,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2024/05/GettyImages-1488311999-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2024/05/GettyImages-1488311999-1152x648.jpg",
      "popularity_score": 160
    },
    {
      "id": "cluster_119",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 17:57:32 +0000",
      "title": "Apple chooses Google’s Gemini over OpenAI’s ChatGPT to power next-gen Siri",
      "neutral_headline": "Apple chooses Google’s Gemini over OpenAI’s ChatGPT to power next-gen Siri",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/apple/2026/01/apple-says-its-new-ai-powered-siri-will-use-googles-gemini-language-models/",
          "published_at": "Mon, 12 Jan 2026 17:57:32 +0000",
          "title": "Apple chooses Google’s Gemini over OpenAI’s ChatGPT to power next-gen Siri",
          "standfirst": "Apple goes with Google's tech despite using OpenAI's ChatGPT elsewhere in iOS.",
          "content": "The \"more intelligent\" version of Siri that Apple plans to release later this year will be backed by Google's Gemini language models, the company announced today. CNBC reports that the deal is part of a \"multi-year partnership\" between Apple and Google that will allow Apple to use Google's AI models in its own software. \"After careful evaluation, we determined that Google’s technology provides the most capable foundation for Apple Foundation Models and we’re excited about the innovative new experiences it will unlock for our users,” reads an Apple statement given to CNBC. Today's announcement confirms reporting by Bloomberg's Mark Gurman late last year that Apple and Google were nearing a deal. Apple didn't disclose terms, but Gurman said that Apple would be paying Google \"about $1 billion a year\" for access to its AI models \"following an extensive evaluation period.\"Read full article Comments",
          "feed_position": 18,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/apple_google_hero_3-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/apple_google_hero_3-1152x648.jpg",
      "popularity_score": 160
    },
    {
      "id": "cluster_112",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 19:36:17 +0000",
      "title": "Apps like Grok are explicitly banned under Google’s rules—why is it still in the Play Store?",
      "neutral_headline": "Apps like Grok are explicitly banned under Google’s rules—why is it...",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/google/2026/01/apps-like-grok-are-explicitly-banned-under-googles-rules-why-is-it-still-in-the-play-store/",
          "published_at": "Mon, 12 Jan 2026 19:36:17 +0000",
          "title": "Apps like Grok are explicitly banned under Google’s rules—why is it still in the Play Store?",
          "standfirst": "Google describes apps exactly like Grok and says they are banned from Google Play.",
          "content": "Elon Musk's xAI recently weakened content guard rails for image generation in the Grok AI bot. This led to a new spate of non-consensual sexual imagery on X, much of it aimed at silencing women on the platform. This, along with the creation of sexualized images of children in the more compliant Grok, has led regulators to begin investigating xAI. In the meantime, Google has rules in place for exactly this eventuality—it's just not enforcing them. It really could not be more clear from Google's publicly available policies that Grok should have been banned yesterday. And yet, it remains in the Play Store. Not only that—it enjoys a T for Teen rating, one notch below the M-rated X app. Apple also still offers the Grok app on its platform, but its rules actually leave more wiggle room. App content restrictions at Apple and Google have evolved in very different ways. From the start, Apple has been prone to removing apps on a whim, so developers have come to expect that Apple's guidelines may not mention every possible eventuality. As Google has shifted from a laissez-faire attitude to more hard-nosed control of the Play Store, it has progressively piled on clarifications in the content policy. As a result, Google's rules are spelled out in no uncertain terms, and Grok runs afoul of them.Read full article Comments",
          "feed_position": 16,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/07/GettyImages-2225386195-1024x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/07/GettyImages-2225386195-1024x648.jpg",
      "popularity_score": 145
    },
    {
      "id": "cluster_113",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 19:25:17 +0000",
      "title": "NASA launches new mission to get the most out of the James Webb Space Telescope",
      "neutral_headline": "NASA launches new mission to get the most out of the James Webb Space Telescope",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/space/2026/01/nasas-newest-telescope-will-play-an-outsize-role-in-finding-earth-2-0/",
          "published_at": "Mon, 12 Jan 2026 19:25:17 +0000",
          "title": "NASA launches new mission to get the most out of the James Webb Space Telescope",
          "standfirst": "\"It was not recognized how serious a problem that is until... about 2017 or 2018.\"",
          "content": "Among other things, the James Webb Space Telescope is designed to get us closer to finding habitable worlds around faraway stars. From its perch a million miles from Earth, Webb's huge gold-coated mirror collects more light than any other telescope put into space. The Webb telescope, launched in 2021 at a cost of more than $10 billion, has the sensitivity to peer into distant planetary systems and detect the telltale chemical fingerprints of molecules critical to or indicative of potential life, like water vapor, carbon dioxide, and methane. Webb can do this while also observing the oldest observable galaxies in the Universe and studying planets, moons, and smaller objects within our own Solar System. Naturally, astronomers want to get the most out of their big-budget observatory. That's where NASA's Pandora mission comes in.Read full article Comments",
          "feed_position": 17,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/Pandora_integrated_blue_BCT-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/Pandora_integrated_blue_BCT-1152x648.jpg",
      "popularity_score": 141
    },
    {
      "id": "cluster_104",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 22:27:50 +0000",
      "title": "Even Linus Torvalds is trying his hand at vibe coding (but just a little)",
      "neutral_headline": "Even Linus Torvalds is trying his hand at vibe coding (but just a little)",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/ai/2026/01/hobby-github-repo-shows-linus-torvalds-vibe-codes-sometimes/",
          "published_at": "Mon, 12 Jan 2026 22:27:50 +0000",
          "title": "Even Linus Torvalds is trying his hand at vibe coding (but just a little)",
          "standfirst": "\"But then I cut out the middle man—me.\"",
          "content": "Linux and Git creator Linus Torvalds' latest project contains code that was \"basically written by vibe coding,\" but you shouldn't read that to mean that Torvalds is embracing that approach for anything and everything. Torvalds sometimes works on small hobby projects over holiday breaks. Last year, he made guitar pedals. This year, he did some work on AudioNoise, which he calls \"another silly guitar-pedal-related repo.\" It creates random digital audio effects. Torvalds revealed that he had used an AI coding tool in the README for the repo:Read full article Comments",
          "feed_position": 10,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2015/08/LinuxCon_Europe_Linus_Torvalds_05-1152x648-1768254932.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2015/08/LinuxCon_Europe_Linus_Torvalds_05-1152x648-1768254932.jpg",
      "popularity_score": 139
    },
    {
      "id": "cluster_109",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 21:27:35 +0000",
      "title": "Judge: Trump violated Fifth Amendment by ending energy grants in only blue states",
      "neutral_headline": "Judge: Trump violated Fifth Amendment by ending energy grants in only blue states",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2026/01/judge-trump-violated-fifth-amendment-by-ending-energy-grants-in-only-blue-states/",
          "published_at": "Mon, 12 Jan 2026 21:27:35 +0000",
          "title": "Judge: Trump violated Fifth Amendment by ending energy grants in only blue states",
          "standfirst": "Donald Trump’s social media post triggers rare Fifth Amendment ruling.",
          "content": "The Trump administration violated the Fifth Amendment when canceling billions of dollars in environmental grants for projects in \"blue states\" that didn't vote for him in the last election, a judge ruled Monday. Trump's blatant discrimination came on the same day as the government shut down last fall. In total, 315 grants were terminated in October, ending support for 223 projects worth approximately $7.5 billion, the Department of Energy confirmed. All the awardees, except for one, were based in states where Donald Trump lost the majority vote to Kamala Harris in 2024. Only seven awardees sued, defending projects that helped states with \"electric vehicle development, updating building energy codes, and addressing methane emissions.\" They accused Trump officials of clearly discriminating against Democratic voters by pointing to their social media posts boasting about punishing blue states.Read full article Comments",
          "feed_position": 13,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2254848310-1024x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-2254848310-1024x648.jpg",
      "popularity_score": 133
    },
    {
      "id": "cluster_110",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 20:00:54 +0000",
      "title": "Switching water sources improved hygiene of Pompeii’s public baths",
      "neutral_headline": "Switching water sources improved hygiene of Pompeii’s public baths",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2026/01/switching-water-sources-improved-hygiene-of-pompeiis-public-baths/",
          "published_at": "Mon, 12 Jan 2026 20:00:54 +0000",
          "title": "Switching water sources improved hygiene of Pompeii’s public baths",
          "standfirst": "Scientists analyzed carbonate deposits from baths, aqueduct to learn more about city's changing water supply.",
          "content": "The eruption of Mount Vesuvius in 79 CE released thermal energy roughly equivalent to 100,000 times the atomic bombs dropped on Hiroshima and Nagasaki at the end of World War II, spewing molten rock, pumice, and hot ash over Pompeii. Pompeii's public baths, aqueduct, and water towers were among the preserved structures frozen in time. A new paper published in the Proceedings of the National Academy of Sciences analyzed calcium carbonate deposits from those structures to learn more about the city's water supply and how it changed over time. Pompeii was founded in the sixth century BCE. Prior research revealed that, early on, the city relied on rainwater stored in cisterns and wells for its water supply. The public baths used weight-lifting machinery to lift water up well shafts that were as deep as 40 meters. As the city developed, so did the complexity of its water supply system, most notably with the construction of an aqueduct between 27 BCE and 14 CE. The authors of this latest paper were interested in the calcium carbonate deposits left by water in well shafts as well as the baths and aqueduct. The different layers have \"different chemical and isotope composition, calcite crystal size, and shape,\" which in turn could reveal information about seasonal changes in temperature, as well as changes over time in the chemical composition of the water. Analyzing those properties would enable them to \"reconstruct the history of such systems—particularly public baths—revealing aspects of their maintenance and the adaptations made during their period of use,\" the authors wrote.Read full article Comments",
          "feed_position": 14,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/bath1-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2026/01/bath1-1152x648.jpg",
      "popularity_score": 133
    },
    {
      "id": "cluster_111",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 19:56:01 +0000",
      "title": "Supreme Court takes case that could strip FCC of authority to issue fines",
      "neutral_headline": "Supreme Court takes case that could strip FCC of authority to issue fines",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2026/01/supreme-court-takes-case-that-could-strip-fcc-of-authority-to-issue-fines/",
          "published_at": "Mon, 12 Jan 2026 19:56:01 +0000",
          "title": "Supreme Court takes case that could strip FCC of authority to issue fines",
          "standfirst": "AT&#038;T and Verizon claim right to a jury trial was violated by FCC fines.",
          "content": "The Supreme Court will hear a case that could invalidate the Federal Communications Commission's authority to issue fines against companies regulated by the FCC. AT&T, Verizon, and T-Mobile challenged the FCC's ability to punish them after the commission fined the carriers for selling customer location data without their users’ consent. AT&T convinced the US Court of Appeals for the 5th Circuit to overturn its fine, while Verizon lost in the 2nd Circuit and T-Mobile lost in the District of Columbia Circuit. Verizon petitioned the Supreme Court to reverse its loss, while the FCC and Justice Department petitioned the court to overturn AT&T's victory in the 5th Circuit. The Supreme Court granted both petitions to hear the challenges and consolidated the cases in a list of orders released Friday. Oral arguments will be held.Read full article Comments",
          "feed_position": 15,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2023/09/getty-supreme-court-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2023/09/getty-supreme-court-1152x648.jpg",
      "popularity_score": 133
    },
    {
      "id": "cluster_121",
      "coverage": 1,
      "updated_at": "Mon, 12 Jan 2026 17:08:56 +0000",
      "title": "Is this the beginning of the end for GameStop?",
      "neutral_headline": "Is this the beginning of the end for GameStop",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/gaming/2026/01/is-this-the-beginning-of-the-end-for-gamestop/",
          "published_at": "Mon, 12 Jan 2026 17:08:56 +0000",
          "title": "Is this the beginning of the end for GameStop?",
          "standfirst": "The sudden closure of hundreds of storefronts isn't exactly a great sign...",
          "content": "Six and a half years ago—after a failed corporate sale attempt, massive financial losses, and the departure/layoff of many key staff—I wrote about what seemed at the time like the \"imminent demise\" of GameStop. Now, after five years of meme stock mania that helped prop up the company's finances a bit, I'll admit the video game and Funko Pop retailer has lasted much longer as a relevant entity than I anticipated. GameStop's surprisingly extended run may be coming to an end, though, with Polygon reporting late last week that GameStop has abruptly shut down 400 stores across the US, with even more closures expected before the end of the month. That comes on top of 590 US stores that were shuttered in fiscal 2024 (which ended in January 2025) and stated plans to close hundreds of remaining international stores across Canada, Australia, and Europe in the coming months, per SEC filings. GameStop still had just over 3,200 stores worldwide as of February 1, 2025, so even hundreds of new and planned store closures don't literally mean the immediate end of the company as a going concern. But when you consider that there were still nearly 6,000 GameStop locations worldwide as of 2019—nearly 4,000 of which were in the US—the long-term trend is clear.Read full article Comments",
          "feed_position": 19,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2019/08/GettyImages-1135950796-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2019/08/GettyImages-1135950796-1152x648.jpg",
      "popularity_score": 130
    }
  ]
}