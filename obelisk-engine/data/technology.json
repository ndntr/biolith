{
  "updated_at": "2025-12-31T23:20:13.407Z",
  "clusters": [
    {
      "id": "cluster_4",
      "coverage": 2,
      "updated_at": "Wed, 31 Dec 2025 21:10:00 GMT",
      "title": "Open source Qwen-Image-2512 launches to compete with Google's Nano Banana Pro in high quality AI image generation",
      "neutral_headline": "How to watch the Sony Honda Afeela CES 2026 press conference",
      "items": [
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/technology/open-source-qwen-image-2512-launches-to-compete-with-googles-nano-banana-pro",
          "published_at": "Wed, 31 Dec 2025 21:10:00 GMT",
          "title": "Open source Qwen-Image-2512 launches to compete with Google's Nano Banana Pro in high quality AI image generation",
          "standfirst": "When Google released its newest AI image model Nano Banana Pro (aka Gemini 3 Pro Image) in November, it reset expectations for the entire field. For the first time, uses of an image model could use natural language to generate dense, text-heavy infographics, slides, and other enterprise-grade visuals without spelling errors. But that leap forward came with a familiar tradeoff. Gemini 3 Pro Image is deeply proprietary, tightly bound to Google’s cloud stack, and priced for premium usage. For enterprises that need predictable costs, deployment sovereignty, or regional localization, the model raised the bar without offering many viable alternatives.Alibaba’s Qwen team of AI researchers — already having a banner year with numerous powerful open source AI model releases — is now answering with its own alternative, Qwen-Image-2512, once again available freely for developers and even large enterprises for commercial purposes under a standard, permissive Apache 2.0 license.The model can be used directly by consumers via Qwen Chat, and its full open-source weights are up on Hugging Face or ModelScope, and inspected or integrated from source on GitHub. For zero-install experimentation, the Qwen team also provides a hosted Hugging Face demo and a browser-based ModelScope demo. Enterprises that prefer managed inference can access the same generation capabilities through Alibaba Cloud’s Model Studio API.A response to a changing enterprise marketThe impact of Gemini 3 Pro Image was not subtle. Its ability to generate production-ready diagrams, slides, menus, and multilingual visuals pushed image generation beyond creative experimentation and into enterprise infrastructure territory—a shift reflected across broader conversations around orchestration, data pipelines, and AI security.In that framing, image models are no longer artistic tools. They are workflow components, expected to slot into documentation systems, design pipelines, marketing automation, and training platforms with consistency and control.Most responses to Google’s move have been proprietary: API-only access, usage-based pricing, and tight platform coupling — such as OpenAI&#x27;s own GPT Image 1.5 released earlier this month.Qwen-Image-2512 takes a different approach, betting that performance parity plus openness is what a large segment of the enterprise market actually wants.What Qwen-Image-2512 improves—and why it mattersThe December 2512 update focuses on three areas that have become non-negotiable for enterprise image generation.Human realism and environmental coherence: Qwen-Image-2512 significantly reduces the “AI look” that has long plagued open models. Facial features show age and texture more accurately, postures adhere more closely to prompts, and background environments are rendered with clearer semantic context. For enterprises using synthetic imagery in training, simulations, or internal communications, this realism is essential for credibility.Natural texture fidelity: Landscapes, water, animal fur, and materials are rendered with finer detail and smoother gradients. These improvements are not cosmetic; they enable synthetic imagery for ecommerce, education, and visualization without extensive manual cleanup.Structured text and layout rendering: Qwen-Image-2512 improves embedded text accuracy and layout consistency, supporting both Chinese and English prompts. Slides, posters, infographics, and mixed text-image compositions are more legible and more faithful to instructions. This is the same category where Gemini 3 Pro Image drew the loudest praise—and where many earlier open models struggled.In blind, human-evaluated testing on Alibaba’s AI Arena, Qwen-Image-2512 ranks as the strongest open-source image model and remains competitive with closed systems, reinforcing its claim as a production-ready option rather than a research preview.Open source changes the deployment calculusWhere Qwen-Image-2512 most clearly differentiates itself is licensing. Released under Apache 2.0, the model can be freely used, modified, fine-tuned, and deployed commercially.For enterprises, this unlocks options that proprietary models do not:Cost control: At scale, per-image API pricing compounds quickly. Self-hosting allows organizations to amortize infrastructure costs instead of paying perpetual usage fees.Data governance: Regulated industries often require strict control over data residency, logging, and auditability.Localization and customization: Teams can adapt models for regional languages, cultural norms, or internal style guides without waiting on a vendor roadmap.By contrast, Gemini 3 Pro Image offers strong governance assurances but remains inseparable from Google’s infrastructure and pricing model.API pricing for managed deploymentsFor teams that prefer managed inference, Qwen-Image-2512 is available via Alibaba Cloud Model Studio as qwen-image-max, priced at $0.075 per generated image. The API accepts text input and returns image output, with rate limits suitable for production workloads. Free quotas are limited, and usage transitions to paid billing once credits are exhausted.This hybrid approach—open weights paired with a commercial API—mirrors how many enterprises deploy AI today: experimentation and customization in-house, with managed services layered on where operational simplicity matters.Competitive, but philosophically differentQwen-Image-2512 is not positioned as a universal replacement for Gemini 3 Pro Image. Google’s model benefits from deep integration with Vertex AI, Workspace, Ads, and Gemini’s broader reasoning stack. For organizations already committed to Google Cloud, Nano Banana Pro fits naturally into existing pipelines.Qwen’s strategy is more modular. The model integrates cleanly with open tooling and custom orchestration layers, making it attractive to teams building their own AI stacks or combining image generation with internal data systems.A signal to the marketThe release of Qwen-Image-2512 reinforces a broader shift: open-source AI is no longer content to trail proprietary systems by a generation. Instead, it is selectively matching the capabilities that matter most for enterprise deployment—text fidelity, layout control, and realism—while preserving the freedoms enterprises increasingly demand.Google’s Gemini 3 Pro Image raised the ceiling. Qwen-Image-2512 shows that enterprises now have a serious open-source alternative—one that aligns performance with cost control, governance, and deployment choice.",
          "content": "When Google released its newest AI image model Nano Banana Pro (aka Gemini 3 Pro Image) in November, it reset expectations for the entire field. For the first time, uses of an image model could use natural language to generate dense, text-heavy infographics, slides, and other enterprise-grade visuals without spelling errors. But that leap forward came with a familiar tradeoff. Gemini 3 Pro Image is deeply proprietary, tightly bound to Google’s cloud stack, and priced for premium usage. For enterprises that need predictable costs, deployment sovereignty, or regional localization, the model raised the bar without offering many viable alternatives.Alibaba’s Qwen team of AI researchers — already having a banner year with numerous powerful open source AI model releases — is now answering with its own alternative, Qwen-Image-2512, once again available freely for developers and even large enterprises for commercial purposes under a standard, permissive Apache 2.0 license.The model can be used directly by consumers via Qwen Chat, and its full open-source weights are up on Hugging Face or ModelScope, and inspected or integrated from source on GitHub. For zero-install experimentation, the Qwen team also provides a hosted Hugging Face demo and a browser-based ModelScope demo. Enterprises that prefer managed inference can access the same generation capabilities through Alibaba Cloud’s Model Studio API.A response to a changing enterprise marketThe impact of Gemini 3 Pro Image was not subtle. Its ability to generate production-ready diagrams, slides, menus, and multilingual visuals pushed image generation beyond creative experimentation and into enterprise infrastructure territory—a shift reflected across broader conversations around orchestration, data pipelines, and AI security.In that framing, image models are no longer artistic tools. They are workflow components, expected to slot into documentation systems, design pipelines, marketing automation, and training platforms with consistency and control.Most responses to Google’s move have been proprietary: API-only access, usage-based pricing, and tight platform coupling — such as OpenAI&#x27;s own GPT Image 1.5 released earlier this month.Qwen-Image-2512 takes a different approach, betting that performance parity plus openness is what a large segment of the enterprise market actually wants.What Qwen-Image-2512 improves—and why it mattersThe December 2512 update focuses on three areas that have become non-negotiable for enterprise image generation.Human realism and environmental coherence: Qwen-Image-2512 significantly reduces the “AI look” that has long plagued open models. Facial features show age and texture more accurately, postures adhere more closely to prompts, and background environments are rendered with clearer semantic context. For enterprises using synthetic imagery in training, simulations, or internal communications, this realism is essential for credibility.Natural texture fidelity: Landscapes, water, animal fur, and materials are rendered with finer detail and smoother gradients. These improvements are not cosmetic; they enable synthetic imagery for ecommerce, education, and visualization without extensive manual cleanup.Structured text and layout rendering: Qwen-Image-2512 improves embedded text accuracy and layout consistency, supporting both Chinese and English prompts. Slides, posters, infographics, and mixed text-image compositions are more legible and more faithful to instructions. This is the same category where Gemini 3 Pro Image drew the loudest praise—and where many earlier open models struggled.In blind, human-evaluated testing on Alibaba’s AI Arena, Qwen-Image-2512 ranks as the strongest open-source image model and remains competitive with closed systems, reinforcing its claim as a production-ready option rather than a research preview.Open source changes the deployment calculusWhere Qwen-Image-2512 most clearly differentiates itself is licensing. Released under Apache 2.0, the model can be freely used, modified, fine-tuned, and deployed commercially.For enterprises, this unlocks options that proprietary models do not:Cost control: At scale, per-image API pricing compounds quickly. Self-hosting allows organizations to amortize infrastructure costs instead of paying perpetual usage fees.Data governance: Regulated industries often require strict control over data residency, logging, and auditability.Localization and customization: Teams can adapt models for regional languages, cultural norms, or internal style guides without waiting on a vendor roadmap.By contrast, Gemini 3 Pro Image offers strong governance assurances but remains inseparable from Google’s infrastructure and pricing model.API pricing for managed deploymentsFor teams that prefer managed inference, Qwen-Image-2512 is available via Alibaba Cloud Model Studio as qwen-image-max, priced at $0.075 per generated image. The API accepts text input and returns image output, with rate limits suitable for production workloads. Free quotas are limited, and usage transitions to paid billing once credits are exhausted.This hybrid approach—open weights paired with a commercial API—mirrors how many enterprises deploy AI today: experimentation and customization in-house, with managed services layered on where operational simplicity matters.Competitive, but philosophically differentQwen-Image-2512 is not positioned as a universal replacement for Gemini 3 Pro Image. Google’s model benefits from deep integration with Vertex AI, Workspace, Ads, and Gemini’s broader reasoning stack. For organizations already committed to Google Cloud, Nano Banana Pro fits naturally into existing pipelines.Qwen’s strategy is more modular. The model integrates cleanly with open tooling and custom orchestration layers, making it attractive to teams building their own AI stacks or combining image generation with internal data systems.A signal to the marketThe release of Qwen-Image-2512 reinforces a broader shift: open-source AI is no longer content to trail proprietary systems by a generation. Instead, it is selectively matching the capabilities that matter most for enterprise deployment—text fidelity, layout control, and realism—while preserving the freedoms enterprises increasingly demand.Google’s Gemini 3 Pro Image raised the ceiling. Qwen-Image-2512 shows that enterprises now have a serious open-source alternative—one that aligns performance with cost control, governance, and deployment choice.",
          "feed_position": 0,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/3nh0udM8PXiBZTLMRhFdC9/6f049ddee65951d098ee322efbf0265d/1767215299__1_.png?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/social-media/instagram-chief-ai-is-so-ubiquitous-it-will-be-more-practical-to-fingerprint-real-media-than-fake-media-202620080.html",
          "published_at": "Wed, 31 Dec 2025 20:26:20 +0000",
          "title": "Instagram chief: AI is so ubiquitous 'it will be more practical to fingerprint real media than fake media'",
          "standfirst": "It's no secret that AI-generated content took over our social media feeds in 2025. Now, Instagram's top exec Adam Mosseri has made it clear that he expects AI content to overtake non-AI imagery and the significant implications that shift has for its creators and photographers.Mosseri shared the thoughts in a lengthy post about the broader trends he expects to shape Instagram in 2026. And he offered a notably candid assessment on how AI is upending the platform. \"Everything that made creators matter—the ability to be real, to connect, to have a voice that couldn’t be faked—is now suddenly accessible to anyone with the right tools,\" he wrote. \"The feeds are starting to fill up with synthetic everything.\"But Mosseri doesn't seem particularly concerned by this shift. He says that there is \"a lot of amazing AI content\" and that the platform may need to rethink its approach to labeling such imagery by \"fingerprinting real media, not just chasing fake.\"From Mosseri (emphasis his):Social media platforms are going to come under increasing pressure to identify and label AI-generated content as such. All the major platforms will do good work identifying AI content, but they will get worse at it over time as AI gets better at imitating reality. There is already a growing number of people who believe, as I do, that it will be more practical to fingerprint real media than fake media. Camera manufacturers could cryptographically sign images at capture, creating a chain of custody.On some level, it's easy to understand how this seems like a more practical approach for Meta. As we've previously reported, technologies that are meant to identify AI content, like watermarks, have proved unreliable at best. They are easy to remove and even easier to ignore altogether. Meta's own labels are far from clear and the company, which has spent tens of billions of dollars on AI this year alone, has admitted it can't reliably detect AI-generated or manipulated content on its platform.That Mosseri is so readily admitting defeat on this issue, though, is telling. AI slop has won. And when it comes to helping Instagram's 3 billion users understand what is real, that should largely be someone else's problem, not Meta's. Camera makers – presumably phone makers and actual camera manufacturers — should come up with their own system that sure sounds a lot like watermarking to \"to verify authenticity at capture.\" Mosseri offers few details about how this would work or be implemented at the scale required to make it feasible.Mosseri also doesn't really address the fact that this is likely to alienate the many photographers and other Instagram creators who have already grown frustrated with the app. The exec regularly fields complaints from the group who want to know why Instagram's algorithm doesn't consistently surface their posts to their on followers.But Mosseri suggests those complaints stem from an outdated vision of what Instagram even is. The feed of \"polished\" square images, he says, \"is dead.\" Camera companies, in his estimation, are \"are betting on the wrong aesthetic\" by trying to \"make everyone look like a professional photographer from the past.\" Instead, he says that more \"raw\" and \"unflattering\" images will be how creators can prove they are real, and not AI. In a world where Instagram has more AI content than not, creators should prioritize images and videos that intentionally make them look bad. This article originally appeared on Engadget at https://www.engadget.com/social-media/instagram-chief-ai-is-so-ubiquitous-it-will-be-more-practical-to-fingerprint-real-media-than-fake-media-202620080.html?src=rss",
          "content": "It's no secret that AI-generated content took over our social media feeds in 2025. Now, Instagram's top exec Adam Mosseri has made it clear that he expects AI content to overtake non-AI imagery and the significant implications that shift has for its creators and photographers.Mosseri shared the thoughts in a lengthy post about the broader trends he expects to shape Instagram in 2026. And he offered a notably candid assessment on how AI is upending the platform. \"Everything that made creators matter—the ability to be real, to connect, to have a voice that couldn’t be faked—is now suddenly accessible to anyone with the right tools,\" he wrote. \"The feeds are starting to fill up with synthetic everything.\"But Mosseri doesn't seem particularly concerned by this shift. He says that there is \"a lot of amazing AI content\" and that the platform may need to rethink its approach to labeling such imagery by \"fingerprinting real media, not just chasing fake.\"From Mosseri (emphasis his):Social media platforms are going to come under increasing pressure to identify and label AI-generated content as such. All the major platforms will do good work identifying AI content, but they will get worse at it over time as AI gets better at imitating reality. There is already a growing number of people who believe, as I do, that it will be more practical to fingerprint real media than fake media. Camera manufacturers could cryptographically sign images at capture, creating a chain of custody.On some level, it's easy to understand how this seems like a more practical approach for Meta. As we've previously reported, technologies that are meant to identify AI content, like watermarks, have proved unreliable at best. They are easy to remove and even easier to ignore altogether. Meta's own labels are far from clear and the company, which has spent tens of billions of dollars on AI this year alone, has admitted it can't reliably detect AI-generated or manipulated content on its platform.That Mosseri is so readily admitting defeat on this issue, though, is telling. AI slop has won. And when it comes to helping Instagram's 3 billion users understand what is real, that should largely be someone else's problem, not Meta's. Camera makers – presumably phone makers and actual camera manufacturers — should come up with their own system that sure sounds a lot like watermarking to \"to verify authenticity at capture.\" Mosseri offers few details about how this would work or be implemented at the scale required to make it feasible.Mosseri also doesn't really address the fact that this is likely to alienate the many photographers and other Instagram creators who have already grown frustrated with the app. The exec regularly fields complaints from the group who want to know why Instagram's algorithm doesn't consistently surface their posts to their on followers.But Mosseri suggests those complaints stem from an outdated vision of what Instagram even is. The feed of \"polished\" square images, he says, \"is dead.\" Camera companies, in his estimation, are \"are betting on the wrong aesthetic\" by trying to \"make everyone look like a professional photographer from the past.\" Instead, he says that more \"raw\" and \"unflattering\" images will be how creators can prove they are real, and not AI. In a world where Instagram has more AI content than not, creators should prioritize images and videos that intentionally make them look bad. This article originally appeared on Engadget at https://www.engadget.com/social-media/instagram-chief-ai-is-so-ubiquitous-it-will-be-more-practical-to-fingerprint-real-media-than-fake-media-202620080.html?src=rss",
          "feed_position": 0
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/mobile/how-to-watch-samsungs-first-look-ces-2026-presentation-190027604.html",
          "published_at": "Wed, 31 Dec 2025 19:00:27 +0000",
          "title": "How to watch Samsung's \"First Look\" CES 2026 presentation",
          "standfirst": "NurPhoto via Getty Images Samsung is arguably the 800-pound gorilla of CES, with a full spectrum of products that range from phones and computers to refrigerators to AI assistants and rolling robots. But for CES 2026, the company is shaking up the schedule a bit: Instead of its longtime midday Monday press conference position, the Korean giant will front-run the entire show with a Sunday night presentation. Samsung has already given a few hints as to what's on the agenda, but what we're really hoping to see is an update on the Ballie robot — a star of previous CES presentations that ostensibly missed its previously promised 2025 release date. How to watch Samsung's \"The First Look\" presentation at CES 2026 The event will stream live from the Wynn Hotel in Las Vegas on Sunday, January 4 at 10PM ET. There are several ways to tune in: you can watch via the Samsung Newsroom, Samsung Electronics’ official YouTube channel or via Samsung TV Plus. (We'll embed the stream here once it appears on the channel.) What to expect from Samsung at CES 2026 Keynote speaker TM Roh, the CEO of Samsung's Device eXperience (DX) Division, will discuss the company's plans for the new year and beyond, which will (of course) include \"new AI-driven customer experiences,\" the company said in a press release. In addition, we'll hear from the President and Head of the Visual Display Business, SW Yong and Executive Vice President and Head of Digital Appliances Business, Cheolgi Kim. Those two will \"share their respective business directions for the upcoming year.\" But if you're looking for more specifics, Samsung is following its \"Advent calendar\" approach to early CES announcements, with new press releases dropping nearly each day. So far, we know that — like competitors LG and Hisense — the company will be offering details on a line of micro RGB TVs (replete with confirmed screen sizes of 55 to 115 inches). Also confirmed: a full line of appliances infused with what Samsung calls Bespoke AI. It's likely Samsung will map out its CES plans in greater detail as the January 4 event approaches, so we'll update this story accordingly when it does.This article originally appeared on Engadget at https://www.engadget.com/mobile/how-to-watch-samsungs-first-look-ces-2026-presentation-190027604.html?src=rss",
          "content": "NurPhoto via Getty Images Samsung is arguably the 800-pound gorilla of CES, with a full spectrum of products that range from phones and computers to refrigerators to AI assistants and rolling robots. But for CES 2026, the company is shaking up the schedule a bit: Instead of its longtime midday Monday press conference position, the Korean giant will front-run the entire show with a Sunday night presentation. Samsung has already given a few hints as to what's on the agenda, but what we're really hoping to see is an update on the Ballie robot — a star of previous CES presentations that ostensibly missed its previously promised 2025 release date. How to watch Samsung's \"The First Look\" presentation at CES 2026 The event will stream live from the Wynn Hotel in Las Vegas on Sunday, January 4 at 10PM ET. There are several ways to tune in: you can watch via the Samsung Newsroom, Samsung Electronics’ official YouTube channel or via Samsung TV Plus. (We'll embed the stream here once it appears on the channel.) What to expect from Samsung at CES 2026 Keynote speaker TM Roh, the CEO of Samsung's Device eXperience (DX) Division, will discuss the company's plans for the new year and beyond, which will (of course) include \"new AI-driven customer experiences,\" the company said in a press release. In addition, we'll hear from the President and Head of the Visual Display Business, SW Yong and Executive Vice President and Head of Digital Appliances Business, Cheolgi Kim. Those two will \"share their respective business directions for the upcoming year.\" But if you're looking for more specifics, Samsung is following its \"Advent calendar\" approach to early CES announcements, with new press releases dropping nearly each day. So far, we know that — like competitors LG and Hisense — the company will be offering details on a line of micro RGB TVs (replete with confirmed screen sizes of 55 to 115 inches). Also confirmed: a full line of appliances infused with what Samsung calls Bespoke AI. It's likely Samsung will map out its CES plans in greater detail as the January 4 event approaches, so we'll update this story accordingly when it does.This article originally appeared on Engadget at https://www.engadget.com/mobile/how-to-watch-samsungs-first-look-ces-2026-presentation-190027604.html?src=rss",
          "feed_position": 1,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2025-12/54afdb10-dd18-11f0-b2f7-d2b3086683ec"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/transportation/evs/how-to-watch-the-sony-honda-afeela-ces-2026-press-conference-130048622.html",
          "published_at": "Wed, 31 Dec 2025 18:42:16 +0000",
          "title": "How to watch the Sony Honda Afeela CES 2026 press conference",
          "standfirst": "Sony's CES 2026 press conference is just days away, but this year has a twist: Instead of an overview of Sony's electronics, video game and Hollywood studio plans for the new year, the presser will apparently have a more narrow focus: electric vehicles. That's because the traditional end-of-press-day slot isn't just Sony, but rather Sony Honda Mobility — the joint venture responsible for the Afeela 1 electric car that has been showcased at CES for the better part of a decade. This year, however, we'll also apparently get to see a an all-new concept model at the event, too. How to watch the Sony Afeela CES 2026 press conference The event will be streamed live from Las Vegas on Monday, January 5 at 8PM ET via the Afeela official YouTube channel. (We'll embed the stream here once it appears on the channel.) What to expect from Sony Afeela at CES What's now the Afeela 1 has been shown in various incarnations since CES 2020, where it was originally announced as Vision-S. But so many more important details were confirmed in the past couple of years, including the price, which starts at a staggering $89,900. However, the earlier impressions were less than impressive, and as of CES 2025, that thought remains the same. Engadget's automotive expert Tim Stevens said earlier this year that the EV \"feels like a PlayStation 4 in the PS5 era,\" and that \"the car lost what little interesting styling it had while sticking true to some specifications that sounded good five years ago.\" Ouch. But the Afeela 1 won't be the only vehicle on display. Its CES booth will showcase \"several Afeela 1 pre-production vehicles in multiple color variations, alongside a new Afeela concept model,\" Sony Honda Mobility said in a press release. We're hoping to hear about what's new and improved at CES 2026, and we're also excited to see its newest concept model. And between booth displays and press releases, we're hoping we'll get to see at least a few new Sony Electronics products on the docket for 2026, too.This article originally appeared on Engadget at https://www.engadget.com/transportation/evs/how-to-watch-the-sony-honda-afeela-ces-2026-press-conference-130048622.html?src=rss",
          "content": "Sony's CES 2026 press conference is just days away, but this year has a twist: Instead of an overview of Sony's electronics, video game and Hollywood studio plans for the new year, the presser will apparently have a more narrow focus: electric vehicles. That's because the traditional end-of-press-day slot isn't just Sony, but rather Sony Honda Mobility — the joint venture responsible for the Afeela 1 electric car that has been showcased at CES for the better part of a decade. This year, however, we'll also apparently get to see a an all-new concept model at the event, too. How to watch the Sony Afeela CES 2026 press conference The event will be streamed live from Las Vegas on Monday, January 5 at 8PM ET via the Afeela official YouTube channel. (We'll embed the stream here once it appears on the channel.) What to expect from Sony Afeela at CES What's now the Afeela 1 has been shown in various incarnations since CES 2020, where it was originally announced as Vision-S. But so many more important details were confirmed in the past couple of years, including the price, which starts at a staggering $89,900. However, the earlier impressions were less than impressive, and as of CES 2025, that thought remains the same. Engadget's automotive expert Tim Stevens said earlier this year that the EV \"feels like a PlayStation 4 in the PS5 era,\" and that \"the car lost what little interesting styling it had while sticking true to some specifications that sounded good five years ago.\" Ouch. But the Afeela 1 won't be the only vehicle on display. Its CES booth will showcase \"several Afeela 1 pre-production vehicles in multiple color variations, alongside a new Afeela concept model,\" Sony Honda Mobility said in a press release. We're hoping to hear about what's new and improved at CES 2026, and we're also excited to see its newest concept model. And between booth displays and press releases, we're hoping we'll get to see at least a few new Sony Electronics products on the docket for 2026, too.This article originally appeared on Engadget at https://www.engadget.com/transportation/evs/how-to-watch-the-sony-honda-afeela-ces-2026-press-conference-130048622.html?src=rss",
          "feed_position": 2,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2025-12/3f099890-d9ef-11f0-a3f3-bb8a02df8254"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/home/how-to-watch-the-lg-press-conference-at-ces-2026-190159474.html",
          "published_at": "Wed, 31 Dec 2025 17:37:17 +0000",
          "title": "How to watch the LG press conference at CES 2026",
          "standfirst": "LG For years, LG has kicked off CES press day with the first event of the morning — and 2026 will be no different. The Korea-based corporation is theming its presentation as \"Innovation in Tune with You,\" and — if it follows the template of past presentations — it will highlight both the consumer electronics and large appliance sides of its mammoth global businesses. Like nearly all tech-centric events these days, expect AI to be the binding theme of the LG presentation at CES 2026. Just be aware that, like Apple, LG has its own customized abbreviation for AI: \"Affectionate Intelligence.\" The company will share \"its vision for elevating daily life through Affectionate Intelligence — delivering harmonized and seamlessly connected customer experiences.\" In other words, the company is aiming for its devices to become more connected and self-automated than ever. Here's how you can stream it and what you can expect. How to watch LG's CES 2026 presentation The event will stream live from Las Vegas on Monday, January 5 at 11AM ET. You've got a few options for tuning in — watch the livestream on the LG website, the LG Global X channel or the LG Global YouTube channel (embedded below). What to expect Here's what LG has already confirmed it will be showcasing at CES 2026: LG will debut its first Micro RGB TV, a display with a cutting-edge screen technology with multicolor backlights that should one-up mini LED displays. The size options are 100 inches, 86 inches and 75 inches. The company is countering Samsung's Frame TVs with its new LG Gallery TV, arriving in 55- and 65-inch screen sizes. Look for a new LG humanoid home automation robot named CLOiD to take the stage. In the audio realm, the Korean multinational will also introduce a Dolby-powered modular home audio system and a new line of its xboom speakers (developed with musician will.i.am). Does that leave any surprises for the CES press conference? We'll find out on January 5. Update, December 31 2025, 12:36PM ET: This story has been updated to include more LG CES pre-announcements, and to embed the YouTube stream. This article originally appeared on Engadget at https://www.engadget.com/home/how-to-watch-the-lg-press-conference-at-ces-2026-190159474.html?src=rss",
          "content": "LG For years, LG has kicked off CES press day with the first event of the morning — and 2026 will be no different. The Korea-based corporation is theming its presentation as \"Innovation in Tune with You,\" and — if it follows the template of past presentations — it will highlight both the consumer electronics and large appliance sides of its mammoth global businesses. Like nearly all tech-centric events these days, expect AI to be the binding theme of the LG presentation at CES 2026. Just be aware that, like Apple, LG has its own customized abbreviation for AI: \"Affectionate Intelligence.\" The company will share \"its vision for elevating daily life through Affectionate Intelligence — delivering harmonized and seamlessly connected customer experiences.\" In other words, the company is aiming for its devices to become more connected and self-automated than ever. Here's how you can stream it and what you can expect. How to watch LG's CES 2026 presentation The event will stream live from Las Vegas on Monday, January 5 at 11AM ET. You've got a few options for tuning in — watch the livestream on the LG website, the LG Global X channel or the LG Global YouTube channel (embedded below). What to expect Here's what LG has already confirmed it will be showcasing at CES 2026: LG will debut its first Micro RGB TV, a display with a cutting-edge screen technology with multicolor backlights that should one-up mini LED displays. The size options are 100 inches, 86 inches and 75 inches. The company is countering Samsung's Frame TVs with its new LG Gallery TV, arriving in 55- and 65-inch screen sizes. Look for a new LG humanoid home automation robot named CLOiD to take the stage. In the audio realm, the Korean multinational will also introduce a Dolby-powered modular home audio system and a new line of its xboom speakers (developed with musician will.i.am). Does that leave any surprises for the CES press conference? We'll find out on January 5. Update, December 31 2025, 12:36PM ET: This story has been updated to include more LG CES pre-announcements, and to embed the YouTube stream. This article originally appeared on Engadget at https://www.engadget.com/home/how-to-watch-the-lg-press-conference-at-ces-2026-190159474.html?src=rss",
          "feed_position": 4,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2025-12/6f089c60-db4f-11f0-ab9d-5c52fd4922f7"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/big-tech/ces-2026-what-to-expect-when-techs-biggest-conference-starts-on-january-4-120000189.html",
          "published_at": "Wed, 31 Dec 2025 17:05:00 +0000",
          "title": "CES 2026: What to expect when tech's biggest conference starts on January 4",
          "standfirst": "The new year is upon us, and that means CES 2026 is imminent. The biggest tech trade show of the calendar comes with a bevy of new and notable announcements that set the tone for trends and expectations for the subsequent 12 months. The CES 2026 show floor is officially open from January 6 through 9, but the fun kicks off with events on Sunday January 4, followed by a host of press conferences on Monday. As always, product demos, announcements and networking will be happening at the Las Vegas Convention Center and other hotels all over the city. As usual, Engadget will be covering the event in-person and remotely, bringing you news and hands-ons straight from the show floor.More specific details and pre-announcements are already trickling out as CES approaches, and thanks to the schedule of the Consumer Technology Association (the trade organization that runs the show) we have a full itinerary of press conferences. We’re also using our experience and expertise to predict what tech trends could rear their heads at the show.The CES 2026 schedulePress conferences and show floor booths are the bread and butter of CES. The CTA has already published a searchable directory of who will have an official presence at the show, along with a schedule of every official panel and presentation. However, the press conference schedule gives us a more digestible rundown of the first 48 hours of big events. On Sunday, January 4, Samsung will kick-off CES with \"The First Look,\" a presentation hosted by TM Roh, the CEO of Samsung's DX Division, on the company's \"vision for the DX (Device eXperience) Division in 2026, along with new AI-driven customer experiences.\" Ahead of that, though, Samsung has already outlined a variety of more specifics (scroll down for details). Concurrent with the Samsung presentation will be the official CES Unveiled mini-show, which is generally comprised of smaller and start-up vendors. That'll be followed by multiple press conferences throughout Monday, January 5. The LG CES 2026 press conference, titled \"Innovation in Tune with You,\" is ostensibly to share \"its vision for elevating daily life through Affectionate Intelligence.\" But, like Samsung, this fellow Korean giant has already spent the three weeks leading up to CES pre-announcing many of its new products, so this may be more of a summary than breaking news. Following LG, we’ll also see press conferences from Bosch and Hisense, as well as the first-ever CES appearance from Lego. As the Las Vegas afternoon rolls around, we get the first of three chip giants: NVIDIA CEO Jensen Huang takes the stage on January 5 at 1PM PT (4PM ET) and, according to the website, his presentation will last about 90 minutes. Based on the description on the listing, the presentation will “showcase the latest NVIDIA solutions driving innovation and productivity across industries.” NVIDIA’s presser is concurrent with one from Hyundai, where the Korean automotive company will focus on in-cabin car tech and robotics. Later in the day, we get to hear from NVIDIA frenemies Intel and AMD. Intel’s 3PM PT (6PM PT) event will ostensibly feature its new Core Ultra Series 3 processors, and AMD CEO Lisa Su will cover AMD's upcoming chip announcements at a keynote address that closes out the day. But expect both of them to be very heavy on AI applications, of course. Sandwiched in between those chip manufacturers will be Sony Honda Mobility. The joint venture will be offering yet more details on its Afeela EV. Finally, on Tuesday, January 6, Lenovo CEO Yuanqing Yang will host Lenovo's Tech World Conference at the Las Vegas Sphere, using the large and decidedly curved screen to share the company's \"commitment to delivering smarter AI for all by constantly redefining how technology can engage, inspire, and empower.\" It’s worth noting that Lenovo is the parent company of Motorola, which still makes phones and foldables that feature AI tools, so it’s possible those devices feature in the presentation as well. Samsung and LG vie for pre-show publicityAs noted above, both Samsung and LG have continued their recent trend of spoiling nearly all of their respective CES announcements in the days and weeks before the show. LG, for example, has said it will debut its first Micro RGB television at CES. While details are scarce, the company’s press release for the LG Micro RGB evo did confirm it has received certifications by Intertek for 100 percent color gamut coverage in DCI-P3 an Adobe RGB, and that it has more than a thousand dimming zones for brightness control. Elsewhere in the TV space, LG is throwing its hat into the “art TV” ring that Samsung pioneered with its Frame TVs: The LG Gallery TV will debut in 55- and 65-inch screen sizes, and it will of course show off various artwork when it’s not otherwise in use. And if PC gaming displays are more your speed, LG will have that covered, too, with a new line of 5K-capable gaming monitors on deck with built-in AI upscaling.But LG’s not just showing off displays. The Korean multinational will also introduce a Dolby-powered modular home audio system, a new line of its xboom speakers (developed with will.i.am) and the company will flex its automation muscles with a humanoid home automation robot named CLOiD. Of course, Samsung refuses to be outdone by its hometown rival, and has also released a pre-CES press release document dump. Samsung will be launching its own lineup of Micro RGB TVs at CES, for starters. The company already introduced its first Micro RGB TV at CES 2025, which was a 115-inch model available for a cool $30,000. Next year, Samsung is expanding the range with 55-, 65-, 75-, 85-, 100- and 115-inch models that use the next evolution of the company’s Micro RGB technology. Samsung is also countering LG’s 5K monitors with a 6K model that aims to deliver glasses-free 3D (another long-time CES staple). It’ll be one of several new displays in the company’s Odyssey gaming line.And on the audio front, Samsung has teased several new soundbars and speakers, including Sonos-style Wi-Fi streaming models call the Music Studio 5 and Studio 7.Outside of the formal introduction of new products and initiatives, reading the tea leaves of what was announced last year and what companies are reportedly working on, we can make some educated guesses at what we could see at CES 2026.New chips from AMD, Intel and QualcommCES is frequently the start of a cascade of new chip announcements for a given year, and one of the first places new silicon appears in real consumer products. AMD will likely use its keynote to introduce new versions of its Ryzen chips, including the recently spotted Ryzen 7 9850X3D, which is expected to offer better single-threaded performance, and the Ryzen 9000G series, which could be built with AMD's Zen 5 architecture. The company might also use its CES stage to go over its new FSR Redstone AI upscaling tech.Intel has already publicly announced that it'll launch its Panther Lake chips at CES 2026. The officially titled Intel Core Ultra Series 3 chips fit into Intel's overall \"AI PC\" push, but are specifically meant for premium laptops. Based on a preview from October 2025, Intel says the first chip made with its 2-nanometer 18A process will offer 50 percent more processing performance than previous generations and for the chip's Arc GPU, a 50 percent performance bump from last generation.Qualcomm is also rumored to be targeting laptops at the show, building on the work it's done moving its Snapdragon chips out of phones and tablets and into other types of computers. The company's Snapdragon X2 Elite and X2 Elite Premium chips should start appearing in laptops at CES 2026, offering a look at the improved speed and AI performance the company promised in 2025.Brighter, \"truer\" screensAs noted above, Samsung and LG appear to be going all-in on Micro RGB display tech for TVs. Expect that to be a huge buzzword at CES, with Hisense and Sony debuting new models, too.Sony announced a collection of new Bravia TVs in April 2025, replacing the company's flagship, filling in its midrange options and adding a new budget model to the mix. The star of this updated Bravia lineup is the Bravia 9, which features a QD-OLED panel, but Sony appears to be prepping entirely new display tech for 2026. In March 2025, Sony introduced a new RGB LED panel that uses individual Mini LED backlights colored in red, green and blue to produce even brighter, more accurate colors. In contrast to a QD-OLED, which filters a layer of blue organic light emitting diodes through quantum dots that change color, Sony's \"General RGB LED Backlight Technology\" can get as bright as a Mini LED panel without needing an extra filter layer or worrying about OLED's problems with burn-in. The company has already trademarked the name \"True RGB,\" which could end up being what Sony calls this new flavor of display if it decides to show them off at CES. It seems entirely likely, because CES is nothing if not a TV show — it’s a sure bet that we’ll see new TVs from the likes of LG and Samsung in addition to Sony. If the company doesn't introduce new display tech for its TVs, it does have a new 240Hz PlayStation monitor coming in 2026 that it could show off at CES instead.Sony isn't the only company hyped on bright screens. Samsung is reportedly pushing an updated version of the HDR10 and HDR10+ standards that could be ready to demo at CES 2026. The new HDR10+ Advanced standard would be Samsung's answer to Dolby Vision 2, which includes support for things bi-directional tone mapping and intelligent features that automatically adapt sports and gaming content. Samsung's take will reportedly offer improved brightness, genre-based tone mapping and intelligent motion smoothing options, among other improvements.And maybe your future TV won’t need a power cord, either: Displace will be showing off a mounting option that includes a 15,000mAh battery to juice up whatever giant TV screen you choose to attach.Ballie Watch 2026The ball-shaped yellow robot lovingly known as \"Ballie\" has been announced twice, first in 2020 and then again in 2024 with a projector in tow. Samsung said Ballie would go on sale in 2025 at CES last year and then shared in April 2025 that Ballie would ship this summer with Google's Gemini onboard. But it's nearly 2026, and Ballie is nowhere to be seen. It's possible Samsung could make a third attempt at announcing its robot at CES 2026, but whether or not it does, robotics will still be a big part of the show.Robot vacuums and mops were a major highlight of CES 2025, and it's safe to expect notable improvements from the new models that are announced at CES 2026. Not every company will adopt the retractable arm of the Roborock Saros Z70, but robot vacuums with legs for rising over small ledges like the Dreame X50 seem like they could become the norm. Roborock could also show off its new Roborock Qrevo Curv 2 Flow, the first of its robot vacuums to feature a retractable roller mop.Beyond just traversing spaces more efficiently, improving robots' navigation could also be a major concern at the show. Prominent members of the AI industry are turning their attention from large language models to world models, which aim to give AI a deep understanding of physical space. Those world models could be the key to making robots — like LG’s aforementioned CLOiD — competent at navigating homes and workplaces, and will likely be a significant talking point at CES 2026.We’ll be updating this article throughout the month as more rumors surface and new products are confirmed — stay tuned for future updates!Update, December 11 2025, 11:03AM ET: This story has been updated to include detail on Lenovo being Motorola’s parent company and how the latter might have a part in the Tuesday presentation.Update, December 16 2025, 1:33PM ET: This story has been updated to include the NVIDIA press conference, which was added to the CTA schedule within the last two days.Update, December 23 2025, 7:28AM ET: This story has been updated to include LG and Samsung’s Micro RGB TV announcements, which were made public in the past seven days. The intro was also tweaked to reflect how soon CES is at this point.Update, December 29 2025, 11:03AM ET: This story has been updated to include additional details on pre-announcements from Samsung, LG and Displace. Update, December 31 2025, 12:05PM ET: This story has been updated to include yet more early LG announcements.This article originally appeared on Engadget at https://www.engadget.com/big-tech/ces-2026-what-to-expect-when-techs-biggest-conference-starts-on-january-4-120000189.html?src=rss",
          "content": "The new year is upon us, and that means CES 2026 is imminent. The biggest tech trade show of the calendar comes with a bevy of new and notable announcements that set the tone for trends and expectations for the subsequent 12 months. The CES 2026 show floor is officially open from January 6 through 9, but the fun kicks off with events on Sunday January 4, followed by a host of press conferences on Monday. As always, product demos, announcements and networking will be happening at the Las Vegas Convention Center and other hotels all over the city. As usual, Engadget will be covering the event in-person and remotely, bringing you news and hands-ons straight from the show floor.More specific details and pre-announcements are already trickling out as CES approaches, and thanks to the schedule of the Consumer Technology Association (the trade organization that runs the show) we have a full itinerary of press conferences. We’re also using our experience and expertise to predict what tech trends could rear their heads at the show.The CES 2026 schedulePress conferences and show floor booths are the bread and butter of CES. The CTA has already published a searchable directory of who will have an official presence at the show, along with a schedule of every official panel and presentation. However, the press conference schedule gives us a more digestible rundown of the first 48 hours of big events. On Sunday, January 4, Samsung will kick-off CES with \"The First Look,\" a presentation hosted by TM Roh, the CEO of Samsung's DX Division, on the company's \"vision for the DX (Device eXperience) Division in 2026, along with new AI-driven customer experiences.\" Ahead of that, though, Samsung has already outlined a variety of more specifics (scroll down for details). Concurrent with the Samsung presentation will be the official CES Unveiled mini-show, which is generally comprised of smaller and start-up vendors. That'll be followed by multiple press conferences throughout Monday, January 5. The LG CES 2026 press conference, titled \"Innovation in Tune with You,\" is ostensibly to share \"its vision for elevating daily life through Affectionate Intelligence.\" But, like Samsung, this fellow Korean giant has already spent the three weeks leading up to CES pre-announcing many of its new products, so this may be more of a summary than breaking news. Following LG, we’ll also see press conferences from Bosch and Hisense, as well as the first-ever CES appearance from Lego. As the Las Vegas afternoon rolls around, we get the first of three chip giants: NVIDIA CEO Jensen Huang takes the stage on January 5 at 1PM PT (4PM ET) and, according to the website, his presentation will last about 90 minutes. Based on the description on the listing, the presentation will “showcase the latest NVIDIA solutions driving innovation and productivity across industries.” NVIDIA’s presser is concurrent with one from Hyundai, where the Korean automotive company will focus on in-cabin car tech and robotics. Later in the day, we get to hear from NVIDIA frenemies Intel and AMD. Intel’s 3PM PT (6PM PT) event will ostensibly feature its new Core Ultra Series 3 processors, and AMD CEO Lisa Su will cover AMD's upcoming chip announcements at a keynote address that closes out the day. But expect both of them to be very heavy on AI applications, of course. Sandwiched in between those chip manufacturers will be Sony Honda Mobility. The joint venture will be offering yet more details on its Afeela EV. Finally, on Tuesday, January 6, Lenovo CEO Yuanqing Yang will host Lenovo's Tech World Conference at the Las Vegas Sphere, using the large and decidedly curved screen to share the company's \"commitment to delivering smarter AI for all by constantly redefining how technology can engage, inspire, and empower.\" It’s worth noting that Lenovo is the parent company of Motorola, which still makes phones and foldables that feature AI tools, so it’s possible those devices feature in the presentation as well. Samsung and LG vie for pre-show publicityAs noted above, both Samsung and LG have continued their recent trend of spoiling nearly all of their respective CES announcements in the days and weeks before the show. LG, for example, has said it will debut its first Micro RGB television at CES. While details are scarce, the company’s press release for the LG Micro RGB evo did confirm it has received certifications by Intertek for 100 percent color gamut coverage in DCI-P3 an Adobe RGB, and that it has more than a thousand dimming zones for brightness control. Elsewhere in the TV space, LG is throwing its hat into the “art TV” ring that Samsung pioneered with its Frame TVs: The LG Gallery TV will debut in 55- and 65-inch screen sizes, and it will of course show off various artwork when it’s not otherwise in use. And if PC gaming displays are more your speed, LG will have that covered, too, with a new line of 5K-capable gaming monitors on deck with built-in AI upscaling.But LG’s not just showing off displays. The Korean multinational will also introduce a Dolby-powered modular home audio system, a new line of its xboom speakers (developed with will.i.am) and the company will flex its automation muscles with a humanoid home automation robot named CLOiD. Of course, Samsung refuses to be outdone by its hometown rival, and has also released a pre-CES press release document dump. Samsung will be launching its own lineup of Micro RGB TVs at CES, for starters. The company already introduced its first Micro RGB TV at CES 2025, which was a 115-inch model available for a cool $30,000. Next year, Samsung is expanding the range with 55-, 65-, 75-, 85-, 100- and 115-inch models that use the next evolution of the company’s Micro RGB technology. Samsung is also countering LG’s 5K monitors with a 6K model that aims to deliver glasses-free 3D (another long-time CES staple). It’ll be one of several new displays in the company’s Odyssey gaming line.And on the audio front, Samsung has teased several new soundbars and speakers, including Sonos-style Wi-Fi streaming models call the Music Studio 5 and Studio 7.Outside of the formal introduction of new products and initiatives, reading the tea leaves of what was announced last year and what companies are reportedly working on, we can make some educated guesses at what we could see at CES 2026.New chips from AMD, Intel and QualcommCES is frequently the start of a cascade of new chip announcements for a given year, and one of the first places new silicon appears in real consumer products. AMD will likely use its keynote to introduce new versions of its Ryzen chips, including the recently spotted Ryzen 7 9850X3D, which is expected to offer better single-threaded performance, and the Ryzen 9000G series, which could be built with AMD's Zen 5 architecture. The company might also use its CES stage to go over its new FSR Redstone AI upscaling tech.Intel has already publicly announced that it'll launch its Panther Lake chips at CES 2026. The officially titled Intel Core Ultra Series 3 chips fit into Intel's overall \"AI PC\" push, but are specifically meant for premium laptops. Based on a preview from October 2025, Intel says the first chip made with its 2-nanometer 18A process will offer 50 percent more processing performance than previous generations and for the chip's Arc GPU, a 50 percent performance bump from last generation.Qualcomm is also rumored to be targeting laptops at the show, building on the work it's done moving its Snapdragon chips out of phones and tablets and into other types of computers. The company's Snapdragon X2 Elite and X2 Elite Premium chips should start appearing in laptops at CES 2026, offering a look at the improved speed and AI performance the company promised in 2025.Brighter, \"truer\" screensAs noted above, Samsung and LG appear to be going all-in on Micro RGB display tech for TVs. Expect that to be a huge buzzword at CES, with Hisense and Sony debuting new models, too.Sony announced a collection of new Bravia TVs in April 2025, replacing the company's flagship, filling in its midrange options and adding a new budget model to the mix. The star of this updated Bravia lineup is the Bravia 9, which features a QD-OLED panel, but Sony appears to be prepping entirely new display tech for 2026. In March 2025, Sony introduced a new RGB LED panel that uses individual Mini LED backlights colored in red, green and blue to produce even brighter, more accurate colors. In contrast to a QD-OLED, which filters a layer of blue organic light emitting diodes through quantum dots that change color, Sony's \"General RGB LED Backlight Technology\" can get as bright as a Mini LED panel without needing an extra filter layer or worrying about OLED's problems with burn-in. The company has already trademarked the name \"True RGB,\" which could end up being what Sony calls this new flavor of display if it decides to show them off at CES. It seems entirely likely, because CES is nothing if not a TV show — it’s a sure bet that we’ll see new TVs from the likes of LG and Samsung in addition to Sony. If the company doesn't introduce new display tech for its TVs, it does have a new 240Hz PlayStation monitor coming in 2026 that it could show off at CES instead.Sony isn't the only company hyped on bright screens. Samsung is reportedly pushing an updated version of the HDR10 and HDR10+ standards that could be ready to demo at CES 2026. The new HDR10+ Advanced standard would be Samsung's answer to Dolby Vision 2, which includes support for things bi-directional tone mapping and intelligent features that automatically adapt sports and gaming content. Samsung's take will reportedly offer improved brightness, genre-based tone mapping and intelligent motion smoothing options, among other improvements.And maybe your future TV won’t need a power cord, either: Displace will be showing off a mounting option that includes a 15,000mAh battery to juice up whatever giant TV screen you choose to attach.Ballie Watch 2026The ball-shaped yellow robot lovingly known as \"Ballie\" has been announced twice, first in 2020 and then again in 2024 with a projector in tow. Samsung said Ballie would go on sale in 2025 at CES last year and then shared in April 2025 that Ballie would ship this summer with Google's Gemini onboard. But it's nearly 2026, and Ballie is nowhere to be seen. It's possible Samsung could make a third attempt at announcing its robot at CES 2026, but whether or not it does, robotics will still be a big part of the show.Robot vacuums and mops were a major highlight of CES 2025, and it's safe to expect notable improvements from the new models that are announced at CES 2026. Not every company will adopt the retractable arm of the Roborock Saros Z70, but robot vacuums with legs for rising over small ledges like the Dreame X50 seem like they could become the norm. Roborock could also show off its new Roborock Qrevo Curv 2 Flow, the first of its robot vacuums to feature a retractable roller mop.Beyond just traversing spaces more efficiently, improving robots' navigation could also be a major concern at the show. Prominent members of the AI industry are turning their attention from large language models to world models, which aim to give AI a deep understanding of physical space. Those world models could be the key to making robots — like LG’s aforementioned CLOiD — competent at navigating homes and workplaces, and will likely be a significant talking point at CES 2026.We’ll be updating this article throughout the month as more rumors surface and new products are confirmed — stay tuned for future updates!Update, December 11 2025, 11:03AM ET: This story has been updated to include detail on Lenovo being Motorola’s parent company and how the latter might have a part in the Tuesday presentation.Update, December 16 2025, 1:33PM ET: This story has been updated to include the NVIDIA press conference, which was added to the CTA schedule within the last two days.Update, December 23 2025, 7:28AM ET: This story has been updated to include LG and Samsung’s Micro RGB TV announcements, which were made public in the past seven days. The intro was also tweaked to reflect how soon CES is at this point.Update, December 29 2025, 11:03AM ET: This story has been updated to include additional details on pre-announcements from Samsung, LG and Displace. Update, December 31 2025, 12:05PM ET: This story has been updated to include yet more early LG announcements.This article originally appeared on Engadget at https://www.engadget.com/big-tech/ces-2026-what-to-expect-when-techs-biggest-conference-starts-on-january-4-120000189.html?src=rss",
          "feed_position": 5
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/cybersecurity/vpn/how-to-use-a-vpn-140000564.html",
          "published_at": "Wed, 31 Dec 2025 14:00:00 +0000",
          "title": "How to use a VPN",
          "standfirst": "Chances are that if a VPN is marketed to everyday users, it more or less runs itself. As long as you go with one of the best VPNs instead of setting up your own manual connection, the interface is likely built so you have to engage with it as little as possible. Generally, if you find yourself constantly thinking about your VPN while using it — as happened to me with Norton VPN — that's a bad thing and you should change providers.That said, it's still useful to get a high-level view of how to use a VPN, especially if you've never used one before. In this article, I'll walk you through how to download, install and configure a VPN on any platform and how to choose servers for specific situations. I'll also discuss specific steps for desktop and laptop computers, phones, tablets, smart TVs, game consoles and routers.How to download and install a VPNThere are over 100 VPNs available on the internet and the two big mobile app stores, but most of them follow a similar approach to download and installation. Except for certain free VPNs, you'll need to create an account on the VPN provider's website, install the VPN client, then use your account to log in. Here are the steps.In a web browser, navigate to a VPN provider's website. For suggestions on where to start, check out our list of the best VPN deals. Click any link that mentions getting the VPN or claiming the deal — as a rule, they aren't hard to find.Select a plan based on the length of time you need the VPN for, the features you want and the number of devices you'd like to use the VPN on simultaneously. Add a payment method. You'll be prompted to create the username and password for a new account.On desktop, you can start with the Mac or Windows app store, though some VPNs require a download directly from their respective website. That’s generally the way to get the most feature-rich version of the VPN, but just make sure you’re downloading from the provider’s website to ensure you don’t get a malicious copycat. On mobile, the app store is your only choice, but don't forget to check the reviews and make sure you're getting the service you paid for — unfortunately, rival (and sometimes less scrupulous) VPN brands can use paid placements to appear higher on search results, even for a rival product. Follow the pre-programmed installation flow. Again, don't let a VPN make any changes to your system if you haven't thoroughly vetted the provider (or gone with one of our recommendations). If it's safe, allow the VPN all the setup permissions it needs.Finally, open the VPN client and sign in with the credentials you created in step two. You're now ready to start using your new VPN.Surfshark in Mac downloads folderSam Chapman for EngadgetWith so many VPNs on the market, this process won't always be the same. Sometimes you'll have to make the account before paying, and some mobile VPNs let you handle the whole process in-app. What's always true, though, is that a good VPN will make the signup flow as easy as possible. If you have trouble before you even log in, that's a red flag.How to configure a VPN appMost VPNs are ready to go out of the box. However, even if you don't think you're a power user, it's a good idea to go through the settings and make sure they match what you need. Here are a few steps I recommend taking.Activate the kill switch. This feature monitors your connection to the VPN server. If it ever drops — either from problems in the server, your client or somewhere in between — the kill switch instantly cuts off your internet until your VPN connection is restored. This saves you from accidentally broadcasting anything sensitive while also ensuring you only connect to legitimate VPN servers (avoiding the Tunnelvision bug).Set up favorites and profiles. VPNs usually let you select servers you'll use regularly, websites you'll often connect to through the VPN and other preferences that will appear immediately when you open the app. Some go further, like Proton VPN, which lets you create user profiles that bundle several preferences together.Decide if you want the VPN to automatically launch and connect when you start up your computer.Check on split tunneling. There may be cases where you want a certain app or site to work outside the VPN, or where you want everything to go unencrypted except one site or app. You can set up a split tunnel when you need it, but if there's one you know you'll use regularly, it helps to build it early.Activate or deactivate any content blockers the VPN comes with, including ad blockers, malware detectors and parental controls, as you see fit.Decide whether you want the VPN to send you notifications, and how.Select a VPN protocol. It's almost always best to let the VPN choose for itself, but it's helpful to at least know where the protocol-switching option is. Occasionally, if one protocol isn't working, you can troubleshoot by selecting another.You can do all this by digging into your VPN's preferences control panel. A gear seems to be the universal icon for that, but Mac users may also find it by picking \"preferences\" or \"settings\" from the menu bar. When you find the preferences menu, go through each tab in turn and make sure everything is the way you like it.How to choose a VPN serverPicking a server location is the final box to check before connecting. You nearly always have the option of letting the VPN pick the best server. Most apps tend to determine the best server with a latency test, so it will almost always be a node very close to your physical location.If you just want anonymity online, that's fine — it doesn't matter which IP address you use as long as it's not your real one. But several VPN use cases do require a specific server location. For those, follow one simple rule: Pick a server in the place you want your signal to come from.Choosing locations on SurfsharkSam Chapman for EngadgetFor example, let's say you want to watch The Office, but you're only subscribed to Netflix and not Peacock. Luckily, Netflix is licensed to show The Office in the U.K. If you connect to a British server location before opening Netflix, you'll be able to access the show without paying for an extra subscription. (Of course, make sure VPNs aren’t expressly banned by the streaming service’s terms of service before accessing it while using one.)Other than streaming, the most common reason you'll need a specific VPN server is to get around firewalls that block websites. Whether it's your school doing the censorship, your workplace or your entire state, the solution is the same: Pick a VPN server outside the restricted region. Censorship systems filter by location — they can't block where they don't have jurisdiction. Get a new virtual location and you should be free and clear. (Always be cognizant of the laws on VPN usage in your location before activating them.)When to use a VPN (and when not to)It's good to get into the habit of connecting to your VPN whenever you get online. You can never be sure what information your ISP is gathering on you. If you're using unprotected Wi-Fi, or a public network with a clearly visible password, anybody might be listening in. Even if you don't need a particular location, always be using your VPN.The only reason you might want to be online without a VPN is that certain websites, especially online banks, get suspicious if they note repeated logins to the same account from too many different IP addresses. For those cases, you can either set up a split tunnel to exclude the website from encryption, or temporarily turn your VPN off altogether.Instructions for specific devicesAlthough most VPNs try to keep their apps similar on every platform, the strictures of differing hardware and software lead them to install and operate differently. In case you still have questions after reading the general guide above, this section goes into detail on every platform where you might use a VPN.How to use a VPN on desktopAfter subscribing to a VPN on Windows, you should be directed to download an EXE file — if this doesn't happen, log into your account on the website and find the downloads center. Find the folder where the EXE is saved, double-click it and follow the onscreen instructions.On Mac, the process is more or less the same, except you'll usually get a PKG file instead of an EXE. Go to your downloads folder (either in Finder or through your web browser) and double-click the PKG file. Grant the VPN whatever permissions it needs. (Again, this is why it’s important to only use a legitimate vendor, such as the ones we recommend.)Once installed, you can open the VPN client at any time by double-clicking the icon again. Some VPNs open as separate windows, while others will add icons to your toolbar. This often varies by platform; if you're concerned that your VPN doesn't look like a screenshot you've seen, check which operating system the image comes from.How to use a VPN on mobileOn Android and iOS, you'll download your VPN app through the Google Play Store or Apple App Store, respectively. Even if you get started through a mobile browser, it will probably redirect you to the app store for the actual download and installation.Follow the usual step for downloading an app: search for its name in the app store, click \"Get\" or \"Install,\" then let your phone cook. As always, so long as it's a vetted VPN, grant it the permissions it needs. You may be able to download and install the VPN first, then create your account and submit payment through the app afterward.One final note: several leading VPNs offer free trials for mobile users. If you see a button that says something like \"get free trial,\" you may be able to use the VPN for several days without paying. Just be warned that if the trial lapses, you might get automatically signed up for a plan that's longer than you'd like.How to use a VPN browser extensionVPNs offer browser extensions as lightweight versions of their main clients. While a desktop or mobile VPN reroutes everything that device sends to the internet, a browser extension only protects traffic through your web browser. You can use one as a primitive form of split tunneling, but they're mainly for basic convenience — most of what you do online goes through a browser, so it's nice to be able to protect your connection without opening a separate app.NordVPN browser extension on ChromeSam Chapman for EngadgetTo use a VPN browser extension, just create your account as normal, then download the extension from your VPN's website. You can manage it from your browser's extensions center. That's a jigsaw piece at the top-right corner on most browsers, including Chrome, Edge and Firefox.How to use a VPN on a smart TVYou can use a VPN to change your location and stream international content directly to a smart TV. The catch is that not all smart TV brands support VPN apps. For those that don't, you'll have to find a workaround.The good news is that a ton of the best smart TVs can natively host VPNs, including Google TV, Android TV, Amazon Fire TV and Apple TV (though only tvOS 17 and above). To use a VPN on Android TV or Apple TV, go through the device's app store. On Fire TV, simply type the name of your chosen VPN provider into the search bar.On smart TVs that don't have native VPN, like LG, Roku and Samsung, you have a few options. You can use a smart DNS feature like ExpressVPN's MediaStreamer to reroute smart TV traffic without full VPN encryption; the steps for this are different for every VPN, so check the provider's website. You can also install a VPN on your router (see below) so your smart TV automatically uses the router's location.Finally, you can get a temporary fix by using your computer as a Wi-Fi hotspot while it has a VPN active. Follow the steps for your operating system.On Windows:In your system settings, go to \"Network & Internet\" and turn on the mobile hotspot.Go to \"Network & Sharing Center\" and click \"Change adapter settings.\"Right-click the name of your VPN provider and go to \"Properties,\" then \"Sharing.\"Check the boxes next to \"Allow other network users to connect through this computer's internet connection\" and \"Allow other network users to control or disable the shared internet connection.\"Click the \"Home networking connection\" dropdown and select \"Microsoft Wi-Fi Direct Virtual Adapter.\"Open your VPN client and connect to a server in your desired location.On your smart TV, open the internet connections menu and select the name of your PC. Your TV is now online through the VPN server.On Mac:Open system settings and go to the \"General\" tab. Scroll down and click \"Sharing.\"Toggle \"Internet Sharing\" on, then click on \"Configure.\"Click the \"Share your connection from\" dropdown, then choose the VPN installed on the Mac. Under \"To computers using,\" select \"Wi-Fi.\"Click on \"Wi-Fi options\" and enter a name and password for your hotspot network.On your smart TV, connect to the network you just created.How to use a VPN on a game consoleRight now, there's no such thing as a game console with native VPN support. If you want to use a VPN while gaming — and I recommend that for safety if you're planning to play online — you can use two of the same methods that work for a smart TV: install a VPN on your router, or get your console online through a Mac or PC hotspot.How to install a VPN on a routerWhen you install a router VPN, anything that gets online through your home network will be protected, including game consoles, TVs and smart devices that don't support VPNs natively. It's not a process for the faint of heart, though. You'll need to get a new router and potentially install VPN firmware on it yourself. If you want to go this route, the easiest option is to get an ExpressVPN Aircove router — not only does it come with all the settings done for you, but it can be managed through the same clean interface as ExpressVPN's other apps.We don't have space here to go through the entire process, but here's a general overview. First, get a router with firmware that supports VPN configurations — most ISP default routers don't, so you'll have to go third-party.Next, go to the downloads center of your VPN's website and look for the section with VPN configurations. A \"configuration\" is a complete set of the information needed to access a certain VPN server through a certain protocol — say, a Proton VPN server in Arizona through OpenVPN. Download a configuration file for the protocol and location you want all your home devices to connect through.Finally, open your router control panel by entering your router's IP address into a web browser address bar, then log in with your router credentials (these should be marked on the router itself unless you've changed them). Go to the VPN tab — which should be there if it's a router with VPN firmware — and upload the profile you downloaded from the VPN website. Use the same router control panel to activate and deactivate the router VPN connection.This article originally appeared on Engadget at https://www.engadget.com/cybersecurity/vpn/how-to-use-a-vpn-140000564.html?src=rss",
          "content": "Chances are that if a VPN is marketed to everyday users, it more or less runs itself. As long as you go with one of the best VPNs instead of setting up your own manual connection, the interface is likely built so you have to engage with it as little as possible. Generally, if you find yourself constantly thinking about your VPN while using it — as happened to me with Norton VPN — that's a bad thing and you should change providers.That said, it's still useful to get a high-level view of how to use a VPN, especially if you've never used one before. In this article, I'll walk you through how to download, install and configure a VPN on any platform and how to choose servers for specific situations. I'll also discuss specific steps for desktop and laptop computers, phones, tablets, smart TVs, game consoles and routers.How to download and install a VPNThere are over 100 VPNs available on the internet and the two big mobile app stores, but most of them follow a similar approach to download and installation. Except for certain free VPNs, you'll need to create an account on the VPN provider's website, install the VPN client, then use your account to log in. Here are the steps.In a web browser, navigate to a VPN provider's website. For suggestions on where to start, check out our list of the best VPN deals. Click any link that mentions getting the VPN or claiming the deal — as a rule, they aren't hard to find.Select a plan based on the length of time you need the VPN for, the features you want and the number of devices you'd like to use the VPN on simultaneously. Add a payment method. You'll be prompted to create the username and password for a new account.On desktop, you can start with the Mac or Windows app store, though some VPNs require a download directly from their respective website. That’s generally the way to get the most feature-rich version of the VPN, but just make sure you’re downloading from the provider’s website to ensure you don’t get a malicious copycat. On mobile, the app store is your only choice, but don't forget to check the reviews and make sure you're getting the service you paid for — unfortunately, rival (and sometimes less scrupulous) VPN brands can use paid placements to appear higher on search results, even for a rival product. Follow the pre-programmed installation flow. Again, don't let a VPN make any changes to your system if you haven't thoroughly vetted the provider (or gone with one of our recommendations). If it's safe, allow the VPN all the setup permissions it needs.Finally, open the VPN client and sign in with the credentials you created in step two. You're now ready to start using your new VPN.Surfshark in Mac downloads folderSam Chapman for EngadgetWith so many VPNs on the market, this process won't always be the same. Sometimes you'll have to make the account before paying, and some mobile VPNs let you handle the whole process in-app. What's always true, though, is that a good VPN will make the signup flow as easy as possible. If you have trouble before you even log in, that's a red flag.How to configure a VPN appMost VPNs are ready to go out of the box. However, even if you don't think you're a power user, it's a good idea to go through the settings and make sure they match what you need. Here are a few steps I recommend taking.Activate the kill switch. This feature monitors your connection to the VPN server. If it ever drops — either from problems in the server, your client or somewhere in between — the kill switch instantly cuts off your internet until your VPN connection is restored. This saves you from accidentally broadcasting anything sensitive while also ensuring you only connect to legitimate VPN servers (avoiding the Tunnelvision bug).Set up favorites and profiles. VPNs usually let you select servers you'll use regularly, websites you'll often connect to through the VPN and other preferences that will appear immediately when you open the app. Some go further, like Proton VPN, which lets you create user profiles that bundle several preferences together.Decide if you want the VPN to automatically launch and connect when you start up your computer.Check on split tunneling. There may be cases where you want a certain app or site to work outside the VPN, or where you want everything to go unencrypted except one site or app. You can set up a split tunnel when you need it, but if there's one you know you'll use regularly, it helps to build it early.Activate or deactivate any content blockers the VPN comes with, including ad blockers, malware detectors and parental controls, as you see fit.Decide whether you want the VPN to send you notifications, and how.Select a VPN protocol. It's almost always best to let the VPN choose for itself, but it's helpful to at least know where the protocol-switching option is. Occasionally, if one protocol isn't working, you can troubleshoot by selecting another.You can do all this by digging into your VPN's preferences control panel. A gear seems to be the universal icon for that, but Mac users may also find it by picking \"preferences\" or \"settings\" from the menu bar. When you find the preferences menu, go through each tab in turn and make sure everything is the way you like it.How to choose a VPN serverPicking a server location is the final box to check before connecting. You nearly always have the option of letting the VPN pick the best server. Most apps tend to determine the best server with a latency test, so it will almost always be a node very close to your physical location.If you just want anonymity online, that's fine — it doesn't matter which IP address you use as long as it's not your real one. But several VPN use cases do require a specific server location. For those, follow one simple rule: Pick a server in the place you want your signal to come from.Choosing locations on SurfsharkSam Chapman for EngadgetFor example, let's say you want to watch The Office, but you're only subscribed to Netflix and not Peacock. Luckily, Netflix is licensed to show The Office in the U.K. If you connect to a British server location before opening Netflix, you'll be able to access the show without paying for an extra subscription. (Of course, make sure VPNs aren’t expressly banned by the streaming service’s terms of service before accessing it while using one.)Other than streaming, the most common reason you'll need a specific VPN server is to get around firewalls that block websites. Whether it's your school doing the censorship, your workplace or your entire state, the solution is the same: Pick a VPN server outside the restricted region. Censorship systems filter by location — they can't block where they don't have jurisdiction. Get a new virtual location and you should be free and clear. (Always be cognizant of the laws on VPN usage in your location before activating them.)When to use a VPN (and when not to)It's good to get into the habit of connecting to your VPN whenever you get online. You can never be sure what information your ISP is gathering on you. If you're using unprotected Wi-Fi, or a public network with a clearly visible password, anybody might be listening in. Even if you don't need a particular location, always be using your VPN.The only reason you might want to be online without a VPN is that certain websites, especially online banks, get suspicious if they note repeated logins to the same account from too many different IP addresses. For those cases, you can either set up a split tunnel to exclude the website from encryption, or temporarily turn your VPN off altogether.Instructions for specific devicesAlthough most VPNs try to keep their apps similar on every platform, the strictures of differing hardware and software lead them to install and operate differently. In case you still have questions after reading the general guide above, this section goes into detail on every platform where you might use a VPN.How to use a VPN on desktopAfter subscribing to a VPN on Windows, you should be directed to download an EXE file — if this doesn't happen, log into your account on the website and find the downloads center. Find the folder where the EXE is saved, double-click it and follow the onscreen instructions.On Mac, the process is more or less the same, except you'll usually get a PKG file instead of an EXE. Go to your downloads folder (either in Finder or through your web browser) and double-click the PKG file. Grant the VPN whatever permissions it needs. (Again, this is why it’s important to only use a legitimate vendor, such as the ones we recommend.)Once installed, you can open the VPN client at any time by double-clicking the icon again. Some VPNs open as separate windows, while others will add icons to your toolbar. This often varies by platform; if you're concerned that your VPN doesn't look like a screenshot you've seen, check which operating system the image comes from.How to use a VPN on mobileOn Android and iOS, you'll download your VPN app through the Google Play Store or Apple App Store, respectively. Even if you get started through a mobile browser, it will probably redirect you to the app store for the actual download and installation.Follow the usual step for downloading an app: search for its name in the app store, click \"Get\" or \"Install,\" then let your phone cook. As always, so long as it's a vetted VPN, grant it the permissions it needs. You may be able to download and install the VPN first, then create your account and submit payment through the app afterward.One final note: several leading VPNs offer free trials for mobile users. If you see a button that says something like \"get free trial,\" you may be able to use the VPN for several days without paying. Just be warned that if the trial lapses, you might get automatically signed up for a plan that's longer than you'd like.How to use a VPN browser extensionVPNs offer browser extensions as lightweight versions of their main clients. While a desktop or mobile VPN reroutes everything that device sends to the internet, a browser extension only protects traffic through your web browser. You can use one as a primitive form of split tunneling, but they're mainly for basic convenience — most of what you do online goes through a browser, so it's nice to be able to protect your connection without opening a separate app.NordVPN browser extension on ChromeSam Chapman for EngadgetTo use a VPN browser extension, just create your account as normal, then download the extension from your VPN's website. You can manage it from your browser's extensions center. That's a jigsaw piece at the top-right corner on most browsers, including Chrome, Edge and Firefox.How to use a VPN on a smart TVYou can use a VPN to change your location and stream international content directly to a smart TV. The catch is that not all smart TV brands support VPN apps. For those that don't, you'll have to find a workaround.The good news is that a ton of the best smart TVs can natively host VPNs, including Google TV, Android TV, Amazon Fire TV and Apple TV (though only tvOS 17 and above). To use a VPN on Android TV or Apple TV, go through the device's app store. On Fire TV, simply type the name of your chosen VPN provider into the search bar.On smart TVs that don't have native VPN, like LG, Roku and Samsung, you have a few options. You can use a smart DNS feature like ExpressVPN's MediaStreamer to reroute smart TV traffic without full VPN encryption; the steps for this are different for every VPN, so check the provider's website. You can also install a VPN on your router (see below) so your smart TV automatically uses the router's location.Finally, you can get a temporary fix by using your computer as a Wi-Fi hotspot while it has a VPN active. Follow the steps for your operating system.On Windows:In your system settings, go to \"Network & Internet\" and turn on the mobile hotspot.Go to \"Network & Sharing Center\" and click \"Change adapter settings.\"Right-click the name of your VPN provider and go to \"Properties,\" then \"Sharing.\"Check the boxes next to \"Allow other network users to connect through this computer's internet connection\" and \"Allow other network users to control or disable the shared internet connection.\"Click the \"Home networking connection\" dropdown and select \"Microsoft Wi-Fi Direct Virtual Adapter.\"Open your VPN client and connect to a server in your desired location.On your smart TV, open the internet connections menu and select the name of your PC. Your TV is now online through the VPN server.On Mac:Open system settings and go to the \"General\" tab. Scroll down and click \"Sharing.\"Toggle \"Internet Sharing\" on, then click on \"Configure.\"Click the \"Share your connection from\" dropdown, then choose the VPN installed on the Mac. Under \"To computers using,\" select \"Wi-Fi.\"Click on \"Wi-Fi options\" and enter a name and password for your hotspot network.On your smart TV, connect to the network you just created.How to use a VPN on a game consoleRight now, there's no such thing as a game console with native VPN support. If you want to use a VPN while gaming — and I recommend that for safety if you're planning to play online — you can use two of the same methods that work for a smart TV: install a VPN on your router, or get your console online through a Mac or PC hotspot.How to install a VPN on a routerWhen you install a router VPN, anything that gets online through your home network will be protected, including game consoles, TVs and smart devices that don't support VPNs natively. It's not a process for the faint of heart, though. You'll need to get a new router and potentially install VPN firmware on it yourself. If you want to go this route, the easiest option is to get an ExpressVPN Aircove router — not only does it come with all the settings done for you, but it can be managed through the same clean interface as ExpressVPN's other apps.We don't have space here to go through the entire process, but here's a general overview. First, get a router with firmware that supports VPN configurations — most ISP default routers don't, so you'll have to go third-party.Next, go to the downloads center of your VPN's website and look for the section with VPN configurations. A \"configuration\" is a complete set of the information needed to access a certain VPN server through a certain protocol — say, a Proton VPN server in Arizona through OpenVPN. Download a configuration file for the protocol and location you want all your home devices to connect through.Finally, open your router control panel by entering your router's IP address into a web browser address bar, then log in with your router credentials (these should be marked on the router itself unless you've changed them). Go to the VPN tab — which should be there if it's a router with VPN firmware — and upload the profile you downloaded from the VPN website. Use the same router control panel to activate and deactivate the router VPN connection.This article originally appeared on Engadget at https://www.engadget.com/cybersecurity/vpn/how-to-use-a-vpn-140000564.html?src=rss",
          "feed_position": 6,
          "image_url": "https://d29szjachogqwa.cloudfront.net/images/user-uploaded/Downloading_Surfshark_on_Mac.png"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/gaming/how-to-watch-the-first-ever-lego-ces-2026-press-conference-live-130005307.html",
          "published_at": "Wed, 31 Dec 2025 13:00:05 +0000",
          "title": "How to watch the first-ever Lego CES 2026 press conference live",
          "standfirst": "picture alliance via Getty Images The Lego Group is hosting its first-ever press conference at CES 2026 – but what the company is going to announce remains a mystery. While the huge toy brick creators haven't given any hints as to what they'll be showcasing, the possibilities run the gamut from new video games to Formula 1 race cars. Here's how you can watch Lego's presentation at CES, and what we might expect to see. How to watch The Lego Group's CES 2026 press conference The Lego CES press conference is scheduled for Monday, January 5 at 1PM ET. While Lego and the Consumer Technology Association haven't yet provided the details, we expect that the press conference will be available as a livestream. Once the details are confirmed, we'll update this post to confirm them. But if a livestream isn't immediately available, the Engadget team will be liveblogging the Lego presser and posting timely details. What to expect Thus far, Lego hasn't shared any public info about its CES plans, so we're largely in the dark as to what to expect. At CES 2025, for instance, the toy production giant partnered with Sony to announce the animated Lego Horizon Adventures online game. As such, Lego may spend some time talking up its new 2026 game, Lego Batman: Legacy of the Dark Knight. The company may also give some stage time to its Lego Group F1 Academy racing car, though that too would be more about brand building than consumer products. And given Lego's focus on the environment, the company may discuss its efforts to reach its 2032 ecological goals, including making its Lego bricks more sustainable and reducing carbon emissions by 37%. While there are plenty of new Lego sets for 2026, however, this is CES, not Toy Fair. So we're assuming that the company will be showcasing something that's more tech-centric. Stay tuned.This article originally appeared on Engadget at https://www.engadget.com/gaming/how-to-watch-the-first-ever-lego-ces-2026-press-conference-live-130005307.html?src=rss",
          "content": "picture alliance via Getty Images The Lego Group is hosting its first-ever press conference at CES 2026 – but what the company is going to announce remains a mystery. While the huge toy brick creators haven't given any hints as to what they'll be showcasing, the possibilities run the gamut from new video games to Formula 1 race cars. Here's how you can watch Lego's presentation at CES, and what we might expect to see. How to watch The Lego Group's CES 2026 press conference The Lego CES press conference is scheduled for Monday, January 5 at 1PM ET. While Lego and the Consumer Technology Association haven't yet provided the details, we expect that the press conference will be available as a livestream. Once the details are confirmed, we'll update this post to confirm them. But if a livestream isn't immediately available, the Engadget team will be liveblogging the Lego presser and posting timely details. What to expect Thus far, Lego hasn't shared any public info about its CES plans, so we're largely in the dark as to what to expect. At CES 2025, for instance, the toy production giant partnered with Sony to announce the animated Lego Horizon Adventures online game. As such, Lego may spend some time talking up its new 2026 game, Lego Batman: Legacy of the Dark Knight. The company may also give some stage time to its Lego Group F1 Academy racing car, though that too would be more about brand building than consumer products. And given Lego's focus on the environment, the company may discuss its efforts to reach its 2032 ecological goals, including making its Lego bricks more sustainable and reducing carbon emissions by 37%. While there are plenty of new Lego sets for 2026, however, this is CES, not Toy Fair. So we're assuming that the company will be showcasing something that's more tech-centric. Stay tuned.This article originally appeared on Engadget at https://www.engadget.com/gaming/how-to-watch-the-first-ever-lego-ces-2026-press-conference-live-130005307.html?src=rss",
          "feed_position": 8,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2025-12/ced0c170-dd13-11f0-b394-1dea3c7b6af3"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/cameras/best-cameras-151524327.html",
          "published_at": "Wed, 31 Dec 2025 10:01:26 +0000",
          "title": "The best cameras for 2026",
          "standfirst": "Choosing a camera is not just about resolution or price. It is about finding something that matches how you like to shoot. Some photographers want a compact mirrorless camera that travels easily. Others want an action cam that can handle rough hikes, or a full-frame system that delivers the best possible image quality. With so many options today, there is a camera built for almost every creative style.We tested the top models across categories to help you decide which one fits your needs. Whether you are filming your first vlog, shooting portraits or capturing fast-moving action, these are the best cameras to help you grow your skills. Best cameras for 2026 Best mirrorless cameras Mirrorless is the largest camera category in terms of models available, so it’s the best way to go if you’re looking for something with the most advanced features. Canon and Nikon recently announced they’re discontinuing development of new DSLRs, simply because most of the advantages of that category are gone, as I detailed in a video. The biggest selling feature of a mirrorless camera is the ability to change lenses depending on the type of shooting you want to do. Best action camera or gimbal camera The most important features to look for in an action cam are image quality, stabilization and battery life. GoPro has easily been beating all rivals recently in all those areas, but DJI has taken a lot of its business with the Osmo Pocket 3 gimbal camera. Best compact camera This category has fewer cameras than it did even a few years ago and many models are older, as manufacturers focus instead on mirrorless models. However, I’m still a big believer in compact cameras. They’re a noticeable step up from smartphones quality-wise, and a lot of people will take a compact traveling or to events when they’d never bother with the hassle of a DSLR or mirrorless camera. Compacts largely have type 1-inch sensors, but a few offer larger options, particularly Fujifilm’s XF-100V. Another popular model, Sony’s XV-1, is primarily aimed at content creators looking to step up. In any case, desirable qualities include image quality, a fast lens, relatively long zoom, flip-out display, good battery life, a high quality EVF, decent video and good pocketability. What to consider before choosing a camera Though smartphones get better for video and photos every year, full cameras still have an edge in many ways. The larger sensors in mirrorless cameras let more light in, and you have a wide choice of lenses with far superior optics. Dedicated cameras are also faster for shooting things like sports or wildlife, offer superior video for content creators and create more professional results. Sensor size There are a few key things to consider to get the most out of a camera. The first is sensor size: in general, the larger the sensor, the better (and usually more expensive) the camera. Full frame is the largest sensor size for mainstream cameras, and it’s available on models like the new Panasonic S9, the Nikon Z III and Canon EOS R5 II. At a size equivalent to 35mm film (36 x 24mm), it offers the best performance in terms of image quality, low-light capability and depth of field. But it’s also very expensive and finicky. While bokeh looks incredible at an aperture of f/1.4, the depth of field is so razor thin that your subject's eyebrow might be in focus but not their eye. This can also make shooting video difficult. The next size category is APS-C (around 23.5 x 15.6mm for most models and 22.2 x 14.8mm for Canon), offered on Fujifilm's X Series lineup, the Canon R10, the Sony ZV-E10 II and the Nikon Z50. It's cheaper than full frame, both for the camera body and lenses, but still brings most of the advantages like decent bokeh, high ISOs for low-light shooting and relatively high resolution. With a sensor size the same as movie cameras, it's ideal for shooting video, and it’s easier to hold focus than with full-frame cameras. Micro Four Thirds (17.3 x 13mm), a format shared by Panasonic and Olympus, is the next step down in sensor size. It offers less bokeh and light-gathering capability than APS-C and full frame, but allows for smaller and lighter cameras and lenses. For video, you can still get reasonably tight depth of field with good prime lenses, but focus is easier to control. The other common sensor size is Type 1 (1 inch), which is actually smaller than one inch at 12.7 x 9.5mm. That's used mostly by compact models like Sony’s ZV-1 vlogging camera. Finally, action cameras like the GoPro Hero 11 and DJI’s Osmo 3 have even smaller sensors (1/1.9 and 1/1.7 inches, respectively). Autofocus For photographers, another key factor is autofocus (AF) speed and accuracy. Most modern mirrorless cameras have hybrid phase-detect AF systems that allow for rapid focus and fast burst speeds. The majority also offer AI features like eye-detect AF for people and animals, which locks in on the subject’s eyes, face or body to keep them in focus. However, some models are faster and more reactive than others. Displays The electronic viewfinder (EVF) and rear display are also crucial. The best models have the sharpest and brightest EVFs that help you judge a shot before taking it. For things like street photography, it’s best to have as bright and sharp a rear display as possible, so it’s easy to see your subject and check focus in all manner of lighting conditions. You may also want a screen that flips out rather than just tilting, too. Lenses DSLRs and mirrorless cameras let you change lenses, but you're stuck with what's built into a compact camera. While that's great for portability, a single lens means you're going to sacrifice something along the way. The Fujifilm X100V, for instance, has a fast but fixed 35mm-equivalent f/2.0 lens and no zoom. The Sony RX100 V has a 24-70mm zoom, but it's slower at the telephoto end (f/2.8) and less sharp than a fixed focal (prime) lens. When choosing a lens for a mirrorless camera, you’ll need to consider the focal or zoom length, along with the minimum aperture. Smaller numbers like f/1.4 for a prime lens or f/2.8 for a zoom are best, as they let you work in darker environments and maximize background blur to isolate your subject. However, those lenses are more complex and thus more expensive. Video recording When it comes to video, there are other factors to consider. Some cameras combine or skip over pixels (line skipping or pixel binning) for video recording, which is not ideal because it can reduce sharpness. Better cameras tend to read out the entire sensor and then “downsample” to improve video sharpness (camera manufacturers don’t often say if video is pixel binned, but will say if it’s downsampled). Another important factor is sensor speed, as slower sensors tend to have more rolling shutter that can create a “jello” effect that skews video. In addition, how’s the battery life? How do you like the handling and feel? How long can you shoot before the camera heats up or stops? Does it support 10-bit HDR video? Is there a microphone and/or a headphone jack? (If you record a lot of interviews, it's preferable to have both.) How's the video autofocus? All of these things play a part in your decision.This article originally appeared on Engadget at https://www.engadget.com/cameras/best-cameras-151524327.html?src=rss",
          "content": "Choosing a camera is not just about resolution or price. It is about finding something that matches how you like to shoot. Some photographers want a compact mirrorless camera that travels easily. Others want an action cam that can handle rough hikes, or a full-frame system that delivers the best possible image quality. With so many options today, there is a camera built for almost every creative style.We tested the top models across categories to help you decide which one fits your needs. Whether you are filming your first vlog, shooting portraits or capturing fast-moving action, these are the best cameras to help you grow your skills. Best cameras for 2026 Best mirrorless cameras Mirrorless is the largest camera category in terms of models available, so it’s the best way to go if you’re looking for something with the most advanced features. Canon and Nikon recently announced they’re discontinuing development of new DSLRs, simply because most of the advantages of that category are gone, as I detailed in a video. The biggest selling feature of a mirrorless camera is the ability to change lenses depending on the type of shooting you want to do. Best action camera or gimbal camera The most important features to look for in an action cam are image quality, stabilization and battery life. GoPro has easily been beating all rivals recently in all those areas, but DJI has taken a lot of its business with the Osmo Pocket 3 gimbal camera. Best compact camera This category has fewer cameras than it did even a few years ago and many models are older, as manufacturers focus instead on mirrorless models. However, I’m still a big believer in compact cameras. They’re a noticeable step up from smartphones quality-wise, and a lot of people will take a compact traveling or to events when they’d never bother with the hassle of a DSLR or mirrorless camera. Compacts largely have type 1-inch sensors, but a few offer larger options, particularly Fujifilm’s XF-100V. Another popular model, Sony’s XV-1, is primarily aimed at content creators looking to step up. In any case, desirable qualities include image quality, a fast lens, relatively long zoom, flip-out display, good battery life, a high quality EVF, decent video and good pocketability. What to consider before choosing a camera Though smartphones get better for video and photos every year, full cameras still have an edge in many ways. The larger sensors in mirrorless cameras let more light in, and you have a wide choice of lenses with far superior optics. Dedicated cameras are also faster for shooting things like sports or wildlife, offer superior video for content creators and create more professional results. Sensor size There are a few key things to consider to get the most out of a camera. The first is sensor size: in general, the larger the sensor, the better (and usually more expensive) the camera. Full frame is the largest sensor size for mainstream cameras, and it’s available on models like the new Panasonic S9, the Nikon Z III and Canon EOS R5 II. At a size equivalent to 35mm film (36 x 24mm), it offers the best performance in terms of image quality, low-light capability and depth of field. But it’s also very expensive and finicky. While bokeh looks incredible at an aperture of f/1.4, the depth of field is so razor thin that your subject's eyebrow might be in focus but not their eye. This can also make shooting video difficult. The next size category is APS-C (around 23.5 x 15.6mm for most models and 22.2 x 14.8mm for Canon), offered on Fujifilm's X Series lineup, the Canon R10, the Sony ZV-E10 II and the Nikon Z50. It's cheaper than full frame, both for the camera body and lenses, but still brings most of the advantages like decent bokeh, high ISOs for low-light shooting and relatively high resolution. With a sensor size the same as movie cameras, it's ideal for shooting video, and it’s easier to hold focus than with full-frame cameras. Micro Four Thirds (17.3 x 13mm), a format shared by Panasonic and Olympus, is the next step down in sensor size. It offers less bokeh and light-gathering capability than APS-C and full frame, but allows for smaller and lighter cameras and lenses. For video, you can still get reasonably tight depth of field with good prime lenses, but focus is easier to control. The other common sensor size is Type 1 (1 inch), which is actually smaller than one inch at 12.7 x 9.5mm. That's used mostly by compact models like Sony’s ZV-1 vlogging camera. Finally, action cameras like the GoPro Hero 11 and DJI’s Osmo 3 have even smaller sensors (1/1.9 and 1/1.7 inches, respectively). Autofocus For photographers, another key factor is autofocus (AF) speed and accuracy. Most modern mirrorless cameras have hybrid phase-detect AF systems that allow for rapid focus and fast burst speeds. The majority also offer AI features like eye-detect AF for people and animals, which locks in on the subject’s eyes, face or body to keep them in focus. However, some models are faster and more reactive than others. Displays The electronic viewfinder (EVF) and rear display are also crucial. The best models have the sharpest and brightest EVFs that help you judge a shot before taking it. For things like street photography, it’s best to have as bright and sharp a rear display as possible, so it’s easy to see your subject and check focus in all manner of lighting conditions. You may also want a screen that flips out rather than just tilting, too. Lenses DSLRs and mirrorless cameras let you change lenses, but you're stuck with what's built into a compact camera. While that's great for portability, a single lens means you're going to sacrifice something along the way. The Fujifilm X100V, for instance, has a fast but fixed 35mm-equivalent f/2.0 lens and no zoom. The Sony RX100 V has a 24-70mm zoom, but it's slower at the telephoto end (f/2.8) and less sharp than a fixed focal (prime) lens. When choosing a lens for a mirrorless camera, you’ll need to consider the focal or zoom length, along with the minimum aperture. Smaller numbers like f/1.4 for a prime lens or f/2.8 for a zoom are best, as they let you work in darker environments and maximize background blur to isolate your subject. However, those lenses are more complex and thus more expensive. Video recording When it comes to video, there are other factors to consider. Some cameras combine or skip over pixels (line skipping or pixel binning) for video recording, which is not ideal because it can reduce sharpness. Better cameras tend to read out the entire sensor and then “downsample” to improve video sharpness (camera manufacturers don’t often say if video is pixel binned, but will say if it’s downsampled). Another important factor is sensor speed, as slower sensors tend to have more rolling shutter that can create a “jello” effect that skews video. In addition, how’s the battery life? How do you like the handling and feel? How long can you shoot before the camera heats up or stops? Does it support 10-bit HDR video? Is there a microphone and/or a headphone jack? (If you record a lot of interviews, it's preferable to have both.) How's the video autofocus? All of these things play a part in your decision.This article originally appeared on Engadget at https://www.engadget.com/cameras/best-cameras-151524327.html?src=rss",
          "feed_position": 10
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/data/six-data-shifts-that-will-shape-enterprise-ai-in-2026",
          "published_at": "Wed, 31 Dec 2025 05:00:00 GMT",
          "title": "Six data shifts that will shape enterprise AI in 2026",
          "standfirst": "For decades the data landscape was relatively static. Relational databases (hello, Oracle!) were the default and dominated, organizing information into familiar columns and rows.That stability eroded as successive waves introduced NoSQL document stores, graph databases, and most recently vector-based systems. In the era of agentic AI, data infrastructure is once again in flux — and evolving faster than at any point in recent memory.As 2026 dawns, one lesson has become unavoidable: data matters more than ever.RAG is dead. Long live RAGPerhaps the most consequential trend out of 2025 that will continue to be debated into 2026 (and maybe beyond) is the role of RAG.The problem is that the original RAG pipeline architecture is much like a basic search. The retrieval finds the result of a specific query, at a specific point in time. It is also often limited to a single data source, or at least that&#x27;s the way RAG pipelines were built in the past (the past being anytime prior to June 2025). Those limitations have led a growing conga line of vendors all claiming that RAG is dying, on the way out, or already dead.What is emerging, though, are alternative approaches (like contextual memory), as well as nuanced and improved approaches to RAG. For example, Snowflake recently announced its agentic document analytics technology, which expands the traditional RAG data pipeline to enable analysis across thousands of sources, without needing to have structured data first. There are also numerous other RAG-like approaches that are emerging including GraphRAG that will likely only grow in usage and capabilities in 2026.So now RAG isn&#x27;t (entirely) dead, at least not yet. Organizations will still find use cases in 2026 where data retrieval is needed and some enhanced version of RAG will likely still fit the bill. Enterprises in 2026 should evaluate use cases individually. Traditional RAG works for static knowledge retrieval, whereas enhanced approaches like GraphRAG suit complex, multi-source queries.Contextual memory is table stakes for agentic AIWhile RAG won&#x27;t entirely disappear in 2026, one approach that will likely surpass it in terms of usage for agentic AI is contextual memory, also known as agentic or long-context memory. This technology enables LLMs to store and access pertinent information over extended periods.Multiple such systems emerged over the course of 2025 including Hindsight, A-MEM framework, General Agentic Memory (GAM), LangMem, and Memobase. RAG will remain useful for static data, but agentic memory is critical for adaptive assistants and agentic AI workflows that must learn from feedback, maintain state, and adapt over time.In 2026, contextual memory will no longer be a novel technique; it will become table stakes for many operational agentic AI deployments.Purpose-built vector databases use cases will changeAt the beginning of the modern generative AI era, purpose-built vector databases (like Pinecone and Milvus, among others) were all the rage. In order for an LLM (generally but not exclusively via RAG) to get access to new information, it needs to access data. The best way to do that is by encoding the data in vectors — that is, a numerical representation of what the data represents.In 2025 what became painfully obvious was that vectors were no longer a specific database type but rather a specific data type that could be integrated into an existing multimodel database. So instead of an organization being required to use a purpose-built system, it could just use an existing database that supports vectors. For example, Oracle supports vectors and so does every database offered by Google.Oh, and it gets better. Amazon S3, long the de facto leader in cloud based object storage, now allows users to store vectors, further negating the need for a dedicated, unique vector database. That doesn’t mean object storage replaces vector search engines — performance, indexing, and filtering still matter — but it does narrow the set of use cases where specialized systems are required.No, that doesn&#x27;t mean purpose-built vector databases are dead. Much like with RAG, there will continue to be use cases for purpose-built vector databases in 2026. What will change is that use cases will likely narrow somewhat for organizations that need the highest levels of performance or a specific optimization that a general-purpose solution doesn&#x27;t support.PostgreSQL ascendantAs 2026 starts, what&#x27;s old is new again. The open-source PostgreSQL database will be 40 years old in 2026, yet it will be more relevant than it has ever been before.Over the course of 2025, the supremacy of PostgreSQL as the go-to database for building any type of GenAI solution became apparent. Snowflake spent $250 million to acquire PostgreSQL database vendor Crunchy Data; Databricks spent $1 billion on Neon; and Supabase raised a $100 million series E giving it a $5 billion valuation.All that money serves as a clear signal that enterprises are defaulting to PostgreSQL. The reasons are many including the open-source base, flexibility, and performance. For vibe coding (a core use case for Supabase and Neon in particular), PostgreSQL is the standard.Expect to see more growth and adoption of PostgreSQL in 2026 as more organizations come to the same conclusions as Snowflake and Databricks.Data researchers will continue to find new ways to solve already solved problems It&#x27;s likely that there will be more innovation to help problems that many organizations likely assume are already: solved problems.In 2025, we saw numerous innovations, like the notion that an AI is able to parse data from an unstructured data source like a PDF. That&#x27;s a capability that has existed for several years, but proved harder to operationalize at scale than many assumed. Databricks now has an advanced parser, and other vendors, including Mistral, have emerged with their own improvements.The same is true with natural language to SQL translation. While some might have assumed that was a solved problem, it&#x27;s one that continued to see innovation in 2025 and will see more in 2026.It&#x27;s critical for enterprises to stay vigilant in 2026. Don&#x27;t assume foundational capabilities like parsing or natural language to SQL are fully solved. Keep evaluating new approaches that may significantly outperform existing tools.Acquisitions, investments, and consolidation will continue2025 was a big year for big money going into data vendors.Meta invested $14.3 billion in data labeling vendor Scale AI; IBM said it plans to acquire data streaming vendor Confluent for $11 billion; and Salesforce picked up Informatica for $8 billion.Organizations should expect the pace of acquisitions of all sizes to continue in 2026, as big vendors realize the foundational importance of data to the success of agentic AI.The impact of acquisitions and consolidation on enterprises in 2026 is hard to predict. It can lead to vendor lock-in, and it can also potentially lead to expanded platform capabilities. In 2026, the question won’t be whether enterprises are using AI — it will be whether their data systems are capable of sustaining it. As agentic AI matures, durable data infrastructure — not clever prompts or short-lived architectures — will determine which deployments scale and which quietly stall out.",
          "content": "For decades the data landscape was relatively static. Relational databases (hello, Oracle!) were the default and dominated, organizing information into familiar columns and rows.That stability eroded as successive waves introduced NoSQL document stores, graph databases, and most recently vector-based systems. In the era of agentic AI, data infrastructure is once again in flux — and evolving faster than at any point in recent memory.As 2026 dawns, one lesson has become unavoidable: data matters more than ever.RAG is dead. Long live RAGPerhaps the most consequential trend out of 2025 that will continue to be debated into 2026 (and maybe beyond) is the role of RAG.The problem is that the original RAG pipeline architecture is much like a basic search. The retrieval finds the result of a specific query, at a specific point in time. It is also often limited to a single data source, or at least that&#x27;s the way RAG pipelines were built in the past (the past being anytime prior to June 2025). Those limitations have led a growing conga line of vendors all claiming that RAG is dying, on the way out, or already dead.What is emerging, though, are alternative approaches (like contextual memory), as well as nuanced and improved approaches to RAG. For example, Snowflake recently announced its agentic document analytics technology, which expands the traditional RAG data pipeline to enable analysis across thousands of sources, without needing to have structured data first. There are also numerous other RAG-like approaches that are emerging including GraphRAG that will likely only grow in usage and capabilities in 2026.So now RAG isn&#x27;t (entirely) dead, at least not yet. Organizations will still find use cases in 2026 where data retrieval is needed and some enhanced version of RAG will likely still fit the bill. Enterprises in 2026 should evaluate use cases individually. Traditional RAG works for static knowledge retrieval, whereas enhanced approaches like GraphRAG suit complex, multi-source queries.Contextual memory is table stakes for agentic AIWhile RAG won&#x27;t entirely disappear in 2026, one approach that will likely surpass it in terms of usage for agentic AI is contextual memory, also known as agentic or long-context memory. This technology enables LLMs to store and access pertinent information over extended periods.Multiple such systems emerged over the course of 2025 including Hindsight, A-MEM framework, General Agentic Memory (GAM), LangMem, and Memobase. RAG will remain useful for static data, but agentic memory is critical for adaptive assistants and agentic AI workflows that must learn from feedback, maintain state, and adapt over time.In 2026, contextual memory will no longer be a novel technique; it will become table stakes for many operational agentic AI deployments.Purpose-built vector databases use cases will changeAt the beginning of the modern generative AI era, purpose-built vector databases (like Pinecone and Milvus, among others) were all the rage. In order for an LLM (generally but not exclusively via RAG) to get access to new information, it needs to access data. The best way to do that is by encoding the data in vectors — that is, a numerical representation of what the data represents.In 2025 what became painfully obvious was that vectors were no longer a specific database type but rather a specific data type that could be integrated into an existing multimodel database. So instead of an organization being required to use a purpose-built system, it could just use an existing database that supports vectors. For example, Oracle supports vectors and so does every database offered by Google.Oh, and it gets better. Amazon S3, long the de facto leader in cloud based object storage, now allows users to store vectors, further negating the need for a dedicated, unique vector database. That doesn’t mean object storage replaces vector search engines — performance, indexing, and filtering still matter — but it does narrow the set of use cases where specialized systems are required.No, that doesn&#x27;t mean purpose-built vector databases are dead. Much like with RAG, there will continue to be use cases for purpose-built vector databases in 2026. What will change is that use cases will likely narrow somewhat for organizations that need the highest levels of performance or a specific optimization that a general-purpose solution doesn&#x27;t support.PostgreSQL ascendantAs 2026 starts, what&#x27;s old is new again. The open-source PostgreSQL database will be 40 years old in 2026, yet it will be more relevant than it has ever been before.Over the course of 2025, the supremacy of PostgreSQL as the go-to database for building any type of GenAI solution became apparent. Snowflake spent $250 million to acquire PostgreSQL database vendor Crunchy Data; Databricks spent $1 billion on Neon; and Supabase raised a $100 million series E giving it a $5 billion valuation.All that money serves as a clear signal that enterprises are defaulting to PostgreSQL. The reasons are many including the open-source base, flexibility, and performance. For vibe coding (a core use case for Supabase and Neon in particular), PostgreSQL is the standard.Expect to see more growth and adoption of PostgreSQL in 2026 as more organizations come to the same conclusions as Snowflake and Databricks.Data researchers will continue to find new ways to solve already solved problems It&#x27;s likely that there will be more innovation to help problems that many organizations likely assume are already: solved problems.In 2025, we saw numerous innovations, like the notion that an AI is able to parse data from an unstructured data source like a PDF. That&#x27;s a capability that has existed for several years, but proved harder to operationalize at scale than many assumed. Databricks now has an advanced parser, and other vendors, including Mistral, have emerged with their own improvements.The same is true with natural language to SQL translation. While some might have assumed that was a solved problem, it&#x27;s one that continued to see innovation in 2025 and will see more in 2026.It&#x27;s critical for enterprises to stay vigilant in 2026. Don&#x27;t assume foundational capabilities like parsing or natural language to SQL are fully solved. Keep evaluating new approaches that may significantly outperform existing tools.Acquisitions, investments, and consolidation will continue2025 was a big year for big money going into data vendors.Meta invested $14.3 billion in data labeling vendor Scale AI; IBM said it plans to acquire data streaming vendor Confluent for $11 billion; and Salesforce picked up Informatica for $8 billion.Organizations should expect the pace of acquisitions of all sizes to continue in 2026, as big vendors realize the foundational importance of data to the success of agentic AI.The impact of acquisitions and consolidation on enterprises in 2026 is hard to predict. It can lead to vendor lock-in, and it can also potentially lead to expanded platform capabilities. In 2026, the question won’t be whether enterprises are using AI — it will be whether their data systems are capable of sustaining it. As agentic AI matures, durable data infrastructure — not clever prompts or short-lived architectures — will determine which deployments scale and which quietly stall out.",
          "feed_position": 1,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/1Am5h54S0EVCRIKGeWcWmG/49983d561b05cc8f59f08aea19a9e3c9/data-in-2026.jpg?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/home/how-to-watch-the-hisense-ces-2026-presentation-live-190040090.html",
          "published_at": "Tue, 30 Dec 2025 19:00:40 +0000",
          "title": "How to watch the Hisense CES 2026 presentation live",
          "standfirst": "Xinhua News Agency via Getty Images Hisense is perhaps best known for its budget-friendly electronics and appliances, like TVs and refrigerators. But at CES 2025, the China-based company showed its high-end chops with a massive 136-inch micro LED TV, which recently became available for a whopping $100,000. So what's on deck for this year's show? New leadership, for starters. The company has two new hires, including Chief Marketing Officer Sarah Larsen and Chief Commercial Officer James Fishler. In a press release, Hisense said Fishler's experience in home entertainment, appliances and HVAC is important as the company \"builds toward a milestone 2026 and its presence at CES.\" We'll give you a rundown of what to expect during Hisense's presentation and how you can watch it. How to watch Hisense will have a livestream available on Monday, January 5 at 1PM ET on its website. We'll embed the link here once it's available. What to expect With its new hires in place, Hisense is clearly aiming to further polish its brand. Between Fishler and Larsen, the new front office is bringing to bear their experience from such high-powered competitors as LG, Samsung and Beats. And in a recent interview with Tom's Guide, Larsen emphasized a continued focus on the company's fast turnaround time from concept to market as a key differentiator for Hisense. As for actual announcements, while you can expect Hisense to tout its strength in appliances and HVAC systems (really), Larsen's aforementioned interview specifically calls out the emerging RGB TV space as a focus. We expect this year's show will be all about explaining the shades of difference between mini and micro LED display technologies, as both Samsung and LG have already thrown down pre-announcement gauntlets on the latter. Will any of them cost less than six figures? Let's hope Hisense has some good news to share on that front.This article originally appeared on Engadget at https://www.engadget.com/home/how-to-watch-the-hisense-ces-2026-presentation-live-190040090.html?src=rss",
          "content": "Xinhua News Agency via Getty Images Hisense is perhaps best known for its budget-friendly electronics and appliances, like TVs and refrigerators. But at CES 2025, the China-based company showed its high-end chops with a massive 136-inch micro LED TV, which recently became available for a whopping $100,000. So what's on deck for this year's show? New leadership, for starters. The company has two new hires, including Chief Marketing Officer Sarah Larsen and Chief Commercial Officer James Fishler. In a press release, Hisense said Fishler's experience in home entertainment, appliances and HVAC is important as the company \"builds toward a milestone 2026 and its presence at CES.\" We'll give you a rundown of what to expect during Hisense's presentation and how you can watch it. How to watch Hisense will have a livestream available on Monday, January 5 at 1PM ET on its website. We'll embed the link here once it's available. What to expect With its new hires in place, Hisense is clearly aiming to further polish its brand. Between Fishler and Larsen, the new front office is bringing to bear their experience from such high-powered competitors as LG, Samsung and Beats. And in a recent interview with Tom's Guide, Larsen emphasized a continued focus on the company's fast turnaround time from concept to market as a key differentiator for Hisense. As for actual announcements, while you can expect Hisense to tout its strength in appliances and HVAC systems (really), Larsen's aforementioned interview specifically calls out the emerging RGB TV space as a focus. We expect this year's show will be all about explaining the shades of difference between mini and micro LED display technologies, as both Samsung and LG have already thrown down pre-announcement gauntlets on the latter. Will any of them cost less than six figures? Let's hope Hisense has some good news to share on that front.This article originally appeared on Engadget at https://www.engadget.com/home/how-to-watch-the-hisense-ces-2026-presentation-live-190040090.html?src=rss",
          "feed_position": 14,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2025-12/834aecc0-dcf1-11f0-b7de-13a29302f310"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/orchestration/why-meta-bought-manus-and-what-it-means-for-your-enterprise-ai-agent",
          "published_at": "Tue, 30 Dec 2025 17:00:00 GMT",
          "title": "Why Meta bought Manus — and what it signals for your enterprise AI agent strategy",
          "standfirst": "Facebook and Instagram parent company Meta’s agreement to acquire Manus for more than $2 billion — announced last night by both companies and reported in The Wall Street Journal — marks one of the clearest signals yet that large tech platforms are no longer just competing on model quality, but on who controls the execution layer of AI-powered work.Manus, a Singapore-based startup founded by Chinese entrepreneurs that debuted earlier this year, has built a general-purpose AI agent designed to autonomously carry out multi-step tasks such as research, analysis, coding, planning, and content generation. The company will continue operating from Singapore and selling its subscription product while its team and technology are integrated into Meta’s broader AI organization. Manus co-founder and CEO Xiao Hong, who goes by “Red,” is expected to report to Meta COO Javier Olivan.The deal arrives as Meta accelerates its AI investments to compete with Google, Microsoft, and OpenAI — and as the industry’s focus shifts from conversational demos to systems that can reliably produce artifacts, complete workflows, and operate with minimal supervision.Manus as an execution layer, not a chat interfaceManus has consistently positioned itself less as an assistant and more as an execution engine. Rather than answering isolated prompts, its agent is designed to plan tasks, invoke tools, iterate on intermediate outputs, and deliver finished work.It gained 2 million users on its waitlist alone after unveiling itself in spring 2025. At that time, Manus outperformed OpenAI&#x27;s Deep Research agent (powered then by the o3 model) and other state-of-the-art systems on the GAIA benchmark, which evaluates how well AI agents complete real-world, multi-step tasks, by more than 10% in some cases.And in the acquisition announcement last night, Manus said its system has processed more than 147 trillion tokens and created over 80 million virtual computers, metrics that suggest sustained, production-level usage rather than limited experimentation. Meta, meanwhile, said Manus can independently execute complex tasks such as market research, coding, and data analysis, and confirmed it will continue operating and selling the Manus service while integrating it into Meta AI and other products.For enterprises, this distinction matters. Many early “agent” systems fail not because the underlying models can’t reason, but because execution breaks down: tools fail silently, intermediate steps drift, or long-running tasks can’t be resumed or audited. Manus’s core value proposition is that it manages those failure modes.What Manus users were actually doing with the agentEvidence of that execution-first positioning shows up clearly in Manus’s own community. In the official Manus Discord server, a “Use Case Channel” post shared by a community member named Yesly on March 6, 2025 catalogued real examples of how users were already deploying the agent.Those use cases went far beyond casual prompting. They included:Generating long-form research reports, such as a detailed analysis of climate change impacts on Earth and human society over the next centuryProducing data-driven visual artifacts, including an NBA scoring efficiency four-quadrant chart based on player statisticsConducting product and market research, such as comparing every MacBook model across Apple’s historyPlanning and synthesizing complex, multi-country travel itineraries, complete with budget estimates, accommodations, and a generated travel handbookTackling technical and academic tasks, including summarizing high-temperature superconductivity research, proposing PhD research directions, and outlining simulation-based approaches to room-temperature superconductorsDrafting structured proposals, such as designs for a solar-powered, self-sufficient home with defined geographic coordinates and engineering constraintsEach example was shared as a replayable Manus session, reinforcing that the system wasn’t just generating text, but orchestrating multi-step work to produce finished outputs.This pattern matters because it shows Manus operating in the messy middle ground where enterprise AI often stalls: tasks that are too complex for a single prompt, but too open-ended for rigid automation.Manus&#x27;s recent updatesThe pace at which Manus shipped updates was also impressive, which likely added to its momentum with users and as a ripe acquisition target for Meta.In October, the company released Manus 1.5, an update aimed squarely at where early agent systems tended to break down: long, brittle tasks that lost context or stalled halfway through. Manus re-architected its core agent engine and saw immediate gains. The company said average task completion times dropped from roughly 15 minutes earlier in the year to under four minutes, nearly a fourfold speedup. The system dynamically allocated more reasoning time and compute to harder problems instead of treating every task the same. Manus also expanded the agent’s context windows, enabling it to track longer conversations and more intricate workflows without dropping key details. Together, those changes reduced outright task failures and improved output quality for research-heavy, analytical, and multi-step jobs that previously required frequent human intervention.In December, Manus built on that foundation with version 1.6, extending those execution gains into more autonomous, creative, and platform-spanning work. The release introduced a higher-performance agent tuned to complete more tasks successfully in a single pass, along with new support for mobile application development, not just web-based projects. Users could describe a mobile app and have the agent handle the end-to-end build process, expanding Manus’s reach beyond the browser. At the same time, the agent carried creative objectives across an entire production arc — from research and ideation to drafting, visual creation, revision, and final delivery — within one continuous session.That included generating and editing images through a visual interface, assembling presentations and reports, and building full-stack web applications the agent could launch, test, and fix on its own. Taken together, the updates reinforced Manus’s positioning not as a prompt-driven assistant, but as an execution system designed to stay with a job, adapt when things broke, and reliably deliver finished work across analytical, creative, web, and mobile workflows.Application-layer traction over proprietary modelsNotably, Manus does not train its own frontier model. Reporting on the deal says it relies on third-party AI models from providers including Anthropic and Alibaba, focusing its differentiation on orchestration, reliability, and execution.That hasn’t prevented commercial traction. Yuchen Jin, co-founder and chief technology officer (CTO) of AI cloud GPU-as-a-service provider Hyperbolic Labs, highlighted this dynamic in a public post discussing the acquisition. Jin noted that Manus by its own admission reached roughly $100 million in annual recurring revenue just eight months after launch, despite having no proprietary large language model (LLM) of its own, relying on the aforementioned providers.“People keep assuming a small update from OpenAI or Google will wipe out a lot of AI startups,” Jin wrote. “But in reality, the AI application layer should be where most of the opportunity is.”A similar interpretation came from Dev Shah, lead developer relations at Resemble AI, who argued that Meta didn’t acquire a model company so much as an “environment company” and that “intelligence cannot exist in isolation.\"His point? Agentic capability emerges from how models are coupled with tools, memory, and execution environments — a new concept he described as “Situated Agency.” From that perspective, Manus’s achievement was not training a proprietary foundation model, but engineering an execution layer that allows models like Claude to browse the web, write and run code, manipulate files, and complete multi-step workflows autonomously. Shah suggested this may align more closely with Meta’s long-term strategy: rather than winning the race for state-of-the-art models, Meta could focus on owning the agentic infrastructure — the orchestration, context engineering, and interfaces — and swap in whichever model performs best over time. If that thesis holds, the Manus acquisition signals a shift toward treating foundation models as interchangeable inputs, while the execution environment becomes the primary source of durable value.These perspectives help explain Meta’s move. Rather than buying another model team, it is acquiring a system that has already proven it can package existing models into a product users will pay for — and keep using.What this means for your enterprise AI strategyFor enterprise technical decision-makers, the Manus acquisition is less a vendor endorsement and more a strategic signal.First, it reinforces that orchestration layers — systems that manage planning, tools, retries, memory, and monitoring — are becoming as important as the models themselves. Enterprises building internal AI capabilities may want to invest more heavily in agent infrastructure that sits above models and can survive rapid shifts in the underlying model ecosystem.In that sense, building an internal agent layer is not speculative or redundant. It is exactly the class of software that large platforms now view as strategically valuable — whether as acquisition targets or as internal accelerators.A video recorded ahead of this announcement by VentureBeat founder and CEO Matt Marshall and Red Dragon co-founder Witteveen delves deeper into this subject. Watch it free below or on YouTube.Second, the deal does not automatically mean enterprises should rush to standardize on Manus itself. Meta’s history with enterprise products gives reason for caution. Tools like Workplace by Facebook gained early adoption but ultimately failed to become deeply embedded enterprise platforms, in part due to shifting internal priorities and inconsistent long-term investment.That history suggests a measured approach. Enterprises evaluating Manus today may want to treat it as a pilot or adjunct tool, not a foundational dependency, until Meta’s integration strategy becomes clearer. Key questions include whether Manus remains product-led rather than ad- or data-driven, how governance and compliance evolve under Meta, and whether the roadmap continues to prioritize execution reliability over surface-level integration.Finally, the acquisition sharpens a broader choice facing enterprises: whether to wait for vendors to define the agent layer, or to build and control it themselves. Manus’s trajectory suggests that the real leverage in AI increasingly lives not in who owns the smartest model, but in who owns the systems that turn reasoning into completed work.In that light, Meta’s acquisition is less about Manus alone — and more about where the next durable layer of the AI stack is taking shape.Why this deal matters beyond MetaFrom the perspective of some of us here at VentureBeat, the Manus acquisition is best read as confirmation of where value is consolidating in the AI stack (and Meta’s enterprise AI agent ambitions, though the latter is far less assured.)The defining signal is not that Manus built novel models, but that it demonstrated how quickly well-designed agents can be turned into revenue-generating products by focusing on execution, speed, and concrete outcomes. That shift — from debating what frontier models can do to measuring what agents actually deliver — increasingly frames how AI progress is evaluated.The deal also sharpens an important distinction for enterprise readers: this is not primarily about adopting a Meta-backed product, but about recognizing that agent orchestration has become strategically material. Manus succeeded by targeting tractable, real-world tasks and shipping agents that worked end to end, even if those use cases skewed more consumer-oriented. The broader implication is that enterprises can apply the same approach in their own domains, building agent systems where they already possess data, expertise, and operational leverage.At the same time, we&#x27;re cautious about reading this as a direct enterprise buying signal. Meta’s history suggests that long-term enterprise trust is difficult to earn without sustained focus and specialized go-to-market muscle. Where the acquisition may make more immediate sense is on the consumer and small-business side of Meta’s own ecosystem, particularly within products already designed to manage commerce, content, and customer interaction at scale. Manus’s agentic capabilities map cleanly onto surfaces like Meta Business Suite, where small businesses already juggle content calendars, inboxes, ads, analytics, and monetization tools across Facebook and Instagram. An execution-oriented agent could plausibly automate or coordinate many of those tasks end to end, from drafting and scheduling posts to responding to messages, optimizing ads, or assembling performance reports.Manus&#x27;s \"Design View\" feature, which launched publicly just a week prior to the Meta acquisition announcement and allows users to generate new imagery with editable discrete components using natural language, would seem to be tailor-made for a social network ad creation experience:Beyond creators and small businesses, those agents could plausibly extend to everyday users navigating Instagram or Facebook for shopping, discovery, or personal expression. An execution-oriented agent could assist regular users with tasks such as browsing and comparing products, managing purchases, assembling wish lists, or coordinating returns, while also helping them create and edit posts, reels, or stories for friends and family — not as professional content, but as casual, social, and entertainment-driven output.That framing aligns closely with Meta’s historical strengths. The company has been most successful when AI capabilities are tightly integrated into high-frequency consumer workflows rather than positioned as standalone enterprise software. A Manus-powered agent that helps users do things — shop, create, plan, or manage interactions inside Meta’s apps — would fit naturally into Instagram and Facebook’s evolution toward more agentic experiences. In that scenario, Manus functions less as an enterprise brand and more as an invisible execution layer, powering AI assistants that operate natively within Meta’s consumer ecosystem, where scale, engagement, and commerce already converge.As a result, the acquisition’s clearest relevance is not whether enterprises should standardize on Manus, but that investments in internal agent frameworks, orchestration layers, and governance now appear increasingly well-justified — because that is precisely the layer large platforms are now willing to pay for.",
          "content": "Facebook and Instagram parent company Meta’s agreement to acquire Manus for more than $2 billion — announced last night by both companies and reported in The Wall Street Journal — marks one of the clearest signals yet that large tech platforms are no longer just competing on model quality, but on who controls the execution layer of AI-powered work.Manus, a Singapore-based startup founded by Chinese entrepreneurs that debuted earlier this year, has built a general-purpose AI agent designed to autonomously carry out multi-step tasks such as research, analysis, coding, planning, and content generation. The company will continue operating from Singapore and selling its subscription product while its team and technology are integrated into Meta’s broader AI organization. Manus co-founder and CEO Xiao Hong, who goes by “Red,” is expected to report to Meta COO Javier Olivan.The deal arrives as Meta accelerates its AI investments to compete with Google, Microsoft, and OpenAI — and as the industry’s focus shifts from conversational demos to systems that can reliably produce artifacts, complete workflows, and operate with minimal supervision.Manus as an execution layer, not a chat interfaceManus has consistently positioned itself less as an assistant and more as an execution engine. Rather than answering isolated prompts, its agent is designed to plan tasks, invoke tools, iterate on intermediate outputs, and deliver finished work.It gained 2 million users on its waitlist alone after unveiling itself in spring 2025. At that time, Manus outperformed OpenAI&#x27;s Deep Research agent (powered then by the o3 model) and other state-of-the-art systems on the GAIA benchmark, which evaluates how well AI agents complete real-world, multi-step tasks, by more than 10% in some cases.And in the acquisition announcement last night, Manus said its system has processed more than 147 trillion tokens and created over 80 million virtual computers, metrics that suggest sustained, production-level usage rather than limited experimentation. Meta, meanwhile, said Manus can independently execute complex tasks such as market research, coding, and data analysis, and confirmed it will continue operating and selling the Manus service while integrating it into Meta AI and other products.For enterprises, this distinction matters. Many early “agent” systems fail not because the underlying models can’t reason, but because execution breaks down: tools fail silently, intermediate steps drift, or long-running tasks can’t be resumed or audited. Manus’s core value proposition is that it manages those failure modes.What Manus users were actually doing with the agentEvidence of that execution-first positioning shows up clearly in Manus’s own community. In the official Manus Discord server, a “Use Case Channel” post shared by a community member named Yesly on March 6, 2025 catalogued real examples of how users were already deploying the agent.Those use cases went far beyond casual prompting. They included:Generating long-form research reports, such as a detailed analysis of climate change impacts on Earth and human society over the next centuryProducing data-driven visual artifacts, including an NBA scoring efficiency four-quadrant chart based on player statisticsConducting product and market research, such as comparing every MacBook model across Apple’s historyPlanning and synthesizing complex, multi-country travel itineraries, complete with budget estimates, accommodations, and a generated travel handbookTackling technical and academic tasks, including summarizing high-temperature superconductivity research, proposing PhD research directions, and outlining simulation-based approaches to room-temperature superconductorsDrafting structured proposals, such as designs for a solar-powered, self-sufficient home with defined geographic coordinates and engineering constraintsEach example was shared as a replayable Manus session, reinforcing that the system wasn’t just generating text, but orchestrating multi-step work to produce finished outputs.This pattern matters because it shows Manus operating in the messy middle ground where enterprise AI often stalls: tasks that are too complex for a single prompt, but too open-ended for rigid automation.Manus&#x27;s recent updatesThe pace at which Manus shipped updates was also impressive, which likely added to its momentum with users and as a ripe acquisition target for Meta.In October, the company released Manus 1.5, an update aimed squarely at where early agent systems tended to break down: long, brittle tasks that lost context or stalled halfway through. Manus re-architected its core agent engine and saw immediate gains. The company said average task completion times dropped from roughly 15 minutes earlier in the year to under four minutes, nearly a fourfold speedup. The system dynamically allocated more reasoning time and compute to harder problems instead of treating every task the same. Manus also expanded the agent’s context windows, enabling it to track longer conversations and more intricate workflows without dropping key details. Together, those changes reduced outright task failures and improved output quality for research-heavy, analytical, and multi-step jobs that previously required frequent human intervention.In December, Manus built on that foundation with version 1.6, extending those execution gains into more autonomous, creative, and platform-spanning work. The release introduced a higher-performance agent tuned to complete more tasks successfully in a single pass, along with new support for mobile application development, not just web-based projects. Users could describe a mobile app and have the agent handle the end-to-end build process, expanding Manus’s reach beyond the browser. At the same time, the agent carried creative objectives across an entire production arc — from research and ideation to drafting, visual creation, revision, and final delivery — within one continuous session.That included generating and editing images through a visual interface, assembling presentations and reports, and building full-stack web applications the agent could launch, test, and fix on its own. Taken together, the updates reinforced Manus’s positioning not as a prompt-driven assistant, but as an execution system designed to stay with a job, adapt when things broke, and reliably deliver finished work across analytical, creative, web, and mobile workflows.Application-layer traction over proprietary modelsNotably, Manus does not train its own frontier model. Reporting on the deal says it relies on third-party AI models from providers including Anthropic and Alibaba, focusing its differentiation on orchestration, reliability, and execution.That hasn’t prevented commercial traction. Yuchen Jin, co-founder and chief technology officer (CTO) of AI cloud GPU-as-a-service provider Hyperbolic Labs, highlighted this dynamic in a public post discussing the acquisition. Jin noted that Manus by its own admission reached roughly $100 million in annual recurring revenue just eight months after launch, despite having no proprietary large language model (LLM) of its own, relying on the aforementioned providers.“People keep assuming a small update from OpenAI or Google will wipe out a lot of AI startups,” Jin wrote. “But in reality, the AI application layer should be where most of the opportunity is.”A similar interpretation came from Dev Shah, lead developer relations at Resemble AI, who argued that Meta didn’t acquire a model company so much as an “environment company” and that “intelligence cannot exist in isolation.\"His point? Agentic capability emerges from how models are coupled with tools, memory, and execution environments — a new concept he described as “Situated Agency.” From that perspective, Manus’s achievement was not training a proprietary foundation model, but engineering an execution layer that allows models like Claude to browse the web, write and run code, manipulate files, and complete multi-step workflows autonomously. Shah suggested this may align more closely with Meta’s long-term strategy: rather than winning the race for state-of-the-art models, Meta could focus on owning the agentic infrastructure — the orchestration, context engineering, and interfaces — and swap in whichever model performs best over time. If that thesis holds, the Manus acquisition signals a shift toward treating foundation models as interchangeable inputs, while the execution environment becomes the primary source of durable value.These perspectives help explain Meta’s move. Rather than buying another model team, it is acquiring a system that has already proven it can package existing models into a product users will pay for — and keep using.What this means for your enterprise AI strategyFor enterprise technical decision-makers, the Manus acquisition is less a vendor endorsement and more a strategic signal.First, it reinforces that orchestration layers — systems that manage planning, tools, retries, memory, and monitoring — are becoming as important as the models themselves. Enterprises building internal AI capabilities may want to invest more heavily in agent infrastructure that sits above models and can survive rapid shifts in the underlying model ecosystem.In that sense, building an internal agent layer is not speculative or redundant. It is exactly the class of software that large platforms now view as strategically valuable — whether as acquisition targets or as internal accelerators.A video recorded ahead of this announcement by VentureBeat founder and CEO Matt Marshall and Red Dragon co-founder Witteveen delves deeper into this subject. Watch it free below or on YouTube.Second, the deal does not automatically mean enterprises should rush to standardize on Manus itself. Meta’s history with enterprise products gives reason for caution. Tools like Workplace by Facebook gained early adoption but ultimately failed to become deeply embedded enterprise platforms, in part due to shifting internal priorities and inconsistent long-term investment.That history suggests a measured approach. Enterprises evaluating Manus today may want to treat it as a pilot or adjunct tool, not a foundational dependency, until Meta’s integration strategy becomes clearer. Key questions include whether Manus remains product-led rather than ad- or data-driven, how governance and compliance evolve under Meta, and whether the roadmap continues to prioritize execution reliability over surface-level integration.Finally, the acquisition sharpens a broader choice facing enterprises: whether to wait for vendors to define the agent layer, or to build and control it themselves. Manus’s trajectory suggests that the real leverage in AI increasingly lives not in who owns the smartest model, but in who owns the systems that turn reasoning into completed work.In that light, Meta’s acquisition is less about Manus alone — and more about where the next durable layer of the AI stack is taking shape.Why this deal matters beyond MetaFrom the perspective of some of us here at VentureBeat, the Manus acquisition is best read as confirmation of where value is consolidating in the AI stack (and Meta’s enterprise AI agent ambitions, though the latter is far less assured.)The defining signal is not that Manus built novel models, but that it demonstrated how quickly well-designed agents can be turned into revenue-generating products by focusing on execution, speed, and concrete outcomes. That shift — from debating what frontier models can do to measuring what agents actually deliver — increasingly frames how AI progress is evaluated.The deal also sharpens an important distinction for enterprise readers: this is not primarily about adopting a Meta-backed product, but about recognizing that agent orchestration has become strategically material. Manus succeeded by targeting tractable, real-world tasks and shipping agents that worked end to end, even if those use cases skewed more consumer-oriented. The broader implication is that enterprises can apply the same approach in their own domains, building agent systems where they already possess data, expertise, and operational leverage.At the same time, we&#x27;re cautious about reading this as a direct enterprise buying signal. Meta’s history suggests that long-term enterprise trust is difficult to earn without sustained focus and specialized go-to-market muscle. Where the acquisition may make more immediate sense is on the consumer and small-business side of Meta’s own ecosystem, particularly within products already designed to manage commerce, content, and customer interaction at scale. Manus’s agentic capabilities map cleanly onto surfaces like Meta Business Suite, where small businesses already juggle content calendars, inboxes, ads, analytics, and monetization tools across Facebook and Instagram. An execution-oriented agent could plausibly automate or coordinate many of those tasks end to end, from drafting and scheduling posts to responding to messages, optimizing ads, or assembling performance reports.Manus&#x27;s \"Design View\" feature, which launched publicly just a week prior to the Meta acquisition announcement and allows users to generate new imagery with editable discrete components using natural language, would seem to be tailor-made for a social network ad creation experience:Beyond creators and small businesses, those agents could plausibly extend to everyday users navigating Instagram or Facebook for shopping, discovery, or personal expression. An execution-oriented agent could assist regular users with tasks such as browsing and comparing products, managing purchases, assembling wish lists, or coordinating returns, while also helping them create and edit posts, reels, or stories for friends and family — not as professional content, but as casual, social, and entertainment-driven output.That framing aligns closely with Meta’s historical strengths. The company has been most successful when AI capabilities are tightly integrated into high-frequency consumer workflows rather than positioned as standalone enterprise software. A Manus-powered agent that helps users do things — shop, create, plan, or manage interactions inside Meta’s apps — would fit naturally into Instagram and Facebook’s evolution toward more agentic experiences. In that scenario, Manus functions less as an enterprise brand and more as an invisible execution layer, powering AI assistants that operate natively within Meta’s consumer ecosystem, where scale, engagement, and commerce already converge.As a result, the acquisition’s clearest relevance is not whether enterprises should standardize on Manus, but that investments in internal agent frameworks, orchestration layers, and governance now appear increasingly well-justified — because that is precisely the layer large platforms are now willing to pay for.",
          "feed_position": 2,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/7nVD4zIrY5Lmnpu1c0lGBG/c788567a8bc13386445b652f6bcbdde6/IQnl4wLLz4s3ciS8747gk.png?w=300&q=30"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/security/machine-identities-outnumber-humans-82-to-1-legacy-iam-cant-keep-up",
          "published_at": "Tue, 30 Dec 2025 14:00:00 GMT",
          "title": "Legacy IAM was built for humans — and AI agents now outnumber them 82 to 1",
          "standfirst": "Active Directory, LDAP, and early PAM were built for humans. AI agents and machines were the exception. Today, they outnumber people 82 to 1, and that human-first identity model is breaking down at machine speed.AI agents are the fastest-growing and least-governed class of these machine identities — and they don’t just authenticate, they act. ServiceNow spent roughly $11.6 billion on security acquisitions in 2025 alone — a signal that identity, not models, is becoming the control plane for enterprise AI risk.CyberArk&#x27;s 2025 research confirms what security teams and AI builders have long suspected: Machine identities now outnumber humans by a wide margin. Microsoft Copilot Studio users created over 1 million AI agents in a single quarter, up 130% from the previous period. Gartner predicts that by 2028, 25% of enterprise breaches will trace back to AI agent abuse.Why legacy architectures fail at machine scaleBuilders don’t create shadow agents or over-permissioned service accounts out of negligence. They do it because cloud IAM is slow, security reviews don’t map cleanly to agent workflows, and production pressure rewards speed over precision. Static credentials become the path of least resistance — until they become the breach vector.Gartner analysts explain the core problem in a report published in May: \"Traditional IAM approaches, designed for human users, fall short of addressing the unique requirements of machines, such as devices and workloads.\"Their research identifies why retrofitting fails: \"Retrofitting human IAM approaches to fit machine IAM use cases leads to fragmented and ineffective management of machine identities, running afoul of regulatory mandates and exposing the organization to unnecessary risks.\"The governance gap is stark. CyberArk&#x27;s 2025 Identity Security Landscape survey of 2,600 security decision-makers reveals a dangerous disconnect: Though machine identities now outnumber humans 82 to 1, 88% of organizations still define only human identities as \"privileged users.\" The result is that machine identities actually have higher rates of sensitive access than humans.That 42% figure represents millions of API keys, service accounts, and automated processes with access to crown jewels, all governed by policies designed for employees who clock in and out.The visibility gap compounds the problem. A Gartner survey of 335 IAM leaders found that IAM teams are only responsible for 44% of an organization&#x27;s machine identities, meaning the majority operate outside security&#x27;s visibility. Without a cohesive machine IAM strategy, Gartner warns, \"organizations risk compromising the security and integrity of their IT infrastructure.\"The Gartner Leaders&#x27; Guide explains why legacy service accounts create systemic risk: They persist after the workloads they support disappear, leaving orphaned credentials with no clear owner or lifecycle.In several enterprise breaches investigated in 2024, attackers didn’t compromise models or endpoints. They reused long-lived API keys tied to abandoned automation workflows — keys no one realized were still active because the agent that created them no longer existed.Elia Zaitsev, CrowdStrike&#x27;s CTO, explained why attackers have shifted away from endpoints and toward identity in a recent VentureBeat interview: \"Cloud, identity and remote management tools and legitimate credentials are where the adversary has been moving because it&#x27;s too hard to operate unconstrained on the endpoint. Why try to bypass and deal with a sophisticated platform like CrowdStrike on the endpoint when you could log in as an admin user?\"Why agentic AI breaks identity assumptionsThe emergence of AI agents requiring their own credentials introduces a category of machine identity that legacy systems never anticipated or were designed for. Gartner&#x27;s researchers specifically call out agentic AI as a critical use case: \"AI agents require credentials to interact with other systems. In some instances, they use delegated human credentials, while in others, they operate with their own credentials. These credentials must be meticulously scoped to adhere to the principle of least privilege.\"The researchers also cite the Model Context Protocol (MCP) as an example of this challenge, the same protocol security researchers have flagged for its lack of built-in authentication. MCP isn’t just missing authentication — it collapses traditional identity boundaries by allowing agents to traverse data and tools without a stable, auditable identity surface.The governance problem compounds when organizations deploy multiple GenAI tools simultaneously. Security teams need visibility into which AI integrations have action capabilities, including the ability to execute tasks, not just generate text, and whether those capabilities have been scoped appropriately. Platforms that unify identity, endpoint, and cloud telemetry are emerging as the only viable way to detect agent abuse in real time. Fragmented point tools simply can’t keep up with machine-speed lateral movement.Machine-to-machine interactions already operate at a scale and speed human governance models were never designed to handle.Getting ahead of dynamic service identity shifts Gartner&#x27;s research points to dynamic service identities as the path forward. They’re defined as being ephemeral, tightly scoped, policy-driven credentials that drastically reduce the attack surface. Because of this, Gartner is advising that security leaders \"move to a dynamic service identity model, rather than defaulting to a legacy service account model. Dynamic service identities do not require separate accounts to be created, thus reducing management overhead and the attack surface.\"The ultimate objective is achieving just-in-time access and zero standing privileges. Platforms that unify identity, endpoint, and cloud telemetry are increasingly the only viable way to detect and contain agent abuse across the full identity attack chain.Practical steps security and AI builders can take today The organizations getting agentic identity right are treating it as a collaboration problem between security teams and AI builders. Based on Gartner&#x27;s Leaders&#x27; Guide, OpenID Foundation guidance, and vendor best practices, these priorities are emerging for enterprises deploying AI agents.Conduct a comprehensive discovery and audit of every account and credential first. It’s a good idea to get a baseline in place first to see how many accounts and credentials are in use across all machines in IT. CISOs and security leaders tell VentureBeat that this often turns up between six and ten times more identities than the security team had known about before the audit. One hotel chain found that it had been tracking only a tenth of its machine identities before the audit.Build and tightly manage agent inventory before production. Being on top of this makes sure AI builders know what they&#x27;re deploying and security teams know what they need to track. When there is too much of a gap between those functions, it&#x27;s easier for shadow agents to get created, evading governance in the process. A shared registry should track ownership, permissions, data access, and API connections for every agentic identity before agents reach production environments.Go all in on dynamic service identities and excel at them. Transition from static service accounts to cloud-native alternatives like AWS IAM roles, Azure managed identities, or Kubernetes service accounts. These identities are ephemeral and need to be tightly scoped, managed and policy-driven. The goal is to excel at compliance while providing AI builders the identities they need to get apps built.Implement just-in-time credentials over static secrets. Integrating just-in-time credential provisioning, automatic secret rotation, and least-privilege defaults into CI/CD pipelines and agent frameworks is critical. These are all foundational elements of zero trust that need to be core to devops pipelines. Take the advice of seasoned security leaders defending AI builders, who often tell VentureBeat to pass along the advice of never trusting perimeter security with any AI devops workflows or CI/CD processes. Go big on zero trust and identity security when it comes to protecting AI builders’ workflows.Establish auditable delegation chains. When agents spawn sub-agents or invoke external APIs, authorization chains become hard to track. Make sure humans are accountable for all services, which include AI agents. Enterprises need behavioral baselines and real-time drift detection to maintain accountability.Deploy continuous monitoring. In keeping with the precepts of zero trust, continuously monitor every use of machine credentials with the deliberate goal of excelling at observability. This includes auditing as it helps detect anomalous activities such as unauthorized privilege escalation and lateral movement. Evaluate posture management. Assess potential exploitation pathways, the extent of possible damage (blast radius), and any shadow admin access. This involves removing unnecessary or outdated access and identifying misconfigurations that attackers could exploit. Start enforcing agent lifecycle management. Every agent needs human oversight, whether as part of a group of agents or in the context of an agent-based workflow. When AI builders move to new projects, their agents should trigger the same offboarding workflows as departing employees. Orphaned agents with standing privileges can become breach vectors.Prioritize unified platforms over point solutions. Fragmented tools create fragmented visibility. Platforms that unify identity, endpoint, and cloud security give AI builders self-service visibility while giving security teams cross-domain detection.Expect to see the gap widen in 2026The gap between what AI builders deploy and what security teams can govern keeps widening. Every major technology transition has, unfortunately, also led to another generation of security breaches often forcing its own unique industry-wide reckoning. Just as hybrid cloud misconfigurations, shadow AI, and API sprawl continue to challenge security leaders and the AI builders they support, 2026 will see the gap widen between what can be contained when it comes to machine identity attacks and what needs to improve to stop determined adversaries.The 82-to-1 ratio isn&#x27;t static. It&#x27;s accelerating. Organizations that continue relying on human-first IAM architectures aren&#x27;t just accepting technical debt; they&#x27;re building security models that grow weaker with every new agent deployed.Agentic AI doesn’t break security because it’s intelligent — it breaks security because it multiplies identity faster than governance can follow. Turning what for many organizations is one of their most glaring security weaknesses into a strength starts by realizing that perimeter-based, legacy identity security is no match for the intensity, speed, and scale of machine-on-machine attacks that are the new normal and will proliferate in 2026.",
          "content": "Active Directory, LDAP, and early PAM were built for humans. AI agents and machines were the exception. Today, they outnumber people 82 to 1, and that human-first identity model is breaking down at machine speed.AI agents are the fastest-growing and least-governed class of these machine identities — and they don’t just authenticate, they act. ServiceNow spent roughly $11.6 billion on security acquisitions in 2025 alone — a signal that identity, not models, is becoming the control plane for enterprise AI risk.CyberArk&#x27;s 2025 research confirms what security teams and AI builders have long suspected: Machine identities now outnumber humans by a wide margin. Microsoft Copilot Studio users created over 1 million AI agents in a single quarter, up 130% from the previous period. Gartner predicts that by 2028, 25% of enterprise breaches will trace back to AI agent abuse.Why legacy architectures fail at machine scaleBuilders don’t create shadow agents or over-permissioned service accounts out of negligence. They do it because cloud IAM is slow, security reviews don’t map cleanly to agent workflows, and production pressure rewards speed over precision. Static credentials become the path of least resistance — until they become the breach vector.Gartner analysts explain the core problem in a report published in May: \"Traditional IAM approaches, designed for human users, fall short of addressing the unique requirements of machines, such as devices and workloads.\"Their research identifies why retrofitting fails: \"Retrofitting human IAM approaches to fit machine IAM use cases leads to fragmented and ineffective management of machine identities, running afoul of regulatory mandates and exposing the organization to unnecessary risks.\"The governance gap is stark. CyberArk&#x27;s 2025 Identity Security Landscape survey of 2,600 security decision-makers reveals a dangerous disconnect: Though machine identities now outnumber humans 82 to 1, 88% of organizations still define only human identities as \"privileged users.\" The result is that machine identities actually have higher rates of sensitive access than humans.That 42% figure represents millions of API keys, service accounts, and automated processes with access to crown jewels, all governed by policies designed for employees who clock in and out.The visibility gap compounds the problem. A Gartner survey of 335 IAM leaders found that IAM teams are only responsible for 44% of an organization&#x27;s machine identities, meaning the majority operate outside security&#x27;s visibility. Without a cohesive machine IAM strategy, Gartner warns, \"organizations risk compromising the security and integrity of their IT infrastructure.\"The Gartner Leaders&#x27; Guide explains why legacy service accounts create systemic risk: They persist after the workloads they support disappear, leaving orphaned credentials with no clear owner or lifecycle.In several enterprise breaches investigated in 2024, attackers didn’t compromise models or endpoints. They reused long-lived API keys tied to abandoned automation workflows — keys no one realized were still active because the agent that created them no longer existed.Elia Zaitsev, CrowdStrike&#x27;s CTO, explained why attackers have shifted away from endpoints and toward identity in a recent VentureBeat interview: \"Cloud, identity and remote management tools and legitimate credentials are where the adversary has been moving because it&#x27;s too hard to operate unconstrained on the endpoint. Why try to bypass and deal with a sophisticated platform like CrowdStrike on the endpoint when you could log in as an admin user?\"Why agentic AI breaks identity assumptionsThe emergence of AI agents requiring their own credentials introduces a category of machine identity that legacy systems never anticipated or were designed for. Gartner&#x27;s researchers specifically call out agentic AI as a critical use case: \"AI agents require credentials to interact with other systems. In some instances, they use delegated human credentials, while in others, they operate with their own credentials. These credentials must be meticulously scoped to adhere to the principle of least privilege.\"The researchers also cite the Model Context Protocol (MCP) as an example of this challenge, the same protocol security researchers have flagged for its lack of built-in authentication. MCP isn’t just missing authentication — it collapses traditional identity boundaries by allowing agents to traverse data and tools without a stable, auditable identity surface.The governance problem compounds when organizations deploy multiple GenAI tools simultaneously. Security teams need visibility into which AI integrations have action capabilities, including the ability to execute tasks, not just generate text, and whether those capabilities have been scoped appropriately. Platforms that unify identity, endpoint, and cloud telemetry are emerging as the only viable way to detect agent abuse in real time. Fragmented point tools simply can’t keep up with machine-speed lateral movement.Machine-to-machine interactions already operate at a scale and speed human governance models were never designed to handle.Getting ahead of dynamic service identity shifts Gartner&#x27;s research points to dynamic service identities as the path forward. They’re defined as being ephemeral, tightly scoped, policy-driven credentials that drastically reduce the attack surface. Because of this, Gartner is advising that security leaders \"move to a dynamic service identity model, rather than defaulting to a legacy service account model. Dynamic service identities do not require separate accounts to be created, thus reducing management overhead and the attack surface.\"The ultimate objective is achieving just-in-time access and zero standing privileges. Platforms that unify identity, endpoint, and cloud telemetry are increasingly the only viable way to detect and contain agent abuse across the full identity attack chain.Practical steps security and AI builders can take today The organizations getting agentic identity right are treating it as a collaboration problem between security teams and AI builders. Based on Gartner&#x27;s Leaders&#x27; Guide, OpenID Foundation guidance, and vendor best practices, these priorities are emerging for enterprises deploying AI agents.Conduct a comprehensive discovery and audit of every account and credential first. It’s a good idea to get a baseline in place first to see how many accounts and credentials are in use across all machines in IT. CISOs and security leaders tell VentureBeat that this often turns up between six and ten times more identities than the security team had known about before the audit. One hotel chain found that it had been tracking only a tenth of its machine identities before the audit.Build and tightly manage agent inventory before production. Being on top of this makes sure AI builders know what they&#x27;re deploying and security teams know what they need to track. When there is too much of a gap between those functions, it&#x27;s easier for shadow agents to get created, evading governance in the process. A shared registry should track ownership, permissions, data access, and API connections for every agentic identity before agents reach production environments.Go all in on dynamic service identities and excel at them. Transition from static service accounts to cloud-native alternatives like AWS IAM roles, Azure managed identities, or Kubernetes service accounts. These identities are ephemeral and need to be tightly scoped, managed and policy-driven. The goal is to excel at compliance while providing AI builders the identities they need to get apps built.Implement just-in-time credentials over static secrets. Integrating just-in-time credential provisioning, automatic secret rotation, and least-privilege defaults into CI/CD pipelines and agent frameworks is critical. These are all foundational elements of zero trust that need to be core to devops pipelines. Take the advice of seasoned security leaders defending AI builders, who often tell VentureBeat to pass along the advice of never trusting perimeter security with any AI devops workflows or CI/CD processes. Go big on zero trust and identity security when it comes to protecting AI builders’ workflows.Establish auditable delegation chains. When agents spawn sub-agents or invoke external APIs, authorization chains become hard to track. Make sure humans are accountable for all services, which include AI agents. Enterprises need behavioral baselines and real-time drift detection to maintain accountability.Deploy continuous monitoring. In keeping with the precepts of zero trust, continuously monitor every use of machine credentials with the deliberate goal of excelling at observability. This includes auditing as it helps detect anomalous activities such as unauthorized privilege escalation and lateral movement. Evaluate posture management. Assess potential exploitation pathways, the extent of possible damage (blast radius), and any shadow admin access. This involves removing unnecessary or outdated access and identifying misconfigurations that attackers could exploit. Start enforcing agent lifecycle management. Every agent needs human oversight, whether as part of a group of agents or in the context of an agent-based workflow. When AI builders move to new projects, their agents should trigger the same offboarding workflows as departing employees. Orphaned agents with standing privileges can become breach vectors.Prioritize unified platforms over point solutions. Fragmented tools create fragmented visibility. Platforms that unify identity, endpoint, and cloud security give AI builders self-service visibility while giving security teams cross-domain detection.Expect to see the gap widen in 2026The gap between what AI builders deploy and what security teams can govern keeps widening. Every major technology transition has, unfortunately, also led to another generation of security breaches often forcing its own unique industry-wide reckoning. Just as hybrid cloud misconfigurations, shadow AI, and API sprawl continue to challenge security leaders and the AI builders they support, 2026 will see the gap widen between what can be contained when it comes to machine identity attacks and what needs to improve to stop determined adversaries.The 82-to-1 ratio isn&#x27;t static. It&#x27;s accelerating. Organizations that continue relying on human-first IAM architectures aren&#x27;t just accepting technical debt; they&#x27;re building security models that grow weaker with every new agent deployed.Agentic AI doesn’t break security because it’s intelligent — it breaks security because it multiplies identity faster than governance can follow. Turning what for many organizations is one of their most glaring security weaknesses into a strength starts by realizing that perimeter-based, legacy identity security is no match for the intensity, speed, and scale of machine-on-machine attacks that are the new normal and will proliferate in 2026.",
          "feed_position": 3,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/7vyBAayF7Y0KIG0vH8a7HW/fbd69ce8561899e446950b1e6ad38fc0/hero_for_identity_article.jpg?w=300&q=30"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/computing/how-to-watch-the-nvidia-ces-2026-presentation-live-130028065.html",
          "published_at": "Tue, 30 Dec 2025 13:00:28 +0000",
          "title": "How to watch the NVIDIA CES 2026 presentation live",
          "standfirst": "CFOTO via Getty Images At CES 2025, NVIDIA spent the bulk of its presentation touting (what else) its leading position in the artificial intelligence arena. But it also spent time talking up new hardware, including its RTX 5000-series GPUs and Project Digits desktop supercomputer (later redubbed Spark). This year, the company's website says it's \"lighting up CES 2026 with the power of AI.\" To that end, NVIDIA is pulling out all the stops at its Vegas installation, promising hands-on demos in its booth at the Fontainebleau, replete with the \"latest NVIDIA solutions driving innovation and productivity across industries.\" But don't worry if you're not on the ground in Vegas. Here's how you can watch the livestream of the company's January 5 press conference, and what to expect from NVIDIA at CES this year. How to watch the NVIDIA CES 2026 keynote NVIDIA CEO Jensen Huang will deliver a 90-minute keynote at CES 2026. The event will be livestreamed on January 5 at 4PM ET via NVIDIA's website (and likely on YouTube as well). We'll embed the link here once it's available. What to expect NVIDIA's game plan for CES is suitably vague so far, including \"cutting-edge AI, robotics, simulation, gaming and content creation at the NVIDIA Showcase.\" It also notes there will be more than 20 demos. Although we're unsure if all of these will be shown during the keynote, we can at least expect to see them throughout the week of CES. Given NVIDIA's sky-high valuation and the fact that the health of the US and global economy seems increasingly linked to infrastructure spending on AI data centers – largely powered by chips from NVIDIA and its competitors — expect Huang's remarks to be as closely followed by Wall Street investors as technology acolytes, if not more so. Will we get any insight on a successor to the company's Blackwell chip? A more detailed look at how NVIDIA's partners are applying AI to real-world robotics? Time will tell, but you might want to keep your stock portfolio in a split screen while taking in Huang's presentation.This article originally appeared on Engadget at https://www.engadget.com/computing/how-to-watch-the-nvidia-ces-2026-presentation-live-130028065.html?src=rss",
          "content": "CFOTO via Getty Images At CES 2025, NVIDIA spent the bulk of its presentation touting (what else) its leading position in the artificial intelligence arena. But it also spent time talking up new hardware, including its RTX 5000-series GPUs and Project Digits desktop supercomputer (later redubbed Spark). This year, the company's website says it's \"lighting up CES 2026 with the power of AI.\" To that end, NVIDIA is pulling out all the stops at its Vegas installation, promising hands-on demos in its booth at the Fontainebleau, replete with the \"latest NVIDIA solutions driving innovation and productivity across industries.\" But don't worry if you're not on the ground in Vegas. Here's how you can watch the livestream of the company's January 5 press conference, and what to expect from NVIDIA at CES this year. How to watch the NVIDIA CES 2026 keynote NVIDIA CEO Jensen Huang will deliver a 90-minute keynote at CES 2026. The event will be livestreamed on January 5 at 4PM ET via NVIDIA's website (and likely on YouTube as well). We'll embed the link here once it's available. What to expect NVIDIA's game plan for CES is suitably vague so far, including \"cutting-edge AI, robotics, simulation, gaming and content creation at the NVIDIA Showcase.\" It also notes there will be more than 20 demos. Although we're unsure if all of these will be shown during the keynote, we can at least expect to see them throughout the week of CES. Given NVIDIA's sky-high valuation and the fact that the health of the US and global economy seems increasingly linked to infrastructure spending on AI data centers – largely powered by chips from NVIDIA and its competitors — expect Huang's remarks to be as closely followed by Wall Street investors as technology acolytes, if not more so. Will we get any insight on a successor to the company's Blackwell chip? A more detailed look at how NVIDIA's partners are applying AI to real-world robotics? Time will tell, but you might want to keep your stock portfolio in a split screen while taking in Huang's presentation.This article originally appeared on Engadget at https://www.engadget.com/computing/how-to-watch-the-nvidia-ces-2026-presentation-live-130028065.html?src=rss",
          "feed_position": 20,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2025-12/b0333b00-dcee-11f0-a993-97c527ebca4b"
        },
        {
          "source": "Engadget",
          "url": "https://www.engadget.com/general/the-morning-after-engadget-newsletter-121511445.html",
          "published_at": "Tue, 30 Dec 2025 12:15:11 +0000",
          "title": "The Morning After: What to expect at CES 2026",
          "standfirst": "CES 2026 is right around the corner, and the pre-show hype cycle/ early reveals suggest, yes, there’s going to be an awful lot of AI-powered insert-product-category-here alongside, thankfully, some major announcements from the likes of Intel, Sony and NVIDIA. Intel is finally unveiling its Panther Lake (Core Ultra Series 3) chips. The first chips built on Intel’s 2nm process could offer a 50 percent performance boost, which is sorely needed amid intense competition. NVIDIA’s Jensen Huang is taking the stage for a keynote expected to feature a lot of AI hype, while AMD’s Lisa Su will likely counter with new Ryzen 9000-series chips and the latest on AI upscaling tech. LG Over the years, CES has consistently been the show for TV innovation and heady next-gen displays. This year, we’ll be talking a lot about Micro RGB. LG is introducing a new Micro RGB Evo panel with over 1,000 dimming zones, while Samsung plans to launch a full range of Micro RGB TVs from 55 inches to 115 inches. In 2025, Sony introduced a new RGB LED panel that uses individual Mini LED backlights in red, green and blue to produce even brighter, more accurate colors. The company has trademarked “True RGB,” which could become what Sony calls its spin on RGB displays. We’ll be covering all the keynotes, press conferences and big reveals in person. And figuring which of the 100s of AI-branded devices and gadgets are worth reporting on. I’m also taking bets on the most niche celebrity appearance/endorsement at CES 2026. We’ve seen 50 Cent, Big Bird, Martha Stewart, Archbishop Desmond Tutu, Guillermo del Toro, Justin Bieber and will.i.am (multiple times), so who will join this pantheon of stars? — Mat Smith The other big stories this morning CD Projekt co-founder has acquired GOG, the company’s game storefront 2025 was the year Xbox died NASA finally has a leader, but its future is no more certain Samsung’s two new speakers will deliver crisp audio while blending into your decor The Music Studio 5 and 7 will be on display at CES 2026. Samsung It isn’t just TVs with Samsung. The company has already teased a pair of new understated speakers. Likely inspired by the Samsung Frame, the new Wi-Fi speakers, called the Music Studio 5 and 7, blend into your living room. The Music Studio 5 has a four-inch woofer and dual tweeters, with a built-in waveguide to deliver better sound. The Music Studio 7 comes with a 3.1.1-channel spatial audio with top-, front-, left- and right-firing speakers. No prices yet. Expect to hear more at CES itself or once the speakers arrive in stores. And as the press image above suggests, we can't wait to sit stoically in front of one, with a glass of water (?). Continue reading. Xiaomi’s 17 Ultra Leica Edition smartphone has a manual zoom ring And a 1-inch sensor, 200MP telephoto camera and 3,500 nit display. Xiaomi Xiaomi’s latest smartphone is once again a spec beast. It features a 1-inch sensor 50MP f/1.67 main camera and 1/1.4-inch 200MP periscope telephoto camera. And it also has an interesting new mechanical feature: a manual zoom ring. This surrounds the rear camera unit. Both the regular Xiaomi 17 Ultra and Leica edition come with a Snapdragon 8 Elite Gen 5 SoC with up to 16GB of LPDDR5X RAM and 1TB of UFS 4.1 storage, along with a 6.9-inch 120Hz AMOLED display that can hit up to 3,500 nits of peak brightness. But the camera features are the standout elements. The 17 Ultra by Leica adds some very, well, Leica touches: a two-tone finish, red dot status symbol on the front, textured edges and film simulations, like Leica’s Monopan 50 black and white. Xiaomi says the zoom ring “[eliminates] the need for tedious screen taps... and can detect displacements as small as 0.03mm.” It can also be reprogrammed for manual focus. Xiaomi’s 17 Ultra by Leica and the regular 17 Ultra start at CNY 7,999 ($1,140) and CNY 6,999 ($995), on par with the latest high-end Pixel 10s and Galaxy S25s. Continue reading. You may soon be able to change your Gmail address A Google support page in Hindi says the feature is ‘gradually rolling out to all users.’ A Google support page in Hindi indicates the ability to change your Gmail address might be coming. The feature would allow you to replace your current @gmail.com address with another. Your old address would remain active as an alias on the account, and all your data would stick around, unaffected. The support page (translated) says “the ability to change your Google Account email address is gradually rolling out to all users.” Continue reading.This article originally appeared on Engadget at https://www.engadget.com/general/the-morning-after-engadget-newsletter-121511445.html?src=rss",
          "content": "CES 2026 is right around the corner, and the pre-show hype cycle/ early reveals suggest, yes, there’s going to be an awful lot of AI-powered insert-product-category-here alongside, thankfully, some major announcements from the likes of Intel, Sony and NVIDIA. Intel is finally unveiling its Panther Lake (Core Ultra Series 3) chips. The first chips built on Intel’s 2nm process could offer a 50 percent performance boost, which is sorely needed amid intense competition. NVIDIA’s Jensen Huang is taking the stage for a keynote expected to feature a lot of AI hype, while AMD’s Lisa Su will likely counter with new Ryzen 9000-series chips and the latest on AI upscaling tech. LG Over the years, CES has consistently been the show for TV innovation and heady next-gen displays. This year, we’ll be talking a lot about Micro RGB. LG is introducing a new Micro RGB Evo panel with over 1,000 dimming zones, while Samsung plans to launch a full range of Micro RGB TVs from 55 inches to 115 inches. In 2025, Sony introduced a new RGB LED panel that uses individual Mini LED backlights in red, green and blue to produce even brighter, more accurate colors. The company has trademarked “True RGB,” which could become what Sony calls its spin on RGB displays. We’ll be covering all the keynotes, press conferences and big reveals in person. And figuring which of the 100s of AI-branded devices and gadgets are worth reporting on. I’m also taking bets on the most niche celebrity appearance/endorsement at CES 2026. We’ve seen 50 Cent, Big Bird, Martha Stewart, Archbishop Desmond Tutu, Guillermo del Toro, Justin Bieber and will.i.am (multiple times), so who will join this pantheon of stars? — Mat Smith The other big stories this morning CD Projekt co-founder has acquired GOG, the company’s game storefront 2025 was the year Xbox died NASA finally has a leader, but its future is no more certain Samsung’s two new speakers will deliver crisp audio while blending into your decor The Music Studio 5 and 7 will be on display at CES 2026. Samsung It isn’t just TVs with Samsung. The company has already teased a pair of new understated speakers. Likely inspired by the Samsung Frame, the new Wi-Fi speakers, called the Music Studio 5 and 7, blend into your living room. The Music Studio 5 has a four-inch woofer and dual tweeters, with a built-in waveguide to deliver better sound. The Music Studio 7 comes with a 3.1.1-channel spatial audio with top-, front-, left- and right-firing speakers. No prices yet. Expect to hear more at CES itself or once the speakers arrive in stores. And as the press image above suggests, we can't wait to sit stoically in front of one, with a glass of water (?). Continue reading. Xiaomi’s 17 Ultra Leica Edition smartphone has a manual zoom ring And a 1-inch sensor, 200MP telephoto camera and 3,500 nit display. Xiaomi Xiaomi’s latest smartphone is once again a spec beast. It features a 1-inch sensor 50MP f/1.67 main camera and 1/1.4-inch 200MP periscope telephoto camera. And it also has an interesting new mechanical feature: a manual zoom ring. This surrounds the rear camera unit. Both the regular Xiaomi 17 Ultra and Leica edition come with a Snapdragon 8 Elite Gen 5 SoC with up to 16GB of LPDDR5X RAM and 1TB of UFS 4.1 storage, along with a 6.9-inch 120Hz AMOLED display that can hit up to 3,500 nits of peak brightness. But the camera features are the standout elements. The 17 Ultra by Leica adds some very, well, Leica touches: a two-tone finish, red dot status symbol on the front, textured edges and film simulations, like Leica’s Monopan 50 black and white. Xiaomi says the zoom ring “[eliminates] the need for tedious screen taps... and can detect displacements as small as 0.03mm.” It can also be reprogrammed for manual focus. Xiaomi’s 17 Ultra by Leica and the regular 17 Ultra start at CNY 7,999 ($1,140) and CNY 6,999 ($995), on par with the latest high-end Pixel 10s and Galaxy S25s. Continue reading. You may soon be able to change your Gmail address A Google support page in Hindi says the feature is ‘gradually rolling out to all users.’ A Google support page in Hindi indicates the ability to change your Gmail address might be coming. The feature would allow you to replace your current @gmail.com address with another. Your old address would remain active as an alias on the account, and all your data would stick around, unaffected. The support page (translated) says “the ability to change your Google Account email address is gradually rolling out to all users.” Continue reading.This article originally appeared on Engadget at https://www.engadget.com/general/the-morning-after-engadget-newsletter-121511445.html?src=rss",
          "feed_position": 21,
          "image_url": "https://s.yimg.com/os/creatr-uploaded-images/2025-12/a56cb510-e573-11f0-b7fd-8ea2e3a10112"
        },
        {
          "source": "VentureBeat",
          "url": "https://venturebeat.com/infrastructure/why-ai-adoption-fails-without-it-led-workflow-integration",
          "published_at": "Tue, 30 Dec 2025 08:00:00 GMT",
          "title": "Why AI adoption fails without IT-led workflow integration",
          "standfirst": "At 77-year-old promotional products company Gold Bond Inc., CIO Matt Price knew generative AI adoption wouldn’t come from rolling out a chatbot. Employees needed AI embedded into the work they already hated doing: messy ERP intake, document processing, and call follow-ups.Instead of pitching benchmarks, Price built a small group of “super-users” to surface Gold Bond–specific examples and train the rest of the org. They then wired Gemini and other models into high-friction workflows, backed by sandbox testing, guardrails, and human review for anything public-facing.The payoff showed up as behavior change, not hype: Daily AI usage rose from 20% to 71%, and 43% of employees reported saving up to two hours a day. “I wanted to bring everybody on the journey,” Price told VentureBeat. “After we reset some expectations, people started leaning towards it. Our adoption has taken off.”ERP streamlining, product visualizations Gold Bond, Inc. — not to be mistaken with the skin care company — is one of the largest suppliers in the $20.5 billion promotional products industry, producing custom swag and corporate gifts for 8,500 active customers.Orders, quotes, and sample requests arrive via the website, email, fax, and more — in every format imaginable. “So it gets very messy,” Price said.AI proved a natural fit. Previously, employees manually keyed order details into the ERP. Now, Google Cloud ingests incoming documents and normalizes them, while Gemini and OpenAI extract and structure the fields before pushing a completed purchase order into the system, Price said.From there, Gold Bond expanded into a pragmatic multi-model approach: Gemini inside Workspace, ChatGPT for backend automation, Claude for QA/reasoning checks, and smaller models for edge experiments.\"We’re pretty agnostic on utilizing AI technology,” Price said. Gold Bond is largely set up as a Google shop, with implementation and change management led by Google premier partner Promevo. Early wins included phone call summaries, email drafting, and contract review. A more advanced use case is AI-assisted “virtual mockups” of branded products; teams use Recraft to iterate on sample visuals before sending previews to customers, Price said.Employees also use AI to generate Google Sheets formulas (including Excel-style XLOOKUP logic), while NotebookLM helps build an internal knowledge base for procedures and training.Other ways Gold Bond uses AI internally: Presentations: Work that took four hours now takes about 30 minutes, Price said. Code auditing: Developers run NetSuite scripts, then use two models to review them before moving to testing.Research: Tracking importer trends and tactics in response to tariffs.AI also compresses early-stage planning. “We go back and forth with AI and come up with a high level project that we can then build out for execution,” Price explained. “We get to concepts a lot quicker. We have a lot fewer meetings, which is great.”To quantify impact, Price’s team runs Kaizen events — short workshops that document baseline workflows and compare them with AI- and automation-assisted versions.To validate multi-LLM workflows, Gold Bond tests changes in a sandbox environment and runs QA scenarios before rollout. “Our technical team, along with the subject matter experts, sign off prior to shipping the changes or integrating to production,” Price said.Change management is a mustAdoption wasn’t automatic — at a legacy company, change management was the work. “It&#x27;s just apprehension a little bit, it&#x27;s something different,” Price said. Most users start with Gemini because it’s built into Workspace, then move to ChatGPT, Claude, or Mistral when they need different capabilities — or a second opinion.Price relies on a “small cool group” of about eight early adopters to test bleeding-edge tools; once they land a use case, they train the rest of the team.“You can&#x27;t just look at something like a new piece of software,\" noted Promevo CTO John Pettit. \"You really have to change people&#x27;s thoughts and behaviors around it.”But even as Price&#x27;s team is promoting widespread use, blind trust is not an option, he emphasized. Gold Bond added policies, DLP controls, and identity layers to reduce shadow AI use. It also uses LibreChat to centralize access to approved tools, enforce paid/approved usage, and block certain models when needed.Human-in-the-loop is mandatory: Public-facing content goes through approval, and outputs must be verified. “You have to set the right temperature of trust, but verify,” he said. Even with strong prompts, outputs still require verification. “You get the data back, you can&#x27;t just blatantly take it and use it.”For instance, he’ll ask for sources and reasoning — “Give me all the work cited, where you are grabbing this data from” — and treats that verification step as part of the workflow, he said.Price also cautioned against overreach. “Agentic solutions can only go so far — there still need to be humans in the loop,” he said. “Some people have bigger visions than what the tech is capable of.”His advice for other enterprises: Don’t overwhelm yourself with the hype. Start simple. Start basic. “Provide detailed prompting, test it, play around with it.”",
          "content": "At 77-year-old promotional products company Gold Bond Inc., CIO Matt Price knew generative AI adoption wouldn’t come from rolling out a chatbot. Employees needed AI embedded into the work they already hated doing: messy ERP intake, document processing, and call follow-ups.Instead of pitching benchmarks, Price built a small group of “super-users” to surface Gold Bond–specific examples and train the rest of the org. They then wired Gemini and other models into high-friction workflows, backed by sandbox testing, guardrails, and human review for anything public-facing.The payoff showed up as behavior change, not hype: Daily AI usage rose from 20% to 71%, and 43% of employees reported saving up to two hours a day. “I wanted to bring everybody on the journey,” Price told VentureBeat. “After we reset some expectations, people started leaning towards it. Our adoption has taken off.”ERP streamlining, product visualizations Gold Bond, Inc. — not to be mistaken with the skin care company — is one of the largest suppliers in the $20.5 billion promotional products industry, producing custom swag and corporate gifts for 8,500 active customers.Orders, quotes, and sample requests arrive via the website, email, fax, and more — in every format imaginable. “So it gets very messy,” Price said.AI proved a natural fit. Previously, employees manually keyed order details into the ERP. Now, Google Cloud ingests incoming documents and normalizes them, while Gemini and OpenAI extract and structure the fields before pushing a completed purchase order into the system, Price said.From there, Gold Bond expanded into a pragmatic multi-model approach: Gemini inside Workspace, ChatGPT for backend automation, Claude for QA/reasoning checks, and smaller models for edge experiments.\"We’re pretty agnostic on utilizing AI technology,” Price said. Gold Bond is largely set up as a Google shop, with implementation and change management led by Google premier partner Promevo. Early wins included phone call summaries, email drafting, and contract review. A more advanced use case is AI-assisted “virtual mockups” of branded products; teams use Recraft to iterate on sample visuals before sending previews to customers, Price said.Employees also use AI to generate Google Sheets formulas (including Excel-style XLOOKUP logic), while NotebookLM helps build an internal knowledge base for procedures and training.Other ways Gold Bond uses AI internally: Presentations: Work that took four hours now takes about 30 minutes, Price said. Code auditing: Developers run NetSuite scripts, then use two models to review them before moving to testing.Research: Tracking importer trends and tactics in response to tariffs.AI also compresses early-stage planning. “We go back and forth with AI and come up with a high level project that we can then build out for execution,” Price explained. “We get to concepts a lot quicker. We have a lot fewer meetings, which is great.”To quantify impact, Price’s team runs Kaizen events — short workshops that document baseline workflows and compare them with AI- and automation-assisted versions.To validate multi-LLM workflows, Gold Bond tests changes in a sandbox environment and runs QA scenarios before rollout. “Our technical team, along with the subject matter experts, sign off prior to shipping the changes or integrating to production,” Price said.Change management is a mustAdoption wasn’t automatic — at a legacy company, change management was the work. “It&#x27;s just apprehension a little bit, it&#x27;s something different,” Price said. Most users start with Gemini because it’s built into Workspace, then move to ChatGPT, Claude, or Mistral when they need different capabilities — or a second opinion.Price relies on a “small cool group” of about eight early adopters to test bleeding-edge tools; once they land a use case, they train the rest of the team.“You can&#x27;t just look at something like a new piece of software,\" noted Promevo CTO John Pettit. \"You really have to change people&#x27;s thoughts and behaviors around it.”But even as Price&#x27;s team is promoting widespread use, blind trust is not an option, he emphasized. Gold Bond added policies, DLP controls, and identity layers to reduce shadow AI use. It also uses LibreChat to centralize access to approved tools, enforce paid/approved usage, and block certain models when needed.Human-in-the-loop is mandatory: Public-facing content goes through approval, and outputs must be verified. “You have to set the right temperature of trust, but verify,” he said. Even with strong prompts, outputs still require verification. “You get the data back, you can&#x27;t just blatantly take it and use it.”For instance, he’ll ask for sources and reasoning — “Give me all the work cited, where you are grabbing this data from” — and treats that verification step as part of the workflow, he said.Price also cautioned against overreach. “Agentic solutions can only go so far — there still need to be humans in the loop,” he said. “Some people have bigger visions than what the tech is capable of.”His advice for other enterprises: Don’t overwhelm yourself with the hype. Start simple. Start basic. “Provide detailed prompting, test it, play around with it.”",
          "feed_position": 4,
          "image_url": "https://images.ctfassets.net/jdtwqhzvc2n1/gHyCdU2l3DYUaXg0ct3lb/e2f02e0f859b29b566632ed017dda6e6/Gold_Bond.png?w=300&q=30"
        }
      ],
      "featured_image": "https://images.ctfassets.net/jdtwqhzvc2n1/3nh0udM8PXiBZTLMRhFdC9/6f049ddee65951d098ee322efbf0265d/1767215299__1_.png?w=300&q=30",
      "popularity_score": 2017.8296091666666
    },
    {
      "id": "cluster_32",
      "coverage": 2,
      "updated_at": "Wed, 31 Dec 2025 08:10:00 -0500",
      "title": "Dealroom: 76 European deep tech and life sciences companies spun out from universities reached $1B valuations or $100M in revenue in 2025, or both (Anna Heim/TechCrunch)",
      "neutral_headline": "Almost 80 European deep tech university spinouts reached $1B valuations or...",
      "items": [
        {
          "source": "TechMeme",
          "url": "http://www.techmeme.com/251231/p12#a251231p12",
          "published_at": "Wed, 31 Dec 2025 08:10:00 -0500",
          "title": "Dealroom: 76 European deep tech and life sciences companies spun out from universities reached $1B valuations or $100M in revenue in 2025, or both (Anna Heim/TechCrunch)",
          "standfirst": "Anna Heim / TechCrunch: Dealroom: 76 European deep tech and life sciences companies spun out from universities reached $1B valuations or $100M in revenue in 2025, or both &mdash; Universities and research labs have long been Europe's deep tech treasure trove. Now, academic spinouts have consolidated &hellip;",
          "content": "Anna Heim / TechCrunch: Dealroom: 76 European deep tech and life sciences companies spun out from universities reached $1B valuations or $100M in revenue in 2025, or both &mdash; Universities and research labs have long been Europe's deep tech treasure trove. Now, academic spinouts have consolidated &hellip;",
          "feed_position": 9,
          "image_url": "http://www.techmeme.com/251231/i12.jpg"
        },
        {
          "source": "TechCrunch",
          "url": "https://techcrunch.com/2025/12/30/76-european-deep-tech-university-spinouts-reached-unicorn-or-centaur-status/",
          "published_at": "Tue, 30 Dec 2025 18:00:00 +0000",
          "title": "Almost 80 European deep tech university spinouts reached $1B valuations or $100M in revenue in 2025",
          "standfirst": "According to Dealroom’s European Spinout Report 2025, 76 European deep tech and life sciences spinouts have either reached $1 billion valuations, $100 million in revenue, or both.",
          "content": "According to Dealroom’s European Spinout Report 2025, 76 European deep tech and life sciences spinouts have either reached $1 billion valuations, $100 million in revenue, or both.",
          "feed_position": 7
        }
      ],
      "featured_image": "http://www.techmeme.com/251231/i12.jpg",
      "popularity_score": 2009.8296091666666
    },
    {
      "id": "cluster_13",
      "coverage": 1,
      "updated_at": "Wed, 31 Dec 2025 16:30:33 +0000",
      "title": "Here we go again: Retiring coal plant forced to stay open by Trump Admin",
      "neutral_headline": "Here we go again: Retiring coal plant forced to stay open by Trump Admin",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2025/12/trump-admin-orders-another-coal-plant-to-stay-open/",
          "published_at": "Wed, 31 Dec 2025 16:30:33 +0000",
          "title": "Here we go again: Retiring coal plant forced to stay open by Trump Admin",
          "standfirst": "This time, a Colorado plant scheduled to shut down will be kept on standby.",
          "content": "On Tuesday, US Secretary of Energy Chris Wright issued a now familiar order: because of a supposed energy emergency, a coal plant scheduled for closure would be forced to remain open. This time, the order targeted one of the three units present at Craig Station in Colorado, which was scheduled to close at the end of this year. The remaining two units were expected to shut in 2028. The supposed reason for this order is an emergency caused by a shortage of generating capacity. \"The reliable supply of power from the coal plant is essential for keeping the region’s electric grid stable,\" according to a statement issued by the Department of Energy. Yet the Colorado Sun notes that Colorado's Public Utilities Commission had already analyzed the impact of its potential closure, and determined, \"Craig Unit 1 is not required for reliability or resource adequacy purposes.\" The order does not require the plant to actually produce electricity; instead, it is ordered to be available in case a shortfall in production occurs. As noted in the Colorado Sun article, actual operation of the plant would potentially violate Colorado laws, which regulate airborne pollution and set limits on greenhouse gas emissions. The cost of maintaining the plant is likely to fall on the local ratepayers, who had already adjusted to the closure plans.Read full article Comments",
          "feed_position": 0,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-161350245.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-161350245.jpg",
      "popularity_score": 361.17210916666664
    },
    {
      "id": "cluster_30",
      "coverage": 1,
      "updated_at": "Wed, 31 Dec 2025 13:15:40 +0000",
      "title": "Supply chains, AI, and the cloud: The biggest failures (and one success) of 2025",
      "neutral_headline": "Supply chains, AI, and the cloud: The biggest failures (and one success) of 2025",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/security/2025/12/supply-chains-ai-and-the-cloud-the-biggest-failures-and-one-success-of-2025/",
          "published_at": "Wed, 31 Dec 2025 13:15:40 +0000",
          "title": "Supply chains, AI, and the cloud: The biggest failures (and one success) of 2025",
          "standfirst": "The past year has seen plenty of hacks and outages. Here are the ones topping the list.",
          "content": "In a roundup of the top stories of 2024, Ars included a supply-chain attack that came dangerously close to inflicting a catastrophe for thousands—possibly millions—of organizations, which included a large assortment of Fortune 500 companies and government agencies. Supply-chain attacks played prominently again this year, as a seemingly unending rash of them hit organizations large and small. For threat actors, supply-chain attacks are the gift that keeps on giving—or, if you will, the hack that keeps on hacking. By compromising a single target with a large number of downstream users—say a cloud service or maintainers or developers of widely used open source or proprietary software—attackers can infect potentially millions of the target’s downstream users. That’s exactly what threat actors did in 2025. Poisoning the well One such event occurred in December 2024, making it worthy of a ranking for 2025. The hackers behind the campaign pocketed as much as $155,000 from thousands of smart-contract parties on the Solana blockchain.Read full article Comments",
          "feed_position": 1,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/signal-quantum-security-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/10/signal-quantum-security-1152x648.jpg",
      "popularity_score": 347.9240536111111
    },
    {
      "id": "cluster_41",
      "coverage": 1,
      "updated_at": "Wed, 31 Dec 2025 12:00:31 +0000",
      "title": "From prophet to product: How AI came back down to earth in 2025",
      "neutral_headline": "From prophet to product: How AI came back down to earth in 2025",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/ai/2025/12/from-prophet-to-product-how-ai-came-back-down-to-earth-in-2025/",
          "published_at": "Wed, 31 Dec 2025 12:00:31 +0000",
          "title": "From prophet to product: How AI came back down to earth in 2025",
          "standfirst": "In a year where lofty promises collided with inconvenient research, would-be oracles became software tools.",
          "content": "Following two years of immense hype in 2023 and 2024, this year felt more like a settling-in period for the LLM-based token prediction industry. After more than two years of public fretting over AI models as future threats to human civilization or the seedlings of future gods, it's starting to look like hype is giving way to pragmatism: Today's AI can be very useful, but it's also clearly imperfect and prone to mistakes. That view isn't universal, of course. There's a lot of money (and rhetoric) betting on a stratospheric, world-rocking trajectory for AI. But the \"when\" keeps getting pushed back, and that's because nearly everyone agrees that more significant technical breakthroughs are required. The original, lofty claims that we're on the verge of artificial general intelligence (AGI) or superintelligence (ASI) have not disappeared. Still, there's a growing awareness that such proclaimations are perhaps best viewed as venture capital marketing. And every commercial foundational model builder out there has to grapple with the reality that, if they're going to make money now, they have to sell practical AI-powered solutions that perform as reliable tools. This has made 2025 a year of wild juxtapositions. For example, in January, OpenAI's CEO, Sam Altman, claimed that the company knew how to build AGI, but by November, he was publicly celebrating that GPT-5.1 finally learned to use em dashes correctly when instructed (but not always). Nvidia soared past a $5 trillion valuation, with Wall Street still projecting high price targets for that company's stock while some banks warned of the potential for an AI bubble that might rival the 2000s dotcom crash.Read full article Comments",
          "feed_position": 2,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/ai-down-to-earth-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/ai-down-to-earth-1152x648.jpg",
      "popularity_score": 336.6715536111111
    },
    {
      "id": "cluster_64",
      "coverage": 1,
      "updated_at": "Tue, 30 Dec 2025 21:30:04 +0000",
      "title": "The science of how (and when) we decide to speak out—or self-censor",
      "neutral_headline": "The science of how (and when) we decide to speak out—or self-censor",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2025/12/the-science-of-how-and-when-we-decide-to-speak-out-or-self-censor/",
          "published_at": "Tue, 30 Dec 2025 21:30:04 +0000",
          "title": "The science of how (and when) we decide to speak out—or self-censor",
          "standfirst": "The study's main takeaway: \"Be bold. It is the thing that slows down authoritarian creep.\"",
          "content": "Freedom of speech is a foundational principle of healthy democracies and hence a primary target for aspiring authoritarians, who typically try to squash dissent. There is a point where the threat from authorities is sufficiently severe that a population will self-censor rather than risk punishment. Social media has complicated matters, blurring traditional boundaries between public and private speech, while new technologies such as facial recognition and moderation algorithms give authoritarians powerful new tools. Researchers explored the nuanced dynamics of how people balance their desire to speak out vs their fear of punishment in a paper published in the Proceedings of the National Academy of Sciences. The authors had previously worked together on a model of political polarization, a project that wrapped up right around the time the social media space was experiencing significant changes in the ways different platforms were handling moderation. Some adopted a decidedly hands-off approach with little to no moderation. Weibo, on the other hand, began releasing the IP addresses of people who posted objectionable commentary, essentially making them targets.Read full article Comments",
          "feed_position": 3,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2014/05/censorship_2-1152x648-1767113011.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2014/05/censorship_2-1152x648-1767113011.jpg",
      "popularity_score": 303
    },
    {
      "id": "cluster_66",
      "coverage": 1,
      "updated_at": "Tue, 30 Dec 2025 20:45:34 +0000",
      "title": "Lawsuit over Trump rejecting medical research grants is settled",
      "neutral_headline": "Lawsuit over Trump rejecting medical research grants is settled",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2025/12/feds-researchers-settle-suit-over-grants-blocked-by-now-illegal-order/",
          "published_at": "Tue, 30 Dec 2025 20:45:34 +0000",
          "title": "Lawsuit over Trump rejecting medical research grants is settled",
          "standfirst": "Settlement forces NIH to review grants previously rejected on ideological grounds.",
          "content": "On Monday, the ACLU announced that it and other organizations representing medical researchers had reached a settlement in their suit against the federal government over grant applications that had been rejected under a policy that has since been voided by the court. The agreement, which still has to be approved by the judge overseeing the case, would see the National Institutes of Health restart reviews of grants that had been blocked on ideological grounds. It doesn't guarantee those grants will ultimately be funded, but it does mean they will go through the standard peer review process. The grants had previously been rejected without review because their content was ideologically opposed by the Trump administration. That policy has since been declared arbitrary and capricious, and thus in violation of the Administrative Procedure Act, a decision that was upheld by the Supreme Court. How'd we get here? Immediately after taking office, the Trump Administration identified a number of categories of research, some of them extremely vague, that it would not be supporting: climate change, DEI, pandemic preparedness, gender ideology, and more. Shortly thereafter, federal agencies started cancelling grants that they deemed to contain elements of these disfavored topics, and blocking consideration of grant applications for the same reasons. As a result, grants were cancelled that funded everything from research into antiviral drugs to the incidence of prostate cancer in African Americans.Read full article Comments",
          "feed_position": 4,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2224898877-1024x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2224898877-1024x648.jpg",
      "popularity_score": 293
    },
    {
      "id": "cluster_67",
      "coverage": 1,
      "updated_at": "Tue, 30 Dec 2025 20:30:01 +0000",
      "title": "DOGE did not find $2T in fraud, but that doesn’t matter, Musk allies say",
      "neutral_headline": "DOGE did not find $2T in fraud, but that doesn’t matter, Musk allies say",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/12/doge-did-not-find-2t-in-fraud-but-that-doesnt-matter-musk-allies-say/",
          "published_at": "Tue, 30 Dec 2025 20:30:01 +0000",
          "title": "DOGE did not find $2T in fraud, but that doesn’t matter, Musk allies say",
          "standfirst": "Musk allies spin DOGE as having a \"higher purpose\" beyond federal budget cuts.",
          "content": "Determining how \"successful\" Elon Musk's Department of Government Efficiency (DOGE) truly was depends on who you ask, but it's increasingly hard to claim that DOGE made any sizable dent in federal spending, which was its primary goal. Just two weeks ago, Musk himself notably downplayed DOGE as only being \"a little bit successful\" on a podcast, marking one of the first times that Musk admitted DOGE didn't live up to its promise. Then, more recently, on Monday, Musk revived evidence-free claims he made while campaigning for Donald Trump, insisting that government fraud remained vast and unchecked, seemingly despite DOGE's efforts. On X, he estimated that \"my lower bound guess for how much fraud there is nationally is [about 20 percent] of the Federal budget, which would mean $1.5 trillion per year. Probably much higher.\" Musk loudly left DOGE in May after clashing with Trump, complaining that a Trump budget bill threatened to undermine DOGE's work. These days, Musk does not appear confident that DOGE was worth the trouble of wading into government. Although he said on the December podcast that he considered DOGE to be his \"best side quest\" ever, the billionaire confirmed that if given the chance to go back in time, he probably would not have helmed the agency as a special government employee.Read full article Comments",
          "feed_position": 5,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2217857584-1024x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2217857584-1024x648.jpg",
      "popularity_score": 283
    },
    {
      "id": "cluster_72",
      "coverage": 1,
      "updated_at": "Tue, 30 Dec 2025 19:00:00 +0000",
      "title": "Stranger Things series finale trailer is here",
      "neutral_headline": "Stranger Things series finale trailer is here",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/culture/2025/12/stranger-things-series-finale-trailer-is-here/",
          "published_at": "Tue, 30 Dec 2025 19:00:00 +0000",
          "title": "Stranger Things series finale trailer is here",
          "standfirst": "Netflix's finale will also have a two-day theatrical release to more than 600 locations.",
          "content": "Stranger Things fans are hyped for the premiere of the hotly anticipated series finale on New Year's Eve: they'll either be glued to their TVs or heading out to watch it in a bona fide theater. Netflix has dropped one last trailer for the finale—not that it really needs to do anything more to boost anticipation. (Some spoilers for Vols. 1 and 2 below but no major Vol. 2 reveals.) As previously reported, in Vol. 1, we found Hawkins under military occupation and Vecna targeting a new group of young children in his human form under the pseudonym “Mr. Whatsit” (a nod to A Wrinkle in Time). He kidnapped Holly Wheeler and took her to the Upside Down, where she found an ally in Max, still in a coma, but with her consciousness hiding in one of Vecna’s old memories. Dustin was struggling to process his grief over losing Eddie Munson in S4, causing a rift with Steve. The rest of the gang was devoted to stockpiling supplies and helping Eleven and Hopper track down Vecna in the Upside Down. They found Kali/Eight, Eleven’s psychic “sister” instead, being held captive in a military laboratory.Read full article Comments",
          "feed_position": 7,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/finale3-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/finale3-1152x648.jpg",
      "popularity_score": 275
    },
    {
      "id": "cluster_68",
      "coverage": 1,
      "updated_at": "Tue, 30 Dec 2025 20:00:43 +0000",
      "title": "NJ’s answer to flooding: it has bought out and demolished 1,200 properties",
      "neutral_headline": "NJ’s answer to flooding: it has bought out and demolished 1,200 properties",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/tech-policy/2025/12/as-floods-become-more-severe-a-new-jersey-program-provides-a-model/",
          "published_at": "Tue, 30 Dec 2025 20:00:43 +0000",
          "title": "NJ’s answer to flooding: it has bought out and demolished 1,200 properties",
          "standfirst": "The state deals with flooding and sea level rise by buying homes in flood prone areas.",
          "content": "MANVILLE, N.J.—Richard Onderko said he will never forget the terrifying Saturday morning back in 1971 when the water rose so swiftly at his childhood home here that he and his brother had to be rescued by boat as the torrential rain from the remnants of Hurricane Doria swept through the neighborhood. It wasn’t the first time—or the last—that the town endured horrific downpours. In fact, the working-class town of 11,000, about 25 miles southwest of Newark, has long been known for getting swamped by tropical storms, nor’easters or even just a wicked rain. It was so bad, Onderko recalled, that the constant threat of flooding had strained his parents’ marriage, with his mom wanting to sell and his dad intent on staying. Eventually, his parents moved to Florida, selling the two-story house on North Second Avenue in 1995. But the new homeowner didn’t do so well either when storms hit, and in 2015, the property was sold one final time: to a state-run program that buys and demolishes houses in flood zones and permanently restores the property to open space.Read full article Comments",
          "feed_position": 6,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-113500412-1152x648-1767123440.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-113500412-1152x648-1767123440.jpg",
      "popularity_score": 273
    },
    {
      "id": "cluster_76",
      "coverage": 1,
      "updated_at": "Tue, 30 Dec 2025 18:45:54 +0000",
      "title": "Condé Nast user database reportedly breached, Ars unaffected",
      "neutral_headline": "Condé Nast user database reportedly breached, Ars unaffected",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/information-technology/2025/12/conde-nast-user-database-reportedly-breached-ars-unaffected/",
          "published_at": "Tue, 30 Dec 2025 18:45:54 +0000",
          "title": "Condé Nast user database reportedly breached, Ars unaffected",
          "standfirst": "A serious data breach has occurred, but Ars users have nothing to worry about.",
          "content": "Earlier this month, a hacker named Lovely claimed to have breached a Condé Nast user database and released a list of more than 2.3 million user records from our sister publication WIRED. The released materials contain demographic information (name, email, address, phone, etc.) but no passwords. The hacker also says that they will release an additional 40 million records for other Condé Nast properties, including our other sister publications Vogue, The New Yorker, Vanity Fair, and more. Of critical note to our readers, Ars Technica was not affected as we run on our own bespoke tech stack. The hacker said that they had urged Condé Nast to patch vulnerabilities to no avail. “Condé Nast does not care about the security of their users data,” the hacker wrote. “It took us an entire month to convince them to fix the vulnerabilities on their websites. We will leak more of their users’ data (40+ million) over the next few weeks. Enjoy!”Read full article Comments",
          "feed_position": 8,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/conde-hack-20205-1152x648-1767119864.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/conde-hack-20205-1152x648-1767119864.jpg",
      "popularity_score": 253
    },
    {
      "id": "cluster_87",
      "coverage": 1,
      "updated_at": "Tue, 30 Dec 2025 15:00:56 +0000",
      "title": "Looking for friends, lobsters may stumble into an ecological trap",
      "neutral_headline": "Looking for friends, lobsters may stumble into an ecological trap",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/science/2025/12/looking-for-friends-lobsters-may-stumble-into-an-ecological-trap/",
          "published_at": "Tue, 30 Dec 2025 15:00:56 +0000",
          "title": "Looking for friends, lobsters may stumble into an ecological trap",
          "standfirst": "Gathering for mutual defense puts young spiny lobsters at risk of predators.",
          "content": "Lobsters are generally notable for their large claws, which can serve as a deterrent to any predators. But there's a whole family of spiny lobsters that lack these claws. They tend to ward off predators by forming large groups that collectively can present a lot of pointy bits towards anything attempting to eat them. In fact, studies found that the lobsters can sense the presence of other species-members using molecules emitted into the water, and use that to find peers to congregate with. A new study, however, finds that this same signal may lure young lobsters to their doom, causing them to try to congregate with older lobsters that are too big to be eaten by nearby predators. The smaller lobsters thus fall victim to a phenomenon called an \"ecological trap,\" which has rarely been seen to occur without human intervention. Lobsters vs. groupers The study was performed in the waters off Florida, where the seafloor is dotted by what are called \"solution holes.\" These features are the product of lower sea levels such as those that occur during periods of expanded glaciers and ice caps. During these times, much of the area off Florida was above sea level, and water dissolved the limestone rocks unevenly. This created an irregular array of small shallow pits and crevices, many of which have been reshaped by sea life since the area was submerged again.Read full article Comments",
          "feed_position": 9,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2251921243-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2251921243-1152x648.jpg",
      "popularity_score": 243
    },
    {
      "id": "cluster_92",
      "coverage": 1,
      "updated_at": "Tue, 30 Dec 2025 14:00:27 +0000",
      "title": "The top 5 most horrifying and fascinating medical cases of 2025",
      "neutral_headline": "The top 5 most horrifying and fascinating medical cases of 2025",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/health/2025/12/the-top-5-most-horrifying-and-fascinating-medical-cases-of-2025/",
          "published_at": "Tue, 30 Dec 2025 14:00:27 +0000",
          "title": "The top 5 most horrifying and fascinating medical cases of 2025",
          "standfirst": "Florida man makes two appearances on the list.",
          "content": "There were a lot of horrifying things in the news this year—a lot. But some of it was horrifying in a good way. Extraordinary medical cases—even the grisly and disturbing ones—offer a reprieve from the onslaught of current events and the stresses of our daily lives. With those remarkable reports, we can marvel at the workings, foibles, and resilience of the human body. They can remind us of the shared indignities from our existence in these mortal meatsacks. We can clear our minds of worry by learning about something we never even knew we should worry about—or by counting our blessings for avoiding so far. And sometimes, the reports are just grotesquely fascinating. Every year, there's a new lineup of such curious clinical conditions. There are always some unfortunate souls to mark medical firsts or present ultra-rare cases. There is also an endless stream of humans making poor life choices—and arriving at an emergency department with the results. This year was no different.Read full article Comments",
          "feed_position": 10,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/moles-many-medical-marvels-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/moles-many-medical-marvels-1152x648.jpg",
      "popularity_score": 133
    },
    {
      "id": "cluster_95",
      "coverage": 1,
      "updated_at": "Tue, 30 Dec 2025 13:30:40 +0000",
      "title": "The 10 best vehicles Ars Technica drove in 2025",
      "neutral_headline": "The 10 best vehicles Ars Technica drove in 2025",
      "items": [
        {
          "source": "Ars Technica",
          "url": "https://arstechnica.com/cars/2025/12/the-10-best-vehicles-ars-technica-drove-in-2025/",
          "published_at": "Tue, 30 Dec 2025 13:30:40 +0000",
          "title": "The 10 best vehicles Ars Technica drove in 2025",
          "standfirst": "Of all the cars we've driven and reviewed this year, these are our picks.",
          "content": "2025 has been a tumultuous year for the car world. After years of EV optimism, revanchists are pushing back against things like clean energy and fuel economy. Automakers have responded, postponing or canceling new electric vehicles in favor of gasoline-burning ones. It hasn't been all bad, though. Despite the changing winds, EV infrastructure continues to be built out and, anecdotally at least, feels far more reliable. We got to witness a pretty epic Formula 1 season right to the wire, in addition to some great sports car and Formula E racing. And we drove a whole bunch of cars, some of which stood out from the pack. Here are the 10 best things we sat behind the wheel of in 2025. 10th: Lotus Emira V6 A Lotus Emira doesn't need to be painted this bright color to remind you that driving can be a pleasure. Credit: Peter Nelson Let's be frank: The supposed resurgence of Lotus hasn't exactly gone to plan. When Geely bought the British Automaker in 2017, many of us hoped that the Chinese company would do for Lotus what it did for Volvo, only in Hethel instead of Gothenburg. Even before tariffs and other protectionist measures undermined the wisdom of building new Lotuses in China, the fact that most of these new cars were big, heavy EVs had already made them a hard sell. But a more traditional Lotus exists and is still built in Norfolk, England: the Lotus Emira.Read full article Comments",
          "feed_position": 11,
          "image_url": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/ars-best-cars-2025-1152x648.jpg"
        }
      ],
      "featured_image": "https://cdn.arstechnica.net/wp-content/uploads/2025/12/ars-best-cars-2025-1152x648.jpg",
      "popularity_score": 130
    }
  ]
}